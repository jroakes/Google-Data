US20230280974A1 - Rendering visual components on applications in response to voice commands - Google Patents
Rendering visual components on applications in response to voice commands Download PDFInfo
- Publication number
- US20230280974A1 US20230280974A1 US18/144,740 US202318144740A US2023280974A1 US 20230280974 A1 US20230280974 A1 US 20230280974A1 US 202318144740 A US202318144740 A US 202318144740A US 2023280974 A1 US2023280974 A1 US 2023280974A1
- Authority
- US
- United States
- Prior art keywords
- action
- application
- agent
- client device
- component
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000000007 visual effect Effects 0.000 title claims abstract description 22
- 238000009877 rendering Methods 0.000 title claims abstract description 21
- 230000004044 response Effects 0.000 title claims description 77
- 230000009471 action Effects 0.000 claims abstract description 376
- 238000000034 method Methods 0.000 claims abstract description 251
- 230000008569 process Effects 0.000 claims abstract description 210
- 238000012545 processing Methods 0.000 claims abstract description 160
- 230000005236 sound signal Effects 0.000 claims abstract description 153
- 238000013475 authorization Methods 0.000 claims description 44
- 239000003795 chemical substances by application Substances 0.000 description 553
- 230000006870 function Effects 0.000 description 66
- 230000003993 interaction Effects 0.000 description 24
- 238000004891 communication Methods 0.000 description 23
- 238000013507 mapping Methods 0.000 description 22
- 230000014509 gene expression Effects 0.000 description 15
- 238000004590 computer program Methods 0.000 description 11
- 235000013353 coffee beverage Nutrition 0.000 description 9
- 238000010586 diagram Methods 0.000 description 9
- 238000013515 script Methods 0.000 description 7
- 238000004422 calculation algorithm Methods 0.000 description 5
- 238000003058 natural language processing Methods 0.000 description 5
- 230000008859 change Effects 0.000 description 4
- 238000001914 filtration Methods 0.000 description 4
- 230000000670 limiting effect Effects 0.000 description 4
- 238000010801 machine learning Methods 0.000 description 4
- 238000007726 management method Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 230000005540 biological transmission Effects 0.000 description 3
- 235000015115 caffè latte Nutrition 0.000 description 3
- 238000003780 insertion Methods 0.000 description 3
- 230000037431 insertion Effects 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000003491 array Methods 0.000 description 2
- 230000003190 augmentative effect Effects 0.000 description 2
- 239000003599 detergent Substances 0.000 description 2
- 238000003384 imaging method Methods 0.000 description 2
- 238000012423 maintenance Methods 0.000 description 2
- 239000003550 marker Substances 0.000 description 2
- 238000007781 pre-processing Methods 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000001360 synchronised effect Effects 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 238000010200 validation analysis Methods 0.000 description 2
- IRLPACMLTUPBCL-KQYNXXCUSA-N 5'-adenylyl sulfate Chemical compound C1=NC=2C(N)=NC=NC=2N1[C@@H]1O[C@H](COP(O)(=O)OS(O)(=O)=O)[C@@H](O)[C@H]1O IRLPACMLTUPBCL-KQYNXXCUSA-N 0.000 description 1
- 230000004075 alteration Effects 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000015556 catabolic process Effects 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 238000006731 degradation reaction Methods 0.000 description 1
- 230000000593 degrading effect Effects 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000011143 downstream manufacturing Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000008030 elimination Effects 0.000 description 1
- 238000003379 elimination reaction Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 230000000877 morphologic effect Effects 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/48—Program initiating; Program switching, e.g. by interrupt
- G06F9/4806—Task transfer initiation or dispatching
- G06F9/4843—Task transfer initiation or dispatching by program, e.g. task dispatcher, supervisor, operating system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/54—Interprogram communication
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/54—Interprogram communication
- G06F9/545—Interprogram communication where tasks reside in different layers, e.g. user- and kernel-space
Definitions
- Applications can be installed on a computing device.
- the computing device can execute the application.
- the application can present digital content.
- a system to render visual components on applications can include an agent registry executed on a data processing system having one or more processors.
- the agent registry can maintain a plurality of action-inventories for a plurality of applications. At least one of the plurality of action-inventories can indicate to render a user interface component for one of the plurality of applications in executing an action.
- the system can include a natural language processor executed on the data processing system.
- the natural language processor can receive a data packet comprising an input audio signal detected by a sensor of a client device.
- the client device can display a graphical user interface of a first application in a foreground process on the client device.
- the natural language processor can parse the input audio signal of the data packet to identify a request.
- the system can include an action handler executed on the data processing system.
- the action handler can select, from the plurality of action-inventories, an action-inventory that executes the action corresponding to the request by a second application of the plurality of applications.
- the second application can be installed on the client device and not in the foreground process.
- the action handler can generate an action data structure in accordance with the action-inventory.
- the system can include an agent interface executed on the data processing system.
- the agent interface can provide the action data structure to the second application to cause the second application to parse the action data structure and execute the action to generate an output.
- the agent interface can determine that the output of the second application from execution of the action is authorized to be presented with the graphical user interface of the first application based on an authorization policy of the second application.
- the agent interface can identify the user interface component of the second application for the action-inventory selected from the plurality of action-inventories, responsive to the determination the output of the second application is authorized to be presented with the first application.
- the agent interface can display, on the client device, the user interface component including the output from the second application with the graphical user interface of the first application authorized to be presented with the second application.
- At least one of the plurality of action-inventories may have an address template for accessing the user interface component in executing the action, the address template defining a first portion and a second portion, the first portion corresponding to one of the plurality of applications, the second portion including an input variable in carrying out the action.
- the natural language processor may parse the input audio signal of the data packet to identify a parameter defining the request to be executed by the second application.
- the action handler may generate, in accordance with the address template of the action-inventory, an address to execute the action of the action-inventory, the address comprising a first substring and a second substring, the first substring corresponding to the second application, the second substring having the parameter used to control execution of the action; and generate the action data structure including the address generated by the action handler in accordance with the address template.
- At least one of the plurality of action-inventories may have an address template for accessing the user interface component in executing the action, the address template defining a first portion and a second portion, the first portion corresponding to one of the plurality of applications, the second portion including an input variable in carrying out the action.
- the natural language processor may parse the input audio signal of the data packet to identify the parameter defining the request to be executed by the second application.
- the action handler to generate, in accordance with the address template of the action-inventory, an address to execute the action of the action-inventory, the address comprising a first substring and a second substring, the first substring corresponding to the second application, the second substring having the parameter used to control execution of the action.
- the agent interface may identify the user interface component of the second application for the action-inventory selected from the plurality of action-inventories using the address generated in accordance with the address template for the action-inventory.
- Each action-inventory may have an agent identifier corresponding to one of the plurality of applications and a request identifier corresponding to the action.
- the natural language processor may parse the input audio signal of the data packet to identify an agent identifier corresponding to an agent, the agent corresponding to the second application installed on the client device.
- the action handler may select, from the plurality of action-inventories, the action-inventory for executing the action having the agent identifier corresponding to the agent identifier identified from parsing the input audio signal.
- the action handler may determine that the interface mode indicator of the action-inventory specifies rendering of the user interface component in executing the action, and identify the user interface component of the second application for the action-inventory, responsive to the determination that the interface mode indicator specifies rendering of the user interface component.
- the agent interface may determine that the client device is not authenticated with the second application for the action-inventory to carry out the action corresponding to a second request; and may present, responsive to the determination that the client device is not authenticated with the second application, a prompt interface on the client device to authenticate the client device with the second application to execute the action corresponding to the second request.
- the agent interface may determine that an output of a third application is not authorized to be presented with the first application based on an authorization policy of the third application, the third application installed on the client device and not in the foreground process, set the third application as the foreground process of the client device, the first application transferred from the foreground process to a non-foreground process running on the client device, and display, on the client device, a graphical user interface of the third application including an output generated by the third application.
- the agent interface may determine that the first application authorizes presentation of the user interface component of the second application with the graphical user interface of the first application based on an authorization policy of the first application, the authorization policy permitting user interface components from a first subset of the plurality of applications to be presented with the graphical user interface and restricting user interface components from a second subset of the plurality of applications to be presented with the graphical user interface, and display, on the client device, the user interface component including the output from the second application with the graphical user interface of the first application, responsive to the determination that the first application authorizes the presentation of the user interface component of the second application with the graphical user interface of the first application.
- the agent interface may provide the action data structure to the second application to cause the second application to monitor for an interaction event on the user interface component from the second application presented with the graphical user interface of the first application and process, responsive to detection of the interaction event, the interaction event to update the user interface component.
- the natural language processor may receive the data packet via a digital assistant application, the data packet comprising the input audio signal detected by the sensor of the client device, the client device displaying the graphical user interface of the digital assistant application corresponding to the first application in the foreground process.
- the agent interface may display, within the graphical user interface of the digital assistant application, the user interface component including a subcomponent indicating the request identified from parsing the input audio signal and the user interface component including the output from an agent corresponding to the second application.
- the agent may lack natural language processing capabilities.
- the natural language processor may receive the data packet, the data packet comprising the input audio signal detected by the sensor of the client device, the client device displaying the graphical user interface of a first agent corresponding to the first application in the foreground process; and the agent interface to display the user interface component including the output from the second application as an overlay on the graphical user interface of the first agent.
- the plurality of action-inventories may have a first subset of action-inventories and a second subset of action-inventories, the first subset including action-inventories defined by an administrator of the data processing system, the second subset including action-inventories provided by an agent service handling one of the plurality of applications.
- the action handler may provide the action data structure to the second application to cause the second application to parse the action data structure to identify the action to be executed, generate the output by executing the action identified from the action data structure and provide the output for the user interface component to be presented with the graphical user interface of the first application authorized to be presented with the second application.
- a data processing system having one or more processors can maintain a plurality of action-inventories for a plurality of applications. At least one of the plurality of action-inventories can indicate to render a user interface component for one of the plurality of applications in executing an action.
- the data processing system can receive a data packet comprising an input audio signal detected by a sensor of a client device.
- the client device can display a graphical user interface of a first application in a foreground process on the client device.
- the data processing system can parse the input audio signal of the data packet to identify a request.
- the data processing system can select, from the plurality of action-inventories, an action-inventory that executes the action corresponding to the request by a second application of the plurality of applications.
- the second application can be installed on the client device and not in the foreground process.
- the data processing system can generate an action data structure in accordance with the action-inventory.
- the data processing system can provide the action data structure to the second application to cause the second application to parse the action data structure and execute the action to generate an output.
- the data processing system can determine that the output of the second application from execution of the action is authorized to be presented with the graphical user interface of the first application based on an authorization policy of the second application.
- the data processing system can identify the user interface component of the second application for the action-inventory selected from the plurality of action-inventories, responsive to the determination the output of the second application is authorized to be presented with the first application.
- the data processing system can display, on the client device, the user interface component including the output from the second application with the graphical user interface of the first application authorized to be presented with the second application.
- FIG. 1 illustrates a block diagram of an example system to render visual components on applications in response to voice input commands, in accordance with an example of the present disclosure.
- FIG. 2 illustrates a sequence diagram of an example data flow to render visual components on applications in response to voice input commands in the system illustrated in FIG. 1 , in accordance with an example of the present disclosure.
- FIG. 3 illustrates a use case diagram of an example client device displaying input messages and generating addresses in the system illustrated in FIG. 1 , in accordance with an example of the present disclosure.
- FIG. 4 illustrates a flow diagram of a method to render visual components on applications in response to voice input commands using the example system illustrated in FIG. 1 , in accordance with an example of the present disclosure.
- FIG. 5 is a block diagram of an example computer system.
- Multiple applications can be installed on a client device to carry out preconfigured functions as requested by a user operating the client device.
- the client device can run a subset of these applications as a foreground process or a non-foreground process as set by an operating system running on client device.
- Foreground processes can correspond to applications with a graphical user interface presented on a display of the client device with which the user may be interacting.
- Non-foreground processes e.g., a service process, a background process, or a cached process
- the applications running on the client device can switch back and forth between executing as a foreground process or non-foreground process.
- the graphical user interface of the application that was previously the foreground process can be replaced by the graphical user interface of the application that is now set as the foreground process.
- the processing and memory allocations can change as a result of the switching between foreground and non-foreground processes.
- One of these applications running in the foreground or the non-foreground can include a digital assistant application.
- the digital assistant application can have natural language processing capabilities. Configured with these capabilities, the digital assistant application can acquire and parse an input audio signal detected via a microphone of the client device to recognize words from the input audio signal. From the recognized words, the digital assistant application can determine a request to be carried out and one or more parameters defining the request. The request can refer to one of the preconfigured functions with input arguments compatible with the parameters that can be executed by the digital assistant application (e.g., retrieving results for a search query). For such requests, the digital assistant application itself can process the request and present an audio or visual output from carrying out the request.
- Some requests may not correspond to any of the functions preconfigured on the digital assistant application (e.g., playing a video from an online video platform).
- the digital assistant application can identify another application that is capable of carrying out the request and invoke the function of the application to pass parameters.
- the digital assistant application can cause a change in which applications are foreground and non-foreground processes, resulting in a shift in the graphical user interface displayed on the client device.
- the processing and memory allocations to each executing application can change and fluctuate, especially when invoking a previously non-executing application.
- the switch can interfere with the overall user experience with the applications on the client device.
- the sudden alteration in the graphical user interface displayed on the client device can force the user to under context switching between the interfaces of multiple applications, thereby degrading human-computer interactions (HCI) with the client device.
- HCI human-computer interactions
- the degradation in the client device can also be exacerbated by change and fluctuation in the new allocations of computing resources in performing the switching of foreground and non-foreground processes.
- the present disclosure provides an improved user interface.
- the present disclosure can provide action-inventories used to identify and present user interface components to be overlaid on the graphical user interfaces of applications in the foreground.
- the action-inventory can specify whether to provide a user interface element of the application while maintaining the application in the background.
- the action-inventory can also include an address template to construct an address for accessing a function of the digital assistant application and for providing the user interface element on the client device.
- the address template of an action-inventory may be, for example, a Uniform Resource Identifier (URI) template for constructing a web address with a hostname, pathname, and queries to invoke a function of an application and for the user interface component.
- URI Uniform Resource Identifier
- the hostname (or a scheme) can reference a location or a domain name of the application.
- the pathname can reference the function.
- the queries can include one or more input arguments used to carry out the function.
- the address template can also define a mapping of the request parsed from input audio signals to functions to be executed by the application.
- the digital assistant application can determine that the request does not correspond with any of the functionalities of the digital assistant application or to any application running as the foreground process. Instead of outputting that the digital assistant application cannot perform the indicated request, the digital assistant application can identify another application to carry out the function. While processing the request, the digital assistant application can remain as a background process with the original application remaining as the foreground process. With the identification, the digital assistant application can select an action-inventory of the application for the function. The selected action-inventory can indicate that a user interface component of the application is to be presented. Using the action-inventory, the digital assistant application can construct, expand, and generate the address for the function in accordance with the address template. The address can include the hostname (or scheme) referencing the application, the path referencing the function and the user interface component, and one or more queries including the parameters.
- the digital assistant application can generate and pass an action data structure (e.g., an API function call) including the address to the application.
- the application to which the action data structure is passed can be opened as a background process, and remain in the background process as the action is processed and completed.
- the passing of the action data structure to the other application can be performed as the original application continues to remain as the foreground process.
- the action data structure can be passed by an operating system on the client device from the digital assistant application to the application via the API function call.
- the action data structure can be directed to reach the referenced application.
- the application can parse the action data structure to identify the address.
- the application can further parse the address to identify the pathname to identify the function to be executed and parse the string query to identify the one or more parameters to define the execution of the function. Based on the identification, the application can execute the function to generate an output. Using the output generated by the application, the digital assistant can render the user interface component of the application. The user interface component of the application can be overlaid on the graphical user interface of the application of the foreground process.
- the digital assistant application can invoke the functions of other applications and present rendering of the user interface components from these applications, thereby augmenting the functions of the digital assistant application.
- the invocation may be performed without the user manually searching, opening, and entering to achieve the desired function, or having to context switch between different applications.
- the elimination of context switching on the part of the user operating the client device can thus improve HCI.
- the applications can remain in the foreground and non-foreground processes, even with the recognition of a request from the input audio signal. As the switching between foreground and background processes is eliminated, the digital assistant application can reduce consumption of computing resources and expenditure of time on the part of the user.
- the system 100 can include at least one data processing system 102 , at least one client device 104 , and at least one agent services 106 .
- the data processing system 102 , the client device 104 , and the agent service 106 can be communicatively coupled with one another via at least one network 114 .
- the components of the system 100 can communicate over a network 114 .
- the network 114 can include, for example, a point-to-point network, a broadcast network, a wide area network, a local area network, a telecommunications network, a data communication network, a computer network, an ATM (Asynchronous Transfer Mode) network, a SONET (Synchronous Optical Network) network, a SDH (Synchronous Digital Hierarchy) network, an NFC (Near-Field Communication) network, a local area network (LAN), a wireless network or a wireline network, and combinations thereof.
- the network 114 can include a wireless link, such as an infrared channel or satellite band.
- the topology of the network 114 may include a bus, star, or ring network topology.
- the network 114 can include mobile telephone networks using any protocol or protocols used to communicate among mobile devices, including advanced mobile phone protocol (AMPS), time division multiple access (TDMA), code-division multiple access (CDMA), global system for mobile communication (GSM), general packet radio services (GPRS), or universal mobile telecommunications system (UMTS).
- AMPS advanced mobile phone protocol
- TDMA time division multiple access
- CDMA code-division multiple access
- GSM global system for mobile communication
- GPRS general packet radio services
- UMTS universal mobile telecommunications system
- Different types of data may be transmitted via different protocols, or the same types of data may be transmitted via different protocols.
- the data processing system 102 and the agent service 106 each can include multiple, logically grouped servers and facilitate distributed computing techniques.
- the logical group of servers may be referred to as a data center, server farm, or a machine farm.
- the servers can be geographically dispersed.
- a data center or machine farm may be administered as a single entity, or the machine farm can include a plurality of machine farms.
- the servers within each machine farm can be heterogeneous—one or more of the servers or machines can operate according to one or more type of operating system platform.
- the data processing system 102 and the agent service 106 each can include servers in a data center that are stored in one or more high-density rack systems, along with associated storage systems, located for example in an enterprise data center.
- the data processing system 102 or the agent service 106 with consolidated servers can improve system manageability, data security, the physical security of the system, and system performance by locating servers and high performance storage systems on localized high performance networks.
- Centralization of all or some of the data processing system 102 or agent service 106 components, including servers and storage systems, and coupling them with advanced system management tools allows more efficient use of server resources, which saves power and processing requirements and reduces bandwidth usage.
- Each of the components of the data processing system 102 can include at least one processing unit, server, virtual server, circuit, engine, agent, appliance, or other logic device such as programmable logic arrays configured to communicate with the data repositories 128 and with other computing devices.
- the agent service 106 can also include at least one processing unit, server, virtual server, circuit, engine, agent, appliance, or other logic device such as programmable logic arrays configured to communicate with a data repository and with other computing devices.
- the data processing system 102 can include an instance of at least one digital assistant application 108 .
- the digital assistant application 108 can include at least one natural language processor (NLP) component 116 (sometimes referred herein as an natural language processor) to parse audio-based inputs.
- the digital assistant application 108 can include at least one audio signal generator component 118 (sometimes referred herein as an audio signal generator) to generate audio-based signals.
- the digital assistant application 108 can include at least one action handler component 120 (sometimes referred herein as an action handler) to generate action data structures based on the audio-based inputs.
- the digital assistant application 108 can include at least one response selector component 122 to select responses to audio-based input signals.
- the digital assistant application 108 can include at least one agent registry component 124 (sometimes referred herein as an agent registry) to maintain action-inventories for generating the action data structures.
- the digital assistant application 108 can include at least one agent interface component 126 (sometimes referred herein as agent interface) to communicate with one or more agent applications 110 .
- the digital assistant application 108 can include at least one data repository 128 .
- the data repository 128 of the data processing system 102 can include one or more local or distributed databases and can include a database management system.
- the data repository 128 can include one or more regular expressions 130 , one or more data parameters 132 , one or more policies 134 , one or more response data 136 , one or more templates 138 , and one or more action inventories 140 .
- the NLP component 116 , the audio signal generator component 118 , the action handler component 120 , the response selector component 122 , the agent registry component 124 , the agent interface component 126 , and the data repository 128 can be separate from one another.
- the data repository 128 can include computer data storage or memory and can store the one or more regular expressions 130 , one or more data parameters 132 , one or more policies 134 , response data 136 , templates 138 , and the one or more action-inventories 140 among other data.
- the data parameters 132 , policies 134 , and templates 138 can include information such as rules about a voice-based session between the client devices 104 and the data processing system 102 .
- the response data 136 can include content items for audio output or associated metadata, as well as input audio messages that can be part of one or more communication sessions with the client devices 104 .
- the action-inventories 140 can include information to invoke or interface with the agent application 110 .
- the data processing system 102 can include an instance of at least one agent application 110 (also referred herein as “agent” or “application”) to execute various functions.
- the agent service 106 can include an instance of at least one agent application 110 to execute various functions.
- the agent application 110 can lack natural language processing capabilities (such as those provided by the NLP component 116 ).
- the agent application 110 can also have a preconfigured or a predetermined set of input capabilities.
- the input capabilities of the agent application 110 can include, for example, inputs from a mouse, a keyboard, a touch screen (e.g., a display 148 ), or a camera of the client device 104 .
- the input capabilities of the agent application 110 can lack audio (e.g., a microphone 146 ) from the client device 104 .
- the agent application 110 can have a graphical user interface (GUI) to receive the inputs from the client device 104 . Based on the input from the predetermined input capabilities, the agent application 110 can carry out or execute one or more functions.
- GUI graphical user interface
- the agent application 110 can be a coffee purchase application, and can have a graphical user interface with one or more element to specify an order of a coffee.
- a user operating the client device 104 can manually enter the coffee, size, and billing information, among others, on the graphical user interface rendered on the display 148 of the client device 104 .
- the agent application 110 on the client device 104 can send the input via the network 114 to the agent service 106 .
- the agent service 106 in turn can carry out the purchase order indicated by the inputs entered through the graphical user interface of the agent application 110 .
- the components of the data processing system 102 can each include at least one processing unit or other logic device such as a programmable logic array engine or module configured to communicate with the data repository 128 .
- the components of the data processing system 102 can be separate components, a single component, or part of multiple data processing systems 102 .
- the system 100 and its components, such as a data processing system 102 can include hardware elements, such as one or more processors, logic devices, or circuits.
- the functionalities of the data processing system 102 can be included or otherwise be accessible from the one or more client devices 104 .
- the functionalities of the data processing system 102 may correspond to the functionalities or interface with the digital assistant application 108 executing on the client devices 104 .
- the client devices 104 can each include and execute a separate instance of the one or more components of the digital assistant application 108 .
- the client devices 104 can otherwise have access to the functionalities of the components of the digital assistant application 108 on a remote data processing system 102 via the network 114 .
- the client device 104 can include the functionalities of the NLP component 116 and access the remainder of the components of the digital assistant application 108 via the network 114 to the data processing system 102 .
- the functionalities of the data processing system 102 may correspond to the functionalities or interface with the agent application 110 executing on the client devices 104 .
- the client devices 104 can otherwise have access to the functionalities of the components of the agent application 110 on the data processing system 102 or the agent service 106 via the network 114 .
- the client device 104 can pre-process input from the mouse, keyboard, touchscreen, or camera for the agent application 110 , and provide the input to the agent service 106 to carry out the function of the agent application 110 .
- the digital assistant application 108 executing on the client device 104 can include the functionalities of the digital assistant application 108 executing on the data processing system 102 .
- the client device 104 can each include at least one logic device such as a computing device having a processor to communicate with each other with the data processing system 102 via the network 114 .
- the client devices 104 can include an instance of any of the components described in relation to the data processing system 102 or the agent service 106 .
- the client device 104 can include an instance of the digital assistant application 108 .
- the client device 104 can include an instance of the agent application 110 .
- the client devices 104 can include a desktop computer, laptop, tablet computer, personal digital assistant, smartphone, mobile device, portable computer, thin client computer, virtual server, speaker-based digital assistant, or other computing device.
- the audio driver can execute an audio file or other instructions to convert an acoustic wave or sound wave acquired from the microphone 146 to generate audio data.
- the audio driver can execute an analog-to-driver converter (ADC) to transform the acoustic wave or sound wave to the audio data.
- ADC analog-to-driver converter
- the client device 104 can include a graphics driver to provide a software interface with the display 148 .
- the graphics driver can execute instructions provided by the data processing system 102 or the agent service 106 to control the display 148 to generate a corresponding rendering thereon.
- the instance of the digital assistant application 108 on the client device 104 can include or be executed by one or more processors, logic array, or memory.
- the instance of the digital assistant application 108 on the client device 104 can detect a keyword and perform an action based on the keyword.
- the digital assistant application 108 on the client device 104 can be an instance of the digital assistant application 108 executed at the data processing system 102 or can perform any of the functions of the digital assistant application 108 .
- the instance of the digital assistant application 108 on the client device 104 can filter out one or more terms or modify the terms prior to transmitting the terms as data to the data processing system 102 (e.g., the instance of the digital assistant application 108 on the data processing system 102 ) for further processing.
- the instance of the digital assistant application 108 on the client device 104 can convert the analog audio signals detected by the speaker 144 into a digital audio signal and transmit one or more data packets carrying the digital audio signal to the data processing system 102 via the network 114 .
- the instance of the digital assistant application 108 on the client device 104 can transmit data packets carrying some or the entire input audio signal responsive to detecting an instruction to perform such transmission.
- the instruction can include, for example, a trigger keyword or other keyword or approval to transmit data packets comprising the input audio signal to the data processing system 102 .
- the instance of the digital assistant application 108 on the client device 104 can perform pre-filtering or pre-processing on the input audio signal to remove certain frequencies of audio.
- the pre-filtering can include filters such as a low-pass filter, high-pass filter, or a bandpass filter.
- the filters can be applied in the frequency domain.
- the filters can be applied using digital signal processing techniques.
- the filter can keep frequencies that correspond to a human voice or human speech, while eliminating frequencies that fall outside the typical frequencies of human speech.
- a bandpass filter can remove frequencies below a first threshold (e.g., 70 Hz, 75 Hz, 80 Hz, 85 Hz, 90 Hz, 95 Hz, 100 Hz, or 105 Hz) and above a second threshold (e.g., 200 Hz, 205 Hz, 210 Hz, 225 Hz, 235 Hz, 245 Hz, or 255 Hz).
- Applying a bandpass filter can reduce computing resource utilization in downstream processing.
- the instance of the digital assistant application 108 on the client device 104 can apply the bandpass filter prior to transmitting the input audio signal to the data processing system 102 , thereby reducing network bandwidth utilization.
- the instance of the digital assistant application 108 on the client device 104 can apply additional pre-processing or pre-filtering techniques such as noise reduction techniques to reduce ambient noise levels that can interfere with the natural language processor. Noise reduction techniques can improve accuracy and speed of the natural language processor, thereby improving the performance of the data processing system 102 and manage rendering of a graphical user interface provided via the display.
- the client device 104 can be operated by an end user that enters voice queries as audio input into the client device 104 (via the microphone 146 or speaker 144 ) and receives audio (or other) output from the data processing system 102 or agent services 106 to present, display, or render to the end user of the client device 104 .
- the digital component can include a computer-generated voice that can be provided from the data processing system to the client device 104 .
- the client device 104 can render the computer-generated voice to the end user via the speaker 144 .
- the computer-generated voice can include recordings from a real person or computer-generated language.
- the client device 104 can provide visual output via the display 148 communicatively coupled to the client device 104 .
- the client device 104 can have or execute as at least one foreground process 150 (also referred herein as a foreground service).
- the foreground process 150 can correspond to at least one of the applications (e.g., the digital assistant application 108 or an agent application 110 ) that is rendered on the display 148 and executing on the client device 104 .
- the foreground process 150 can correspond to an application with a graphical user interface occupying a majority of the display 148 .
- the application running on the client device 104 as the foreground process 150 can have a graphical user interface rendered on the display 148 of the client device 104 .
- the graphical user interface of the application in the foreground process 150 can occupy at least a portion of the display 148 of the client device 104 (e.g., at least a third the display 148 ).
- the application running on the client device 104 as the foreground process 150 can wait for user interactions with the graphical user interface (e.g., via the I/O devices communicatively coupled with the client device 104 ) to carry out one or more routines.
- the application in the foreground process 150 can process the user interactions to carry out the one or more routines of the application.
- the application in the foreground process 150 can generate an output and can render the resultant output on the graphical user interface of the application in the foreground process 150 .
- the client device 104 can have or execute at least one non-foreground process 152 (also referred herein as a background process, a background service, non-foreground process, or a non-foreground service).
- the non-foreground process 152 can correspond to at least at least one of the applications (e.g., the digital assistant application 108 or another agent application 110 ) that is not rendered on the display 148 and executing on the client device 104 .
- the application running on the client device 104 as the non-foreground process 152 can lack any rendering of the graphical user interface of the application on the display 148 of the client device 104 .
- the non-foreground process 152 can correspond to an application that is minimized and without any graphical user interface component rendered in the display 148 .
- the application running on the client device 104 as the non-foreground process 152 can execute one or more routines without receipt of any user interactions with the application.
- an application running as the non-foreground process 152 can process audio detected on the microphone 146 of the client device 104 , without having any graphical user interface rendered on the display 148 .
- the application in the foreground process 150 can generate an output without rendering the resultant output as a part of the graphical user rendered on the display 148 .
- the invocation can be by the operating system executing on the client device 104 , by another application (e.g., the digital assistant application 108 or the agent application 110 ), or by a user interaction event.
- the client device 104 can set an application running in the foreground process 150 to the non-foreground process 152 .
- the client device 104 can set an application running in the non-foreground process 152 to the foreground process 150 .
- the client device 104 can also instantiate the application into the foreground process 150 .
- a web browser can be executing on the client device 104 as the foreground process 150 with a graphical user interface rendered on the display 148 .
- a clock application can be running on the client device 104 as the non-foreground process 152 to track time, without the rendering of any graphical user interface rendered on the display 148 .
- the client device 104 can set the web browser as the non-foreground process 152 thereby removing the rendering of the graphical user interface of the web browser.
- the client device 104 can set the clock application as the foreground process 150 .
- the data processing system 102 and the agent service 106 each can include at least one server having at least one processor.
- the data processing system 102 and the agent service 106 each can include a plurality of servers located in at least one data center or server farm.
- the data processing system 102 can include at least one computation resource or server.
- the data processing system 102 can include, interface, or otherwise communicate with at least one communications interface 112 .
- the data processing system 102 can include, interface, or otherwise communicate with at least one instance of the digital assistant application 108 on the data processing system 102 .
- the instance of the digital assistant application 108 on the data processing system 102 can include, interface, or otherwise communicate with: the at least one NLP component 116 , the at least one audio signal generator component 118 , at least one action handler component 120 , the at least one response selector component 122 , the at least one agent registry component 124 , and the at least one agent interface component 126 , among others.
- the data processing system 102 can include, interface, or otherwise communicate with at least one data repository 128 .
- the at least one data repository 128 can include or store, in one or more data structures or databases, regular expressions 130 , data parameters 132 , policies 134 , response data 136 , templates 138 , and action-inventories 140 .
- the data repository 128 can include one or more local or distributed databases, and can include a database management.
- the instance of digital assistant application 108 of the data processing system 102 can execute or run an instance of the agent registry component 124 to maintain the set of action-inventories 140 on the data repository 128 .
- the agent registry component 124 can maintain and store the set of action-inventories 140 for one or more agent applications 110 on the data repository 128 .
- a subset of the action-inventories 140 can be defined by an administrator of the data processing system 102 or the digital assistant application 108 for an application type corresponding to the agent application 110 .
- the administrator of the digital assistant application 108 can configure or define a built-in set of action-inventories 140 for purchase order applications.
- Another subset of action-inventories 140 can be provided by one of the agent services 106 handling resources for the corresponding agent application 110 .
- At least one of the action-inventories 140 can be a preset action-inventory 140 to be used to carry out an action, in response to determining that a request does not match any other action-inventory 140 of the agent application 110 .
- the agent service 106 can provide a set of customized action-inventories 140 to the data processing system 102 for storage onto the data repository 128 .
- the agent registry component 124 can receive the subset of action-inventories 140 provided by the agent service 106 . Upon receipt, the agent registry component 124 can store the received action-inventories 140 onto the data repository 128 .
- the existence of the action-inventory 140 for the action of the agent application 110 on the data repository 128 can indicate that the agent application 110 is capable of performing the action corresponding to the action-inventory 140 .
- the agent registry component 124 can maintain and store a set of user interface components 142 for one or more agent applications 110 on the data repository 128 .
- Each user interface component 142 can be associated with at least one of the action-inventories 140 .
- a subset of the user interface components 142 can be defined by an administrator of the data processing system 102 or the digital assistant application 108 .
- the administrator of the digital assistant application 108 can configure or define a built-in set of user interface components 142 .
- Another subset of user interface components 142 can be provided by one of the agent services 106 handling resources for the corresponding agent application 110 .
- the agent service 106 can store and maintain the user interface component 142 corresponding to the subset.
- the agent service 106 can provide a set of customized user interface components 142 to the data processing system 102 for storage onto the data repository 128 .
- the agent registry component 124 can receive the subset of user interface components 142 provided by the agent service 106 . Upon receipt, the agent registry component 124 can store the received user interface components 142 onto the data repository 128 .
- the maintenance of the user interface components 142 by the agent service 106 can be separate from the maintenance of the user interface component 142 on the data repository 128 .
- Each user interface component 142 can have one or more interface elements. Each interface element can correspond to a subcomponent of the user interface component 142 . At least one interface element can correspond to a request to be identified from input audio signals detected by the microphone 146 of the client device 104 . At least one interface element can correspond to a response indicating an output generated by the agent application 110 associated with the user interface component 142 . Furthermore, each user interface component can have one or more properties. The properties can include an element type (e.g., command button, scroll bar, textbox, and image), a size, a location within the user interface component 142 , transparency, and shape, among others.
- element type e.g., command button, scroll bar, textbox, and image
- a user interface component 142 can have a button located generally along the bottom, a textbox above the button, and a slot for an image generally in the middle, among others.
- the user interface component 142 itself can have one or more properties.
- the properties of the user interface component 142 can include a size, a location within the display 148 , transparency, and shape, among others.
- Each action-inventory 140 can have at least one address template for at least one action by one of the agent applications 110 .
- the address template may be for a single action of the agent application 110 or multiple actions to be carried by the agent application 110 .
- the address template may be also for accessing or retrieving the user interface component 142 to be generated in carrying out the action.
- the address template for the action can include a first portion and a second portion.
- the address template can be, for example, a Uniform Resource Identifier (URI) template in accordance to which an URI is to be generated.
- the first portion of the address template can correspond to one of the agent applications 110 or the agent service 106 providing resources for the agent application 110 .
- the first portion of the address template can include a scheme in a custom deep-link URI referencing the agent application 110 .
- the first portion of the address template can also include a hostname referencing the agent service 106 .
- the action-inventory 140 can include a mapping (sometimes referred herein as a parameter mapping) for the second portion of the address template.
- the mapping can specify or define a correspondence between at least one input variable of the second portion and one or more words to be identified from parsing input audio signals.
- the one or more words from parsing input audio signals can correspond to the input variables defining the action to be executed by the agent application 110 .
- the mapping can define the insertion of the words into the input variables of the address template.
- the mapping for a coffee purchase order action can specify insertion of a coffee name parsed from the audio signal to a coffee input variable of the address template.
- the action-inventory 140 can specify or include a set of permitted values (sometimes referred as an entity-inventory) for each input variable.
- each action-inventory 140 maintained by the agent registry component 124 can include information related to the action for the action-inventory 140 .
- the action-inventory 140 can include an interface mode indicator. The indicator can specify whether the user interface component 142 of the agent application 110 is to be displayed in carrying out the action.
- the user interface component 142 can be a visual element to be rendered in the display 148 of the client device 104 .
- the user interface component 142 can have a size less than a size of the display 148 .
- the user interface component 142 can include a box-sized visual element to occupy a 12.5% to 25% of the display 148 .
- the user interface component 142 can be associated with at least one of the action-inventories 140 maintained on the data repository 128 .
- the user interface component 142 can be associated with the action-inventory 140 .
- the action-inventory 140 can include an address referencing the user interface component 142 .
- the address referencing the user interface component 142 can differ or be separate from the address template for executing the action.
- the address referencing the user interface component 142 can have the same scheme or hostname as the address template for the same action-inventory 140 .
- the address can include the hostname of the agent service 106 managing resources for the agent application 110 .
- the address referencing the user interface component 142 can have a different hostname as the address template of the same action-inventory 140 .
- the hostname can reference the data repository 128 of the data processing system 102 .
- the action-inventory 140 can also include an user interface identifier corresponding to the user interface component 142 .
- the identifier may be used to index and reference the user interface component 142 maintained on the data repository 128 .
- the address template of the action-inventory 140 can be used to reference the user interface component 142 along with the action to be executed by the agent application 110 .
- Action-inventory 140 can be for an order ride to be executed by a personal fitness application.
- the agent registry component 124 can construct, create, or generate the set of action-inventories 140 for each agent application 110 .
- the agent service 104 for the agent application 110 can provide a configuration file associated with the agent application 110 to the agent registry component 124 via the network 114 .
- the configuration file can be, for example, an application binary interface (ABI) for the agent application 110 submitted by an application developer associated with the agent application 110 to the agent registry component 124 .
- the agent service 104 for the agent application 110 can provide metadata associated with the agent application 110 .
- the metadata can include the agent identifier and an application type to the agent registry component 124 , among others.
- the application type can indicate a usage of the agent application 110 .
- the agent service 104 for the agent application 110 can provide the interface mode indicator for one or more actions capable of being executed by the agent application 110 .
- the agent registry component 124 can receive the information from the agent service 106 for the generation of the set of action-inventories 140 of the agent application 110 .
- the agent registry component 124 can read, ingest, and parse the configuration file for the agent application 110 to identify one or more actions. For each identified function, the agent registry component 124 can generate an action-inventory 140 for the action. The action can be one of the actions of the set of action-inventories 140 defined by the administrator of the data processing system 102 or the digital assistant application 108 for the application type of the agent application 110 .
- the agent registry component 1240 generate the address template for the action to be executed by the agent application 110 .
- the first portion of the address template (e.g., the hostname or scheme) can correspond to the agent application 110 .
- the second portion of the address template can be predefined for the action (e.g., using a preset pathname or a query string).
- the agent registry component 124 can also generate a mapping for the action-inventory 140 for the identified function.
- the agent registry component 124 can include or add a request identifier corresponding to the identified action into the action-inventory 140 .
- the mapping can include the set of permitted values for the input variables of the address template.
- the agent registry component 124 can include or add an agent identifier corresponding to the agent application 110 into the action-inventory 140 .
- the agent registry component 124 can store the action-inventory 140 onto the data repository 128 .
- the data processing system 102 can include at least one communications interface 112 .
- the communications interface 112 can be configured, constructed, or operational to receive and transmit information using, for example, data packets.
- the communications interface 112 can receive and transmit information using one or more protocols, such as a network protocol.
- the communications interface 112 can include a hardware interface, software interface, wired interface, or wireless interface.
- the communications interface 112 can be a data interface or a network interface that enables the components of the system 100 to communicate with one another.
- the communications interface 112 of the data processing system 102 can provide or transmit one or more data packets that include the action data structure, audio signals, or other data via the network 114 to the client devices 104 or the agent service 106 .
- the data processing system 102 can provide the output signal from the data repository 128 or from the audio signal generator component 118 to the client devices 104 .
- the data processing system 102 can include an application, script, or program installed at the client device 104 , such as the instance of the digital assistant application 108 on the client device 104 to communicate input audio signals to the communications interface 112 of the data processing system 102 and to drive components of the client computing device to render output audio signals or visual output.
- the data processing system 102 can receive data packets, a digital file, or other signals that include or identify an input audio signal (or input audio signals).
- the client device 104 can detect the audio signal via the speaker 144 and convert the analog audio signal to a digital file via an analog-to-digital converter.
- the audio driver can include an analog-to-digital converter component.
- the pre-processor component can convert the audio signals to a digital file that can be transmitted via data packets over network 114 .
- the instance of the digital assistant application 108 of the data processing system 102 or the client device 104 can execute or run an NLP component 116 to receive or obtain the data packets including the input audio signal detected by the microphone 146 of the client device 104 .
- the data packets can provide a digital file.
- the NLP component 116 can receive or obtain the digital file or data packets comprising the audio signal and parse the audio signal.
- the NLP component 116 can provide for interactions between a human and a computer.
- the NLP component 116 can be configured with techniques for understanding natural language and enabling the data processing system 102 to derive meaning from human or natural language input.
- the NLP component 116 can include or be configured with techniques based on machine learning, such as statistical machine learning.
- the NLP component 116 can utilize decision trees, statistical models, or probabilistic models to parse the input audio signal.
- the NLP component 116 can perform, for example, functions such as named entity recognition (e.g., given a stream of text, determine which items in the text map to names, such as people or places, and what the type of each such name is, such as person, location (e.g., “home”), or organization), natural language generation (e.g., convert information from computer databases or semantic intents into understandable human language), natural language understanding (e.g., convert text into more formal representations such as first-order logic structures that a computer module can manipulate), machine translation (e.g., automatically translate text from one human language to another), morphological segmentation (e.g., separating words into individual morphemes and identify the class of the morphemes, which can be challenging based on the complexity of the morphology or structure of the words of the language being considered), question answering (e.g., determining an answer to a human-language question, which can be specific or open-ended), or semantic processing (e.g., processing that can occur after identifying a word and
- the NLP component 116 (and the digital assistant application 108 as a whole) can execute as one of the applications in the non-foreground process 152 .
- the client device 104 can continue to the digital assistant application 108 as one of the applications in the non-foreground process 152 .
- the client device 104 can have one or more other applications (e.g., an agent application 110 ) running as the non-foreground processes 152 .
- the client device 104 can have at least one application (e.g., another agent application 110 ) running in the foreground process 150 .
- the instance of the NLP component 116 of the digital assistant application 108 on the client device 104 can initiate processing of the input audio signal without switching of the foreground process 150 or the non-foreground process 152 .
- the applications running as the foreground process 150 and the non-foreground process 152 can be maintained.
- the NLP component 116 on the client device 104 can receive and apply the natural language processing functions on the input audio signal, without minimizing a graphical user interface of an application running as the foreground process 150 .
- the NLP component 116 (and the digital assistant application 108 as a whole) can execute as one of the applications in the foreground process 150 , when the input audio signal is received. Subsequent to the receipt of the input audio signal, the digital assistant application 108 can continue to execute as one of the applications in the foreground process 150 of the client device 104 .
- the NLP component 116 can parse and convert the input audio signal into recognized string by comparing the input signal against a stored, representative set of audio waveforms (e.g., in the data repository 128 ) and choosing the closest matches.
- the NLP component 116 can also partition or divide the input audio signal into one or more audio segments of a time duration (e.g., 15 seconds to 2 minutes) to process each segment.
- the set of audio waveforms can be stored in data repository 128 or other database accessible to the data processing system 102 .
- the representative waveforms are generated across a large set of users, and then may be augmented with speech samples from the user.
- the NLP component 116 matches the text to words that are associated, for example via training across users or through manual specification, with actions that the data processing system 102 can serve.
- the NLP component 116 can determine that the input audio signal acquired from the microphone 146 does not contain any recognizable strings.
- the NLP component 116 can determine that the input audio signal contains silence (e.g., with a maximum amplitude of less than 0 dB) in determining that the input audio signal does not contain any recognizable strings.
- the NLP component 116 can determine a signal-to-noise (SNR) of the input audio signal.
- the NLP component 116 can compare the SNR of the input audio signal to a threshold SNR (e.g., ⁇ 20 dB). Responsive to the determination the SNR of the input audio signal is greater than the threshold SNR, the NLP component 116 can determine that the input audio signal does not contain any recognizable strings.
- SNR signal-to-noise
- the data processing system 102 can receive image or video input signals, in addition to, or instead of, input audio signals.
- the NLP component 116 can convert image or video input to text or digital files.
- the NLP component 116 can process, analyze, or interpret image or video input to perform actions, generate requests, or select or identify data structures.
- the data processing system 102 can process the image or video input signals using, for example, image interpretation techniques, computer vision, a machine-learning engine, or other techniques to recognize or interpret the image or video to convert the image or video to a digital file.
- the one or more image interpretation techniques, computer vision techniques, or machine learning techniques can be collectively referred to as imaging techniques.
- the data processing system 102 e.g., the NLP component 116
- the NLP component 116 can determine or identify at least one request.
- the input audio signal can include, for example, a query, question, command, instructions, or other statement in a natural language.
- the request can correspond to at least one trigger keyword identified from the recognized string converted from the input audio signal.
- the request can indicate an action to be taken.
- the NLP component 116 can parse the input audio signal to identify at least one request to leave home for the evening to attend dinner and a movie.
- the trigger keyword can include at least one word, phrase, root or partial word, or derivative indicating an action to be taken.
- the trigger keyword “go” or “to go to” from the input audio signal can indicate a request for transport.
- the input audio signal (or the identified request) does not directly express an intent for transport, however the trigger keyword indicates that transport is an ancillary action to at least one other action that is indicated by the request.
- the NLP component 116 can apply a semantic processing technique to the input audio signal to identify the trigger keyword corresponding to the request.
- the NLP component 116 can apply a semantic processing technique to the input audio signal to identify a trigger phrase that includes one or more trigger keywords, such as a first trigger keyword and a second trigger keyword.
- the input audio signal can include the sentence “Find the nearest café.”
- the NLP component 116 can determine that the input audio signal includes a trigger keyword “find.”
- the NLP component 116 can determine that the request is to search for a location near the client device 104 .
- the NLP component 116 can identify at least one parameter defining the request.
- the parameter can define the request, functioning as a supplement or a constraint on the action corresponding to the request to be taken.
- the parameter can further specify the action to be-taken.
- the parameter can include a subset of the recognized strings (excluding the trigger keywords) converted from the input audio signal.
- the NLP component 116 can apply a semantic processing technique to the input audio signal to identify one or more descriptor words related to the identified trigger keyword. From the example “Find the nearest café,” the NLP component 116 may have identified the term “Find” as the trigger keyword. In conjunction, the NLP component 116 can identify “nearest” as a first parameter and “café” as a second parameter defining the request “find.”
- the NLP component 116 can identify an application identifier from the strings recognized from the input audio signal.
- the application identifier can correspond to one of the agent applications 110 .
- the NLP component 116 can perform named entity recognition algorithm to the strings recognized from the input audio signal.
- the NLP component 116 can maintain a list of agent identifiers for agent applications 110 .
- the list can include agent identifiers of agent applications 110 installed on the client device 104 that received the input audio signal including the request.
- the NLP component 116 can determine that the input audio signal includes the application identifier corresponding to an agent application 110 .
- the NLP component 116 can determine that the input audio signal “Order car with ride sharing service XYZ” includes with an explicit agent application 110 “ride sharing service XYZ.” Conversely, by applying the named entity recognition algorithm, the NLP component 116 can determine that the input audio lacks any application identifier corresponding to any of the agent applications 110 .
- the NLP component 116 can also use the regular expressions 130 maintained on the data repository 128 to determine whether the request corresponds to the function of the digital assistant application 108 .
- the regular expression 130 can define a pattern to match to determine whether the keywords identified from the input audio signal references the at least one function of the digital assistant application 108 .
- the regular expression 130 can also specify which keywords to use to carry out the command indicated in the input audio signal.
- the regular expression 130 may be of the form ⁇ [request], [auxiliary arguments] ⁇ .
- the regular expression 130 can specify that the one or more keywords include a request for the digital assistant application 108 and auxiliary arguments.
- the regular expression 130 can specify a sequence for the request and the referential keywords in the one or more keywords identified from the input audio signal.
- the NLP component 116 can determine that the request identified from the input audio signal matches one of the list of requests for functions of the digital assistant application 108 . In response to the determination, the NLP component 116 can determine that the request corresponds to one of the functions of the digital assistant application 108 . For example, the NLP component 116 can parse the words “What is the weather?” from the input audio signal, and can identify the input audio signal includes a request for weather. The list of requests for the functions of the digital assistant application 108 can specify that the request for weather is one of the functions of the digital assistant application 108 . In this example, the NLP component 116 can determine the match between the two requests, and can determine that the request references one of the functions of the digital assistant application 108 .
- the digital assistant application 108 can execute and fulfill the request identified from the parsing of the input audio signal.
- the digital assistant application 108 can execute the request in accordance with the parameter defining the request.
- the request may be to search for a definition of a word (e.g., “abnegation”).
- the digital assistant application 108 can perform an internet search to retrieve the definition of the word.
- the digital assistant application 108 can invoke the response selector component 122 .
- the response selector component 122 can select or identify responses phrases using the policies 134 or the response data 136 maintained on the data repository 128 .
- the policies 134 can be particular to a request, and can specify the response data 136 for the request.
- the response selector component 122 can search the policies 134 for generating the output using the request type of the response in fulfilling the request. Once the policy 134 is identified, the response selector component 122 can select the response phrase.
- the response phrase can include a set of strings, such as words or phrases.
- the digital assistant application 108 can display the response phrase on the display 148 of the client device 104 . For example, the digital assistant application 108 can display a content item including the response phrase on a graphical user interface of the digital assistant application 108 on the display 148 of the client device 104 .
- the NLP component 116 can determine that the request parsed from the input audio signal does not match any of the list of requests for function of the digital assistant application 108 . In response to the determination, the NLP component 116 can determine whether the request corresponds to one of the functions of the agent application 110 . To determine, the NLP component 116 can access a list of requests for functions of the agent applications 110 . The list of requests for functions can be for agent applications 110 installed on the client device 104 . The list of requests for the functions of the agent applications 110 can be maintained on the data repository 128 . The list of requests can include sets of strings for trigger keywords or requests corresponding to the requests predetermined to be associated with functions of the agent application 110 . The NLP component 116 can compare the request with the list of requests for functions of the agent application 110 .
- the NLP component 116 can determine that the request identified from the input audio signal matches one of the list of requests for functions of the agent application 110 . In response to the determination, the NLP component 116 can determine that the request corresponds to one of the functions of the agent application 110 .
- the NLP component 116 can parse the words “Buy me laundry detergent” from the input audio signal, and can identify the input audio signal includes a purchase for detergent.
- the list of requests for the functions of the agent application 110 can specify that the request for weather is one of the functions of the agent application 110 .
- the NLP component 116 can determine the match between the two requests, and can determine that the request references one of the functions of the agent application 110 , namely the one for such purchases.
- the NLP component 116 can determine that the request does not correspond to a function of the digital assistant application 108 , in response to determining that the input audio signal includes the application identifier of the agent application 110 .
- the NLP component 116 can determine that the request does not correspond to any function of any of the agent applications 110 from the comparison. With the determination that the request does not correspond to the digital assistant application 108 or the agent application 110 , the response selector component 122 can generate or identify a response phrase indicating that the request does not correspond to any functionality available on the client device 014 .
- the response phrase can include “Sorry, I don't understand your request.” The response phrase can be selected from the response data 136 based on the determination.
- the audio signal generator component 118 can generate a corresponding output audio signal using the one or more words of the response phrase. The audio signal generator component 118 can play the output audio signal on the speaker 144 of the client device 104 .
- the instance of the digital assistant application 108 running on the data processing system 102 or the client device 104 can execute the action handler component 120 to generate action data structures based on the determination.
- the action handler component 120 can execute scripts or programs based on input received from the NLP component 116 .
- the agent service 106 can provide the scripts or programs.
- the agent service 106 can make the scripts or programs available to the data processing system 102 through an API.
- the action handler component 120 can determine parameters or responses to input fields and can package the data into an action data structure.
- the action data structure can be provided to the agent application 110 through the API.
- the action handler component 120 can transmit the action data structure to the agent service 106 for fulfillment of the request identified from the input audio signal parsed by the NLP component 116 .
- the action handler component 120 can identify or select an action-inventory 140 from the data repository 128 for the action corresponding to the request. At least one action-inventory 140 can be selected by the action handler component 120 from the set of action-inventories 140 maintained on the data repository 128 .
- the action handler component 120 can use the request identified from the input audio signal to search the data repository 128 for the action-inventory 140 .
- the action handler component 120 can also use the one or more parameters identified from the input audio signal to search the data repository 128 for the action-inventory 140 .
- the action handler component 120 can traverse through the set of action-inventories 140 maintained on the data repository 128 . For each action-inventory 140 , the action handler component 120 can identify the request identifier of the action-inventory 140 .
- the action handler component 120 can compare the request identified from the input audio signal with the request identifier of the action-inventory 140 .
- the comparison of the request identifier from the input audio signal and the request identifier of the action-inventory 140 can be in accordance with a semantic knowledge graph.
- the action handler component 120 can invoke the NLP component 116 to determine whether the request is semantically related with the request identifier using the semantic knowledge graph.
- the semantic knowledge graph can include a set of nodes and edges. Each node can correspond to one or more words or a phrase. Each edge can define a semantic distance between a pair of words indicated by the corresponding words on the nodes.
- the NLP component 116 can identify the node corresponding to the request, the node corresponding to the request identifier, and the semantic distance indicated in the pair of nodes.
- the NLP component 116 can compare the semantic distance to a threshold distance. When the semantic distance is determined to not satisfy the threshold distance (e.g., greater than), the NLP component 116 can determine that the request does not match the request identifier of the action-inventory 140 . On the other hand, when the semantic distance is determined to satisfy the threshold distance (e.g., less than or equal to), the NLP component 116 can determine that the request does match the request identifier of the action-inventory 140 .
- the action handler component 120 can determine that the request does not match the request identifier of the action-inventory 140 . In response to the determination, the action handler component 120 can identify the next action-inventory 140 in the set maintained on the data repository 128 and can repeat the comparison.
- the action handler component 120 can determine that the request matches the request identifier of the action-inventory 140 . In response to the determination, the action handler component 120 can select the action-inventory 140 for the action corresponding to the request. As multiple requests may correspond to the same action-inventory 140 as discussed above, the action handler component 120 can select the action-inventory 140 that was or is to be selected for another request. The action handler component 120 can also compare the one or more parameters identified from the input audio signal with the input variables of the action-inventory 140 , prior to selection of the action-inventory 140 . The action handler component 120 can identify the input variables from the parameter mapping or the address template of the action-inventory 140 .
- the action handler component 120 can exclude the auxiliary parameters as indicated in the action-inventory 140 in comparing the identified parameters with the input variables of the action-inventory 140 .
- the action handler component 120 can determine that the identified parameters match the input variables of the action-inventory 140 in accordance to the set of permitted values for each input variable.
- the action handler component 120 can select the action-inventory 140 for the action corresponding to the request defined by the one or more parameters.
- the action handler component 120 can also halt the traversal of the set of action-inventories 140 in the data repository 128 . Otherwise, the action handler component 120 can determine that the identified parameters do not match the input variables of the action-inventory 140 .
- the action handler component 120 can identify another action-inventory 140 for the action corresponding to the same request, and can repeat the comparison. Upon comparing with all the action-inventories 140 , the action handler component 120 can determine that the identified parameters do not match the input variables in any of the action-inventories 140 of the agent application 110 . In response to the determination, the action handler component 120 can select the preset action-inventory 140 for the agent application 110 to respond to the request.
- the action handler component 120 can also use the agent identifier identified from the input audio signal to search the data repository 128 for the action-inventory 140 . Prior to searching using the request, the action handler component 120 can identify a subset of the action-inventories 140 with the application identifier matching the agent identifier. The agent identifier can correspond to one of the agent applications 110 . Each action-inventory 140 of the identify subset can include the application identifier that is determined to match the agent identifier identified from the input audio signal. To identify the subset, the action handler component 120 can compare the agent identifier with the application identifier of each action-inventory 140 .
- the action handler component 120 can include the action-inventory 140 into the subset. Otherwise, when the agent identifier is determined to not match to the application identifier of the action-inventory 140 , the action handler component 120 can exclude the action-inventory 140 from the subset. The action handler component 120 can traverse through the identified subset of action-inventories 140 for the application identifier using the request to select the action-inventory 140 for the action corresponding to the request.
- the action handler component 120 can select the action-inventory 140 from the set maintained on the data repository 128 based on the request, the one or more parameters, and an agent usage history.
- the agent usage history can indicate usage statistics of the agent application 110 on the client device 104 running the digital assistant application 108 from which the input audio signal is received.
- the agent usage history can also indicate functions of the agent application 110 invoked on the client device 104 .
- the agent usage history can be limited to a defined time window prior to the request identified from the input audio signal detected on the client device 104 .
- the action handler component 120 can identify a subset of action-inventories 140 with request identifiers matching the request and the one or more parameters.
- the subset can include the action-inventories 140 with different application identifiers corresponding to multiple agent applications 110 .
- the action handler component 120 can use the agent usage history for the client device 104 to select the action-inventory 140 from the subset.
- the action handler component 120 can identify at least one agent identifier from the agent usage history.
- the action handler component 120 can compare the agent identifier with application identifier of each action-inventory 140 in the identified subset.
- the action handler component 120 can determine that application identifier of the action-inventory 140 matches the agent identifier from the agent usage history from the comparison. In response to the determination, the action handler component 120 can select the action-inventory 140 for the action to be executed by the agent application 110 in accordance with the parameter.
- the action handler component 120 can determine that the application identifier of the action-inventory 140 does not match the agent identifier from the agent usage history from the comparison. In response to the determination, the action handler component 120 can identify another action-inventory 140 and can repeat the comparison using the agent identifier.
- the action handler component 120 can validate the one or more parameters identified from the input audio signal against the address template of the action-inventory 140 .
- the validation by the action handler component 120 may be to check whether there are prerequisite parameters as specified by the action-inventory 140 to carry out the action corresponding to the request. For example, for a ridesharing request, the action handler component 120 can check whether there is a parameter corresponding to the destination as parsed from the input audio signal.
- the action handler component 120 can use the parameter mapping for the address template of the action-inventory 140 in validating the parameters.
- the action handler component 120 can identify the correspondences between parameters and input variables defined in the mapping for the action-inventory 140 .
- the action handler component 120 can identify a subset of correspondences for input variables not specified as auxiliary by the mapping of the action-inventory 140 for the address template.
- the action handler component 120 can determine whether at least one of the parameters corresponds to the input variable as specified by the mapping. The determination of the correspondence can be performed using semantic analysis algorithms as provided by the NLP component 116 (e.g., semantic knowledge graph). For each input variable, the action handler component 120 can invoke the NLP component 116 to determine whether the input variable corresponds to any of the parameters, or vice-versa. In response to determination that all the input variables corresponds to at least one parameter, the action handler component 120 can determine that the parameters are successfully validated. The action handler component 120 can also continue to use the address template of the action-inventory 140 to generate an address to fulfill the action corresponding to the request.
- semantic analysis algorithms as provided by the NLP component 116 (e.g., semantic knowledge graph).
- the action handler component 120 can invoke the NLP component 116 to determine whether the input variable corresponds to any of the parameters, or vice-versa.
- the action handler component 120 can determine that the parameters are successfully validated.
- the action handler component 120 can also
- the action handler component 120 can determine that the parameters are not successfully validated.
- the action handler component 120 can cease or terminate further processing of the request and the parameters (e.g., generation of an address using the address template of the action-inventory 140 ).
- the action handler component 120 can detect or determine an error in validating the one or more parameters identified from the input audio signal.
- the digital assistant application 108 can present an indication (e.g., using a visual content item or audio) that the request is invalid or unsuccessful.
- the action handler component 120 can provide an indication of the error to the agent service 106 or an administrator for the agent application 110 associated with the action-inventory 140 .
- the action handler component 120 can determine whether the client device 104 is authenticated with the agent application 110 associated with the action-inventory 140 .
- the action handler component 120 can identify the agent application 110 associated with the action-inventory 140 .
- the action handler component 120 can check an authentication status of the client device 104 with the agent application 110 . The checking of the authentication status can be performed using an account identifier of the client device 104 or the digital assistant application 108 .
- the action handler component 120 can send a request for the authentication status to the agent service 106 managing resources of the agent application 110 .
- the request can include the account identifier of the client device 104 or of the digital assistant application 108 .
- the agent service 106 can provide a response to the action handler component 120 indicating whether the client device 104 is authenticated with the agent application 110 .
- the action handler component 120 can determine that the client device 104 is authenticated with the agent application 110 .
- the action handler component 120 can also continue processing the request.
- the action handler component 120 can determine that the client device 104 is not authenticated with the agent application 110 .
- the action handler component 120 can cease or terminate further processing of the request (e.g., generation of an address using the address template of the action-inventory 140 ).
- the digital assistant application 108 can also present an indication to authenticate with the agent application 110 to carry out the action corresponding to the request. For example, the digital assistant application 108 can render a prompt for logging into the agent application 110 on the display 148 of the client device 104 .
- the prompt can include a field for an account identifier of the user and the passcode.
- the action handler component 120 can also determine whether the agent application 110 associated with the action-inventory 140 is installed on the client device 104 .
- the action handler component 120 can identify the agent application 110 associated with the action-inventory 140 . Once identified, the action handler component 120 can access the client device 104 to determine whether the agent application 110 is installed on the client device 104 . For example, the action handler component 120 can access a list of installed applications on the client device 104 to determine whether the agent application 110 is installed.
- the action handler component 120 can check the application identifier or the agent identifier to the list. In response to the determination that the agent application 110 is installed, the action handler component 120 can continue to process the request.
- the action handler component 120 can terminate processing of the request (e.g., generation of an address using the address template of the action-inventory 140 ).
- the digital assistant application 108 can also present an indication to install the agent application 110 to carry out the action corresponding to the request.
- the digital assistant application 108 can render a prompt indicating that the corresponding agent application 110 is not installed on the display 148 of the client device 104 .
- the prompt can also include a link to install the agent application 110 .
- the action-inventory 140 selected by the action handler component 120 can correspond to the agent application 110 not running as one of applications in the foreground process 150 or the non-foreground process 152 .
- the action-inventory 140 can correspond to the agent application 110 installed on the client device 104 , but not running on the client device 104 .
- the action-inventory 140 selected by the action handler component 120 can correspond to the agent application 110 running in the non-foreground process 152 .
- another application e.g., another agent application 110 or the digital assistant application 108
- the application in the foreground process 150 can have a graphical user interface rendered on at least a portion of the display 148 of the client device 104 .
- the application running in the non-foreground process 152 can lack a graphical user interface rendered on the display 148 of the client device 104 , and can execute routines (e.g., the action).
- the action-inventory 140 selected by the action handler component 120 can correspond to the agent application 110 running in the foreground process 150 .
- the agent application 110 corresponding to the action-inventory 140 can have a graphical user interface currently rendered on the display 148 of the client device 104 .
- the action handler component 120 can generate an address in accordance with the address template of the action-inventory 140 .
- the action handler component 120 can identify the address template of the action-inventory 140 selected from the set maintained on the data repository 128 .
- the address template of the action-inventory 140 can include: a first portion referencing the agent application 110 ; and a second portion referencing the action to be performed and the one or more input variable used to execute the action.
- the address template can be a URI template.
- the first portion can include a scheme referencing the agent application 110 or a hostname referencing the agent service 106 or the agent application 110 .
- the second portion can include a pathname referencing the action and one or more query strings corresponding to one or more input variable sin the URI.
- the address template of the action-inventory 140 can be “https://ex_personalmetrics.example.com/checksteps ⁇ ?username ⁇ ” for a request to check a number of steps taken.
- the first portion can correspond to “https://ex_personalmetrics.example.com/” and the second portion can correspond to “checksteps ⁇ ?username ⁇ .”
- the address generated in accordance with the address template can include a first substring and a second substring.
- the first substring of the generated address can correspond to the first portion of the address template.
- the generated address can be a portion of the URI, and the first substring can include the scheme referencing the agent application 110 or the hostname referencing the agent service 106 or the agent application 110 .
- the scheme and the hostname can be taken by the action handler component 120 from the first portion of the address template.
- the second substring of the generated address can correspond to the second portion of the second template.
- the generated address can also be a portion of the URI, and the second substring can include the pathname referencing the action to be executed by the agent application 110 and query strings including the parameters identified from the input audio signal.
- the action handler component 120 can use or set the first portion of the address template as a first substring of the address to reference the agent service 106 or the agent application 110 associated with the agent service 106 .
- the action handler component 120 can use the second portion of the address template as a second substring of the address to correspond to the action and the parameters used to execute the action.
- the action handler component 120 can populate the input variables of the second portion of the address template with the one or more parameters identified from the input audio signal.
- the action handler component 120 can populate in accordance with the parameter mapping of the action-inventory 140 defined for the address template.
- the parameter mapping can define a correspondence between the input variables of the second portion and words to be identified from parsing input audio signals.
- the action handler component 120 can use the parameters as the words identified from the input audio signal to insert into the second portion of the address template in populating the input variables.
- the population or of the second portion of the address template can be in accordance with expression expansion in URI templates. For each input variable specified in the address template, the action handler component 120 can identify the corresponding parameters as specified by the mapping of the action-inventory 140 .
- the action handler component 120 can insert, replace, or set the query string with a name of the input variable and the identified parameter.
- the input variable may be “ ⁇ ?username ⁇ ” in the address template and the parameter identified may be “example_user_id”.
- the action handler component 120 can determine whether the input variable is an auxiliary as specified in the mapping for the address template of the action-inventory 140 . In response to the determination that the input variable is auxiliary, the action handler component 120 can also determine whether there is an identified parameter that corresponds to the auxiliary input variable. When there is determined to be no parameters that correspond to the auxiliary input variable, the action handler component 120 can omit or remove the in auxiliary input variable from the address. The generated address can thus lack one or more parameters corresponding to auxiliary input variables in the address template. Otherwise, when there is determined to be a parameter that corresponds to the auxiliary input variable, the action handler component 120 can insert or populate the auxiliary input variable with the parameter. Conversely, in response to the determination that the input variable is not auxiliary, the action handler component 120 can identify a parameter from the identified parameters to include into the input variable.
- the action handler component 120 can validate the address.
- the validation of the address by the action handler component 120 may be to check whether the address is well-formed.
- the action handler component 120 can check the address against a regular expression 130 for addresses (e.g., URIs or URLs).
- the regular expression 130 for addresses can include, for example, schema, hostnames, pathnames, and query strings separated by delimiters for valid addresses.
- the action handler component 120 can compare the address against the regular expression 130 to determine whether the address matches the pattern for addresses as specified by the regular expression 130 . In response to the determination that the address matches, the action handler component 120 can determine that the generated address is successfully validated.
- the action handler component 120 can determine that the generated address is not successfully validated. Based on the determination, the action handler component 120 can also detect an error in validating the generated address. The error can indicate that the address is not well-formed. Furthermore, the action handler component 120 can cease or terminate further processing of the address (e.g., generating an action data structure). The action handler component 120 can provide an indication of the error to the agent service 106 or an administrator for the agent application 110 associated with the action-inventory 140 .
- the action handler component 120 can package or generate an action data structure using the address for executing the action.
- the generation of the action data structure can be in accordance with an application programming interface (API).
- API application programming interface
- the API can be specified by an operating system running on the client device 104 , and can be used by both the digital assistant application 108 and the agent application 110 .
- the action data structure can be passed between the digital assistant application 108 and the agent application 110 via the API.
- the generation of the action data structure can be in accordance with the Hypertext Transfer Protocol (HTTP).
- HTTP Hypertext Transfer Protocol
- the action data structure generated by the action handler component 120 can be an HTTP request (e.g., GET or POST functions).
- the action data structure can include at least one header and at least one body.
- the header can include a device modality (e.g., a smartphone, smart speaker, tablet, laptop, and desktop) of the client device 104 executing the digital assistant application 108 and the agent application 110 .
- the header can include a source address set to the client device 104 .
- the header can include the generated address set a destination address.
- the body of the action data structure can include other information related to the request identified from parsing the input audio signal. For example, the body can include an identifier referencing the digital assistant application 108 or the client device 104 running the digital assistant application 108 .
- the instance of the digital assistant application 108 on the data processing system 102 or the client device 104 can execute the agent interface component 126 to provide the action data structure to the agent application 110 .
- the agent interface component 126 can provide or direct the action data to the agent application 110 .
- the agent interface component 126 can transmit the action data structure over the network 114 to the agent service 106 managing resources for the agent application 110 (e.g., as an HTTP request).
- an HTTP request corresponding to the action data structure can be transmitted by the agent interface component 126 over the network 114 .
- the agent interface component 126 can invoke the agent application 110 using the action data structure (e.g., via the API).
- the agent interface component 126 can make a function call to invoke the agent application 110 in accordance to an API and facilitated by the operating system of the client device 104 .
- the agent interface component 126 can pass the action data structure to the agent application 110 .
- the agent interface component 126 can pass the action data structure to the agent application 110 .
- the data processing system 102 , the client device 104 , or the agent service 106 can execute an instance of the agent application 110 .
- the agent application 110 can receive the action data structure from the agent interface component 126 .
- the action data structure may have been sent via the network 114 , and can the destination address can include the hostname referencing the agent application 110 or the agent service 106 . Since the destination address of the action data structure references the agent application 110 or the agent service 106 , the action data structure can be routed to the agent application 110 or the agent service 106 by the network 114 .
- the destination address of the action data structure can be a deep link with a scheme referencing the agent application 110 .
- the action data structure can be directed by the client device 104 or the network 114 to the agent application 110 referenced by the scheme.
- the agent application 110 can parse the action data structure to identify the address.
- the identification of the address can be from the destination address header of the action data structure.
- the agent application 110 can parse the address to identify the first substring of the address referencing the agent application 110 itself or the agent service 106 managing resources for the agent application 110 .
- the agent application 110 can parse the address the second substring of the address.
- the agent application 110 can further parser the second substring of the address to identify the action and the one or more parameters used to perform the action.
- the agent application 110 can parse the pathname of the URI to identify the action and parse the one or more query strings to identify the one or more parameters used to execute the action.
- the agent application 110 can further identify “checksteps” as a request to get a number of steps measured and the “Example_user_id” as the name of the user whose steps were measured. With the identification, the agent application 110 can perform, carry out, or execute the action in accordance with the one or more parameters.
- the agent application 110 can perform the series of processes and routines to retrieve the number of steps taken by the user of the agent application 110 with the account identifier “example_user_id.”
- the agent application 110 can provide the output of the execution of the action to the agent interface component 126 .
- the agent application 110 can also generate an output to indicate whether the execution of the action is successful or a failure to send to the digital assistant application 108 .
- the agent application 110 can fail in completing the action or terminate short of completion. For example, while processing the request for number of steps, the agent service 106 can determine that the information on the number of steps taken by the corresponding user cannot be retrieved. Based on the determination, the agent application 110 can create the output indicating the failure. In response to the failure, the agent application 110 can generate the output indicating failure. Conversely, the agent application 110 can complete the execution of the action. In response to the completion, the agent application 110 can generate the output indicating success. The agent application 110 can provide the output of the execution of the action to the agent interface component 126 .
- the agent interface component 126 can control whether the agent application 110 is to be run as the non-foreground process 152 or the foreground process 150 .
- the setting or controlling of the foreground process 150 or the non-foreground process 152 can be based on the interface mode indicator of the action-inventory 140 used to generate the action data structure.
- the setting of the foreground process 150 and the non-foreground process 152 by the agent interface component 126 can be facilitated by that operating system running on the client device 104 .
- the agent application 110 can be running as one the applications in the foreground process 150 or the non-foreground process 152 , or neither the foreground process 150 nor the non-foreground process 152 .
- the agent interface component 126 can control whether the agent application 110 is to be set from the non-foreground process 152 to the foreground process 150 .
- the agent interface component 126 can identify which applications are running in the foreground process 150 and as the non-foreground process 152 , such as the digital assistant application 108 and the agent applications 110 installed on the client device 104 .
- the agent interface component 126 can access the operating system of the client device 104 to identify applications running as the foreground process 150 and applications running in the non-foreground process 152 .
- the agent interface component 126 can identify whether the agent application 110 (or other applications on the client device 104 ) is one of the foreground process 150 or the non-foreground process 152 .
- the agent interface component 126 can identify the agent application 110 as running as one of the applications in the foreground process 150 .
- the agent interface component 126 can identify the agent application 110 as running as one of the applications in the non-foreground process 152 .
- the agent interface component 126 can identify the agent application 110 as running as one of the applications neither in the foreground process 150 nor the non-foreground process 152 .
- the agent interface component 126 can cause the client device 104 to initiate execution of the agent application 110 .
- the agent interface component 126 can identify the interface mode indicator.
- the interface mode indicator can specify the user interface component 142 of the agent application 110 is to be rendered on the display 148 of the client device 104 in executing the action corresponding to the action-inventory 140 .
- the agent interface component 126 can set the agent application 110 as the foreground process 150 .
- the agent interface component 126 can launch the agent application 110 in the foreground process 150 .
- the agent interface component 126 can remove the application originally in the foreground process 150 (e.g., another agent application 110 or the digital assistant application 108 ) and set to the non-foreground process 152 .
- the agent handler component 126 can set the agent application 110 from the non-foreground process 152 to the foreground process 150 .
- the agent interface component 126 can maintain the agent application 110 in the foreground process 150 .
- the agent interface component 126 can apply at least one authorization policy of the agent application 110 .
- the authorization policy can specify for which applications the output of the agent application 110 is restricted or permitted to be presented with.
- the authorization policy for one personal fitness application can specify that the output is to be restricted from being presented with another personal fitness application.
- the authorization policy can include a list of application (also referred herein a blacklist) for which the output of the agent application 110 is restricted from concurrent presentation.
- the authorization policy can also include a list of application (also referred herein as a whitelist) for which the output of the agent application 110 is permitted for concurrent presentation.
- Each list can include agent identifiers (e.g., application file names) corresponding to the included applications.
- the authorization policy can be specific to an output type of the agent application 110 .
- the output type can depend on the request identified from parsing the input audio signal. For example, the authorization policy can restrict displays of a number of steps while another personal fitness application is in the foreground but can permit displays of account credential information.
- the determination of whether to apply the authorization policy can be based on whether the agent application 110 is running in the foreground process 150 or the non-foreground process 152 .
- the agent interface component 126 can determine not to apply the authorization policy.
- the agent interface component 126 can also permit the user interface component 142 of the agent application 110 with the output to be rendered in the display 148 of the client device 104 .
- the agent interface component 126 can determine to apply the authorization policy. In this manner, the agent application 110 can remain as part of the non-foreground process 152 or not executing on the client device 104 .
- the agent interface component 126 can also identify the authorization policy based on the request parsed from the input audio signal. The agent interface component 126 can compare the identified request with the output type for the authorization policy. Upon determining a match, the agent interface component 126 can determine to apply the authorization policy specified for the corresponding output type.
- the agent interface component 126 can determine whether the output of the agent application 110 is authorized to be presented with the graphical user interface component of the application in the foreground process 150 .
- the agent interface component 126 can identify the application in the foreground process 150 .
- the agent interface component 126 can identify the agent identifier of the application in the foreground process 150 .
- the agent interface component 126 can compare the application with the lists of application specified in the authorization policy of the agent application 110 . In comparing, the agent interface component 126 can use the identified agent identifier to search both the whitelist and the blacklist of the authorization policy for the agent application 110 .
- the agent interface component 126 can determine to permit the output of the agent application 110 to be presented with the application. On the other hand, if the application in the foreground process 150 matches one of the applications in the blacklist, the agent interface component 126 can determine to restrict the output from being presented with the application.
- the agent interface component 126 can identify the user interface component 142 of the agent application 110 .
- the agent interface component 126 can use the address generated in accordance with the address template of the action-inventory 140 .
- the address generated in accordance with the address template can reference both the action to be executed by the agent application 110 and the user interface component 142 to be rendered in carrying out the action.
- the address generated in accordance with the address template can reference the agent application 110 or the agent service 106 managing resources for the agent application 110 .
- the agent interface component 126 can identify a separate address or identifier for the user interface component 142 from the action-inventory 140 .
- the address can reference the user interface component 142 maintained by the agent service 106 .
- the address and identifier can reference the user interface component 142 maintained on the data repository 128 .
- the agent interface component 126 can identify the user interface component 142 to be rendered in conjunction with the execution of the action by the agent application 110 .
- the agent interface component 126 can access or retrieve the user interface component 142 .
- the agent interface component 126 can retrieve the user interface component 142 .
- the agent interface component 126 can send a request for the user interface component 142 to the agent service 106 including the address.
- the request can be directed to the agent service 106 handling resources for the agent application 110 .
- the agent service 106 in turn can provide a response with the referenced user interface component 142 to the agent interface component 126 .
- the agent interface component 126 can retrieve the user interface component 142 .
- the address included in the action-inventory 140 can be separate from the address generated using the address template, and can reference the agent service 106 or the data repository 128 .
- the agent interface component 126 can send a request for the user interface component 142 including the address.
- the request can be directed to the agent service 106 handling resources for the agent application 110 .
- the request can be directed to the data repository 128 .
- the agent service 106 or the data repository 128 in turn can provide a response with the referenced user interface component 142 to the agent interface component 126 .
- the agent interface component 126 can retrieve the user interface component 142 from the data repository 128 .
- the identifier can correspond to an index of the user interface component 142 maintained on the data repository 128 .
- the agent interface component 126 can search the data repository 128 using the identifier to find the user interface component 142 . Once identified, the agent interface component 126 can retrieve the user interface component 142 from the data repository 128 .
- the agent interface component 126 can also apply an authorization policy of the application in the foreground process 150 .
- the authorization policy can specify which applications the graphical user interface component of the agent in the foreground process 150 is restricted or permitted to be presented with.
- the authorization policy for one banking application can specify that the output is to be restricted from being presented with another banking application.
- the authorization policy can include a list of application (also referred herein a blacklist) for which the graphical user interface of the application in the foreground process 150 is restricted from concurrent presentation.
- the authorization policy can also include a list of application (also referred herein as a whitelist) for which the graphical user interface of the application in the foreground process 150 is permitted for concurrent presentation.
- Each list can include agent identifiers (e.g., application file names) corresponding to the included applications.
- the agent interface component 126 can determine whether the graphical user interface component of the application in the foreground process 150 is authorized to be presented with the output of the agent application 110 .
- the agent interface component 126 can identify the agent application 110 executing the action.
- the agent interface component 126 can identify the agent identifier of the agent application 110 .
- the agent interface component 126 can compare the application with the lists of application specified in the authorization policy of the application in the foreground process 150 . In comparing, the agent interface component 126 can use the identified agent identifier to search both the whitelist and the blacklist of the authorization policy for the application in the foreground process 150 .
- the agent interface component 126 can determine to permit concurrent presentation with the graphical user interface of the application in the foreground 150 . On the other hand, if the agent application 110 matches one of the application in the blacklist, the agent interface component 126 can determine to restrict the concurrent presentation.
- the agent interface component 126 can render the user interface component 142 of the agent application 110 in the display 148 of the client device 104 .
- the user interface component 142 of the agent application 110 can be rendered on the display 148 concurrent with the graphical user interface of the application running in the foreground process 150 .
- the agent interface component 126 can render the user interface component 142 as an overlay superimposed on the graphical user interface of the application in the foreground process 150 .
- the agent interface component 126 can also include the user interface component 142 as a portion of the graphical user interface of the application in the foreground process 150 .
- the application running as the foreground process 150 can be the digital assistant application 108 , and can have a graphical user interface including a dialog with the user operating the digital assistant application 108 on the client device 104 .
- the agent interface component 126 can insert the user interface component 142 in the dialog of the graphical user interface of the digital assistant application 108 .
- the agent interface component 126 can add or insert information into the user interface component 142 rendered on the display 148 of the client device 104 .
- the agent interface component 126 can add, insert, or include the words or the request from the input audio signal into a corresponding interface element of the user interface component 142 .
- the words or the request can be identified by the NLP component 116 from parsing the input audio signal.
- the agent interface component 126 can incorporate the words with the request onto a textbox of the user interface component 142 .
- the agent interface component 126 can retrieve, receive, or identify the output generated by the agent application 110 .
- the agent interface component 126 can withhold the rendering of the user interface component 142 and can wait for the output from the agent application 110 .
- the agent interface component 126 can add, insert, or include the output into a corresponding interface element of the user interface component 142 .
- the agent interface component 126 can receive an output indicating the number of steps taken by the user of the agent application 110 installed on the client device 104 .
- the agent interface component 126 can insert the number of steps into another textbox of the user interface component 142 .
- the agent interface component 126 can render the user interface component 142 on the display 148 .
- the agent interface component 126 can monitor for user interaction events with the user interface component 142 .
- the user interaction events can include, for example, hover-overs, clicks, scrolling, flicking, keypresses, and touch screens, among others.
- the agent interface component 126 can process the user interaction event to update the user interface component 142 .
- the agent interface component 126 can relay the detected user interaction event to the agent application 110 .
- the agent application 110 can process the user interaction event (e.g., using an event handler) and can generate another output in response to the user interaction event.
- the agent application 110 can provide the output to the agent interface component 126 .
- the agent interface component 126 can update the user interface component 142 .
- the agent interface component 126 can maintain the application originally in the foreground process 150 , while adding a user interface component 142 from the agent application 110 for display as an overlay.
- the agent interface component 126 can have both the original application as well as the agent application 110 both as applications in the foreground process 150 .
- the agent interface component 126 can also maintain visual components from both applications rendered on the display 148 of the client device 104 , thereby reducing user of the client device 104 to perform context switching. By keeping both, the agent interface component 459 can also reduce computing resources incurred from switching the rendering and between the foreground process 150 and the non-foreground process 152 .
- the agent interface component 126 can set the agent application 110 as the foreground process 150 .
- the agent interface component 126 can invoke and launch the agent application 110 (e.g., via a the operating system of the client device 104 ).
- the agent interface component 126 can move the application originally running as the foreground process 150 and can set the application as running as the non-foreground process 152 .
- the agent interface component 126 can remove the graphical user interface component of the application originally in the foreground process 150 from being rendered on the display 148 .
- agent interface component 126 can invoke the agent application 110 using the action data structure generated by the action handler component 120 . Instead of rendering the user interface component 142 , the agent interface component 126 can render the graphical user interface of the agent application 110 on the display 148 of the client device 104 .
- the agent application 110 can display the output of the action indicated in the address of the action data structure on the graphical user interface rendered in the display 148 .
- the agent application 110 can monitor for user interaction events with the graphical user interface.
- the agent application 110 can process the user interaction event (e.g., using an event handler) and can generate another output in response to the user interaction event.
- the agent application 110 can update the graphical user interface rendered in the display 148 of the client device 104 using the generated output.
- the digital assistant application 108 can regulate and reduce consumption of computing resources on the client device 104 from the switching.
- FIG. 2 depicts a sequence diagram of an example data flow 200 to rending visual components on applications in the system 100 illustrated in FIG. 1 .
- the data flow 200 can be implemented or performed by the system 100 described above in conjunction with FIG. 1 or system 500 detailed below in conjunction with FIG. 5 .
- the data flow 200 an include communications in the form of packets (e.g., HTTP messages) among the microphone 150 , the client device 104 , the data processing system 102 , the agent service 106 via the network 114 , and the display 148 .
- packets e.g., HTTP messages
- a local instance of the digital assistant application 108 running on the client device 104 can receive audio data 205 (e.g., the input audio signal) from a microphone 146 communicatively coupled with the client device 104 .
- the digital assistant application 108 can execute on the client device 104 in the non-foreground process 152 , without any visual component rendered on the display 148 .
- the client device 104 can have another application in the foreground process 150 , with a graphical user interface rendered on the display 148 .
- the digital assistant application 108 on the client device 104 can perform initial processing (e.g., automated speech recognition) on the audio data 205 to identify one or more words from the audio data 205 .
- the digital assistant application 108 can provide the one or more recognized words from the audio data 205 as an input 210 to a remote instance of the digital assistant application 108 running on the data processing system 102 .
- the NLP component 116 on the data processing system 102 in turn can further process the one or more words of the input 210 to identify a request and one or more parameters.
- the request can indicate an action to be performed by the agent application 110 in accordance with the one or more parameters.
- the action handler component 120 on the data processing system 102 can identify an action-inventory 140 from the data repository 128 using the request and the one or more parameters.
- the agent application 110 associated with the identified action-inventory 140 can be not executing on the client device 104 or can be running as one of the applications of the non-foreground process 152 .
- the action handler component 120 can generate an address in accordance with an address template of the identified action-inventory 140 .
- the address can include a first substring (e.g., a scheme or hostname) referencing the agent application that is to carry out the action.
- the address can additionally include a second string corresponding to the action to be executed and the one or more parameters in accordance to which the action is to be executed.
- the action handler component 120 can package the address into an action data structure to provide as an input 215 to the agent service 106 for the agent application 110 .
- the agent service 106 for the agent application 110 can parse the action data structure to identify the first substring and the second substring.
- the client device 104 can maintain applications in running as the foreground process 150 and as the non-foreground process 152 .
- the agent application 110 executing on the agent service 106 can further parse the second substring to identify the action and the one or more parameters. With the identification, the agent application 110 can execute the action in accordance with the parameters.
- the agent application 110 can generate an output 220 to provide to the client device 104 via the data processing system 102 .
- the agent service 106 can provide the output 220 to the data processing system 102 .
- the digital assistant application 108 on the data processing system 102 can send the output 220 as an output 225 to the client device 104 .
- the agent interface component 126 can determine that the action-inventory 140 specifies that a user interface component 142 is to be rendered in carrying out the action. In response to the determination, the agent interface component 126 can identify a user interface component 235 (e.g., using the address generated using the address template of the action-inventory 140 ) from the set of user interface components 142 . The agent interface component 126 can provide the user interface component 235 to the client device 104 . Using the user interface component 235 , the client device 104 can display rendering data 240 for the display 148 of the client device 104 . The rendering data 240 can include the user interface component 235 as an overlay superimposed on the graphical user interface of the application in the foreground process 150 . The client device 104 can maintain the applications in the foreground process 150 and the non-foreground process 152 prior to the receipt of the audio data 205 from the microphone 146 .
- a user interface component 235 e.g., using the address generated using the address template of the action-
- FIG. 3 illustrates a use case diagram of an example client device 104 displaying input messages and providing user interface components in the system 100 illustrated in FIG. 1 .
- the client device 104 can have an application running as the foreground process 150 and a local instance of the digital assistant application 108 running as the non-foreground process 152 .
- the application running as the foreground process 150 can have a graphical user interface 305 rendered within the display 148 of the client device 104 .
- the graphical user interface 305 can be for a web browser application running as the foreground process 150 .
- the NLP component 116 of the digital assistant application 108 running on the client device 104 can receive an input audio signal via the microphone 146 of the client device 104 .
- the NLP component 116 can apply automated speech recognition to identify one or more words 310 from the input audio signal. For example, the NLP component 116 can identify the words 310 “How many steps have I taken today?” from the input audio signal.
- the NLP component 116 on the client device 104 or the data processing system 102 can perform further processing on the words 310 .
- the NLP component 116 can identify the request from the words 310 recognized from the input audio signal.
- the NLP component 116 can identify the request as a fetch request for a number of steps taken by a user operating or otherwise associated with the client device 104 .
- the action handler component 120 can identify an agent application 110 capable of performing the request.
- the action handler component 120 can also select an action-inventory 140 from the data repository 128 .
- the action-inventory 140 can be for the action corresponding to the request to be executed by the agent application 110 .
- the action handler component 120 can invoke the agent application 110 to carry out the action corresponding to the request.
- the agent interface component 126 can identify that a user interface component 142 is to be displayed in carrying out the action of the action-inventory 140 .
- the action-inventory 140 can specify whether a user interface component 142 is to be displayed, and can identify which user interface component 142 is to be presented.
- the agent interface component 126 can determine whether concurrent presentation is permitted with the graphical user interface component 305 of the foreground application. The determination can be in accordance with an authorization policy for the agent application 110 .
- the authorization policy for the agent application 110 can specify which applications are allowed to be concurrently presented with the user interface component 142 of the agent application 110 . In the example depicted, the agent interface component 126 can determine that concurrent presentation is permitted.
- the agent interface component 126 can identify the user interface component 142 from the action-inventory 140 for carrying out the action indicated by the words 310 . Upon identification, the agent interface component 126 can render the user interface component 142 as an overlay component 315 superimposed on top of the graphical user interface 305 of the foreground process 150 .
- the user interface component 142 rendered as the overlay component 315 can include the words 310 of the request corresponding to the action, “How many steps?”
- the user interface component 142 can also have the resultant output from carrying out the action, “You have taken 3147 steps today.”
- the user interface component 142 rendered as the overlay component 315 can also include a command button 320 . Upon interaction with the command button 320 , the agent interface component 125 can close the overlay component 315 , causing the user interface component 142 to disappear from the rendering on the display 148 of the client device 104 .
- FIG. 4 illustrates a block diagram of an example method 400 of rending visual components on applications.
- the method 400 can be implemented or executed by the system 100 described above in conjunction with FIGS. 1 - 3 or system 500 detailed below in conjunction with FIG. 5 .
- the NLP component 116 can receive an input audio signal ( 405 ).
- the NLP component 116 can parse the input audio signal ( 410 ).
- the action handler component 120 can select an action-inventory 140 ( 415 ).
- the action handler component 120 can generate an address using the action-inventory 140 ( 420 ).
- the action handler component 120 can generate an action data structure ( 425 ).
- the agent interface component 126 can direct the action data structure ( 430 ).
- the agent interface component 126 can determine whether a user interface component 142 is authorized to be presented with a foreground application ( 435 ). If determined to be authorized, the agent interface component 126 can present the user interface component 142 ( 440 ). If determined to be not authorized, the agent interface component 126 can invoke an agent application 110 associated with the user interface component ( 445 ).
- the NLP component 116 can receive an input audio signal ( 405 ).
- the NLP component 116 can receive one or more data packets including the input audio signal acquired at a sensor (e.g., the microphone 146 ) on the client device 104 .
- the NLP component 116 of the digital assistant application 108 executed at least partially by the data processing system 102 can receive the input audio signal.
- the input audio signal can include a conversation facilitated by the digital assistant application 108 .
- the conversation can include one or more inputs and outputs.
- the conversation can be audio based, text based, or a combination of audio and text.
- the input audio signal can include text input, or other types of input that can provide conversational information.
- the client device 104 can have applications running as the foreground process 150 and the non-foreground process 152 .
- the NLP component 116 can parse the input audio signal ( 410 ). By parsing the input audio signal, the NLP component 116 can identify a request and one or more parameters using various natural language processing techniques.
- the request can be an intent or request that can be fulfilled by the digital assistant application 108 or the agent application 110 .
- the parameters can define the request. Based on the request, the NLP component 116 can determine that the request corresponds to one of the functions of the agent application 110 .
- the action handler component 120 can select an action-inventory 140 ( 415 ). Based on the request identified from the input audio signal, the action handler component 120 can select the action-inventory 140 from the set maintained on the data repository 128 . For each action-inventory 140 , the action handler component 120 can compare the request with the request identifier of the action-inventory 140 . The comparison can be performed using a semantic knowledge graph. From the comparison, the action handler component 120 can determine that the request matches the request identifier of the action-inventory 140 . In response to the determination, the action handler component 120 can select the action-inventory 140 from the set maintained on the data repository 128 . The selected action-inventory can indicate that the user interface component 142 of the agent application 110 that is not running as the foreground process 150 is to be presented.
- the action handler component 120 can generate an address using the action-inventory 140 ( 420 ).
- the generation of the address can be in accordance with an address template of the action-inventory 140 .
- the address template can have a first portion and a second portion.
- the first portion can reference the agent application 110 or the agent service 106 for the agent application 110 to carry out the action.
- the second portion can reference the action and can include one or more input variables for the action.
- the action handler component 120 can take the first portion and the second portion from the address template in generating the address.
- the action handler component 120 can insert or populate the parameters into the input variables of the second portion of the address template.
- the action handler component 120 can generate an action data structure ( 425 ).
- the action data structure can include a header and a body.
- the action handler component 120 can insert the address generated in accordance with the address template of the action-inventory 140 into the header of the action data structure.
- the action handler component 120 can set the address as a destination address of the header to route the action data structure to the agent application 110 or the agent service 106 for the agent application 110 .
- the agent interface component 126 can direct the action data structure ( 430 ).
- the agent interface component 126 can direct the action data structure to the agent application 110 .
- the agent interface component 126 can invoke the agent application 110 using the action data structure and can pass the action data structure to the agent application 110 .
- the agent application 110 in turn can parse the action data structure to identify the action and one or more parameters form the address included in the action data structure.
- the agent application 110 can execute the action in accordance with the parameters.
- the agent interface component 126 can determine whether a user interface component 142 is authorized to be presented with a foreground application ( 435 ). The determination of whether the user interface component 142 is authorized can be in accordance with an authorization policy.
- the authorization policy can specify whether the output of the agent application 110 is permitted or restricted from concurrent presentation with the graphical user interface of the application running as the foreground process 150 .
- the agent interface component 126 can identify the application running in the foreground process 150 .
- the agent interface component 126 can compare the application with the specifications of the authorization policy. When the authorization policy permits concurrent presentation, the agent interface component 126 can determine that the user interface component 142 is authorized to be presented. Otherwise, when the authorization policy restricts the concurrent presentation, the agent interface component 126 can determine that the user interface component 142 is not authorized to be presented with the application running as the foreground process 150 .
- the agent interface component 126 can present the user interface component 142 ( 440 ).
- the agent interface component 126 can identify the user interface component 142 from the selected action-inventory 140 (e.g., using an address include therein).
- the agent interface component 126 can render the identified user interface component 142 as an overlay on the graphical user interface of the application running in the foreground process 150 .
- the agent interface component 126 can include the request for the action and the output generated by the agent application 110 in the user interface component 142 .
- the agent interface component 126 can invoke an agent application 110 associated with the user interface component 142 ( 445 ).
- the agent interface component 126 can remove the original application running from the foreground process 150 and set the application to the non-foreground process 152 , thereby removing the rendering of the graphical user interface of the application.
- the agent interface component 126 can set with the agent application 110 as the foreground process 150 .
- the agent interface component 126 can also render the graphical user interface of the agent application 110 on the display 148 of the client device.
- FIG. 5 is a block diagram of an example computer system 500 .
- the computer system or computing device 500 can include or be used to implement the system 100 or its components such as the data processing system 102 .
- the computing system 500 includes a bus 505 or other communication component for communicating information and a processor 510 or processing circuit coupled to the bus 505 for processing information.
- the computing system 500 can also include one or more processors 510 or processing circuits coupled to the bus for processing information.
- the computing system 500 also includes main memory 515 , such as a random access memory (RAM) or other dynamic storage device, coupled to the bus 505 for storing information and instructions to be executed by the processor 510 .
- the main memory 515 can be or include the data repository 128 .
- the main memory 515 can also be used for storing position information, temporary variables, or other intermediate information during execution of instructions by the processor 510 .
- the computing system 500 may further include a read-only memory (ROM) 520 or other static storage device coupled to the bus 505 for storing static information and instructions for the processor 510 .
- ROM read-only memory
- a storage device 525 such as a solid state device, magnetic disk or optical disk, can be coupled to the bus 505 to persistently store information and instructions.
- the storage device 525 can include or be part of the data repositories 128 .
- the computing system 500 may be coupled via the bus 505 to a display 535 , such as a liquid crystal display or active matrix display, for displaying information to a user.
- a display 535 such as a liquid crystal display or active matrix display, for displaying information to a user.
- An input device 530 such as a keyboard including alphanumeric and other keys, may be coupled to the bus 505 for communicating information and command selections to the processor 510 .
- the input device 530 can include a touch screen display 535 .
- the input device 530 can also include a cursor control, such as a mouse, a trackball, or cursor direction keys, for communicating direction information and command selections to the processor 510 and for controlling cursor movement on the display 535 .
- the display 535 can be part of the data processing system 102 , the client devices 104 , or other components of FIG. 1 , for example.
- the processes, systems and methods described herein can be implemented by the computing system 500 in response to the processor 510 executing an arrangement of instructions contained in main memory 515 .
- Such instructions can be read into main memory 515 from another computer-readable medium, such as the storage device 525 .
- Execution of the arrangement of instructions contained in main memory 515 causes the computing system 500 to perform the illustrative processes described herein.
- One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 515 .
- Hard-wired circuitry can be used in place of or in combination with software instructions together with the systems and methods described herein. Systems and methods described herein are not limited to any specific combination of hardware circuitry and software.
- the users may be provided with an opportunity to control whether programs or features that may collect personal information (e.g., information about a user's social network, social actions, or activities; a user's preferences; or a user's location), or to control whether or how to receive content from a content server or other data processing system that may be more relevant to the user.
- personal information e.g., information about a user's social network, social actions, or activities; a user's preferences; or a user's location
- certain data may be anonymized in one or more ways before it is stored or used, so that personally identifiable information is removed when generating parameters.
- a user's identity may be anonymized so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, postal code, or state level), so that a particular location of a user cannot be determined.
- location information such as to a city, postal code, or state level
- the user may have control over how information is collected about him or her and used by the content server.
- the subject matter and the operations described in this specification can be implemented in digital electronic circuitry or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- the subject matter described in this specification can be implemented as one or more computer programs, e.g., one or more circuits of computer program instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatuses.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- a computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial-access memory array or device, or a combination of one or more of them. While a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium can also be, or be included in, one or more separate components or media (e.g., multiple CDs, disks, or other storage devices).
- the operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
- data processing system encompass various apparatuses, devices, and machines for processing data, including, by way of example, a programmable processor, a computer, a system on a chip, or multiple ones, or combinations of the foregoing.
- the apparatus can include special-purpose logic circuitry, e.g., an FPGA (field-programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them.
- the apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
- the components of system 100 can include or share one or more data processing apparatuses, systems, computing devices, or processors.
- a computer program (also known as a program, software, software application, app, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program can correspond to a file in a file system.
- a computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs (e.g., components of the data processing system 102 ) to perform actions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatuses can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field-programmable gate array) or an ASIC (application-specific integrated circuit).
- Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- the subject matter described herein can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification, or a combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- LAN local area network
- WAN wide area network
- inter-network e.g., the Internet
- peer-to-peer networks e.g., ad hoc peer-to-peer networks.
- the computing system such as system 100 or system 500 can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network (e.g., the network 114 ).
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data (e.g., data packets representing a content item) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device).
- Data generated at the client device e.g., a result of the user interaction
- can be received from the client device at the server e.g., received by the data processing system 102 from the client devices 104 or the agent service 106 ).
- the separation of various system components does not require separation in all implementations, and the described program components can be included in a single hardware or software product.
- the NLP component 116 , the audio signal generator component 118 , the action handler component 120 , the response selector component 122 , the agent registry component 124 , and the agent interface component 126 can be a single component, app, or program, or a logic device having one or more processing circuits, or part of one or more servers of the data processing system 102 .
- references to implementations, elements, or acts of the systems and methods herein referred to in the singular may also embrace implementations including a plurality of these elements, and any references in plural to any implementation, element, or act herein may also embrace implementations including only a single element.
- References in the singular or plural form are not intended to limit the presently disclosed systems or methods, their components, acts, or elements to single or plural configurations.
- References to any act or element being based on any information, act, or element may include implementations where the act or element is based at least in part on any information, act, or element.
- any implementation disclosed herein may be combined with any other implementation or embodiment, and references to “an implementation,” “some implementations,” “one implementation,” or the like are not necessarily mutually exclusive and are intended to indicate that a particular feature, structure, or characteristic described in connection with the implementation may be included in at least one implementation or embodiment. Such terms as used herein are not necessarily all referring to the same implementation. Any implementation may be combined with any other implementation, inclusively or exclusively, in any manner consistent with the aspects and implementations disclosed herein.
- references to “or” may be construed as inclusive so that any terms described using “or” may indicate any of a single, more than one, and all of the described terms.
- a reference to “at least one of ‘A’ and ‘B’” can include only ‘A’, only ‘B’, as well as both ‘A’ and ‘B’.
- Such references used in conjunction with “comprising” or other open terminology can include additional items.
Abstract
The present disclosure is generally related to systems and methods of rendering visual components on applications. At least one action-inventory can indicate to render a user interface component. A data processing system can identify a request from an input audio signal from a client device. The client device can display a first application in a foreground process. The data processing system can select an action-inventory to execute the action corresponding to the request by a second application. The data processing system can generate an action data structure using the action-inventory to provide to the second application. The data processing system can determine that an output of the second application is authorized to be presented with the first application. The data processing system can display, on the client device, a user interface component from the second application with the first application.
Description
- Applications can be installed on a computing device. The computing device can execute the application. The application can present digital content.
- According to a first aspect of the disclosure, a system to render visual components on applications. The system can include an agent registry executed on a data processing system having one or more processors. The agent registry can maintain a plurality of action-inventories for a plurality of applications. At least one of the plurality of action-inventories can indicate to render a user interface component for one of the plurality of applications in executing an action. The system can include a natural language processor executed on the data processing system. The natural language processor can receive a data packet comprising an input audio signal detected by a sensor of a client device. The client device can display a graphical user interface of a first application in a foreground process on the client device. The natural language processor can parse the input audio signal of the data packet to identify a request. The system can include an action handler executed on the data processing system. The action handler can select, from the plurality of action-inventories, an action-inventory that executes the action corresponding to the request by a second application of the plurality of applications. The second application can be installed on the client device and not in the foreground process. The action handler can generate an action data structure in accordance with the action-inventory. The system can include an agent interface executed on the data processing system. The agent interface can provide the action data structure to the second application to cause the second application to parse the action data structure and execute the action to generate an output. The agent interface can determine that the output of the second application from execution of the action is authorized to be presented with the graphical user interface of the first application based on an authorization policy of the second application. The agent interface can identify the user interface component of the second application for the action-inventory selected from the plurality of action-inventories, responsive to the determination the output of the second application is authorized to be presented with the first application. The agent interface can display, on the client device, the user interface component including the output from the second application with the graphical user interface of the first application authorized to be presented with the second application.
- At least one of the plurality of action-inventories may have an address template for accessing the user interface component in executing the action, the address template defining a first portion and a second portion, the first portion corresponding to one of the plurality of applications, the second portion including an input variable in carrying out the action. The natural language processor may parse the input audio signal of the data packet to identify a parameter defining the request to be executed by the second application. The action handler may generate, in accordance with the address template of the action-inventory, an address to execute the action of the action-inventory, the address comprising a first substring and a second substring, the first substring corresponding to the second application, the second substring having the parameter used to control execution of the action; and generate the action data structure including the address generated by the action handler in accordance with the address template.
- At least one of the plurality of action-inventories may have an address template for accessing the user interface component in executing the action, the address template defining a first portion and a second portion, the first portion corresponding to one of the plurality of applications, the second portion including an input variable in carrying out the action. The natural language processor may parse the input audio signal of the data packet to identify the parameter defining the request to be executed by the second application. The action handler to generate, in accordance with the address template of the action-inventory, an address to execute the action of the action-inventory, the address comprising a first substring and a second substring, the first substring corresponding to the second application, the second substring having the parameter used to control execution of the action. The agent interface may identify the user interface component of the second application for the action-inventory selected from the plurality of action-inventories using the address generated in accordance with the address template for the action-inventory.
- Each action-inventory may have an agent identifier corresponding to one of the plurality of applications and a request identifier corresponding to the action. The natural language processor may parse the input audio signal of the data packet to identify an agent identifier corresponding to an agent, the agent corresponding to the second application installed on the client device. The action handler may select, from the plurality of action-inventories, the action-inventory for executing the action having the agent identifier corresponding to the agent identifier identified from parsing the input audio signal.
- The action handler may determine that the interface mode indicator of the action-inventory specifies rendering of the user interface component in executing the action, and identify the user interface component of the second application for the action-inventory, responsive to the determination that the interface mode indicator specifies rendering of the user interface component.
- The agent interface may determine that the client device is not authenticated with the second application for the action-inventory to carry out the action corresponding to a second request; and may present, responsive to the determination that the client device is not authenticated with the second application, a prompt interface on the client device to authenticate the client device with the second application to execute the action corresponding to the second request.
- The agent interface may determine that an output of a third application is not authorized to be presented with the first application based on an authorization policy of the third application, the third application installed on the client device and not in the foreground process, set the third application as the foreground process of the client device, the first application transferred from the foreground process to a non-foreground process running on the client device, and display, on the client device, a graphical user interface of the third application including an output generated by the third application.
- The agent interface may determine that the first application authorizes presentation of the user interface component of the second application with the graphical user interface of the first application based on an authorization policy of the first application, the authorization policy permitting user interface components from a first subset of the plurality of applications to be presented with the graphical user interface and restricting user interface components from a second subset of the plurality of applications to be presented with the graphical user interface, and display, on the client device, the user interface component including the output from the second application with the graphical user interface of the first application, responsive to the determination that the first application authorizes the presentation of the user interface component of the second application with the graphical user interface of the first application.
- The agent interface may provide the action data structure to the second application to cause the second application to monitor for an interaction event on the user interface component from the second application presented with the graphical user interface of the first application and process, responsive to detection of the interaction event, the interaction event to update the user interface component.
- The natural language processor may receive the data packet via a digital assistant application, the data packet comprising the input audio signal detected by the sensor of the client device, the client device displaying the graphical user interface of the digital assistant application corresponding to the first application in the foreground process. The agent interface may display, within the graphical user interface of the digital assistant application, the user interface component including a subcomponent indicating the request identified from parsing the input audio signal and the user interface component including the output from an agent corresponding to the second application. The agent may lack natural language processing capabilities.
- The natural language processor may receive the data packet, the data packet comprising the input audio signal detected by the sensor of the client device, the client device displaying the graphical user interface of a first agent corresponding to the first application in the foreground process; and the agent interface to display the user interface component including the output from the second application as an overlay on the graphical user interface of the first agent.
- The plurality of action-inventories may have a first subset of action-inventories and a second subset of action-inventories, the first subset including action-inventories defined by an administrator of the data processing system, the second subset including action-inventories provided by an agent service handling one of the plurality of applications.
- The action handler may provide the action data structure to the second application to cause the second application to parse the action data structure to identify the action to be executed, generate the output by executing the action identified from the action data structure and provide the output for the user interface component to be presented with the graphical user interface of the first application authorized to be presented with the second application.
- According to a second aspect of the disclosure, a method of rendering visual components on applications. A data processing system having one or more processors can maintain a plurality of action-inventories for a plurality of applications. At least one of the plurality of action-inventories can indicate to render a user interface component for one of the plurality of applications in executing an action. The data processing system can receive a data packet comprising an input audio signal detected by a sensor of a client device. The client device can display a graphical user interface of a first application in a foreground process on the client device. The data processing system can parse the input audio signal of the data packet to identify a request. The data processing system can select, from the plurality of action-inventories, an action-inventory that executes the action corresponding to the request by a second application of the plurality of applications. The second application can be installed on the client device and not in the foreground process. The data processing system can generate an action data structure in accordance with the action-inventory. The data processing system can provide the action data structure to the second application to cause the second application to parse the action data structure and execute the action to generate an output. The data processing system can determine that the output of the second application from execution of the action is authorized to be presented with the graphical user interface of the first application based on an authorization policy of the second application. The data processing system can identify the user interface component of the second application for the action-inventory selected from the plurality of action-inventories, responsive to the determination the output of the second application is authorized to be presented with the first application. The data processing system can display, on the client device, the user interface component including the output from the second application with the graphical user interface of the first application authorized to be presented with the second application.
- Optional features of the first aspect described above or in the detailed description below may be combined with the second aspect.
- These and other aspects and implementations are discussed in detail below. The foregoing information and the following detailed description include illustrative examples of various aspects and implementations and provide an overview or framework for understanding the nature and character of the claimed aspects and implementations. The drawings provide illustration and a further understanding of the various aspects and implementations, and are incorporated in and constitute a part of this specification.
- The accompanying drawings are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing. In the drawings:
-
FIG. 1 illustrates a block diagram of an example system to render visual components on applications in response to voice input commands, in accordance with an example of the present disclosure. -
FIG. 2 illustrates a sequence diagram of an example data flow to render visual components on applications in response to voice input commands in the system illustrated inFIG. 1 , in accordance with an example of the present disclosure. -
FIG. 3 illustrates a use case diagram of an example client device displaying input messages and generating addresses in the system illustrated inFIG. 1 , in accordance with an example of the present disclosure. -
FIG. 4 illustrates a flow diagram of a method to render visual components on applications in response to voice input commands using the example system illustrated inFIG. 1 , in accordance with an example of the present disclosure. -
FIG. 5 is a block diagram of an example computer system. - Following below are more detailed descriptions of various concepts related to and implementations of, methods, apparatuses, and systems to render visual components on applications in response to voice input commands. The various concepts introduced above and discussed in greater detail below may be implemented in any of numerous ways.
- Multiple applications can be installed on a client device to carry out preconfigured functions as requested by a user operating the client device. At a given time, the client device can run a subset of these applications as a foreground process or a non-foreground process as set by an operating system running on client device. Foreground processes (including visible processes) can correspond to applications with a graphical user interface presented on a display of the client device with which the user may be interacting. Non-foreground processes (e.g., a service process, a background process, or a cached process) can correspond to applications not displaying visual components to the user and executing without affecting the graphical user interface of the foreground process. The applications running on the client device can switch back and forth between executing as a foreground process or non-foreground process. Upon switching, the graphical user interface of the application that was previously the foreground process can be replaced by the graphical user interface of the application that is now set as the foreground process. In addition, the processing and memory allocations can change as a result of the switching between foreground and non-foreground processes.
- One of these applications running in the foreground or the non-foreground can include a digital assistant application. Unlike at least some of the other applications installed on the client device, the digital assistant application can have natural language processing capabilities. Configured with these capabilities, the digital assistant application can acquire and parse an input audio signal detected via a microphone of the client device to recognize words from the input audio signal. From the recognized words, the digital assistant application can determine a request to be carried out and one or more parameters defining the request. The request can refer to one of the preconfigured functions with input arguments compatible with the parameters that can be executed by the digital assistant application (e.g., retrieving results for a search query). For such requests, the digital assistant application itself can process the request and present an audio or visual output from carrying out the request. Some requests, however, may not correspond to any of the functions preconfigured on the digital assistant application (e.g., playing a video from an online video platform). For these types of requests, the digital assistant application can identify another application that is capable of carrying out the request and invoke the function of the application to pass parameters.
- By invoking the other application to carry out the request, the digital assistant application can cause a change in which applications are foreground and non-foreground processes, resulting in a shift in the graphical user interface displayed on the client device. In addition, the processing and memory allocations to each executing application can change and fluctuate, especially when invoking a previously non-executing application. While the desired request may be eventually carried out on the client device with the invoked application, the switch can interfere with the overall user experience with the applications on the client device. For example, the sudden alteration in the graphical user interface displayed on the client device can force the user to under context switching between the interfaces of multiple applications, thereby degrading human-computer interactions (HCI) with the client device. The degradation in the client device can also be exacerbated by change and fluctuation in the new allocations of computing resources in performing the switching of foreground and non-foreground processes.
- The present disclosure provides an improved user interface. In particular, to address the technical challenges in interfacing, the present disclosure can provide action-inventories used to identify and present user interface components to be overlaid on the graphical user interfaces of applications in the foreground. The action-inventory can specify whether to provide a user interface element of the application while maintaining the application in the background. The action-inventory can also include an address template to construct an address for accessing a function of the digital assistant application and for providing the user interface element on the client device. The address template of an action-inventory may be, for example, a Uniform Resource Identifier (URI) template for constructing a web address with a hostname, pathname, and queries to invoke a function of an application and for the user interface component. The hostname (or a scheme) can reference a location or a domain name of the application. The pathname can reference the function. The queries can include one or more input arguments used to carry out the function. The address template can also define a mapping of the request parsed from input audio signals to functions to be executed by the application.
- Upon the recognition of the request from the input audio signal, the digital assistant application can determine that the request does not correspond with any of the functionalities of the digital assistant application or to any application running as the foreground process. Instead of outputting that the digital assistant application cannot perform the indicated request, the digital assistant application can identify another application to carry out the function. While processing the request, the digital assistant application can remain as a background process with the original application remaining as the foreground process. With the identification, the digital assistant application can select an action-inventory of the application for the function. The selected action-inventory can indicate that a user interface component of the application is to be presented. Using the action-inventory, the digital assistant application can construct, expand, and generate the address for the function in accordance with the address template. The address can include the hostname (or scheme) referencing the application, the path referencing the function and the user interface component, and one or more queries including the parameters.
- With the generation of the address in accordance with the address template, the digital assistant application can generate and pass an action data structure (e.g., an API function call) including the address to the application. The application to which the action data structure is passed can be opened as a background process, and remain in the background process as the action is processed and completed. The passing of the action data structure to the other application can be performed as the original application continues to remain as the foreground process. For example, the action data structure can be passed by an operating system on the client device from the digital assistant application to the application via the API function call. As the address includes the hostname referencing the application, the action data structure can be directed to reach the referenced application. Upon receipt, the application can parse the action data structure to identify the address. The application can further parse the address to identify the pathname to identify the function to be executed and parse the string query to identify the one or more parameters to define the execution of the function. Based on the identification, the application can execute the function to generate an output. Using the output generated by the application, the digital assistant can render the user interface component of the application. The user interface component of the application can be overlaid on the graphical user interface of the application of the foreground process.
- In this manner, the digital assistant application can invoke the functions of other applications and present rendering of the user interface components from these applications, thereby augmenting the functions of the digital assistant application. The invocation may be performed without the user manually searching, opening, and entering to achieve the desired function, or having to context switch between different applications. The elimination of context switching on the part of the user operating the client device can thus improve HCI. Furthermore, the applications can remain in the foreground and non-foreground processes, even with the recognition of a request from the input audio signal. As the switching between foreground and background processes is eliminated, the digital assistant application can reduce consumption of computing resources and expenditure of time on the part of the user.
- Referring to
FIG. 1 , depicted is anexample system 100 to render visual components on applications. Thesystem 100 can include at least onedata processing system 102, at least oneclient device 104, and at least one agent services 106. Thedata processing system 102, theclient device 104, and theagent service 106 can be communicatively coupled with one another via at least onenetwork 114. - The components of the
system 100 can communicate over anetwork 114. Thenetwork 114 can include, for example, a point-to-point network, a broadcast network, a wide area network, a local area network, a telecommunications network, a data communication network, a computer network, an ATM (Asynchronous Transfer Mode) network, a SONET (Synchronous Optical Network) network, a SDH (Synchronous Digital Hierarchy) network, an NFC (Near-Field Communication) network, a local area network (LAN), a wireless network or a wireline network, and combinations thereof. Thenetwork 114 can include a wireless link, such as an infrared channel or satellite band. The topology of thenetwork 114 may include a bus, star, or ring network topology. Thenetwork 114 can include mobile telephone networks using any protocol or protocols used to communicate among mobile devices, including advanced mobile phone protocol (AMPS), time division multiple access (TDMA), code-division multiple access (CDMA), global system for mobile communication (GSM), general packet radio services (GPRS), or universal mobile telecommunications system (UMTS). Different types of data may be transmitted via different protocols, or the same types of data may be transmitted via different protocols. - The
data processing system 102 and theagent service 106 each can include multiple, logically grouped servers and facilitate distributed computing techniques. The logical group of servers may be referred to as a data center, server farm, or a machine farm. The servers can be geographically dispersed. A data center or machine farm may be administered as a single entity, or the machine farm can include a plurality of machine farms. The servers within each machine farm can be heterogeneous—one or more of the servers or machines can operate according to one or more type of operating system platform. Thedata processing system 102 and theagent service 106 each can include servers in a data center that are stored in one or more high-density rack systems, along with associated storage systems, located for example in an enterprise data center. In this way, thedata processing system 102 or theagent service 106 with consolidated servers can improve system manageability, data security, the physical security of the system, and system performance by locating servers and high performance storage systems on localized high performance networks. Centralization of all or some of thedata processing system 102 oragent service 106 components, including servers and storage systems, and coupling them with advanced system management tools allows more efficient use of server resources, which saves power and processing requirements and reduces bandwidth usage. Each of the components of thedata processing system 102 can include at least one processing unit, server, virtual server, circuit, engine, agent, appliance, or other logic device such as programmable logic arrays configured to communicate with thedata repositories 128 and with other computing devices. Theagent service 106 can also include at least one processing unit, server, virtual server, circuit, engine, agent, appliance, or other logic device such as programmable logic arrays configured to communicate with a data repository and with other computing devices. - The
data processing system 102 can include an instance of at least onedigital assistant application 108. Thedigital assistant application 108 can include at least one natural language processor (NLP) component 116 (sometimes referred herein as an natural language processor) to parse audio-based inputs. Thedigital assistant application 108 can include at least one audio signal generator component 118 (sometimes referred herein as an audio signal generator) to generate audio-based signals. Thedigital assistant application 108 can include at least one action handler component 120 (sometimes referred herein as an action handler) to generate action data structures based on the audio-based inputs. Thedigital assistant application 108 can include at least oneresponse selector component 122 to select responses to audio-based input signals. Thedigital assistant application 108 can include at least one agent registry component 124 (sometimes referred herein as an agent registry) to maintain action-inventories for generating the action data structures. Thedigital assistant application 108 can include at least one agent interface component 126 (sometimes referred herein as agent interface) to communicate with one ormore agent applications 110. - The
digital assistant application 108 can include at least onedata repository 128. Thedata repository 128 of thedata processing system 102 can include one or more local or distributed databases and can include a database management system. Thedata repository 128 can include one or moreregular expressions 130, one ormore data parameters 132, one ormore policies 134, one ormore response data 136, one ormore templates 138, and one ormore action inventories 140. TheNLP component 116, the audiosignal generator component 118, theaction handler component 120, theresponse selector component 122, theagent registry component 124, theagent interface component 126, and thedata repository 128 can be separate from one another. Thedata repository 128 can include computer data storage or memory and can store the one or moreregular expressions 130, one ormore data parameters 132, one ormore policies 134,response data 136,templates 138, and the one or more action-inventories 140 among other data. Thedata parameters 132,policies 134, andtemplates 138 can include information such as rules about a voice-based session between theclient devices 104 and thedata processing system 102. Theresponse data 136 can include content items for audio output or associated metadata, as well as input audio messages that can be part of one or more communication sessions with theclient devices 104. The action-inventories 140 can include information to invoke or interface with theagent application 110. - The
data processing system 102 can include an instance of at least one agent application 110 (also referred herein as “agent” or “application”) to execute various functions. Theagent service 106 can include an instance of at least oneagent application 110 to execute various functions. Theagent application 110 can lack natural language processing capabilities (such as those provided by the NLP component 116). Theagent application 110 can also have a preconfigured or a predetermined set of input capabilities. The input capabilities of theagent application 110 can include, for example, inputs from a mouse, a keyboard, a touch screen (e.g., a display 148), or a camera of theclient device 104. The input capabilities of theagent application 110 can lack audio (e.g., a microphone 146) from theclient device 104. Theagent application 110 can have a graphical user interface (GUI) to receive the inputs from theclient device 104. Based on the input from the predetermined input capabilities, theagent application 110 can carry out or execute one or more functions. For example, theagent application 110 can be a coffee purchase application, and can have a graphical user interface with one or more element to specify an order of a coffee. In this application, a user operating theclient device 104 can manually enter the coffee, size, and billing information, among others, on the graphical user interface rendered on thedisplay 148 of theclient device 104. Once set, theagent application 110 on theclient device 104 can send the input via thenetwork 114 to theagent service 106. Theagent service 106 in turn can carry out the purchase order indicated by the inputs entered through the graphical user interface of theagent application 110. - The components of the
data processing system 102 can each include at least one processing unit or other logic device such as a programmable logic array engine or module configured to communicate with thedata repository 128. The components of thedata processing system 102 can be separate components, a single component, or part of multipledata processing systems 102. Thesystem 100 and its components, such as adata processing system 102, can include hardware elements, such as one or more processors, logic devices, or circuits. - The functionalities of the
data processing system 102, such as thedigital assistant application 108 or theagent application 110, can be included or otherwise be accessible from the one ormore client devices 104. The functionalities of thedata processing system 102 may correspond to the functionalities or interface with thedigital assistant application 108 executing on theclient devices 104. Theclient devices 104 can each include and execute a separate instance of the one or more components of thedigital assistant application 108. Theclient devices 104 can otherwise have access to the functionalities of the components of thedigital assistant application 108 on a remotedata processing system 102 via thenetwork 114. For example, theclient device 104 can include the functionalities of theNLP component 116 and access the remainder of the components of thedigital assistant application 108 via thenetwork 114 to thedata processing system 102. The functionalities of thedata processing system 102 may correspond to the functionalities or interface with theagent application 110 executing on theclient devices 104. Theclient devices 104 can otherwise have access to the functionalities of the components of theagent application 110 on thedata processing system 102 or theagent service 106 via thenetwork 114. For example, theclient device 104 can pre-process input from the mouse, keyboard, touchscreen, or camera for theagent application 110, and provide the input to theagent service 106 to carry out the function of theagent application 110. Thedigital assistant application 108 executing on theclient device 104 can include the functionalities of thedigital assistant application 108 executing on thedata processing system 102. - The
client device 104 can each include at least one logic device such as a computing device having a processor to communicate with each other with thedata processing system 102 via thenetwork 114. Theclient devices 104 can include an instance of any of the components described in relation to thedata processing system 102 or theagent service 106. Theclient device 104 can include an instance of thedigital assistant application 108. Theclient device 104 can include an instance of theagent application 110. Theclient devices 104 can include a desktop computer, laptop, tablet computer, personal digital assistant, smartphone, mobile device, portable computer, thin client computer, virtual server, speaker-based digital assistant, or other computing device. - The
client device 104 can include, execute, interface, or otherwise communicate with one or more of at least one instance of thedigital assistant application 108, at least one instance of theagent application 110, and at least onecommunications interface 112, among others. In addition, theclient device 104 can include, interface, or otherwise communicate with at least onespeaker 144, at least onemicrophone 146, and at least onedisplay 148. Theclient device 104 can include an audio driver to provide a software interface with thespeaker 144 and themicrophone 146. The audio driver can execute instructions provided by thedata processing system 102 to control thespeaker 144 to generate a corresponding acoustic wave or sound wave. The audio driver can execute an audio file or other instructions to convert an acoustic wave or sound wave acquired from themicrophone 146 to generate audio data. For example, the audio driver can execute an analog-to-driver converter (ADC) to transform the acoustic wave or sound wave to the audio data. Theclient device 104 can include a graphics driver to provide a software interface with thedisplay 148. The graphics driver can execute instructions provided by thedata processing system 102 or theagent service 106 to control thedisplay 148 to generate a corresponding rendering thereon. - The instance of the
digital assistant application 108 on theclient device 104 can include or be executed by one or more processors, logic array, or memory. The instance of thedigital assistant application 108 on theclient device 104 can detect a keyword and perform an action based on the keyword. Thedigital assistant application 108 on theclient device 104 can be an instance of thedigital assistant application 108 executed at thedata processing system 102 or can perform any of the functions of thedigital assistant application 108. The instance of thedigital assistant application 108 on theclient device 104 can filter out one or more terms or modify the terms prior to transmitting the terms as data to the data processing system 102 (e.g., the instance of thedigital assistant application 108 on the data processing system 102) for further processing. The instance of thedigital assistant application 108 on theclient device 104 can convert the analog audio signals detected by thespeaker 144 into a digital audio signal and transmit one or more data packets carrying the digital audio signal to thedata processing system 102 via thenetwork 114. The instance of thedigital assistant application 108 on theclient device 104 can transmit data packets carrying some or the entire input audio signal responsive to detecting an instruction to perform such transmission. The instruction can include, for example, a trigger keyword or other keyword or approval to transmit data packets comprising the input audio signal to thedata processing system 102. - The instance of the
digital assistant application 108 on theclient device 104 can perform pre-filtering or pre-processing on the input audio signal to remove certain frequencies of audio. The pre-filtering can include filters such as a low-pass filter, high-pass filter, or a bandpass filter. The filters can be applied in the frequency domain. The filters can be applied using digital signal processing techniques. The filter can keep frequencies that correspond to a human voice or human speech, while eliminating frequencies that fall outside the typical frequencies of human speech. For example, a bandpass filter can remove frequencies below a first threshold (e.g., 70 Hz, 75 Hz, 80 Hz, 85 Hz, 90 Hz, 95 Hz, 100 Hz, or 105 Hz) and above a second threshold (e.g., 200 Hz, 205 Hz, 210 Hz, 225 Hz, 235 Hz, 245 Hz, or 255 Hz). Applying a bandpass filter can reduce computing resource utilization in downstream processing. The instance of thedigital assistant application 108 on theclient device 104 can apply the bandpass filter prior to transmitting the input audio signal to thedata processing system 102, thereby reducing network bandwidth utilization. However, based on the computing resources available to theclient device 104 and the available network bandwidth, it may be more efficient to provide the input audio signal to thedata processing system 102 to allow thedata processing system 102 to perform the filtering. The instance of thedigital assistant application 108 on theclient device 104 can apply additional pre-processing or pre-filtering techniques such as noise reduction techniques to reduce ambient noise levels that can interfere with the natural language processor. Noise reduction techniques can improve accuracy and speed of the natural language processor, thereby improving the performance of thedata processing system 102 and manage rendering of a graphical user interface provided via the display. - The
client device 104 can be operated by an end user that enters voice queries as audio input into the client device 104 (via themicrophone 146 or speaker 144) and receives audio (or other) output from thedata processing system 102 oragent services 106 to present, display, or render to the end user of theclient device 104. The digital component can include a computer-generated voice that can be provided from the data processing system to theclient device 104. Theclient device 104 can render the computer-generated voice to the end user via thespeaker 144. The computer-generated voice can include recordings from a real person or computer-generated language. Theclient device 104 can provide visual output via thedisplay 148 communicatively coupled to theclient device 104. - The
client device 104 can have or execute as at least one foreground process 150 (also referred herein as a foreground service). Theforeground process 150 can correspond to at least one of the applications (e.g., thedigital assistant application 108 or an agent application 110) that is rendered on thedisplay 148 and executing on theclient device 104. For example, theforeground process 150 can correspond to an application with a graphical user interface occupying a majority of thedisplay 148. The application running on theclient device 104 as theforeground process 150 can have a graphical user interface rendered on thedisplay 148 of theclient device 104. The graphical user interface of the application in theforeground process 150 can occupy at least a portion of thedisplay 148 of the client device 104 (e.g., at least a third the display 148). The application running on theclient device 104 as theforeground process 150 can wait for user interactions with the graphical user interface (e.g., via the I/O devices communicatively coupled with the client device 104) to carry out one or more routines. The application in theforeground process 150 can process the user interactions to carry out the one or more routines of the application. In the processing of the user interactions, the application in theforeground process 150 can generate an output and can render the resultant output on the graphical user interface of the application in theforeground process 150. - The
client device 104 can have or execute at least one non-foreground process 152 (also referred herein as a background process, a background service, non-foreground process, or a non-foreground service). Thenon-foreground process 152 can correspond to at least at least one of the applications (e.g., thedigital assistant application 108 or another agent application 110) that is not rendered on thedisplay 148 and executing on theclient device 104. The application running on theclient device 104 as thenon-foreground process 152 can lack any rendering of the graphical user interface of the application on thedisplay 148 of theclient device 104. For example, thenon-foreground process 152 can correspond to an application that is minimized and without any graphical user interface component rendered in thedisplay 148. The application running on theclient device 104 as thenon-foreground process 152 can execute one or more routines without receipt of any user interactions with the application. For example, an application running as thenon-foreground process 152 can process audio detected on themicrophone 146 of theclient device 104, without having any graphical user interface rendered on thedisplay 148. In the processing of the user interactions, the application in theforeground process 150 can generate an output without rendering the resultant output as a part of the graphical user rendered on thedisplay 148. - The
client device 104 can manage the applications in theforeground process 150 and the non-foreground process 152 (e.g., using an operating system executing on the client device 104). Theclient device 104 can switch applications between theforeground process 150 and thenon-foreground process 152. The switch of the applications between theforeground process 150 and thenon-foreground process 152 can be in response to an invocation of at least one application. The invoked application can be running in thenon-foreground process 152 or in neither theforeground process 150 and thenon-foreground process 152. The invocation can be by the operating system executing on theclient device 104, by another application (e.g., thedigital assistant application 108 or the agent application 110), or by a user interaction event. In response to the invocation, theclient device 104 can set an application running in theforeground process 150 to thenon-foreground process 152. In addition, theclient device 104 can set an application running in thenon-foreground process 152 to theforeground process 150. When the invoked application is not running as thenon-foreground process 152, theclient device 104 can also instantiate the application into theforeground process 150. For example, a web browser can be executing on theclient device 104 as theforeground process 150 with a graphical user interface rendered on thedisplay 148. A clock application can be running on theclient device 104 as thenon-foreground process 152 to track time, without the rendering of any graphical user interface rendered on thedisplay 148. Upon the invocation of an alarm function of the clock application, theclient device 104 can set the web browser as thenon-foreground process 152 thereby removing the rendering of the graphical user interface of the web browser. In conjunction, theclient device 104 can set the clock application as theforeground process 150. - The
data processing system 102 and theagent service 106 each can include at least one server having at least one processor. For example, thedata processing system 102 and theagent service 106 each can include a plurality of servers located in at least one data center or server farm. Thedata processing system 102 can include at least one computation resource or server. Thedata processing system 102 can include, interface, or otherwise communicate with at least onecommunications interface 112. Thedata processing system 102 can include, interface, or otherwise communicate with at least one instance of thedigital assistant application 108 on thedata processing system 102. The instance of thedigital assistant application 108 on thedata processing system 102 can include, interface, or otherwise communicate with: the at least oneNLP component 116, the at least one audiosignal generator component 118, at least oneaction handler component 120, the at least oneresponse selector component 122, the at least oneagent registry component 124, and the at least oneagent interface component 126, among others. Thedata processing system 102 can include, interface, or otherwise communicate with at least onedata repository 128. The at least onedata repository 128 can include or store, in one or more data structures or databases,regular expressions 130,data parameters 132,policies 134,response data 136,templates 138, and action-inventories 140. Thedata repository 128 can include one or more local or distributed databases, and can include a database management. - The instance of digital
assistant application 108 of thedata processing system 102 can execute or run an instance of theagent registry component 124 to maintain the set of action-inventories 140 on thedata repository 128. Theagent registry component 124 can maintain and store the set of action-inventories 140 for one ormore agent applications 110 on thedata repository 128. A subset of the action-inventories 140 can be defined by an administrator of thedata processing system 102 or thedigital assistant application 108 for an application type corresponding to theagent application 110. For example, the administrator of thedigital assistant application 108 can configure or define a built-in set of action-inventories 140 for purchase order applications. Another subset of action-inventories 140 can be provided by one of theagent services 106 handling resources for thecorresponding agent application 110. At least one of the action-inventories 140 can be a preset action-inventory 140 to be used to carry out an action, in response to determining that a request does not match any other action-inventory 140 of theagent application 110. For example, theagent service 106 can provide a set of customized action-inventories 140 to thedata processing system 102 for storage onto thedata repository 128. Theagent registry component 124 can receive the subset of action-inventories 140 provided by theagent service 106. Upon receipt, theagent registry component 124 can store the received action-inventories 140 onto thedata repository 128. The existence of the action-inventory 140 for the action of theagent application 110 on thedata repository 128 can indicate that theagent application 110 is capable of performing the action corresponding to the action-inventory 140. - The
agent registry component 124 can maintain and store a set ofuser interface components 142 for one ormore agent applications 110 on thedata repository 128. Eachuser interface component 142 can be associated with at least one of the action-inventories 140. A subset of theuser interface components 142 can be defined by an administrator of thedata processing system 102 or thedigital assistant application 108. For example, the administrator of thedigital assistant application 108 can configure or define a built-in set ofuser interface components 142. Another subset ofuser interface components 142 can be provided by one of theagent services 106 handling resources for thecorresponding agent application 110. Theagent service 106 can store and maintain theuser interface component 142 corresponding to the subset. For example, theagent service 106 can provide a set of customizeduser interface components 142 to thedata processing system 102 for storage onto thedata repository 128. Theagent registry component 124 can receive the subset ofuser interface components 142 provided by theagent service 106. Upon receipt, theagent registry component 124 can store the receiveduser interface components 142 onto thedata repository 128. The maintenance of theuser interface components 142 by theagent service 106 can be separate from the maintenance of theuser interface component 142 on thedata repository 128. - Each
user interface component 142 can have one or more interface elements. Each interface element can correspond to a subcomponent of theuser interface component 142. At least one interface element can correspond to a request to be identified from input audio signals detected by themicrophone 146 of theclient device 104. At least one interface element can correspond to a response indicating an output generated by theagent application 110 associated with theuser interface component 142. Furthermore, each user interface component can have one or more properties. The properties can include an element type (e.g., command button, scroll bar, textbox, and image), a size, a location within theuser interface component 142, transparency, and shape, among others. For example, auser interface component 142 can have a button located generally along the bottom, a textbox above the button, and a slot for an image generally in the middle, among others. Theuser interface component 142 itself can have one or more properties. The properties of theuser interface component 142 can include a size, a location within thedisplay 148, transparency, and shape, among others. - Each action-
inventory 140 can have at least one address template for at least one action by one of theagent applications 110. The address template may be for a single action of theagent application 110 or multiple actions to be carried by theagent application 110. The address template may be also for accessing or retrieving theuser interface component 142 to be generated in carrying out the action. The address template for the action can include a first portion and a second portion. The address template can be, for example, a Uniform Resource Identifier (URI) template in accordance to which an URI is to be generated. The first portion of the address template can correspond to one of theagent applications 110 or theagent service 106 providing resources for theagent application 110. For example, the first portion of the address template can include a scheme in a custom deep-link URI referencing theagent application 110. The first portion of the address template can also include a hostname referencing theagent service 106. - The second portion of the address template can correspond to the action that is to be performed by the
agent application 110. The second portion of the address template can also include one or more input variables (also referred herein as input arguments of fields) used to execute the action. For example, the second portion of the address template can include a pathname in a URI corresponding to the action to be taken. The second portion can also include one or more query strings following the pathname in the URI. Each query string can correspond to one of the input variables used to execute the action. A subset of the input variables can include optional or auxiliary input variables. The action can be executed by theagent application 110 without the auxiliary input variables in the address. - The action-
inventory 140 can include a mapping (sometimes referred herein as a parameter mapping) for the second portion of the address template. The mapping can specify or define a correspondence between at least one input variable of the second portion and one or more words to be identified from parsing input audio signals. The one or more words from parsing input audio signals can correspond to the input variables defining the action to be executed by theagent application 110. The mapping can define the insertion of the words into the input variables of the address template. For example, the mapping for a coffee purchase order action can specify insertion of a coffee name parsed from the audio signal to a coffee input variable of the address template. The action-inventory 140 can specify or include a set of permitted values (sometimes referred as an entity-inventory) for each input variable. The set of permitted values can be a part of the mapping for the action-inventory 140. For example, the permitted values for a coffee purchase order can include “latte,” “cafe latte,” and “latte coffee,” and the like for an input variable for the coffee name. The action-inventory 140 can also include a marker indicating a corresponding input variable of the address template as optional or auxiliary. The marker can be part of the mapping for the second portion of the address template. - In addition to the address template, each action-
inventory 140 maintained by theagent registry component 124 can include information related to the action for the action-inventory 140. The action-inventory 140 can include an interface mode indicator. The indicator can specify whether theuser interface component 142 of theagent application 110 is to be displayed in carrying out the action. Theuser interface component 142 can be a visual element to be rendered in thedisplay 148 of theclient device 104. Theuser interface component 142 can have a size less than a size of thedisplay 148. For example, theuser interface component 142 can include a box-sized visual element to occupy a 12.5% to 25% of thedisplay 148. Theuser interface component 142 can be associated with at least one of the action-inventories 140 maintained on thedata repository 128. - The
user interface component 142 can be associated with the action-inventory 140. The action-inventory 140 can include an address referencing theuser interface component 142. The address referencing theuser interface component 142 can differ or be separate from the address template for executing the action. The address referencing theuser interface component 142 can have the same scheme or hostname as the address template for the same action-inventory 140. For example, the address can include the hostname of theagent service 106 managing resources for theagent application 110. The address referencing theuser interface component 142 can have a different hostname as the address template of the same action-inventory 140. For example, the hostname can reference thedata repository 128 of thedata processing system 102. The action-inventory 140 can also include an user interface identifier corresponding to theuser interface component 142. The identifier may be used to index and reference theuser interface component 142 maintained on thedata repository 128. In addition, the address template of the action-inventory 140 can be used to reference theuser interface component 142 along with the action to be executed by theagent application 110. - The action-
inventory 140 can also include at least one agent identifier corresponding to one of theagent applications 110 to carry out the action. The agent identifier can include a set of alphanumeric characters referencing theagent application 110. The agent identifier can be used to identify theagent application 110 associated with the action-inventory 140 from words parsed from input audio signals. The action-inventory 140 can include at least one request identifier corresponding to the action to be carried by theagent application 110. The request identifier (sometimes referred herein as an “intent identifier” or “intent name”) can include a set of alphanumeric characters referencing the action to be carried out. The address template can be uniquely identified by one or more request identifiers. The request identifier can be used to identify the action-inventory 140 from words parsed from input audio signals. - Each action-
inventory 140 can be implemented or specified using a markup language, such as an Extensible Markup Language (XML) file. For example, the action-inventory 140 can be an XML file of the following form: -
<actions> <action intentName=“actions.intent.CHECK_STEPS” > <fulfillment fulfillmentMode=“actions.fulfillment.SLICE” urlTemplate=“https://ex_personalmetrics.example.com/ checksteps{?username}”> <parameter-mapping intentParameter=“user.name” urlParameter=“username” required=“true” /> </fulfillment> <fulfillment urlTemplate = “https://ex_personalmetrics.example.com/ checksteps” /> </action> </actions>
In the example above, the action-inventory 140 can be for an order ride to be executed by a personal fitness application. The entry “actions.intent.CHECK_STEPS” may correspond to the request identifier for the action-inventory 140. The entry “actions.fulfillment.SLICE” can specify the display of theuser interface component 142 associated with the action-inventory 140. The entry “https://ex_personalmetrics.example.com/checksteps{?username}” can correspond to the address template, in which “https://ex_personalmetrics.example.com/ corresponds to the first portion and “checksteps{?username}” corresponds to the second portion. The entry starting with “parameter-mapping” may correspond to the mapping. Lastly, the entry “https://ex_personal metrics.example.com/checksteps” can correspond to a separate address for theuser interface component 142. - The
agent registry component 124 can construct, create, or generate the set of action-inventories 140 for eachagent application 110. Theagent service 104 for theagent application 110 can provide a configuration file associated with theagent application 110 to theagent registry component 124 via thenetwork 114. The configuration file can be, for example, an application binary interface (ABI) for theagent application 110 submitted by an application developer associated with theagent application 110 to theagent registry component 124. Theagent service 104 for theagent application 110 can provide metadata associated with theagent application 110. The metadata can include the agent identifier and an application type to theagent registry component 124, among others. The application type can indicate a usage of theagent application 110. Theagent service 104 for theagent application 110 can provide the interface mode indicator for one or more actions capable of being executed by theagent application 110. Theagent registry component 124 can receive the information from theagent service 106 for the generation of the set of action-inventories 140 of theagent application 110. - The
agent registry component 124 can read, ingest, and parse the configuration file for theagent application 110 to identify one or more actions. For each identified function, theagent registry component 124 can generate an action-inventory 140 for the action. The action can be one of the actions of the set of action-inventories 140 defined by the administrator of thedata processing system 102 or thedigital assistant application 108 for the application type of theagent application 110. The agent registry component 1240 generate the address template for the action to be executed by theagent application 110. The first portion of the address template (e.g., the hostname or scheme) can correspond to theagent application 110. The second portion of the address template can be predefined for the action (e.g., using a preset pathname or a query string). Theagent registry component 124 can also generate a mapping for the action-inventory 140 for the identified function. Theagent registry component 124 can include or add a request identifier corresponding to the identified action into the action-inventory 140. The mapping can include the set of permitted values for the input variables of the address template. Theagent registry component 124 can include or add an agent identifier corresponding to theagent application 110 into the action-inventory 140. With the construction of the action-inventory 140, theagent registry component 124 can store the action-inventory 140 onto thedata repository 128. - The
data processing system 102 can include at least onecommunications interface 112. Thecommunications interface 112 can be configured, constructed, or operational to receive and transmit information using, for example, data packets. Thecommunications interface 112 can receive and transmit information using one or more protocols, such as a network protocol. Thecommunications interface 112 can include a hardware interface, software interface, wired interface, or wireless interface. Thecommunications interface 112 can be a data interface or a network interface that enables the components of thesystem 100 to communicate with one another. The communications interface 112 of thedata processing system 102 can provide or transmit one or more data packets that include the action data structure, audio signals, or other data via thenetwork 114 to theclient devices 104 or theagent service 106. For example, thedata processing system 102 can provide the output signal from thedata repository 128 or from the audiosignal generator component 118 to theclient devices 104. - The
data processing system 102 can also instruct, via data packet transmissions, theclient devices 104 to perform the functions indicated in the action data structure. The output signal can be obtained, generated, transformed to, or transmitted as one or more data packets (or other communications protocol) from the data processing system 102 (or other computing device) to theclient devices 104. Thecommunications interface 112 can facilitate translating or formatting data from one format to another format. For example, thecommunications interface 112 can include an application programming interface (“API”) that includes definitions for communicating between various components, such as software components. An application, script, program, or other components that are associated with thedata processing system 102 can be installed at theclient devices 104. The application can enabled theclient devices 104 to communicate input audio signals (and other data) to thecommunications interface 112 of thedata processing system 102. - The
data processing system 102 can include an application, script, or program installed at theclient device 104, such as the instance of thedigital assistant application 108 on theclient device 104 to communicate input audio signals to thecommunications interface 112 of thedata processing system 102 and to drive components of the client computing device to render output audio signals or visual output. Thedata processing system 102 can receive data packets, a digital file, or other signals that include or identify an input audio signal (or input audio signals). Theclient device 104 can detect the audio signal via thespeaker 144 and convert the analog audio signal to a digital file via an analog-to-digital converter. For example, the audio driver can include an analog-to-digital converter component. The pre-processor component can convert the audio signals to a digital file that can be transmitted via data packets overnetwork 114. - The instance of the
digital assistant application 108 of thedata processing system 102 or theclient device 104 can execute or run anNLP component 116 to receive or obtain the data packets including the input audio signal detected by themicrophone 146 of theclient device 104. The data packets can provide a digital file. TheNLP component 116 can receive or obtain the digital file or data packets comprising the audio signal and parse the audio signal. For example, theNLP component 116 can provide for interactions between a human and a computer. TheNLP component 116 can be configured with techniques for understanding natural language and enabling thedata processing system 102 to derive meaning from human or natural language input. TheNLP component 116 can include or be configured with techniques based on machine learning, such as statistical machine learning. TheNLP component 116 can utilize decision trees, statistical models, or probabilistic models to parse the input audio signal. - The
NLP component 116 can perform, for example, functions such as named entity recognition (e.g., given a stream of text, determine which items in the text map to names, such as people or places, and what the type of each such name is, such as person, location (e.g., “home”), or organization), natural language generation (e.g., convert information from computer databases or semantic intents into understandable human language), natural language understanding (e.g., convert text into more formal representations such as first-order logic structures that a computer module can manipulate), machine translation (e.g., automatically translate text from one human language to another), morphological segmentation (e.g., separating words into individual morphemes and identify the class of the morphemes, which can be challenging based on the complexity of the morphology or structure of the words of the language being considered), question answering (e.g., determining an answer to a human-language question, which can be specific or open-ended), or semantic processing (e.g., processing that can occur after identifying a word and encoding its meaning in order to relate the identified word to other words with similar meanings). - The NLP component 116 (and the
digital assistant application 108 as a whole) can execute as one of the applications in thenon-foreground process 152. When the input audio signal is detected on themicrophone 146, theclient device 104 can continue to thedigital assistant application 108 as one of the applications in thenon-foreground process 152. Theclient device 104 can have one or more other applications (e.g., an agent application 110) running as the non-foreground processes 152. Furthermore, theclient device 104 can have at least one application (e.g., another agent application 110) running in theforeground process 150. The instance of theNLP component 116 of thedigital assistant application 108 on theclient device 104 can initiate processing of the input audio signal without switching of theforeground process 150 or thenon-foreground process 152. The applications running as theforeground process 150 and thenon-foreground process 152 can be maintained. For example, theNLP component 116 on theclient device 104 can receive and apply the natural language processing functions on the input audio signal, without minimizing a graphical user interface of an application running as theforeground process 150. Alternatively, the NLP component 116 (and thedigital assistant application 108 as a whole) can execute as one of the applications in theforeground process 150, when the input audio signal is received. Subsequent to the receipt of the input audio signal, thedigital assistant application 108 can continue to execute as one of the applications in theforeground process 150 of theclient device 104. - The
NLP component 116 can parse and convert the input audio signal into recognized string by comparing the input signal against a stored, representative set of audio waveforms (e.g., in the data repository 128) and choosing the closest matches. TheNLP component 116 can also partition or divide the input audio signal into one or more audio segments of a time duration (e.g., 15 seconds to 2 minutes) to process each segment. The set of audio waveforms can be stored indata repository 128 or other database accessible to thedata processing system 102. The representative waveforms are generated across a large set of users, and then may be augmented with speech samples from the user. After the audio signal is converted into recognized text, theNLP component 116 matches the text to words that are associated, for example via training across users or through manual specification, with actions that thedata processing system 102 can serve. TheNLP component 116 can determine that the input audio signal acquired from themicrophone 146 does not contain any recognizable strings. TheNLP component 116 can determine that the input audio signal contains silence (e.g., with a maximum amplitude of less than 0 dB) in determining that the input audio signal does not contain any recognizable strings. Additionally, theNLP component 116 can determine a signal-to-noise (SNR) of the input audio signal. TheNLP component 116 can compare the SNR of the input audio signal to a threshold SNR (e.g., −20 dB). Responsive to the determination the SNR of the input audio signal is greater than the threshold SNR, theNLP component 116 can determine that the input audio signal does not contain any recognizable strings. - The
data processing system 102 can receive image or video input signals, in addition to, or instead of, input audio signals. TheNLP component 116 can convert image or video input to text or digital files. TheNLP component 116 can process, analyze, or interpret image or video input to perform actions, generate requests, or select or identify data structures. Thedata processing system 102 can process the image or video input signals using, for example, image interpretation techniques, computer vision, a machine-learning engine, or other techniques to recognize or interpret the image or video to convert the image or video to a digital file. The one or more image interpretation techniques, computer vision techniques, or machine learning techniques can be collectively referred to as imaging techniques. The data processing system 102 (e.g., the NLP component 116) can be configured with the imaging techniques, in addition to, or instead of, audio processing techniques. - From parsing the input audio signal, the
NLP component 116 can determine or identify at least one request. The input audio signal can include, for example, a query, question, command, instructions, or other statement in a natural language. The request can correspond to at least one trigger keyword identified from the recognized string converted from the input audio signal. The request can indicate an action to be taken. For example, theNLP component 116 can parse the input audio signal to identify at least one request to leave home for the evening to attend dinner and a movie. The trigger keyword can include at least one word, phrase, root or partial word, or derivative indicating an action to be taken. For example, the trigger keyword “go” or “to go to” from the input audio signal can indicate a request for transport. In this example, the input audio signal (or the identified request) does not directly express an intent for transport, however the trigger keyword indicates that transport is an ancillary action to at least one other action that is indicated by the request. - To identify the request, the
NLP component 116 can apply a semantic processing technique to the input audio signal to identify the trigger keyword corresponding to the request. TheNLP component 116 can apply a semantic processing technique to the input audio signal to identify a trigger phrase that includes one or more trigger keywords, such as a first trigger keyword and a second trigger keyword. For example, the input audio signal can include the sentence “Find the nearest café.” TheNLP component 116 can determine that the input audio signal includes a trigger keyword “find.” TheNLP component 116 can determine that the request is to search for a location near theclient device 104. - In addition to the request, the
NLP component 116 can identify at least one parameter defining the request. The parameter can define the request, functioning as a supplement or a constraint on the action corresponding to the request to be taken. The parameter can further specify the action to be-taken. The parameter can include a subset of the recognized strings (excluding the trigger keywords) converted from the input audio signal. TheNLP component 116 can apply a semantic processing technique to the input audio signal to identify one or more descriptor words related to the identified trigger keyword. From the example “Find the nearest café,” theNLP component 116 may have identified the term “Find” as the trigger keyword. In conjunction, theNLP component 116 can identify “nearest” as a first parameter and “café” as a second parameter defining the request “find.” - Furthermore, the
NLP component 116 can identify an application identifier from the strings recognized from the input audio signal. The application identifier can correspond to one of theagent applications 110. To identify the application identifier, theNLP component 116 can perform named entity recognition algorithm to the strings recognized from the input audio signal. In using the named entity recognition algorithm, theNLP component 116 can maintain a list of agent identifiers foragent applications 110. The list can include agent identifiers ofagent applications 110 installed on theclient device 104 that received the input audio signal including the request. By applying the named-entity recognition algorithm, theNLP component 116 can determine that the input audio signal includes the application identifier corresponding to anagent application 110. For example, theNLP component 116 can determine that the input audio signal “Order car with ride sharing service XYZ” includes with anexplicit agent application 110 “ride sharing service XYZ.” Conversely, by applying the named entity recognition algorithm, theNLP component 116 can determine that the input audio lacks any application identifier corresponding to any of theagent applications 110. - Based on the request, the
NLP component 116 can determine whether the request corresponds to a function of thedigital assistant application 108 or a function of theagent application 110. To determine, theNLP component 116 can access a list of requests for functions of thedigital assistant application 108. The list of requests for the functions of thedigital assistant application 108 can be maintained on thedata repository 128. The list of requests can include sets of strings for trigger keywords and requests corresponding to the requests predetermined to be associated with functions of thedigital assistant application 108. For example, the list can include “take”, “show”, “search for,” and “find,” among others. With the identification of the request, theNLP component 116 can compare the request with the list of requests for functions of thedigital assistant application 108. - The
NLP component 116 can also use theregular expressions 130 maintained on thedata repository 128 to determine whether the request corresponds to the function of thedigital assistant application 108. Theregular expression 130 can define a pattern to match to determine whether the keywords identified from the input audio signal references the at least one function of thedigital assistant application 108. Theregular expression 130 can also specify which keywords to use to carry out the command indicated in the input audio signal. For example, theregular expression 130 may be of the form {[request], [auxiliary arguments]}. For the keywords of the input audio signal to be determined to reference the functions of thedigital assistant application 108, theregular expression 130 can specify that the one or more keywords include a request for thedigital assistant application 108 and auxiliary arguments. Theregular expression 130 can specify a sequence for the request and the referential keywords in the one or more keywords identified from the input audio signal. - Based on the comparison, the
NLP component 116 can determine that the request identified from the input audio signal matches one of the list of requests for functions of thedigital assistant application 108. In response to the determination, theNLP component 116 can determine that the request corresponds to one of the functions of thedigital assistant application 108. For example, theNLP component 116 can parse the words “What is the weather?” from the input audio signal, and can identify the input audio signal includes a request for weather. The list of requests for the functions of thedigital assistant application 108 can specify that the request for weather is one of the functions of thedigital assistant application 108. In this example, theNLP component 116 can determine the match between the two requests, and can determine that the request references one of the functions of thedigital assistant application 108. - In response to the determination that the request corresponds to the function of the
digital assistant application 108, thedigital assistant application 108 can execute and fulfill the request identified from the parsing of the input audio signal. In addition, thedigital assistant application 108 can execute the request in accordance with the parameter defining the request. For example, the request may be to search for a definition of a word (e.g., “abnegation”). Thedigital assistant application 108 can perform an internet search to retrieve the definition of the word. In fulfilling the request, thedigital assistant application 108 can invoke theresponse selector component 122. Theresponse selector component 122 can select or identify responses phrases using thepolicies 134 or theresponse data 136 maintained on thedata repository 128. Thepolicies 134 can be particular to a request, and can specify theresponse data 136 for the request. Theresponse selector component 122 can search thepolicies 134 for generating the output using the request type of the response in fulfilling the request. Once thepolicy 134 is identified, theresponse selector component 122 can select the response phrase. The response phrase can include a set of strings, such as words or phrases. Thedigital assistant application 108 can display the response phrase on thedisplay 148 of theclient device 104. For example, thedigital assistant application 108 can display a content item including the response phrase on a graphical user interface of thedigital assistant application 108 on thedisplay 148 of theclient device 104. - With the identification of the response phrase, the
digital assistant application 108 can invoke and the audiosignal generator component 118. The instance of thedigital assistant application 108 running on thedata processing system 102 or theclient device 104 can execute the audiosignal generator component 118. Using the selected response phrase, the audiosignal generator component 118 can generate an output audio signal. Based on one or more words included in the response phrase, the audiosignal generator component 118 can generate the output audio signal. The audiosignal generator component 118 can play the output audio signal on thespeaker 144 of theclient device 104. - Conversely, from the comparison, the
NLP component 116 can determine that the request parsed from the input audio signal does not match any of the list of requests for function of thedigital assistant application 108. In response to the determination, theNLP component 116 can determine whether the request corresponds to one of the functions of theagent application 110. To determine, theNLP component 116 can access a list of requests for functions of theagent applications 110. The list of requests for functions can be foragent applications 110 installed on theclient device 104. The list of requests for the functions of theagent applications 110 can be maintained on thedata repository 128. The list of requests can include sets of strings for trigger keywords or requests corresponding to the requests predetermined to be associated with functions of theagent application 110. TheNLP component 116 can compare the request with the list of requests for functions of theagent application 110. - From the comparison, the
NLP component 116 can determine that the request identified from the input audio signal matches one of the list of requests for functions of theagent application 110. In response to the determination, theNLP component 116 can determine that the request corresponds to one of the functions of theagent application 110. For example, theNLP component 116 can parse the words “Buy me laundry detergent” from the input audio signal, and can identify the input audio signal includes a purchase for detergent. The list of requests for the functions of theagent application 110 can specify that the request for weather is one of the functions of theagent application 110. In this example, theNLP component 116 can determine the match between the two requests, and can determine that the request references one of the functions of theagent application 110, namely the one for such purchases. In addition, theNLP component 116 can determine that the request does not correspond to a function of thedigital assistant application 108, in response to determining that the input audio signal includes the application identifier of theagent application 110. - On the other hand, the
NLP component 116 can determine that the request does not correspond to any function of any of theagent applications 110 from the comparison. With the determination that the request does not correspond to thedigital assistant application 108 or theagent application 110, theresponse selector component 122 can generate or identify a response phrase indicating that the request does not correspond to any functionality available on the client device 014. For example, the response phrase can include “Sorry, I don't understand your request.” The response phrase can be selected from theresponse data 136 based on the determination. In addition, the audiosignal generator component 118 can generate a corresponding output audio signal using the one or more words of the response phrase. The audiosignal generator component 118 can play the output audio signal on thespeaker 144 of theclient device 104. - The instance of the
digital assistant application 108 running on thedata processing system 102 or theclient device 104 can execute theaction handler component 120 to generate action data structures based on the determination. Theaction handler component 120 can execute scripts or programs based on input received from theNLP component 116. Theagent service 106 can provide the scripts or programs. Theagent service 106 can make the scripts or programs available to thedata processing system 102 through an API. Theaction handler component 120 can determine parameters or responses to input fields and can package the data into an action data structure. The action data structure can be provided to theagent application 110 through the API. Theaction handler component 120 can transmit the action data structure to theagent service 106 for fulfillment of the request identified from the input audio signal parsed by theNLP component 116. - In response to the determination that the request corresponds to the function of the
agent application 110, theaction handler component 120 can identify or select an action-inventory 140 from thedata repository 128 for the action corresponding to the request. At least one action-inventory 140 can be selected by theaction handler component 120 from the set of action-inventories 140 maintained on thedata repository 128. Theaction handler component 120 can use the request identified from the input audio signal to search thedata repository 128 for the action-inventory 140. Theaction handler component 120 can also use the one or more parameters identified from the input audio signal to search thedata repository 128 for the action-inventory 140. Theaction handler component 120 can traverse through the set of action-inventories 140 maintained on thedata repository 128. For each action-inventory 140, theaction handler component 120 can identify the request identifier of the action-inventory 140. - The
action handler component 120 can compare the request identified from the input audio signal with the request identifier of the action-inventory 140. The comparison of the request identifier from the input audio signal and the request identifier of the action-inventory 140 can be in accordance with a semantic knowledge graph. Theaction handler component 120 can invoke theNLP component 116 to determine whether the request is semantically related with the request identifier using the semantic knowledge graph. The semantic knowledge graph can include a set of nodes and edges. Each node can correspond to one or more words or a phrase. Each edge can define a semantic distance between a pair of words indicated by the corresponding words on the nodes. From the semantic knowledge graph, theNLP component 116 can identify the node corresponding to the request, the node corresponding to the request identifier, and the semantic distance indicated in the pair of nodes. TheNLP component 116 can compare the semantic distance to a threshold distance. When the semantic distance is determined to not satisfy the threshold distance (e.g., greater than), theNLP component 116 can determine that the request does not match the request identifier of the action-inventory 140. On the other hand, when the semantic distance is determined to satisfy the threshold distance (e.g., less than or equal to), theNLP component 116 can determine that the request does match the request identifier of the action-inventory 140. From the comparison, theaction handler component 120 can determine that the request does not match the request identifier of the action-inventory 140. In response to the determination, theaction handler component 120 can identify the next action-inventory 140 in the set maintained on thedata repository 128 and can repeat the comparison. - Conversely, from the comparison, the
action handler component 120 can determine that the request matches the request identifier of the action-inventory 140. In response to the determination, theaction handler component 120 can select the action-inventory 140 for the action corresponding to the request. As multiple requests may correspond to the same action-inventory 140 as discussed above, theaction handler component 120 can select the action-inventory 140 that was or is to be selected for another request. Theaction handler component 120 can also compare the one or more parameters identified from the input audio signal with the input variables of the action-inventory 140, prior to selection of the action-inventory 140. Theaction handler component 120 can identify the input variables from the parameter mapping or the address template of the action-inventory 140. Theaction handler component 120 can exclude the auxiliary parameters as indicated in the action-inventory 140 in comparing the identified parameters with the input variables of the action-inventory 140. Theaction handler component 120 can determine that the identified parameters match the input variables of the action-inventory 140 in accordance to the set of permitted values for each input variable. In response to the determination, theaction handler component 120 can select the action-inventory 140 for the action corresponding to the request defined by the one or more parameters. Theaction handler component 120 can also halt the traversal of the set of action-inventories 140 in thedata repository 128. Otherwise, theaction handler component 120 can determine that the identified parameters do not match the input variables of the action-inventory 140. Theaction handler component 120 can identify another action-inventory 140 for the action corresponding to the same request, and can repeat the comparison. Upon comparing with all the action-inventories 140, theaction handler component 120 can determine that the identified parameters do not match the input variables in any of the action-inventories 140 of theagent application 110. In response to the determination, theaction handler component 120 can select the preset action-inventory 140 for theagent application 110 to respond to the request. - The
action handler component 120 can also use the agent identifier identified from the input audio signal to search thedata repository 128 for the action-inventory 140. Prior to searching using the request, theaction handler component 120 can identify a subset of the action-inventories 140 with the application identifier matching the agent identifier. The agent identifier can correspond to one of theagent applications 110. Each action-inventory 140 of the identify subset can include the application identifier that is determined to match the agent identifier identified from the input audio signal. To identify the subset, theaction handler component 120 can compare the agent identifier with the application identifier of each action-inventory 140. When the agent identifier is determined to match the application identifier of the action-inventory 140, theaction handler component 120 can include the action-inventory 140 into the subset. Otherwise, when the agent identifier is determined to not match to the application identifier of the action-inventory 140, theaction handler component 120 can exclude the action-inventory 140 from the subset. Theaction handler component 120 can traverse through the identified subset of action-inventories 140 for the application identifier using the request to select the action-inventory 140 for the action corresponding to the request. - When the input audio signal is determined to lack an agent identifier, the
action handler component 120 can select the action-inventory 140 from the set maintained on thedata repository 128 based on the request, the one or more parameters, and an agent usage history. The agent usage history can indicate usage statistics of theagent application 110 on theclient device 104 running thedigital assistant application 108 from which the input audio signal is received. The agent usage history can also indicate functions of theagent application 110 invoked on theclient device 104. The agent usage history can be limited to a defined time window prior to the request identified from the input audio signal detected on theclient device 104. Theaction handler component 120 can identify a subset of action-inventories 140 with request identifiers matching the request and the one or more parameters. The subset can include the action-inventories 140 with different application identifiers corresponding tomultiple agent applications 110. Theaction handler component 120 can use the agent usage history for theclient device 104 to select the action-inventory 140 from the subset. Theaction handler component 120 can identify at least one agent identifier from the agent usage history. Theaction handler component 120 can compare the agent identifier with application identifier of each action-inventory 140 in the identified subset. Theaction handler component 120 can determine that application identifier of the action-inventory 140 matches the agent identifier from the agent usage history from the comparison. In response to the determination, theaction handler component 120 can select the action-inventory 140 for the action to be executed by theagent application 110 in accordance with the parameter. Conversely, theaction handler component 120 can determine that the application identifier of the action-inventory 140 does not match the agent identifier from the agent usage history from the comparison. In response to the determination, theaction handler component 120 can identify another action-inventory 140 and can repeat the comparison using the agent identifier. - With the identification of the action-
inventory 140, theaction handler component 120 can validate the one or more parameters identified from the input audio signal against the address template of the action-inventory 140. The validation by theaction handler component 120 may be to check whether there are prerequisite parameters as specified by the action-inventory 140 to carry out the action corresponding to the request. For example, for a ridesharing request, theaction handler component 120 can check whether there is a parameter corresponding to the destination as parsed from the input audio signal. Theaction handler component 120 can use the parameter mapping for the address template of the action-inventory 140 in validating the parameters. Theaction handler component 120 can identify the correspondences between parameters and input variables defined in the mapping for the action-inventory 140. Theaction handler component 120 can identify a subset of correspondences for input variables not specified as auxiliary by the mapping of the action-inventory 140 for the address template. - From each input variable not indicated as auxiliary, the
action handler component 120 can determine whether at least one of the parameters corresponds to the input variable as specified by the mapping. The determination of the correspondence can be performed using semantic analysis algorithms as provided by the NLP component 116 (e.g., semantic knowledge graph). For each input variable, theaction handler component 120 can invoke theNLP component 116 to determine whether the input variable corresponds to any of the parameters, or vice-versa. In response to determination that all the input variables corresponds to at least one parameter, theaction handler component 120 can determine that the parameters are successfully validated. Theaction handler component 120 can also continue to use the address template of the action-inventory 140 to generate an address to fulfill the action corresponding to the request. - On the other hand, in response to the determination that at least one input variable (not indicated as auxiliary) does not correspond to any parameter, the
action handler component 120 can determine that the parameters are not successfully validated. Theaction handler component 120 can cease or terminate further processing of the request and the parameters (e.g., generation of an address using the address template of the action-inventory 140). Theaction handler component 120 can detect or determine an error in validating the one or more parameters identified from the input audio signal. Thedigital assistant application 108 can present an indication (e.g., using a visual content item or audio) that the request is invalid or unsuccessful. Theaction handler component 120 can provide an indication of the error to theagent service 106 or an administrator for theagent application 110 associated with the action-inventory 140. - Upon identifying the action-
inventory 140, theaction handler component 120 can determine whether theclient device 104 is authenticated with theagent application 110 associated with the action-inventory 140. Theaction handler component 120 can identify theagent application 110 associated with the action-inventory 140. Once identified, theaction handler component 120 can check an authentication status of theclient device 104 with theagent application 110. The checking of the authentication status can be performed using an account identifier of theclient device 104 or thedigital assistant application 108. Theaction handler component 120 can send a request for the authentication status to theagent service 106 managing resources of theagent application 110. The request can include the account identifier of theclient device 104 or of thedigital assistant application 108. Theagent service 106 can provide a response to theaction handler component 120 indicating whether theclient device 104 is authenticated with theagent application 110. When the response indicates that theclient device 104 is authenticated, theaction handler component 120 can determine that theclient device 104 is authenticated with theagent application 110. Theaction handler component 120 can also continue processing the request. On the other hand, when the response indicates that theclient device 104 is not authenticated, theaction handler component 120 can determine that theclient device 104 is not authenticated with theagent application 110. Theaction handler component 120 can cease or terminate further processing of the request (e.g., generation of an address using the address template of the action-inventory 140). Thedigital assistant application 108 can also present an indication to authenticate with theagent application 110 to carry out the action corresponding to the request. For example, thedigital assistant application 108 can render a prompt for logging into theagent application 110 on thedisplay 148 of theclient device 104. The prompt can include a field for an account identifier of the user and the passcode. - In addition, the
action handler component 120 can also determine whether theagent application 110 associated with the action-inventory 140 is installed on theclient device 104. Theaction handler component 120 can identify theagent application 110 associated with the action-inventory 140. Once identified, theaction handler component 120 can access theclient device 104 to determine whether theagent application 110 is installed on theclient device 104. For example, theaction handler component 120 can access a list of installed applications on theclient device 104 to determine whether theagent application 110 is installed. Theaction handler component 120 can check the application identifier or the agent identifier to the list. In response to the determination that theagent application 110 is installed, theaction handler component 120 can continue to process the request. On the other hand, in response to the determination that theagent application 110 is not installed, theaction handler component 120 can terminate processing of the request (e.g., generation of an address using the address template of the action-inventory 140). Thedigital assistant application 108 can also present an indication to install theagent application 110 to carry out the action corresponding to the request. For example, thedigital assistant application 108 can render a prompt indicating that thecorresponding agent application 110 is not installed on thedisplay 148 of theclient device 104. The prompt can also include a link to install theagent application 110. - The action-
inventory 140 selected by theaction handler component 120 can correspond to theagent application 110 not running as one of applications in theforeground process 150 or thenon-foreground process 152. For example, the action-inventory 140 can correspond to theagent application 110 installed on theclient device 104, but not running on theclient device 104. The action-inventory 140 selected by theaction handler component 120 can correspond to theagent application 110 running in thenon-foreground process 152. For example, another application (e.g., anotheragent application 110 or the digital assistant application 108) can be running in an application of theforeground process 150. The application in theforeground process 150 can have a graphical user interface rendered on at least a portion of thedisplay 148 of theclient device 104. In contrast, the application running in thenon-foreground process 152 can lack a graphical user interface rendered on thedisplay 148 of theclient device 104, and can execute routines (e.g., the action). In addition, the action-inventory 140 selected by theaction handler component 120 can correspond to theagent application 110 running in theforeground process 150. For example, theagent application 110 corresponding to the action-inventory 140 can have a graphical user interface currently rendered on thedisplay 148 of theclient device 104. - The
action handler component 120 can generate an address in accordance with the address template of the action-inventory 140. Theaction handler component 120 can identify the address template of the action-inventory 140 selected from the set maintained on thedata repository 128. As described above, the address template of the action-inventory 140 can include: a first portion referencing theagent application 110; and a second portion referencing the action to be performed and the one or more input variable used to execute the action. For example, the address template can be a URI template. The first portion can include a scheme referencing theagent application 110 or a hostname referencing theagent service 106 or theagent application 110. The second portion can include a pathname referencing the action and one or more query strings corresponding to one or more input variable sin the URI. For example, the address template of the action-inventory 140 can be “https://ex_personalmetrics.example.com/checksteps{?username}” for a request to check a number of steps taken. The first portion can correspond to “https://ex_personalmetrics.example.com/” and the second portion can correspond to “checksteps{?username}.” - The address generated in accordance with the address template can include a first substring and a second substring. The first substring of the generated address can correspond to the first portion of the address template. The generated address can be a portion of the URI, and the first substring can include the scheme referencing the
agent application 110 or the hostname referencing theagent service 106 or theagent application 110. The scheme and the hostname can be taken by theaction handler component 120 from the first portion of the address template. The second substring of the generated address can correspond to the second portion of the second template. The generated address can also be a portion of the URI, and the second substring can include the pathname referencing the action to be executed by theagent application 110 and query strings including the parameters identified from the input audio signal. For example, the address generated in accordance with the address template can be “https://ex_personalmetrics.example.com/checksteps?username=example_user_id.” The first substring of the address can include “https://ex_personalmetrics.example.com” and the second substring of the address can include “checksteps?username=example_user_id.” - In generating the address, the
action handler component 120 can use or set the first portion of the address template as a first substring of the address to reference theagent service 106 or theagent application 110 associated with theagent service 106. Theaction handler component 120 can use the second portion of the address template as a second substring of the address to correspond to the action and the parameters used to execute the action. Theaction handler component 120 can populate the input variables of the second portion of the address template with the one or more parameters identified from the input audio signal. - The
action handler component 120 can populate in accordance with the parameter mapping of the action-inventory 140 defined for the address template. As discussed above, the parameter mapping can define a correspondence between the input variables of the second portion and words to be identified from parsing input audio signals. Theaction handler component 120 can use the parameters as the words identified from the input audio signal to insert into the second portion of the address template in populating the input variables. The population or of the second portion of the address template can be in accordance with expression expansion in URI templates. For each input variable specified in the address template, theaction handler component 120 can identify the corresponding parameters as specified by the mapping of the action-inventory 140. With the identification, theaction handler component 120 can insert, replace, or set the query string with a name of the input variable and the identified parameter. For example, the input variable may be “{?username}” in the address template and the parameter identified may be “example_user_id”. In this example, theaction handler component 120 can replace with “?username=example_user_id” in the second substring of the address in accordance with expression expansion for URI templates. - In populating the input variables of the address template, the
action handler component 120 can determine whether the input variable is an auxiliary as specified in the mapping for the address template of the action-inventory 140. In response to the determination that the input variable is auxiliary, theaction handler component 120 can also determine whether there is an identified parameter that corresponds to the auxiliary input variable. When there is determined to be no parameters that correspond to the auxiliary input variable, theaction handler component 120 can omit or remove the in auxiliary input variable from the address. The generated address can thus lack one or more parameters corresponding to auxiliary input variables in the address template. Otherwise, when there is determined to be a parameter that corresponds to the auxiliary input variable, theaction handler component 120 can insert or populate the auxiliary input variable with the parameter. Conversely, in response to the determination that the input variable is not auxiliary, theaction handler component 120 can identify a parameter from the identified parameters to include into the input variable. - With the generation of the address in accordance with the address template, the
action handler component 120 can validate the address. The validation of the address by theaction handler component 120 may be to check whether the address is well-formed. To validate, theaction handler component 120 can check the address against aregular expression 130 for addresses (e.g., URIs or URLs). Theregular expression 130 for addresses can include, for example, schema, hostnames, pathnames, and query strings separated by delimiters for valid addresses. Theaction handler component 120 can compare the address against theregular expression 130 to determine whether the address matches the pattern for addresses as specified by theregular expression 130. In response to the determination that the address matches, theaction handler component 120 can determine that the generated address is successfully validated. - Conversely, in response to the determination that the address does not match, the
action handler component 120 can determine that the generated address is not successfully validated. Based on the determination, theaction handler component 120 can also detect an error in validating the generated address. The error can indicate that the address is not well-formed. Furthermore, theaction handler component 120 can cease or terminate further processing of the address (e.g., generating an action data structure). Theaction handler component 120 can provide an indication of the error to theagent service 106 or an administrator for theagent application 110 associated with the action-inventory 140. - The
action handler component 120 can package or generate an action data structure using the address for executing the action. The generation of the action data structure can be in accordance with an application programming interface (API). For example, the API can be specified by an operating system running on theclient device 104, and can be used by both thedigital assistant application 108 and theagent application 110. The action data structure can be passed between thedigital assistant application 108 and theagent application 110 via the API. The generation of the action data structure can be in accordance with the Hypertext Transfer Protocol (HTTP). For example, the action data structure generated by theaction handler component 120 can be an HTTP request (e.g., GET or POST functions). The action data structure can include at least one header and at least one body. The header can include a device modality (e.g., a smartphone, smart speaker, tablet, laptop, and desktop) of theclient device 104 executing thedigital assistant application 108 and theagent application 110. The header can include a source address set to theclient device 104. The header can include the generated address set a destination address. The body of the action data structure can include other information related to the request identified from parsing the input audio signal. For example, the body can include an identifier referencing thedigital assistant application 108 or theclient device 104 running thedigital assistant application 108. - The instance of the
digital assistant application 108 on thedata processing system 102 or theclient device 104 can execute theagent interface component 126 to provide the action data structure to theagent application 110. With the generation of the action data structure, theagent interface component 126 can provide or direct the action data to theagent application 110. Theagent interface component 126 can transmit the action data structure over thenetwork 114 to theagent service 106 managing resources for the agent application 110 (e.g., as an HTTP request). For example, an HTTP request corresponding to the action data structure can be transmitted by theagent interface component 126 over thenetwork 114. Theagent interface component 126 can invoke theagent application 110 using the action data structure (e.g., via the API). For example, theagent interface component 126 can make a function call to invoke theagent application 110 in accordance to an API and facilitated by the operating system of theclient device 104. In invoking, theagent interface component 126 can pass the action data structure to theagent application 110. In invoking, theagent interface component 126 can pass the action data structure to theagent application 110. - The
data processing system 102, theclient device 104, or theagent service 106 can execute an instance of theagent application 110. As the destination address of the data structure can reference theagent application 110 or theagent service 106, theagent application 110 can receive the action data structure from theagent interface component 126. For example, the action data structure may have been sent via thenetwork 114, and can the destination address can include the hostname referencing theagent application 110 or theagent service 106. Since the destination address of the action data structure references theagent application 110 or theagent service 106, the action data structure can be routed to theagent application 110 or theagent service 106 by thenetwork 114. In another example, the destination address of the action data structure can be a deep link with a scheme referencing theagent application 110. By invoking the destination address, the action data structure can be directed by theclient device 104 or thenetwork 114 to theagent application 110 referenced by the scheme. - Upon passing of the action data structure, the
agent application 110 can parse the action data structure to identify the address. The identification of the address can be from the destination address header of the action data structure. Theagent application 110 can parse the address to identify the first substring of the address referencing theagent application 110 itself or theagent service 106 managing resources for theagent application 110. Theagent application 110 can parse the address the second substring of the address. Theagent application 110 can further parser the second substring of the address to identify the action and the one or more parameters used to perform the action. Theagent application 110 can parse the pathname of the URI to identify the action and parse the one or more query strings to identify the one or more parameters used to execute the action. For example, theagent application 110 can parse the address “https://ex_personalmetrics.example.com/checksteps?username=example_user_id” to identify the second substring “checksteps?username=example_user_id.” Theagent application 110 can further identify “checksteps” as a request to get a number of steps measured and the “Example_user_id” as the name of the user whose steps were measured. With the identification, theagent application 110 can perform, carry out, or execute the action in accordance with the one or more parameters. Using the previous example, theagent application 110 can perform the series of processes and routines to retrieve the number of steps taken by the user of theagent application 110 with the account identifier “example_user_id.” Theagent application 110 can provide the output of the execution of the action to theagent interface component 126. - The
agent application 110 can also generate an output to indicate whether the execution of the action is successful or a failure to send to thedigital assistant application 108. In executing the agent, theagent application 110 can fail in completing the action or terminate short of completion. For example, while processing the request for number of steps, theagent service 106 can determine that the information on the number of steps taken by the corresponding user cannot be retrieved. Based on the determination, theagent application 110 can create the output indicating the failure. In response to the failure, theagent application 110 can generate the output indicating failure. Conversely, theagent application 110 can complete the execution of the action. In response to the completion, theagent application 110 can generate the output indicating success. Theagent application 110 can provide the output of the execution of the action to theagent interface component 126. - In conjunction with the passing of the action data structure, the
agent interface component 126 can control whether theagent application 110 is to be run as thenon-foreground process 152 or theforeground process 150. The setting or controlling of theforeground process 150 or thenon-foreground process 152 can be based on the interface mode indicator of the action-inventory 140 used to generate the action data structure. The setting of theforeground process 150 and thenon-foreground process 152 by theagent interface component 126 can be facilitated by that operating system running on theclient device 104. As discussed above, theagent application 110 can be running as one the applications in theforeground process 150 or thenon-foreground process 152, or neither theforeground process 150 nor thenon-foreground process 152. With the passing of the action data structure, theagent interface component 126 can control whether theagent application 110 is to be set from thenon-foreground process 152 to theforeground process 150. - Furthermore, the
agent interface component 126 can identify which applications are running in theforeground process 150 and as thenon-foreground process 152, such as thedigital assistant application 108 and theagent applications 110 installed on theclient device 104. For example, theagent interface component 126 can access the operating system of theclient device 104 to identify applications running as theforeground process 150 and applications running in thenon-foreground process 152. Theagent interface component 126 can identify whether the agent application 110 (or other applications on the client device 104) is one of theforeground process 150 or thenon-foreground process 152. Theagent interface component 126 can identify theagent application 110 as running as one of the applications in theforeground process 150. Theagent interface component 126 can identify theagent application 110 as running as one of the applications in thenon-foreground process 152. Theagent interface component 126 can identify theagent application 110 as running as one of the applications neither in theforeground process 150 nor thenon-foreground process 152. In response to determining that theagent application 110 is running neither as theforeground process 150 nor thenon-foreground process 152, theagent interface component 126 can cause theclient device 104 to initiate execution of theagent application 110. - From the selected action-
inventory 140, theagent interface component 126 can identify the interface mode indicator. The interface mode indicator can specify theuser interface component 142 of theagent application 110 is to be rendered on thedisplay 148 of theclient device 104 in executing the action corresponding to the action-inventory 140. When the interface mode indicator specifies that nouser interface component 142 is to be rendered, theagent interface component 126 can set theagent application 110 as theforeground process 150. When theagent application 110 is identified as not executing on theclient device 104, theagent interface component 126 can launch theagent application 110 in theforeground process 150. When theagent application 110 is identified as running in thenon-foreground process 152, theagent interface component 126 can remove the application originally in the foreground process 150 (e.g., anotheragent application 110 or the digital assistant application 108) and set to thenon-foreground process 152. In addition, theagent handler component 126 can set theagent application 110 from thenon-foreground process 152 to theforeground process 150. When theagent application 110 is identified as already in theforeground process 150, theagent interface component 126 can maintain theagent application 110 in theforeground process 150. - When the interface mode indicator specifies that a
user interface component 142 is to be rendered, theagent interface component 126 can apply at least one authorization policy of theagent application 110. The authorization policy can specify for which applications the output of theagent application 110 is restricted or permitted to be presented with. For example, the authorization policy for one personal fitness application can specify that the output is to be restricted from being presented with another personal fitness application. The authorization policy can include a list of application (also referred herein a blacklist) for which the output of theagent application 110 is restricted from concurrent presentation. The authorization policy can also include a list of application (also referred herein as a whitelist) for which the output of theagent application 110 is permitted for concurrent presentation. Each list can include agent identifiers (e.g., application file names) corresponding to the included applications. The authorization policy can be specific to an output type of theagent application 110. The output type can depend on the request identified from parsing the input audio signal. For example, the authorization policy can restrict displays of a number of steps while another personal fitness application is in the foreground but can permit displays of account credential information. - The determination of whether to apply the authorization policy can be based on whether the
agent application 110 is running in theforeground process 150 or thenon-foreground process 152. When theagent application 110 is identified as in theforeground process 150, theagent interface component 126 can determine not to apply the authorization policy. Theagent interface component 126 can also permit theuser interface component 142 of theagent application 110 with the output to be rendered in thedisplay 148 of theclient device 104. When another application besidesagent application 110 is identified as running in theforeground process 150, theagent interface component 126 can determine to apply the authorization policy. In this manner, theagent application 110 can remain as part of thenon-foreground process 152 or not executing on theclient device 104. In determining whether to apply, theagent interface component 126 can also identify the authorization policy based on the request parsed from the input audio signal. Theagent interface component 126 can compare the identified request with the output type for the authorization policy. Upon determining a match, theagent interface component 126 can determine to apply the authorization policy specified for the corresponding output type. - In accordance with the authorization policy, the
agent interface component 126 can determine whether the output of theagent application 110 is authorized to be presented with the graphical user interface component of the application in theforeground process 150. Theagent interface component 126 can identify the application in theforeground process 150. Theagent interface component 126 can identify the agent identifier of the application in theforeground process 150. Theagent interface component 126 can compare the application with the lists of application specified in the authorization policy of theagent application 110. In comparing, theagent interface component 126 can use the identified agent identifier to search both the whitelist and the blacklist of the authorization policy for theagent application 110. If the application in theforeground process 150 matches one of the applications in the whitelist, theagent interface component 126 can determine to permit the output of theagent application 110 to be presented with the application. On the other hand, if the application in theforeground process 150 matches one of the applications in the blacklist, theagent interface component 126 can determine to restrict the output from being presented with the application. - When the output of the
agent application 110 is determined to be permitted, theagent interface component 126 can identify theuser interface component 142 of theagent application 110. To identify theuser interface component 142, theagent interface component 126 can use the address generated in accordance with the address template of the action-inventory 140. For example, the address generated in accordance with the address template can reference both the action to be executed by theagent application 110 and theuser interface component 142 to be rendered in carrying out the action. The address generated in accordance with the address template can reference theagent application 110 or theagent service 106 managing resources for theagent application 110. Theagent interface component 126 can identify a separate address or identifier for theuser interface component 142 from the action-inventory 140. The address can reference theuser interface component 142 maintained by theagent service 106. The address and identifier can reference theuser interface component 142 maintained on thedata repository 128. Using the separate address or identifier, theagent interface component 126 can identify theuser interface component 142 to be rendered in conjunction with the execution of the action by theagent application 110. - Upon identifying, the
agent interface component 126 can access or retrieve theuser interface component 142. Using the address generated according to the address template of the action-inventory 140, theagent interface component 126 can retrieve theuser interface component 142. Theagent interface component 126 can send a request for theuser interface component 142 to theagent service 106 including the address. As the address references theagent service 106, the request can be directed to theagent service 106 handling resources for theagent application 110. Theagent service 106 in turn can provide a response with the referenceduser interface component 142 to theagent interface component 126. Using the address included in the action-inventory 140, theagent interface component 126 can retrieve theuser interface component 142. The address included in the action-inventory 140 can be separate from the address generated using the address template, and can reference theagent service 106 or thedata repository 128. Theagent interface component 126 can send a request for theuser interface component 142 including the address. When the address references theagent service 106, the request can be directed to theagent service 106 handling resources for theagent application 110. When the address references thedata repository 128, the request can be directed to thedata repository 128. Theagent service 106 or thedata repository 128 in turn can provide a response with the referenceduser interface component 142 to theagent interface component 126. Using the identifier included in the action-inventory 140, theagent interface component 126 can retrieve theuser interface component 142 from thedata repository 128. The identifier can correspond to an index of theuser interface component 142 maintained on thedata repository 128. Theagent interface component 126 can search thedata repository 128 using the identifier to find theuser interface component 142. Once identified, theagent interface component 126 can retrieve theuser interface component 142 from thedata repository 128. - Prior to rendering the
user interface component 142, theagent interface component 126 can also apply an authorization policy of the application in theforeground process 150. The authorization policy can specify which applications the graphical user interface component of the agent in theforeground process 150 is restricted or permitted to be presented with. For example, the authorization policy for one banking application can specify that the output is to be restricted from being presented with another banking application. The authorization policy can include a list of application (also referred herein a blacklist) for which the graphical user interface of the application in theforeground process 150 is restricted from concurrent presentation. The authorization policy can also include a list of application (also referred herein as a whitelist) for which the graphical user interface of the application in theforeground process 150 is permitted for concurrent presentation. Each list can include agent identifiers (e.g., application file names) corresponding to the included applications. - In accordance with the authorization policy, the
agent interface component 126 can determine whether the graphical user interface component of the application in theforeground process 150 is authorized to be presented with the output of theagent application 110. Theagent interface component 126 can identify theagent application 110 executing the action. Theagent interface component 126 can identify the agent identifier of theagent application 110. Theagent interface component 126 can compare the application with the lists of application specified in the authorization policy of the application in theforeground process 150. In comparing, theagent interface component 126 can use the identified agent identifier to search both the whitelist and the blacklist of the authorization policy for the application in theforeground process 150. If theagent application 110 matches one of the applications in the whitelist, theagent interface component 126 can determine to permit concurrent presentation with the graphical user interface of the application in theforeground 150. On the other hand, if theagent application 110 matches one of the application in the blacklist, theagent interface component 126 can determine to restrict the concurrent presentation. - With the identification of the
user interface component 142, theagent interface component 126 can render theuser interface component 142 of theagent application 110 in thedisplay 148 of theclient device 104. With the determination that concurrent presentation is permitted, theuser interface component 142 of theagent application 110 can be rendered on thedisplay 148 concurrent with the graphical user interface of the application running in theforeground process 150. Theagent interface component 126 can render theuser interface component 142 as an overlay superimposed on the graphical user interface of the application in theforeground process 150. Theagent interface component 126 can also include theuser interface component 142 as a portion of the graphical user interface of the application in theforeground process 150. For example, the application running as theforeground process 150 can be thedigital assistant application 108, and can have a graphical user interface including a dialog with the user operating thedigital assistant application 108 on theclient device 104. Theagent interface component 126 can insert theuser interface component 142 in the dialog of the graphical user interface of thedigital assistant application 108. - The
agent interface component 126 can add or insert information into theuser interface component 142 rendered on thedisplay 148 of theclient device 104. Theagent interface component 126 can add, insert, or include the words or the request from the input audio signal into a corresponding interface element of theuser interface component 142. The words or the request can be identified by theNLP component 116 from parsing the input audio signal. For example, theagent interface component 126 can incorporate the words with the request onto a textbox of theuser interface component 142. Theagent interface component 126 can retrieve, receive, or identify the output generated by theagent application 110. Theagent interface component 126 can withhold the rendering of theuser interface component 142 and can wait for the output from theagent application 110. Upon receipt of the output from theagent application 110, theagent interface component 126 can add, insert, or include the output into a corresponding interface element of theuser interface component 142. For example, theagent interface component 126 can receive an output indicating the number of steps taken by the user of theagent application 110 installed on theclient device 104. Theagent interface component 126 can insert the number of steps into another textbox of theuser interface component 142. With the receipt and insertion of the output, theagent interface component 126 can render theuser interface component 142 on thedisplay 148. - As the
user interface component 142 is rendered on thedisplay 148 along with the graphical user interface of the application in theforeground process 150, theagent interface component 126 can monitor for user interaction events with theuser interface component 142. The user interaction events can include, for example, hover-overs, clicks, scrolling, flicking, keypresses, and touch screens, among others. Upon detecting a user interaction event on theuser interface component 142, theagent interface component 126 can process the user interaction event to update theuser interface component 142. Theagent interface component 126 can relay the detected user interaction event to theagent application 110. Theagent application 110 can process the user interaction event (e.g., using an event handler) and can generate another output in response to the user interaction event. Theagent application 110 can provide the output to theagent interface component 126. Using the output received from theagent application 110, theagent interface component 126 can update theuser interface component 142. - In this manner, the
agent interface component 126 can maintain the application originally in theforeground process 150, while adding auser interface component 142 from theagent application 110 for display as an overlay. Theagent interface component 126 can have both the original application as well as theagent application 110 both as applications in theforeground process 150. Theagent interface component 126 can also maintain visual components from both applications rendered on thedisplay 148 of theclient device 104, thereby reducing user of theclient device 104 to perform context switching. By keeping both, the agent interface component 459 can also reduce computing resources incurred from switching the rendering and between theforeground process 150 and thenon-foreground process 152. - Conversely, when the output of the
agent application 110 is determined to be restricted, theagent interface component 126 can set theagent application 110 as theforeground process 150. When theagent application 110 is identified as not running on theclient device 104, theagent interface component 126 can invoke and launch the agent application 110 (e.g., via a the operating system of the client device 104). Theagent interface component 126 can move the application originally running as theforeground process 150 and can set the application as running as thenon-foreground process 152. In switching theforeground process 150 and thenon-foreground process 152, theagent interface component 126 can remove the graphical user interface component of the application originally in theforeground process 150 from being rendered on thedisplay 148. In addition, theagent interface component 126 can invoke theagent application 110 using the action data structure generated by theaction handler component 120. Instead of rendering theuser interface component 142, theagent interface component 126 can render the graphical user interface of theagent application 110 on thedisplay 148 of theclient device 104. - Running as the
foreground process 150, theagent application 110 can display the output of the action indicated in the address of the action data structure on the graphical user interface rendered in thedisplay 148. Theagent application 110 can monitor for user interaction events with the graphical user interface. Upon detecting a user interaction event on theuser interface component 142, theagent application 110 can process the user interaction event (e.g., using an event handler) and can generate another output in response to the user interaction event. Theagent application 110 can update the graphical user interface rendered in thedisplay 148 of theclient device 104 using the generated output. - By controlling the switching between the
foreground process 150 and thenon-foreground process 152, the user involvement to search for theagent application 110 may be eliminated, thereby improving the HCI with the applications on theclient device 104. In addition, thedigital assistant application 108 can regulate and reduce consumption of computing resources on theclient device 104 from the switching. -
FIG. 2 depicts a sequence diagram of anexample data flow 200 to rending visual components on applications in thesystem 100 illustrated inFIG. 1 . The data flow 200 can be implemented or performed by thesystem 100 described above in conjunction withFIG. 1 orsystem 500 detailed below in conjunction withFIG. 5 . The data flow 200 an include communications in the form of packets (e.g., HTTP messages) among themicrophone 150, theclient device 104, thedata processing system 102, theagent service 106 via thenetwork 114, and thedisplay 148. - A local instance of the
digital assistant application 108 running on theclient device 104 can receive audio data 205 (e.g., the input audio signal) from amicrophone 146 communicatively coupled with theclient device 104. Thedigital assistant application 108 can execute on theclient device 104 in thenon-foreground process 152, without any visual component rendered on thedisplay 148. Theclient device 104 can have another application in theforeground process 150, with a graphical user interface rendered on thedisplay 148. Thedigital assistant application 108 on theclient device 104 can perform initial processing (e.g., automated speech recognition) on theaudio data 205 to identify one or more words from theaudio data 205. Thedigital assistant application 108 can provide the one or more recognized words from theaudio data 205 as aninput 210 to a remote instance of thedigital assistant application 108 running on thedata processing system 102. TheNLP component 116 on thedata processing system 102 in turn can further process the one or more words of theinput 210 to identify a request and one or more parameters. The request can indicate an action to be performed by theagent application 110 in accordance with the one or more parameters. - The
action handler component 120 on thedata processing system 102 can identify an action-inventory 140 from thedata repository 128 using the request and the one or more parameters. Theagent application 110 associated with the identified action-inventory 140 can be not executing on theclient device 104 or can be running as one of the applications of thenon-foreground process 152. Theaction handler component 120 can generate an address in accordance with an address template of the identified action-inventory 140. The address can include a first substring (e.g., a scheme or hostname) referencing the agent application that is to carry out the action. The address can additionally include a second string corresponding to the action to be executed and the one or more parameters in accordance to which the action is to be executed. Theaction handler component 120 can package the address into an action data structure to provide as aninput 215 to theagent service 106 for theagent application 110. - Upon receipt of the
input 215, theagent service 106 for theagent application 110 can parse the action data structure to identify the first substring and the second substring. As the input is processed, theclient device 104 can maintain applications in running as theforeground process 150 and as thenon-foreground process 152. Theagent application 110 executing on theagent service 106 can further parse the second substring to identify the action and the one or more parameters. With the identification, theagent application 110 can execute the action in accordance with the parameters. In executing the action, theagent application 110 can generate anoutput 220 to provide to theclient device 104 via thedata processing system 102. Theagent service 106 can provide theoutput 220 to thedata processing system 102. Thedigital assistant application 108 on thedata processing system 102 can send theoutput 220 as anoutput 225 to theclient device 104. - In conjunction, the
agent interface component 126 can determine that the action-inventory 140 specifies that auser interface component 142 is to be rendered in carrying out the action. In response to the determination, theagent interface component 126 can identify a user interface component 235 (e.g., using the address generated using the address template of the action-inventory 140) from the set ofuser interface components 142. Theagent interface component 126 can provide theuser interface component 235 to theclient device 104. Using theuser interface component 235, theclient device 104 can displayrendering data 240 for thedisplay 148 of theclient device 104. Therendering data 240 can include theuser interface component 235 as an overlay superimposed on the graphical user interface of the application in theforeground process 150. Theclient device 104 can maintain the applications in theforeground process 150 and thenon-foreground process 152 prior to the receipt of theaudio data 205 from themicrophone 146. -
FIG. 3 illustrates a use case diagram of anexample client device 104 displaying input messages and providing user interface components in thesystem 100 illustrated inFIG. 1 . Theclient device 104 can have an application running as theforeground process 150 and a local instance of thedigital assistant application 108 running as thenon-foreground process 152. The application running as theforeground process 150 can have agraphical user interface 305 rendered within thedisplay 148 of theclient device 104. As depicted, thegraphical user interface 305 can be for a web browser application running as theforeground process 150. In this context, theNLP component 116 of thedigital assistant application 108 running on theclient device 104 can receive an input audio signal via themicrophone 146 of theclient device 104. TheNLP component 116 can apply automated speech recognition to identify one ormore words 310 from the input audio signal. For example, theNLP component 116 can identify thewords 310 “How many steps have I taken today?” from the input audio signal. - The
NLP component 116 on theclient device 104 or thedata processing system 102 can perform further processing on thewords 310. By further processing, theNLP component 116 can identify the request from thewords 310 recognized from the input audio signal. In the example depicted, theNLP component 116 can identify the request as a fetch request for a number of steps taken by a user operating or otherwise associated with theclient device 104. Using the identified request, theaction handler component 120 can identify anagent application 110 capable of performing the request. Theaction handler component 120 can also select an action-inventory 140 from thedata repository 128. The action-inventory 140 can be for the action corresponding to the request to be executed by theagent application 110. Using the action-inventory 140, theaction handler component 120 can invoke theagent application 110 to carry out the action corresponding to the request. - In conjunction, the
agent interface component 126 can identify that auser interface component 142 is to be displayed in carrying out the action of the action-inventory 140. The action-inventory 140 can specify whether auser interface component 142 is to be displayed, and can identify whichuser interface component 142 is to be presented. In response to the identification, theagent interface component 126 can determine whether concurrent presentation is permitted with the graphicaluser interface component 305 of the foreground application. The determination can be in accordance with an authorization policy for theagent application 110. The authorization policy for theagent application 110 can specify which applications are allowed to be concurrently presented with theuser interface component 142 of theagent application 110. In the example depicted, theagent interface component 126 can determine that concurrent presentation is permitted. - In response to the determination, the
agent interface component 126 can identify theuser interface component 142 from the action-inventory 140 for carrying out the action indicated by thewords 310. Upon identification, theagent interface component 126 can render theuser interface component 142 as anoverlay component 315 superimposed on top of thegraphical user interface 305 of theforeground process 150. In the example depicted, theuser interface component 142 rendered as theoverlay component 315 can include thewords 310 of the request corresponding to the action, “How many steps?” In addition, theuser interface component 142 can also have the resultant output from carrying out the action, “You have taken 3147 steps today.” Theuser interface component 142 rendered as theoverlay component 315 can also include acommand button 320. Upon interaction with thecommand button 320, the agent interface component 125 can close theoverlay component 315, causing theuser interface component 142 to disappear from the rendering on thedisplay 148 of theclient device 104. -
FIG. 4 illustrates a block diagram of anexample method 400 of rending visual components on applications. Themethod 400 can be implemented or executed by thesystem 100 described above in conjunction withFIGS. 1-3 orsystem 500 detailed below in conjunction withFIG. 5 . In brief overview, theNLP component 116 can receive an input audio signal (405). TheNLP component 116 can parse the input audio signal (410). Theaction handler component 120 can select an action-inventory 140 (415). Theaction handler component 120 can generate an address using the action-inventory 140 (420). Theaction handler component 120 can generate an action data structure (425). Theagent interface component 126 can direct the action data structure (430). Theagent interface component 126 can determine whether auser interface component 142 is authorized to be presented with a foreground application (435). If determined to be authorized, theagent interface component 126 can present the user interface component 142 (440). If determined to be not authorized, theagent interface component 126 can invoke anagent application 110 associated with the user interface component (445). - In further detail, the
NLP component 116 can receive an input audio signal (405). TheNLP component 116 can receive one or more data packets including the input audio signal acquired at a sensor (e.g., the microphone 146) on theclient device 104. For example, theNLP component 116 of thedigital assistant application 108 executed at least partially by thedata processing system 102 can receive the input audio signal. The input audio signal can include a conversation facilitated by thedigital assistant application 108. The conversation can include one or more inputs and outputs. The conversation can be audio based, text based, or a combination of audio and text. The input audio signal can include text input, or other types of input that can provide conversational information. When the input audio signal is received, theclient device 104 can have applications running as theforeground process 150 and thenon-foreground process 152. - The
NLP component 116 can parse the input audio signal (410). By parsing the input audio signal, theNLP component 116 can identify a request and one or more parameters using various natural language processing techniques. The request can be an intent or request that can be fulfilled by thedigital assistant application 108 or theagent application 110. The parameters can define the request. Based on the request, theNLP component 116 can determine that the request corresponds to one of the functions of theagent application 110. - The
action handler component 120 can select an action-inventory 140 (415). Based on the request identified from the input audio signal, theaction handler component 120 can select the action-inventory 140 from the set maintained on thedata repository 128. For each action-inventory 140, theaction handler component 120 can compare the request with the request identifier of the action-inventory 140. The comparison can be performed using a semantic knowledge graph. From the comparison, theaction handler component 120 can determine that the request matches the request identifier of the action-inventory 140. In response to the determination, theaction handler component 120 can select the action-inventory 140 from the set maintained on thedata repository 128. The selected action-inventory can indicate that theuser interface component 142 of theagent application 110 that is not running as theforeground process 150 is to be presented. - The
action handler component 120 can generate an address using the action-inventory 140 (420). The generation of the address can be in accordance with an address template of the action-inventory 140. The address template can have a first portion and a second portion. The first portion can reference theagent application 110 or theagent service 106 for theagent application 110 to carry out the action. The second portion can reference the action and can include one or more input variables for the action. Theaction handler component 120 can take the first portion and the second portion from the address template in generating the address. Theaction handler component 120 can insert or populate the parameters into the input variables of the second portion of the address template. - The
action handler component 120 can generate an action data structure (425). The action data structure can include a header and a body. Theaction handler component 120 can insert the address generated in accordance with the address template of the action-inventory 140 into the header of the action data structure. Theaction handler component 120 can set the address as a destination address of the header to route the action data structure to theagent application 110 or theagent service 106 for theagent application 110. - The
agent interface component 126 can direct the action data structure (430). Theagent interface component 126 can direct the action data structure to theagent application 110. Theagent interface component 126 can invoke theagent application 110 using the action data structure and can pass the action data structure to theagent application 110. Theagent application 110 in turn can parse the action data structure to identify the action and one or more parameters form the address included in the action data structure. Theagent application 110 can execute the action in accordance with the parameters. - The
agent interface component 126 can determine whether auser interface component 142 is authorized to be presented with a foreground application (435). The determination of whether theuser interface component 142 is authorized can be in accordance with an authorization policy. The authorization policy can specify whether the output of theagent application 110 is permitted or restricted from concurrent presentation with the graphical user interface of the application running as theforeground process 150. Theagent interface component 126 can identify the application running in theforeground process 150. Theagent interface component 126 can compare the application with the specifications of the authorization policy. When the authorization policy permits concurrent presentation, theagent interface component 126 can determine that theuser interface component 142 is authorized to be presented. Otherwise, when the authorization policy restricts the concurrent presentation, theagent interface component 126 can determine that theuser interface component 142 is not authorized to be presented with the application running as theforeground process 150. - If determined to be authorized, the
agent interface component 126 can present the user interface component 142 (440). Theagent interface component 126 can identify theuser interface component 142 from the selected action-inventory 140 (e.g., using an address include therein). Theagent interface component 126 can render the identifieduser interface component 142 as an overlay on the graphical user interface of the application running in theforeground process 150. Theagent interface component 126 can include the request for the action and the output generated by theagent application 110 in theuser interface component 142. - On the other hand, if determined to be not authorized, the
agent interface component 126 can invoke anagent application 110 associated with the user interface component 142 (445). Theagent interface component 126 can remove the original application running from theforeground process 150 and set the application to thenon-foreground process 152, thereby removing the rendering of the graphical user interface of the application. Theagent interface component 126 can set with theagent application 110 as theforeground process 150. Theagent interface component 126 can also render the graphical user interface of theagent application 110 on thedisplay 148 of the client device. -
FIG. 5 is a block diagram of anexample computer system 500. The computer system orcomputing device 500 can include or be used to implement thesystem 100 or its components such as thedata processing system 102. Thecomputing system 500 includes abus 505 or other communication component for communicating information and aprocessor 510 or processing circuit coupled to thebus 505 for processing information. Thecomputing system 500 can also include one ormore processors 510 or processing circuits coupled to the bus for processing information. Thecomputing system 500 also includesmain memory 515, such as a random access memory (RAM) or other dynamic storage device, coupled to thebus 505 for storing information and instructions to be executed by theprocessor 510. Themain memory 515 can be or include thedata repository 128. Themain memory 515 can also be used for storing position information, temporary variables, or other intermediate information during execution of instructions by theprocessor 510. Thecomputing system 500 may further include a read-only memory (ROM) 520 or other static storage device coupled to thebus 505 for storing static information and instructions for theprocessor 510. Astorage device 525, such as a solid state device, magnetic disk or optical disk, can be coupled to thebus 505 to persistently store information and instructions. Thestorage device 525 can include or be part of thedata repositories 128. - The
computing system 500 may be coupled via thebus 505 to adisplay 535, such as a liquid crystal display or active matrix display, for displaying information to a user. Aninput device 530, such as a keyboard including alphanumeric and other keys, may be coupled to thebus 505 for communicating information and command selections to theprocessor 510. Theinput device 530 can include atouch screen display 535. Theinput device 530 can also include a cursor control, such as a mouse, a trackball, or cursor direction keys, for communicating direction information and command selections to theprocessor 510 and for controlling cursor movement on thedisplay 535. Thedisplay 535 can be part of thedata processing system 102, theclient devices 104, or other components ofFIG. 1 , for example. - The processes, systems and methods described herein can be implemented by the
computing system 500 in response to theprocessor 510 executing an arrangement of instructions contained inmain memory 515. Such instructions can be read intomain memory 515 from another computer-readable medium, such as thestorage device 525. Execution of the arrangement of instructions contained inmain memory 515 causes thecomputing system 500 to perform the illustrative processes described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained inmain memory 515. Hard-wired circuitry can be used in place of or in combination with software instructions together with the systems and methods described herein. Systems and methods described herein are not limited to any specific combination of hardware circuitry and software. - Although an example computing system has been described in
FIG. 5 , the subject matter including the operations described in this specification can be implemented in other types of digital electronic circuitry or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. - For situations in which the systems discussed herein collect personal information about users, or may make use of personal information, the users may be provided with an opportunity to control whether programs or features that may collect personal information (e.g., information about a user's social network, social actions, or activities; a user's preferences; or a user's location), or to control whether or how to receive content from a content server or other data processing system that may be more relevant to the user. In addition, certain data may be anonymized in one or more ways before it is stored or used, so that personally identifiable information is removed when generating parameters. For example, a user's identity may be anonymized so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, postal code, or state level), so that a particular location of a user cannot be determined. Thus, the user may have control over how information is collected about him or her and used by the content server.
- The subject matter and the operations described in this specification can be implemented in digital electronic circuitry or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs, e.g., one or more circuits of computer program instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatuses. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. A computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial-access memory array or device, or a combination of one or more of them. While a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium can also be, or be included in, one or more separate components or media (e.g., multiple CDs, disks, or other storage devices). The operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
- The terms “data processing system,” “computing device,” “component,” or “data processing apparatus” encompass various apparatuses, devices, and machines for processing data, including, by way of example, a programmable processor, a computer, a system on a chip, or multiple ones, or combinations of the foregoing. The apparatus can include special-purpose logic circuitry, e.g., an FPGA (field-programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures. The components of
system 100 can include or share one or more data processing apparatuses, systems, computing devices, or processors. - A computer program (also known as a program, software, software application, app, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program can correspond to a file in a file system. A computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs (e.g., components of the data processing system 102) to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatuses can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field-programmable gate array) or an ASIC (application-specific integrated circuit). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- The subject matter described herein can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification, or a combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- The computing system such as
system 100 orsystem 500 can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network (e.g., the network 114). The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, a server transmits data (e.g., data packets representing a content item) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) can be received from the client device at the server (e.g., received by thedata processing system 102 from theclient devices 104 or the agent service 106). - While operations are depicted in the drawings in a particular order, such operations are not required to be performed in the particular order shown or in sequential order, and all illustrated operations are not required to be performed. Actions described herein can be performed in a different order.
- The separation of various system components does not require separation in all implementations, and the described program components can be included in a single hardware or software product. For example, the
NLP component 116, the audiosignal generator component 118, theaction handler component 120, theresponse selector component 122, theagent registry component 124, and theagent interface component 126 can be a single component, app, or program, or a logic device having one or more processing circuits, or part of one or more servers of thedata processing system 102. - Having now described some illustrative implementations, it is apparent that the foregoing is illustrative and not limiting, having been presented by way of example. In particular, although many of the examples presented herein involve specific combinations of method acts or system elements, those acts and those elements may be combined in other ways to accomplish the same objectives. Acts, elements, and features discussed in connection with one implementation are not intended to be excluded from a similar role in other implementations.
- The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of “including,” “comprising,” “having,” “containing,” “involving,” “characterized by,” “characterized in that,” and variations thereof herein, is meant to encompass the items listed thereafter, equivalents thereof, and additional items, as well as alternate implementations consisting of the items listed thereafter exclusively. In one implementation, the systems and methods described herein consist of one, each combination of more than one, or all of the described elements, acts, or components.
- Any references to implementations, elements, or acts of the systems and methods herein referred to in the singular may also embrace implementations including a plurality of these elements, and any references in plural to any implementation, element, or act herein may also embrace implementations including only a single element. References in the singular or plural form are not intended to limit the presently disclosed systems or methods, their components, acts, or elements to single or plural configurations. References to any act or element being based on any information, act, or element may include implementations where the act or element is based at least in part on any information, act, or element.
- Any implementation disclosed herein may be combined with any other implementation or embodiment, and references to “an implementation,” “some implementations,” “one implementation,” or the like are not necessarily mutually exclusive and are intended to indicate that a particular feature, structure, or characteristic described in connection with the implementation may be included in at least one implementation or embodiment. Such terms as used herein are not necessarily all referring to the same implementation. Any implementation may be combined with any other implementation, inclusively or exclusively, in any manner consistent with the aspects and implementations disclosed herein.
- References to “or” may be construed as inclusive so that any terms described using “or” may indicate any of a single, more than one, and all of the described terms. A reference to “at least one of ‘A’ and ‘B’” can include only ‘A’, only ‘B’, as well as both ‘A’ and ‘B’. Such references used in conjunction with “comprising” or other open terminology can include additional items.
- Where technical features in the drawings, detailed description, or any claim are followed by reference signs, the reference signs have been included to increase the intelligibility of the drawings, detailed description, and claims. Accordingly, neither the reference signs nor their absence have any limiting effect on the scope of any claim elements.
- The systems and methods described herein may be embodied in other specific forms without departing from the characteristics thereof. The foregoing implementations are illustrative rather than limiting of the described systems and methods. Scope of the systems and methods described herein is thus indicated by the appended claims, rather than the foregoing description, and changes that come within the meaning and range of equivalency of the claims are embraced therein.
Claims (20)
1. A system to render visual components on applications, comprising:
a natural language processor executed on a data processing system having one or more processors, the natural language processor to:
receive a data packet comprising an input audio signal detected by a sensor of a client device, the client device displaying a graphical user interface of a first application that is running in a foreground process on the client device; and
parse the input audio signal of the data packet to identify a request;
an action handler executed on the data processing system to:
select an action-inventory that executes an action corresponding to the request by a second application, the second application installed on the client device and not in the foreground process; and
generate an action data structure in accordance with the action-inventory; and
an agent interface executed on the data processing system to:
provide the action data structure to the second application to cause the second application to parse the action data structure and execute the action to generate an output;
determine, based on an authorization policy, that the output of the second application from execution of the action is not authorized to be presented with the graphical user interface of the first application; and
in response to determining that the output of the second application from execution of the action is not authorized to be presented with the graphical user interface of the first application:
removing the first application from the foreground process on the client device and setting the first application to a non-foreground process on the client device;
setting the second application to run in the foreground process on the client device; and
displaying a graphical user interface of the second application, including the output generated by the second application, on the client device.
2. The system of claim 1 , further comprising:
the action-inventory having an agent identifier corresponding to the second application and a request identifier corresponding to the action;
the natural language processor to parse the input audio signal of the data packet to identify an agent identifier corresponding to an agent, the agent corresponding to the second application installed on the client device; and
the action handler to select the action-inventory for executing the action having the agent identifier corresponding to the agent identifier identified from parsing the input audio signal.
3. The system of claim 1 , comprising the agent interface to:
determine that the client device is not authenticated with the second application for the action-inventory to carry out the action corresponding to a second request; and
present, responsive to the determination that the client device is not authenticated with the second application, a prompt interface on the client device to authenticate the client device with the second application to execute the action corresponding to the second request.
4. The system of claim 1 , comprising the action handler to provide the action data structure to the second application to cause the second application to:
parse the action data structure to identify the action to be executed; and
generate the output by executing the action identified from the action data structure.
5. The system of claim 1 , wherein the action data structure comprises an address to execute the action of the action-inventory.
6. The system of claim 5 , wherein the address comprises a first substring corresponding to the second application.
7. The system of claim 6 , wherein the address comprises a second substring corresponding to the action.
8. The system of claim 7 , wherein the second substring comprises at least one input variable used to execute the action.
9. The system of claim 1 , wherein the authorization policy is associated with the second application.
10. The system of claim 9 , wherein the authorization policy comprises a whitelist and the first application is not included in the whitelist.
11. The system of claim 9 , wherein the authorization policy comprises a blacklist and the first application is included in the blacklist.
12. A method of rendering visual components on applications, comprising:
receiving, by a data processing system having one or more processors, a data packet comprising an input audio signal detected by a sensor of a client device, the client device displaying a graphical user interface of a first application that is running in a foreground process on the client device;
parsing, by the data processing system, the input audio signal of the data packet to identify a request;
selecting, by the data processing system, an action-inventory that executes an action corresponding to the request by a second application, the second application installed on the client device and not in the foreground process;
generating, by the data processing system, an action data structure in accordance with the action-inventory;
providing, by the data processing system, the action data structure to the second application to cause the second application to parse the action data structure and execute the action to generate an output;
determining, by the data processing system, based on an authorization policy, that the output of the second application from execution of the action is not authorized to be presented with the graphical user interface of the first application; and
in response to determining that the output of the second application from execution of the action is not authorized to be presented with the graphical user interface of the first application:
removing, by the data processing system, the first application from the foreground process on the client device and setting the first application to a non-foreground process on the client device;
setting, by the data processing system, the second application to run in the foreground process on the client device; and
displaying, by the data processing system, a graphical user interface of the second application, including the output generated by the second application, on the client device.
13. The method of claim 12 , further comprising providing the action data structure to the second application to cause the second application to:
parse the action data structure to identify the action to be executed; and
generate the output by executing the action identified from the action data structure.
14. The method of claim 12 , wherein the action data structure comprises an address to execute the action of the action-inventory.
15. The method of claim 14 , wherein the address comprises a first substring corresponding to the second application.
16. The method of claim 15 , wherein the address comprises a second substring corresponding to the action.
17. The method of claim 16 , wherein the second substring comprises at least one input variable used to execute the action.
18. The method of claim 12 , wherein the authorization policy is associated with the second application.
19. The method of claim 18 , wherein the authorization policy comprises a whitelist and the first application is not included in the whitelist.
20. The method of claim 18 , wherein the authorization policy comprises a blacklist and the first application is included in the blacklist.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18/144,740 US20230280974A1 (en) | 2019-05-06 | 2023-05-08 | Rendering visual components on applications in response to voice commands |
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/030930 WO2020226619A1 (en) | 2019-05-06 | 2019-05-06 | Rendering visual components on applications in response to voice commands |
US16/466,254 US11360738B2 (en) | 2019-05-06 | 2019-05-06 | Rendering visual components on applications in response to voice commands |
US17/837,896 US11675566B2 (en) | 2019-05-06 | 2022-06-10 | Rendering visual components on applications in response to voice commands |
US18/144,740 US20230280974A1 (en) | 2019-05-06 | 2023-05-08 | Rendering visual components on applications in response to voice commands |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/837,896 Continuation US11675566B2 (en) | 2019-05-06 | 2022-06-10 | Rendering visual components on applications in response to voice commands |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230280974A1 true US20230280974A1 (en) | 2023-09-07 |
Family
ID=66776878
Family Applications (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/466,254 Active 2040-02-22 US11360738B2 (en) | 2019-05-06 | 2019-05-06 | Rendering visual components on applications in response to voice commands |
US17/837,896 Active US11675566B2 (en) | 2019-05-06 | 2022-06-10 | Rendering visual components on applications in response to voice commands |
US18/144,740 Pending US20230280974A1 (en) | 2019-05-06 | 2023-05-08 | Rendering visual components on applications in response to voice commands |
Family Applications Before (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/466,254 Active 2040-02-22 US11360738B2 (en) | 2019-05-06 | 2019-05-06 | Rendering visual components on applications in response to voice commands |
US17/837,896 Active US11675566B2 (en) | 2019-05-06 | 2022-06-10 | Rendering visual components on applications in response to voice commands |
Country Status (4)
Country | Link |
---|---|
US (3) | US11360738B2 (en) |
EP (1) | EP3752917A1 (en) |
CN (1) | CN112262370A (en) |
WO (1) | WO2020226619A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11360738B2 (en) | 2019-05-06 | 2022-06-14 | Google Llc | Rendering visual components on applications in response to voice commands |
CN113181657B (en) * | 2021-04-30 | 2024-04-05 | 北京读我网络技术有限公司 | Cross-platform rendering method and device |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8165886B1 (en) * | 2007-10-04 | 2012-04-24 | Great Northern Research LLC | Speech interface system and method for control and interaction with applications on a computing system |
US20150112962A1 (en) * | 2012-05-31 | 2015-04-23 | Doat Media Ltd. | System and method for launching applications on a user device based on the user intent |
WO2013012107A1 (en) * | 2011-07-19 | 2013-01-24 | 엘지전자 주식회사 | Electronic device and method for controlling same |
EP2750309A4 (en) * | 2011-08-26 | 2015-07-15 | Japan Broadcasting Corp | Receiver and reception method |
US9575720B2 (en) * | 2013-07-31 | 2017-02-21 | Google Inc. | Visual confirmation for a recognized voice-initiated action |
JP2016539432A (en) * | 2013-09-13 | 2016-12-15 | クアルコム，インコーポレイテッド | Wireless communication device with definitive control of foreground access of user interface |
US9959129B2 (en) * | 2015-01-09 | 2018-05-01 | Microsoft Technology Licensing, Llc | Headless task completion within digital personal assistants |
KR20190133100A (en) * | 2018-05-22 | 2019-12-02 | 삼성전자주식회사 | Electronic device and operating method for outputting a response for a voice input, by using application |
US11360738B2 (en) | 2019-05-06 | 2022-06-14 | Google Llc | Rendering visual components on applications in response to voice commands |
-
2019
- 2019-05-06 US US16/466,254 patent/US11360738B2/en active Active
- 2019-05-06 WO PCT/US2019/030930 patent/WO2020226619A1/en unknown
- 2019-05-06 EP EP19729113.1A patent/EP3752917A1/en active Pending
- 2019-05-06 CN CN201980002177.6A patent/CN112262370A/en active Pending
-
2022
- 2022-06-10 US US17/837,896 patent/US11675566B2/en active Active
-
2023
- 2023-05-08 US US18/144,740 patent/US20230280974A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US11675566B2 (en) | 2023-06-13 |
US11360738B2 (en) | 2022-06-14 |
CN112262370A (en) | 2021-01-22 |
EP3752917A1 (en) | 2020-12-23 |
US20210334067A1 (en) | 2021-10-28 |
US20220308830A1 (en) | 2022-09-29 |
WO2020226619A1 (en) | 2020-11-12 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11790904B2 (en) | Voice application platform | |
US11450321B2 (en) | Voice application platform | |
US11615791B2 (en) | Voice application platform | |
US20230280974A1 (en) | Rendering visual components on applications in response to voice commands | |
US11922209B2 (en) | Invoking functions of agents via digital assistant applications using address templates | |
US11775254B2 (en) | Analyzing graphical user interfaces to facilitate automatic interaction | |
US11437029B2 (en) | Voice application platform | |
US11664025B2 (en) | Activation of remote devices in a networked system | |
US20210141820A1 (en) | Omnichannel virtual assistant using artificial intelligence | |
US20220027124A1 (en) | Verifying operational statuses of agents interfacing with digital assistant applications | |
CA3102093A1 (en) | Voice application platform |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:THAKKAR, ANUJ;REEL/FRAME:063600/0230Effective date: 20190903 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP., ISSUE FEE NOT PAID |