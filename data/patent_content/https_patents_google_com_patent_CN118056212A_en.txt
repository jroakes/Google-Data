CN118056212A - Retraining method with automatic verification for machine learning model - Google Patents
Retraining method with automatic verification for machine learning model Download PDFInfo
- Publication number
- CN118056212A CN118056212A CN202280043004.0A CN202280043004A CN118056212A CN 118056212 A CN118056212 A CN 118056212A CN 202280043004 A CN202280043004 A CN 202280043004A CN 118056212 A CN118056212 A CN 118056212A
- Authority
- CN
- China
- Prior art keywords
- model
- dataset
- integrated
- base
- base model
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000010801 machine learning Methods 0.000 title claims abstract description 36
- 238000000034 method Methods 0.000 title claims description 61
- 238000012795 verification Methods 0.000 title description 24
- 238000012549 training Methods 0.000 claims description 89
- 238000012360 testing method Methods 0.000 claims description 46
- 238000004519 manufacturing process Methods 0.000 claims description 41
- 238000003860 storage Methods 0.000 claims description 8
- 238000001514 detection method Methods 0.000 description 30
- 230000015654 memory Effects 0.000 description 13
- 230000008569 process Effects 0.000 description 10
- 238000009826 distribution Methods 0.000 description 8
- 238000012545 processing Methods 0.000 description 8
- 230000011218 segmentation Effects 0.000 description 8
- 230000010354 integration Effects 0.000 description 7
- 238000010586 diagram Methods 0.000 description 6
- 230000000694 effects Effects 0.000 description 6
- 238000013528 artificial neural network Methods 0.000 description 5
- 238000011156 evaluation Methods 0.000 description 5
- 230000006870 function Effects 0.000 description 5
- 230000008901 benefit Effects 0.000 description 4
- 230000015556 catabolic process Effects 0.000 description 4
- 238000004891 communication Methods 0.000 description 4
- 238000004590 computer program Methods 0.000 description 4
- 238000006731 degradation reaction Methods 0.000 description 4
- 239000003550 marker Substances 0.000 description 4
- 238000012544 monitoring process Methods 0.000 description 4
- 238000012552 review Methods 0.000 description 4
- 230000002776 aggregation Effects 0.000 description 3
- 238000004220 aggregation Methods 0.000 description 3
- 238000001914 filtration Methods 0.000 description 3
- 238000004422 calculation algorithm Methods 0.000 description 2
- 238000011478 gradient descent method Methods 0.000 description 2
- 238000007689 inspection Methods 0.000 description 2
- 239000000203 mixture Substances 0.000 description 2
- 238000005457 optimization Methods 0.000 description 2
- 230000002441 reversible effect Effects 0.000 description 2
- 230000035945 sensitivity Effects 0.000 description 2
- 230000002123 temporal effect Effects 0.000 description 2
- 239000008186 active pharmaceutical agent Substances 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000001427 coherent effect Effects 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 238000011109 contamination Methods 0.000 description 1
- 230000007123 defense Effects 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 230000014509 gene expression Effects 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000000737 periodic effect Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 238000013515 script Methods 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000010200 validation analysis Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 239000002699 waste material Substances 0.000 description 1
Abstract
Aspects of the present disclosure relate to retraining an integrated machine learning model. The integrated model can include a base model and an overlay model. The base model can be trained, validated, and manually verified on older data sets. The overlay model can be trained on newer data sets and automatically validated. The combination of base model predictions and overlay model predictions (biased towards the base model predictions) can form an integrated model prediction. The model weights used to optimize the integrated model can determine the bias and indicate that the overlay model contributes too much or too little to the integrated model.
Description
Background
Machine learning models in production settings may experience degradation in performance over time due to changes in data distribution. Thus, these machine learning models should be periodically retrained on updated near data. However, retraining in some production settings (e.g., high impact systems that can directly impact revenue, cost, and/or trust in machine learning models) can require manual and labor intensive verification and/or inspection. This problem is exacerbated if the machine learning model is a neural network, because the neural network will have unstable predictions based on different initializations of weights.
Disclosure of Invention
Aspects of the present disclosure relate to retraining a machine learning model into an integrated model (ensable model). The integrated model can include a base model that is trained, validated, and manually verified on the older dataset, and can include an overlay model (overlay model) that is trained and automatically validated on the newer dataset. The integrated model prediction can be based on a combination of the base model prediction and the overlay model prediction, as well as a bias towards the base model prediction. The model weights used to optimize the integrated model can determine the bias and indicate that the overlay model contributes too much or too little to the integrated model.
An aspect of the present disclosure provides a computer-implemented method for retraining a machine learning model. The method includes generating, with one or more processors, an integrated model. The integrated model includes a base model and an overlay model, wherein the base model has been trained using the first data set. The method also includes training, with the one or more processors, the overlay model with a training data set, wherein the training data set is a first subset of a second data set that is more recent than the first data set. The method also includes validating, with the one or more processors, the trained overlay model with the test dataset using the plurality of metrics, wherein the test dataset is a second subset of the second dataset. The method further includes determining, with the one or more processors and based on the plurality of metrics, an amount of bias to be provided to the base model as compared to the overlay model when performing the prediction using the integrated model. The method also includes performing, with the one or more processors, a prediction using the integrated model based on a combination of the prediction from the base model and the prediction from the overlay model, and the determined bias toward the prediction from the base model.
In an example, the method further includes training, with the one or more processors, a base model with a base model training dataset, wherein the base model training dataset is a first subset of the first dataset. In another example, the method further comprises: validating, with the one or more processors, the base model with a base model test dataset, wherein the base model test dataset is a second subset of the first dataset; and manually inspecting the base model using the base model test dataset of the first dataset.
In yet another example, the plurality of metrics includes classification performance. In yet another example, determining the bias amount further includes determining a maximum classification performance of the integrated model in the search interval using a search method. In yet another example, the search interval includes a starting weight value to a weight value of 1, the starting weight value being greater than 0.5.
In yet another example, the method further includes determining, with the one or more processors, that the determined bias amount is within a range within the search interval in which both the overlay model and the base model contribute more than a threshold amount to the integrated model prediction. In yet another example, the method further includes determining, with the one or more processors, that the determined bias amount is within a range within the search interval in which the coverage model contributes less than a threshold amount to the integrated model prediction; and retraining, with the one or more processors, the overlay model with the second data set. In yet another example, the method further includes determining, with the one or more processors, that the determined bias amount is within a range within the search interval in which the contribution of the base model to the integrated model prediction is below a threshold amount; and replacing the base model with one or more processors.
In yet another example, the plurality of metrics includes stability. In yet another example, the method further includes determining, with the one or more processors, that the integrated model prediction is within a threshold as compared to predictions from a previous model, such that the integrated model is pushed toward production.
Another aspect of the present disclosure provides a system, comprising: one or more processors; and one or more storage devices coupled to the one or more processors and storing instructions that, when executed by the one or more processors, cause the one or more processors to perform operations for retraining the machine learning model. The operations include generating an integrated model. The integrated model includes a base model and an overlay model, wherein the base model has been trained using the first data set. The operations also include training the overlay model with a training data set, wherein the training data set is a first subset of a second data set that is more recent than the first data set. The operations also include validating the trained overlay model with the test dataset using the plurality of metrics, wherein the test dataset is a second subset of the second dataset. The operations further comprise: the amount of bias to be provided to the base model as compared to the overlay model when performing the prediction using the integrated model is determined based on a plurality of metrics. The operations further comprise: based on a combination of predictions from the base model and predictions from the overlay model, and the determined bias toward predictions from the base model, predictions are performed using the integrated model.
In an example, the operations further include training a base model with a base model training dataset, wherein the base model training dataset is a first subset of the first dataset. In another example, the operations further comprise validating the base model with a base model test dataset, wherein the base model test dataset is a second subset of the first dataset; and manually inspecting the base model using the base model test dataset of the first dataset.
In yet another example, the plurality of metrics includes classification performance. In yet another example, determining the bias amount further includes determining a maximum classification performance of the integrated model in the search interval using a search method. In yet another example, the search interval includes a starting weight value to a weight value of 1, the starting weight value being greater than 0.5.
In yet another example, the operations further include determining that the determined bias amount is within a range within the search interval in which both the overlay model and the base model contribute more than a threshold amount to the integrated model prediction. In yet another example, the operations further include determining that the determined bias amount is within a range within the search interval in which the coverage model contribution to the integrated model prediction is below a threshold amount; and retraining the overlay model with the second data set. In yet another example, the operations further include determining that the determined bias amount is within a range within the search interval in which the contribution of the base model to the integrated model prediction is below a threshold amount; replacing the base model.
In yet another example, the plurality of metrics includes stability. In yet another example, the operations further include determining that the integrated model prediction is within a threshold value as compared to predictions from a previous model, such that the integrated model is pushed toward production.
Yet another aspect of the disclosure provides a computer-readable medium for storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations for retraining a machine learning model. The operations include generating an integrated model. The integrated model includes a base model and an overlay model, wherein the base model has been trained using the first data set. The operations also include training the overlay model with a training data set, wherein the training data set is a first subset of a second data set that is more recent than the first data set. The operations also include validating the trained overlay model with the test dataset using the plurality of metrics, wherein the test dataset is a second subset of the second dataset. The operations further comprise: the amount of bias to be provided to the base model as compared to the overlay model when performing the prediction using the integrated model is determined based on a plurality of metrics. The operations also include performing a prediction using the integrated model based on a combination of the prediction from the base model and the prediction from the overlay model, and the determined bias toward the prediction from the base model.
In an example, the operations further include training a base model with a base model training dataset, wherein the base model training dataset is a first subset of the first dataset. In another example, the operations further comprise validating the base model with a base model test dataset, wherein the base model test dataset is a second subset of the first dataset; and manually inspecting the base model using the base model test dataset of the first dataset.
In yet another example, the plurality of metrics includes classification performance. In yet another example, determining the bias amount further includes determining a maximum classification performance of the integrated model in the search interval using a search method. In yet another example, the search interval includes a starting weight value to a weight value of 1, the starting weight value being greater than 0.5.
In yet another example, the operations further include determining that the determined bias amount is within a range within the search interval in which both the overlay model and the base model contribute more than a threshold amount to the integrated model prediction. In yet another example, the operations further include determining that the determined bias amount is within a range within the search interval in which the coverage model contribution to the integrated model prediction is below a threshold amount; and retraining the overlay model with the second data set. In yet another example, the operations further include determining that the determined bias amount is within a range within the search interval in which the contribution of the base model to the integrated model prediction is below a threshold amount; replacing the base model.
In yet another example, the plurality of metrics includes stability. In yet another example, the operations further include determining that the integrated model prediction is within a threshold value as compared to predictions from a previous model, such that the integrated model is pushed toward production.
Drawings
FIG. 1 depicts a block diagram of an example integrated model generation system, in accordance with aspects of the present disclosure.
FIG. 2 depicts an example timing diagram for model training and retraining based on model classification performance degradation in accordance with aspects of the present disclosure.
FIG. 3 depicts a flowchart of example process for retraining a machine learning model using an integrated model, according to aspects of the disclosure.
FIG. 4 depicts a block diagram of an example environment for retraining a machine learning model using an integrated model, in accordance with aspects of the present disclosure.
Detailed Description
Implementations for retraining a machine learning model with automatic verification are generally disclosed herein. The retrained machine learning model can correspond to: an integrated model comprising a base model trained, validated, and manually verified on older datasets from older time ranges; and overlay models trained and automatically verified on newer datasets from newer time ranges (e.g., more recent than older time ranges). The base model can be trained using a base model training dataset that is a first subset of the older dataset and can be validated using a base model test dataset that is a second subset of the older dataset. The overlay model can be trained using an overlay model training dataset that is a first subset of the newer dataset and can be validated using an overlay model test dataset that is a second subset of the newer dataset.
Predictions from the integrated model can correspond to a combination of base model predictions and overlay model predictions, as well as an offset toward the base model predictions. The offset can correspond to how much the base model prediction affects the integrated model prediction. The bias can be determined by model weights corresponding to parameters for optimizing the integrated model. The model weights can also indicate when the overlay model does not contribute enough or too much to the integrated model, and when the base model begins to perform poorly so that it should be replaced.
Retraining the integration model can be implemented in the field of spam detection to protect advertising metrics, such as advertising revenue, by identifying and filtering out invalid traffic. The invalid traffic can include robotic traffic, click earn traffic, or other types of abusive traffic. Spam detection can identify publishers of invalid traffic as fraudulent so that they can terminate or stop providing advertisements to them from the advertising network. Spam detection can also identify publishers that are a mixture of invalid traffic and natural traffic, such that invalid traffic can be filtered out while natural traffic can be allowed. In this way, various aspects of the present disclosure can be applied to such technical areas as detecting fraudulent activity within the context of advertisements. Importantly, a machine learning model (such as an integrated model) performs correctly and accurately when applied in the context of fraudulent activity detection, enabling fraudulent traffic to be accurately filtered out while allowing efficient and legitimate traffic. Thus, the disclosed system and method of retraining an integrated model provides the technical effect of enabling more accurate fraud detection.
Retraining the integrated model can also be implemented in other areas where there is a high risk of making mistakes, where model verification involves manual review, and/or data fluctuations over time. For example, other areas can include identifying financial fraud, monitoring engineering security, and computer security for sensitive businesses. Accordingly, the disclosed systems and methods provide improved accuracy of machine learning models for a wide range of technical applications.
For the overlay model, the newer data set can be separated into training and testing data sets. The overlay model is trained with the training data set and is automatically validated by computing a plurality of metrics on the test data set. The plurality of metrics can include classification performance and stability.
The classification performance can correspond to how well the machine learning model performs classification using the test dataset. Classification performance can be determined by, for example, the area under the receiver operating characteristic curve (AUCROC), the area under the precision recall curve (AUCPR), or the F-score. The classification performance of the overlay model should be comparable to the base model, wherein how far the overlay model can deviate from the base model in terms of classification performance is domain-specific. For example, in the field of spam detection, AUCPR of the base model can be 0.53 and AUCPR of the overlay model can be 0.49, while the integrated model can have a higher AUCPR, such as 0.55. It should be noted that these differences in classification performance are merely examples, and the integrated model can be pushed towards production as long as the integrated model has higher classification performance than the base model or previous models in production. Pushed to production can include being deployed for real world use as a web service, as offline batch predictions, or as embedded on edge/mobile devices. In other words, being pushed towards production means that the integrated model is in a real world setting for use in the intended application. For example, when the integrated model is pushed towards production, this may mean that the integrated model is used as a web service or embedded on a user device to identify and filter fraudulent online activities.
Because the base model is fully trained, including manual verification rather than just automatic verification, the integrated model predictions should be biased toward the base model predictions even if the overlay model is trained on newer data. The optimal model weight corresponding to the optimal bias amount given to the base model can be determined by determining the maximum classification performance of the integrated model in the search interval from the start weight value to 1 using a search method such as a grid search or a general line search with a step size of 0.05. The step size is domain specific and can be in the range of 0.01 to 0.1, depending on the sensitivity of the classification performance to the weight values. The starting weight value can control the minimum bias of the base model and can be domain specific. For example, in the field of spam detection, the starting weight value can be 0.7. The starting weight value may be greater than 0.5 to provide a greater bias toward the base model.
If the optimal weight is within the search interval and less than 1, the optimal weight indicates that both the overlay model and the base model contribute significantly to the integrated model prediction. Thus, the integrated model can be a good candidate to be pushed towards production. In the field of spam detection, the optimal weight can be less than or equal to 0.95. If the optimal weight is about 1 or equal to 1, the optimal weight indicates that the overlay model contributes little or no to the integrated model prediction. Thus, the integrated model should not be pushed towards production, and the overlay model should be retrained in a wider time frame that includes more recent data. If the optimal weight is about or equal to the starting weight value, the optimal weight indicates that the base model contributes little or no to the integrated model prediction. Thus, the base model may need to be replaced, including new model training, verification, and manual verification.
Stability can correspond to the degree of stability of the machine learning model with respect to time. Stability can evaluate whether the predicted distribution from a previous machine learning model already in production is comparable to the predicted distribution from the integrated model. For example, stability can be determined by a Population Stability Index (PSI) or a feature stability index (CSI).
The stability metric may be domain specific. For example, in the field of spam detection, the difference in the total number of entities marked by the previous model and the retrained integrated model in production should be less than or equal to a threshold percentage, such as 10%. The tagged entity can correspond to an entity having a risk score that exceeds a decision threshold. As another example in the field of spam detection, the number of entities (such as 20) that are highest ranked in terms of cost by the previous model and the retrained integrated model in production should not differ by more than a threshold percentage (such as 10%). The marked entities can be ranked in reverse order by cost, with the number of entities that precede being the highest cost weighted ranked entity. As yet another example in the field of spam detection, the dollar amount of advertising revenue difference for a tagged entity between a previous model in production and a retrained integrated model should not differ by more than a threshold percentage, such as 15%. The above example can be beneficial in the field of spam detection by avoiding instances where the highest priced entity is labeled by the retrained model and not by the underlying model. These highest cost-rated entities that are labeled by the model that are retrained and not by the underlying model will have a higher likelihood of false positives, particularly for some users in the spam detection field that are less likely to receive spam.
If multiple metrics pass verification, the integrated model can be pushed towards production. For classification performance, the optimal weights should lie within the range of the search interval. For the stability metric, the difference between the previous model in production and the retrained integrated model should be less than or equal to the percentage threshold.
FIG. 1 depicts a block diagram of an example integrated model generation system 100. The integrated model generation system 100 can be configured to receive input data according to a user interface. For example, the integrated model generation system 100 can receive data as part of a call to an Application Programming Interface (API) exposing the integrated model generation system 100. The integrated model generation system 100 can be implemented on one or more computing devices. The input to the integrated model generation system 100 can be provided, for example, through a storage medium (including remote storage connected to one or more computing devices over a network) or through a user interface on a client computing device coupled to the integrated model generation system 100 as an input.
The integrated model generation system 100 can be configured to receive training data 102 for generating a base model, retraining data 104 for generating an overlay model, and performance data 106 for determining an amount of bias to provide to the base model. The integrated model generation system 100 can be configured to implement techniques for generating an integrated model having an offset towards a base model, as will be described further below.
The training data 102 can correspond to data used to generate a base model. The training data 102 can be in any form suitable for training a base model according to one of a variety of different learning techniques. Learning techniques for training the base model can include supervised learning, unsupervised learning, and semi-supervised learning techniques. For example, training data 102 can include a plurality of training samples that can be received as input by a base model. When processing the labeled training examples, the training examples can be labeled with the desired output of the base model. The marker and model outputs can be evaluated by an evaluation metric, wherein the evaluation metric can be counter-propagated through the base model to update the weights of the base model. Training can correspond to a complete process of generating a base model, which can include selecting the most predictive signal, generating features of the signal, finding optimal hyper-parameters, training to find optimal weights, and verifying base model performance on a test dataset. Features can be normalized, such as for gradient descent methods.
The retraining data 104 can correspond to data used to generate an overlay model. The retraining data 104 can be derived from a newer data set from a new time range as compared to the training data 102, wherein the training data 102 can be derived from an older data set from an older time range. The retraining data 104 can be in any form suitable for training an overlay model according to one of a variety of different learning techniques. Learning techniques for training the overlay model can include supervised learning, unsupervised learning, and semi-supervised learning techniques. For example, the retraining data 104 can include a plurality of retraining examples that can be received as input by an overlay model. When processing the labeled training examples, the desired output of the overlay model can be utilized to label the retraining examples. The marker and model outputs can be evaluated by an evaluation metric, wherein the evaluation metric can be counter-propagated through the overlay model to update the weight of the overlay model. Retraining can correspond to a finer process than training.
The performance data 106 can correspond to data used to determine the amount of bias to be provided to the base model. The performance data 106 can include data of classification performance related to how well the integrated model can perform classification. The maximum classification performance of the integrated model can correspond to the optimal bias to be given to the base model.
The integrated model generation system 100 can be configured to output an integrated model 108 having a determined bias toward the base model based on the training data 102, the retraining data 104, and the performance data 106. The integrated model 108 can be sent as an output, for example, displayed on a user display. The integrated model generation system 100 can be configured to provide the integrated model 108 as a set of computer-readable instructions, such as one or more computer programs. A computer program can be written in any type of programming language and in accordance with any programming paradigm (e.g., declarative, procedural, assembly, object oriented, data oriented, functional, imperative programming paradigm, etc.). A computer program can be written to perform one or more different functions and operate within a computing environment (e.g., on a physical device, virtual machine, or across multiple devices). Computer programs can also implement the functions described in this specification, for example, functions performed by a system, engine, module, or model.
The integrated model generation system 100 can include a training engine 110. The training engine 110 can generate the base model by a process that involves selecting the most predictive signal, generating features of the signal, finding optimal superparameters, training to find optimal weights, and verifying the base model performance on the test dataset. Features can be normalized, such as for gradient descent methods.
The classification performance of the underlying model is expected to degrade over time due to changes in the underlying input data distribution. The base model can be monitored to ensure that it performs well enough to continue to function. To improve model performance on the new data, the base model can be periodically (e.g., monthly or every two months) retrained. The base model can be retrained at a frequency such that the new data does not deviate too much from the initial training data, but still deviate enough such that retrained does not waste resources.
The integrated model generation system 100 can include a retraining engine 112. The retraining engine 112 is capable of generating an overlay model through a retraining process. Retraining should improve classification performance for more recent data without requiring manual verification. Retraining can be similar to training, but omitting signal selection to instead rely on the same signal being selected during training. Retraining can also omit feature engineering, but reuse feature engineering decisions made during training. Retraining can also reduce hyper-parameter tuning, but reuse model architecture that was found to be optimal during training, but can alter learning rates and optimization algorithms to achieve better prediction stability.
FIG. 2 depicts an example time diagram 200 of model training and retraining based on model classification performance degradation. After training n, model classification performance deteriorates over time. Thus, the model is tuned through several periodic cycles of model retraining (depicted by retraining 1 through k). Although only two retraining blocks are depicted in fig. 2 for simplicity, any number of retraining cycles can exist between exercises. Eventually, the model performance becomes inadequate, requiring the generation of a new model by training n+1. When the classification performance of the model falls below the degradation threshold, the model may become insufficient. Other triggers for training the new model may be adding or removing features or changing training algorithms.
Referring back to the retraining engine 112 of FIG. 1, as part of retraining to generate an overlay model, data from entities is reused at different times. Thus, each sample in the retraining data 104 can contain three parts: keys, features, and indicia. To manage these examples and successfully generate overlay models, these examples can have certain properties. First, the entity can have unique and stable keys to be used correctly in multiple retraining runs and to avoid data contamination between models. Second, the tags and features can have a specific temporal relationship. Since the features of the sample are typically aggregated, the data for the labels should not overlap with the feature aggregation window. Third, to minimize feedback, the existing positive labels of the entities should not be known during the feature aggregation window.
The samples used by the retraining engine 112 for retraining, testing, and verification are specified by keys and points in time. The set of samples used for retraining runs can correspond to data rounds (epochs). Each round contains a training segmentation, a test segmentation, and a verification segmentation. Since retraining and verification occur in different rounds, the verification metrics should not be skewed by moving the entity between divisions from one round to the next. Thus, segmentation can be done deterministically in the key space. Additionally, the training segmentations can be separated in time from the test and validation segmentations to further minimize the impact of actions taken during the training segmentations on the test segmentations.
Because the concept of risk varies slightly between different vertical industries, the retraining engine 112 may not know how to evaluate risk for any particular entity. In general, risk can correspond to the likelihood of future actions, and high quality markers are created through various manual and automatic processes, so the temporal separation between feature extraction and marker generation can capture this intuition.
The data round can correspond to a time window from the beginning of the training feature window to the test mark collection time. For each retraining run, the data runs may be combined into a single coherent set of samples over which the coverage model is optimized. The set can contain data from the entire time since the last retraining. However, each physical key can transform over time, undergoing the following lifecycle: birth-the first time an entity generates traffic; detection-detection of invalid traffic from an entity; action-entity is enforced in some way and is enforced as a positive marker, which is now a positive example of training; and dead or dormant-the entity no longer generates traffic.
Each physical bond can potentially produce both positive and negative examples. The aggregation method may select the most recently available sample for any entity. This prevents multiple training on the same key and allows for maximum sample of positive samples, as even filtered entities that no longer produce traffic will be included as long as the turn has not been included in the previous retraining summary.
The integrated model generation system 100 can include a bias engine 114. The bias engine 114 can determine the amount of bias to provide to the base model. The retrained model can correspond to an integration of the base model and the overlay model. The base model can correspond to a fully trained, tuned, and manually reviewed model. The overlay model can correspond to a model trained on a newer data set, where the newer data set can be the same set of signals as collected during the newer time frame for the base model.
To perform the inference, the integrated model predictive score can be calculated as follows:
gα＝(1-α)xi+αyi (1)
Where x i and y i are the various support teams for the overlay model and the base model, respectively, of the entity, and α is an adjustable model weight that can correspond to the bias amount to be provided to the base model.
The base model can remain in the integrated model to take advantage of manual review during initial startup. When the time to retrain comes, the overlay model can be replaced with a subsequent overlay model trained on a more recent round. Model weights can be automatically adjusted via bias engine 114 using grid search based on classification performance from classification performance data 106. Classification performance can be determined by, for example, the area under the receiver operating characteristic curve (AUCROC), the area under the precision recall curve (AUCPR), or the F-score. The classification performance of the overlay model should be comparable to the base model, but as long as the integrated model has higher classification performance than the base model or previous models in production, the integrated model can be output 108 from the integrated model generation system 100 to be pushed toward production.
The model weight corresponding to the optimal bias amount to be given to the base model can be determined by determining the maximum classification performance of the integrated model in the search interval from the start weight value to 1 using the search method. Example search methods can include grid searches or general line searches with steps ranging from 0.01 to 0.1, depending on the sensitivity of classification performance to weight values. The starting weight value can control the minimum bias of the base model and can be greater than 0.5 to provide more bias toward the base model. This utilizes a fully trained manual review from the base model, as compared to automatic verification from the retraining of the overlay model. Thus, a more accurate integrated model can be generated based on benefits achieved from complete training of the base model as well as benefits achieved by the integrated model trained on newer data sets.
When the model weight is within the search interval and less than 1 (e.g., in the range of 0.55 to 0.95), the model weight can indicate that both the overlay model and the base model contribute significantly to the integrated model prediction. The two model contributions are significant and can indicate that the integrated model is truly outputting predictions, where the predictions are a combination of predictions from the overlay model and the base model. When the contribution of both the overlay model and the base model is above the respective threshold amounts such that the model weight is greater than the starting weight value but less than 1, both models can contribute significantly. When both the overlay model and the base model contribute significantly to the integrated model, the integrated model can be output 108 as candidates to be pushed to production.
When the model weight is about 1 or equal to 1, the model weight can indicate that the overlay model contributes little or no to the integrated model prediction. The minimal or no contribution of the overlay model can indicate that the integrated model is substantially a base model. When the contribution of the overlay model is below a threshold amount, the overlay model may be without significant contribution. When the overlay model contributes little or no, the overlay model should be retrained via the retrainer engine 112 using a wider turn including updated near data.
When the model weight is about or equal to the starting weight value, the model weight can indicate that the base model contributes little or no to the integrated model prediction. Minimal or no contribution from the base model can indicate that the integrated model is essentially an overlay model. When the contribution of the base model is below a threshold amount, the base model may be without significant contribution. When the base model contributes little or no, the base model may need to be replaced via the training engine 110, including new model training, verification, and manual verification.
The integrated model generation system 100 can include a stability engine 116. The stability engine 116 can determine the stability of the integrated model. Stability can correspond to the degree of stability of the machine learning model with respect to time. The stability engine 116 can evaluate whether the predicted distribution from a previous integrated model already in production is comparable to the predicted distribution from the current integrated model that is output 108. For example, stability can be determined by a Population Stability Index (PSI) or a feature stability index (CSI).
To improve stability and combat the feedback loop, the stability engine 116 can implement a signature enhancement. The previous integration model is used to generate additional positive labels for the newer data sets. By utilizing the previous integration model, information about the currently detected pattern and previous optimizations can be transferred to the integration model that is being output 108. This can purposefully bias the retraining metrics towards existing evaluations and penalize overlay models that deviate too far from the base model during retraining. The set of sample tags can be determined by the following equation:
Laug＝L∪{e|gα(e)>τ} (2)
Where τ is the calibration threshold and e is the entity. The sample calibration threshold may be 0.75, 0.9, or 0.99, but may also be a lower value for rare event predictions, such as 0.01. Adding samples with predictions above the calibration threshold to the subsequent training set can increase the likelihood that the newer integrated model will behave similarly to the previous integrated model.
When the integrated model obtains an output 108 to be pushed to production, it becomes the subject of production monitoring. Classification performance may be monitored to determine whether the integrated model should be retrained by retrained engine 112 or fully trained by training engine 110.
FIG. 3 depicts a flowchart of an example process 300 for retraining a machine learning model using an integrated model. The example process 300 can be performed on a system of one or more processors at one or more locations, such as the integrated model generation system 100 of fig. 1.
As indicated at block 310, the integrated model generation system 100 is capable of training a base model of an integrated model. The base model can be trained on older data sets from older time-scales to determine optimal weights for the base model. As indicated at block 320, the integrated model generation system 100 can automatically verify a base model of the integrated model using test data from the older data set. Automatic verification can include rules that ensure that the model is performing as expected, such as whether the overall model accuracy is above a threshold. The base model may be manually inspected as indicated in block 330. Manual inspection may include balancing factors such as sacrificing percent accuracy in a geographic area to achieve a generally higher percent accuracy.
After a period of time, the integrated model generation system 100 may train an overlay model of the integrated model, as shown in block 340. The overlay model may be trained on newer data sets from newer time ranges as compared to older data sets to determine optimal weights for the overlay model. As indicated at block 350, the integrated model generation system can automatically verify the overlay model. For the overlay model, the newer data set may be partitioned into a training data set and a test data set. The overlay model can be trained with the training data set and can be automatically validated by computing a plurality of metrics on the test data set. The plurality of metrics may include classification performance and stability. The classification performance may correspond to how well the machine learning model performs classification using the test dataset. Classification performance may be determined by, for example, the area under the receiver operating characteristic curve (AUCROC), the area under the precision recall curve (AUCPR), or the F-score. Stability may correspond to the degree of stability of the machine learning model with respect to time. Stability can evaluate whether the predicted distribution from a previous machine learning model already in production is comparable to the predicted distribution from the integrated model. For example, stability may be determined by a Population Stability Index (PSI) or a feature stability index (CSI).
As indicated at block 360, the integrated model generation system 100 can determine an amount of bias to provide to a base model of the integrated model based on the classification performance. The maximum classification performance of the integrated model in the search interval from the starting weight value to 1 can determine the model weight corresponding to the optimal bias given to the base model. The starting weight value may be greater than 0.5 to provide a greater bias toward the base model. If the model weight is within the range of the search interval and less than 1, the model weight may indicate that both the overlay model and the base model contribute significantly to the integrated model prediction. Thus, the integrated model may be a good candidate to be pushed towards production. If the optimal weight is about 1 or equal to 1, the optimal weight indicates that the overlay model contributes little or no to the integrated model prediction. Thus, the integrated model should not be pushed towards production, and the overlay model should be retrained over a wider time frame including updated near data. If the optimal weight is about or equal to the starting weight value, the optimal weight indicates that the base model contributes little or no to the integrated model prediction. Thus, the base model may need to be replaced, including new model training, verification, and manual verification.
As shown in block 370, the integrated model generation system 100 can determine the stability of the integrated model. The classification performance of the overlay model should be comparable to the base model, e.g., within a threshold percentage, because the classification performance of the overlay model should not deviate too far from the base model. Furthermore, the classification performance of the integrated model should be comparable to the integrated model previously in production, e.g. within a threshold percentage.
When the model weight is within a sufficient range between the starting weight and 1 and/or the integrated model is sufficiently comparable to the integrated model previously in production, the integrated model generation system 100 may push the integrated model toward production, as shown in block 380. Pushing to production may include being deployed for real world use, as a web service, as offline batch predictions, or as embedded on edge/mobile devices.
Retraining the machine learning model via the integrated model may be implemented in the field of spam detection to protect advertising revenue by identifying and filtering out invalid traffic. The invalid traffic may include robotic traffic, click earn traffic, or other types of abuse. Spam detection can identify publishers of invalid traffic as fraudulent so that they can terminate or stop providing advertisements to them from the advertising network. Spam detection can also identify publishers that are a mixture of invalid traffic and natural traffic, such that invalid traffic can be filtered out while natural traffic can be allowed. In this way, the disclosed methods and systems for retraining a machine learning model have the technical effect of producing a more accurate machine learning model that can be more efficient at filtering fraudulent activity in a spam detection context.
Integrated model retraining may also be implemented in other areas where errors can have greater consequences, where model verification involves manual review and/or where data fluctuates over time. In other words, the disclosed methods and systems for retraining a machine learning model can yield a more accurate machine learning model for use in a variety of different technical applications. For example, other areas may include identifying financial fraud, monitoring engineering security, and computer security for sensitive businesses.
For identifying financial fraud, the data may fluctuate over time as fraudsters generate new fraud techniques or recover older fraud techniques to evade detection. Furthermore, the real world consequences of false positive and false negative cases can be expensive in terms of both monetary value and user satisfaction, so classification accuracy when data changes is particularly important. Similarly, for monitoring the safety of engineering construction, the accuracy of classification when data changes is critical to avoiding the expensive real world consequences of false positive and negative cases. For computer security, bad actors can adapt defenses to circumvent detection, and false positive and false negative cases can also lead to expensive real world consequences.
For the spam detection field, the integrated model generation system 100 can train, validate, and manually verify the underlying model to distinguish between valid traffic and invalid traffic. After a period of time, the underlying model may degrade in its ability to distinguish between valid traffic and invalid traffic for spam detection. The integrated model generation system 100 is capable of training and automatically validating the overlay model to more accurately distinguish between valid traffic and invalid traffic. The overlay model can be trained and validated on newer data than the base model.
The integrated model may include a combination of base and overlay models for their ability to distinguish between active and inactive flows, as well as a determined bias provided to the base model. The integrated model generation system 100 can determine that the model weights can correspond to the offsets. For the spam detection field, the model weights can be determined by the maximum ability of the integrated model to distinguish between valid traffic and invalid traffic in a search interval from an example starting weight value of 0.7 to 1. The maximum capability in the search interval can be determined using a search method such as a grid search or a line search with an example step size of 0.05.
If the model weight is in the range of 0.75 to 0.95, the model weight may indicate that both the overlay model and the base model have significant impact on the integrated model's ability to distinguish between effective and ineffective traffic. Thus, the integrated model may be a good candidate for spam detection to be pushed towards production. If the model weight is in the range of 0.95 to 1, the optimal weight may indicate that the overlay model contributes little or no to the integrated model's ability to distinguish between valid and invalid traffic. Thus, the integrated model should not be pushed towards production, and the overlay model should be retrained over a wider time frame including updated near data. If the model weights are in the range of 0.7 to 0.75, the model weights may indicate that the base model contributes little or no to the integrated model's ability to distinguish between valid and invalid flows. Thus, the base model may need to be replaced, including new model training, verification, and manual verification. It should be noted that the ranges described herein are exemplary and that the integrated model may be candidates for pushing towards production as long as the model weights are in the range between the starting value and 1 and the performance metrics of the integrated model are improved compared to the base model or the overlay model itself.
The integrated model generation system 100 is also capable of determining whether the integrated model is sufficiently stable compared to an integrated model previously in production. For example, the difference in the total number of inactive traffic entities of the integrated model previously in production and the current integrated model should be less than or equal to a threshold percentage, such as 5%, 10%, or 15%. An invalid traffic entity may correspond to an entity having a risk score exceeding a decision threshold. As another example, the number of entities (such as 5, 10, or 20) that are highest ranked in terms of cost by the integrated model previously in production and the current integrated model should not differ by more than a threshold percentage, such as 10%. The marked entities may be ranked in reverse order by cost, with the number of entities that precede being the highest cost weighted ranked entity. As yet another example, the dollar amount of advertising revenue difference for an inactive traffic entity between an integrated model that was previously in production and a current integrated model should not differ by more than a threshold percentage, such as 10%, 15%, or 20%. The above example can avoid situations where the highest priced entities are untrained model marked by the underlying model because those highest priced entities have a higher likelihood of false positives.
FIG. 4 depicts a block diagram of an example environment 400 for retraining a machine learning model using an integrated model. Environment 400 can be implemented on one or more devices having one or more processors at one or more locations, such as in a server computing device 402. The client computing device 404 and the server computing device 402 can be communicatively coupled to one or more storage devices 406 through a network 408. The storage device 406 may be a combination of volatile and nonvolatile memory and can be in the same or different physical location as the computing devices 402, 404. For example, storage device 406 can include any type of computer-readable medium capable of storing information, such as hard drives, solid state drives, magnetic tape drives, optical storage, memory cards, ROM, RAM, DVD, CD-ROM, writable memory, and read-only memory. The computer readable medium is optionally non-transitory.
The server computing device 402 can include one or more processors 410 and memory 412. Memory 412 is capable of storing information accessible by processor 410, including instructions 414 capable of being executed by processor 410. The memory 412 can also include data 416 that can be retrieved, manipulated, or stored by the processor 410. Memory 412 may be one type of computer-readable medium, such as volatile and non-volatile memory, which is optionally non-transitory, capable of storing information accessible by processor 410. The processor 410 can include one or more Central Processing Units (CPUs), graphics Processing Units (GPUs), field Programmable Gate Arrays (FPGAs), and/or Application Specific Integrated Circuits (ASICs), such as Tensor Processing Units (TPUs).
The instructions 414 can include one or more instructions that, when executed by the processor 410, cause the one or more processors to perform actions defined by the instructions. The instructions 414 can be stored in an object code format for direct processing by the processor 410, or in other formats including interpretable scripts or a collection of independent source code modules that are interpreted or precompiled as needed. The instructions 414 can include instructions for implementing the integrated model generation system 418, wherein the integrated model generation system 418 can correspond to the integrated model generation system 100 of fig. 1. The integrated model generation system 418 can be executed using the processor 410 and/or using other processors remote from the server computing device 402.
The data 416 can be retrieved, stored, or modified by the processor 410 according to the instructions 414. The data 416 can be stored in a computer register, in a relational or non-relational database as a table with a plurality of different fields and records, or as JSON, YAML, proto or XML documents. The data 416 can also be formatted in a computer-readable format such as, but not limited to, binary values, ASCII, or Unicode. In addition, the data 416 can include information sufficient to identify relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memory (including other network locations), or information used by a function to calculate relevant data.
Client computing device 404 can also be configured similarly to server computing device 402, with one or more processors 420, memory 422, instructions 424, and data 426. The client computing device 404 can also include user input 428 and user output 430. User input 428 can include any suitable mechanism or technique for receiving input from a user, such as a keyboard, mouse, mechanical actuator, soft actuator, touch screen, microphone, and sensor.
The server computing device 402 can be configured to send data to the client computing device 404, and the client computing device 404 can be configured to display at least a portion of the received data on a display implemented as part of the user output 430. The user output 430 can also be used to display an interface between the client computing device 404 and the server computing device 402. The user output 430 can alternatively or additionally include one or more speakers, transducers, or other audio outputs, a haptic interface that provides non-visual and non-audible information to a platform user of the client computing device 404, or other haptic feedback.
Although fig. 4 shows processors 410, 420 and memories 412, 422 as being within computing devices 402, 404, the components described herein can include multiple processors and memories that can operate in different physical locations and that are not within the system computing device. For example, some of the instructions 414, 424 and data 416, 426 can be stored on a removable SD card, while other instructions are stored within a read-only computer chip. Some or all of the instructions and data can be stored in a location physically remote from the processors 410, 420 but still accessible to the processors 410, 420. Similarly, the processors 410, 420 can include a collection of processors capable of performing concurrent and/or sequential operations. The computing devices 402, 404 can each include one or more internal clocks that provide timing information that can be used for time measurement of operations and programs run by the computing devices 402, 404.
The server computing device 402 can be connected to a data center 432 housing hardware accelerators 432A-N through a network 408. Data center 432 may be one of a plurality of data centers or other facilities where various types of computing devices, such as hardware accelerators, are located. As described herein, the computing resources housed in the data center 432 can be designated for deploying the integration model.
The server computing device 402 can be configured to receive a request to process data 426 from the client computing device 404 on a computing resource in the data center 432. For example, environment 400 may be part of a computing platform configured to provide various services to users through various user interfaces and/or APIs that expose the platform services. The one or more services may be a machine learning framework or a set of tools for generating and/or utilizing a spam detection neural network or other machine learning spam detection model and distributing the results of the spam detection. The client computing device 404 is capable of receiving and transmitting data for generating an integrated model for spam detection. The integrated model generation system 418 can receive data and in response generate one or more integrated models for spam detection.
As other examples of potential services provided by the platform implementing the environment 400, the server computing device 402 can maintain various integration models according to different spam detection policies or other implementations. For example, the server computing device 402 can maintain different families for deploying neural networks on various types of TPUs and/or GPUs housed in the data center 432 or otherwise available for processing.
Devices 402, 404 and data center 432 can have the capability to communicate directly and indirectly over network 408. For example, using a web socket, client computing device 404 can connect to a service operating in data center 432 via an internet protocol. The devices 402, 404 are able to establish a listening socket that can accept an originating connection for sending and receiving information. The network 408 itself can include various configurations and protocols including the Internet, the world Wide Web, intranets, virtual private networks, wide area networks, local networks, and private networks using communication protocols proprietary to one or more companies. The network 408 can support various short-range and long-range connections. Short-range and long-range connections can be made over different bandwidths, such as generally withStandard-associated 2.402GHz to 2.480GHz, typically associated with/>2.4GHz and 5GHz associated with communication protocols; or have various communication standards, such as/>, for wireless broadband communicationAnd (5) standard. Additionally or alternatively, the network 408 can also support wired connections between the devices 402, 404 and the data center 432, including through various types of ethernet connections.
While a single server computing device 402, client computing device 404, and data center 432 are shown in fig. 4, it will be appreciated that aspects of the disclosure can be implemented in accordance with a variety of different configurations and numbers of computing devices, including in examples for sequential or parallel processing, or through a distributed network of multiple devices. In some implementations, aspects of the present disclosure can be performed on a single device connected to a hardware accelerator configured to process neural networks, and any combination thereof.
The foregoing alternative examples are not mutually exclusive, unless otherwise specified, but may be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject matter defined by the claims, the foregoing description of the embodiments should be taken by way of illustration rather than by way of limitation of the subject matter defined by the claims. In addition, the provision of examples described herein and terms of expressions such as "such as," "including," etc. should not be construed as limiting the subject matter of the claims to a particular example; rather, these examples are intended to be illustrative of only one of many possible embodiments. Furthermore, the same reference numbers in different drawings may identify the same or similar elements.
Claims (33)
1. A computer-implemented method for retraining a machine learning model, comprising:
generating, with the one or more processors, an integrated model comprising a base model and an overlay model, wherein the base model has been trained using the first data set;
training, with the one or more processors, the overlay model with a training data set, wherein the training data set is a first subset of a second data set that is more recent than the first data set;
Validating, with the one or more processors, the trained overlay model with the test dataset using the plurality of metrics, wherein the test dataset is a second subset of the second dataset;
Determining, with the one or more processors and based on the plurality of metrics, an amount of bias to be provided to the base model as compared to the overlay model when performing the prediction using the integrated model; and
The method further includes performing, with the one or more processors, a prediction using the integrated model based on a combination of the prediction from the base model and the prediction from the overlay model, and the determined bias toward the prediction from the base model.
2. The computer-implemented method of claim 1, further comprising: the base model is trained using a base model training dataset, wherein the base model training dataset is a first subset of the first dataset, using the one or more processors.
3. The computer-implemented method of claim 2, further comprising:
Validating, with the one or more processors, the base model with a base model test dataset, wherein the base model test dataset is a second subset of the first dataset; and
The base model is manually inspected using the base model test dataset of the first dataset.
4. The computer-implemented method of any preceding claim, wherein the plurality of metrics includes classification performance.
5. The computer-implemented method of claim 4, wherein determining the bias amount further comprises using a search method to determine a maximum classification performance of the integrated model in the search interval.
6. The computer-implemented method of claim 5, wherein the search interval includes a starting weight value to a weight value of 1, the starting weight value being greater than 0.5.
7. The computer-implemented method of claim 5, further comprising: the one or more processors are configured to determine, based on the determined bias amount, that the determined bias amount is within a range within the search interval in which both the overlay model and the base model contribute more than a threshold amount to the integrated model prediction.
8. The computer-implemented method of claim 5, further comprising:
Determining, with the one or more processors, that the determined bias amount is within a range within the search interval in which the coverage model contributes less than a threshold amount to the integrated model prediction; and
The overlay model is retrained with the second data set using the one or more processors.
9. The computer-implemented method of claim 5, further comprising:
determining, with the one or more processors, that the determined bias amount is within a range within the search interval in which the contribution of the base model to the integrated model prediction is below a threshold amount; and
The base model is replaced with one or more processors.
10. The computer-implemented method of any of claims 1-3, wherein the plurality of metrics includes stability.
11. The computer-implemented method of claim 10, further comprising determining, with the one or more processors, that the integrated model prediction is within a threshold as compared to predictions from a previous model, such that the integrated model is pushed toward production.
12. A system, comprising:
one or more processors; and
One or more storage devices coupled to the one or more processors and storing instructions that, when executed by the one or more processors, cause the one or more processors to perform operations for retraining a machine learning model, the operations comprising:
Generating an integrated model, the integrated model comprising a base model and an overlay model, wherein the base model has been trained using a first data set;
Training the overlay model with a training dataset, wherein the training dataset is a first subset of a second dataset that is more recent than the first dataset;
Validating the trained overlay model with a test dataset using the plurality of metrics, wherein the test dataset is a second subset of the second dataset;
determining an amount of bias to be provided to the base model as compared to the overlay model when performing the prediction using the integrated model based on the plurality of metrics; and
Based on a combination of predictions from the base model and predictions from the overlay model, and the determined bias toward predictions from the base model, predictions are performed using the integrated model.
13. The system of claim 12, wherein the operations further comprise training a base model with a base model training dataset, wherein the base model training dataset is a first subset of the first dataset.
14. The system of claim 13, wherein the operations further comprise:
Validating the base model with the base model test dataset, wherein the base model test dataset is a second subset of the first dataset; and
The base model is manually inspected using the base model test dataset of the first dataset.
15. The system of claim 12, wherein the plurality of metrics includes classification performance.
16. The system of claim 15, wherein determining the bias amount further comprises determining a maximum classification performance of the integrated model in the search interval using a search method.
17. The system of claim 16, wherein the search interval includes a start weight value to a weight value of 1, the start weight value being greater than 0.5.
18. The system of claim 16, wherein the operations further comprise determining that the determined bias amount is within a range within the search interval in which both the overlay model and the base model contribute more than a threshold amount to the integrated model prediction.
19. The system of claim 16, wherein the operations further comprise:
determining that the determined bias amount is within a range within the search interval in which the coverage model contributes less than a threshold amount to the integrated model prediction; and
The overlay model is retrained using the second data set.
20. The system of claim 16, wherein the operations further comprise:
Determining that the determined bias amount is within a range within the search interval in which the contribution of the base model to the integrated model prediction is below a threshold amount; and
Replacing the base model.
21. The system of claim 12, wherein the plurality of metrics includes stability.
22. The system of claim 21, wherein the operations further comprise determining that the integrated model prediction is within a threshold value as compared to predictions from a previous model such that the integrated model is pushed toward production.
23. A computer-readable medium storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations for retraining a machine learning model, the operations comprising:
Generating an integrated model, the integrated model comprising a base model and an overlay model, wherein the base model has been trained using a first data set;
Training the overlay model with a training dataset, wherein the training dataset is a first subset of a second dataset that is more recent than the first dataset;
Validating the trained overlay model with a test dataset using the plurality of metrics, wherein the test dataset is a second subset of the second dataset;
Determining an amount of bias to be provided to the base model as compared to the overlay model when performing the prediction using the integrated model based on the plurality of metrics; and
Based on a combination of predictions from the base model and predictions from the overlay model, and the determined bias toward predictions from the base model, predictions are performed using the integrated model.
24. The computer-readable medium of claim 23, wherein the operations further comprise training a base model with a base model training dataset, wherein the base model training dataset is a first subset of the first dataset.
25. The computer-readable medium of claim 24, wherein the operations further comprise:
Validating the base model with the base model test dataset, wherein the base model test dataset is a second subset of the first dataset; and
The base model is manually inspected using the base model test dataset of the first dataset.
26. The computer readable medium of claim 23, wherein the plurality of metrics includes classification performance.
27. The computer readable medium of claim 26, wherein determining the bias amount further comprises using a search method to determine a maximum classification performance of the integrated model in the search interval.
28. The computer readable medium of claim 27, wherein the search interval includes a start weight value to a weight value of 1, the start weight value being greater than 0.5.
29. The computer-readable medium of claim 27, wherein the operations further comprise determining that the determined bias amount is within a range within the search interval in which both the overlay model and the base model contribute more than a threshold amount to the integrated model prediction.
30. The computer-readable medium of claim 27, wherein the operations further comprise:
determining that the determined bias amount is within a range within the search interval in which the coverage model contributes less than a threshold amount to the integrated model prediction; and
The overlay model is retrained using the second data set.
31. The computer-readable medium of claim 27, wherein the operations further comprise:
Determining that the determined bias amount is within a range within the search interval in which the contribution of the base model to the integrated model prediction is below a threshold amount; and
Replacing the base model.
32. The computer readable medium of claim 23, wherein the plurality of metrics includes stability.
33. The computer-readable medium of claim 32, wherein the operations further comprise determining that the integrated model prediction is within a threshold value as compared to predictions from a previous model such that the integrated model is pushed toward production.
Publications (1)
Publication Number | Publication Date |
---|---|
CN118056212A true CN118056212A (en) | 2024-05-17 |
Family
ID=
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Zurell et al. | Do joint species distribution models reliably detect interspecific interactions from co‐occurrence data in homogenous environments? | |
EP3985578A1 (en) | Method and system for automatically training machine learning model | |
CN107633265B (en) | Data processing method and device for optimizing credit evaluation model | |
US11818163B2 (en) | Automatic machine learning vulnerability identification and retraining | |
US20190378010A1 (en) | Unsupervised machine learning system to automate functions on a graph structure | |
US10438297B2 (en) | Anti-money laundering platform for mining and analyzing data to identify money launderers | |
US20220114399A1 (en) | System and method for machine learning fairness testing | |
CN109978033B (en) | Method and device for constructing same-operator recognition model and method and device for identifying same-operator | |
CN111612165A (en) | Predictive analysis platform | |
CN108182515B (en) | Intelligent rule engine rule output method, equipment and computer readable storage medium | |
Vasudevan et al. | When does dough become a bagel? analyzing the remaining mistakes on imagenet | |
CN110263157B (en) | Data risk prediction method, device and equipment | |
CN108876213B (en) | Block chain-based product management method, device, medium and electronic equipment | |
KR20160068620A (en) | Abnormal pattern analysis method, abnormal pattern analysis apparatus performing the same and storage media storing the same | |
Garrido et al. | A Robust profit measure for binary classification model evaluation | |
CN112927061A (en) | User operation detection method and program product | |
CN110866832A (en) | Risk control method, system, storage medium and computing device | |
CN112883990A (en) | Data classification method and device, computer storage medium and electronic equipment | |
CN116414815A (en) | Data quality detection method, device, computer equipment and storage medium | |
CN117094184B (en) | Modeling method, system and medium of risk prediction model based on intranet platform | |
Pu et al. | Beyond artificial reality: Finding and monitoring live events from social sensors | |
CN111582722B (en) | Risk identification method and device, electronic equipment and readable storage medium | |
CN118056212A (en) | Retraining method with automatic verification for machine learning model | |
US20190156160A1 (en) | Method for classifying user action sequence | |
CN113988226B (en) | Data desensitization validity verification method and device, computer equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication |