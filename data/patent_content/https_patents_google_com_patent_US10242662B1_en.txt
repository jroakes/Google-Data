US10242662B1 - Biasing voice correction suggestions - Google Patents
Biasing voice correction suggestions Download PDFInfo
- Publication number
- US10242662B1 US10242662B1 US15/789,575 US201715789575A US10242662B1 US 10242662 B1 US10242662 B1 US 10242662B1 US 201715789575 A US201715789575 A US 201715789575A US 10242662 B1 US10242662 B1 US 10242662B1
- Authority
- US
- United States
- Prior art keywords
- transcription
- search query
- user interface
- graphical user
- receiving
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/01—Assessment or evaluation of speech recognition systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/187—Phonemic context, e.g. pronunciation rules, phonotactical constraints or phoneme n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G10L15/265—
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/221—Announcement of recognition results
Definitions
- This specification relates to natural language processing.
- Conventional devices can include software to respond to speech of a user of the device.
- the speech can typically include instructions to the device to call a phone number, text a phone number, or search for information on the mobile device or the Internet.
- the device can employ conventional speech to text processes to recognize a voice input from the user.
- This specification describes technologies for correction of voice recognition outputs. These technologies generally involve providing a recognition output to a received voice input that includes a misrecognition, receiving a user input modifying the recognition output, and providing suggested corrected recognition outputs.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a voice input from a user device; generating a recognition output; receiving a user selection of one or more terms in the recognition output; receiving a user input of one or more letters replacing the user selected one or more terms; determining suggested correction candidates based in part on the user input and the voice input; and providing one or more suggested correction candidates to the user device as suggested corrected recognition outputs.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions.
- one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- the method further includes receiving a user selection of one of the suggested corrected recognition outputs; and providing one or more search results responsive to the selected suggested query.
- Determining suggested correction candidates includes: obtaining candidate suggestions for a corrected recognition output based on a collection of queries, wherein the candidate suggestions are ranked according to a respective score for each candidate suggestion; re-ranking the candidate suggestions based on phonetic similarity between each candidate suggestion and the received voice input; and determining one or more suggested correction candidates based on the re-ranking.
- Obtaining candidate suggestions includes determining queries that match a portion of the recognition output with the user input to a collection of queries and ranking the queries based on a respective popularity.
- Determining phonetic similarity includes calculating a phonetic distance between each candidate suggestion and the received voice input.
- Re-raking the candidate suggestions includes modifying the respective scores of the candidate suggestions based on whether the corresponding phonetic similarity satisfies a threshold.
- the method further includes: receiving additional user input letters for the one or more terms; and in response to the additional user input, updating the suggested corrected recognition outputs
- Misrecognized voice inputs can be corrected with biased correction to a particular recognized term without the user starting the voice recognition process over with a new voice input.
- the biased correction allows the user's intended voice input to be identified more quickly.
- Correcting voice input improves voice recognition versatility, e.g., in performing a search in response to a voice query, or voice aided word processing.
- FIG. 1 is an illustration of a voice search flow including a biased correction in an example user interface.
- FIG. 2 is a block diagram of a system for providing corrections to voice queries.
- FIG. 3 is a flow diagram of an example process for correcting a voice input.
- FIG. 4 is a flow diagram of an example process for determining a suggested correction.
- FIG. 1 is an illustration 100 of a voice search including a biased correction in an example search user interface.
- a search user interface is presented as one example of using the voice recognition correction techniques described in this specification.
- a search system provides a search user interface 102 a for displaying on a user device 106 , e.g., a mobile phone, tablet, or other user device configured to receive voice input.
- the user interface 102 a includes a search box 104 for receiving a query and a microphone icon 108 for receiving a first voice query from a user instead of a typed input.
- the user inputs a voice query 101 , e.g., by selecting the microphone icon 108 in the user interface 102 a .
- the user interface includes a keyboard layout that can be presented for typed input to the search box.
- the search system receives the voice query 101 from a user.
- the user provides a voice input of “Baroque pictures” to the user device 106 .
- a user interface 102 b is presented including a display of a recognition output 112 generated from the voice query in the search box 104 .
- the recognition output 112 is [rock pictures].
- a speech-to-text process can be performed on the first voice query to generate a purported transcription as the corresponding recognition output.
- the search system can provide the transcription to the user device 106 for display in the user interface 102 b.
- the recognition output can be used by a search system.
- the search system provides search results 110 responsive to the query [rock pictures] for display in user interface 102 b .
- the search results 110 can include links to particular resources determined by the search system to be responsive to the recognized query.
- the recognized query was misrecognized.
- the user in response to the misrecognition, can interact with the user interface 102 c to select a portion 114 of the recognition output 112 corresponding to the misrecognized term “rock”. For example, in a touch interface the user can touch the misrecognized term with a finger or stylus. In a non-touch interface, the user can navigate a cursor to the misrecognized term and select the term e.g., with a mouse input.
- the user manually inputs one or more text characters into the search field 104 for the selected misrecognition term, in this case a letter “b.”
- the partial query “ b pictures” one or more corrected queries are determined and provided to the user device as suggested queries 116 .
- the suggested queries 116 can be selected based on a popularity of candidate queries as well as phonetic features of the candidate queries relative to the voice input, as described in greater detail below.
- the suggested queries can be updated. For example, in user interface 102 e , the user has manually input two letters of the misrecognized term as “ba”.
- the suggestions for corrected queries 118 for “ba pictures” include “baroque pictures” 119 , which was the user's initial voice input. Varying numbers of letters may be needed to identify the correct suggestion.
- the user can select a particular query suggestion of the displayed suggested corrected queries 118 , e.g., using a touch or cursor input. In response to the selection, the suggested search query is used as a corrected search query by the search system.
- the corrected search query “Baroque pictures” is displayed in the search field 104 .
- the search system provides search results 120 responsive to corrected query [baroque pictures] for presentation in the user interface 102 f
- the search results 120 can include links to particular resources determined by the search system to be responsive to the corrected query.
- FIG. 2 is a block diagram of a system 200 for providing corrections to voice queries. Other suitable systems can be provided for correcting other types of voice input.
- the system 200 provides search results relevant to submitted queries as can be implemented in an internet, an intranet, or another client and server environment.
- the system 200 is an example of an information retrieval system in which the systems, components, and techniques described below can be implemented.
- a user 202 can interact with a search system 214 through a client device 204 .
- the client 204 can be a computer coupled to the search system 214 through a local area network (LAN) or wide area network (WAN), e.g., the Internet.
- the search system 214 and the client device 204 can be one machine.
- a user can install a desktop search application on the client device 204 .
- the client device 204 will generally include a random access memory (RAM) 206 and a processor 208 .
- RAM random access memory
- the user 202 can submit voice queries 210 to a search engine 230 within a search system 214 .
- the voice query 210 is transmitted through a network to the search system 214 .
- the search system 214 can be implemented as, for example, computer programs running on one or more computers in one or more locations that are coupled to each other through a network.
- the search system 214 includes a voice recognition engine 254 .
- the voice recognition engine receives the voice query 210 and transcribes the voice query to a recognized query, e.g., using suitable text-to-speech techniques.
- the recognized query is returned to the client device 204 for presentation to the user 202 . Additionally, the recognized query is provided to the search engine 230 .
- the search system 214 further includes an index database 222 and a search engine 230 .
- the search system 214 responds to an input query by generating search results 228 , which are transmitted through the network to the client device 204 in a form that can be presented to the user 202 (e.g., as a search results web page to be displayed in a web browser running on the client device 204 ).
- the search engine 230 identifies resources that match, or are responsive to, the query 210 .
- the search engine 230 will generally include an indexing engine 220 that indexes resources (e.g., web pages, images, or news articles on the Internet) found in a corpus (e.g., a collection or repository of content), an index database 222 that stores the index information, and a ranking engine 252 (or other software) to rank the resources that match the query 210 .
- the indexing and ranking of the resources can be performed using conventional techniques.
- the search engine 130 can transmit the search results 228 through the network to the client device 204 for presentation to the user 202 .
- the search system also includes a voice correction engine 256 .
- the correction engine 256 corrects the recognized query in response to a correction input from the user 202 .
- the voice correction engine 256 can determine that a correction input, e.g., a selection and text input associated with a particular term of the recognized query is received following presentation of the recognition output.
- the voice correction engine 256 can determine candidate corrected queries, score the candidate correction queries, and provide one or more corrected queries to the user device 204 based on the scores, e.g., top n ranked candidate correction queries. In some implementations, corrected queries need to satisfy a threshold score to be provided.
- Determining candidate corrected queries can include using a suggest engine 258 to identify candidate search queries based on the recognition output and the user input correcting a portion of the recognition output.
- the suggest engine 258 can be part of the search system 214 as in FIG. 2 or can be a separate system in communication with the search system 214 .
- one or more of the voice recognition engine 256 or the voice correction engine 256 can be part of a separate system in communication with the search system 214 .
- the voice recognition engine, suggest engine, and voice correction engine 256 can be part of a separate system from the search system 214 , for example, when used for voice inputs other than search queries.
- the candidate search queries identified by the suggest engine 258 can be further processed by the voice recognition engine 254 and voice query correction engine 256 to identify one or more ranked candidate corrected queries to present to the user device 204 .
- the search engine 230 identifies resources that are responsive to the particular corrected query.
- FIG. 3 is a flowchart of an example process 300 for correcting a voice input.
- the process 300 will be described as being performed by a system of one or more computers, located in one or more locations, and programmed appropriately in accordance with this specification.
- a search system e.g., the search system 214 of FIG. 2 , appropriately programmed, can perform the process 300 .
- the system receives a voice input from a user device ( 302 ).
- a voice input indicator e.g., a microphone indicator
- a search user interface of the user device can include a microphone indicator allowing a user to submit a voice query to a search system.
- a microphone of the user device captures voice input from the user.
- the voice input is then transmitted by the user device to the system.
- the voice input can be, for example, “baroque pictures.”
- the system generates a recognition output ( 304 ).
- the recognition output is a transcription of the received voice input.
- a suitable speech to text technique can be used to provide voice recognition of the voice input and convert it into a recognized text output.
- the speech to text technique can include the use of an acoustic model that identifies phonemes or other linguistic units from the audio of the voice input and a language model that assigns probabilities to particular words or sequences of words.
- the speech to text technique can correct or compensate for errors in the voice input, e.g., based on spelling and/or grammar rules.
- the recognition output is provided to the user device, for example, for display in a user interface.
- the recognition output can be displayed, for example, to indicate the system's recognition of the voice input.
- the user can then examine the presented recognition output to determine whether the system correctly recognized the voice input.
- the voice input “baroque pictures” may be recognized as [rock pictures]. Here the word “baroque” was misrecognized as “rock
- the system can perform an action responsive to the first recognition output. For example, for a search query voice input, the system can obtain search results using the first recognition output as a search query. One or more search results responsive to the search query can be provided to the user device for display in the search interface.
- the action is part of a particular task, e.g., creating a calendar entry.
- the first recognition output corresponds to the action, e.g., inputting text in a text editor or e-mail.
- the action is to provide an output text, e.g., in response to user dictation, corresponding to the recognition output.
- the output can be provided, for example, to a word processor or e-mail program.
- the system receives a user selection of one or more terms of the recognition output ( 306 ).
- the user selection can be made by direct input, e.g., using a finger on a touch screen, or using a particular input device e.g., a mouse cursor or stylus.
- the user can touch a particular term or terms of the recognition output.
- the selected term or terms can be highlighted by the user device indicating the selected term or terms.
- the user can select the word “rock.”
- the system receives a user input of one or more letters replacing the selected term or terms of the recognition output ( 308 ).
- a keyboard can be presented in a touch interface. The user can then enter one or more letters.
- the user can use a keyboard to provide input to replace the selected one or more terms of the recognition output.
- the user can input a letter “b”.
- the initial letter may result in a correct suggestion being presented.
- the user can type additional letters if a suggestion is not presented or if a correct suggestion is not presented. For example, the user may type “baro” before the correct suggestion is presented. Suggestions can be obtained in response to each letter entry by the user.
- the system determines suggestions for a corrected voice recognition ( 310 ).
- the system can determine the suggested corrected voice recognitions from the user input terms based on a suggest system and phonetic analysis of candidates provided by the suggest system. Determining suggested corrected voice recognitions is described in greater detail below with respect to FIG. 4 .
- the system provides one or more top ranked suggested corrected voice recognitions to the user device as suggested corrected recognition outputs 312 for presentation to the user.
- the user interface can include a drop down list of suggested corrected recognition outputs below the modified form of the recognition output.
- a specified number of top suggested corrected recognition outputs can be provided for presentation to the user.
- each corrected recognition output provided satisfies a threshold score value.
- An individual corrected recognition output can be selected by the user in a similar manner as the selections described above, e.g., a user touch input or input from a user input device.
- the system determines whether one of the suggested corrected recognition outputs is selected ( 314 ). If a suggested corrected recognition output is selected (YES branch of decision 314 ), an action is performed based on the selection ( 316 ).
- the action can include, for example, submitting the selected suggested corrected recognition to a search system as a search query and obtaining search results responsive to the search query.
- the process can iteratively identify suggested corrected recognition outputs as the system receives additional letters input by the user. For example, after entering “ ⁇ right arrow over (b) ⁇ pictures” the suggested corrected recognition outputs may not match the user's voice input. However, after entering “ba pictures” the suggested queries can include “baroque pictures,” which corresponds to the user's intended voice input.
- the system identifies suggested corrected voice recognitions in response to the user selection of one or more terms in the recognition output and prior to any user input of one or more letters.
- the system can determine the suggested corrected voice recognitions from the unselected terms in the recognition output and based on a suggest system and phonetic analysis of candidates provided by the suggest system
- FIG. 4 is a flow diagram of an example process 400 for determining a suggested correction.
- the process 400 will be described as being performed by a system of one or more computers, located in one or more locations, and programmed appropriately in accordance with this specification.
- the system receives user input modifying a recognition output ( 402 ).
- the recognition output can be generated in response to a voice input, e.g., of a search query, provided by a user to a user device.
- a voice input e.g., of a search query
- the user can provide an input selecting a particular term or terms of the recognition output.
- the user can then manually input one or more letters replacing the selected term or terms.
- the user can input a first letter of a corrected word that should replace a misrecognized term of the recognition output.
- the system obtains candidate suggestions for a corrected recognition output, e.g., a corrected search query ( 404 ).
- a corrected recognition output e.g., a corrected search query ( 404 ).
- the modified form of the recognition output following the user input is provided to a suggest system.
- the suggest system uses the portion of the recognition output to determine one or more candidate corrected recognition outputs.
- the candidate corrected recognition outputs predict a completed recognition output based on the user modifications.
- the suggest system can compare the modified output to a collection of queries.
- the collection of queries can include queries submitted by a collection of different users over a specified amount of time.
- the comparison can identify queries in the collection of queries that match the modified portion of the recognition output. For example, if the recognition output “rock pictures” is modified by the user as “b pictures” the suggest system identifies queries in the collection that includes a word beginning with the letter “b” and including the word “pictures.” From the matching queries, a specified number of top ranking queries can be selected as candidate corrected recognition outputs. From the example of “b pictures,” the identified top ranking candidates can include: birthday pictures, beautiful pictures, baby pictures, and bed bug pictures.
- the ranking of the queries can be determined based, for example, on a score calculated according to a popularity measure associated with the query. For example, the popularity measure can be based on how many times each query was submitted by different users of the collection of users during the specified amount of time.
- the system re-ranks the candidate suggestions ( 406 ).
- the re-ranking can be based on a phonetic similarity of the candidate suggestions relative to the user voice input.
- each candidate suggestion can be associated with a score used to initially rank the candidate suggestions.
- the score can be adjusted based on a phonetic similarity between each candidate suggestion and the received voice input.
- the phonetic similarity measures how similar the sound of the candidate suggestion is to the voice input.
- Phonetic similarity between each candidate suggestion and the received voice input can be determined using a suitable distance measure.
- the phonetic distance can be based on a minimum edit distance between the candidate suggestion and the voice input.
- the minimum edit distance measures a number of different sounds between the candidate suggestion and the voice input.
- the minimum edit distance is calculated using phonetic dictionaries.
- the minimum edit distance is calculated using acoustic models, for example, acoustic Hidden-Markov-Models.
- the phonetic representation of the original voice query when evaluating a correction request from the user can be obtained from the client device.
- the phonetic representation can be returned to the client with the recognition output.
- the client can then provide the phonetic representation with the correction request, e.g., the user selection of one or more terms of the recognition output or the entry of one or more letters replacing the selection.
- the phonetic representations can be stored by the system when determining the recognition output. The stored phonetic representation can be retrieved in response to the correction request.
- the score is boosted, e.g., by a multiplier, if the phonetic distance between the candidate suggestion and the voice input satisfies a threshold.
- the score can be reduced, e.g., by a multiplier, if the phonetic distance does not satisfy the threshold.
- the threshold is based on a number of different sounds out of a total number of sounds. For example, for a particular candidate suggestion, if the number of different sounds, absolute number or percentage, is less than the threshold then a multiplier can be applied to boost the score of that candidate suggestion.
- the candidate suggestions can be re-ranked according to the scores after any boost or reduction is applied.
- the system provides one or more re-ranked candidate suggestions to the user device as suggested corrected recognition outputs ( 408 ).
- the system provides the one or more suggested corrected recognition outputs to be presented on the user device in ranked order, e.g., a top five suggested corrected recognition outputs.
- the suggested corrected recognition outputs can be presented in a list below the user input field showing the modified recognition output.
- a total score for each candidate suggestion is calculated in a single step.
- the candidate suggestions identified e.g., by a suggest system
- the combined score can be used to rank the suggestions and identify one or more top ranked suggestions to provide to the user device as suggested corrected recognition outputs.
- each individual suggested corrected recognition output is selectable by the user, e.g., though touch or user device input.
- an action can be performed. For example, search results can be obtained and provided in response to the suggested corrected recognition output.
- the user instead of selecting one of the suggested corrected recognition outputs, the user further modifies the modified recognition output, e.g., by inputting additional letters.
- the system can identify one or more new suggested corrected recognition outputs in a similar manner as described above.
- engine will be used broadly to refer to a software based system or subsystem that can perform one or more specific functions. Generally, an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non-transitory storage medium for execution by, or to control the operation of, data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- the central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- Control of the various systems described in this specification, or portions of them, can be implemented in a computer program product that includes instructions that are stored on one or more non-transitory machine-readable storage media, and that are executable on one or more processing devices.
- the systems described in this specification, or portions of them, can each be implemented as an apparatus, method, or electronic system that may include one or more processing devices and memory to store executable instructions to perform the operations described in this specification.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the user device, which acts as a client.
- Data generated at the user device e.g., a result of the user interaction, can be received from the user device at the server.
Abstract
Methods, systems, and apparatus, including computer programs encoded on computer storage media, for natural language processing. One of the method includes receiving a voice input from a user device; generating a recognition output; receiving a user selection of one or more terms in the recognition output; receiving a user input of one or more letters replacing the user selected one or more terms; determining suggested correction candidates based in part on the user input and the voice input; and providing one or more suggested correction candidates to the user device as suggested corrected recognition outputs.
Description
This is a continuation of U.S. application Ser. No. 14/988,074, filed on Jan. 5, 2016. The disclosure of the prior application is considered part of and is incorporated by reference in the disclosure of this application.
This specification relates to natural language processing.
Conventional devices can include software to respond to speech of a user of the device. The speech can typically include instructions to the device to call a phone number, text a phone number, or search for information on the mobile device or the Internet. The device can employ conventional speech to text processes to recognize a voice input from the user.
This specification describes technologies for correction of voice recognition outputs. These technologies generally involve providing a recognition output to a received voice input that includes a misrecognition, receiving a user input modifying the recognition output, and providing suggested corrected recognition outputs.
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a voice input from a user device; generating a recognition output; receiving a user selection of one or more terms in the recognition output; receiving a user input of one or more letters replacing the user selected one or more terms; determining suggested correction candidates based in part on the user input and the voice input; and providing one or more suggested correction candidates to the user device as suggested corrected recognition outputs. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
The foregoing and other embodiments can each optionally include one or more of the following features, alone or in combination. In particular, one embodiment includes all the following features in combination. The method further includes receiving a user selection of one of the suggested corrected recognition outputs; and providing one or more search results responsive to the selected suggested query. Determining suggested correction candidates includes: obtaining candidate suggestions for a corrected recognition output based on a collection of queries, wherein the candidate suggestions are ranked according to a respective score for each candidate suggestion; re-ranking the candidate suggestions based on phonetic similarity between each candidate suggestion and the received voice input; and determining one or more suggested correction candidates based on the re-ranking. Obtaining candidate suggestions includes determining queries that match a portion of the recognition output with the user input to a collection of queries and ranking the queries based on a respective popularity. Determining phonetic similarity includes calculating a phonetic distance between each candidate suggestion and the received voice input. Re-raking the candidate suggestions includes modifying the respective scores of the candidate suggestions based on whether the corresponding phonetic similarity satisfies a threshold. The method further includes: receiving additional user input letters for the one or more terms; and in response to the additional user input, updating the suggested corrected recognition outputs
The subject matter described in this specification can be implemented in particular embodiments so as to realize one or more of the following advantages. Misrecognized voice inputs can be corrected with biased correction to a particular recognized term without the user starting the voice recognition process over with a new voice input. The biased correction allows the user's intended voice input to be identified more quickly. Correcting voice input improves voice recognition versatility, e.g., in performing a search in response to a voice query, or voice aided word processing.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Like reference numbers and designations in the various drawings indicate like elements.
In a user session, the search system receives the voice query 101 from a user. In particular, the user provides a voice input of “Baroque pictures” to the user device 106. In response to the voice query, a user interface 102 b is presented including a display of a recognition output 112 generated from the voice query in the search box 104. The recognition output 112 is [rock pictures]. For example, a speech-to-text process can be performed on the first voice query to generate a purported transcription as the corresponding recognition output. The search system can provide the transcription to the user device 106 for display in the user interface 102 b.
Furthermore, the recognition output can be used by a search system. The search system provides search results 110 responsive to the query [rock pictures] for display in user interface 102 b. For example, the search results 110 can include links to particular resources determined by the search system to be responsive to the recognized query. However, in this example, the recognized query was misrecognized.
As shown in user interface 102 c, in response to the misrecognition, the user can interact with the user interface 102 c to select a portion 114 of the recognition output 112 corresponding to the misrecognized term “rock”. For example, in a touch interface the user can touch the misrecognized term with a finger or stylus. In a non-touch interface, the user can navigate a cursor to the misrecognized term and select the term e.g., with a mouse input.
As shown in user interface 102 d, the user manually inputs one or more text characters into the search field 104 for the selected misrecognition term, in this case a letter “b.” Using the partial query “b pictures,” one or more corrected queries are determined and provided to the user device as suggested queries 116. The suggested queries 116 can be selected based on a popularity of candidate queries as well as phonetic features of the candidate queries relative to the voice input, as described in greater detail below.
If the user continues to type in letters for the misrecognized term, the suggested queries can be updated. For example, in user interface 102 e, the user has manually input two letters of the misrecognized term as “ba”. The suggestions for corrected queries 118 for “ba pictures” include “baroque pictures” 119, which was the user's initial voice input. Varying numbers of letters may be needed to identify the correct suggestion. The user can select a particular query suggestion of the displayed suggested corrected queries 118, e.g., using a touch or cursor input. In response to the selection, the suggested search query is used as a corrected search query by the search system.
As shown in user interface 102 f, the corrected search query “Baroque pictures” is displayed in the search field 104. Furthermore, in response to the corrected search query, the search system provides search results 120 responsive to corrected query [baroque pictures] for presentation in the user interface 102 f For example, the search results 120 can include links to particular resources determined by the search system to be responsive to the corrected query.
A user 202 can interact with a search system 214 through a client device 204. For example, the client 204 can be a computer coupled to the search system 214 through a local area network (LAN) or wide area network (WAN), e.g., the Internet. In some implementations, the search system 214 and the client device 204 can be one machine. For example, a user can install a desktop search application on the client device 204. The client device 204 will generally include a random access memory (RAM) 206 and a processor 208.
The user 202 can submit voice queries 210 to a search engine 230 within a search system 214. When the user 202 submits a voice query 210, the voice query 210 is transmitted through a network to the search system 214. The search system 214 can be implemented as, for example, computer programs running on one or more computers in one or more locations that are coupled to each other through a network.
The search system 214 includes a voice recognition engine 254. The voice recognition engine receives the voice query 210 and transcribes the voice query to a recognized query, e.g., using suitable text-to-speech techniques. In some implementations, the recognized query is returned to the client device 204 for presentation to the user 202. Additionally, the recognized query is provided to the search engine 230.
The search system 214 further includes an index database 222 and a search engine 230. The search system 214 responds to an input query by generating search results 228, which are transmitted through the network to the client device 204 in a form that can be presented to the user 202 (e.g., as a search results web page to be displayed in a web browser running on the client device 204).
When the recognized query determined from the voice query 210 is received by the search engine 230, the search engine 230 identifies resources that match, or are responsive to, the query 210. The search engine 230 will generally include an indexing engine 220 that indexes resources (e.g., web pages, images, or news articles on the Internet) found in a corpus (e.g., a collection or repository of content), an index database 222 that stores the index information, and a ranking engine 252 (or other software) to rank the resources that match the query 210. The indexing and ranking of the resources can be performed using conventional techniques. The search engine 130 can transmit the search results 228 through the network to the client device 204 for presentation to the user 202.
The search system also includes a voice correction engine 256. The correction engine 256 corrects the recognized query in response to a correction input from the user 202. In particular, the voice correction engine 256 can determine that a correction input, e.g., a selection and text input associated with a particular term of the recognized query is received following presentation of the recognition output. The voice correction engine 256 can determine candidate corrected queries, score the candidate correction queries, and provide one or more corrected queries to the user device 204 based on the scores, e.g., top n ranked candidate correction queries. In some implementations, corrected queries need to satisfy a threshold score to be provided. Determining candidate corrected queries can include using a suggest engine 258 to identify candidate search queries based on the recognition output and the user input correcting a portion of the recognition output. The suggest engine 258 can be part of the search system 214 as in FIG. 2 or can be a separate system in communication with the search system 214. Similarly, one or more of the voice recognition engine 256 or the voice correction engine 256 can be part of a separate system in communication with the search system 214. Also, in other implementations, the voice recognition engine, suggest engine, and voice correction engine 256 can be part of a separate system from the search system 214, for example, when used for voice inputs other than search queries.
The candidate search queries identified by the suggest engine 258 can be further processed by the voice recognition engine 254 and voice query correction engine 256 to identify one or more ranked candidate corrected queries to present to the user device 204. In response to a user input to the user device 204 selecting a particular corrected query, the search engine 230 identifies resources that are responsive to the particular corrected query.
The system receives a voice input from a user device (302). For example, the user can select a voice input indicator, e.g., a microphone indicator, associated with a particular user interface indicating a voice input can be provided. For example, a search user interface of the user device can include a microphone indicator allowing a user to submit a voice query to a search system. When selected by the user, a microphone of the user device captures voice input from the user. The voice input is then transmitted by the user device to the system. The voice input can be, for example, “baroque pictures.”
The system generates a recognition output (304). The recognition output is a transcription of the received voice input. For example, a suitable speech to text technique can be used to provide voice recognition of the voice input and convert it into a recognized text output. The speech to text technique can include the use of an acoustic model that identifies phonemes or other linguistic units from the audio of the voice input and a language model that assigns probabilities to particular words or sequences of words. In some implementations, the speech to text technique can correct or compensate for errors in the voice input, e.g., based on spelling and/or grammar rules. The recognition output is provided to the user device, for example, for display in a user interface. The recognition output can be displayed, for example, to indicate the system's recognition of the voice input. The user can then examine the presented recognition output to determine whether the system correctly recognized the voice input. For example, the voice input “baroque pictures” may be recognized as [rock pictures]. Here the word “baroque” was misrecognized as “rock.”
Additionally, the system can perform an action responsive to the first recognition output. For example, for a search query voice input, the system can obtain search results using the first recognition output as a search query. One or more search results responsive to the search query can be provided to the user device for display in the search interface. In some implementations, the action is part of a particular task, e.g., creating a calendar entry. In some other implementations, the first recognition output corresponds to the action, e.g., inputting text in a text editor or e-mail. In some other implementations, the action is to provide an output text, e.g., in response to user dictation, corresponding to the recognition output. The output can be provided, for example, to a word processor or e-mail program.
The system receives a user selection of one or more terms of the recognition output (306). The user selection can be made by direct input, e.g., using a finger on a touch screen, or using a particular input device e.g., a mouse cursor or stylus. For example, the user can touch a particular term or terms of the recognition output. In response, the selected term or terms can be highlighted by the user device indicating the selected term or terms. Thus, for a recognition input “rock pictures,” the user can select the word “rock.”
The system receives a user input of one or more letters replacing the selected term or terms of the recognition output (308). For example, in response to the user selection of one or more terms of the recognition output, a keyboard can be presented in a touch interface. The user can then enter one or more letters. In some other implementations, the user can use a keyboard to provide input to replace the selected one or more terms of the recognition output.
In the example of “rock pictures” after selecting the term “rock” the user can input a letter “b”. In some implementations, as described below, the initial letter may result in a correct suggestion being presented. However, the user can type additional letters if a suggestion is not presented or if a correct suggestion is not presented. For example, the user may type “baro” before the correct suggestion is presented. Suggestions can be obtained in response to each letter entry by the user.
The system determines suggestions for a corrected voice recognition (310). The system can determine the suggested corrected voice recognitions from the user input terms based on a suggest system and phonetic analysis of candidates provided by the suggest system. Determining suggested corrected voice recognitions is described in greater detail below with respect to FIG. 4 .
The system provides one or more top ranked suggested corrected voice recognitions to the user device as suggested corrected recognition outputs 312 for presentation to the user. For example, the user interface can include a drop down list of suggested corrected recognition outputs below the modified form of the recognition output. A specified number of top suggested corrected recognition outputs can be provided for presentation to the user. In some other implementations, each corrected recognition output provided satisfies a threshold score value. An individual corrected recognition output can be selected by the user in a similar manner as the selections described above, e.g., a user touch input or input from a user input device.
The system determines whether one of the suggested corrected recognition outputs is selected (314). If a suggested corrected recognition output is selected (YES branch of decision 314), an action is performed based on the selection (316). The action can include, for example, submitting the selected suggested corrected recognition to a search system as a search query and obtaining search results responsive to the search query.
If the system determines that one of the suggested corrected recognition outputs is not selected (NO branch of decision 314), the user can input one or more additional letters replacing the selected one or more terms shown by a return path to 308. Thus, the process can iteratively identify suggested corrected recognition outputs as the system receives additional letters input by the user. For example, after entering “{right arrow over (b)} pictures” the suggested corrected recognition outputs may not match the user's voice input. However, after entering “ba pictures” the suggested queries can include “baroque pictures,” which corresponds to the user's intended voice input.
In some implementations, the system identifies suggested corrected voice recognitions in response to the user selection of one or more terms in the recognition output and prior to any user input of one or more letters. The system can determine the suggested corrected voice recognitions from the unselected terms in the recognition output and based on a suggest system and phonetic analysis of candidates provided by the suggest system
The system receives user input modifying a recognition output (402). The recognition output can be generated in response to a voice input, e.g., of a search query, provided by a user to a user device. For example, as described above with respect to FIGS. 1-3 , the user can provide an input selecting a particular term or terms of the recognition output. The user can then manually input one or more letters replacing the selected term or terms. For example, the user can input a first letter of a corrected word that should replace a misrecognized term of the recognition output.
The system obtains candidate suggestions for a corrected recognition output, e.g., a corrected search query (404). In some implementations, the modified form of the recognition output following the user input is provided to a suggest system. The suggest system uses the portion of the recognition output to determine one or more candidate corrected recognition outputs. In particular, the candidate corrected recognition outputs predict a completed recognition output based on the user modifications.
For example, the suggest system can compare the modified output to a collection of queries. The collection of queries can include queries submitted by a collection of different users over a specified amount of time. The comparison can identify queries in the collection of queries that match the modified portion of the recognition output. For example, if the recognition output “rock pictures” is modified by the user as “b pictures” the suggest system identifies queries in the collection that includes a word beginning with the letter “b” and including the word “pictures.” From the matching queries, a specified number of top ranking queries can be selected as candidate corrected recognition outputs. From the example of “b pictures,” the identified top ranking candidates can include: birthday pictures, beautiful pictures, baby pictures, and bed bug pictures. The ranking of the queries can be determined based, for example, on a score calculated according to a popularity measure associated with the query. For example, the popularity measure can be based on how many times each query was submitted by different users of the collection of users during the specified amount of time.
The system re-ranks the candidate suggestions (406). The re-ranking can be based on a phonetic similarity of the candidate suggestions relative to the user voice input. In particular, each candidate suggestion can be associated with a score used to initially rank the candidate suggestions. The score can be adjusted based on a phonetic similarity between each candidate suggestion and the received voice input. The phonetic similarity measures how similar the sound of the candidate suggestion is to the voice input. Phonetic similarity between each candidate suggestion and the received voice input can be determined using a suitable distance measure. For example, the phonetic distance can be based on a minimum edit distance between the candidate suggestion and the voice input. In some implementations, the minimum edit distance measures a number of different sounds between the candidate suggestion and the voice input. In some implementations, the minimum edit distance is calculated using phonetic dictionaries. In some other implementations, the minimum edit distance is calculated using acoustic models, for example, acoustic Hidden-Markov-Models.
The phonetic representation of the original voice query when evaluating a correction request from the user can be obtained from the client device. For example, the phonetic representation can be returned to the client with the recognition output. The client can then provide the phonetic representation with the correction request, e.g., the user selection of one or more terms of the recognition output or the entry of one or more letters replacing the selection. In some other implementations, the phonetic representations can be stored by the system when determining the recognition output. The stored phonetic representation can be retrieved in response to the correction request.
In some implementations, the score is boosted, e.g., by a multiplier, if the phonetic distance between the candidate suggestion and the voice input satisfies a threshold. Alternatively, in some other implementations, the score can be reduced, e.g., by a multiplier, if the phonetic distance does not satisfy the threshold. In some implementations the threshold is based on a number of different sounds out of a total number of sounds. For example, for a particular candidate suggestion, if the number of different sounds, absolute number or percentage, is less than the threshold then a multiplier can be applied to boost the score of that candidate suggestion. The candidate suggestions can be re-ranked according to the scores after any boost or reduction is applied.
The system provides one or more re-ranked candidate suggestions to the user device as suggested corrected recognition outputs (408). In some implementations, the system provides the one or more suggested corrected recognition outputs to be presented on the user device in ranked order, e.g., a top five suggested corrected recognition outputs. For example, the suggested corrected recognition outputs can be presented in a list below the user input field showing the modified recognition output.
In some implementations, a total score for each candidate suggestion is calculated in a single step. For example, the candidate suggestions identified, e.g., by a suggest system, can be scored based on a combination of the suggest score and a score adjustment based on the phonetic similarity of each candidate suggestion relative to the user voice input. The combined score can be used to rank the suggestions and identify one or more top ranked suggestions to provide to the user device as suggested corrected recognition outputs.
In some implementations, each individual suggested corrected recognition output is selectable by the user, e.g., though touch or user device input. After selection of a particular suggested corrected recognition output, an action can be performed. For example, search results can be obtained and provided in response to the suggested corrected recognition output.
In some implementations, instead of selecting one of the suggested corrected recognition outputs, the user further modifies the modified recognition output, e.g., by inputting additional letters. In response, the system can identify one or more new suggested corrected recognition outputs in a similar manner as described above.
In this specification the term “engine” will be used broadly to refer to a software based system or subsystem that can perform one or more specific functions. Generally, an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
The term “data processing apparatus” refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
Control of the various systems described in this specification, or portions of them, can be implemented in a computer program product that includes instructions that are stored on one or more non-transitory machine-readable storage media, and that are executable on one or more processing devices. The systems described in this specification, or portions of them, can each be implemented as an apparatus, method, or electronic system that may include one or more processing devices and memory to store executable instructions to perform the operations described in this specification.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the user device, which acts as a client. Data generated at the user device, e.g., a result of the user interaction, can be received from the user device at the server.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (20)
1. A method comprising:
receiving, at a user device, a voice input from a user of the user device, the voice input comprising a search query;
displaying, by the user device, an initial transcription of the search query in a graphical user interface, the initial transcription including a misrecognized term;
receiving, at the user device, a selection indication indicating a user selection in the graphical user interface of the misrecognized term of the initial transcription of the search query;
after receiving the selection indication, receiving, at the user device, a prefix inputted by the user through the graphical user interface, the prefix replacing the misrecognized term of the initial transcription of the search query; and
displaying, by the user device, a list of candidate transcriptions for the search query in the graphical user interface, the list of candidate transcription ranked according to a respective phonetic similarity between each candidate transcription and the received voice input, each candidate transcription comprising the prefix and having a respective score that satisfies a threshold score value.
2. The method of claim 1 , further comprising:
receiving, at the user device, a text character inputted by the user through the graphical user interface that follows the prefix; and
displaying, by the user device, an updated list of candidate transcriptions of the search query in the graphical user interface, the updated list of candidate transcriptions ranked according to a respective phonetic similarity between each candidate transcription in the updated list and the received voice input, each candidate transcription in the updated list comprising the prefix and the text character.
3. The method of claim 2 , wherein the text character includes a letter.
4. The method of claim 1 , further comprising:
receiving, at the user device, another selection indication indicating another user selection in the graphical user interface of one of the candidate transcriptions displayed in the graphical user interface as a corrected transcription of the search query; and
displaying, by the user device, one or more search results corresponding to the corrected transcription of the search query in the graphical user interface.
5. The method of claim 4 , further comprising, when displaying the one or more search results corresponding to the corrected transcription of the search query, displaying, by the user device, the corrected transcription of the search query in the graphical user interface of the user device.
6. The method of claim 1 , further comprising:
prior to receiving the selection indication indicating the user selection in the graphical user interface of the misrecognized term of the initial transcription of the search query, displaying, by the user device, one or more initial search results corresponding to the initial transcription of the search query;
after displaying the list of candidate transcriptions for the search query, receiving, at the user device, another selection indication indicating another user selection in the graphical user interface of one of the displayed candidate transcriptions as a corrected transcription of the search query; and
updating, by the user device, the one or more initial search results corresponding to the initial transcription of the search query displayed in the graphical user interface with one or more corrected search results corresponding to the corrected transcription of the search query.
7. The method of claim 1 , further comprising, in response to receiving the selection indication of the user selection in the graphical user interface of the misrecognized term, superimposing, by the user device, a graphical indicator in the graphical user interface that indicates the selection of the misrecognized term of the initial transcription of the search query.
8. A system comprising:
one or more computers of a user device and one or more storage devices of the user device storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:
receiving a voice input from a user of the user device, the voice input comprising a search query;
displaying an initial transcription of the search query in a graphical user interface, the initial transcription including a misrecognized term;
receiving a selection indication indicating a user selection in the graphical user interface of the misrecognized term of the initial transcription of the search query;
after receiving the selection indication, receiving a prefix inputted by the user through the graphical user interface, the prefix replacing the misrecognized term of the initial transcription of the search query; and
displaying a list of candidate transcriptions for the search query in the graphical user interface, the list of candidate transcriptions ranked according to a respective phonetic similarity between each candidate transcription and the received voice input, each candidate transcription comprising the prefix and having a respective score that satisfies a threshold score value.
9. The system of claim 8 , wherein the operations further comprise:
receiving a text character inputted by the user through the graphical user interface that follows the prefix; and
displaying an updated list of candidate transcriptions of the search query in the graphical user interface, the updated list of candidate transcriptions ranked according to a respective phonetic similarity between each candidate transcription in the updated list and the received voice input, each candidate transcription in the updated list comprising the prefix and the text character.
10. The system of claim 9 , wherein the text character includes a letter.
11. The system of claim 8 , wherein the operations further comprise:
receiving another selection indication indicating another user selection in the graphical user interface of the candidate transcriptions displayed in the graphical user interface as a corrected transcription of the search query; and
displaying one or more search results corresponding to the corrected transcription of the search query in the graphical user interface.
12. The system of claim 11 , wherein the operations further comprise, when displaying the one or more search results corresponding to the corrected transcription of the search query, displaying the corrected transcription of the search query in the graphical user interface of the user device.
13. The system of claim 8 , wherein the the operations further comprise:
prior to receiving the selection indication indicating the user selection in the graphical user interface of the misrecognized term of the initial transcription of the search query, displaying one or more initial search results corresponding to the initial transcription of the search query;
after displaying the list of candidate transcriptions for the corrected transcription of the search query, receiving another selection indication indicating another user selection in the graphical user interface of one of the displayed candidate transcriptions as a corrected transcription of the search query; and
updating the one or more initial search results corresponding to the initial transcription of the search query displayed in the graphical user interface with one or more corrected search results corresponding to the corrected transcription of the search query.
14. The system of claim 8 , wherein the operations further comprise, in response to receiving the selection indication indicating the user selection in the graphical user interface of the misrecognized term, superimposing, by the user device, a graphical indicator in the graphical user interface that indicates the selection of the misrecognized term of the initial transcription of the search query.
15. One or more non-transitory computer-readable storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:
receiving a voice input from a user of the user device, the voice input comprising a search query;
displaying an initial transcription of the search query in a graphical user interface, the initial transcription including a misrecognized term;
receiving a selection indication indicating a user selection in the graphical user interface of the misrecognized term of the initial transcription of the search query;
after receiving the selection indication, receiving a prefix inputted by the user through the graphical user interface, the prefix replacing the misrecognized term of the initial transcription of the search query; and
displaying a list of candidate transcriptions for the search query in the graphical user interface, the list of candidate transcriptions ranked according to a respective phonetic similarity between each candidate transcription and the received voice input, each candidate transcription comprising the prefix and having a respective score that satisfies a threshold score value.
16. The non-transitory computer-readable storage medium of claim 15 , wherein the operations further comprise:
receiving a text character inputted by the user through the graphical user interface that follows the prefix; and
displaying an updated list of candidate transcriptions of the search query in the graphical user interface, the updated list of candidate transcriptions ranked according to a respective phonetic similarity between each candidate transcription in the updated list and the received voice input, each candidate transcription in the updated list comprising the prefix and the text character.
17. The non-transitory computer-readable storage medium of claim 15 , wherein the operations further comprise:
receiving another selection indication indicating another user selection in the graphical user interface of the candidate transcriptions displayed in the graphical user interface as a corrected transcription of the search query; and
displaying one or more search results corresponding to the corrected transcription of the search query in the graphical user interface.
18. The non-transitory computer-readable storage medium of claim 17 , wherein the operations further comprise, when displaying the one or more search results corresponding to the corrected transcription of the search query, displaying the corrected transcription of the search query in the graphical user interface of the user device.
19. The non-transitory computer-readable storage medium of claim 15 , wherein the the operations further comprise:
prior to receiving the selection indication indicating the user selection in the graphical user interface of the misrecognized term of the initial transcription of the search query, displaying one or more initial search results corresponding to the initial transcription of the search query;
after displaying the list of candidate transcriptions for the corrected transcription of the search query, receiving another selection indication indicating another user selection in the graphical user interface of one of the displayed candidate transcriptions as a corrected transcription of the search query; and
updating the one or more initial search results corresponding to the initial transcription of the search query displayed in the graphical user interface with one or more corrected search results corresponding to the corrected transcription of the search query.
20. The non-transitory computer-readable storage medium of claim 15 , wherein the operations further comprise, in response to receiving the selection indication indicating the user selection in the graphical user interface of the misrecognized term, superimposing, by the user device, a graphical indicator in the graphical user interface that indicates the selection of the misrecognized term of the initial transcription of the search query.
Priority Applications (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/789,575 US10242662B1 (en) | 2016-01-05 | 2017-10-20 | Biasing voice correction suggestions |
US16/268,957 US10529316B1 (en) | 2016-01-05 | 2019-02-06 | Biasing voice correction suggestions |
US16/701,685 US10679609B2 (en) | 2016-01-05 | 2019-12-03 | Biasing voice correction suggestions |
US16/874,634 US11302305B2 (en) | 2016-01-05 | 2020-05-14 | Biasing voice correction suggestions |
US17/656,214 US11881207B2 (en) | 2016-01-05 | 2022-03-23 | Biasing voice correction suggestions |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/988,074 US10049655B1 (en) | 2016-01-05 | 2016-01-05 | Biasing voice correction suggestions |
US15/789,575 US10242662B1 (en) | 2016-01-05 | 2017-10-20 | Biasing voice correction suggestions |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/988,074 Continuation US10049655B1 (en) | 2016-01-05 | 2016-01-05 | Biasing voice correction suggestions |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/268,957 Continuation US10529316B1 (en) | 2016-01-05 | 2019-02-06 | Biasing voice correction suggestions |
Publications (1)
Publication Number | Publication Date |
---|---|
US10242662B1 true US10242662B1 (en) | 2019-03-26 |
Family
ID=63079111
Family Applications (6)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/988,074 Active 2036-03-02 US10049655B1 (en) | 2016-01-05 | 2016-01-05 | Biasing voice correction suggestions |
US15/789,575 Active 2036-01-14 US10242662B1 (en) | 2016-01-05 | 2017-10-20 | Biasing voice correction suggestions |
US16/268,957 Active US10529316B1 (en) | 2016-01-05 | 2019-02-06 | Biasing voice correction suggestions |
US16/701,685 Active US10679609B2 (en) | 2016-01-05 | 2019-12-03 | Biasing voice correction suggestions |
US16/874,634 Active US11302305B2 (en) | 2016-01-05 | 2020-05-14 | Biasing voice correction suggestions |
US17/656,214 Active US11881207B2 (en) | 2016-01-05 | 2022-03-23 | Biasing voice correction suggestions |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/988,074 Active 2036-03-02 US10049655B1 (en) | 2016-01-05 | 2016-01-05 | Biasing voice correction suggestions |
Family Applications After (4)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/268,957 Active US10529316B1 (en) | 2016-01-05 | 2019-02-06 | Biasing voice correction suggestions |
US16/701,685 Active US10679609B2 (en) | 2016-01-05 | 2019-12-03 | Biasing voice correction suggestions |
US16/874,634 Active US11302305B2 (en) | 2016-01-05 | 2020-05-14 | Biasing voice correction suggestions |
US17/656,214 Active US11881207B2 (en) | 2016-01-05 | 2022-03-23 | Biasing voice correction suggestions |
Country Status (1)
Country | Link |
---|---|
US (6) | US10049655B1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230206913A1 (en) * | 2021-06-09 | 2023-06-29 | Merlyn Mind Inc. | Multimodal Intent Entity Resolver |
US20230267918A1 (en) * | 2022-02-24 | 2023-08-24 | Cisco Technology, Inc. | Automatic out of vocabulary word detection in speech recognition |
Families Citing this family (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10049655B1 (en) | 2016-01-05 | 2018-08-14 | Google Llc | Biasing voice correction suggestions |
JP7017027B2 (en) * | 2017-03-17 | 2022-02-08 | 富士フイルムビジネスイノベーション株式会社 | Search device, search program, and search system |
US11429789B2 (en) * | 2019-06-12 | 2022-08-30 | International Business Machines Corporation | Natural language processing and candidate response identification |
JP7326931B2 (en) * | 2019-07-02 | 2023-08-16 | 富士通株式会社 | Program, information processing device, and information processing method |
CN110956958A (en) * | 2019-12-04 | 2020-04-03 | 深圳追一科技有限公司 | Searching method, searching device, terminal equipment and storage medium |
JP7332486B2 (en) * | 2020-01-08 | 2023-08-23 | 株式会社東芝 | SYMBOL STRING CONVERTER AND SYMBOL STRING CONVERSION METHOD |
KR20220124547A (en) * | 2021-03-03 | 2022-09-14 | 삼성전자주식회사 | Electronic device for correcting user's voice input and operating method for the same |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6513005B1 (en) * | 1999-07-27 | 2003-01-28 | International Business Machines Corporation | Method for correcting error characters in results of speech recognition and speech recognition system using the same |
US6735565B2 (en) | 2001-09-17 | 2004-05-11 | Koninklijke Philips Electronics N.V. | Select a recognition error by comparing the phonetic |
US6912498B2 (en) | 2000-05-02 | 2005-06-28 | Scansoft, Inc. | Error correction in speech recognition by correcting text around selected area |
US20060015338A1 (en) | 2002-09-24 | 2006-01-19 | Gilles Poussin | Voice recognition method with automatic correction |
US7356467B2 (en) | 2003-04-25 | 2008-04-08 | Sony Deutschland Gmbh | Method for processing recognized speech using an iterative process |
US20080120102A1 (en) * | 2006-11-17 | 2008-05-22 | Rao Ashwin P | Predictive speech-to-text input |
US8290772B1 (en) | 2011-10-03 | 2012-10-16 | Google Inc. | Interactive text editing |
US20140019127A1 (en) | 2012-07-12 | 2014-01-16 | Samsung Electronics Co., Ltd. | Method for correcting voice recognition error and broadcast receiving apparatus applying the same |
US8719014B2 (en) | 2010-09-27 | 2014-05-06 | Apple Inc. | Electronic device with text error correction based on voice recognition data |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6122613A (en) * | 1997-01-30 | 2000-09-19 | Dragon Systems, Inc. | Speech recognition using multiple recognizers (selectively) applied to the same input sample |
US7200555B1 (en) * | 2000-07-05 | 2007-04-03 | International Business Machines Corporation | Speech recognition correction for devices having limited or no display |
US7296019B1 (en) * | 2001-10-23 | 2007-11-13 | Microsoft Corporation | System and methods for providing runtime spelling analysis and correction |
US20060293889A1 (en) * | 2005-06-27 | 2006-12-28 | Nokia Corporation | Error correction for speech recognition systems |
US20060293890A1 (en) * | 2005-06-28 | 2006-12-28 | Avaya Technology Corp. | Speech recognition assisted autocompletion of composite characters |
US8457946B2 (en) * | 2007-04-26 | 2013-06-04 | Microsoft Corporation | Recognition architecture for generating Asian characters |
KR100919225B1 (en) * | 2007-09-19 | 2009-09-28 | 한국전자통신연구원 | The method and apparatus for post-processing conversation error using multilevel check in voice conversation system |
US8954329B2 (en) * | 2011-05-23 | 2015-02-10 | Nuance Communications, Inc. | Methods and apparatus for acoustic disambiguation by insertion of disambiguating textual information |
US9653071B2 (en) * | 2014-02-08 | 2017-05-16 | Honda Motor Co., Ltd. | Method and system for the correction-centric detection of critical speech recognition errors in spoken short messages |
US10922322B2 (en) * | 2014-07-22 | 2021-02-16 | Nuance Communications, Inc. | Systems and methods for speech-based searching of content repositories |
US10049655B1 (en) * | 2016-01-05 | 2018-08-14 | Google Llc | Biasing voice correction suggestions |
-
2016
- 2016-01-05 US US14/988,074 patent/US10049655B1/en active Active
-
2017
- 2017-10-20 US US15/789,575 patent/US10242662B1/en active Active
-
2019
- 2019-02-06 US US16/268,957 patent/US10529316B1/en active Active
- 2019-12-03 US US16/701,685 patent/US10679609B2/en active Active
-
2020
- 2020-05-14 US US16/874,634 patent/US11302305B2/en active Active
-
2022
- 2022-03-23 US US17/656,214 patent/US11881207B2/en active Active
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6513005B1 (en) * | 1999-07-27 | 2003-01-28 | International Business Machines Corporation | Method for correcting error characters in results of speech recognition and speech recognition system using the same |
US6912498B2 (en) | 2000-05-02 | 2005-06-28 | Scansoft, Inc. | Error correction in speech recognition by correcting text around selected area |
US6735565B2 (en) | 2001-09-17 | 2004-05-11 | Koninklijke Philips Electronics N.V. | Select a recognition error by comparing the phonetic |
US20060015338A1 (en) | 2002-09-24 | 2006-01-19 | Gilles Poussin | Voice recognition method with automatic correction |
US7356467B2 (en) | 2003-04-25 | 2008-04-08 | Sony Deutschland Gmbh | Method for processing recognized speech using an iterative process |
US20080120102A1 (en) * | 2006-11-17 | 2008-05-22 | Rao Ashwin P | Predictive speech-to-text input |
US8719014B2 (en) | 2010-09-27 | 2014-05-06 | Apple Inc. | Electronic device with text error correction based on voice recognition data |
US8290772B1 (en) | 2011-10-03 | 2012-10-16 | Google Inc. | Interactive text editing |
US20140019127A1 (en) | 2012-07-12 | 2014-01-16 | Samsung Electronics Co., Ltd. | Method for correcting voice recognition error and broadcast receiving apparatus applying the same |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230206913A1 (en) * | 2021-06-09 | 2023-06-29 | Merlyn Mind Inc. | Multimodal Intent Entity Resolver |
US20230267918A1 (en) * | 2022-02-24 | 2023-08-24 | Cisco Technology, Inc. | Automatic out of vocabulary word detection in speech recognition |
Also Published As
Publication number | Publication date |
---|---|
US20200105247A1 (en) | 2020-04-02 |
US20210158796A1 (en) | 2021-05-27 |
US10679609B2 (en) | 2020-06-09 |
US10529316B1 (en) | 2020-01-07 |
US11302305B2 (en) | 2022-04-12 |
US11881207B2 (en) | 2024-01-23 |
US10049655B1 (en) | 2018-08-14 |
US20220215828A1 (en) | 2022-07-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11797763B2 (en) | Allowing spelling of arbitrary words | |
US11881207B2 (en) | Biasing voice correction suggestions | |
US11682381B2 (en) | Acoustic model training using corrected terms | |
US10127909B2 (en) | Query rewrite corrections | |
US11501764B2 (en) | Apparatus for media entity pronunciation using deep learning | |
US11694033B2 (en) | Transparent iterative multi-concept semantic search | |
CN116543764A (en) | Control circuit of action mechanism and vehicle |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |