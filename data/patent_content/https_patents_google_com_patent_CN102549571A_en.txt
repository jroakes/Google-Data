CN102549571A - Landmarks from digital photo collections - Google Patents
Landmarks from digital photo collections Download PDFInfo
- Publication number
- CN102549571A CN102549571A CN2010800308493A CN201080030849A CN102549571A CN 102549571 A CN102549571 A CN 102549571A CN 2010800308493 A CN2010800308493 A CN 2010800308493A CN 201080030849 A CN201080030849 A CN 201080030849A CN 102549571 A CN102549571 A CN 102549571A
- Authority
- CN
- China
- Prior art keywords
- image
- terrestrial reference
- gram
- text
- number word
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/432—Query formulation
- G06F16/434—Query formulation using image data, e.g. images, photos, pictures taken by a user
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/587—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using geographical or spatial information, e.g. location
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/10—Terrestrial scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/70—Labelling scene content, e.g. deriving syntactic or semantic representations
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/26—Techniques for post-processing, e.g. correcting the recognition result
- G06V30/262—Techniques for post-processing, e.g. correcting the recognition result using context analysis, e.g. lexical, syntactic or semantic context
- G06V30/274—Syntactic or semantic context, e.g. balancing
Abstract
Methods and systems for automatic detection of landmarks in digital images and annotation of those images are disclosed. A method for detecting and annotating landmarks in digital images includes the steps of automatically assigning a tag descriptive of a landmark to one or more images in a plurality of text-associated digital images to generate a set of landmark- tagged images, learning an appearance model for the landmark from the set of landmark-tagged images, and detecting the landmark in a new digital image using the appearance model. The method can also include a step of annotating the new image with the tag descriptive of the landmark.
Description
Technical field
Present invention relates in general to digital image set, relate more specifically to discern the welcome terrestrial reference in the large-scale digital image set.
Background technology
Along with the use increase of digital picture, the digital memory capacity that increases and the interconnectivity that provides by digital media, the addressable large-scale further digital picture storehouse of the crowd that quantity increases day by day such as the Internet.Take the picture of various themes from the people with extensive interest of all places all over the world, and make those pictures can be used for other people for example to check on the Internet.For example, can put up (post) by the people of level of skill to web from all over the world the various terrestrial references and the digital picture of tourist destination with different pictures taken.Such picture can illustrate from different angles, the identical terrestrial reference under different condition and/or from different distance, taken.
As the designator of welcome terrestrial reference or to the guide of welcome terrestrial reference, obtainable a large amount of such images can be useful.In order to utilize the information that is included in these large-scale digital picture storehouses, necessary is that said storehouse is organized.For example, the digital picture website such as Picasa network album (from the Google in mountain scene city, California) begins in advanced menu, people can drill through downwards (drill down) to as far as its picture can with detailed theme include.As an alternative, people can search for the one or more websites with digital picture.For example, the image of the terrestrial reference that is associated with the welcome tourist destination tabulation of having announced has been downloaded in some travel information websites.
Most of traditional digital picture organization system depends on the user and marks picture.Along with a large amount of new pictures are added to these digital image sets, possibly infeasiblely be that the user increases the consistent mode handmarking's picture of complete sum of the serviceability of those digital image sets with meeting.Transferred California Google equally, be entitled as the U.S. Patent application 12/119 of " Automatic Discovery of PopularLandmarks (finding welcome terrestrial reference automatically) "; Having described in 359 can be from the automatic information extraction of these large sets system of (such as, most popular travel purpose ground).The system of in application 12/119,359, describing uses processing pipeline, and said processing pipeline comprises based on the cluster stage of geocoding with based on cluster stage of the visual signature of the coupling of image.Yet, neededly be to find terrestrial reference automatically and the image that comprises terrestrial reference carried out the additive method of note.
Summary of the invention
The method and system that is used for detecting the terrestrial reference of digital picture automatically and those images is carried out note is disclosed.In one embodiment, being used for detecting also, the ground calibration method of note digital picture may further comprise the steps: the label that will describe terrestrial reference is distributed to the one or more images in a plurality of text incidence number word images automatically.This generates terrestrial reference mark image set.Can mark the display model that image set study is used for terrestrial reference from terrestrial reference.This allows to use display model in new digital picture, to detect terrestrial reference.This method can also may further comprise the steps: the label with describing terrestrial reference carries out note to new image.
Another embodiment is used for detecting automatically the also system of the terrestrial reference of note digital picture.At least one processor that this system has the related digital image set of at least one text that is stored in the storage medium and is couple to this medium communicatedly.Processor is configured to the label of describing terrestrial reference is distributed to the one or more images in a plurality of text incidence number word images automatically.This generates terrestrial reference mark image set.Can mark the display model that image set study is used for terrestrial reference from terrestrial reference.This allows to use display model in new digital picture, to detect terrestrial reference.
Be described in detail with reference to the attached drawings further characteristic of the present invention and advantage below, with and structure and the operation of various embodiment.It should be noted that and the invention is not restricted to specific embodiment described here.Be merely illustration purpose at this, such embodiment is provided.Based on the instruction that is included in this, additional embodiments will be conspicuous to various equivalent modifications.
Description of drawings
Will be with reference to embodiments of the invention, the example of these embodiment can illustrate in the accompanying drawings.These accompanying drawings be intended to the explanation and unrestricted.Although in the linguistic context of these embodiment, usually described the present invention, should be understood that this also is not intended to scope of the present invention is limited in these specific embodiments.
Fig. 1 shows the system that is used for detecting automatically the terrestrial reference of digital picture according to an embodiment of the invention.
Fig. 2 shows the more details of assembly of the system of Fig. 1 according to an embodiment of the invention.
Fig. 3 according to an embodiment of the inventionly is used for detecting automatically the terrestrial reference in the digital picture, and digital picture is carried out the process of note.
Fig. 4 is according to an embodiment of the invention being used for the process of giving the text associated images of selecting about the label distribution of terrestrial reference.
Fig. 5 is the process that is used for generating based on the text associated images n-gram (n unit) tabulation according to an embodiment of the invention.
Fig. 6 is the process that is used for selecting from the n-gram tabulation that the process according to Fig. 4 generates the n-gram collection according to an embodiment of the invention.
Embodiment
Though described the present invention with reference to the illustrative example that is used for application-specific at this, should be understood that, the invention is not restricted to this.Those skilled in the art uses in this instruction will recognize the other field that other modification, application and embodiment and the present invention in the scope of the invention will have remarkable function therein.
Summary
The present invention includes the method and system of the object that is used for identification automatically and breakdown figures image.For example, embodiments of the invention can be based on the tourism terrestrial reference that addressable digital image set is discerned, classified and sorts most popular on the Internet.Method and system of the present invention can make it possible to the up-to-date tabulation and the image collection of most popular tourism position are effectively safeguarded.In certain embodiments, the popularity of tourism position can be estimated based on the quantity that is posted to this images of positions on the Internet by the user.
Many individuals takes the digital picture of position of visiting in the environment, their daily routines of theirs in the neighbourhood and the place of when their tour, visiting.Employed camera is each mass and precision level.The individual who catches image is various levels of skill.Image is from various angles, in the different light rays level, be captured with varying environment vision noise level, in that various weather conditions are inferior.Many digitizings in these images can be obtained many being posted on the shared website of picture or in these images then through other means.Through the network such as the Internet visit to huge digital image set can be obtained, said digital picture is such as digital picture.
Usually, the online user who puts up image carries out note also for example through adding one or more labels and/or note to the image of being puted up.Can use label to come image is named.Can also give image to distribute and image-related keyword label distribution.For example, the image of Eiffel Tower possibly distributed to the people's who poses before it has label " Eiffel Tower ", " Paris ", " France ", " Europe ", " summer " or is illustrated in iron tower name.Label is valuable as the organization tool in various granular level: " France " can be useful; With with image classification under to search at the terrestrial reference of France, can image be excluded in outside the search of the terrestrial reference in " Paris " and/or " France " and only make " Eiffel Tower " to become label.Although when confirming to be included in the terrestrial reference in those images, accuracy and the difference of serviceability of the label of image is arranged, for making up the purpose of automatic terrestrial reference identification system, it is valuable information source that the user marks image library.
Other potential information sources comprise various other documents and the electron source that connects text and image.The picture that for example, can comprise its theme about the magazine article of Eiffel Tower.Content of newspaper, magazine and periodical content, by the article that the individual writes and/or puts up, comprise blog post about various terrestrial references etc., generally include the image that directly is bound by textual description.Image with cognizable terrestrial reference related text can be called as terrestrial reference mark image.
Embodiments of the invention utilize some types about the obtainable data of image, to obtain information about welcome terrestrial reference.For example, geographical labels, text label, author information, timestamp (for example, time or origin) and vision matching information are the information of some types of utilizing in an embodiment of the present invention.Part in this information is obtainable with each image (for example, in the EXIF label that is associated with image).Other information are that the user distributes or algorithm assigns.When obtaining respectively, each in these data types possibly have a large amount of weakness.For example, geographic position data (for example, geographical labels) is usually based on the position of the camera rather than the terrestrial reference of being taken pictures.And in some cases, therefore the information that geographical location information provides based on the user such as the city title possibly be inaccurate.The text label that author and third party provided possibly not have the accurate description terrestrial reference.The author information of each image can be based on the people of camera identifier, seizure image or with the people of image upload to the website.Such as in than the zonule, existing under the situation that some terrestrial references, terrestrial reference seem alike and/or picture quality is not enough, vision matching information also possibly be wrong.Therefore, but embodiments of the invention utilize the acquired information of some types to obtain higher degree terrestrial reference in digital picture detects and accurate note.
The system that is used for automatic terrestrial reference identification and note
The system 100 that is used to make up the welcome landmark image database of note according to an embodiment of the invention has been shown in Fig. 1.System 100 comprises computing machine 101, user interface 102, network 103 and 104, text/image collection of document 107, n-gram set 108, n-gram filter database 109, not image data base 110, display model database 111, the image 112 and the text/image source 105 of note of note.Those that it will be appreciated by those skilled in the art that system 100 can comprise than list in the above are more, still less or different assemblies and module, and still consistent with the present invention.
Computing machine 101 can comprise one or more computing machines, server or the similar computing equipment via the communication media interconnection.For example, computing machine 101 can comprise the one or more business computing servers that couple via one or more LANs, said LAN such as Ethernet, gigabit Ethernet, WIFI network or analog.Computing machine 101 comprises processor 121, volatile memory 122, non-volatile storage 123, network interface 124, database interface 125, couple communication media 126 and the annotation of images device module 127 of non-supervision of the module of computing machine 101.Processor 121 can comprise one or more commercial CPU (CPU), graphics processor unit (GPU), field programmable gate array (FPGA), digital signal processor (DSP) and special IC (ASIC).Processor 121 is controlled at processing in the computing machine 101, receives input and data are outputed to computing machine 101 from computing machine 101.For example, the processing logic of the annotation of images device module 127 of non-supervision can be carried out on processor 121.
Volatile memory 122 can comprise volatile memory, such as dynamic RAM (DRAM), static RAM (SRAM) or analog.Volatile memory 122 can be used for the intermediate result of processing of storage configuration parameter, source data and module 127.Configuration parameter can comprise the link information about text/image source 105, and disposes for example other parameters of the operation of the processing of the annotation of images device module 127 of non-supervision.Non-volatile storage 123 can comprise one or more non-volatile memory devices, such as disk, CD, flash memory, ROM (read-only memory) (ROM) or analog.Non-volatile storage 123 can be used to store logical order, the configuration parameter of the annotation of images device module 127 that is used for non-supervision, and is used for the centre and other results of the processing in the memory module 127.
Network interface 124 can comprise communicate by letter with the entity that is connected to computing machine 101 through the network that comprises network 103 functional, and said entity is such as text/image source 105.For example, network interface 124 can comprise the processing components that comprises Internet protocol (IP) and HTTP(Hypertext Transport Protocol) processing, so that make computing machine 101 can be connected to text/image source 105 to obtain text and image information.For example, http protocol handling machine software may be implemented as the part of network interface 124.According to embodiments of the invention, database interface 125 comprises computing machine 101 is connected to one or more database function property of when handling image for terrestrial reference, using.The use that should be noted in the discussion above that term " database " is not necessarily index according to base management system (DBMS), but comprises any data acquisition.Therefore, database interface 125 can comprise the DBMS that is connected to the one or more DBMS system that comprises one or more database 107-112 functional or with the processing logic of the database communication of the type of the database 107-112 of each type.Communication media 126 can connect the module of computing machine 101, comprises module 121-125 and 127.Communication media 126 can comprise communication facilities, such as pci bus, USB, Ethernet or analog.
According to embodiments of the invention, the annotation of images device module 127 of non-supervision comprises following functional: discern terrestrial reference, be selected terrestrial reference generation display model; And image carried out note.Be described below, the terrestrial reference that is included in the image can be discerned based on the clear and definite label that has been associated with image or through the algorithm means.The functional of the annotation of images device module 127 of non-supervision can realize with software, firmware, hardware or its any combination.In one embodiment, the functional processing logic that is used for the annotation of images device module 127 of non-supervision can be realized with computer programming language or script such as C, C++, compilation, Java, JavaScript, Perl or analog.
Network 103 can comprise the device that computing machine 101 is connected to one or more text/image source 105.Network 104 can comprise the device that computing machine 101 is connected to one or more database 107-112.Network 103 and 104 can comprise one or more network mediums, comprising: the peripheral connection, such as USB, live wire; Or LAN, such as Ethernet, WIFI; Or wide area network, such as PSTN or the Internet.In one embodiment, network 103 comprises the Internet, and network 104 comprises the LAN based on Ethernet.
User interface 102 can use any one of interconnection mechanism or make up and be connected to one or more computing machines 101, said interconnection mechanism such as pci bus, IEEE 1394 fire-wire interfaces, Ethernet interface, IEEE 802.11 interfaces or analog.User interface 102 allows user or other external entities and computing machine 101 mutual.In certain embodiments, one or more database 107-112 can also be mutual through user interface 102.One or more can being included in the user interface 130 in graphical user interface, web interface and the API.
Text/image source 105 can comprise various types of digital document set of image that comprises terrestrial reference and the text that is associated (for example, terrestrial reference mark image).In one embodiment, text/image source 105 comprises the one or more picture set with the picture that is associated with note and label.As employed at this, note is meant the title of distributing to picture.As employed at this, label is meant one or more speech or the phrase of distributing to picture.Usually, note and label are distributed by the author of picture (for example, the creator of picture or upload to the people that picture is shared the website with picture).Yet note and label can also be distributed to picture by third party or automation tools.Only if each is by identification separately, term in the following description " label " comprises label and note.
Text/image source 105 can also comprise image hyperlink to the set of the hypertext document of document (vice versa), and can also comprise newspaper storehouse, magazine and journal collection, blog filing, has the digital library of digitized book, picture warehouse and the individual and the enterprise web site of third party's note.For example, tourism and/or travelling related web site, digital guide-book, website, city etc. are some resources of description that generally comprise image and those terrestrial references of terrestrial reference.Yet any numerical data set that is mutually related that wherein can obtain between one or more images and the text that is associated can be included in the text/image source 105.
Text/image set 107 is databases; In certain embodiments; Be kept at the local replica and/or the modified version of the text/image data of original visit in the long-range text/image source 105 therein, for example handle with the annotation of images device 127 of more convenient and the non-supervision of reliable access cause.For example; Because through possibly being to relate to than long delay such as data and image in the network 103 accessing texts/figure image source 105 of the wide area network of the Internet; In computing machine 101, can there be following process (not shown): in local or local additional network site; Such as in text/image set 107, make the such data and the copy of image.Text/image set 107 can also comprise the image collection that has been marked, for example the set of the user's picture in the Picasa network album and/or according to the image collection of dies of the present invention.In certain embodiments; The data structure that text/image set 107 can comprise corresponding to each image; Wherein data structure is included in image and/or the one or more pointers of document in the text/image source 105, for example to avoid and must create from the image in text/image source 105 and/or the independent copy of document.
N-gram set 108 is the databases that comprise the n-gram set.N-gram can extract from note, label or the text document that is associated with image set 107 of text/image for example or text/image source 105.As employed at this, n-gram is the sequence of one or more speech.The selection of N-gram can use with for example in text analyzing the one or more similar method in employed some technology accomplish.According to embodiments of the invention, selection and the extraction of n-gram have been further described below.
N-gram filter database 109 comprises to be treated from one or more n-gram tabulations of 108 li filtrations of n-gram set and/or waits to be applied to one or more filtering rules of n-gram set 108.For example; A tabulation in the n-gram filter database 109 can be " bad speech tabulation "; The n-gram that wherein in the tabulation of bad speech, occurs does not extract from text/image set 107 or text/image source 105, and if find that they exist, then remove from n-gram set 108.Another tabulation can be too frequent n-gram tabulation in the image related text, to occur, therefore seldom is worth as landmark identifiers.Speech such as " the " and " of " can be thought in this classification.Another tabulation can be a list of phrases, and it is too frequent by known appearance, is not enough useful as landmark identifiers discriminatory therefore.
According to embodiments of the invention, the image data base 110 of note does not comprise as yet not by the image of note (for example, mark).For example, the image data base 110 of note can not comprise the not digital picture of mark that one or more users upload, and handles to use embodiments of the invention.
According to embodiments of the invention, the image data base 112 of note comprises the image that is marked.For example, according to embodiments of the invention,, they are stored in the image data base 112 of note after being handled by the annotation of images device of non-supervision 127 from the image of the image data base 110 of mark not.Those skilled in the art will recognize that,, can arrange and/or fulfillment database 107-112 with the variety of way consistent with the present invention although database 107-112 is described to self contained data base in the above.
Fig. 2 shows the more details of the annotation of images device module 127 of non-supervision according to an embodiment of the invention.In this embodiment, the annotation of images device module 127 of non-supervision comprises three processing modules: terrestrial reference recognizer 201, display model maker 202 and annotation of images device 203.Module 201,202 and 203 can make up with software, firmware, hardware or its and realize.In one embodiment, module 201-203 uses the C++ programming language to realize with software.In one embodiment, computer program can make the logical record of the computer program logic that comprises module 201-203 on computer-readable medium, said computer-readable medium such as hard disk, flash disk or other forms of storage.
Terrestrial reference identifier module 201 comprises the functional of terrestrial reference in identification text/image set 107 and/or the text/image source 105.In one embodiment, terrestrial reference identifier module 201 can be used as input with the text that is associated with the image from text/image source 105, and such image and the text that is associated are copied to text/image set 107.Terrestrial reference identifier module 201 can also be analyzed the text in the text/image source 105 when using and upgrading n-gram set 108.N-gram filter database 109 can also be used in the processing in the terrestrial reference identifier module 201.
Display model maker 202 is included as each terrestrial reference of for example being discerned by terrestrial reference identifier module 201 and generates the functional of one or more display models.In one example, display model maker 202 can be used as input with the terrestrial reference discerned with the image that text/image is gathered in 107, and generates one or more display models in the terrestrial reference each.The display model that is generated can be written into display model database 111.
As employed at this, the template of using when display model is some common trait in automatic recognisable image.In one embodiment of the invention, the display model that is used for the identification terrestrial reference can comprise proper vector, and it comprises the numerical score of confirming the characteristics of image collection in advance.The method of process identification in the image and generating feature vector is well-known in the art.For example; At DavidG.Lowe; " Object recognition from local scale-invariant features, " International Conference on Computer Vision, Corfu; Greece (September1999) has described the method for the process identification in the image among the pp.1150-1157.Except that vision identification assembly, display model can also comprise information, such as the geographical location information of the terrestrial reference of correspondence.For example, the geographical location information of the specific landmark in the display model can be specified geo point and/or geographic area.Designated geographic area can reduce the uncertainty of creating owing to the difference of the accuracy of the geographical location information of image.
Annotation of images device module 203 comprises following functional: the automatic terrestrial reference in the recognisable image, and with the such image of the appropriate note of information of the terrestrial reference of discerning one or more correspondences.In one embodiment, annotation of images device module 203 can be used from the display model of display model database 111 and come automatic identification from the terrestrial reference in the image of the image data base 110 of note not.Can for example come image is carried out note according to the terrestrial reference of the institute's identification in each image then through the one or more labels of association, and the image data base 112 that can the image of institute's note be write note.
The method that is used for automatic terrestrial reference identification and note
Fig. 3 shows process 300 of the image that comprises one or more welcome terrestrial references being carried out note according to an embodiment of the invention.Process 300 can for example realize in the annotation of images device module 127 of non-supervision.The step 301-304 of process 300 can suitably realize in terrestrial reference identifier module 201, display model maker module 202 and annotation of images device module 203 under the situation.It will be appreciated by those skilled in the art that the functional module 201-203 that can use that describes in this reference process 300 realizes with the mode except that the mode that is described below.For example, in one embodiment, each can be detached process for terrestrial reference identifier module 201, display model maker module 202 and annotation of images device module 203, and it is implementation procedure 300 together.In another embodiment, each can be separate threads for terrestrial reference identifier module 201, display model maker module 202 and annotation of images device module 203, and it is implementation procedure 300 together.In yet another embodiment, terrestrial reference identifier module 201, display model maker module 202 and annotation of images device module 203 can all be embodied as single process, its implementation procedure 300.
In step 301, image and the text that is associated with those images are analyzed with the identification terrestrial reference, be in particular welcome terrestrial reference.Generally speaking, welcome terrestrial reference is those the most frequent terrestrial references in such as the picture/text source of being analyzed in text/image source 105, to occur.In one embodiment, the input of the processing in the step 301 is one or more picture/texts source of one or more computer-accessible of positive implementation 300 above that.For example, process 300 possibly just carried out on computing machine 101, and can have the accessibility to text/image source 105 through network 103.According to an embodiment, can be the image set of selecting, the terrestrial reference of being discerned and text that is associated and the n-gram in those images from the output of step 301.For example, the output of step 301 can be written into text/image set 107.Further describe step 301 with reference to figure 4-6 below.
In step 302, obtain or learn being used for one or more display models at the terrestrial reference of step 301 identification.Those skilled in the art will recognize that, can use one in many methods to come to learn display model from the terrestrial reference mark image that the result as step 301 obtains.According to an embodiment, the display model of specific landmark comprises proper vector, and its numeral go up to quantize to be considered to comprise one or more visual aspects of one or more images of specific landmark.As previously mentioned; It is well-known in the art that proper vector generates; And the David G.Lowe that quotes in the above, the method that proper vector generates has been described, such as what can use in the present invention in " Object recognition from local scale-invariant features ".For example, proper vector comprises for the geostationary big measure feature of many change condition, said change condition such as camera distance, camera angle, picture quality, light condition etc. ideally.In some embodiments of the invention, can also comprise the non-visual aspects of image with the corresponding one or more display models of specific image, such as geographical location information.Display model can comprise any information that can in image, use during the existing of this terrestrial reference in automatic identification, comprises the visual signature and the geographical location information of specific landmark.
In step 303, use one or more display models of in step 302, obtaining to come the corresponding terrestrial reference in the detected image.In one embodiment, the accordingly timestamp of the one or more display models in the display model database 111 in detecting the image data base 110 of note not is used.For example, can with from the proper vector of the display model of display model database 111 with compare for the proper vector of just considering that generates from the image of the image data base 110 of note not.If the proper vector coupling surpasses the threshold level of confirming in advance, the image that then just is being considered is recognized as the corresponding terrestrial reference of display model that comprises with coupling.Process identification technology such as what use in can step 303 in an embodiment of the present invention, generally is well-known.The Lowe that quotes has in the above described a method of the process identification that can use in the present invention in " Object recognition from local scale-invariantfeatures ".
In step 304,, then can carry out note to this image if the image of confirming just to be analyzed has and the corresponding specific landmark of one or more display models that for example in the detection of step 303, uses in it.The image of note and corresponding note can be written into the image data base 112 of note.The note that is associated with the image of note can comprise with the display model of finding in the image of this note, to have coupling in each text that is associated.Be contemplated that also the note that is associated with the image of note can comprise text or the phrase based on the other processing of the text that is associated with corresponding display model.For example; Be that step 304 can comprise the other processing of generation such as the sentence of " at David's statue of Rome, ITA ", " David's statue in Rome, ITA Wei Qiao palace " or analog among the embodiment such as the form of the simple label in " David's statue " and " Rome " with text that corresponding display model is associated.
In Fig. 4, further show in detail the processing that relates in the step 301.The functional step 401-403 that comprises of step 301.In step 401, generate the speech of description terrestrial reference or the n-gram collection and/or the existing n-gram collection of renewal of phrase.For example, step 401 can be used as input with text/image source 105, and the n-gram in the generation n-gram set 108 is as output.The more detailed description of step 401 is provided below, as how generating the one or more n-gram that describe terrestrial reference about Fig. 5.
In step 402, terrestrial reference is confirmed that useful n-gram collection marks to tentatively thinking.For example, the initial n-gram collection of in step 402, being thought can be the n-gram collection that in step 401, obtains from text/image source 105.The processing of step 402 can be created the n-gram tabulation in the n-gram set 108.Come n-gram is filtered according to various standards, said various standards comprise marks to each n-gram, and only keeps the n-gram with highest score of preparatory quantification.N-gram score value S (k) is distributed to each among the n-gramN (k) in the n-gram set 108.Be described below the method for definite S (k).Further described the processing of step 402 below with reference to figure 6.
In step 403, for image distributes the label from n-gram set 108.For example, right for each image and n-gram combination, can the distributive pairing score value.Can define the pairing score value, so that the pairing score value of higher value means that strong relevant image and n-gram are right.In one example; Can serve as reasons and distribute by with undefined pairing score value: the bond strength L (i between I (i) and N (k) from the image I (i) of picture/text set 107 with from the pairing that the n-gramN (k) of n-gram set 108 forms; K) with the product of the n-gram score value of N (k); Be L (i, k) * S (k).Definite L (i, method k) have been described below.Can be through focusing on n-gram with higher pairing score value, and appropriate intercepting tabulation, generate candidate n-gram tabulation.In an example, when the pairing score value falls into a half of the highest pairing score value that is lower than tabulation, tabulation can be by intercepting.By this way, can distribute maximally related n-gram for each image.
Fig. 5 shows the treatment step 501-504 when generating the n-gram collection according to above-mentioned steps 401.In step 501, for example through the one or more text/image of terrestrial reference identifier module 201 visits source 105.Can comprise through local network or such as the wide area network of the Internet the visit in text/image source 105 and to be connected to such source.Being selected the text/image source 105 of handling can discern based on the whole bag of tricks; Said method such as from user or operator's input, program assembly to the automatic identification of website and classification (for example, the web auto-programming is to the identification of picture library website) or website or the tabulation of other storehouses kept watch on to content.The method that is connected to such as the source in text/image source 105 is well-known.Under the situation of needs, aspects such as the copyright that embodiment of the present invention also should be considered when using all image of each side, to relate to, privacy.
In step 502, from text/image source 105 the potential terrestrial reference descriptor of the text retrieval n-gram tabulation that is associated of image.The picture library that is associated with label and/or note from picture wherein extracts n-gram and can comprise and collect label and/or the note collection that is associated with the picture of the picture library in text/image source 105.When the picture/text source comprises other documents that image and corresponding text are associated and/or content, can use the one or more potential words (label) that extract in many text analyzing methods corresponding to terrestrial reference.For example, can use method well-known in the art come to analyze automatically with tour site in the text that is associated of image, said method is such as at the word frequency-anti-document frequency (TF-IDF) that can be used for discerning on the text of potential label.In one embodiment, TF-IDF is applied to and the label that is associated from the picture in the picture library in text/image source 105.
In advance definite rule be can use and the constriction and/or the tally set of terrestrial reference come to confirm to be meant through filtering from potential a large amount of obtainable labels.For example, in step 503, the n-gram collection of the potential terrestrial reference descriptor that can one or more filtering rules or standard application be collected in step 502.A filtrator that can be applied to potential terrestrial reference descriptor n-gram tabulation is bad speech filtrator.Bad speech filtrator comprises that being confirmed as between terrestrial reference, distinguishing in advance is the tabulation of going bad and/or do not have the n-gram and the phrase of help.Another filtrator that is employed can be the stop-word tabulation.The stop-word tabulation can comprise that the expection meeting occurs in label and/or descriptor too frequent, so that they can not be helpful n-gram as the terrestrial reference descriptor.Speech such as " of ", " the " and " and " is the example n-gram that can be included in the stop-word tabulation.Another filtrator that can be employed is that minimum reliability is measured, such as author's filtrator of minimum number.Can use author's filtrator of minimum number to come to remove and have any n-gram that its label, uses those n-gram less than uniqueness (unique) author of quantification in advance from potential terrestrial reference descriptor n-gram tabulation.For example, what can confirm in advance is, for waiting to be included in any n-gram in the n-gram set 108, should in the label that three or more unique authors use, detect this n-gram.
In step 504, can remaining potential terrestrial reference descriptor n-gram tabulation after in step 503, using one or more rules and/or filtrator be write in the n-gram set 108.The n-gram collection from n-gram set 108 that uses with post-processing step by such as treatment step 402 is the n-gram collection that is filtered according to aforesaid some filtrators, therefore will only comprise the n-gram that describes terrestrial reference in fact.
Fig. 6 shows the processed steps 601-608 that relates in the illustrated steps 402 according to an embodiment.In step 601, for the image that is associated with the n-gram that in step 401, selects distributes associated weight.In one embodiment, the copying image that will be associated with the n-gram that in step 401, selects is gathered in 107 to text/image, and carries out weight allocation and other processing to those images.The associated weight W (i) of image I (i) be image I (i) with text/image set 107 in oppositely the measuring of related levels of other images.For example, if image I (i) is uncorrelated with any other image in the text/image set 107, then be that image I (i) is distributed 1 associated weight; If image I (i) is relevant with 2 other images that text/image is gathered in 107, then be the associated weight of each distribution 1/3 in image I (i) and its two associated pictures.Can use rule or the regular set confirmed in advance to confirm whether two images are relevant.For example, when two images by same author and in very near geographic position (for example, each other in 1/4 mile) can think that these two images are relevant when taking.
In step 602, gather the image creation match map image pattern 107 from for example text/image.Node in the match map image pattern is represented the image in the text/image set 107.The degree of the corresponding images match of node that each the bar limit in the match map image pattern is represented to be connected with two.For example, (i j) can be based on the numerical value that the coupling between the proper vector of proper vector and image I (j) of image I (i) obtains to distribute to the coupling score value M on the limit between image I (i) and I (j).Can be the configurable weight of single characteristic allocation in the proper vector, and (i j) can be the summation of such weight of matching characteristic to mate score value M.
In step 603, form connection (being called as image-title connects) between each of the image in n-gram in n-gram set 108 and the text/image set 107.If the label of image comprises n-gram, then to connect can be to be set to 1 binary variable for image-title, otherwise be 0.Yet, in order to increase result's robustness, through averaging on the visually similar image set, rather than consider that single image makes output smoothing.For example, the image between image I (i) and n-gram k-title connect L (i k) can be defined as:
Wherein, as stated, (i be the coupling score value between image I (i) and I (j) among the images match figure j), and W (j) is the associated weight of image I (j) to M.
In step 604, estimate the geographical reliability of each image in the text/image set 107.Based on having the conforming comparison of vision of confirming the image of the geographical position coordinates that distance is interior in advance mutual, the geographical reliability G (i) of image I (i) is the estimation of accuracy of the geographical location information of image.For example,
Wherein, n can be a configurable parameter.
In step 605, can be that each n-gramN (k) calculates geographical difference alternatively.For example, the geographical difference V (k) of N (k) can be represented as:
The wherein geographic position of loc (i) presentation video I (i), and EW is the weighting expectation.Weighting is helpful when being desirably in and catching the difference of remarkable location point for n-gram.Weight can be calculated as L (i, k) * W (i) * G (i), the i.e. product of the geographical reliability of image-title connection, image weights and image.Subsequently, can the n-gram that have greater than the V (k) of threshold value geographical difference be filtered out from n-gram set 108.
In step 606; Use measures to confirm the n-gram score value S (k) of each n-gramN (k) in the text/image set 107; Saidly measure the inner Join intensity that is designed to catch between the image in its label, have n-gramN (k), and in its label, have the image of n-gramN (k) and in its label, do not have the outer type intensity between the image of n-gramN (k).For example, S (k) can be represented as:
S (k) is big more, and that this n-gramN (k) possibly be meant more is meaningful, differentiable entity visually, therefore, possibly be the terrestrial reference title more.
In step 607, after n-gram is marked, can carry out further filtration alternatively and discern most popular terrestrial reference n-gram.For example, can average, confirm the threshold value mean scores the n-gram score value of the n-gram with the highest n-gram score value of preparatory quantification.After this, can gather 108 all n-gram of removing except that those n-gram from n-gram with the score value that is higher than the threshold value mean scores.
In step 608, merge the n-gram that is considered to be meant same landmark locations.Although scoring step, and generally stay the n-gram tabulation that free burial ground for the destitute intentionally refers to terrestrial reference based on the subsequent filtration of score value, the many n-gram that are meant same terrestrial reference possibly still remain in n-gram and gather in 108.Owing to some reasons, comprise the difference statement and the substring intercepting that are used for the different titles of same terrestrial reference, same title, possibly there are a plurality of n-gram that are meant same terrestrial reference.Desired is with meaningful ways such repetition n-gram to be combined.In order to solve this point, in one example, if two n-gramN (k) and N (l) have apart from each other in advance confirming the score value that distance is interior, and if their images of being attached to be significantly overlapping, then with these two n-gramN (k and N (l) merging.Can be for example through confirm the significantly overlapping of image to get off: consider (i, k), and whether definite Bhattacharyya distance is higher than and confirms threshold value in advance the right Bhattacharyya distance L of each image I (i) and n-gramN (k).The calculating of Bhattacharyya distance is well-known in the art.
Conclusion
The processing capacity property of module 127 and/or module 201-203 can make up with software, hardware or its and realize.For example, module 201 and 203 can all be embodied as software module, or the partial function property of display model maker module 202 can be used such as the hardware of field programmable gate array (FPGA) and realizes.The annotation of images device module 127 and/or the computing machine 101 that it will be appreciated by those skilled in the art that non-supervision can comprise other assembly and the module that promotes function of the present invention.
Will be appreciated that embodiment part rather than summary of the invention and summary partly are intended to be used for the construe requirement.Summary of the invention can be illustrated of the present invention one or more but not all exemplary embodiments that the inventor considers with summary part, so summary of the invention and summary are partly and be not intended to and limit the present invention and appended claim by any way.
By means of the functional configuration piece of the realization of function and the relation thereof of explanation appointment the present invention has been described in the above.For the ease of describing, at random define the border of these functional configuration pieces at this.Can limit for selecting the border, as long as the function of said appointment and relation thereof are suitably carried out.
The aforementioned description of specific embodiment has disclosed general characteristic of the present invention so fully; Make that other people can be under the situation that does not deviate from general thoughts of the present invention; Knowledge through in the technology that is applied in this area is various application and easily revise and/or adjust such specific embodiment, and need not carry out excessive experiment.Therefore, be based on this instruction that provides and guidance, such adjustment and modification are intended in the implication and scope of the equivalent of the disclosed embodiments.Should be understood that, be used to describe and unrestricted purpose, so the term of this instructions or word should be by the technician according to said instructions with instruct and explain at this word or term.
Width of the present invention and scope should be by any one restrictions of above-mentioned exemplary embodiment, but should only limit according to accompanying claims and equivalent thereof.
Claims (23)
1. one kind is used for detecting the also ground calibration method of note digital picture:
(a) the one or more images in a plurality of text incidence number word images distribute the label of describing terrestrial reference automatically, generating terrestrial reference mark image set, the image in the wherein said terrestrial reference mark image set by algorithm confirm as and comprise said terrestrial reference;
(b) mark the display model that image set study is used for said terrestrial reference from said terrestrial reference; And
(c) use said display model to come in new image, to detect said terrestrial reference,
The wherein said stage (a)-(c) is carried out by at least one processor.
2. method according to claim 1 further comprises:
(d) with the said label of describing said terrestrial reference said new image is carried out note.
3. method according to claim 1, wherein (a) comprising the stage:
(i) generate terrestrial reference n-gram tabulation from said a plurality of text incidence number word images;
Each the terrestrial reference n-gram that (ii) concentrates for n-gram calculates the n-gram score value, and wherein said n-gram collection is the subclass of said terrestrial reference n-gram tabulation; And
(iii) be that said image distributes the said label of describing said terrestrial reference, the said label of wherein describing said terrestrial reference is based at least one concentrated terrestrial reference n-gram of said n-gram.
4. method according to claim 3, wherein (a) (i) comprising the stage:
Visit said a plurality of text incidence number word image electronically; And
From with said a plurality of text incidence number word images the said terrestrial reference n-gram of text retrieval that is associated of image at least one.
5. method according to claim 4, wherein (a) (i) further comprises the stage:
Selection has the said terrestrial reference n-gram that minimum reliability is at least measured.
6. method according to claim 5, wherein said measure of reliability is based on unique author's quantity.
7. method according to claim 3, wherein (a) (ii) comprises the stage:
Distribute associated weight to said a plurality of text incidence number word images; Wherein said associated weight is based on the correlativity of the metadata of the image in said a plurality of text incidence number word images, and wherein said a plurality of text incidence number word image comprises said text associated diagram image set;
Generate the match map image pattern from said a plurality of text incidence number word images; And
Image in said terrestrial reference n-gram and the said a plurality of text incidence number word image is connect, to be created on the connection between the image in terrestrial reference n-gram and the said a plurality of text incidence number word image.
8. method according to claim 7, wherein (a) (ii) further comprises the stage:
Use said match map image pattern to come to estimate geographical reliability score value for each image in said a plurality of text incidence number word images.
9. method according to claim 7, wherein said n-gram score value is based on said match map image pattern.
10. method according to claim 9; Wherein said n-gram score value is calculated as the ratio of intensity of external edge of intensity and said match map image pattern of the internal edges of said match map image pattern; Wherein internal edges is present between the image with at least one common terrestrial reference n-gram, and wherein external edge is present between the image that does not have at least one common terrestrial reference n-gram.
11. method according to claim 8, wherein (a) (ii) further comprises the stage:
Be the terrestrial reference n-gram compute geographic location difference of said n-gram collection, wherein said difference is based on the geographic position of the image with the concentrated said terrestrial reference n-gram of its n-gram in the said match map image pattern; And
Remove any terrestrial reference n-gram with the geographic position difference that surpasses in advance definite threshold value from said n-gram collection.
12. method according to claim 7, wherein (a) (i) further comprises the stage:
Merge two or more terrestrial references n-gram that said n-gram concentrates.
13. method according to claim 12, wherein said merging are at least based on following one:
The similarity of the said score value of said two or more terrestrial references n-gram, and
Image overlapping with said two or more terrestrial references n-gram among the terrestrial reference n-gram of connection.
14. method according to claim 7, wherein said metadata comprise and at least one following relevant information:
The author,
The geographic position, and
Play source time.
15. method according to claim 7, each association list in the wherein said match map are shown in the matching characteristic descriptor between two images of said a plurality of text incidence number word images.
16. one kind is used for detecting the also system of the terrestrial reference of note digital picture automatically, comprises:
Be stored in the related digital image set of at least one text in the storage medium; And
Be couple at least one processor of said medium communicatedly, said at least one processor is configured to:
One or more images in a plurality of text incidence number word images distribute the label of describing terrestrial reference automatically, generating terrestrial reference mark image set, the image in the wherein said terrestrial reference mark image set by algorithm confirm as and comprise said terrestrial reference;
The display model that is used for said terrestrial reference from the study of said terrestrial reference mark image set; And
Use said display model to come in new image, to detect said terrestrial reference.
17. system according to claim 16, wherein said at least one processor further is configured to:
Said label with describing said terrestrial reference carries out note to said new image.
18. system according to claim 16, wherein said at least one processor further is configured to:
Generate terrestrial reference n-gram tabulation from said a plurality of text incidence number word images;
Each the terrestrial reference n-gram that concentrates for n-gram calculates the n-gram score value, and wherein said n-gram collection is the subclass of said terrestrial reference n-gram tabulation; And
For said image distributes the said label of describing said terrestrial reference, the said label of wherein describing said terrestrial reference is based at least one concentrated terrestrial reference n-gram of said n-gram.
19. system according to claim 18, wherein said at least one processor further is configured to:
Distribute associated weight to said a plurality of text incidence number word images; Wherein said associated weight is based on the correlativity of the metadata of the image in said a plurality of text incidence number word images, and wherein said a plurality of text incidence number word image comprises said text associated diagram image set;
Generate the match map image pattern from said a plurality of text incidence number word images; And
Image in said terrestrial reference n-gram and the said a plurality of text incidence number word image is connect, to be created on the connection between the image in terrestrial reference n-gram and the said a plurality of text incidence number word image.
20. a computer program that comprises computer-readable medium, said computer-readable medium record on it and be used to make the processor can be to the computer program logic of image name, said computer program logic comprises:
First module; It is configured to make said processor to distribute the label of describing terrestrial reference by the one or more images in a plurality of text incidence number word images; Generating terrestrial reference mark image set, the image in the wherein said terrestrial reference mark image set by algorithm confirm as and comprise said terrestrial reference;
Second module, it is configured to make said processor can be used for the display model of said terrestrial reference from the study of said terrestrial reference mark image set; And
Three module, it is configured to make said processor can use said display model to come in new image, to detect said terrestrial reference.
21. computer program according to claim 20 further comprises:
Four module, it is configured to make the said label of the said terrestrial reference of the said processor enough descriptions of ability that said new image is carried out note.
22. computer program according to claim 20, wherein said first module further is configured to:
Generate terrestrial reference n-gram tabulation from said a plurality of text incidence number word images;
Each the terrestrial reference n-gram that concentrates for n-gram calculates the n-gram score value, and wherein said n-gram collection is the subclass of said terrestrial reference n-gram tabulation; And
For said image distributes the said label of describing said terrestrial reference, the said label of wherein describing said terrestrial reference is based at least one concentrated terrestrial reference n-gram of said n-gram.
23. computer program according to claim 22, wherein said first module further is configured to:
Distribute associated weight to said a plurality of text incidence number word images; Wherein said associated weight is based on the correlativity of the metadata of the image in said a plurality of text incidence number word images, and wherein said a plurality of text incidence number word image comprises said text associated diagram image set;
Generate the match map image pattern from said a plurality of text incidence number word images; And
Image in said terrestrial reference n-gram and the said a plurality of text incidence number word image is connect, to be created on the connection between the image in terrestrial reference n-gram and the said a plurality of text incidence number word image.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/466,880 | 2009-05-15 | ||
US12/466,880 US8396287B2 (en) | 2009-05-15 | 2009-05-15 | Landmarks from digital photo collections |
PCT/US2010/034930 WO2010132789A1 (en) | 2009-05-15 | 2010-05-14 | Landmarks from digital photo collections |
Publications (2)
Publication Number | Publication Date |
---|---|
CN102549571A true CN102549571A (en) | 2012-07-04 |
CN102549571B CN102549571B (en) | 2015-11-25 |
Family
ID=42629562
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201080030849.3A Active CN102549571B (en) | 2009-05-15 | 2010-05-14 | From the terrestrial reference of digital picture set |
Country Status (8)
Country | Link |
---|---|
US (4) | US8396287B2 (en) |
EP (1) | EP2430572A1 (en) |
JP (1) | JP5680063B2 (en) |
KR (1) | KR101672570B1 (en) |
CN (1) | CN102549571B (en) |
AU (1) | AU2010248862B2 (en) |
CA (1) | CA2762090C (en) |
WO (1) | WO2010132789A1 (en) |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN103226575A (en) * | 2013-04-01 | 2013-07-31 | 北京小米科技有限责任公司 | Image processing method and device |
CN103853797A (en) * | 2012-12-07 | 2014-06-11 | 中兴通讯股份有限公司 | Image retrieval method and system based on n-gram image indexing structure |
CN103853792A (en) * | 2012-12-07 | 2014-06-11 | 中兴通讯股份有限公司 | Automatic image semantic annotation method and system |
CN104541515A (en) * | 2012-08-08 | 2015-04-22 | 谷歌公司 | Browsing images of a point of interest within an image graph |
CN104794171A (en) * | 2015-03-31 | 2015-07-22 | 百度在线网络技术（北京）有限公司 | Method and device for marking geographical location information of picture |
Families Citing this family (70)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8676001B2 (en) | 2008-05-12 | 2014-03-18 | Google Inc. | Automatic discovery of popular landmarks |
US8396287B2 (en) * | 2009-05-15 | 2013-03-12 | Google Inc. | Landmarks from digital photo collections |
US8611592B2 (en) * | 2009-08-26 | 2013-12-17 | Apple Inc. | Landmark identification using metadata |
JP2011055250A (en) * | 2009-09-02 | 2011-03-17 | Sony Corp | Information providing method and apparatus, information display method and mobile terminal, program, and information providing system |
US8897816B2 (en) | 2010-06-17 | 2014-11-25 | Nokia Corporation | Method and apparatus for locating information from surroundings |
US8385593B2 (en) * | 2010-06-18 | 2013-02-26 | Google Inc. | Selecting representative images for establishments |
US8724910B1 (en) * | 2010-08-31 | 2014-05-13 | Google Inc. | Selection of representative images |
US9384408B2 (en) * | 2011-01-12 | 2016-07-05 | Yahoo! Inc. | Image analysis system and method using image recognition and text search |
US20120213404A1 (en) | 2011-02-18 | 2012-08-23 | Google Inc. | Automatic event recognition and cross-user photo clustering |
JP2012190244A (en) * | 2011-03-10 | 2012-10-04 | Fujitsu Ltd | Information providing method and information providing device |
US9251130B1 (en) * | 2011-03-31 | 2016-02-02 | Amazon Technologies, Inc. | Tagging annotations of electronic books |
US9552376B2 (en) | 2011-06-09 | 2017-01-24 | MemoryWeb, LLC | Method and apparatus for managing digital files |
US8688514B1 (en) | 2011-06-24 | 2014-04-01 | Google Inc. | Ad selection using image data |
US11087424B1 (en) | 2011-06-24 | 2021-08-10 | Google Llc | Image recognition-based content item selection |
US10972530B2 (en) | 2016-12-30 | 2021-04-06 | Google Llc | Audio-based data structure generation |
US8635519B2 (en) | 2011-08-26 | 2014-01-21 | Luminate, Inc. | System and method for sharing content based on positional tagging |
US20130086112A1 (en) | 2011-10-03 | 2013-04-04 | James R. Everingham | Image browsing system and method for a digital content platform |
US8737678B2 (en) | 2011-10-05 | 2014-05-27 | Luminate, Inc. | Platform for providing interactive applications on a digital content platform |
USD736224S1 (en) | 2011-10-10 | 2015-08-11 | Yahoo! Inc. | Portion of a display screen with a graphical user interface |
USD737290S1 (en) | 2011-10-10 | 2015-08-25 | Yahoo! Inc. | Portion of a display screen with a graphical user interface |
US11093692B2 (en) | 2011-11-14 | 2021-08-17 | Google Llc | Extracting audiovisual features from digital components |
US10586127B1 (en) | 2011-11-14 | 2020-03-10 | Google Llc | Extracting audiovisual features from content elements on online documents |
JP5775466B2 (en) * | 2012-01-13 | 2015-09-09 | インターナショナル・ビジネス・マシーンズ・コーポレーションＩｎｔｅｒｎａｔｉｏｎａｌ Ｂｕｓｉｎｅｓｓ Ｍａｃｈｉｎｅｓ Ｃｏｒｐｏｒａｔｉｏｎ | Chat extraction system, method, and program for extracting chat part from conversation |
US9026540B1 (en) * | 2012-01-31 | 2015-05-05 | Google Inc. | Systems and methods for information match scoring |
US9495334B2 (en) * | 2012-02-01 | 2016-11-15 | Adobe Systems Incorporated | Visualizing content referenced in an electronic document |
US8255495B1 (en) | 2012-03-22 | 2012-08-28 | Luminate, Inc. | Digital image and content display systems and methods |
US9122927B2 (en) | 2012-03-26 | 2015-09-01 | Google Inc. | Generating an image tour based on a set of images |
US8495489B1 (en) | 2012-05-16 | 2013-07-23 | Luminate, Inc. | System and method for creating and displaying image annotations |
US8996305B2 (en) * | 2012-06-07 | 2015-03-31 | Yahoo! Inc. | System and method for discovering photograph hotspots |
US9020278B2 (en) * | 2012-06-08 | 2015-04-28 | Samsung Electronics Co., Ltd. | Conversion of camera settings to reference picture |
US9391792B2 (en) | 2012-06-27 | 2016-07-12 | Google Inc. | System and method for event content stream |
US9036865B2 (en) * | 2012-09-12 | 2015-05-19 | International Business Machines Corporation | Location determination for an object using visual data |
US9317531B2 (en) * | 2012-10-18 | 2016-04-19 | Microsoft Technology Licensing, Llc | Autocaptioning of images |
US9418370B2 (en) | 2012-10-23 | 2016-08-16 | Google Inc. | Obtaining event reviews |
US9208170B1 (en) | 2013-03-15 | 2015-12-08 | Google Inc. | Classifying natural mapping features |
US10474714B2 (en) * | 2013-05-01 | 2019-11-12 | Kble Ltd | Method and component for classifying resources of a database |
CN103218460B (en) * | 2013-05-14 | 2016-08-10 | 清华大学 | Image tag complementing method based on the sparse reconstruct of optimum linearity |
US10402661B2 (en) | 2013-07-22 | 2019-09-03 | Opengate Development, Llc | Shape/object recognition using still/scan/moving image optical digital media processing |
US9082047B2 (en) * | 2013-08-20 | 2015-07-14 | Xerox Corporation | Learning beautiful and ugly visual attributes |
US9208171B1 (en) * | 2013-09-05 | 2015-12-08 | Google Inc. | Geographically locating and posing images in a large-scale image repository and processing framework |
US9069794B1 (en) * | 2013-10-11 | 2015-06-30 | Google Inc. | Determining location information for images using landmark, caption, and metadata location data |
US9531722B1 (en) | 2013-10-31 | 2016-12-27 | Google Inc. | Methods for generating an activity stream |
US9542457B1 (en) | 2013-11-07 | 2017-01-10 | Google Inc. | Methods for displaying object history information |
US9614880B1 (en) | 2013-11-12 | 2017-04-04 | Google Inc. | Methods for real-time notifications in an activity stream |
US10013639B1 (en) | 2013-12-16 | 2018-07-03 | Amazon Technologies, Inc. | Analyzing digital images based on criteria |
US9509772B1 (en) | 2014-02-13 | 2016-11-29 | Google Inc. | Visualization and control of ongoing ingress actions |
US10318543B1 (en) | 2014-03-20 | 2019-06-11 | Google Llc | Obtaining and enhancing metadata for content items |
US9536199B1 (en) | 2014-06-09 | 2017-01-03 | Google Inc. | Recommendations based on device usage |
US9507791B2 (en) | 2014-06-12 | 2016-11-29 | Google Inc. | Storage system user interface with floating file collection |
US10078781B2 (en) | 2014-06-13 | 2018-09-18 | Google Llc | Automatically organizing images |
US9842102B2 (en) * | 2014-11-10 | 2017-12-12 | Oracle International Corporation | Automatic ontology generation for natural-language processing applications |
US9471695B1 (en) * | 2014-12-02 | 2016-10-18 | Google Inc. | Semantic image navigation experiences |
US9870420B2 (en) | 2015-01-19 | 2018-01-16 | Google Llc | Classification and storage of documents |
RU2632133C2 (en) | 2015-09-29 | 2017-10-02 | Общество С Ограниченной Ответственностью "Яндекс" | Method (versions) and system (versions) for creating prediction model and determining prediction model accuracy |
US20170132821A1 (en) * | 2015-11-06 | 2017-05-11 | Microsoft Technology Licensing, Llc | Caption generation for visual media |
CN106776658B (en) * | 2015-11-25 | 2020-05-19 | 宏碁股份有限公司 | Photo arrangement method and electronic device thereof |
US10026021B2 (en) * | 2016-09-27 | 2018-07-17 | Facebook, Inc. | Training image-recognition systems using a joint embedding model on online social networks |
CA2977847A1 (en) * | 2017-01-27 | 2018-07-27 | Hootsuite Media Inc. | Automated extraction tools and their use in social content tagging systems |
JP7142420B2 (en) * | 2017-07-10 | 2022-09-27 | キヤノン株式会社 | Image processing device, learning method, trained model, image processing method |
RU2693324C2 (en) | 2017-11-24 | 2019-07-02 | Общество С Ограниченной Ответственностью "Яндекс" | Method and a server for converting a categorical factor value into its numerical representation |
US20200265081A1 (en) * | 2017-12-04 | 2020-08-20 | Huawei Technologies Co., Ltd. | Method and device for creating album title |
US11163941B1 (en) * | 2018-03-30 | 2021-11-02 | Snap Inc. | Annotating a collection of media content items |
CN110147701A (en) * | 2018-06-27 | 2019-08-20 | 腾讯科技（深圳）有限公司 | Key point mask method, device, computer equipment and storage medium |
CN112368546A (en) * | 2018-09-06 | 2021-02-12 | 谷歌有限责任公司 | Displaying personalized landmarks in mapping applications |
US10929714B2 (en) | 2018-11-19 | 2021-02-23 | Ford Global Technologies, Llc | High-throughput automated annotation of visual data for training neural networks used for landmark detection |
US10936178B2 (en) | 2019-01-07 | 2021-03-02 | MemoryWeb, LLC | Systems and methods for analyzing and organizing digital photos and videos |
US11790170B2 (en) * | 2019-01-10 | 2023-10-17 | Chevron U.S.A. Inc. | Converting unstructured technical reports to structured technical reports using machine learning |
CN110222569B (en) * | 2019-05-05 | 2021-04-23 | 北京三快在线科技有限公司 | Object detection method and device, electronic equipment and readable storage medium |
US11501067B1 (en) * | 2020-04-23 | 2022-11-15 | Wells Fargo Bank, N.A. | Systems and methods for screening data instances based on a target text of a target corpus |
US10909167B1 (en) * | 2020-09-17 | 2021-02-02 | Pure Memories Ltd | Systems and methods for organizing an image gallery |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101188021A (en) * | 2006-09-19 | 2008-05-28 | 西门子公司 | Result filter and method for selecting the result data of an application for automatic pattern recognition |
CN101228785A (en) * | 2005-07-26 | 2008-07-23 | 松下电器产业株式会社 | Image data management device and image data management method |
Family Cites Families (68)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH07168855A (en) | 1993-09-21 | 1995-07-04 | Toshiba Corp | Information recording/reproducing device |
JP3307843B2 (en) * | 1996-10-30 | 2002-07-24 | 松下電器産業株式会社 | Map display device in hypertext structure |
AU3639699A (en) | 1998-04-13 | 1999-11-01 | Eyematic Interfaces, Inc. | Wavelet-based facial motion capture for avatar animation |
JPH11328194A (en) | 1998-05-13 | 1999-11-30 | Nippon Telegr & Teleph Corp <Ntt> | Keyword retrieval method and device and storage medium storing keyword retrieval program |
US6711293B1 (en) | 1999-03-08 | 2004-03-23 | The University Of British Columbia | Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image |
JP2000259669A (en) * | 1999-03-12 | 2000-09-22 | Ntt Data Corp | Document classification device and its method |
US6411724B1 (en) * | 1999-07-02 | 2002-06-25 | Koninklijke Philips Electronics N.V. | Using meta-descriptors to represent multimedia information |
JP2002010178A (en) | 2000-06-19 | 2002-01-11 | Sony Corp | Image managing system and method for managing image as well as storage medium |
US7233942B2 (en) | 2000-10-10 | 2007-06-19 | Truelocal Inc. | Method and apparatus for providing geographically authenticated electronic documents |
JP3437555B2 (en) * | 2001-03-06 | 2003-08-18 | キヤノン株式会社 | Specific point detection method and device |
GB0114271D0 (en) * | 2001-06-12 | 2001-08-01 | Univ Manchester | Parameterisation |
JP2004021717A (en) | 2002-06-18 | 2004-01-22 | Toshiba Corp | Spatial data analyzer, spatial data analyzing program, and spatial data analyzing method |
US7911497B2 (en) | 2003-04-25 | 2011-03-22 | Lockheed Martin Corporation | Method and apparatus for video on demand |
JP4388301B2 (en) | 2003-05-08 | 2009-12-24 | オリンパス株式会社 | Image search apparatus, image search method, image search program, and recording medium recording the program |
US7313574B2 (en) | 2003-10-02 | 2007-12-25 | Nokia Corporation | Method for clustering and querying media items |
US20060020597A1 (en) | 2003-11-26 | 2006-01-26 | Yesvideo, Inc. | Use of image similarity in summarizing a collection of visual images |
WO2005055138A2 (en) | 2003-11-26 | 2005-06-16 | Yesvideo, Inc. | Statical modeling of a visual image for use in determining similarity between visual images |
US20060015497A1 (en) | 2003-11-26 | 2006-01-19 | Yesvideo, Inc. | Content-based indexing or grouping of visual images, with particular use of image similarity to effect same |
US7697792B2 (en) | 2003-11-26 | 2010-04-13 | Yesvideo, Inc. | Process-response statistical modeling of a visual image for use in determining similarity between visual images |
US7707239B2 (en) | 2004-11-01 | 2010-04-27 | Scenera Technologies, Llc | Using local networks for location information and image tagging |
US7574409B2 (en) | 2004-11-04 | 2009-08-11 | Vericept Corporation | Method, apparatus, and system for clustering and classification |
US7653249B2 (en) | 2004-11-17 | 2010-01-26 | Eastman Kodak Company | Variance-based event clustering for automatically classifying images |
US8027832B2 (en) * | 2005-02-11 | 2011-09-27 | Microsoft Corporation | Efficient language identification |
US8732175B2 (en) * | 2005-04-21 | 2014-05-20 | Yahoo! Inc. | Interestingness ranking of media objects |
US7760917B2 (en) * | 2005-05-09 | 2010-07-20 | Like.Com | Computer-implemented method for performing similarity searches |
US7353114B1 (en) | 2005-06-27 | 2008-04-01 | Google Inc. | Markup language for an interactive geographic information system |
US7840558B2 (en) | 2005-11-04 | 2010-11-23 | Microsoft Corporation | Geo-tagged based listing service and mapping engine |
JP2007142672A (en) | 2005-11-16 | 2007-06-07 | Fujifilm Corp | Method and device for image classification, and digital camera |
US8098899B2 (en) | 2005-11-14 | 2012-01-17 | Fujifilm Corporation | Landmark search system for digital camera, map data, and method of sorting image data |
US7663671B2 (en) | 2005-11-22 | 2010-02-16 | Eastman Kodak Company | Location based image classification with map segmentation |
EP1816836A3 (en) | 2005-12-30 | 2010-01-13 | LG Electronics Inc. | Apparatus and method for managing images of mobile terminal |
US7725451B2 (en) | 2006-01-23 | 2010-05-25 | Microsoft Corporation | Generating clusters of images for search results |
JP4671235B2 (en) | 2006-01-26 | 2011-04-13 | 田岡化学工業株式会社 | Method for producing fluorene derivative |
KR100641791B1 (en) * | 2006-02-14 | 2006-11-02 | (주)올라웍스 | Tagging Method and System for Digital Data |
US20070208776A1 (en) | 2006-03-06 | 2007-09-06 | Microsoft Corporation | Assignment of metadata |
JP2007316876A (en) * | 2006-05-25 | 2007-12-06 | Hitachi Ltd | Document retrieval program |
WO2007146298A2 (en) | 2006-06-12 | 2007-12-21 | Metacarta, Inc. | Systems and methods for hierarchical organization and presentation of geographic search results |
JP2007334505A (en) | 2006-06-13 | 2007-12-27 | Mitsubishi Electric Corp | Facility retrieval system, and mobile terminal and server to be used for the system |
US7739221B2 (en) | 2006-06-28 | 2010-06-15 | Microsoft Corporation | Visual and multi-dimensional search |
JP2008033399A (en) | 2006-07-26 | 2008-02-14 | Fujifilm Corp | Information providing system |
US7707208B2 (en) | 2006-10-10 | 2010-04-27 | Microsoft Corporation | Identifying sight for a location |
US7657504B2 (en) * | 2006-10-10 | 2010-02-02 | Microsoft Corporation | User interface for displaying images of sights |
US20080154886A1 (en) | 2006-10-30 | 2008-06-26 | Seeqpod, Inc. | System and method for summarizing search results |
US8037051B2 (en) | 2006-11-08 | 2011-10-11 | Intertrust Technologies Corporation | Matching and recommending relevant videos and media to individual search engine results |
JP4891740B2 (en) * | 2006-11-22 | 2012-03-07 | 株式会社日立製作所 | Content search apparatus and content search method |
US20080118160A1 (en) | 2006-11-22 | 2008-05-22 | Nokia Corporation | System and method for browsing an image database |
JP2008165303A (en) * | 2006-12-27 | 2008-07-17 | Fujifilm Corp | Content registration device, content registration method and content registration program |
JP4672692B2 (en) * | 2007-03-14 | 2011-04-20 | 株式会社東芝 | Word recognition system and word recognition program |
US20080268876A1 (en) | 2007-04-24 | 2008-10-30 | Natasha Gelfand | Method, Device, Mobile Terminal, and Computer Program Product for a Point of Interest Based Scheme for Improving Mobile Visual Searching Functionalities |
US8155399B2 (en) * | 2007-06-12 | 2012-04-10 | Utc Fire & Security Corporation | Generic face alignment via boosting |
WO2008152805A1 (en) * | 2007-06-14 | 2008-12-18 | Panasonic Corporation | Image recognizing apparatus and image recognizing method |
US20080320036A1 (en) * | 2007-06-22 | 2008-12-25 | Winter Gentle E | Automatic data collection |
US7870227B2 (en) | 2007-07-31 | 2011-01-11 | Yahoo! Inc. | System and method for merging internet protocol address to location data from multiple sources |
US10318110B2 (en) | 2007-08-13 | 2019-06-11 | Oath Inc. | Location-based visualization of geo-referenced context |
US20080104040A1 (en) | 2007-09-26 | 2008-05-01 | Ramakrishna Krishnamsetty C | Visually intuitive search method |
US9612126B2 (en) | 2007-12-03 | 2017-04-04 | Nokia Technologies Oy | Visual travel guide |
US8150098B2 (en) * | 2007-12-20 | 2012-04-03 | Eastman Kodak Company | Grouping images by location |
US8019536B2 (en) | 2007-12-28 | 2011-09-13 | At&T Intellectual Property I, L.P. | Methods, devices, and computer program products for geo-tagged photographic image augmented GPS navigation |
US7925653B2 (en) | 2008-02-27 | 2011-04-12 | General Electric Company | Method and system for accessing a group of objects in an electronic document |
US8676001B2 (en) | 2008-05-12 | 2014-03-18 | Google Inc. | Automatic discovery of popular landmarks |
US20090292685A1 (en) | 2008-05-22 | 2009-11-26 | Microsoft Corporation | Video search re-ranking via multi-graph propagation |
US8086048B2 (en) | 2008-05-23 | 2011-12-27 | Yahoo! Inc. | System to compile landmark image search results |
US8126249B2 (en) * | 2008-05-30 | 2012-02-28 | Optasia Medical Limited | Methods of and system for detection and tracking of osteoporosis |
US20100076976A1 (en) * | 2008-09-06 | 2010-03-25 | Zlatko Manolov Sotirov | Method of Automatically Tagging Image Data |
US8037011B2 (en) * | 2008-09-15 | 2011-10-11 | Motorola Mobility, Inc. | Method and apparatus for recommending content items |
US20100205176A1 (en) * | 2009-02-12 | 2010-08-12 | Microsoft Corporation | Discovering City Landmarks from Online Journals |
US8483715B2 (en) * | 2009-03-26 | 2013-07-09 | Yahoo! Inc. | Computer based location identification using images |
US8396287B2 (en) * | 2009-05-15 | 2013-03-12 | Google Inc. | Landmarks from digital photo collections |
-
2009
- 2009-05-15 US US12/466,880 patent/US8396287B2/en active Active
-
2010
- 2010-05-14 CA CA2762090A patent/CA2762090C/en active Active
- 2010-05-14 WO PCT/US2010/034930 patent/WO2010132789A1/en active Application Filing
- 2010-05-14 AU AU2010248862A patent/AU2010248862B2/en active Active
- 2010-05-14 EP EP10724937A patent/EP2430572A1/en not_active Ceased
- 2010-05-14 JP JP2012511045A patent/JP5680063B2/en active Active
- 2010-05-14 CN CN201080030849.3A patent/CN102549571B/en active Active
- 2010-05-14 KR KR1020117029949A patent/KR101672570B1/en active IP Right Grant
-
2013
- 2013-02-05 US US13/759,916 patent/US9020247B2/en active Active
-
2015
- 2015-04-10 US US14/683,643 patent/US9721188B2/en active Active
-
2017
- 2017-07-30 US US15/663,796 patent/US10303975B2/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101228785A (en) * | 2005-07-26 | 2008-07-23 | 松下电器产业株式会社 | Image data management device and image data management method |
CN101188021A (en) * | 2006-09-19 | 2008-05-28 | 西门子公司 | Result filter and method for selecting the result data of an application for automatic pattern recognition |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104541515A (en) * | 2012-08-08 | 2015-04-22 | 谷歌公司 | Browsing images of a point of interest within an image graph |
CN103853797A (en) * | 2012-12-07 | 2014-06-11 | 中兴通讯股份有限公司 | Image retrieval method and system based on n-gram image indexing structure |
CN103853792A (en) * | 2012-12-07 | 2014-06-11 | 中兴通讯股份有限公司 | Automatic image semantic annotation method and system |
CN103853797B (en) * | 2012-12-07 | 2017-10-17 | 中兴通讯股份有限公司 | A kind of picture retrieval method and system based on n member picture indices structures |
CN103226575A (en) * | 2013-04-01 | 2013-07-31 | 北京小米科技有限责任公司 | Image processing method and device |
CN104794171A (en) * | 2015-03-31 | 2015-07-22 | 百度在线网络技术（北京）有限公司 | Method and device for marking geographical location information of picture |
CN104794171B (en) * | 2015-03-31 | 2018-06-05 | 百度在线网络技术（北京）有限公司 | Mark the method and device of picture geographical location information |
Also Published As
Publication number | Publication date |
---|---|
EP2430572A1 (en) | 2012-03-21 |
US20130202198A1 (en) | 2013-08-08 |
US10303975B2 (en) | 2019-05-28 |
US20100290699A1 (en) | 2010-11-18 |
US20180211134A1 (en) | 2018-07-26 |
CA2762090A1 (en) | 2010-11-18 |
US8396287B2 (en) | 2013-03-12 |
CA2762090C (en) | 2018-09-04 |
KR101672570B1 (en) | 2016-11-03 |
AU2010248862A1 (en) | 2012-01-12 |
JP2012527057A (en) | 2012-11-01 |
US20150213329A1 (en) | 2015-07-30 |
KR20120026093A (en) | 2012-03-16 |
AU2010248862B2 (en) | 2016-06-09 |
JP5680063B2 (en) | 2015-03-04 |
CN102549571B (en) | 2015-11-25 |
US9721188B2 (en) | 2017-08-01 |
WO2010132789A1 (en) | 2010-11-18 |
US9020247B2 (en) | 2015-04-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN102549571B (en) | From the terrestrial reference of digital picture set | |
Zheng et al. | Tour the world: building a web-scale landmark recognition engine | |
CN102053991B (en) | Method and system for multi-language document retrieval | |
Burie et al. | ICDAR2015 competition on smartphone document capture and OCR (SmartDoc) | |
Zhu et al. | Land use classification using convolutional neural networks applied to ground-level images | |
JP6211407B2 (en) | Image search system, image search device, search server device, image search method, and image search program | |
Weyand et al. | Visual landmark recognition from internet photo collections: A large-scale evaluation | |
Al-asadi et al. | Object based image retrieval using enhanced SURF | |
US9208171B1 (en) | Geographically locating and posing images in a large-scale image repository and processing framework | |
Weyand et al. | An evaluation of two automatic landmark building discovery algorithms for city reconstruction | |
Ivanov et al. | Object-based tag propagation for semi-automatic annotation of images | |
Gominski et al. | Challenging deep image descriptors for retrieval in heterogeneous iconographic collections | |
Li et al. | Global-scale location prediction for social images using geo-visual ranking | |
Moreira et al. | Image provenance analysis | |
Jones et al. | Automated annotation of landmark images using community contributed datasets and web resources | |
WO2017107361A1 (en) | Method and device for determining landscape information of picture | |
Evans et al. | Livemaps: Learning geo-intent from images of maps on a large scale | |
Allili et al. | Image Copy Detection and Evolution Visualisation Using Three Graphs." | |
Glistrup et al. | Urban Image Geo-Localization Using Open Data on Public Spaces | |
Girdhar et al. | Mobile Visual Search for Digital Heritage Applications | |
Evertsen | Automatic image tagging based on context information | |
Cheng et al. | Image near-duplicate retrieval using local dependencies in spatial-scale space | |
Weiland et al. | Weakly supervised construction of a repository of iconic images | |
Dang et al. | Camera-based document image spotting system for complex linguistic maps | |
EID et al. | Image Retrieval based on Reverse Geocoding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
C14 | Grant of patent or utility model | ||
GR01 | Patent grant | ||
CP01 | Change in the name or title of a patent holder | ||
CP01 | Change in the name or title of a patent holder |
Address after: American CaliforniaPatentee after: Google limited liability companyAddress before: American CaliforniaPatentee before: Google Inc. |