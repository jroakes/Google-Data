WO2020061008A1 - Receptive-field-conforming convolution models for video coding - Google Patents
Receptive-field-conforming convolution models for video coding Download PDFInfo
- Publication number
- WO2020061008A1 WO2020061008A1 PCT/US2019/051458 US2019051458W WO2020061008A1 WO 2020061008 A1 WO2020061008 A1 WO 2020061008A1 US 2019051458 W US2019051458 W US 2019051458W WO 2020061008 A1 WO2020061008 A1 WO 2020061008A1
- Authority
- WO
- WIPO (PCT)
- Prior art keywords
- block
- size
- layers
- feature
- layer
- Prior art date
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
Definitions
- Digital video streams may represent video using a sequence of frames or still images.
- Digital video can be used for various applications, including, for example, video conferencing, high-definition video entertainment, video advertisements, or sharing of user generated videos.
- a digital video stream can contain a large amount of data and consume a significant amount of computing or communication resources of a computing device for processing, transmission, or storage of the video data.
- Various approaches have been proposed to reduce the amount of data in video streams, including compression and other encoding techniques.
- Coding efficiency can mean encoding a video at the lowest possible bit rate while minimizing distortion (i.e., while maintaining a certain level of video quality).
- the improved coding efficiency has resulted in increased computational complexity. That is, more computation time is required by an encoder to achieve the improved coding efficiency. As such, it is desirable to obtain improved coding efficiencies with less computation time (i.e., reduced computational complexity).
- One aspect of the disclosed implementations is a convolutional neural network (CNN) for determining a mode decision for encoding a block in video coding.
- the CNN includes feature extraction layers for extracting features of the block for determining the mode decision.
- a non-overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size, the mode decision comprises a block partitioning of the block, the block has a NxN size, and a smallest partition output for the block has a SxS size.
- the CNN also includes multiple classifiers.
- Each classifier comprises classification layers, each classification layer of the classification layers receiving respective feature maps having a respective feature dimension.
- An initial classification layer of each classifier can receive the feature maps as an output of a final feature extraction layer of the feature extraction layers.
- the output map can indicate one or more mode decisions for the block. For example, the output map can indicate a partition decision.
- the output map may be used to encode the block.
- Another aspect is a method of determining a mode decision for encoding a block in video coding using a convolutional neural network (CNN).
- the method includes extracting, using feature extraction layers of the CNN, features of the block for determining the mode decision, wherein a non-overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size, the mode decision comprises a block partitioning of the block, the block has a NxN size, and a smallest partition output for the block has a SxS size.
- the method also includes inferring, by multiple classifiers of the CNN that each include classification layers, the mode decision.
- Inferring the mode decisions includes applying, at some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half, and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
- An initial classification layer of each classifier may receive the feature maps as an output of a final feature extraction layer of the feature extraction layers.
- the mode decision as indicated by the output, may be used to encode the block.
- the apparatus includes a processor configured to execute a method including receiving, in a compressed bitstream, an indication of a partitioning of the image block into sub-blocks.
- An encoder determined the partitioning of the image block using a convolutional neural network (CNN) that includes feature extraction layers for extracting features of the block for determining the partitioning, wherein a non-overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size, the block has a NxN size, and a smallest partition output for the block has a SxS size.
- CNN convolutional neural network
- the CNN also includes multiple classifiers, wherein each classifier comprises classification layers, each classification layer of the classification layers receiving respective feature maps having a respective feature dimension.
- An initial classification layer of each classifier can receive the feature maps as an output of a final feature extraction layer of the feature extraction layers.
- the method also includes decoding the image block using the indication of the partitioning of the image block.
- CNN convolutional neural network
- the CNN includes feature extraction layers; a concatenation layer that receives, from the feature extraction layers, first feature maps of the block, where each first feature map is of size SxS; and classifiers.
- Each classifier includes classification layers, each classification layer receives second feature maps having a respective feature dimension.
- N/S by: applying, at least some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
- Another aspect is a method of determining a block partitioning in video coding using a convolutional neural network (CNN), the block has an NxN size, and a smallest partition determined by the CNN is of size SxS.
- the method includes extracting, using feature extraction layers of the CNN, first feature maps of the block, where each first feature map is of size SxS; concatenating, using a concatenation layer of the CNN, the first feature maps of the block; and inferring, by classifiers of the CNN each including classification layers, block partitioning.
- CNN convolutional neural network
- classification layers a kernel of size lxl to reduce the respective feature dimension in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
- Another aspect is an apparatus for decoding an image block, including a processor that receives, in a compressed bitstream, an indication of a quad-tree partitioning of the image block into sub-blocks, wherein an encoder determined the quad-tree partitioning of the image block using a convolutional neural network (CNN); and decodes the image block using the indication of the quad-tree partitioning of the image block.
- the CNN includes feature extraction layers; a concatenation layer that receives, from the feature extraction layers, first feature maps of the block, where each first feature map is of size SxS; and classifiers. Each classifier includes classification layers. Each classification layer receives second feature maps having a respective feature dimension.
- FIG. 1 is a schematic of a video encoding and decoding system.
- FIG. 2 is a block diagram of an example of a computing device that can implement a transmitting station or a receiving station.
- FIG. 3 is a diagram of a video stream to be encoded and subsequently decoded.
- FIG. 4 is a block diagram of an encoder according to implementations of this disclosure.
- FIG. 5 is a block diagram of a decoder according to implementations of this disclosure.
- FIG. 6 is a block diagram of a representation of a portion of a frame according to implementations of this disclosure.
- FIG. 7 is a block diagram of an example of a quad-tree representation of a block according to implementations of this disclosure.
- FIG. 8 is a flowchart of a process for searching for a best mode to code a block.
- FIG. 9 is a block diagram of an example of estimating the rate and distortion costs of coding an image block using a prediction mode.
- FIG. 10 is a block diagram of a first example of a convolutional neural network (CNN) for a mode decision according to implementations of this disclosure.
- CNN convolutional neural network
- FIG. 11 is an example of convolution operations according to implementations of this disclosure.
- FIG. 12 is an example of receptive fields according to implementations of this disclosure.
- FIG. 13 is a block diagram of a second example of a CNN for a mode decision according to implementations of this disclosure.
- FIG. 14 is a block diagram of a third example of a CNN for a mode decision according to implementations of this disclosure.
- FIG. 15 is a block diagram of a fourth example of a CNN for a mode decision according to implementations of this disclosure.
- FIG. 16 is a block diagram of a fifth example of a CNN for a mode decision according to implementations of this disclosure.
- FIG. 17 is a flowchart of a process for encoding, by an encoder, an image block according to implementations of this disclosure.
- FIG. 18 is an example of approximating a non-linear function of a quantization parameter using linear segments according to implementations of this disclosure.
- FIG. 19 is an example of a rate-distortion performance comparison of a first machine-learning model that uses a non-linear QP function as input and a second machine- learning model that uses a linear QP function as input.
- FIG. 20 is an example of non-square partitions of a block.
- Encoding techniques may be designed to maximize coding efficiency.
- Coding efficiency can mean encoding a video at the lowest possible bit rate while minimizing distortion (e.g., while maintaining a certain level of video quality).
- Coding efficiency is typically measured in terms of both rate and distortion.
- Rate refers to the number of bits required for encoding (such as encoding a block, a frame, etc.).
- Distortion measures the quality loss between, for example, a source video block and a reconstructed version of the source video block. For example, the distortion may be calculated as a mean-square error between pixel values of the source block and those of the reconstructed block.
- a video codec optimizes the amount of distortion against the rate required to encode the video.
- Modem video codecs (e.g., H.264, which is also known as MPEG-4 AVC; VP9; H.265, which is also known as HEVC; AVS2; and AV1) define and use a large number of tools and configurations (e.g., parameters) to improve coding efficiency.
- a video encoder can use a mode decision to examine (e.g., test, evaluate, etc.) at least some of the valid combinations of parameters to select a combination that results in a relatively low rate- distortion value.
- An example of a mode decision is an intra-prediction mode decision, which determines the best intra-prediction mode for coding a block.
- a mode decision is a partition decision, which determines an optimal partitioning of a coding unit.
- Another example of a mode decision includes a decision as to a transform type and/or size to use in transforming a block (such as a residual or an image block) from the pixel domain to the frequency domain to form a transform block that includes transform coefficients.
- a metric can be computed for each of the examined combinations and the respective metrics compared.
- the metric can combine the rate and distortion described above to produce a rate- distortion (RD) value or cost.
- the RD value or cost may be a single scalar value.
- the RD cost associated with a specific mode may be determined by performing at least a subset of the encoding steps of the encoder.
- the subset of the encoding steps can include, depending on the mode for which a RD cost is to be determined, at least one of determining a prediction block, determining a residual block, determining a transform type, determining an interpolation filter, quantizing a transform block, entropy encoding, and so on.
- an encoder may terminate a mode search as soon as it finds a mode with a RD cost that is less than a set threshold. This means, however, that a better mode may have been found later on if the encoder had continued in mode search.
- an exhaustive search may or may not be performed, but the entire RD cost calculation is replaced by a coarse estimation. This can further degrade decision making by an encoder.
- a best mode such as a partition mode. For example, instead of performing all of the encoding steps (i.e., a brute-force or exhaustive approach) for determining the rate and distortion for various partitioning modes to compare those modes and select a best mode, a machine-learning model can be used to estimate or infer the best mode.
- the machine-learning model may be trained using the vast amount of training data that is available from an encoder performing standard encoding techniques, such as those described below. More specifically, the training data can be used during the learning phase of machine learning to derive (e.g., learn, infer, etc.) the machine-learning model that is (e.g., defines, constitutes, etc.) a mapping from the input data (e.g., block data) to an output.
- the training data can be used during the learning phase of machine learning to derive (e.g., learn, infer, etc.) the machine-learning model that is (e.g., defines, constitutes, etc.) a mapping from the input data (e.g., block data) to an output.
- the model computes the output as a deterministic function of its input.
- the machine-learning model can be a neural network model, which can be implemented by a convolutional neural network (CNN).
- CNN convolutional neural network
- a well-trained machine-learning model can be expected to closely match the brute-force approach in coding efficiency but at a significantly lower computational cost or with a regular or dataflow-oriented computational cost.
- the architecture of the machine-learning model can also be critical to the performance and/or prediction capability of the model.
- the models described herein comprise convolution layers of a CNN with filter designs that respect boundaries for recursive partitioning. That is, when analyzing an image region, such as for determining a quad-tree partitioning, the features extracted (e.g., calculated, inferred, etc.) for the image region are confined to the image region itself, and not for other image regions. Further, the models described herein allow the models to have a small size (i.e., those with a reduced number of parameters as compared to models using fully connected layers). The inference accuracy for mode decision in video encoding can be significantly improved while reducing the computational complexity.
- FIG. 1 is a schematic of a video encoding and decoding system 100.
- a transmitting station 102 can be, for example, a computer having an internal configuration of hardware, such as that described with respect to FIG. 2.
- other suitable transmitting station 102 can be, for example, a computer having an internal configuration of hardware, such as that described with respect to FIG. 2.
- other suitable transmitting station 102 can be, for example, a computer having an internal configuration of hardware, such as that described with respect to FIG. 2.
- other suitable transmitting station 102 can be, for example, a computer having an internal configuration of hardware, such as that described with respect to FIG. 2.
- other suitable transmitting station 102 can be, for example, a computer having an internal configuration of hardware, such as that described with respect to FIG. 2.
- other suitable transmitting station 102 can be, for example, a computer having an internal configuration of hardware, such as that described with respect to FIG. 2.
- other suitable transmitting station 102 can be, for example, a computer having an internal configuration of hardware,
- the processing of the transmitting station 102 can be distributed among multiple devices.
- a network 104 can connect the transmitting station 102 and a receiving station 106 for encoding and decoding of the video stream.
- the video stream can be encoded in the transmitting station 102, and the encoded video stream can be decoded in the receiving station 106.
- the network 104 can be, for example, the Internet.
- the network 104 can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmitting station 102 to, in this example, the receiving station 106.
- the receiving station 106 can be a computer having an internal configuration of hardware, such as that described with respect to FIG. 2. However, other suitable implementations of the receiving station 106 are possible. For example, the processing of the receiving station 106 can be distributed among multiple devices.
- a video stream can be encoded and then stored for transmission at a later time to the receiving station 106 or any other device having memory.
- the receiving station 106 receives (e.g., via the network 104, a computer bus, and/or some communication pathway) the encoded video stream and stores the video stream for later decoding.
- a real-time transport protocol RTP
- a transport protocol other than RTP e.g., an HTTP-based video streaming protocol
- the transmitting station 102 and/or the receiving station 106 may include the ability to both encode and decode a video stream as described below.
- the receiving station 106 could be a video conference participant who receives an encoded video bitstream from a video conference server (e.g., the transmitting station 102) to decode and view and further encodes and transmits its own video bitstream to the video conference server for decoding and viewing by other participants.
- FIG. 2 is a block diagram of an example of a computing device 200 that can implement a transmitting station or a receiving station.
- the computing device 200 can implement one or both of the transmitting station 102 and the receiving station 106 of FIG. 1.
- the computing device 200 can be in the form of a computing system including multiple computing devices, or in the form of a single computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like.
- a CPU 202 in the computing device 200 can be a central processing unit.
- the CPU 202 can be any other type of device, or multiple devices, now-existing or hereafter developed, capable of manipulating or processing information.
- the disclosed implementations can be practiced with a single processor as shown (e.g., the CPU 202), advantages in speed and efficiency can be achieved by using more than one processor.
- a memory 204 in the computing device 200 can be a read only memory (ROM) device or a random-access memory (RAM) device. Any other suitable type of storage device can be used as the memory 204.
- the memory 204 can include code and data 206 that is accessed by the CPU 202 using a bus 212.
- the memory 204 can further include an operating system 208 and application programs 210, the application programs 210 including at least one program that permits the CPU 202 to perform the methods described herein.
- the application programs 210 can include applications 1 through N, which further include a video coding application that performs the methods described herein.
- the computing device 200 can also include a secondary storage 214, which can, for example, be a memory card used with a computing device 200 that is mobile. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- a secondary storage 214 can, for example, be a memory card used with a computing device 200 that is mobile. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- the computing device 200 can also include one or more output devices, such as a display 218.
- the display 218 may be, in one example, a touch-sensitive display that combines a display with a touch-sensitive element that is operable to sense touch inputs.
- the display 218 can be coupled to the CPU 202 via the bus 212.
- Other output devices that permit a user to program or otherwise use the computing device 200 can be provided in addition to or as an alternative to the display 218.
- the output device is or includes a display
- the display can be implemented in various ways, including as a liquid crystal display (LCD); a cathode -ray tube (CRT) display; or a light-emitting diode (LED) display, such as an organic LED
- the computing device 200 can also include or be in communication with an image-sensing device 220, for example, a camera, or any other image-sensing device, now existing or hereafter developed, that can sense an image, such as the image of a user operating the computing device 200.
- the image-sensing device 220 can be positioned such that it is directed toward the user operating the computing device 200.
- the position and optical axis of the image-sensing device 220 can be configured such that the field of vision includes an area that is directly adjacent to the display 218 and from which the display 218 is visible.
- the computing device 200 can also include or be in communication with a sound sensing device 222, for example, a microphone, or any other sound-sensing device, now existing or hereafter developed, that can sense sounds near the computing device 200.
- the sound-sensing device 222 can be positioned such that it is directed toward the user operating the computing device 200 and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates the computing device 200.
- FIG. 2 depicts the CPU 202 and the memory 204 of the computing device 200 as being integrated into a single unit, other configurations can be utilized.
- the operations of the CPU 202 can be distributed across multiple machines (each machine having one or more processors) that can be coupled directly or across a local area or other network.
- the memory 204 can be distributed across multiple machines, such as a network-based memory or memory in multiple machines performing the operations of the computing device 200.
- the bus 212 of the computing device 200 can be composed of multiple buses.
- the secondary storage 214 can be directly coupled to the other components of the computing device 200 or can be accessed via a network and can comprise a single integrated unit, such as a memory card, or multiple units, such as multiple memory cards.
- the computing device 200 can thus be implemented in a wide variety of configurations.
- FIG. 3 is a diagram of an example of a video stream 300 to be encoded and subsequently decoded.
- the video stream 300 includes a video sequence 302.
- the video sequence 302 includes a number of adjacent frames 304. While three frames are depicted as the adjacent frames 304, the video sequence 302 can include any number of
- the frame 306 can be divided into a series of segments 308 or planes.
- the segments 308 can be subsets of frames that permit parallel processing, for example.
- the segments 308 can also be subsets of frames that can separate the video data into separate colors.
- the frame 306 of color video data can include a luminance (or luma) plane and two chrominance (or chroma) planes.
- the segments 308 may be sampled at different resolutions.
- the frame 306 may be further subdivided into blocks 310, which can contain data corresponding to, for example, 16x16 pixels in the frame 306.
- the blocks 310 can also be arranged to include data from one or more segments 308 of pixel data.
- the blocks 310 can also be of any other suitable size, such as 4x4 pixels, 8x8 pixels, 16x8 pixels, 8x16 pixels, 16x16 pixels, or larger.
- a block may comprise luma pixels from the luma plane, or chroma pixels from the chroma plane.
- FIG. 4 is a block diagram of an encoder 400 in accordance with implementations of this disclosure.
- the encoder 400 can be implemented, as described above, in the transmitting station 102, such as by providing a computer software program stored in memory, for example, the memory 204.
- the computer software program can include machine instructions that, when executed by a processor, such as the CPU 202, cause the transmitting station 102 to encode video data in manners described herein.
- the encoder 400 can also be implemented as specialized hardware included in, for example, the transmitting station 102.
- the encoder 400 has the following stages to perform the various functions in a forward path (shown by the solid connection lines) to produce an encoded or compressed bitstream 420 using the video stream 300 as input: an intra/inter-prediction stage 402, a transform stage 404, a quantization stage 406, and an entropy encoding stage 408.
- the encoder 400 may also include a reconstruction path (shown by the dotted connection lines) to reconstruct a frame for encoding of future blocks.
- the encoder 400 has the following stages to perform the various functions in the reconstruction path: a dequantization stage 410, an inverse transform stage 412, a reconstruction stage 414, and a loop filtering stage 416.
- Other structural variations of the encoder 400 can be used to encode the video stream 300.
- the frame 306 can be processed in units of blocks.
- a block can be encoded using intra-frame prediction (also called intra-prediction) or inter-frame prediction (also called inter-prediction), or a combination of both.
- intra-prediction also called intra-prediction
- inter-prediction also called inter-prediction
- a prediction block can be formed.
- intra-prediction all or part of a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed.
- inter-prediction all or part of a prediction block may be formed from samples in one or more previously constructed reference frames determined using motion vectors.
- the prediction block can be subtracted from the current block at the intra/inter-prediction stage 402 to produce a residual block (also called a residual).
- the transform stage 404 transforms the residual into transform coefficients in, for example, the frequency domain using block-based transforms.
- block-based transforms i.e., transform types
- DCT Discrete Cosine Transform
- ADST Asymmetric Discrete Sine Transform
- Other block-based transforms are possible.
- combinations of different transforms may be applied to a single residual.
- the DCT transforms the residual block into the frequency domain where the transform coefficient values are based on spatial frequency.
- the lowest frequency (DC) coefficient is at the top-left of the matrix, and the highest frequency coefficient is at the bottom-right of the matrix. It is worth noting that the size of a prediction block, and hence the resulting residual block, may be different from the size of the transform block. For example, the prediction block may be split into smaller blocks to which separate transforms are applied.
- the quantization stage 406 converts the transform coefficients into discrete quantum values, which are referred to as quantized transform coefficients, using a quantizer value or a quantization level. For example, the transform coefficients may be divided by the quantizer value and truncated.
- the quantized transform coefficients are then entropy encoded by the entropy encoding stage 408. Entropy coding may be performed using any number of techniques, including token and binary trees.
- the entropy-encoded coefficients, together with other information used to decode the block (which may include, for example, the type of prediction used, transform type, motion vectors, and quantizer value), are then output to the compressed bitstream 420.
- the information to decode the block may be entropy coded into block, frame, slice, and/or section headers within the compressed bitstream 420.
- the compressed bitstream 420 can also be referred to as an encoded video stream or encoded video bitstream; these terms will be used interchangeably herein.
- the reconstruction path in FIG. 4 can be used to ensure that both the encoder 400 and a decoder 500 (described below) use the same reference frames and blocks to decode the compressed bitstream 420.
- the reconstruction path performs functions that are similar to functions that take place during the decoding process and that are discussed in more detail below, including dequantizing the quantized transform coefficients at the dequantization stage 410 and inverse transforming the dequantized transform coefficients at the inverse transform stage 412 to produce a derivative residual block (also called a derivative residual).
- the prediction block that was predicted at the intra/inter-prediction stage 402 can be added to the derivative residual to create a reconstructed block.
- the loop filtering stage 416 can be applied to the reconstructed block to reduce distortion, such as blocking artifacts.
- encoder 400 can be used to encode the compressed bitstream 420.
- a non-transform based encoder can quantize the residual signal directly without the transform stage 404 for certain blocks or frames.
- a non-transform based encoder can quantize the residual signal directly without the transform stage 404 for certain blocks or frames.
- an encoder 400 can have the quantization stage 406 and the dequantization stage 410 combined into a single stage.
- FIG. 5 is a block diagram of a decoder 500 in accordance with implementations of this disclosure.
- the decoder 500 can be implemented in the receiving station 106, for example, by providing a computer software program stored in the memory 204.
- the computer software program can include machine instructions that, when executed by a processor, such as the CPU 202, cause the receiving station 106 to decode video data in the manners described below.
- the decoder 500 can also be implemented in hardware included in, for example, the transmitting station 102 or the receiving station 106.
- the decoder 500 similar to the reconstruction path of the encoder 400 discussed above, includes in one example the following stages to perform various functions to produce an output video stream 516 from the compressed bitstream 420: an entropy decoding stage 502, a dequantization stage 504, an inverse transform stage 506, an intra/inter-prediction stage 508, a reconstruction stage 510, a loop filtering stage 512, and a post filtering stage 514.
- stages to perform various functions to produce an output video stream 516 from the compressed bitstream 420 includes in one example the following stages to perform various functions to produce an output video stream 516 from the compressed bitstream 420: an entropy decoding stage 502, a dequantization stage 504, an inverse transform stage 506, an intra/inter-prediction stage 508, a reconstruction stage 510, a loop filtering stage 512, and a post filtering stage 514.
- Other structural variations of the decoder 500 can be used to decode the compressed bitstream 420.
- the data elements within the compressed bitstream 420 can be decoded by the entropy decoding stage 502 to produce a set of quantized transform coefficients.
- the dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and the inverse transform stage 506 inverse transforms the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and the inverse transform stage 506 inverse transforms the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and the inverse transform stage 506 inverse transforms the quantized transform coefficients.
- the decoder 500 can use the intra/inter-prediction stage 508 to create the same prediction block as was created in the encoder 400, for example, at the intra/inter-prediction stage 402.
- the prediction block can be added to the derivative residual to create a
- the loop filtering stage 512 can be applied to the reconstructed block to reduce blocking artifacts. Other filtering can be applied to the reconstructed block.
- the post filtering stage 514 is applied to the reconstructed block to reduce blocking distortion, and the result is output as an output video stream 516.
- the output video stream 516 can also be referred to as a decoded video stream; these terms will be used
- the decoder 500 can be used to decode the compressed bitstream 420.
- the decoder 500 can produce the output video stream 516 without the post filtering stage 514.
- the post filtering stage 514 is applied after the loop filtering stage 512.
- the loop filtering stage 512 can include an optional deblocking filtering stage.
- the encoder 400 includes an optional deblocking filtering stage in the loop filtering stage 416.
- FIG. 6 is a block diagram of a representation of a portion 600 of a frame, such as the frame 306 of FIG. 3, according to implementations of this disclosure.
- the portion 600 of the frame includes four 64x64 blocks 610, which may be referred to as superblocks, in two rows and two columns in a matrix or Cartesian plane.
- a superblock can have a larger or a smaller size. While FIG. 6 is explained with respect to a superblock of size 64x64, the description is easily extendable to larger (e.g., 128x128) or smaller superblock sizes.
- a superblock can be a basic or maximum coding unit (CU).
- Each superblock can include four 32x32 blocks 620.
- Each 32x32 block 620 can include four 16x16 blocks 630.
- Each 16x16 block 630 can include four 8x8 blocks 640.
- Each 8x8 block 640 can include four 4x4 blocks 650.
- Each 4x4 block 650 can include 16 pixels, which can be represented in four rows and four columns in each respective block in the Cartesian plane or matrix. The pixels can include information representing an image captured in the frame, such as luminance information, color information, and location information.
- a block such as a 16x16-pixel block as shown, can include a luminance block 660, which can include luminance pixels 662; and two chrominance blocks 670/680, such as a U or Cb chrominance block 670, and a V or Cr chrominance block 680.
- the chrominance blocks 670/680 can include chrominance pixels 690.
- the luminance block 660 can include 16x16 luminance pixels 662
- each chrominance block 670/680 can include 8x8 chrominance pixels 690, as shown.
- NxN blocks in some implementations, NxM, where N1M, blocks can be used.
- NxM 32x64 blocks, 64x32 blocks, 16x32 blocks, 32x16 blocks, or any other size blocks can be used.
- Nx2N blocks, 2NxN blocks, or a combination thereof can be used.
- video coding can include ordered block-level coding.
- Ordered block- level coding can include coding blocks of a frame in an order, such as raster- scan order, wherein blocks can be identified and processed starting with a block in the upper left corner of the frame, or a portion of the frame, and proceeding along rows from left to right and from the top row to the bottom row, identifying each block in turn for processing.
- the superblock in the top row and left column of a frame can be the first block coded
- the superblock immediately to the right of the first block can be the second block coded.
- the second row from the top can be the second row coded, such that the superblock in the left column of the second row can be coded after the superblock in the rightmost column of the first row.
- coding a block can include using quad-tree coding, which can include coding smaller block units with a block in raster-scan order.
- quad-tree coding can include coding smaller block units with a block in raster-scan order.
- the 64x64 superblock shown in the bottom-left comer of the portion of the frame shown in FIG. 6, for example, can be coded using quad-tree coding in which the top-left 32x32 block can be coded, then the top-right 32x32 block can be coded, then the bottom-left 32x32 block can be coded, and then the bottom-right 32x32 block can be coded.
- Each 32x32 block can be coded using quad-tree coding in which the top-left 16x16 block can be coded, then the top-right 16x16 block can be coded, then the bottom- left 16x16 block can be coded, and then the bottom-right 16x16 block can be coded.
- Each 16x16 block can be coded using quad- tree coding in which the top-left 8x8 block can be coded, then the top-right 8x8 block can be coded, then the bottom-left 8x8 block can be coded, and then the bottom-right 8x8 block can be coded.
- Each 8x8 block can be coded using quad-tree coding in which the top-left 4x4 block can be coded, then the top- right 4x4 block can be coded, then the bottom-left 4x4 block can be coded, and then the bottom-right 4x4 block can be coded.
- 8x8 blocks can be omitted for a 16x16 block, and the 16x16 block can be coded using quad- tree coding in which the top-left 4x4 block can be coded, and then the other 4x4 blocks in the 16x16 block can be coded in raster-scan order.
- video coding can include compressing the information included in an original, or input, frame by omitting some of the information in the original frame from a corresponding encoded frame.
- coding can include reducing spectral redundancy, reducing spatial redundancy, reducing temporal redundancy, or a combination thereof.
- reducing spectral redundancy can include using a color model based on a luminance component (Y) and two chrominance components (U and V or Cb and Cr), which can be referred to as the YUV or YCbCr color model or color space.
- YUV color model can include using a relatively large amount of information to represent the luminance component of a portion of a frame and using a relatively small amount of information to represent each corresponding chrominance component for the portion of the frame.
- a portion of a frame can be represented by a high-resolution luminance component, which can include a 16x16 block of pixels, and by two lower resolution chrominance components, each of which representing the portion of the frame as an 8x8 block of pixels.
- a pixel can indicate a value (e.g., a value in the range from 0 to 255) and can be stored or transmitted using, for example, eight bits.
- Reducing spatial redundancy can include transforming a block into the frequency domain as described above.
- a unit of an encoder such as the entropy encoding stage 408 of FIG. 4, can perform a DCT using transform coefficient values based on spatial frequency.
- Reducing temporal redundancy can include using similarities between frames to encode a frame using a relatively small amount of data based on one or more reference frames, which can be previously encoded, decoded, and reconstructed frames of the video stream.
- a block or a pixel of a current frame can be similar to a spatially corresponding block or pixel of a reference frame.
- a block or a pixel of a current frame can be similar to a block or a pixel of a reference frame at a different spatial location.
- reducing temporal redundancy can include generating motion information indicating the spatial difference (e.g., a translation between the location of the block or the pixel in the current frame and the corresponding location of the block or the pixel in the reference frame).
- Reducing temporal redundancy can include identifying a block or a pixel in a reference frame, or a portion of the reference frame, that corresponds with a current block or pixel of a current frame.
- a reference frame, or a portion of a reference frame, which can be stored in memory can be searched for the best block or pixel to use for encoding a current block or pixel of the current frame.
- the search may identify the block of the reference frame for which the difference in pixel values between the reference block and the current block is minimized, and can be referred to as motion searching.
- the portion of the reference frame searched can be limited.
- the portion of the reference frame searched which can be referred to as the search area, can include a limited number of rows of the reference frame.
- identifying the reference block can include calculating a cost function, such as a sum of absolute differences (SAD), between the pixels of the blocks in the search area and the pixels of the current block.
- SAD sum of absolute differences
- the spatial difference between the location of the reference block in the reference frame and the current block in the current frame can be represented as a motion vector.
- the difference in pixel values between the reference block and the current block can be referred to as differential data, residual data, or as a residual block.
- generating motion vectors can be referred to as motion estimation, and a pixel of a current block can be indicated based on location using Cartesian coordinates such as / x,y .
- a pixel of the search area of the reference frame can be indicated based on a location using Cartesian coordinates such as r x,y.
- a motion vector (MV) for the current block can be determined based on, for example, a SAD between the pixels of the current frame and the corresponding pixels of the reference frame.
- a CU or block may be coded using quad-tree partitioning or coding as shown in the example of FIG. 7.
- the example shows quad-tree partitioning of a block 700.
- the block 700 can be partitioned differently, such as by an encoder (e.g., the encoder 400 of FIG. 4) or a machine-learning model as described below.
- the block 700 is partitioned into four blocks, namely, the blocks 700-1, 700-2, 700-3, and 700-4.
- the block 700-2 is further partitioned into the blocks 702-1, 702-2, 702-3, and 702-4.
- the size of the block 700 is NxN (e.g., 128x128)
- the blocks 700-1, 700-2, 700-3, and 700-4 are each of size N/2xN/2 (e.g., 64x64)
- the blocks 702-1, 702-2, 702-3, and 702-4 are each of size N/4xN/4 (e.g., 32x32).
- NxN e.g. 128x128
- the blocks 700-1, 700-2, 700-3, and 700-4 are each of size N/2xN/2 (e.g., 64x64)
- the blocks 702-1, 702-2, 702-3, and 702-4 are each of size N/4xN/4 (e.g., 32x32).
- a block is partitioned, it is partitioned into four equally sized
- a quad-tree data representation is used to describe how the block 700 is partitioned into sub-blocks, such as blocks 700-1, 700-2, 700-3, 700-4, 702-1, 702-2, 702-3, and 702-4.
- a quadtree 704 of the partition of the block 700 is shown.
- Each node of the quadtree 704 is assigned a flag of“1” if the node is further split into four sub-nodes and assigned a flag of“0” if the node is not split.
- the flag can be referred to as a split bit (e.g., 1) or a stop bit (e.g., 0) and is coded in a compressed bitstream.
- a node In a quadtree, a node either has four child nodes or has no child nodes. A node that has no child nodes corresponds to a block that is not split further. Each of the child nodes of a split block corresponds to a sub-block.
- each node corresponds to a sub-block of the block 700.
- the corresponding sub-block is shown between parentheses.
- a node 704-1 which has a value of 0, corresponds to the block 700-1.
- a root node 704-0 corresponds to the block 700.
- the value of the root node 704-0 is the split bit (e.g., 1).
- the flags indicate whether a sub-block of the block 700 is further split into four sub-sub- blocks.
- a node 704-2 includes a flag of“1” because the block 700-2 is split into the blocks 702-1, 702-2, 702-3, and 702-4.
- Each of nodes 704-1, 704-3, and 704-4 includes a flag of“0” because the corresponding blocks are not split.
- no flag of“0” or“1” is necessary for these nodes. That the blocks 702-5, 702-6, 702-7, and 702-8 are not split further can be inferred from the absence of additional flags corresponding to these blocks.
- the smallest sub block is 32x32 pixels, but further partitioning is possible.
- the quad-tree data for the quadtree 704 can be represented by the binary data of “10100,” where each bit represents a node of the quadtree 704.
- the binary data indicates the partitioning of the block 700 to the encoder and decoder.
- the encoder can encode the binary data in a compressed bitstream, such as the compressed bitstream 420 of FIG. 4, in a case where the encoder needs to communicate the binary data to a decoder, such as the decoder 500 of FIG. 5.
- the blocks corresponding to the leaf nodes of the quadtree 704 can be used as the bases for prediction. That is, prediction can be performed for each of the blocks 700-1, 702-1, 702-2, 702-3, 702-4, 700-3, and 700-4, referred to herein as coding blocks.
- the coding block can be a luminance block or a chrominance block. It
- the block partitioning can be determined with respect to luminance blocks.
- the same partition, or a different partition, can be used with the chrominance blocks.
- a prediction type (e.g., intra- or inter-prediction) is determined at the coding block. That is, a coding block is the decision point for prediction.
- a mode decision process determines the partitioning of a coding block, such as the block 700.
- the partition decision process calculates the RD costs of different combinations of coding parameters. That is, for example, different combinations of prediction blocks and predictions (e.g., intra-prediction, inter prediction, etc.) are examined to determine an optimal partitioning.
- FIG. 8 is a flowchart of a process 800 for searching for a best mode to code a block.
- the process 800 is an illustrative, high level process of a mode decision process that determines a best mode. For ease of description, the process 800 is described with respect to selecting an intra-prediction mode for encoding a prediction block. Other examples of best modes that can be determined by processes similar to the process 800 include determining a transform type and determining a transform size.
- the process 800 can be implemented by an encoder, such as the encoder 400 of FIG. 4, using a brute-force approach to mode decision.
- the process 800 receives an image block.
- the image block can be a prediction unit.
- each of the leaf node coding blocks e.g., a block 700-1, 702-1, 702-2, 702-3, 702-4, 700-3, or 700-4
- the image block can be one such prediction unit.
- the process 800 determines (e.g., selects, calculates, chooses, etc.) a list of modes.
- the list of modes can include K modes, where K is an integer number.
- the list of modes can be denoted ⁇ mi, m 2 , ..., ni k ⁇ .
- the encoder can have available a list of intra prediction modes. For example, the list of available intra-prediction modes can be
- the list of modes determined at 804 can be any subset of the list of available intra-prediction modes.
- the process 800 initializes a BEST_COST variable to a high value (e.g., INT_MAX, which may be equal to 2,147,483,647) and initializes a loop variable i to 1, which corresponds to the first mode to be examined.
- a high value e.g., INT_MAX, which may be equal to 2,147,483,647
- the process 800 computes (e.g., calculates) an RD_COSTi for the mode;.
- the process 800 proceeds back to 808; otherwise the process 800 proceeds to 818.
- the process 800 outputs the index of the best mode, BEST_MODE.
- Outputting the best mode can mean returning the best mode to a caller of the process 800.
- Outputting the best mode can mean encoding the image using the best mode.
- Outputting the best mode can have other semantics.
- the process 800 term i nates after outputting the best mode at 818.
- FIG. 9 is a block diagram of an example 900 of estimating the rate and distortion costs of coding an image block X by using a prediction mode m j .
- the process 900 can be performed by an encoder, such as the encoder 400 of FIG. 4.
- the process 900 includes performing a hypothetical encoding of the image block X using the prediction mode m j to determine the RD cost of encoding the block.
- the process 900 can be used by the process 800 at 808.
- a hypothetical encoding process is a process that carries out the coding steps but does not output bits into a compressed bitstream, such as the compressed bitstream 420 of FIG. 4. Since the purpose is to estimate a rate (also referred as bit rate), a hypothetical encoding process may be regarded or called a rate estimation process.
- the hypothetical encoding process computes the number of bits (RATE) required to encode the image block X.
- the example 900 also calculates a distortion (DISTORTION) based on a difference between the image block X and a reconstructed version of the image block X.
- a prediction, using the mode mi is determined. The prediction can be determined as described with respect to intra/inter-prediction stage 402 of FIG. 4.
- a residual is determined as a difference between the image block 902 and the prediction.
- the residual is transformed and quantized, such as described, respectively, with respect to the transform stage 404 and the quantization stage 406 of FIG. 4.
- the rate (RATE) is calculated by a rate estimator 912, which performs the hypothetical encoding.
- the rate estimator 912 can perform entropy encoding, such as described with respect to the entropy encoding stage 408 of FIG. 4.
- the quantized residual is dequantized at 914 (such as described, for example, with respect to the dequantization stage 410 of FIG. 4), inverse transformed at 916 (such as described, for example, with respect to the inverse transform stage 412 of FIG. 4), and reconstructed at 918 (such as described, for example, with respect to the reconstruction stage 414 of FIG. 4) to generate a reconstructed block.
- a distortion estimator 920 calculates the distortion (i.e., the loss in video quality) between the image block X and the reconstructed block.
- the distortion can be a mean square error between pixel values of the image block X and the reconstructed block.
- the distortion can be a sum of absolute differences error between pixel values of the image block X and the reconstructed block. Any other suitable distortion measure can be used.
- the rate, RATE, and distortion, DISTORTION are then combined into a scalar value (i.e., the RD cost) by using the Lagrange multiplier according to DISTORTION + ⁇ mode x RATE .
- the Lagrange multiplier A mode can be calculated as described herein, depending on the encoder performing the operations of the example 900.
- FIGS. 8 and 9 illustrate that a brute-force approach to mode decision is largely a serial process that essentially codes an image block X by using candidate modes to determine the mode with the best cost.
- Machine learning can be used to reduce the computational complexity in mode decisions.
- a machine-learning model such as a classification deep-leaming model, includes two main portions: a feature-extraction portion and a classification portion.
- the feature-extraction portion detects features of the model.
- the classification portion attempts to classify the detected features into a desired response.
- Each of the portions can include one or more layers and/or one or more operations.
- a CNN is an example of a machine-learning model.
- the feature extraction portion can include a set of convolutional operations, which is typically a series of filters that are used to filter an input image based on a filter (e.g., a square of size k).
- these filters can be used to find features in an input image.
- the features can include, for example, edges, corners, endpoints, and so on.
- later convolutional operations can find higher-level features.
- the classification portion may be a set of fully connected layers.
- the fully connected layers can be thought of as looking at all the input features of an image in order to generate a high-level classifier.
- stages e.g., a series
- high-level classifiers eventually generate the desired classification output.
- a CNN network is often composed of a number of convolutional operations (e.g., the convolution layers of the feature-extraction portion) followed by a number of fully connected layers (also called Dense layers) forming the classification portion.
- the number of operations of each type and their respective sizes are typically determined during the training phase of the machine learning.
- additional layers and/or operations can be included in each portion. For example, combinations of Pooling, MaxPooling, Dropout, Activation,
- Normalization, BatchNormalization, and other operations can be grouped with convolution operations (i.e., in the features-extraction portion) and/or the fully connected operation (i.e., in the layers of the classification portion).
- a convolution operation can use a
- a convolution layer can be a group of operations starting with a Convolution2D or SeparableConvolution2D operation followed by zero or more operations (e.g., Pooling, Dropout, Activation,
- a Dense layer can be a group of operations or layers starting with a Dense operation (i.e., a fully connected layer) followed by zero or more operations (e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof) until another convolution layer, another Dense layer, or the output of the network is reached.
- a Dense operation i.e., a fully connected layer
- zero or more operations e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof.
- the boundary between feature extraction based on convolutional networks and a feature classification using Dense operations can be marked by a Flatten operation, which flattens the multidimensional matrix from the feature extraction into a vector.
- Each of the convolution layers may consist of a set of filters. While a filter is applied to a subset of the input data at a time, the filter is applied across the full input, such as by sweeping over the input.
- the operations performed by this layer may be linear/matrix multiplications.
- An example of a convolution filter is described below.
- the output of the convolution filter may be further filtered using an activation function.
- the activation function may be a linear function or non-linear function (e.g., a sigmoid function, an arcTan function, a tanH function, a ReLu function, or the like).
- Each of the fully connected operations is a linear operation in which every input is connected to every output by a weight.
- a fully connected layer with N number of inputs and M outputs can have a total of NxM weights.
- a Dense operation may be generally followed by a non-linear activation function to generate an output of that layer.
- Some CNN network architectures may include several feature extraction portions that extract features at different granularities (e.g., at different sub-block sizes of a superblock) and a flattening layer (which may be referred to as a concatenation layer) that receives the output(s) of the last convolution layer of each of the extraction portions.
- the flattening layer aggregates all the features extracted by the different feature extractions portions into one input set.
- the output of the flattening layer may be fed into (i.e., used as input to) the fully connected layers of the classification portion.
- the number of parameters of the entire network may be dominated (e.g., defined, set) by the number of parameters at the interface between the feature extraction portion (i.e., the convolution layers) and the classification portion (i.e., the fully connected layers). That is, the number of parameters of the network is dominated by the parameters of the flattening layer.
- CNN architectures described above which include a flattening layer whose output is fed into fully connected layers
- models with fully connected layers tend to have a large number of parameters and operations.
- the machine-learning model may include over 1 million parameters.
- Such large models may not be effectively or efficiently used, if at all, to infer classifications on devices (e.g., mobile devices) that may be constrained (e.g., computationally constrained, energy constrained, and/or memory constrained). That is, some devices may not have sufficient computational capabilities (for example, in terms of speed) or memory storage (e.g., RAM) to handle (e.g., execute) such large models.
- devices e.g., mobile devices
- constrained e.g., computationally constrained, energy constrained, and/or memory constrained
- memory storage e.g., RAM
- the fully connected layers of such network architectures are said to have a global view of all the features that are extracted by the feature extraction portions.
- the fully connected layers may, for example, lose a correlation between a feature and the location of the feature in the input image.
- receptive fields of the convolution operations can become mixed by the fully connected layers.
- a receptive field can be defined as the region in the input space that a particular feature is looking at and/or is affected by. An example of a receptive field is described below. Presently, however, the problem wherein the receptive fields become mixed may be illustrated briefly with regards to FIG. 7
- a CNN as described above e.g., a CNN that includes a flattening layer and fully connected layers
- the CNN may extract features corresponding to different regions and/or sub-block sizes of the block 700.
- features extracted from blocks 700-1, 700-2, 700-3, and 700-4 of the block 700 are flattened into one input vector to the fully connected layers.
- features of at least one of the blocks 700-1, 700-3, and 700-4 may be used by the fully connected layers.
- features of sub-blocks e.g., the blocks 700-1, 700-3, and 700-4
- the inference may lead to erroneous inferences and/or inferences that are based on irrelevant information.
- the CNN architectures according to the present teachings herein analyze frames and blocks of frames in a way that the receptive field of a cascade of convolutional layers respect quad-tree boundaries. That is, when analyzing an image region, such as for determining a quad-tree partitioning, the receptive fields of any features extracted (e.g., calculated, inferred, etc.) for the image region are confined to the image region itself. This may be achieved by using an all-convolution network, and carefully designing the filter sizes of the convolutional operations to yield a matrix representation of the analysis region, whether a 64x64 region, a 32x32 region, or a 16x16 region, for example.
- FIG. 10 is a block diagram of an example of a convolutional neural network (CNN) 1000 for a mode decision according to implementations of this disclosure.
- the CNN 1000 can be used for determining a block partition of an image block.
- the block can be a superblock.
- the CNN 1000 can be used to determine the block size used in the intra/inter-prediction stage 402 of FIG.
- the partition can be a quad-tree partition, such as described with respect to FIG. 7.
- the CNN 1000 can be used to determine a partition for an intra-coded block or an inter-coded block of a frame, such as the frame 304 of FIG. 3.
- the CNN 1000 can be used by an encoder where the smallest possible block partition is an 8x8 partition. As such, determinations of whether to split a block need be made for blocks (i.e., sub-blocks of the superblock) that are 16x16 or larger.
- the number of parallel branches of the feature extraction portion of the CNN 1000 can be parameterizable (e.g., configurable). For example, in a configuration, only one branch (e.g., a linear branch) can be used. This is possible as long as the receptive field conformance property, as further described below, is maintained.
- the receptive field conformance property means that the receptive field boundary of the block does not cross the boundaries of the block.
- a block 1002 (i.e., an image block) to be encoded is presented to the CNN 1000.
- the block 1002 can be a one color-plane block.
- the block 1002 can be a luminance block. That the block is a one color-plane block is illustrated by the“xl” in“64x64x1” in FIG. 10.
- the block 1002 can be a superblock. While a superblock of size 64x64 is shown and used to describe the CNN 1000, the block 1002 can be of any size.
- the block 1002 can be 128x128, 32x32, or any size block for which a, e.g., quad tree, partitioning has been determined by an encoder, such as an encoder 400 of FIG. 4.
- the block 1002 (i.e., the block that is used as input to the CNN 1000) can include pixels that are outside of the block for which a partitioning is to be determined. For example, if a partitioning of a 64x64 block is to be determined, then a block of size 65x65x1 can be used as input to the CNN 1000. That is, for example, the left and top neighboring pixels of the block for which a partitioning is to determined can be included in the input block to the CNN 1000. An example of such as configuration is described below.
- One or more feature extraction layers 1003 form three branches as shown; namely a branch 1003-A, a branch 1003-B, and a branch 1003-C.
- the number of branches in the feature extraction layer can be configurable to include more or fewer branches.
- Each of the branches can include one or more layers.
- respective feature maps is extracted.
- features maps such as feature maps 1004
- the feature maps 1004 is of size 8x8x256. This is to be interpreted as follows: the feature maps 1004 includes 256 feature maps and each of the 256 feature maps is of size 8x8 pixels (or features).
- the feature maps 1004 can be thought of as a set of 256 matrices where each matrix is of size 8x8.
- the feature extraction of each partition type can be separated, instead of sharing the feature extraction as in FIG. 10. Architectures using such a configuration are described in more detail below.
- the number of features at a feature map can be configurable.
- the feature maps 1004 is shown to be 8x8x256, it can be 8x8xN, where N is any desired number of features.
- Other sizes are possible in different implementations, such as those described with respect to FIG. 13, below.
- a feature compression rate can be applied to a machine-learning model to expand or reduce the number of features in the model.
- the feature compression rate can be multiplied by all feature maps for feature expansion (or reduction).
- the branch 1003-A extracts, in a first layer of the branch 1003-A, features corresponding to 8x8 blocks of the block 1002.
- the branch 1003-A convolves, with the block 1002, 256 filters (also referred to as kernels).
- FIG. 11 is an example 1100 of convolution operations according to implementations of this disclosure. The convolution operations can be used to generate any of the feature maps described herein.
- the example 1100 includes a region 1102 of an image.
- the region 1102 is shown as a 6x6 region for the purposes of this example.
- convolution filters can be applied to any size block, superblock, region of image, or an image.
- a filter 1104 of size 3x3 is used in this example.
- filters can have different sizes.
- the example 1100 uses a non-overlapping convolution operation with a stride that is equal to the filter size. As such, the stride size, in each of the horizontal and vertical directions is 3.
- the filter 1104 is shown as including binary (i.e., zero and one) values.
- the values of a filter can be any value (e.g., positive and/or negative real values).
- Feature map 1114 is the output of convolving the region 1102 and the filter 1104.
- the filter 1104 is first convolved (e.g., using a matrix multiplication operation) with a sub-region 1106.
- the filter 1104 is then convolved with a sub-region 1108.
- the filter 1104 is then convolved with a sub-region 1110.
- a pixel 1120 can be calculated as (9x0 + 5x1 + 1x0 + 5x0 +
- the branch 1003-A convolves, with the block 1002, 256 filters, each having a size 8x8.
- a stride that is equal to the size of the filters i.e., a stride that is equal to 8) is used.
- 256 feature maps i.e., the feature maps 1004
- a filter of size 8 x 8 is defined by a kernel of the same size where each entry in the kernel can be a real number. In an example, the entries can be non-negative integers that are greater than 1. Filtering an 8 x8 block may thus be achieved by computing the inner product between the block and a filter kernel of the same size.
- filter kernels i.e., the real numbers which constitute the values of the kernels
- the branch 1003-B extracts 256 feature maps (i.e., feature maps 1008), each of size 8x8.
- the branch 1003-B first extracts, at a first layer of the branch 1003-B, feature maps 1006 by convolving the block 1002 with 128 filters, each of size 4x4, and using a stride of 4 (i.e., a stride that is equal to the filter size).
- a stride of 4 i.e., a stride that is equal to the filter size.
- each of the 128 feature maps of the feature maps 1006 is convolved with two 2x2 filters, using a stride of 2, thereby resulting in the feature maps 1008.
- the branch 1003-C extracts 256 feature maps (i.e., feature maps 1014), each of size 8x8.
- the branch 1003-C first extracts, at a first layer of the branch 1003-C, feature maps 1010 by convolving the block 1002 with 64 filters, each of size 2x2, and using a stride of 2.
- each of the 64 feature maps of the feature maps 1010 is convolved with two 2x2 filters, using a stride of 2, thereby resulting in 128 feature maps (i.e., feature maps 1012).
- each of the 128 features maps of the feature maps 1012 is convolved with two 2x2 filters, using a stride of 2, thereby resulting in the feature maps 1014.
- the unit is downsized (i.e., down-sampled), in each dimension, by the size of the filter.
- the feature maps 1010 are feature maps of the 32x32 blocks of the block 1002.
- the feature maps 1006 are feature maps of the 16x16 blocks of the block 1002.
- the feature maps 1004 are feature maps of the 8x8 blocks of the block 1002.
- the feature maps 1008 normalize the feature maps 1006 to be, like the feature maps 1004, of size 8x8.
- the feature maps 1012 followed by the feature maps 1014 normalize the feature maps 1010 to be, similarly, of size 8x8.
- the feature maps can be normalized, via successive convolutions, to be feature maps of the smallest possible partition that can be used by the encoder.
- the size 8x8 corresponding to the smallest possible partition type that can be used by the encoder when the CNN 1000 of FIG. 10 is used.
- the feature extraction layers 1003 can normalize the feature maps to be of size 4x4.
- the feature extraction layers 1003 can include an additional branch and each of the branches would generate, via successive convolutions, feature maps that are each of size 4x4.
- the feature maps can be normalized to a size that does not necessarily correspond to the smallest partition size.
- the features maps can be normalized to any size that is larger than or equal 8x8.
- a concatenation layer 1016 receives the feature maps 1004, 1008, and 1014. Additionally, when the CNN 1000 is used to determine (e.g., infer, provide, etc.) a partition for the block 1002 that is to be intra-predicted, at least some samples of the neighboring blocks can also be used as input to the concatenation layer 1016. This is because intra prediction uses at least some samples (i.e., pixels) of neighboring blocks. While samples from the top neighboring block (indicated with TOP in FIG. 10) and samples from the left neighboring block (indicated with LEFT in FIG. 10) are shown for illustrative purposes, other neighboring blocks may be used, depending on the scan order used to process blocks of a video frame.
- LEFT and TOP are used in the case of a raster scan order.
- all the samples of the top and left neighboring blocks are used as inputs to the concatenation layer 1016.
- samples of the top and left neighboring blocks can be included in the input block (e.g., the block 1002 of FIG. 10).
- samples from neighboring blocks may or may not be used as inputs to the CNN.
- TOP can be a row of previously reconstructed pixels that are peripheral to the top edge of the block 1002; and LEFT can be a column of previously reconstructed pixels that are peripheral to the left edge of the block 1002.
- TOP and LEFT can be added to the input block that is presented to the CNN, or can be presented separately to the CNN.
- a non-linear function of a quantization parameter may optionally be used as an input to the CNN.
- QP in video codecs can be used to control the tradeoff between rate and distortion.
- a larger QP means higher quantization (e.g., of transform coefficients) resulting in a lower rate but higher distortion; and a smaller QP means lower quantization resulting in a higher rate but a lower distortion.
- the value of the QP can be fixed.
- an encoder can use one QP value to encode all frames and/or all blocks of a video.
- the QP can change, for example, from frame to frame.
- the encoder can change the QP value based on fluctuations in network bandwidth.
- the QP can be used to calculate the RD cost associated with each combination of parameters in an exhaustive search as described above with regards to FIGS. 8 and 9.
- the QP can be used to derive a multiplier that is used to combine the rate and distortion values into one metric.
- Some codecs may refer to the multiplier as the Lagrange multiplier (denoted A mode ) ; other codecs may use a similar multiplier that is referred as rdmult. Each codec may have a different method of calculating the multiplier. Unless the context makes clear, the multiplier is referred to herein, regardless of the codec, as the Lagrange multiplier or Lagrange parameter.
- the Lagrange multiplier can be used to evaluate the RD costs of competing modes (i.e., competing combinations of parameters). Specifically, let r m denote the rate (in bits) resulting from using a mode m and let d m denote the resulting distortion. The RD cost of selecting the mode m can be computed as a scalar value d m + mocLe r m . By using the Lagrange parameter mode , it is then possible to compare the cost of two modes and select one with the lower combined RD cost. This technique of evaluating RD cost is a basis of mode decision processes in at least some video codecs.
- Different video codecs may use different techniques to compute the Lagrange multipliers from the QPs. This is due in part to the fact that the different codecs may have different meanings (e.g., definitions, semantics, etc.) for, and method of use of, QPs.
- Codecs that implement the HEVC standard may use a formula that is similar.
- the multiplier has a non-linear (e.g., exponential or quadratic) relationship to the QP.
- the multipliers may undergo further changes before being used in the respective codecs to account for additional side information included in a compressed bitstream by the encoder. Examples of side information include picture type (e.g., intra vs. inter predicted frame), color components (e.g., luminance or chrominance), and/or region of interest.
- side information include picture type (e.g., intra vs. inter predicted frame), color components (e.g., luminance or chrominance), and/or region of interest.
- additional changes can be linear changes to the multipliers.
- a disconnect may result between how the QP is used in evaluating the RD cost and how the QP is used in training machine-learning models.
- better performance can be achieved by using non-linear (e.g., exponential, quadratic, etc.) forms of the QPs as input to machine-learning models as compared to using linear (e.g., scalar) forms of the QPs.
- non-linear e.g., exponential, quadratic, etc.
- linear e.g., scalar
- Better performance can mean smaller network size and/or better inference performance.
- the non-linear function can be approximated by piecewise linear segments.
- QP may be used as an input to the CNN.
- FIG. 1 In the example of FIG. 1
- QP is used as an input to the concatenation layer 1016. More specifically, a quadratic function (i.e., QP 2 ) is illustrated in FIG. 10. As described above, however, the function used depends on the codec; more specifically, the function used depends on the standard implemented by the codec. For example, a quadratic function may be used in the case of a codec that implements H.263, VP9, or AV1; and an exponential function may be used in the case of a codec that implements H.264 or HEVC.
- a quadratic function may be used in the case of a codec that implements H.263, VP9, or AV1
- an exponential function may be used in the case of a codec that implements H.264 or HEVC.
- a total of 897 inputs can be received by the concatenation layer 1016.
- the inputs include 256 inputs of the feature maps 1004, 256 inputs of the feature maps 1008, 256 inputs feature maps 1014, 64 inputs at TOP, 64 inputs at LEFT, and the non-linear value of QP (such as QP 2 .
- a sample i.e., a pixel
- the concatenation layer 1016 receives 898 inputs.
- the CNN 1000 includes three classifiers, namely classifiers 1018, 1020, and 1022.
- Each of the classifiers 1018, 1020, 1022 includes a set of classification layers and uses convolutions as further described below.
- the classifier 1018 infers (i.e., outputs) partition decisions for sub-blocks of size 16x16 of the block 1002. It is noted that the block 1002 can be partitioned into 16 blocks (comprising 4x4 outputs), each block of size 16x16. As such, the classifier 1018 reduces, to a size of 4x4, the feature maps (which are each of size 8x8) received from the concatenation layer 1016.
- feature maps 1019 are obtained from the feature maps received from the concatenation layer 1016 by applying non-overlapping convolutions using 2x2 separable convolution filters to combine some of the feature maps into one, thereby resulting in 256 feature maps, each of size 4x4.
- a series of lxl convolutions are applied, successively, to gradually reduce the feature dimension size to 1.
- 1x1x128 convolutions (where the number of filters is 128) are applied to the feature maps 1019, resulting in 4x4x128 feature maps, to which 1x1x64 convolutions (where the number of filters is 64) are applied, resulting in 4x4x64 feature maps, to which 1x1x32 convolutions are applied resulting in 4x4x32 feature maps, to which a lxlxl convolution is applied resulting in a 4x4x1 feature map, namely the feature map 1025.
- the classifier 1018 Infers whether to split or not split the sub-block. As such, the classifier 1018 outputs 16 decisions
- the 16 decisions can be binary decisions. That is, the feature map 1025 can be thought of as a matrix of binary decisions. For example, a zero (0) can correspond to a decision not to split a sub block and a one (1) can correspond to a decision to split the sub-block.
- the order of the output of the classifier 1018 can correspond to a raster scan order of the 16x16 sub-blocks of the block 1002.
- the decisions can correspond to probabilities (i.e., values that range from 0 to 1), or some other values, such as values that range from 0 to 100. When a decision is greater than a threshold that is appropriate for the range of the decision values (e.g., 0.9, 0.75, 90%, etc.), it can be considered to correspond to a binary decision of 1.
- the classifier 1020 infers (i.e., outputs) partition decisions for sub-blocks of size 32x32 of the block 1002.
- the classifier 1020 receives the feature maps 1019 and convolves each of the feature maps with 2x2 separable convolution filters to combine feature maps of the feature maps 1019 into one, thereby resulting in feature maps 1021. It is noted that the block 1002 can be partitioned into 2x2 blocks, each of size 32x32.
- the classifier 1020 reduces, to the size of 2x2, the feature maps 1019 (which are each of size 4x4) through a series of non-overlapping convolutions using lxl filters to gradually reduce the feature dimension size to 1, as described above with respect to the feature maps 1019, thereby resulting in a feature map 1027.
- the classifier 1020 infers whether to split or not split the sub-block.
- the classifier 1020 outputs 4 decisions corresponding, respectively, to each of the 32x32 sub-blocks of the block 1002.
- the classifier 1022 infers (i.e., outputs) partition decisions for the block 1002 itself.
- the classifier 1022 receives the feature maps 1021 and convolves each of the feature maps with a 2x2 separable convolution filter resulting in feature maps 1023, which combines some of the feature maps of the features maps 1021 into 1. It is noted that the block 1002 can be partitioned into only one lxl block of size 64x64.
- the classifier 1022 reduces, to the size of lxl, the feature maps 1023 (which are each of size lxl) through a series of non overlapping convolutions using lxl filters to gradually reduce the feature dimension size to 1, as described above with respect to the feature maps 1019, thereby resulting in a feature map 1029.
- the classifier 1022 infers whether to split or not split the block 1002.
- the classifier 1022 outputs one decision corresponding to whether to split or not split the block 1002 into four 32x32 sub-blocks.
- Separable convolution filters of size 2x2 are described to obtain the feature maps 1019, 1021, and 1023 (of the classifiers 1018, 1020, and 1022, respectively) in order to ultimately determine, for a block of size 64x64, 4x4 16x16 partitions (i.e., the feature map 1025), 2x2 32x32 partitions (i.e., the feature map 1027), and lxl 64x64 partition (i.e., the feature map 1029), respectively.
- any convolutional filters of size 2 k can be used as long the classifiers 1018, 1020, and 1022 determine, as described, 4x4 16x16 partitions (i.e., the feature map 1025), 2x2 32x32 partitions (i.e., the feature map 1027), and lxl 64x64 partition (i.e., the feature map 1029).
- the feature map 1025 which has a dimension of 4x4x1
- the feature map 1025 which has a dimension of 4x4x1
- a feature maps 1034 which is of size 4x4x32.
- dot-dashed line 1036 This is illustrated by a dot-dashed line 1036. The same can be applicable to the classifiers 1020 and 1022 with respect to the feature map 1027 and the feature map 1029, respectively.
- a parameter can be used as a configuration parameter (i.e., a threshold parameter) of the CNN. If the number of remaining features is less than or equal to the threshold parameter, then the number of features of the next layer can be set to 1. In the example of the CNN 1000 of FIG. 10, the threshold parameter is set to 32. Because the number of features of the feature maps 1034 is equal to the threshold parameter (i.e., 32), then the next layer corresponds to the layer that produces the feature map 1025, which has a feature dimension of 1.
- each of the classifiers can be configured with a different respective threshold parameter. In another example, all the classifiers can be configured to use the same threshold parameter.
- the feature map dimensionality (i.e., the last dimension of a feature maps) within a classifier can be reduced using a feature reduction parameter F.
- a classifier can reduce the number of channels according to the progression IncomingFeature , IncomingFeature/F, IncomingFeature/F 2 , . . .,1, where IncomingFeature is the number of features that are initially received by the layer.
- each of the classifiers can be configured with a different respective feature reduction parameter.
- all the classifiers can be configured to use the same feature reduction parameter.
- the classifier 1018 is now used to illustrate the threshold parameter and the feature reduction parameter.
- IncomingFeature is 256 (as illustrated by the features maps 1019, which is of size 4x4x256), the feature reduction parameter F is 2, and the threshold parameter is 32.
- the classifier 1018 reduces the number of channels according to the progression 256, 256/2, 256/2 2 , 256/2 3 , and 1. That is, the classifier 1018 reduces the number of channels according to the progression 256, 128, 64, 32, and 1.
- the classifier 1018 does not include a layer where the number of channels is 256/2 4 (i.e., 16) because the threshold parameter 32 for the number of channels is reached at the progression 256/2 3 (i.e., 32).
- the CNN 1000 can be extended to infer partition decisions for other block sizes.
- an encoder may allow the smallest partition to be of size 4x4.
- a branch can be added to the feature extraction layers 1003 such that each branch of the feature extraction layers 1003 can generate feature maps, each of size 4x4, as inputs to the concatenation layer 1016.
- a classifier can be added between the concatenation layer 1016 and the classifier 1018.
- the added classifier infers (i.e., outputs) partition decisions for sub-blocks of size 8x8 of the block 1002.
- the block 1002 can be partitioned into 8x8 sub-blocks, each of size 8x8.
- the added classifier reduces, to a size of 8x8x1, the feature maps received from the concatenation layer 1016 through a series of non-overlapping convolutions using 2x2 filters in this example.
- the CNN 1000 can be configured to infer partition decisions of a 128x128 block.
- a CNN can be configured to include classifiers that determine, respectively, one (i.e., a lxl output matrix) 128x128 decision (i.e., one decision
- the CNN 1000 can include early termination features. For example, if the classifier 1022 infers that the block 1002 is not to be split, then processing through the classifiers 1020 and 1018 need not be continued. Similarly, if the classifier 1020 infers that none of the 32x32 sub-blocks of the block 1002 are to be split, then processing through the classifier 1020 need not be continued.
- the CNN 1000 is one example of an all convolution architecture that is receptive field conformant. Receptive field conformance may be further explained with reference to FIG. 12.
- FIG. 12 is an example 1200 of receptive fields according to implementations of this disclosure.
- the example 1200 includes an input 1202.
- the example 1200 and the explanation herein are adapted from Dang Ha The Hien,“A guide to receptive field arithmetic for Convolutional Neural Networks,” April 2017, [retrieved on August 06, 2018]. Retrieved from the Internet ⁇ URL: https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for- convolutional-neural-networks-e0f5l4068807>.
- the input 1202 can be a portion of an image for which it is desirable to extract features (e.g., a feature map).
- the input 1202 can be, for example, the block 700, one of the blocks 700-1, 700-2, 700-3, and 700-4, or one of the blocks 702-1, 702-2, 702-3, and 702-4.
- the input 1202 is shown as having a size of 5x5 pixels. However, the size of the input 1202 is not pertinent to the description of the concept of a receptive field.
- Padding defines how the border of a sample is handled during a convolution. Further descriptions of the concepts of padding, stride, and kernel (i.e., filter) size are omitted herein as such concepts are well- known to a person skilled in the art.
- the example 1200 illustrates a first feature map 1204 that is the result of convolving the input 1202 with a first filter and a second feature map 1206 that is the result of convolving the first feature map with a second filter.
- the first filter and the second filter can have different values.
- the values of the filters can be determined (e.g., learned) during the training phase.
- a pixel 1208 (which may also be referred to as a feature) of the first feature map 1204 results from the convolution of pixels of the input 1202. Such pixels are the receptive field of the pixel 1208. Note that because the convolution uses padding, some of the pixels (e.g., the padding pixels) used for generating the pixel 1208 are outside of the input.
- the receptive field of the pixel 1208 is defined by a square whose comers are marked by black squares, such as a black square 1213. Dashed lines, such as a dashed line 1212, emanating from the corners of the pixel 1208 also illustrate the receptive field of the pixel 1208. The end points of the dashed lines are the black squares.
- a pixel 1210 (which may also be referred to as a feature) of the second feature map 1206 results from the convolution of pixels of the first feature map 1204. Such pixels are the receptive field of the pixel 1210 in the first feature map 1204 and can be further projected onto the input 1202 to determine receptive field in the input 1202. Note that because the convolution uses padding, some of the pixels (e.g., the padding pixels) used for generating the pixel 1210 are outside of the first feature map 1204. The padding pixels of the first feature map 1204 are not shown so as not to clutter FIG. 12.
- the receptive field of the pixel 1210 in the input 1202 is defined by a square whose comers are marked by black circles, such as a black circle 1215.
- Dot-dashed lines such as a dot-dashed line 1214, emanating from the corners of the pixel 1210 also illustrate the receptive field in the input 1202 of the pixel 1210.
- the end points of the dot-dashed lines are the black circles.
- the receptive field can play an important role in image analysis during video encoding.
- the receptive field of a series of convolution layers can be interpreted as the “region” of the image (e.g., a block, a superblock, a frame, or any other portion of an image) that each pixel (e.g., a feature)“sees” (e.g., influenced by, summarizes, etc.) when computing the pixel.
- Pixels at the initial input layer e.g., the input 1202
- the second layer which includes the second feature map 1206
- each analysis region becomes confined to the boundaries of its quad-tree representation. That is, for example, it is desirable that features describing a region of an image, and which are used for inferring a mode decision of the region of the image, do not mix pixels from other regions of the image. For example, and referring again to FIG. 7, features describing the blocks 700-2 and/or the blocks 702-1, 702-2, 702-3, and 702-4 desirably do not include, in their respective fields, pixels from any of the blocks 700-1, 700-3, or 700-4.
- n out is the number of output features in a layer.
- a first layer corresponds to (e.g., includes) the first feature map 1204 and a second layer corresponds to (e.g., includes) the second feature map 1206.
- n in is the number of input features to the layer.
- the number of input features to the second layer is the number of features in the first feature map 1204, namely 9.
- the variables k, p, and s are, respectively, the convolution kernel size, the convolution padding size, and the convolution stride size/value.
- Equation (1) calculates the number of output features of a layer based on the number of input features and the convolution properties.
- Equation (2) calculates a distance (i.e., a jump j out ) between two adjacent features in the output feature map.
- Equation (3) calculates the receptive field size (i.e., r out ) of the output feature map, which is define as the area that is covered by k input features and the extra area that is covered by the receptive field of the input feature that is on the border.
- Equation (4) calculates the center position (i.e., start out ) of the receptive field of the first output feature (e.g., the pixel 1208 and the pixel 1210 correspond, respectively, to the first output feature in the first feature map 1204 and the second feature map 1206).
- the CNN architectures described herein are all- convolutional networks. That is, the feature extraction and the classification layers use convolution operations. In the feature extraction layers, non-overlapping convolution operations are performed on the input at each layer. At at least some of the feature extraction layers, the non-overlapping convolution operations are performed by setting the stride value the same as the kernel size. At least some of the kernel sizes can be even numbers (i.e., multiples of 2).
- each convolution layer in the feature extraction layers uses a 2x2 kernel, with the stride of 2.
- N can be 2 k .
- non-overlapping convolution operations i.e., between the concatenation layer 1016 and the first layer of the classifiers 1018, with a kernel size 2 are performed to reduce the number of channels from 8x8 (i.e., the size of each of the feature maps of the concatenation layer 1016 as described above) to 4x4 (i.e., the size of each of the feature maps 1019), and from then on apply kernel size lxl and gradually reduce the feature dimension size to 1 (i.e., the feature map 1025, which is of size 4x4x1).
- the output of the last classification layer is 4x4x1, which is the partition determination of the 16 sub-blocks of the input 1002.
- Each of the 16 sub-blocks is of size 16x16 pixels.
- the partition decision for each of the 32x32 sub-blocks can be inferred by the classifiers 1020; and the partition of the 64x64 block can be inferred by the classifiers 1022.
- a kernel of size lxl can be used to reduce the dimensionality of feature maps. For example, an input of size 4x4x32 (32 features maps, each of size 4x4), when convolved with one filter of size lxl would result in a feature map of size of 4x4x1. As such, a kernel of size lxl can be used to pool (e.g., combine) information from multiple feature maps. Further, a kernel of size lxl, as used herein, does not mix values from different locations of the input.
- the inventive CNN architectures described herein include the combination of using non-overlapping kernel sizes with an all-convolutional network (for feature extraction and for classification) that respects receptive fields. This combination allows for a flexible CNN architecture of reduced size over using fully connected layers for classification. Further, the CNN architectures described herein can improve inferences over alternative structures.
- the CNN architecture described herein is flexible in that the number of layers, the number of feature extraction branches, and the number of features used in each layer are all configurable. This allows for deployment flexibility of the model architecture for different application constraints. For example, a larger model such as the model of FIG. 10, can be used for applications with higher accuracy requirements and lower power/hardware footprint constraint. A smaller model, such as the examples described below with regards to FIGS. 13-16, can be used when accuracy is less important and/or a higher power/hardware footprint constraint exists. Whether using a relatively small or a relatively large model, a concatenation layer, such as the concatenation layer 1016, is optional. [00171] FIG.
- the CNN 1300 is a block diagram of a second example of a CNN 1300 for a mode decision according to implementations of this disclosure. Like the CNN 1000, the CNN 1300 includes inputs 1302, feature extraction layers 1304, and classifiers 1306. Among other differences, the CNN 1300 of FIG. 13 has fewer feature extraction layers, fewer classification layers in the classifiers, and fewer number of features in each layer than the CNN 1000.
- partition decisions for 16x16 sub-blocks i.e., a feature map of size 4x4x1 at the output
- partition decisions for 32x32 sub blocks i.e., a feature map of size 2x2x1 at the output
- partition decisions for the 64x64 block i.e., a feature map of size lxlxl at the output
- This general principle in a receptive field conformant CNN can be seen with reference to FIG. 10. Namely, given the input 1002, where the end is the feature map 1025, the feature map 1027, or the feature map 1029, the multiplication where the kernel size equals the stride value has at most 64/4, 64/2, or 64/1, or respectively 16, 32, or 64 for partitions of 16x16, 32x32, or 64x64. Similar determinations may be used for partitioning 128x128, 8x8, or 4x4, for example, using 128x128, 8x8, or 4x4 inputs.
- the inputs 1302 include a (e.g., luma) input block having a size 65x65x1, and the output comprises partition decisions for 16x16 sub-blocks (i.e., a feature map of size 4x4x1 at the output), partition decisions for 32x32 sub-blocks (i.e., a feature map of size 2x2x1 at the output), and partition decisions for the 64x64 block (i.e., a feature map of size lxlxl at the output).
- the inputs 1302 include a (e.g., luma) input block having a size 65x65x1
- the output comprises partition decisions for 16x16 sub-blocks (i.e., a feature map of size 4x4x1 at the output), partition decisions for 32x32 sub-blocks (i.e., a feature map of size 2x2x1 at the output), and partition decisions for the 64x64 block (i.e., a feature map of size lxlxl at the output
- a block of size 65x65x1 can be used as input to the CNN 1300. That is, for example, the (e.g., left and top) neighboring pixels of the block for which a partitioning is to determined can be included in the input block to the CNN 1300.
- a first filter e.g., a first filter in each branch of the feature extraction layers
- the stride can be 2 k .
- the same determinations may be used for partitioning 128x128, 8x8, or 4x4 blocks, for example, using 129x129, 9x9, or 5x5 inputs.
- FIG. 13 shows that an image block to be encoded is presented as inputs 1302 to the CNN 1300.
- the image block can be a one color-plane block, such as the luminance block 65x65x1. While a superblock of size 64x64 is used to describe the CNN 1300, the block can be of any size.
- the feature extraction layers 1304 form three branches 1308-A, 1308-B, 1308-C.
- the number of branches in the feature extraction layer can be configurable to include more or fewer branches.
- Each of the branches can include one or more of the feature extraction layers 1304.
- respective feature maps is extracted.
- the feature extraction layers comprise a number or cardinality of branches equal to a number or cardinality of possible quad-tree partition decisions for the block, but this is not required.
- each of the branches comprises at least one of the feature extraction layers.
- the branch 1308-A extracts, in a first or initial layer 1310-0 of the branch 1308-A, features corresponding to 8x8 blocks of the block.
- the branch 1308-A convolves k filters with the block, each having a size 8x8.
- a stride that is equal to 2 k + 1 is used.
- 16 feature maps, each of size 8x8, are extracted.
- the same convolution operation is performed at each first layer 1312-0, 1314-0 of the branches 1308-B, 1308-C.
- each of the 16 feature maps of the initial layer 1310-0 is convolved using filters having a size equal to a stride value to extract 32 feature maps, each of size 4x4.
- each of the 32 feature maps of the second layer 1310-1 is convolved using filters having a size equal to a stride value to extract 32 feature maps, each of size 2x2.
- each of the 32 feature maps of the third layer 1310-2 is convolved using filters having a size equal to a stride value to extract 64 feature maps, each of size lxl.
- each of the 16 feature maps of the initial layer 1312-0 is convolved using filters having a size equal to a stride value to extract 32 feature maps, each of size 4x4.
- each of the 32 feature maps of the second layer 1312-1 is convolved using filters having a size equal to a stride value to extract 64 feature maps, each of size 2x2.
- each of the 16 feature maps of the initial layer 1314-0 is convolved using filters having a size equal to a stride value to extract 64 feature maps, each of size 4x4.
- the CNN 1300 includes three classifiers, namely classifiers 1320-A, 1320-B, and 1320-C.
- Each of the classifiers 1320- A, 1320-B, 1320-C includes one or more classification layers and uses convolutions as further described below.
- the multiple classifiers comprise a respective classifier corresponding to a respective branch of the branches of the feature extraction layers.
- each of the final layers of the feature extraction layers 1304 before the classifiers 1306, namely the fourth layer 1310-3 of the first branch 1308-A, the third layer 1312-1 of the second branch 1308-B, and the second layer 1314-1 of the third branch, include N feature maps.
- the dimension of the N feature maps in each branch is equal to the number of possible mode decisions (e.g., the quad-tree partitions) for the block size associated with the corresponding classifier of the classifiers 1306. That is, the classifier 1320-A receives 64 feature maps of size lxl from the layer 1310-3, and the classifier 1320-A infers (i.e., outputs) one partition decision for the 64x64 input block.
- the classifier 1320-B receives 64 feature maps of size 2x2 from the layer 1312-2, and the classifier 1320-B infers four partition decisions, one for each of the 32x32 sub-blocks.
- the classifier 1320-C receives 64 feature maps of size 4x4 from the layer 1314-1, and the classifier 1320-C infers sixteen partition decisions, one for each of the 16x16 sub-blocks.
- the decisions can be binary decisions. That is, the feature maps output from each of the classifiers 1320-A, 1320-B, 1320- C can be thought of as a matrix of binary decisions. For example, a zero (0) can correspond to a decision not to split a sub-block and a one (1) can correspond to a decision to split the sub block.
- the order of the output of a classifier can correspond to a raster scan order of the sub blocks of the input block.
- the decisions can correspond to probabilities (i.e., values that range from 0 to 1), or some other values, such as values that range from 0 to 100.
- a threshold that is appropriate for the range of the decision values (e.g., 0.9, 0.75, 90%, etc.)
- it can be considered to correspond to a binary decision of 1.
- a series of convolutions at least one of which is a lxl convolution, is applied, successively, to gradually reduce the feature dimension size to 1.
- convolutions are applied to the 64 feature maps from the extraction layer 1310-3, resulting in 1x1x24 feature maps, to which convolutions are applied at the second classification layer 1322-1 of the classifier 1320-A, resulting in 1x1x8 feature maps, to which convolutions are applied at the third classification layer 1322-2 of the classifier 1320-A, resulting in a final lxlxl feature map.
- a series of convolutions at least one of which is a lxl convolution, is applied, successively, to gradually reduce the feature dimension size to 1.
- convolutions are applied to the 64 feature maps from the extraction layer 1312-2, resulting in 2x2x24 feature maps, to which convolutions are applied at the second classification layer 1324-1 of the classifier 1320-B, resulting in 2x2x8 feature maps, to which convolutions are applied at the third classification layer 1324-2 of the classifier 1320-B, resulting in a final 2x2x1 feature map.
- a series of convolutions at least one of which is a lxl convolution, is applied, successively, to gradually reduce the feature dimension size to 1.
- convolutions are applied to the 64 feature maps from the extraction layer 1312-2, resulting in 4x4x24 feature maps, to which convolutions are applied at the second classification layer 1326-1 of the classifier 1320-C, resulting in 4x4x8 feature maps, to which convolutions are applied at the third classification layer 1326-2 of the classifier 1320-C, resulting in a final 4x4x1 feature map.
- each of the classifiers 1306, (e.g., non-overlapping) convolutional filters of any size can be used as long the classifiers 1306 determine, as described, 4x4 16x16 partitions, 2x2 32x32 partitions, and lxl 64x64 partition.
- a QP may be used as input to one or more of the feature extraction layers 1304 of the CNN 1300.
- the QP can be input as a non linear function f(QP).
- the input function can be QP 2 or QP n .
- the variable n may be a real number.
- the variable n may be a non- zero (e.g., positive) integer with an absolute value greater than 1.
- FIG. 13 shows a CNN 1300 where inputs 1302 extend from luminance-only (luma) data of the block to luma and chrominance (chroma) data of the block.
- luma luminance-only
- chroma chrominance
- the chroma channels may input in dimensions 32x32x2 in an arrangement such as that in FIG. 10.
- the first kernel size may equal the stride value.
- the chroma data is included in the inputs 1302 with dimensions of 33x33x2 by including the neighboring pixels (one row and one column in this example).
- the first kernel is 2 k +1 with a stride value of 2 k as described with respect to the luma channel.
- the kernel and stride relationships are those described with regards to the luma channel to obtain 16-4x4 partitions, except that the multiplication of kemels/strides can be at most 8 because 32/4 is equal to 8.
- the branch 1308-A extracts, in a second kernel of the initial layer 1310-0, features corresponding to 4x4 blocks of the chroma blocks.
- the branch 1308-A convolves k filters with the chroma blocks, each having a size 4x4.
- a stride that is equal to 2 k + 1 is used.
- 8 feature maps, each of size 4x4, are extracted.
- the same convolution operation is performed at each first layer 1312-0, 1314-0 of the branches 1308-B, 1308-C.
- each of the 8 feature maps having the size 4x4 is convolved with the feature maps corresponding to the luma block for the additional feature extraction layers 1304.
- chroma data is optional but may improve the inference performance of a CNN over using luma data alone. For example, in some videos, content may not be well captured in the luma channel.
- additional features are available for feature extraction, such as in the feature extraction layers 1304, improving inference accuracy at the classifiers, such as the classifiers 1306.
- FIG. 13 and its variations refer to luminance and chrominance data in the YUV color space
- the CNN architectures described herein may be used with other color spaces, such as RGB or LUV, for example.
- one column and one row of pixels is used for the (e.g., left and top) neighboring pixels in each of the luma and chroma inputs.
- more than one row/column represented by the value v may be used.
- the input would be (64 + v)x(64 + v)xl for a 64x64 luma block and (32 + v)x(32 + v)x2 for the two 32x32 chroma blocks, and the first kernel has a kernel size of 2 k + v and a stride value of 2 k .
- these examples of luma and chroma block sizes refer to 4:2:0 format. If another format is used, the relative block sizes between the luma and chroma samples will differ from these examples.
- FIGS. 10 and 13, and their variations described herein can be designed so that a global feature adjustment rate r may be applied across all layers for reducing/expanding the number of features.
- This feature adjustment rate can be a configurable hyper-parameter. After applying the feature adjustment rate, the number of features can be reduced/expended from n to n*r.
- FIG. 14 is a block diagram of a third example of a CNN 1400 for a mode decision according to implementations of this disclosure.
- the CNN 1400 of FIG. 14 includes feature extraction layers 1404 and classifiers 1406.
- the feature extraction layers 1404 include one or more common feature extraction layers 1408 and additional feature extraction layers 1410, 1412, 1414.
- the common feature extraction layers 1408 provide features maps to teach of additional 64x64 feature extraction layers 1410, additional 32x32 feature extraction layers 1412, and additional 16x16 feature extraction layers 1414.
- the common feature extraction layers 1408 are shared by one or more of the classifiers 1406.
- the common feature extraction layers 1408 are shared by each of the 64x64 classifier layers 1416, the 32x32 classifier layers 1418, and the 16x16 classifier layers 1420.
- An advantage of not sharing common feature extraction layers is that such a CNN can maximize the inference accuracy through the design of specific extraction layers for each classifier.
- sharing common feature extraction layers between different classifiers has an advantage in that the sharing can effectively reduce model size. Sharing common feature extraction layers may be seen in additional detail in the example of FIG. 15.
- FIG. 15 is a block diagram of a fourth example of a CNN 1500 for a mode decision according to implementations of this disclosure.
- the CNN 1500 of FIG. 15 modifies the architecture of the CNN 1300 by forcing the three classifiers 1306 to share some feature extraction layers.
- the classifiers 1306 i.e., the layers of the classifiers 1306) in the CNN 1500 are unchanged from those in the CNN 1300 of FIG. 13.
- the inputs 1502 comprise luma data in the form of input 65x65x1 and chroma data in the form of inputs 33x33x2.
- Each of the three classifiers 1320-A, 1320-B, 1320-C share first/initial feature extraction layers 1510, 1512 of sizes 8x8x16 and 4x4x8, respectively.
- the CNN 1500 also includes additional feature extraction layers that are specific to each of the classifiers 1306.
- the classifier 1320-C for 16x16 blocks receives input through an additional feature extraction layer 1516 having a size 4x4x64.
- the classifier 1320-B for 32x32 blocks receives input through an additional feature extraction layer 1518 having a size 2x2x64.
- the classifier 1320-A for 64x64 blocks receives input through two additional feature extraction layers 1520 and 1522, respectively having sizes 2x2x32 and 1x1x64.
- sharing common feature extraction layers can reduce model size.
- sharing common feature extraction layers can increase model robustness to noise.
- the mode decision inferred by the CNNs described above for a block is the block partitioning.
- existing video codecs also use brute force approaches to decide the optimal prediction modes and transform unit sizes for compression of the block. These mode decisions are similar to the mode decision of block partitioning in the sense that they all use features based on the raw image content and a quantization value of the block. Further, the same receptive-field-conforming principle should be observed for the decisions of prediction mode (PM) and transform unit (TU) size and/or type on the block/sub block level. Accordingly, the teachings herein may be extended to include these mode decisions.
- PM prediction mode
- TU transform unit
- FIG. 16 is a block diagram of a fifth example of a CNN 1600 for a mode decision according to implementations of this disclosure.
- the CNN 1600 extends the architecture previously described to infer prediction (e.g., intra prediction) modes and transform unit sizes.
- the classifiers for PM and TU at each sub-block size can share common feature extraction layers 1602 with the classifiers for the partition decisions, such as those described with regard to FIGS. 10 and 13. More specifically, the CNN 1600 indicates that the classifiers for each of the partition decisions for 64x64 blocks, 32x32 blocks, and 16x16 blocks comprises separate classifier layers 0 to m, where m is a positive integer.
- the classifiers for PM and TU at each sub-block size also share the first/initial classification layers 1604, 1606, 1608, respectively with its corresponding block partition classifiers.
- the initial classification layers 1604, 1606, 1608 provide respective inputs (e.g., feature maps) to an extra classifier for the PM and TU at each sub-block size, respectively classifiers 1610, 1612, 1614.
- the classifiers 1610, 1612, 1614 may each comprise one or more classifier layers. This arrangement may be desirable because block partitioning and TU/PM
- mode decisions for TU/PM have their own classification layers to implement classification rules that are unique to those mode decisions.
- CNNs 1000, 1300, 1400, 1500, 1600 are described for determining a partitioning of a 64x64 block from a 64x64 partition down to whether each 16x16 sub-block should be further partitioned into 8x8 blocks.
- the disclosure herein is not so limited.
- a CNN architecture according to implementations of the disclosure can be generalized as follows.
- a CNN for determining a mode decision in video coding where the block is of size NxN (e.g., 64x64, 128x128) and where a smallest partition determined by the CNN is of size SxS (e.g., 4x4, 8x8), can include feature extraction layers, optionally a concatenation layer, and classifiers.
- the classifiers include all-convolutional layers. Other values of N and S can be possible. In some examples, N can be 32, 64, or 128, and S can be 4, 8, or 16.
- the layer may receive, from the feature extraction layers, feature maps of the block.
- Each feature map may be of a defined size, such as 8x8.
- each of the classifiers includes one or more classification layers.
- Each classification layer receives feature maps having a respective feature dimension.
- the classifier 1018 includes 5 classification layers (illustrated by the 5 squares representing the feature maps of each layer)
- the classifier 1020 includes 4 classification layers
- the classifier 1022 includes 3 classification layers.
- each of the classifiers 1320-A, 1320-B, 1320-C includes 3 classification layers (illustrated by the 3 squares representing the feature maps of each layer).
- a classifier can infer partition decisions for sub-blocks of size (aS)x(aS) of the block by instructions that include applying, at each successive classification layer of the classification layers, a kernel of size lxl to reduce the respective feature dimension at least in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
- the classifier 1022 convolves the feature maps 1023 with 32 kernels, each of size lxl, thereby resulting in feature maps 1031, which have dimensions of 1x1x32.
- a classifier may convolve the feature maps of a previous layer with a different number of kernels to reduce the respective feature dimension by more than half (here, by 2 3 ).
- a first classifier 1018 can receive the first feature maps as an output of a final feature extraction layer of the feature extraction layers through the concatenation layer (e.g., the concatenation layer 1016), wherein a first non-overlapping convolution operation using a first 2x2 kernel is applied to reduce the first feature maps to a size of (S/2)x(S/2).
- the first layer of the classifier 1018 receives the 8x8 feature maps from the concatenation layer 1016 and reduces them to the size of 4x4 (i.e., the feature maps 1019).
- the feature maps 1019 is shown as having a dimension of 256.
- the CNN may also include a second classifier 1022 that infers partition decisions for sub-blocks of size (b8)c(b8).
- b 8.
- the second classifier can receive third feature maps, each of size MxM, from a third classifier.
- the third classifier can be the classifier 1020.
- the second classifier can apply a second non-overlapping convolution operation using a second 2x2 kernel to reduce the third feature maps to a size of
- the classifier 1022 receives the features maps 1021 from the classifier 1020 and applies a second non-overlapping convolution operation using a second 2x2 kernel to generate the feature maps 1023.
- an initial classification layer of each classifier receives the feature maps as an output of a final feature extraction layer of the feature extraction layers, directly from the final feature extraction layer.
- the feature maps received at an initial classification layer include specific feature dimensionalities for illustrative purposes.
- a kernel of size (2 k +v, 2 k +v) and a stride size of (2 k , 2 k ) can be used to propagate the v left/top information and observe (e.g., preserve) the perception field.
- Each of the output values corresponds a block location and can be a value indicating whether a sub-block at that location should be partitioned or not. For example, a value of 0 can indicate that the sub-block is not to be partitioned and a value of 1 can indicate that the sub-block is to be partitioned. Other values are, of course, possible.
- the first/initial feature extraction layer can apply a non-overlapping convolution filter to the block to generate feature maps of the block for the next feature extraction layer.
- Each successive feature extraction layer after the initial feature extraction layer can apply a non-overlapping convolution filter to the feature maps from the previous layer.
- a non overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size.
- a first feature extraction layer can apply a (N/S)x(N/S) non overlapping convolution filter to the block to generate a first subset (e.g., cardinality) of the feature maps of the block.
- a first subset e.g., cardinality
- a second feature extraction layer can apply a MxM non-overlapping convolutional filter to the block to generate maps each of size
- the feature extraction layers 1003-B and 1003- C can be examples of the second feature extraction layer.
- a non-linear value of a quantization parameter can be used as an input to the CNN.
- the non-linear value of QP is shown as an input to the concatenation layer 1016.
- the non-linear value of the QP can be used as an input to other layers of the CNN.
- the non-linear value of the QP can be used as the input to at least one of the classification layers.
- a CNN that is configured as described above can be used by an encoder, such as the encoder 400 of FIG. 4, to infer mode decisions that include block partitioning and optionally other mode decisions.
- the mode decisions are not derived by brute force methods as are known in the art.
- a CNN described herein can be used by the intra/inter-prediction stage 402.
- an encoder can predict the blocks of the partitions using known prediction techniques, such as inter prediction, intra-prediction, other techniques, or a combination thereof, or can obtain the prediction mode from the CNN.
- the resulting residual may be encoded as described with regard to FIG. 4, optionally where the transform type/size is also provided by the CNN.
- FIG. 17 is a flowchart of a process 1700 for encoding, by an encoder, an image block according to implementations of this disclosure.
- the process 1700 trains, using input data, a machine-learning model to infer one or more mode decisions.
- the process 1700 uses the trained machine-learning model to infer a mode decision for an image block, which is to be encoded.
- the mode decision can be a quad-tree partition decision of the image block.
- the image block can be a block of an image (e.g., a video frame) that is encoded using intra-prediction.
- the mode decision can be a partition that includes partitions described with respect to FIG. 20 described below. As further described below, some of the partitions of FIG. 20 include square and non-square sub-partition; and each of the square sub-partitions can be further partitioned according to one of the partitions of FIG. 20.
- the process 1700 trains the machine-learning (ML) model.
- the ML model can be trained using a training data 1712.
- Each training datum of the training data 1712 can include a video block that was encoded by traditional encoding methods (e.g., by an encoder such as described with respect to FIGS. 4 and 6-8); a QP used by the encoder; zero or more additional inputs corresponding to inputs used by the encoder in determining the mode decision (e.g., block partitioning and optionally prediction mode and/or transform unit size) for encoding the video block; and the resulting mode decision determined by the encoder.
- the mode decision e.g., block partitioning and optionally prediction mode and/or transform unit size
- parameters of the ML model are generated such that, for at least some of the training data, the ML model can infer, for a training datum, the resulting mode decision of the training datum for a set of inputs that includes the video block, the value corresponding to a quantization parameter, and zero or more additional inputs of the training datum.
- the value corresponding to the QP has a non-linear relation to the QP. That is, the value is derived from the QP based on a non-linear function of the QP.
- the non-linear function can be an exponential function, a quadratic function, or some other non-linear function of the QP.
- the non-linear function is of a same type as a function used by the encoder for determining a multiplier used in a rate-distortion calculation, as described above.
- the resulting mode decision determined by the encoder can be indicative of the quad-tree partition of the training block of the training datum.
- Many indications (e.g., representations) of the quad-tree partition are possible.
- a vector (e.g., sequence) of binary flags, as described with respect to the quadtree 704 can be used.
- the zero or more additional inputs corresponding to inputs used by the second encoder in determining the mode decision for encoding the video block can include at least some of the samples (i.e., first samples) of the top neighboring block, at least some of the samples (i.e., second samples) of the left neighboring block of the input block, at least some of the samples of the top-left neighboring block, or a combination thereof.
- the top-left neighboring block can be considered to be part of either the top neighboring block or the left neighboring block.
- the first samples or the second samples can be considered to include samples from the top-left neighboring block.
- the ML model leams (e.g., trains, builds, derives, etc.) a mapping (i.e., a function) that accepts, as input, a block and a non-linear value of a quantization parameter (e.g., QP 2 as shown in FIG. 10) and outputs at least a partitioning of the block.
- a mapping i.e., a function
- the ML model be trained using a large range of input blocks and a large range of possible QP values, such as QP values that are used in representative of real- world applications.
- the ML model may well learn how to determine a mode decision for dark blocks but provide unreliable output when presented with non-dark blocks during the inference phase.
- the encoder uses a discrete set of the QP values, then it is preferable that each of the QP values is well represented in the training data set. For example, if the QP value can vary from 0 to 1, then it is preferable that the training data include varying QP values in the range 0 to 1.
- the ML model may misbehave (e.g., provide erroneous output) when the missed QP value is presented to the ML model during the inference phase.
- a missed QP value i.e., a QP value that is not used during the training phase
- the missed QP can be interpolated from QP values that are used during the training phase and the interpolated QP value can then be used during the inference phase.
- the ML model can then be used by the process 1700 during an inference phase.
- the inference phase includes the operations 1704 and 1706.
- a separation 1710 indicates that the training phase and the inference phase can be separated in time.
- the inferencing phase can be performed by a first encoder and the training data 1712 can be generated by a second encoder.
- the first encoder and the second encoder are the same encoder. That is, the training data 1712 can be generated by the same encoder that performs the inference phase.
- the inference phase uses a machine-learning model that is trained as described with respect to 1702.
- inputs are presented to ML module. That is, the inputs are presented to a module that incorporates, includes, executes, implements, and the like the ML model.
- the inputs include the image block and optionally a non-linear function of a value corresponding to a QP. As described above, the first value is derived (i.e., results) from the non-linear function using the QP as input to the non-linear function.
- the inputs can also include additional inputs, as described above with respect to the zero or more additional inputs.
- the non-linear function can be approximated by linear segments. Approximating the non-linear function by piecewise linear segments is illustrated with respect to FIG. 18.
- FIG. 18 is an example 1800 of approximating a non-linear function of a quantization parameter using linear segments according to implementations of this disclosure.
- a quadratic function is used to describe the non-linear function.
- other non-linear function types are possible.
- the example 1800 shows, as a dashed curve, a non-linear function 1802 of the quantization parameter.
- the non-linear function 1802 is QP 2 .
- the QP values range from 0 to 1.
- the example 1800 illustrates splitting the range 0 to 1 into several segments; namely, segments 1804, 1806, and 1808. While three segments are illustrated, more or fewer, but more than 1, segments can be used.
- the range 0 to 1 can be split into a first range that includes the QP values 0 to 0.25, a second range that includes the QP values 0.25 to 0.75, and a third range that includes the QP values 0.75 to 1.
- FIG. 19 is an example 1900 of a rate-distortion performance comparison of a first machine learning model 1916 that uses as input a non-linear QP function and a second machine learning model 1926 that uses a linear QP function.
- the peak signal-to-noise ratio (PSNR) is used as the distortion metric. That is, in the graphs 1910 and 1920, the x-axis indicates the data rate for encoding the sample video sequence measured in kbps, while the y-axis indicates PSNR quality measured in decibels (dB).
- the results of graphs 1910 and 1920 were obtained by experimentation as described below.
- the first machine-learning model 1916 is a model of a CNN that has an architecture as described with respect to FIG. 10, by example. Whereas the fully connected layers of the second machine-learning model 1926 has 1.2 million parameters, the first machine-learning model 1916 (which is an all-convolutional model and does not include fully connected classification layers) is much smaller with only 300,000 parameters (using a feature compression rate of 0.5). As such, and due at least in part to the smaller model size, it is possible to perform inferring on a power- and/or capacity-constrained platform (e.g., a mobile device) using the first machine-learning model 1916.
- the reduced model size is due in part to each of, or the combination of, using the non-linear value of the QP (in this example, QP 2 ) and the CNN architecture, which has the receptive field conforming properties described herein.
- the first machine-learning model 1916 and the second machine-learning model 1926 are depicted as generic machine-learning models with an input layer, internal layers, and an output layer.
- the first machine-learning model 1916 and the second machine-learning model 1926 are depicted only to illustrate that the first machine-learning model 1916 uses a non-linear function of QP, namely QP 2 , whereas the second machine-learning model 1926 uses a linear function of QP, namely the QP value itself.
- a curve 1912 of graphs 1910 and 1920 depicts the rate-distortion performance of a (e.g., VP9) encoder, as described with respect to FIG. 4. That is the curve 1912 is generated based on brute-force encoding (i.e., encoding that is not based on a machine-learning model).
- a curve 1914 of the graph 1910 depicts the rate-distortion performance resulting from using the first machine-learning model 1916 to infer block partitions to be used in a VP9 software encoder.
- a curve 1924 of the graph 1920 depicts the rate-distortion performance resulting from using the second machine-learning model 1926 to infer block partitions to be used in a VP9 software encoder.
- the graphs show that, on average, higher rate-distortion performance can be achieved when using QP 2 .
- the performance, in RD-rate, is approximately 1.78% worse than brute-force encoding when using QP 2 ; whereas using QP, the performance is approximately 3.6% worse than brute-force encoding.
- the process 1700 obtains one or more mode decisions from the machine-learning model.
- the process 1700 obtains the mode decisions as described with respect to FIGS. 10 and 13-16. That is, for example, the CNN can provide an output that is indicative of a quad-tree partition of an image block.
- the process 1700 encodes the image block using the one or more mode decisions. That is, and continuing with the example of inferring a block partitioning, for each of the sub-blocks (i.e., according to the output that is indicative of a quad-tree partition), the process 2000 can intra-predict (or inter-predict) the block as described with respect to the intra/inter-prediction stage 402 of FIG. 4, and consistent with the description of FIG. 4, ultimately entropy encode, as described with respect to the entropy encoding stage 408, the image block in a compressed bitstream, such as the bitstream 420 of FIG. 4.
- An encoder that uses a machine-learning model, such as one of the ML models described herein, to infer mode decision parameters for image block, can encode the mode decisions in a compressed bitstream, such as the bitstream 420 of FIG. 4.
- the image block can be a superblock and the mode decision can be indicative of a quad-tree partition of the superblock.
- a decoder such as the decoder 500 of FIG. 5, can decode the image block using the mode decisions received in the compressed bitstream.
- the process of decoding an image block can include receiving, in a compressed bitstream, such as the compressed bitstream 420 of FIG. 5, an indication of a partitioning of the image block into sub-blocks; and decoding the image block using the indication of the partitioning and optionally the prediction mode and transform unit size (or type) of the image block.
- a quad-tree such as described with respect to FIG. 7, can be output in a compressed bitstream, such as the bitstream 420 of FIG. 4.
- a decoder such as the decoder 500 of FIG. 5, can decode from the compressed bitstream the quad-tree in the process of decoding a block (i.e., a superblock).
- the quad-tree can be determined (e.g., inferred) in the encoder using a CNN that is configured as described above and output in the compressed bitstream.
- a decoder decodes, from the compressed bitstream, the quad tree, which was inferred by the CNN that is configured as described with respect to any of FIGS. 10, 13, 14, 15, or 16.
- a CNN can be used to infer non-square partitions that may or may not be represented by a quad-tree. That is, for example, a non-square partition can correspond to an internal node of the quad-tree having a number or cardinality of children that is greater than or equal to two children.
- FIG. 20 is an example 2000 of non-square partitions of a block. Some encoders may partition a superblock, such as a super-block of size 64x64, 128x128, or any other size, of a square sub-block of the superblock, into one of the partitions of the example 2000.
- a partition type 2002 (which may be referred to as the PARTITION_VERT_A) splits an NxN coding block into two horizontally adjacent square blocks 2002A, 2002B, each of size N/2xN/2, and a rectangular prediction unit of size NxN/2.
- a partition type 2008 (which may be referred to as the PARTITION_VERT_B) splits an NxN coding block into a rectangular prediction unit of size NxN/2 and two horizontally adjacent square blocks 2008A, 2008B, each of size N/2xN/2.
- a partition type 2004 (which may be referred to as the PARTITION_HORZ_A) splits an NxN coding block into two vertically adjacent square blocks 2004A, 2004B, each of size N/2xN/2, and a rectangular prediction unit of size N/2xN.
- a partition type 2010 (which may be referred to as the PARTITION_HORZ_B) splits an NxN coding block into a rectangular prediction unit of size N/2xN and two vertically adjacent square blocks 2010A, 2010B, each of size N/2xN/2.
- a partition type 2006 (which may be referred to as the PARTITION_VERT_4) splits an NxN coding block into four vertically adjacent rectangular blocks, each of size NxN/4.
- a partition type 2012 (which may be referred to as the PARTITION_HORZ_4) splits an NxN coding block into four horizontally adjacent rectangular blocks, each of size N/4xN.
- partition types can be used by a codec.
- the example 2000 illustrates four partition types that may be available at an encoder.
- a partition type 2014 (also referred to herein as the PARTITIONS PLIT partition type and partition- split partition type) splits an NxN coding block into four equally sized square sub-blocks. For example, if the coding block 2014 is of size NxN, then each of the four sub-blocks of the
- PARTITIONS PLIT partition type such as a sub-block 2014A, is of size N/4xN/4.
- a partition type 2016 (also referred to herein as the PARTITION_VERT partition type) splits the coding block into two adjacent rectangular prediction units 2016 A, 2016B, each of size NxN/2.
- a partition type 2018 (also referred to herein as the PARTITION_HORZ partition type) splits the coding block into two adjacent rectangular prediction units, each of size N/2xN.
- a partition type 2020 (also referred to herein as the PARTITIONNONE partition type and partition-none partition type) uses one prediction unit for the coding block such that the prediction unit has the same size (i.e., NxN) as the coding block.
- partition types 2014-2020 are referred to herein as basic partition types and the partitions 2002-2012 are referred to herein as extended partition types.
- a partition can be represented by a tree.
- a tree can be represented by a vector.
- P denote the set of all valid partitions (or, equivalently, the respective representations of the partitions). Accordingly, a CNN can be trained to infer a mapping into the set P. Configuring a CNN to infer the partitions described with respect to FIG. 20 includes defining an appropriate set P and using appropriate training data.
- non-square partitions can be composed by smaller square groups. Accordingly, the techniques described herein may be generalized to non-square partitions such as shown in FIG. 20.
- Implementations of the transmitting station 102 and/or the receiving station 106 can be realized in hardware, software, or any combination thereof.
- the hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit.
- IP intellectual property
- ASICs application-specific integrated circuits
- programmable logic arrays optical processors
- programmable logic controllers programmable logic controllers
- microcode microcontrollers
- servers microprocessors, digital signal processors, or any other suitable circuit.
- the term“processor” should be understood as encompassing any of the foregoing hardware, either singly or in combination.
- the terms“signal” and“data” are used interchangeably. Further, portions of the transmitting station 102 and the receiving station 106 do not necessarily have to be implemented in the same manner.
- the transmitting station 102 or the receiving station 106 can be implemented using a general-purpose computer or general-purpose processor with a computer program that, when executed, carries out any of the respective methods, algorithms, and/or instructions described herein.
- a special-purpose computer/processor which can contain other hardware for carrying out any of the methods, algorithms, or instructions described herein, can be utilized.
- the transmitting station 102 and the receiving station 106 can, for example, be implemented on computers in a video conferencing system. Alternatively, the transmitting station 102 can be implemented on a server, and the receiving station 106 can be
- the transmitting station 102 using an encoder 400, can encode content into an encoded video signal and transmit the encoded video signal to the server.
- the communications device can then decode the encoded video signal using a decoder 500.
- the communications device can decode content stored locally on the communications device, for example, content that was not transmitted by the transmitting station 102.
- Other transmitting station 102 and receiving station 106 implementation schemes are available.
- the receiving station 106 can be a generally stationary personal computer rather than a portable communications device, and/or a device including an encoder 400 may also include a decoder 500.
- implementations of the present disclosure can take the form of a computer program product accessible from, for example, a tangible computer- usable or computer-readable medium.
- a computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor.
- the medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.
- CNN convolutional neural network
- N/S by: applying, at some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
- Example 2 The CNN of Example 1, wherein the classifiers includes a first classifier that infers partition decisions for sub-blocks of size (2S)x(2S), the first classifier is configured to: receive the first feature maps from the concatenation layer; and apply a first non-overlapping convolution operation using a first 2x2 kernel to reduce the first feature maps to a size of (S/2)x(S/2).
- the classifiers includes a first classifier that infers partition decisions for sub-blocks of size (2S)x(2S), the first classifier is configured to: receive the first feature maps from the concatenation layer; and apply a first non-overlapping convolution operation using a first 2x2 kernel to reduce the first feature maps to a size of (S/2)x(S/2).
- Example 3 The CNN of Example 2, wherein the classifiers includes a second classifier that infers partition decisions for sub-blocks of size (b8)c(b8), the second classifier being different from the first classifier and is configured to: receive third feature maps, each of size MxM, from a third classifier; and apply a second non-overlapping convolution operation using a second 2x2 kernel to reduce the third feature maps to a size of
- Example 4 The CNN of any of Examples 1 to 3, wherein the feature extraction layers comprise: a first feature extraction layer that applies an (N/S)x(N/S) non-overlapping convolutional filter to the block to generate a first subset of the first feature maps of the block.
- Example 5 The CNN of Example 4, wherein the feature extraction layers further comprise: a second feature extraction layer that is configured to: apply an MxM non overlapping convolutional filter to the block to generate maps each of size (N/M)x(N/M), wherein M is less than S, is greater than 1, and is a power of 2; and successively apply non overlapping 2x2 convolutional filters to the maps to generate a second subset of the first feature maps of the block.
- a second feature extraction layer that is configured to: apply an MxM non overlapping convolutional filter to the block to generate maps each of size (N/M)x(N/M), wherein M is less than S, is greater than 1, and is a power of 2; and successively apply non overlapping 2x2 convolutional filters to the maps to generate a second subset of the first feature maps of the block.
- Example 6 The CNN of any of Examples 1 to 5, wherein N is 64 and S is 8.
- Example 7 The CNN of any of Examples 1 to 5, wherein N is 128 and S is 8.
- Example 8 The CNN of any of Examples 1 to 7, wherein a non-linear value of a quantization parameter (QP) is used as an input to the CNN.
- QP quantization parameter
- Example 9 The CNN of Example 8, wherein the non-linear value of the QP is used as the input to the concatenation layer.
- Example 10 The CNN of Example 8, wherein the non-linear value of the QP is used as the input to at least one of the classification layers.
- CNN convolutional neural network
- N/S by: applying, at some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
- Example 12 The method of Example 11, wherein the classifiers includes a first classifier that infers partition decisions for sub-blocks of size (2S)x(2S), and wherein inferring, by the each classifier, partition decisions for sub-blocks of size N/(aS)xN/(aS) of the block further comprises: receiving, by the first classifier, the first feature maps from the concatenation layer; and applying, by the first classifier, a first non-overlapping convolution operation using a first 2x2 kernel to reduce the first feature maps to a size of (S/2)x(S/2).
- the classifiers includes a first classifier that infers partition decisions for sub-blocks of size (2S)x(2S), and wherein inferring, by the each classifier, partition decisions for sub-blocks of size N/(aS)xN/(aS) of the block further comprises: receiving, by the first classifier, the first feature maps from the concatenation layer; and applying, by the first classifier, a first non
- Example 13 The method of Example 12, wherein the classifiers includes a second classifier that infers partition decisions for sub-blocks of size N/(b8)cN/(b8), the second classifier being different from the first classifier, and further comprising: receiving, by the second classifier, third feature maps, each of size MxM, from a third classifier; and applying, by the second classifier, a second non-overlapping convolution operation using a second 2x2 kernel to reduce the third feature maps to a size of (M/2)x(m/2).
- Example 14 The method of any of Examples 11 to 13, comprising: applying, by a first feature extraction layer of the feature extraction layers, an (N/S)x(N/S) non-overlapping convolutional filter to the block to generate a first subset of the first feature maps of the block.
- Example 15 The method of Example 14, wherein the feature extraction layers further comprise a second feature extraction layer, further comprising: applying, by the second feature extraction layer, an MxM non-overlapping convolutional filter to the block to generate maps each of size (N/M)x(N/M), wherein M is less than S, is greater than 1, and is a power of 2; and successively applying non-overlapping 2x2 convolutional filters to the maps to generate a second subset of the first feature maps of the block.
- Example 16 The method of any of Examples 11 to 15, wherein N is 64 and S is 8.
- Example 17 The method of any of Examples 11 to 15, wherein N is 128 and S is 8.
- Example 18 The method of any of Examples 11 to 17, further comprising: using a non-linear value of a quantization parameter (QP) as an input to the concatenation layer.
- QP quantization parameter
- CNN convolutional neural network
- N/S by: applying, at some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map; and decodes the image block using the indication of the quad tree partitioning of the image block.
- Example 20 The apparatus of Example 19, wherein a non-linear value of a quantization parameter (QP) is used as an input to the CNN.
- QP quantization parameter
Abstract
Convolutional neural networks that determine a mode decision for encoding a block include feature extraction layers and multiple classifiers. A non-overlapping convolutional operation is performed at a feature extraction layer by setting a stride value equal to a kernel size. The block has a N×N size, and a smallest partition output for the block has a S×S size. Classification layers of each classifier receive feature maps having a feature dimension. An initial classification layer receives the feature maps as an output of a final feature extraction layer. Each classifier infers partition decisions for sub-blocks of size (αS)×(αS) of the block, wherein α is a power of 2 and α=2,..., N/S, by applying, at some successive classification layers, a 1×1 kernel to reduce respective feature dimensions; and outputting, by a last of the classification layers, an output corresponding to a N/(αS)×N/(αS)×1 output map.
Description
RECEPTIVE-FIELD-CONFORMING CONVOLUTION MODELS FOR VIDEO
CODING
BACKGROUND
[0001] Digital video streams may represent video using a sequence of frames or still images. Digital video can be used for various applications, including, for example, video conferencing, high-definition video entertainment, video advertisements, or sharing of user generated videos. A digital video stream can contain a large amount of data and consume a significant amount of computing or communication resources of a computing device for processing, transmission, or storage of the video data. Various approaches have been proposed to reduce the amount of data in video streams, including compression and other encoding techniques.
[0002] Over the years, the coding efficiency of video encoders has improved. Coding efficiency can mean encoding a video at the lowest possible bit rate while minimizing distortion (i.e., while maintaining a certain level of video quality). However, the improved coding efficiency has resulted in increased computational complexity. That is, more computation time is required by an encoder to achieve the improved coding efficiency. As such, it is desirable to obtain improved coding efficiencies with less computation time (i.e., reduced computational complexity).
SUMMARY
[0003] One aspect of the disclosed implementations is a convolutional neural network (CNN) for determining a mode decision for encoding a block in video coding. The CNN includes feature extraction layers for extracting features of the block for determining the mode decision. A non-overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size, the mode decision comprises a block partitioning of the block, the block has a NxN size, and a smallest partition output for the block has a SxS size. The CNN also includes multiple classifiers.
Each classifier comprises classification layers, each classification layer of the classification layers receiving respective feature maps having a respective feature dimension. Each classifier is configured to infer partition decisions for sub-blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S, by applying, at some of successive
classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension, and outputting by a final layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map. An initial classification layer of each classifier can receive the feature maps as an output of a final feature extraction layer of the feature extraction layers. The output map can indicate one or more mode decisions for the block. For example, the output map can indicate a partition decision. The output map may be used to encode the block.
[0004] Another aspect is a method of determining a mode decision for encoding a block in video coding using a convolutional neural network (CNN). The method includes extracting, using feature extraction layers of the CNN, features of the block for determining the mode decision, wherein a non-overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size, the mode decision comprises a block partitioning of the block, the block has a NxN size, and a smallest partition output for the block has a SxS size. The method also includes inferring, by multiple classifiers of the CNN that each include classification layers, the mode decision. Inferring the mode decision includes receiving, by each classification layer, respective feature maps having a respective feature dimension, and inferring, by a respective classifier of the multiple classifiers, partition decisions for sub-blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S. Inferring the mode decisions includes applying, at some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half, and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map. An initial classification layer of each classifier may receive the feature maps as an output of a final feature extraction layer of the feature extraction layers. The mode decision, as indicated by the output, may be used to encode the block.
[0005] Another aspect is an apparatus for decoding an image block. The apparatus includes a processor configured to execute a method including receiving, in a compressed bitstream, an indication of a partitioning of the image block into sub-blocks. An encoder determined the partitioning of the image block using a convolutional neural network (CNN) that includes feature extraction layers for extracting features of the block for determining the partitioning, wherein a non-overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size, the block has a NxN size, and a smallest partition output for the block has a SxS size. The CNN
also includes multiple classifiers, wherein each classifier comprises classification layers, each classification layer of the classification layers receiving respective feature maps having a respective feature dimension. Each classifier is configured to infer partition decisions for sub blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S, by applying, at some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension, and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map. An initial classification layer of each classifier can receive the feature maps as an output of a final feature extraction layer of the feature extraction layers. The method also includes decoding the image block using the indication of the partitioning of the image block.
[0006] Another aspect of the disclosed implementations is a convolutional neural network (CNN) for determining a block partitioning in video coding, the block has a size of NxN, and a smallest partition determined by the CNN is of size SxS. The CNN includes feature extraction layers; a concatenation layer that receives, from the feature extraction layers, first feature maps of the block, where each first feature map is of size SxS; and classifiers. Each classifier includes classification layers, each classification layer receives second feature maps having a respective feature dimension. Each classifier is configured to infer partition decisions for sub-blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S, by: applying, at least some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
[0007] Another aspect is a method of determining a block partitioning in video coding using a convolutional neural network (CNN), the block has an NxN size, and a smallest partition determined by the CNN is of size SxS. The method includes extracting, using feature extraction layers of the CNN, first feature maps of the block, where each first feature map is of size SxS; concatenating, using a concatenation layer of the CNN, the first feature maps of the block; and inferring, by classifiers of the CNN each including classification layers, block partitioning. The inferring includes receiving, by each classification layer, second feature maps having a respective feature dimension; inferring, by the each classifier, partition decisions for sub-blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S, by: applying, at some of successive classification layers of the
classification layers, a kernel of size lxl to reduce the respective feature dimension in half;
and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
[0008] Another aspect is an apparatus for decoding an image block, including a processor that receives, in a compressed bitstream, an indication of a quad-tree partitioning of the image block into sub-blocks, wherein an encoder determined the quad-tree partitioning of the image block using a convolutional neural network (CNN); and decodes the image block using the indication of the quad-tree partitioning of the image block. The CNN includes feature extraction layers; a concatenation layer that receives, from the feature extraction layers, first feature maps of the block, where each first feature map is of size SxS; and classifiers. Each classifier includes classification layers. Each classification layer receives second feature maps having a respective feature dimension. Each classifier is configured to infer partition decisions for sub-blocks of size (aS)x(aS) of the block, where a is a power of 2 and a=2, . . ., N/S, by: applying, at some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
[0009] These and other aspects of the present disclosure are disclosed in the following detailed description of the embodiments, the appended claims, and the accompanying figures.
BRIEF DESCRIPTION OF THE DRAWINGS
[0010] The description herein makes reference to the accompanying drawings, wherein like reference numerals refer to like parts throughout the several views.
[0011] FIG. 1 is a schematic of a video encoding and decoding system.
[0012] FIG. 2 is a block diagram of an example of a computing device that can implement a transmitting station or a receiving station.
[0013] FIG. 3 is a diagram of a video stream to be encoded and subsequently decoded.
[0014] FIG. 4 is a block diagram of an encoder according to implementations of this disclosure.
[0015] FIG. 5 is a block diagram of a decoder according to implementations of this disclosure.
[0016] FIG. 6 is a block diagram of a representation of a portion of a frame according to implementations of this disclosure.
[0017] FIG. 7 is a block diagram of an example of a quad-tree representation of a block according to implementations of this disclosure.
[0018] FIG. 8 is a flowchart of a process for searching for a best mode to code a block.
[0019] FIG. 9 is a block diagram of an example of estimating the rate and distortion costs of coding an image block using a prediction mode.
[0020] FIG. 10 is a block diagram of a first example of a convolutional neural network (CNN) for a mode decision according to implementations of this disclosure.
[0021] FIG. 11 is an example of convolution operations according to implementations of this disclosure.
[0022] FIG. 12 is an example of receptive fields according to implementations of this disclosure.
[0023] FIG. 13 is a block diagram of a second example of a CNN for a mode decision according to implementations of this disclosure.
[0024] FIG. 14 is a block diagram of a third example of a CNN for a mode decision according to implementations of this disclosure.
[0025] FIG. 15 is a block diagram of a fourth example of a CNN for a mode decision according to implementations of this disclosure.
[0026] FIG. 16 is a block diagram of a fifth example of a CNN for a mode decision according to implementations of this disclosure.
[0027] FIG. 17 is a flowchart of a process for encoding, by an encoder, an image block according to implementations of this disclosure.
[0028] FIG. 18 is an example of approximating a non-linear function of a quantization parameter using linear segments according to implementations of this disclosure.
[0029] FIG. 19 is an example of a rate-distortion performance comparison of a first machine-learning model that uses a non-linear QP function as input and a second machine- learning model that uses a linear QP function as input.
[0030] FIG. 20 is an example of non-square partitions of a block.
DETAILED DESCRIPTION
[0031] Encoding techniques may be designed to maximize coding efficiency. Coding efficiency can mean encoding a video at the lowest possible bit rate while minimizing distortion (e.g., while maintaining a certain level of video quality). Coding efficiency is typically measured in terms of both rate and distortion. Rate refers to the number of bits required for encoding (such as encoding a block, a frame, etc.). Distortion measures the quality loss between, for example, a source video block and a reconstructed version of the
source video block. For example, the distortion may be calculated as a mean-square error between pixel values of the source block and those of the reconstructed block. By performing a rate-distortion optimization process, a video codec optimizes the amount of distortion against the rate required to encode the video.
[0032] Modem video codecs (e.g., H.264, which is also known as MPEG-4 AVC; VP9; H.265, which is also known as HEVC; AVS2; and AV1) define and use a large number of tools and configurations (e.g., parameters) to improve coding efficiency. A video encoder can use a mode decision to examine (e.g., test, evaluate, etc.) at least some of the valid combinations of parameters to select a combination that results in a relatively low rate- distortion value. An example of a mode decision is an intra-prediction mode decision, which determines the best intra-prediction mode for coding a block. Another example of a mode decision is a partition decision, which determines an optimal partitioning of a coding unit. Another example of a mode decision includes a decision as to a transform type and/or size to use in transforming a block (such as a residual or an image block) from the pixel domain to the frequency domain to form a transform block that includes transform coefficients.
[0033] To evaluate whether one combination is better than another, a metric can be computed for each of the examined combinations and the respective metrics compared. In an example, the metric can combine the rate and distortion described above to produce a rate- distortion (RD) value or cost. The RD value or cost may be a single scalar value.
[0034] As mentioned, a best mode can be selected from many possible combinations. For example, the RD cost associated with a specific mode (or a specific combination of tools) may be determined by performing at least a subset of the encoding steps of the encoder. The subset of the encoding steps can include, depending on the mode for which a RD cost is to be determined, at least one of determining a prediction block, determining a residual block, determining a transform type, determining an interpolation filter, quantizing a transform block, entropy encoding, and so on. Note that these encoding steps are neither intended to be an exhaustive list of encoding steps that a typical encoder may perform nor presented in any particular order (that is, an encoder does not necessarily perform these steps, as listed, sequentially). As the number of possible tools and parameters increases, the number of combinations also increases, which, in turn, increases the time required to determine the best mode.
[0035] Instead of an exhaustive search, an encoder may terminate a mode search as soon as it finds a mode with a RD cost that is less than a set threshold. This means, however, that a
better mode may have been found later on if the encoder had continued in mode search. In some cases, an exhaustive search may or may not be performed, but the entire RD cost calculation is replaced by a coarse estimation. This can further degrade decision making by an encoder.
[0036] Techniques such as machine learning may be exploited to reduce the time required to determine a best mode, such as a partition mode. For example, instead of performing all of the encoding steps (i.e., a brute-force or exhaustive approach) for determining the rate and distortion for various partitioning modes to compare those modes and select a best mode, a machine-learning model can be used to estimate or infer the best mode.
[0037] The machine-learning model may be trained using the vast amount of training data that is available from an encoder performing standard encoding techniques, such as those described below. More specifically, the training data can be used during the learning phase of machine learning to derive (e.g., learn, infer, etc.) the machine-learning model that is (e.g., defines, constitutes, etc.) a mapping from the input data (e.g., block data) to an output.
[0038] Once a machine-learning model is trained, the model computes the output as a deterministic function of its input. In an example, the machine-learning model can be a neural network model, which can be implemented by a convolutional neural network (CNN). A well-trained machine-learning model can be expected to closely match the brute-force approach in coding efficiency but at a significantly lower computational cost or with a regular or dataflow-oriented computational cost.
[0039] In addition to using appropriate training data and inputs to the machine-learning model, the architecture of the machine-learning model can also be critical to the performance and/or prediction capability of the model. The models described herein comprise convolution layers of a CNN with filter designs that respect boundaries for recursive partitioning. That is, when analyzing an image region, such as for determining a quad-tree partitioning, the features extracted (e.g., calculated, inferred, etc.) for the image region are confined to the image region itself, and not for other image regions. Further, the models described herein allow the models to have a small size (i.e., those with a reduced number of parameters as compared to models using fully connected layers). The inference accuracy for mode decision in video encoding can be significantly improved while reducing the computational complexity.
[0040] Further details of the inventive CNN architectures according to the teachings herein will be discussed below first with reference to a block-based codec with the teachings
may be incorporated. Although a block-based codec is described as an example, other codecs may be used with the present teachings, including a feature-based codec.
[0041] FIG. 1 is a schematic of a video encoding and decoding system 100. A transmitting station 102 can be, for example, a computer having an internal configuration of hardware, such as that described with respect to FIG. 2. However, other suitable
implementations of the transmitting station 102 are possible. For example, the processing of the transmitting station 102 can be distributed among multiple devices.
[0042] A network 104 can connect the transmitting station 102 and a receiving station 106 for encoding and decoding of the video stream. Specifically, the video stream can be encoded in the transmitting station 102, and the encoded video stream can be decoded in the receiving station 106. The network 104 can be, for example, the Internet. The network 104 can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmitting station 102 to, in this example, the receiving station 106.
[0043] In one example, the receiving station 106 can be a computer having an internal configuration of hardware, such as that described with respect to FIG. 2. However, other suitable implementations of the receiving station 106 are possible. For example, the processing of the receiving station 106 can be distributed among multiple devices.
[0044] Other implementations of the video encoding and decoding system 100 are possible. For example, an implementation can omit the network 104. In another
implementation, a video stream can be encoded and then stored for transmission at a later time to the receiving station 106 or any other device having memory. In one implementation, the receiving station 106 receives (e.g., via the network 104, a computer bus, and/or some communication pathway) the encoded video stream and stores the video stream for later decoding. In an example implementation, a real-time transport protocol (RTP) is used for transmission of the encoded video over the network 104. In another implementation, a transport protocol other than RTP (e.g., an HTTP-based video streaming protocol) may be used.
[0045] When used in a video conferencing system, for example, the transmitting station 102 and/or the receiving station 106 may include the ability to both encode and decode a video stream as described below. For example, the receiving station 106 could be a video conference participant who receives an encoded video bitstream from a video conference server (e.g., the transmitting station 102) to decode and view and further encodes and
transmits its own video bitstream to the video conference server for decoding and viewing by other participants.
[0046] FIG. 2 is a block diagram of an example of a computing device 200 that can implement a transmitting station or a receiving station. For example, the computing device 200 can implement one or both of the transmitting station 102 and the receiving station 106 of FIG. 1. The computing device 200 can be in the form of a computing system including multiple computing devices, or in the form of a single computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like.
[0047] A CPU 202 in the computing device 200 can be a central processing unit.
Alternatively, the CPU 202 can be any other type of device, or multiple devices, now-existing or hereafter developed, capable of manipulating or processing information. Although the disclosed implementations can be practiced with a single processor as shown (e.g., the CPU 202), advantages in speed and efficiency can be achieved by using more than one processor.
[0048] In an implementation, a memory 204 in the computing device 200 can be a read only memory (ROM) device or a random-access memory (RAM) device. Any other suitable type of storage device can be used as the memory 204. The memory 204 can include code and data 206 that is accessed by the CPU 202 using a bus 212. The memory 204 can further include an operating system 208 and application programs 210, the application programs 210 including at least one program that permits the CPU 202 to perform the methods described herein. For example, the application programs 210 can include applications 1 through N, which further include a video coding application that performs the methods described herein. The computing device 200 can also include a secondary storage 214, which can, for example, be a memory card used with a computing device 200 that is mobile. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
[0049] The computing device 200 can also include one or more output devices, such as a display 218. The display 218 may be, in one example, a touch-sensitive display that combines a display with a touch-sensitive element that is operable to sense touch inputs. The display 218 can be coupled to the CPU 202 via the bus 212. Other output devices that permit a user to program or otherwise use the computing device 200 can be provided in addition to or as an alternative to the display 218. When the output device is or includes a display, the display can
be implemented in various ways, including as a liquid crystal display (LCD); a cathode -ray tube (CRT) display; or a light-emitting diode (LED) display, such as an organic LED
(OLED) display.
[0050] The computing device 200 can also include or be in communication with an image-sensing device 220, for example, a camera, or any other image-sensing device, now existing or hereafter developed, that can sense an image, such as the image of a user operating the computing device 200. The image-sensing device 220 can be positioned such that it is directed toward the user operating the computing device 200. In an example, the position and optical axis of the image-sensing device 220 can be configured such that the field of vision includes an area that is directly adjacent to the display 218 and from which the display 218 is visible.
[0051] The computing device 200 can also include or be in communication with a sound sensing device 222, for example, a microphone, or any other sound-sensing device, now existing or hereafter developed, that can sense sounds near the computing device 200. The sound-sensing device 222 can be positioned such that it is directed toward the user operating the computing device 200 and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates the computing device 200.
[0052] Although FIG. 2 depicts the CPU 202 and the memory 204 of the computing device 200 as being integrated into a single unit, other configurations can be utilized. The operations of the CPU 202 can be distributed across multiple machines (each machine having one or more processors) that can be coupled directly or across a local area or other network. The memory 204 can be distributed across multiple machines, such as a network-based memory or memory in multiple machines performing the operations of the computing device 200. Although depicted here as a single bus, the bus 212 of the computing device 200 can be composed of multiple buses. Further, the secondary storage 214 can be directly coupled to the other components of the computing device 200 or can be accessed via a network and can comprise a single integrated unit, such as a memory card, or multiple units, such as multiple memory cards. The computing device 200 can thus be implemented in a wide variety of configurations.
[0053] FIG. 3 is a diagram of an example of a video stream 300 to be encoded and subsequently decoded. The video stream 300 includes a video sequence 302. At the next level, the video sequence 302 includes a number of adjacent frames 304. While three frames are depicted as the adjacent frames 304, the video sequence 302 can include any number of
-lO-
adjacent frames 304. The adjacent frames 304 can then be further subdivided into individual frames, for example, a frame 306. At the next level, the frame 306 can be divided into a series of segments 308 or planes. The segments 308 can be subsets of frames that permit parallel processing, for example. The segments 308 can also be subsets of frames that can separate the video data into separate colors. For example, the frame 306 of color video data can include a luminance (or luma) plane and two chrominance (or chroma) planes. The segments 308 may be sampled at different resolutions.
[0054] Whether or not the frame 306 is divided into the segments 308, the frame 306 may be further subdivided into blocks 310, which can contain data corresponding to, for example, 16x16 pixels in the frame 306. The blocks 310 can also be arranged to include data from one or more segments 308 of pixel data. The blocks 310 can also be of any other suitable size, such as 4x4 pixels, 8x8 pixels, 16x8 pixels, 8x16 pixels, 16x16 pixels, or larger. For example, and as described below with regards to FIG. 6, a block may comprise luma pixels from the luma plane, or chroma pixels from the chroma plane.
[0055] FIG. 4 is a block diagram of an encoder 400 in accordance with implementations of this disclosure. The encoder 400 can be implemented, as described above, in the transmitting station 102, such as by providing a computer software program stored in memory, for example, the memory 204. The computer software program can include machine instructions that, when executed by a processor, such as the CPU 202, cause the transmitting station 102 to encode video data in manners described herein. The encoder 400 can also be implemented as specialized hardware included in, for example, the transmitting station 102. The encoder 400 has the following stages to perform the various functions in a forward path (shown by the solid connection lines) to produce an encoded or compressed bitstream 420 using the video stream 300 as input: an intra/inter-prediction stage 402, a transform stage 404, a quantization stage 406, and an entropy encoding stage 408. The encoder 400 may also include a reconstruction path (shown by the dotted connection lines) to reconstruct a frame for encoding of future blocks. In FIG. 4, the encoder 400 has the following stages to perform the various functions in the reconstruction path: a dequantization stage 410, an inverse transform stage 412, a reconstruction stage 414, and a loop filtering stage 416. Other structural variations of the encoder 400 can be used to encode the video stream 300.
[0056] When the video stream 300 is presented for encoding, the frame 306 can be processed in units of blocks. At the intra/inter-prediction stage 402, a block can be encoded using intra-frame prediction (also called intra-prediction) or inter-frame prediction (also
called inter-prediction), or a combination of both. In any case, a prediction block can be formed. In the case of intra-prediction, all or part of a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed. In the case of inter-prediction, all or part of a prediction block may be formed from samples in one or more previously constructed reference frames determined using motion vectors.
[0057] Next, still referring to FIG. 4, the prediction block can be subtracted from the current block at the intra/inter-prediction stage 402 to produce a residual block (also called a residual). The transform stage 404 transforms the residual into transform coefficients in, for example, the frequency domain using block-based transforms. Such block-based transforms (i.e., transform types) include, for example, the Discrete Cosine Transform (DCT) and the Asymmetric Discrete Sine Transform (ADST). Other block-based transforms are possible. Further, combinations of different transforms may be applied to a single residual. In one example of application of a transform, the DCT transforms the residual block into the frequency domain where the transform coefficient values are based on spatial frequency. The lowest frequency (DC) coefficient is at the top-left of the matrix, and the highest frequency coefficient is at the bottom-right of the matrix. It is worth noting that the size of a prediction block, and hence the resulting residual block, may be different from the size of the transform block. For example, the prediction block may be split into smaller blocks to which separate transforms are applied.
[0058] The quantization stage 406 converts the transform coefficients into discrete quantum values, which are referred to as quantized transform coefficients, using a quantizer value or a quantization level. For example, the transform coefficients may be divided by the quantizer value and truncated. The quantized transform coefficients are then entropy encoded by the entropy encoding stage 408. Entropy coding may be performed using any number of techniques, including token and binary trees. The entropy-encoded coefficients, together with other information used to decode the block (which may include, for example, the type of prediction used, transform type, motion vectors, and quantizer value), are then output to the compressed bitstream 420. The information to decode the block may be entropy coded into block, frame, slice, and/or section headers within the compressed bitstream 420. The compressed bitstream 420 can also be referred to as an encoded video stream or encoded video bitstream; these terms will be used interchangeably herein.
[0059] The reconstruction path in FIG. 4 (shown by the dotted connection lines) can be used to ensure that both the encoder 400 and a decoder 500 (described below) use the same
reference frames and blocks to decode the compressed bitstream 420. The reconstruction path performs functions that are similar to functions that take place during the decoding process and that are discussed in more detail below, including dequantizing the quantized transform coefficients at the dequantization stage 410 and inverse transforming the dequantized transform coefficients at the inverse transform stage 412 to produce a derivative residual block (also called a derivative residual). At the reconstruction stage 414, the prediction block that was predicted at the intra/inter-prediction stage 402 can be added to the derivative residual to create a reconstructed block. The loop filtering stage 416 can be applied to the reconstructed block to reduce distortion, such as blocking artifacts.
[0060] Other variations of the encoder 400 can be used to encode the compressed bitstream 420. For example, a non-transform based encoder can quantize the residual signal directly without the transform stage 404 for certain blocks or frames. In another
implementation, an encoder 400 can have the quantization stage 406 and the dequantization stage 410 combined into a single stage.
[0061] FIG. 5 is a block diagram of a decoder 500 in accordance with implementations of this disclosure. The decoder 500 can be implemented in the receiving station 106, for example, by providing a computer software program stored in the memory 204. The computer software program can include machine instructions that, when executed by a processor, such as the CPU 202, cause the receiving station 106 to decode video data in the manners described below. The decoder 500 can also be implemented in hardware included in, for example, the transmitting station 102 or the receiving station 106.
[0062] The decoder 500, similar to the reconstruction path of the encoder 400 discussed above, includes in one example the following stages to perform various functions to produce an output video stream 516 from the compressed bitstream 420: an entropy decoding stage 502, a dequantization stage 504, an inverse transform stage 506, an intra/inter-prediction stage 508, a reconstruction stage 510, a loop filtering stage 512, and a post filtering stage 514. Other structural variations of the decoder 500 can be used to decode the compressed bitstream 420.
[0063] When the compressed bitstream 420 is presented for decoding, the data elements within the compressed bitstream 420 can be decoded by the entropy decoding stage 502 to produce a set of quantized transform coefficients. The dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and the inverse transform stage 506 inverse transforms the
43
dequantized transform coefficients using the selected transform type to produce a derivative residual that can be identical to that created by the inverse transform stage 412 in the encoder 400. Using header information decoded from the compressed bitstream 420, the decoder 500 can use the intra/inter-prediction stage 508 to create the same prediction block as was created in the encoder 400, for example, at the intra/inter-prediction stage 402. At the reconstruction stage 510, the prediction block can be added to the derivative residual to create a
reconstructed block. The loop filtering stage 512 can be applied to the reconstructed block to reduce blocking artifacts. Other filtering can be applied to the reconstructed block. In an example, the post filtering stage 514 is applied to the reconstructed block to reduce blocking distortion, and the result is output as an output video stream 516. The output video stream 516 can also be referred to as a decoded video stream; these terms will be used
interchangeably herein.
[0064] Other variations of the decoder 500 can be used to decode the compressed bitstream 420. For example, the decoder 500 can produce the output video stream 516 without the post filtering stage 514. In some implementations of the decoder 500, the post filtering stage 514 is applied after the loop filtering stage 512. The loop filtering stage 512 can include an optional deblocking filtering stage. Additionally, or alternatively, the encoder 400 includes an optional deblocking filtering stage in the loop filtering stage 416.
[0065] FIG. 6 is a block diagram of a representation of a portion 600 of a frame, such as the frame 306 of FIG. 3, according to implementations of this disclosure. As shown, the portion 600 of the frame includes four 64x64 blocks 610, which may be referred to as superblocks, in two rows and two columns in a matrix or Cartesian plane. A superblock can have a larger or a smaller size. While FIG. 6 is explained with respect to a superblock of size 64x64, the description is easily extendable to larger (e.g., 128x128) or smaller superblock sizes.
[0066] In an example, and without loss of generality, a superblock can be a basic or maximum coding unit (CU). Each superblock can include four 32x32 blocks 620. Each 32x32 block 620 can include four 16x16 blocks 630. Each 16x16 block 630 can include four 8x8 blocks 640. Each 8x8 block 640 can include four 4x4 blocks 650. Each 4x4 block 650 can include 16 pixels, which can be represented in four rows and four columns in each respective block in the Cartesian plane or matrix. The pixels can include information representing an image captured in the frame, such as luminance information, color information, and location information. In an example, a block, such as a 16x16-pixel block as
shown, can include a luminance block 660, which can include luminance pixels 662; and two chrominance blocks 670/680, such as a U or Cb chrominance block 670, and a V or Cr chrominance block 680. The chrominance blocks 670/680 can include chrominance pixels 690. For example, the luminance block 660 can include 16x16 luminance pixels 662, and each chrominance block 670/680 can include 8x8 chrominance pixels 690, as shown.
Although one arrangement of blocks is shown, any arrangement can be used. Although FIG.
6 shows NxN blocks, in some implementations, NxM, where N¹M, blocks can be used. For example, 32x64 blocks, 64x32 blocks, 16x32 blocks, 32x16 blocks, or any other size blocks can be used. In some implementations, Nx2N blocks, 2NxN blocks, or a combination thereof can be used.
[0067] In some implementations, video coding can include ordered block-level coding. Ordered block- level coding can include coding blocks of a frame in an order, such as raster- scan order, wherein blocks can be identified and processed starting with a block in the upper left corner of the frame, or a portion of the frame, and proceeding along rows from left to right and from the top row to the bottom row, identifying each block in turn for processing. For example, the superblock in the top row and left column of a frame can be the first block coded, and the superblock immediately to the right of the first block can be the second block coded. The second row from the top can be the second row coded, such that the superblock in the left column of the second row can be coded after the superblock in the rightmost column of the first row.
[0068] In an example, coding a block can include using quad-tree coding, which can include coding smaller block units with a block in raster-scan order. The 64x64 superblock shown in the bottom-left comer of the portion of the frame shown in FIG. 6, for example, can be coded using quad-tree coding in which the top-left 32x32 block can be coded, then the top-right 32x32 block can be coded, then the bottom-left 32x32 block can be coded, and then the bottom-right 32x32 block can be coded. Each 32x32 block can be coded using quad-tree coding in which the top-left 16x16 block can be coded, then the top-right 16x16 block can be coded, then the bottom- left 16x16 block can be coded, and then the bottom-right 16x16 block can be coded. Each 16x16 block can be coded using quad- tree coding in which the top-left 8x8 block can be coded, then the top-right 8x8 block can be coded, then the bottom-left 8x8 block can be coded, and then the bottom-right 8x8 block can be coded. Each 8x8 block can be coded using quad-tree coding in which the top-left 4x4 block can be coded, then the top- right 4x4 block can be coded, then the bottom-left 4x4 block can be coded, and then the
bottom-right 4x4 block can be coded. In some implementations, 8x8 blocks can be omitted for a 16x16 block, and the 16x16 block can be coded using quad- tree coding in which the top-left 4x4 block can be coded, and then the other 4x4 blocks in the 16x16 block can be coded in raster-scan order.
[0069] In an example, video coding can include compressing the information included in an original, or input, frame by omitting some of the information in the original frame from a corresponding encoded frame. For example, coding can include reducing spectral redundancy, reducing spatial redundancy, reducing temporal redundancy, or a combination thereof.
[0070] In an example, reducing spectral redundancy can include using a color model based on a luminance component (Y) and two chrominance components (U and V or Cb and Cr), which can be referred to as the YUV or YCbCr color model or color space. Using the YUV color model can include using a relatively large amount of information to represent the luminance component of a portion of a frame and using a relatively small amount of information to represent each corresponding chrominance component for the portion of the frame. For example, a portion of a frame can be represented by a high-resolution luminance component, which can include a 16x16 block of pixels, and by two lower resolution chrominance components, each of which representing the portion of the frame as an 8x8 block of pixels. A pixel can indicate a value (e.g., a value in the range from 0 to 255) and can be stored or transmitted using, for example, eight bits. Although this disclosure is described with reference to the YUV color model, any color model can be used.
[0071] Reducing spatial redundancy can include transforming a block into the frequency domain as described above. For example, a unit of an encoder, such as the entropy encoding stage 408 of FIG. 4, can perform a DCT using transform coefficient values based on spatial frequency.
[0072] Reducing temporal redundancy can include using similarities between frames to encode a frame using a relatively small amount of data based on one or more reference frames, which can be previously encoded, decoded, and reconstructed frames of the video stream. For example, a block or a pixel of a current frame can be similar to a spatially corresponding block or pixel of a reference frame. A block or a pixel of a current frame can be similar to a block or a pixel of a reference frame at a different spatial location. As such, reducing temporal redundancy can include generating motion information indicating the
spatial difference (e.g., a translation between the location of the block or the pixel in the current frame and the corresponding location of the block or the pixel in the reference frame).
[0073] Reducing temporal redundancy can include identifying a block or a pixel in a reference frame, or a portion of the reference frame, that corresponds with a current block or pixel of a current frame. For example, a reference frame, or a portion of a reference frame, which can be stored in memory, can be searched for the best block or pixel to use for encoding a current block or pixel of the current frame. For example, the search may identify the block of the reference frame for which the difference in pixel values between the reference block and the current block is minimized, and can be referred to as motion searching. The portion of the reference frame searched can be limited. For example, the portion of the reference frame searched, which can be referred to as the search area, can include a limited number of rows of the reference frame. In an example, identifying the reference block can include calculating a cost function, such as a sum of absolute differences (SAD), between the pixels of the blocks in the search area and the pixels of the current block.
[0074] The spatial difference between the location of the reference block in the reference frame and the current block in the current frame can be represented as a motion vector. The difference in pixel values between the reference block and the current block can be referred to as differential data, residual data, or as a residual block. In some implementations, generating motion vectors can be referred to as motion estimation, and a pixel of a current block can be indicated based on location using Cartesian coordinates such as /x,y. Similarly, a pixel of the search area of the reference frame can be indicated based on a location using Cartesian coordinates such as rx,y. A motion vector (MV) for the current block can be determined based on, for example, a SAD between the pixels of the current frame and the corresponding pixels of the reference frame.
[0075] Although other partitions are possible, as described above with regards to FIG. 6, a CU or block may be coded using quad-tree partitioning or coding as shown in the example of FIG. 7. The example shows quad-tree partitioning of a block 700. However, the block 700 can be partitioned differently, such as by an encoder (e.g., the encoder 400 of FIG. 4) or a machine-learning model as described below.
[0076] The block 700 is partitioned into four blocks, namely, the blocks 700-1, 700-2, 700-3, and 700-4. The block 700-2 is further partitioned into the blocks 702-1, 702-2, 702-3, and 702-4. As such, if, for example, the size of the block 700 is NxN (e.g., 128x128), then the blocks 700-1, 700-2, 700-3, and 700-4 are each of size N/2xN/2 (e.g., 64x64), and the
blocks 702-1, 702-2, 702-3, and 702-4 are each of size N/4xN/4 (e.g., 32x32). If a block is partitioned, it is partitioned into four equally sized, non-overlapping square sub-blocks.
[0077] A quad-tree data representation is used to describe how the block 700 is partitioned into sub-blocks, such as blocks 700-1, 700-2, 700-3, 700-4, 702-1, 702-2, 702-3, and 702-4. A quadtree 704 of the partition of the block 700 is shown. Each node of the quadtree 704 is assigned a flag of“1” if the node is further split into four sub-nodes and assigned a flag of“0” if the node is not split. The flag can be referred to as a split bit (e.g., 1) or a stop bit (e.g., 0) and is coded in a compressed bitstream. In a quadtree, a node either has four child nodes or has no child nodes. A node that has no child nodes corresponds to a block that is not split further. Each of the child nodes of a split block corresponds to a sub-block.
[0078] In the quadtree 704, each node corresponds to a sub-block of the block 700. The corresponding sub-block is shown between parentheses. For example, a node 704-1, which has a value of 0, corresponds to the block 700-1.
[0079] A root node 704-0 corresponds to the block 700. As the block 700 is split into four sub-blocks, the value of the root node 704-0 is the split bit (e.g., 1). At an intermediate level, the flags indicate whether a sub-block of the block 700 is further split into four sub-sub- blocks. In this case, a node 704-2 includes a flag of“1” because the block 700-2 is split into the blocks 702-1, 702-2, 702-3, and 702-4. Each of nodes 704-1, 704-3, and 704-4 includes a flag of“0” because the corresponding blocks are not split. As nodes 704-5, 704-6, 704-7, and 704-8 are at a bottom level of the quadtree, no flag of“0” or“1” is necessary for these nodes. That the blocks 702-5, 702-6, 702-7, and 702-8 are not split further can be inferred from the absence of additional flags corresponding to these blocks. In this example, the smallest sub block is 32x32 pixels, but further partitioning is possible.
[0080] The quad-tree data for the quadtree 704 can be represented by the binary data of “10100,” where each bit represents a node of the quadtree 704. The binary data indicates the partitioning of the block 700 to the encoder and decoder. The encoder can encode the binary data in a compressed bitstream, such as the compressed bitstream 420 of FIG. 4, in a case where the encoder needs to communicate the binary data to a decoder, such as the decoder 500 of FIG. 5.
[0081] The blocks corresponding to the leaf nodes of the quadtree 704 can be used as the bases for prediction. That is, prediction can be performed for each of the blocks 700-1, 702-1, 702-2, 702-3, 702-4, 700-3, and 700-4, referred to herein as coding blocks. As mentioned with respect to FIG. 6, the coding block can be a luminance block or a chrominance block. It
48
is noted that, in an example, the block partitioning can be determined with respect to luminance blocks. The same partition, or a different partition, can be used with the chrominance blocks.
[0082] A prediction type (e.g., intra- or inter-prediction) is determined at the coding block. That is, a coding block is the decision point for prediction.
[0083] A mode decision process (e.g., partition decision process) determines the partitioning of a coding block, such as the block 700. The partition decision process calculates the RD costs of different combinations of coding parameters. That is, for example, different combinations of prediction blocks and predictions (e.g., intra-prediction, inter prediction, etc.) are examined to determine an optimal partitioning.
[0084] As a person skilled in the art recognizes, many mode decision processes can be performed by an encoder.
[0085] FIG. 8 is a flowchart of a process 800 for searching for a best mode to code a block. The process 800 is an illustrative, high level process of a mode decision process that determines a best mode. For ease of description, the process 800 is described with respect to selecting an intra-prediction mode for encoding a prediction block. Other examples of best modes that can be determined by processes similar to the process 800 include determining a transform type and determining a transform size. The process 800 can be implemented by an encoder, such as the encoder 400 of FIG. 4, using a brute-force approach to mode decision.
[0086] At 802, the process 800 receives an image block. As the process 800 is described with respect to determining an intra-prediction mode, the image block can be a prediction unit. As described with respect to FIG. 7, each of the leaf node coding blocks (e.g., a block 700-1, 702-1, 702-2, 702-3, 702-4, 700-3, or 700-4) can be partitioned into one or more prediction units until a smallest prediction unit/block size is reached such that further partitioning is not possible. The image block can be one such prediction unit.
[0087] At 804, the process 800 determines (e.g., selects, calculates, chooses, etc.) a list of modes. The list of modes can include K modes, where K is an integer number. The list of modes can be denoted {mi, m2, ..., nik}. The encoder can have available a list of intra prediction modes. For example, the list of available intra-prediction modes can be
{DC_PRED, V_PRED, H_PRED, D45_PRED, Dl35_PRED, Dl l7_PRED, Dl53_PRED, D207_PRED, D63_PRED, SMOOTH_PRED, SMOOTH_V_PRED, and
SMOOTH_H_PRED, PAETH_PRED}. A description of these intra-prediction modes is
omitted as the description is not pertinent to the understanding of this disclosure. The list of modes determined at 804 can be any subset of the list of available intra-prediction modes.
[0088] At 806, the process 800 initializes a BEST_COST variable to a high value (e.g., INT_MAX, which may be equal to 2,147,483,647) and initializes a loop variable i to 1, which corresponds to the first mode to be examined.
[0089] At 808, the process 800 computes (e.g., calculates) an RD_COSTi for the mode;. At 810, the process 800 tests whether the RD cost, RD_COSTi, of the current mode under examination, modei, is less than the current best cost, BEST_COST. If the test at 810 is positive, then at 812, the process 800 updates the best cost to be the cost of the current mode (i.e., BEST_COST = RD_COSTi) and sets the current best mode index (BEST_MODE) to the loop variable i (BEST_MODE = i). The process 800 then proceeds to 814 to increment the loop variable i (i.e., i = i + 1) to prepare for examining the next mode (if any). If the test at 810 is negative, then the process 800 proceeds to 814.
[0090] At 816, if there are more modes to examine (i.e., if i < K), the process 800 proceeds back to 808; otherwise the process 800 proceeds to 818. At 818, the process 800 outputs the index of the best mode, BEST_MODE. Outputting the best mode can mean returning the best mode to a caller of the process 800. Outputting the best mode can mean encoding the image using the best mode. Outputting the best mode can have other semantics. The process 800 term i nates after outputting the best mode at 818.
[0091] FIG. 9 is a block diagram of an example 900 of estimating the rate and distortion costs of coding an image block X by using a prediction mode m j. The process 900 can be performed by an encoder, such as the encoder 400 of FIG. 4. The process 900 includes performing a hypothetical encoding of the image block X using the prediction mode mj to determine the RD cost of encoding the block. The process 900 can be used by the process 800 at 808.
[0092] A hypothetical encoding process is a process that carries out the coding steps but does not output bits into a compressed bitstream, such as the compressed bitstream 420 of FIG. 4. Since the purpose is to estimate a rate (also referred as bit rate), a hypothetical encoding process may be regarded or called a rate estimation process. The hypothetical encoding process computes the number of bits (RATE) required to encode the image block X. The example 900 also calculates a distortion (DISTORTION) based on a difference between the image block X and a reconstructed version of the image block X.
[0093] At 904, a prediction, using the mode mi, is determined. The prediction can be determined as described with respect to intra/inter-prediction stage 402 of FIG. 4. At 906, a residual is determined as a difference between the image block 902 and the prediction. At 908 and 910, the residual is transformed and quantized, such as described, respectively, with respect to the transform stage 404 and the quantization stage 406 of FIG. 4. The rate (RATE) is calculated by a rate estimator 912, which performs the hypothetical encoding. In an example, the rate estimator 912 can perform entropy encoding, such as described with respect to the entropy encoding stage 408 of FIG. 4.
[0094] The quantized residual is dequantized at 914 (such as described, for example, with respect to the dequantization stage 410 of FIG. 4), inverse transformed at 916 (such as described, for example, with respect to the inverse transform stage 412 of FIG. 4), and reconstructed at 918 (such as described, for example, with respect to the reconstruction stage 414 of FIG. 4) to generate a reconstructed block. A distortion estimator 920 calculates the distortion (i.e., the loss in video quality) between the image block X and the reconstructed block. In an example, the distortion can be a mean square error between pixel values of the image block X and the reconstructed block. The distortion can be a sum of absolute differences error between pixel values of the image block X and the reconstructed block. Any other suitable distortion measure can be used.
[0095] The rate, RATE, and distortion, DISTORTION, are then combined into a scalar value (i.e., the RD cost) by using the Lagrange multiplier according to DISTORTION + ^mode x RATE .
[0096] The Lagrange multiplier Amode can be calculated as described herein, depending on the encoder performing the operations of the example 900.
[0097] FIGS. 8 and 9 illustrate that a brute-force approach to mode decision is largely a serial process that essentially codes an image block X by using candidate modes to determine the mode with the best cost. Machine learning can be used to reduce the computational complexity in mode decisions.
[0098] At a high level, and without loss of generality, a machine-learning model, such as a classification deep-leaming model, includes two main portions: a feature-extraction portion and a classification portion. The feature-extraction portion detects features of the model. The classification portion attempts to classify the detected features into a desired response. Each of the portions can include one or more layers and/or one or more operations.
[0099] As mentioned above, a CNN is an example of a machine-learning model. In a CNN, the feature extraction portion can include a set of convolutional operations, which is typically a series of filters that are used to filter an input image based on a filter (e.g., a square of size k). For example, and in the context of machine vision, these filters can be used to find features in an input image. The features can include, for example, edges, corners, endpoints, and so on. As the number of stacked convolutional operations increases, later convolutional operations can find higher-level features.
[00100] In a CNN, the classification portion may be a set of fully connected layers. The fully connected layers can be thought of as looking at all the input features of an image in order to generate a high-level classifier. Several stages (e.g., a series) of high-level classifiers eventually generate the desired classification output.
[00101] As can be discerned from this description, a CNN network is often composed of a number of convolutional operations (e.g., the convolution layers of the feature-extraction portion) followed by a number of fully connected layers (also called Dense layers) forming the classification portion. The number of operations of each type and their respective sizes are typically determined during the training phase of the machine learning. As a person skilled in the art recognizes, additional layers and/or operations can be included in each portion. For example, combinations of Pooling, MaxPooling, Dropout, Activation,
Normalization, BatchNormalization, and other operations can be grouped with convolution operations (i.e., in the features-extraction portion) and/or the fully connected operation (i.e., in the layers of the classification portion). A convolution operation can use a
SeparableConvolution2D or Convolution2D operation. For example, a convolution layer can be a group of operations starting with a Convolution2D or SeparableConvolution2D operation followed by zero or more operations (e.g., Pooling, Dropout, Activation,
Normalization, BatchNormalization, other operations, or a combination thereof), until another convolution layer, Dense layer, or the output of the CNN is reached.
[00102] Similarly, a Dense layer can be a group of operations or layers starting with a Dense operation (i.e., a fully connected layer) followed by zero or more operations (e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof) until another convolution layer, another Dense layer, or the output of the network is reached. The boundary between feature extraction based on convolutional networks and a feature classification using Dense operations can be marked by a Flatten
operation, which flattens the multidimensional matrix from the feature extraction into a vector.
[00103] Each of the convolution layers may consist of a set of filters. While a filter is applied to a subset of the input data at a time, the filter is applied across the full input, such as by sweeping over the input. The operations performed by this layer may be linear/matrix multiplications. An example of a convolution filter is described below. The output of the convolution filter may be further filtered using an activation function. The activation function may be a linear function or non-linear function (e.g., a sigmoid function, an arcTan function, a tanH function, a ReLu function, or the like).
[00104] Each of the fully connected operations is a linear operation in which every input is connected to every output by a weight. As such, a fully connected layer with N number of inputs and M outputs can have a total of NxM weights. A Dense operation may be generally followed by a non-linear activation function to generate an output of that layer.
[00105] Some CNN network architectures may include several feature extraction portions that extract features at different granularities (e.g., at different sub-block sizes of a superblock) and a flattening layer (which may be referred to as a concatenation layer) that receives the output(s) of the last convolution layer of each of the extraction portions. The flattening layer aggregates all the features extracted by the different feature extractions portions into one input set. The output of the flattening layer may be fed into (i.e., used as input to) the fully connected layers of the classification portion. As such, the number of parameters of the entire network may be dominated (e.g., defined, set) by the number of parameters at the interface between the feature extraction portion (i.e., the convolution layers) and the classification portion (i.e., the fully connected layers). That is, the number of parameters of the network is dominated by the parameters of the flattening layer. This is one disadvantage of the above-described architectures.
[00106] Other disadvantages of the CNN architectures described above, which include a flattening layer whose output is fed into fully connected layers, exist. First, models with fully connected layers tend to have a large number of parameters and operations. In some situations, the machine-learning model may include over 1 million parameters. Such large models may not be effectively or efficiently used, if at all, to infer classifications on devices (e.g., mobile devices) that may be constrained (e.g., computationally constrained, energy constrained, and/or memory constrained). That is, some devices may not have sufficient
computational capabilities (for example, in terms of speed) or memory storage (e.g., RAM) to handle (e.g., execute) such large models.
[00107] Second, the fully connected layers of such network architectures are said to have a global view of all the features that are extracted by the feature extraction portions. As such, the fully connected layers may, for example, lose a correlation between a feature and the location of the feature in the input image. As such, receptive fields of the convolution operations can become mixed by the fully connected layers. A receptive field can be defined as the region in the input space that a particular feature is looking at and/or is affected by. An example of a receptive field is described below. Presently, however, the problem wherein the receptive fields become mixed may be illustrated briefly with regards to FIG. 7
[00108] Namely, when a CNN as described above (e.g., a CNN that includes a flattening layer and fully connected layers) may be used to determine a partition of a block 700 of FIG. 7. The CNN may extract features corresponding to different regions and/or sub-block sizes of the block 700. As such, for example, features extracted from blocks 700-1, 700-2, 700-3, and 700-4 of the block 700 are flattened into one input vector to the fully connected layers. In inferring whether to partition the sub-block 700-2 into blocks 702-1, 702-2, 702-3, and 702-4, by the fully connected layers, features of at least one of the blocks 700-1, 700-3, and 700-4 may be used by the fully connected layers. As such, features of sub-blocks (e.g., the blocks 700-1, 700-3, and 700-4), which are unrelated to the sub-block 700-2 for which a partition decision is to be inferred, may be used in the inference. This may lead to erroneous inferences and/or inferences that are based on irrelevant information.
[00109] In contrast, the CNN architectures according to the present teachings herein analyze frames and blocks of frames in a way that the receptive field of a cascade of convolutional layers respect quad-tree boundaries. That is, when analyzing an image region, such as for determining a quad-tree partitioning, the receptive fields of any features extracted (e.g., calculated, inferred, etc.) for the image region are confined to the image region itself. This may be achieved by using an all-convolution network, and carefully designing the filter sizes of the convolutional operations to yield a matrix representation of the analysis region, whether a 64x64 region, a 32x32 region, or a 16x16 region, for example. Implementations according to this disclosure can ensure that machine-learning models (generated during training and used during inference) for determining, e.g., block partitioning, are not erroneously based on irrelevant or extraneous features, such as pixels from outside the image region.
[00110] One example of such a receptive-field-conforming convolution model for video coding is shown in FIG. 10. Specifically, FIG. 10 is a block diagram of an example of a convolutional neural network (CNN) 1000 for a mode decision according to implementations of this disclosure. The CNN 1000 can be used for determining a block partition of an image block. The block can be a superblock. For example, the CNN 1000 can be used to determine the block size used in the intra/inter-prediction stage 402 of FIG. 4. The partition can be a quad-tree partition, such as described with respect to FIG. 7. The CNN 1000 can be used to determine a partition for an intra-coded block or an inter-coded block of a frame, such as the frame 304 of FIG. 3. The CNN 1000 can be used by an encoder where the smallest possible block partition is an 8x8 partition. As such, determinations of whether to split a block need be made for blocks (i.e., sub-blocks of the superblock) that are 16x16 or larger.
[00111] As further described below, the number of parallel branches of the feature extraction portion of the CNN 1000 can be parameterizable (e.g., configurable). For example, in a configuration, only one branch (e.g., a linear branch) can be used. This is possible as long as the receptive field conformance property, as further described below, is maintained.
Except for the top and left rows of the block, the receptive field conformance property means that the receptive field boundary of the block does not cross the boundaries of the block.
[00112] A block 1002 (i.e., an image block) to be encoded is presented to the CNN 1000. The block 1002 can be a one color-plane block. As such, the block 1002 can be a luminance block. That the block is a one color-plane block is illustrated by the“xl” in“64x64x1” in FIG. 10. As mentioned, the block 1002 can be a superblock. While a superblock of size 64x64 is shown and used to describe the CNN 1000, the block 1002 can be of any size. For example, the block 1002 can be 128x128, 32x32, or any size block for which a, e.g., quad tree, partitioning has been determined by an encoder, such as an encoder 400 of FIG. 4. In another example, and since prediction modes can depend on adjacent (i.e., peripheral) pixels to a block that is to be partitioned, the block 1002 (i.e., the block that is used as input to the CNN 1000) can include pixels that are outside of the block for which a partitioning is to be determined. For example, if a partitioning of a 64x64 block is to be determined, then a block of size 65x65x1 can be used as input to the CNN 1000. That is, for example, the left and top neighboring pixels of the block for which a partitioning is to determined can be included in the input block to the CNN 1000. An example of such as configuration is described below.
[00113] One or more feature extraction layers 1003 form three branches as shown; namely a branch 1003-A, a branch 1003-B, and a branch 1003-C. The number of branches in the
feature extraction layer can be configurable to include more or fewer branches. Each of the branches can include one or more layers. At each layer, respective feature maps is extracted. In the description below, features maps, such as feature maps 1004, is referred to as having a dimension of AxBxC. For example, the feature maps 1004 is of size 8x8x256. This is to be interpreted as follows: the feature maps 1004 includes 256 feature maps and each of the 256 feature maps is of size 8x8 pixels (or features). In other words, the feature maps 1004 can be thought of as a set of 256 matrices where each matrix is of size 8x8. In one configuration of the CNN 1000, the feature extraction of each partition type can be separated, instead of sharing the feature extraction as in FIG. 10. Architectures using such a configuration are described in more detail below.
[00114] The number of features at a feature map can be configurable. For example, while the feature maps 1004 is shown to be 8x8x256, it can be 8x8xN, where N is any desired number of features. Other sizes are possible in different implementations, such as those described with respect to FIG. 13, below. In some examples, a feature compression rate can be applied to a machine-learning model to expand or reduce the number of features in the model. For example, the feature compression rate can be multiplied by all feature maps for feature expansion (or reduction).
[00115] The branch 1003-A extracts, in a first layer of the branch 1003-A, features corresponding to 8x8 blocks of the block 1002. The branch 1003-A convolves, with the block 1002, 256 filters (also referred to as kernels). FIG. 11 is an example 1100 of convolution operations according to implementations of this disclosure. The convolution operations can be used to generate any of the feature maps described herein.
[00116] The example 1100 includes a region 1102 of an image. The region 1102 is shown as a 6x6 region for the purposes of this example. However, it is to be understood that convolution filters can be applied to any size block, superblock, region of image, or an image.
[00117] A filter 1104 of size 3x3 is used in this example. However, filters can have different sizes. The example 1100 uses a non-overlapping convolution operation with a stride that is equal to the filter size. As such, the stride size, in each of the horizontal and vertical directions is 3. The filter 1104 is shown as including binary (i.e., zero and one) values.
However, the values of a filter can be any value (e.g., positive and/or negative real values).
As mentioned above, the values of a filter can be determined, by the machine-learning model, during the training phase of the machine-learning model. Feature map 1114 is the output of convolving the region 1102 and the filter 1104.
[00118] The filter 1104 is first convolved (e.g., using a matrix multiplication operation) with a sub-region 1106. As a result, a pixel 1116 of the feature map 1114 can be calculated as (0x0 + 9x1 + 3x0 + 9x0 + 4x1 + 8x1 + 5x0 + 1x0 + 2x1) = 23. The filter 1104 is then convolved with a sub-region 1108. As a result, a pixel 1118 can be calculated as (4x0 + 8x1 + 3x0 + 0x0 + 0x1 + 0x1 + 9x0 + 9x0 + 10x1) = 18. The filter 1104 is then convolved with a sub-region 1110. As a result, a pixel 1120 can be calculated as (9x0 + 5x1 + 1x0 + 5x0 +
9x1 + 3x1 + 8x0 + 3x0 + 6x1) = 23. The filter 1104 is then convolved with a sub-region 1112. As a result, a pixel 1122 can be calculated as (5x0 + lxl + 6x0 + 2x0 + 7x1 + 7x1 + 8x0 + 10x0 + 3x1) = 18.
[00119] Referring again to FIG. 10, the branch 1003-A convolves, with the block 1002, 256 filters, each having a size 8x8. A stride that is equal to the size of the filters (i.e., a stride that is equal to 8) is used. As a result, 256 feature maps (i.e., the feature maps 1004), each of size 8x8, are extracted. A filter of size 8 x 8 is defined by a kernel of the same size where each entry in the kernel can be a real number. In an example, the entries can be non-negative integers that are greater than 1. Filtering an 8 x8 block may thus be achieved by computing the inner product between the block and a filter kernel of the same size. In machine learning, filter kernels (i.e., the real numbers which constitute the values of the kernels) can be learned in the training process.
[00120] The branch 1003-B extracts 256 feature maps (i.e., feature maps 1008), each of size 8x8. The branch 1003-B first extracts, at a first layer of the branch 1003-B, feature maps 1006 by convolving the block 1002 with 128 filters, each of size 4x4, and using a stride of 4 (i.e., a stride that is equal to the filter size). At a second layer of the branch 1003-B, each of the 128 feature maps of the feature maps 1006 is convolved with two 2x2 filters, using a stride of 2, thereby resulting in the feature maps 1008.
[00121] The branch 1003-C extracts 256 feature maps (i.e., feature maps 1014), each of size 8x8. The branch 1003-C first extracts, at a first layer of the branch 1003-C, feature maps 1010 by convolving the block 1002 with 64 filters, each of size 2x2, and using a stride of 2. At a second layer of the branch 1003-B, each of the 64 feature maps of the feature maps 1010 is convolved with two 2x2 filters, using a stride of 2, thereby resulting in 128 feature maps (i.e., feature maps 1012). At a third layer of the branch 1003-C, each of the 128 features maps of the feature maps 1012 is convolved with two 2x2 filters, using a stride of 2, thereby resulting in the feature maps 1014.
[00122] It is to be noted that, each time a filter is applied to a unit (e.g., the region 1102 or a feature map), the unit is downsized (i.e., down-sampled), in each dimension, by the size of the filter.
[00123] The feature maps 1010 are feature maps of the 32x32 blocks of the block 1002. The feature maps 1006 are feature maps of the 16x16 blocks of the block 1002. The feature maps 1004 are feature maps of the 8x8 blocks of the block 1002. The feature maps 1008 normalize the feature maps 1006 to be, like the feature maps 1004, of size 8x8. Likewise, the feature maps 1012 followed by the feature maps 1014 normalize the feature maps 1010 to be, similarly, of size 8x8.
[00124] In an example, the feature maps can be normalized, via successive convolutions, to be feature maps of the smallest possible partition that can be used by the encoder. As such, the size 8x8 corresponding to the smallest possible partition type that can be used by the encoder when the CNN 1000 of FIG. 10 is used. Similarly, if the smallest possible partition were 4x4, then the feature extraction layers 1003 can normalize the feature maps to be of size 4x4. In an example, the feature extraction layers 1003 can include an additional branch and each of the branches would generate, via successive convolutions, feature maps that are each of size 4x4. In another example, the feature maps can be normalized to a size that does not necessarily correspond to the smallest partition size. For example, the features maps can be normalized to any size that is larger than or equal 8x8.
[00125] A concatenation layer 1016 receives the feature maps 1004, 1008, and 1014. Additionally, when the CNN 1000 is used to determine (e.g., infer, provide, etc.) a partition for the block 1002 that is to be intra-predicted, at least some samples of the neighboring blocks can also be used as input to the concatenation layer 1016. This is because intra prediction uses at least some samples (i.e., pixels) of neighboring blocks. While samples from the top neighboring block (indicated with TOP in FIG. 10) and samples from the left neighboring block (indicated with LEFT in FIG. 10) are shown for illustrative purposes, other neighboring blocks may be used, depending on the scan order used to process blocks of a video frame. For example, LEFT and TOP are used in the case of a raster scan order. In an implementation, all the samples of the top and left neighboring blocks are used as inputs to the concatenation layer 1016. However, and as mentioned above, samples of the top and left neighboring blocks can be included in the input block (e.g., the block 1002 of FIG. 10). Additionally, in a CNN that is used to determine other mode decision parameters (e.g., an
inter-prediction parameter or transform parameter), samples from neighboring blocks may or may not be used as inputs to the CNN.
[00126] In an implementation, and as a person skilled in the art appreciates, TOP can be a row of previously reconstructed pixels that are peripheral to the top edge of the block 1002; and LEFT can be a column of previously reconstructed pixels that are peripheral to the left edge of the block 1002. There can be up to 64 samples corresponding to TOP and up to 64 samples corresponding to LEFT. As mentioned above, one or both of TOP and LEFT can be added to the input block that is presented to the CNN, or can be presented separately to the CNN.
[00127] A non-linear function of a quantization parameter (QP, q, or Q) may optionally be used as an input to the CNN. A QP in video codecs can be used to control the tradeoff between rate and distortion. Usually, a larger QP means higher quantization (e.g., of transform coefficients) resulting in a lower rate but higher distortion; and a smaller QP means lower quantization resulting in a higher rate but a lower distortion.
[00128] The value of the QP can be fixed. For example, an encoder can use one QP value to encode all frames and/or all blocks of a video. In other examples, the QP can change, for example, from frame to frame. For example, in the case of a video conference application, the encoder can change the QP value based on fluctuations in network bandwidth.
[00129] As the QP can be used to control the tradeoff between rate and distortion, the QP can be used to calculate the RD cost associated with each combination of parameters in an exhaustive search as described above with regards to FIGS. 8 and 9. In an example, the QP can be used to derive a multiplier that is used to combine the rate and distortion values into one metric. Some codecs may refer to the multiplier as the Lagrange multiplier (denoted Amode ); other codecs may use a similar multiplier that is referred as rdmult. Each codec may have a different method of calculating the multiplier. Unless the context makes clear, the multiplier is referred to herein, regardless of the codec, as the Lagrange multiplier or Lagrange parameter.
[00130] To reiterate, the Lagrange multiplier can be used to evaluate the RD costs of competing modes (i.e., competing combinations of parameters). Specifically, let rm denote the rate (in bits) resulting from using a mode m and let dm denote the resulting distortion. The RD cost of selecting the mode m can be computed as a scalar value dm + mocLerm. By using the Lagrange parameter mode , it is then possible to compare the cost of two modes and
select one with the lower combined RD cost. This technique of evaluating RD cost is a basis of mode decision processes in at least some video codecs.
[00131] Different video codecs may use different techniques to compute the Lagrange multipliers from the QPs. This is due in part to the fact that the different codecs may have different meanings (e.g., definitions, semantics, etc.) for, and method of use of, QPs. For example, codecs that implement the H.264 standard may derive the Lagrange multiplier mocLe according to mode = 0.85 x 2^p_12^3. Codecs that implement the HEVC standard may use a formula that is similar. Codecs that implement the H.263 standard may derive the Lagrange multiplier Amode according to mode = 0.85 · QH2632· Codecs that implement the VP9 standard may derive the multiplier rdmult according to rdmult = 88 · q2/ 24.
Codecs that implement the AV 1 standard may derive the Lagrange multiplier mode according to mode = 0.12 · QAV1 2 / 256.
[00132] As can be seen in the above cases, the multiplier has a non-linear (e.g., exponential or quadratic) relationship to the QP. Note that the multipliers may undergo further changes before being used in the respective codecs to account for additional side information included in a compressed bitstream by the encoder. Examples of side information include picture type (e.g., intra vs. inter predicted frame), color components (e.g., luminance or chrominance), and/or region of interest. In an example, such additional changes can be linear changes to the multipliers.
[00133] Given the above, if the value of the QP itself is used as an input to a machine learning model, a disconnect may result between how the QP is used in evaluating the RD cost and how the QP is used in training machine-learning models. For codecs that use QP in the determination of RD cost, better performance can be achieved by using non-linear (e.g., exponential, quadratic, etc.) forms of the QPs as input to machine-learning models as compared to using linear (e.g., scalar) forms of the QPs. Better performance can mean smaller network size and/or better inference performance. In an example, the non-linear function can be approximated by piecewise linear segments.
[00134] As mentioned, QP may be used as an input to the CNN. In the example of FIG.
10, QP is used as an input to the concatenation layer 1016. More specifically, a quadratic function (i.e., QP2) is illustrated in FIG. 10. As described above, however, the function used depends on the codec; more specifically, the function used depends on the standard implemented by the codec. For example, a quadratic function may be used in the case of a
codec that implements H.263, VP9, or AV1; and an exponential function may be used in the case of a codec that implements H.264 or HEVC.
[00135] A total of 897 inputs can be received by the concatenation layer 1016. The inputs include 256 inputs of the feature maps 1004, 256 inputs of the feature maps 1008, 256 inputs feature maps 1014, 64 inputs at TOP, 64 inputs at LEFT, and the non-linear value of QP (such as QP2. In some implementations, a sample (i.e., a pixel) that is adjacent to the top-left comer of the block 1002 can also be used as an input to the concatenation layer 1016. In such a case, the concatenation layer 1016 receives 898 inputs.
[00136] The CNN 1000 includes three classifiers, namely classifiers 1018, 1020, and 1022. Each of the classifiers 1018, 1020, 1022 includes a set of classification layers and uses convolutions as further described below.
[00137] The classifier 1018 infers (i.e., outputs) partition decisions for sub-blocks of size 16x16 of the block 1002. It is noted that the block 1002 can be partitioned into 16 blocks (comprising 4x4 outputs), each block of size 16x16. As such, the classifier 1018 reduces, to a size of 4x4, the feature maps (which are each of size 8x8) received from the concatenation layer 1016.
[00138] First, feature maps 1019 are obtained from the feature maps received from the concatenation layer 1016 by applying non-overlapping convolutions using 2x2 separable convolution filters to combine some of the feature maps into one, thereby resulting in 256 feature maps, each of size 4x4.
[00139] Secondly, a series of lxl convolutions are applied, successively, to gradually reduce the feature dimension size to 1. As such, 1x1x128 convolutions (where the number of filters is 128) are applied to the feature maps 1019, resulting in 4x4x128 feature maps, to which 1x1x64 convolutions (where the number of filters is 64) are applied, resulting in 4x4x64 feature maps, to which 1x1x32 convolutions are applied resulting in 4x4x32 feature maps, to which a lxlxl convolution is applied resulting in a 4x4x1 feature map, namely the feature map 1025.
[00140] For each 16x16 sub-block of the block 1002, the classifier 1018 infers whether to split or not split the sub-block. As such, the classifier 1018 outputs 16 decisions
corresponding, respectively, to each of the 16x16 sub-blocks of the block 1002. The 16 decisions can be binary decisions. That is, the feature map 1025 can be thought of as a matrix of binary decisions. For example, a zero (0) can correspond to a decision not to split a sub block and a one (1) can correspond to a decision to split the sub-block. The order of the
output of the classifier 1018 can correspond to a raster scan order of the 16x16 sub-blocks of the block 1002. In another example, the decisions can correspond to probabilities (i.e., values that range from 0 to 1), or some other values, such as values that range from 0 to 100. When a decision is greater than a threshold that is appropriate for the range of the decision values (e.g., 0.9, 0.75, 90%, etc.), it can be considered to correspond to a binary decision of 1.
[00141] The classifier 1020 infers (i.e., outputs) partition decisions for sub-blocks of size 32x32 of the block 1002. The classifier 1020 receives the feature maps 1019 and convolves each of the feature maps with 2x2 separable convolution filters to combine feature maps of the feature maps 1019 into one, thereby resulting in feature maps 1021. It is noted that the block 1002 can be partitioned into 2x2 blocks, each of size 32x32. As such, the classifier 1020 reduces, to the size of 2x2, the feature maps 1019 (which are each of size 4x4) through a series of non-overlapping convolutions using lxl filters to gradually reduce the feature dimension size to 1, as described above with respect to the feature maps 1019, thereby resulting in a feature map 1027. For each 32x32 sub-block of the block 1002, the classifier 1020 infers whether to split or not split the sub-block. As such, the classifier 1020 outputs 4 decisions corresponding, respectively, to each of the 32x32 sub-blocks of the block 1002.
[00142] The classifier 1022 infers (i.e., outputs) partition decisions for the block 1002 itself. The classifier 1022 receives the feature maps 1021 and convolves each of the feature maps with a 2x2 separable convolution filter resulting in feature maps 1023, which combines some of the feature maps of the features maps 1021 into 1. It is noted that the block 1002 can be partitioned into only one lxl block of size 64x64. As such, the classifier 1022 reduces, to the size of lxl, the feature maps 1023 (which are each of size lxl) through a series of non overlapping convolutions using lxl filters to gradually reduce the feature dimension size to 1, as described above with respect to the feature maps 1019, thereby resulting in a feature map 1029. For the block 1002, the classifier 1022 infers whether to split or not split the block 1002. As such, the classifier 1022 outputs one decision corresponding to whether to split or not split the block 1002 into four 32x32 sub-blocks.
[00143] Separable convolution filters of size 2x2 are described to obtain the feature maps 1019, 1021, and 1023 (of the classifiers 1018, 1020, and 1022, respectively) in order to ultimately determine, for a block of size 64x64, 4x4 16x16 partitions (i.e., the feature map 1025), 2x2 32x32 partitions (i.e., the feature map 1027), and lxl 64x64 partition (i.e., the feature map 1029), respectively. However, in the general case, any convolutional filters of size 2k can be used as long the classifiers 1018, 1020, and 1022 determine, as described, 4x4
16x16 partitions (i.e., the feature map 1025), 2x2 32x32 partitions (i.e., the feature map 1027), and lxl 64x64 partition (i.e., the feature map 1029).
[00144] In the classifier 1018, the feature map 1025, which has a dimension of 4x4x1, is shown as being directly derived (i.e., there are no additional intervening convolution operations) from a feature maps 1034, which is of size 4x4x32. However, that need not be the case and any number of additional convolution operations can be used between the feature maps 1034 and the feature map 1025. This is illustrated by a dot-dashed line 1036. The same can be applicable to the classifiers 1020 and 1022 with respect to the feature map 1027 and the feature map 1029, respectively.
[00145] In an example, a parameter can be used as a configuration parameter (i.e., a threshold parameter) of the CNN. If the number of remaining features is less than or equal to the threshold parameter, then the number of features of the next layer can be set to 1. In the example of the CNN 1000 of FIG. 10, the threshold parameter is set to 32. Because the number of features of the feature maps 1034 is equal to the threshold parameter (i.e., 32), then the next layer corresponds to the layer that produces the feature map 1025, which has a feature dimension of 1. In an example, each of the classifiers can be configured with a different respective threshold parameter. In another example, all the classifiers can be configured to use the same threshold parameter.
[00146] In an example, the feature map dimensionality (i.e., the last dimension of a feature maps) within a classifier can be reduced using a feature reduction parameter F. For example, a classifier can reduce the number of channels according to the progression IncomingFeature , IncomingFeature/F, IncomingFeature/F2 , . . .,1, where IncomingFeature is the number of features that are initially received by the layer. In an example, each of the classifiers can be configured with a different respective feature reduction parameter. In another example, all the classifiers can be configured to use the same feature reduction parameter.
[00147] The classifier 1018 is now used to illustrate the threshold parameter and the feature reduction parameter. With respect to the classifier 1018, IncomingFeature is 256 (as illustrated by the features maps 1019, which is of size 4x4x256), the feature reduction parameter F is 2, and the threshold parameter is 32. As such, the classifier 1018 reduces the number of channels according to the progression 256, 256/2, 256/22, 256/23, and 1. That is, the classifier 1018 reduces the number of channels according to the progression 256, 128, 64, 32, and 1. The classifier 1018 does not include a layer where the number of channels is
256/24 (i.e., 16) because the threshold parameter 32 for the number of channels is reached at the progression 256/23 (i.e., 32).
[00148] The CNN 1000 can be extended to infer partition decisions for other block sizes. For example, an encoder may allow the smallest partition to be of size 4x4. As such, to infer partition decisions for sub-blocks of size 8x8, a branch can be added to the feature extraction layers 1003 such that each branch of the feature extraction layers 1003 can generate feature maps, each of size 4x4, as inputs to the concatenation layer 1016. Additionally, a classifier can be added between the concatenation layer 1016 and the classifier 1018. The added classifier infers (i.e., outputs) partition decisions for sub-blocks of size 8x8 of the block 1002. It is noted that the block 1002 can be partitioned into 8x8 sub-blocks, each of size 8x8. The added classifier reduces, to a size of 8x8x1, the feature maps received from the concatenation layer 1016 through a series of non-overlapping convolutions using 2x2 filters in this example.
[00149] The CNN 1000 can be configured to infer partition decisions of a 128x128 block. For the 128x128 block, a CNN can be configured to include classifiers that determine, respectively, one (i.e., a lxl output matrix) 128x128 decision (i.e., one decision
corresponding to whether the block is or is not to be split), four (i.e., a 2x2 output matrix) 64x64 decisions, 16 (i.e., a 4x4 output matrix) 32x32 decisions, and 64 (i.e., a 8x8 output matrix) 16x16 decisions.
[00150] In some implementations, the CNN 1000 can include early termination features. For example, if the classifier 1022 infers that the block 1002 is not to be split, then processing through the classifiers 1020 and 1018 need not be continued. Similarly, if the classifier 1020 infers that none of the 32x32 sub-blocks of the block 1002 are to be split, then processing through the classifier 1020 need not be continued.
[00151] The CNN 1000 is one example of an all convolution architecture that is receptive field conformant. Receptive field conformance may be further explained with reference to FIG. 12. FIG. 12 is an example 1200 of receptive fields according to implementations of this disclosure. The example 1200 includes an input 1202. The example 1200 and the explanation herein are adapted from Dang Ha The Hien,“A guide to receptive field arithmetic for Convolutional Neural Networks,” April 2017, [retrieved on August 06, 2018]. Retrieved from the Internet <URL: https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for- convolutional-neural-networks-e0f5l4068807>.
[00152] The input 1202 can be a portion of an image for which it is desirable to extract features (e.g., a feature map). The input 1202 can be, for example, the block 700, one of the
blocks 700-1, 700-2, 700-3, and 700-4, or one of the blocks 702-1, 702-2, 702-3, and 702-4. The input 1202 is shown as having a size of 5x5 pixels. However, the size of the input 1202 is not pertinent to the description of the concept of a receptive field.
[00153] The example 1200 illustrates convolution operations that use filters/kernels of size k = 3 x 3 (also referred to as a kernel size of 3), a padding size p = l x l, and a stride s = 2 x 2 (also referred to as a stride value of 2). An example of a filter of size k = 3 x 3 with a stride value of 3 is illustrated with respect to the filter 1104 of FIG. 11. Padding defines how the border of a sample is handled during a convolution. Further descriptions of the concepts of padding, stride, and kernel (i.e., filter) size are omitted herein as such concepts are well- known to a person skilled in the art.
[00154] The example 1200 illustrates a first feature map 1204 that is the result of convolving the input 1202 with a first filter and a second feature map 1206 that is the result of convolving the first feature map with a second filter. The first filter and the second filter can have different values. In machine learning, the values of the filters can be determined (e.g., learned) during the training phase.
[00155] A pixel 1208 (which may also be referred to as a feature) of the first feature map 1204 results from the convolution of pixels of the input 1202. Such pixels are the receptive field of the pixel 1208. Note that because the convolution uses padding, some of the pixels (e.g., the padding pixels) used for generating the pixel 1208 are outside of the input. The receptive field of the pixel 1208 is defined by a square whose comers are marked by black squares, such as a black square 1213. Dashed lines, such as a dashed line 1212, emanating from the corners of the pixel 1208 also illustrate the receptive field of the pixel 1208. The end points of the dashed lines are the black squares.
[00156] A pixel 1210 (which may also be referred to as a feature) of the second feature map 1206 results from the convolution of pixels of the first feature map 1204. Such pixels are the receptive field of the pixel 1210 in the first feature map 1204 and can be further projected onto the input 1202 to determine receptive field in the input 1202. Note that because the convolution uses padding, some of the pixels (e.g., the padding pixels) used for generating the pixel 1210 are outside of the first feature map 1204. The padding pixels of the first feature map 1204 are not shown so as not to clutter FIG. 12. The receptive field of the pixel 1210 in the input 1202 is defined by a square whose comers are marked by black circles, such as a black circle 1215. Dot-dashed lines, such as a dot-dashed line 1214, emanating from the
corners of the pixel 1210 also illustrate the receptive field in the input 1202 of the pixel 1210. The end points of the dot-dashed lines are the black circles.
[00157] The receptive field can play an important role in image analysis during video encoding. The receptive field of a series of convolution layers can be interpreted as the “region” of the image (e.g., a block, a superblock, a frame, or any other portion of an image) that each pixel (e.g., a feature)“sees” (e.g., influenced by, summarizes, etc.) when computing the pixel. Pixels at the initial input layer (e.g., the input 1202) become features (via a series of convolutions) for later layers (e.g., the second layer, which includes the second feature map 1206) of a CNN that will aid the CNN to analyze the initial input layer.
[00158] When using a CNN to analyze a model for determining a mode decision (e.g., a partitioning using quad-tree representations), it is desirable that each analysis region becomes confined to the boundaries of its quad-tree representation. That is, for example, it is desirable that features describing a region of an image, and which are used for inferring a mode decision of the region of the image, do not mix pixels from other regions of the image. For example, and referring again to FIG. 7, features describing the blocks 700-2 and/or the blocks 702-1, 702-2, 702-3, and 702-4 desirably do not include, in their respective fields, pixels from any of the blocks 700-1, 700-3, or 700-4.
[00159] The following four equations can be used to calculate the receptive field in each layer of a CNN.
[00160] In the above, nout is the number of output features in a layer. In the example 1200, a first layer corresponds to (e.g., includes) the first feature map 1204 and a second layer corresponds to (e.g., includes) the second feature map 1206. Further, nin is the number of input features to the layer. For example, the number of input features to the second layer is the number of features in the first feature map 1204, namely 9. The variables k, p, and s (collectively referred to as the convolution properties) are, respectively, the convolution kernel size, the convolution padding size, and the convolution stride size/value.
[00161] Equation (1) calculates the number of output features of a layer based on the number of input features and the convolution properties. Equation (2) calculates a distance
(i.e., a jump jout) between two adjacent features in the output feature map. Equation (3) calculates the receptive field size (i.e., rout) of the output feature map, which is define as the area that is covered by k input features and the extra area that is covered by the receptive field of the input feature that is on the border. Equation (4) calculates the center position (i.e., startout) of the receptive field of the first output feature (e.g., the pixel 1208 and the pixel 1210 correspond, respectively, to the first output feature in the first feature map 1204 and the second feature map 1206).
[00162] Having described the concept of receptive fields in more detail, characteristics shared by receptive field conforming CNN architectures according to the teachings herein are next described, along with variations.
[00163] As mentioned briefly above, the CNN architectures described herein are all- convolutional networks. That is, the feature extraction and the classification layers use convolution operations. In the feature extraction layers, non-overlapping convolution operations are performed on the input at each layer. At at least some of the feature extraction layers, the non-overlapping convolution operations are performed by setting the stride value the same as the kernel size. At least some of the kernel sizes can be even numbers (i.e., multiples of 2).
[00164] For example, in FIG. 10, each convolution layer in the feature extraction layers uses a 2x2 kernel, with the stride of 2. This non-overlapping convolution operation ensures that, at the last feature extraction layer (i.e., immediately before the concatenation layer 1016), each one of the NxN channels (i.e., feature maps of size NxN), where N=8 in the example of FIG. 10, only sees (e.g., uses, is affected by, etc.) information from its corresponding sub-block of size (64/N)x(64/N), where 64x64, in the example of FIG. 10, corresponds to the size of the block 1002 (i.e., the input block). In an example, N can be 2k.
[00165] In the classification layers, instead of fully connected layers, convolution reduction with lxl kernels is performed until the number of desired outputs is reached. Convolutional layers are used in the classification layers. As such, the receptive fields are respected (e.g., preserved).
[00166] For example, in the CNN 1000, in order to infer partition decisions (i.e., by the classifier 1018) for sub-blocks of size 16x16 (i.e., as the feature map 1025, which is the output of the classifier 1018), non-overlapping convolution operations (i.e., between the concatenation layer 1016 and the first layer of the classifiers 1018) with a kernel size 2 are performed to reduce the number of channels from 8x8 (i.e., the size of each of the feature
maps of the concatenation layer 1016 as described above) to 4x4 (i.e., the size of each of the feature maps 1019), and from then on apply kernel size lxl and gradually reduce the feature dimension size to 1 (i.e., the feature map 1025, which is of size 4x4x1). The output of the last classification layer is 4x4x1, which is the partition determination of the 16 sub-blocks of the input 1002. Each of the 16 sub-blocks is of size 16x16 pixels.
[00167] Similarly, the partition decision for each of the 32x32 sub-blocks can be inferred by the classifiers 1020; and the partition of the 64x64 block can be inferred by the classifiers 1022.
[00168] As a person skilled in the art recognizes, a kernel of size lxl can be used to reduce the dimensionality of feature maps. For example, an input of size 4x4x32 (32 features maps, each of size 4x4), when convolved with one filter of size lxl would result in a feature map of size of 4x4x1. As such, a kernel of size lxl can be used to pool (e.g., combine) information from multiple feature maps. Further, a kernel of size lxl, as used herein, does not mix values from different locations of the input. That is, continuing with the example above, when determining the value at location (x, y) of the feature map of size 4x4x1, only the 32 values at the location (x, y) of the each of the 32 maps of the feature maps of size 4x4x32 are used. As such, by using lxl convolutions, the receptive fields can be preserved (e.g., respected).
[00169] The inventive CNN architectures described herein include the combination of using non-overlapping kernel sizes with an all-convolutional network (for feature extraction and for classification) that respects receptive fields. This combination allows for a flexible CNN architecture of reduced size over using fully connected layers for classification. Further, the CNN architectures described herein can improve inferences over alternative structures.
[00170] As mentioned, the CNN architecture described herein is flexible in that the number of layers, the number of feature extraction branches, and the number of features used in each layer are all configurable. This allows for deployment flexibility of the model architecture for different application constraints. For example, a larger model such as the model of FIG. 10, can be used for applications with higher accuracy requirements and lower power/hardware footprint constraint. A smaller model, such as the examples described below with regards to FIGS. 13-16, can be used when accuracy is less important and/or a higher power/hardware footprint constraint exists. Whether using a relatively small or a relatively large model, a concatenation layer, such as the concatenation layer 1016, is optional.
[00171] FIG. 13 is a block diagram of a second example of a CNN 1300 for a mode decision according to implementations of this disclosure. Like the CNN 1000, the CNN 1300 includes inputs 1302, feature extraction layers 1304, and classifiers 1306. Among other differences, the CNN 1300 of FIG. 13 has fewer feature extraction layers, fewer classification layers in the classifiers, and fewer number of features in each layer than the CNN 1000.
[00172] In general, for a CNN herein, where an input has a size 64x64x1, partition decisions for 16x16 sub-blocks (i.e., a feature map of size 4x4x1 at the output) means that the multiplication of the kernels/strides can be at most 16. Partition decisions for 32x32 sub blocks (i.e., a feature map of size 2x2x1 at the output) means that the multiplication of the kernels/strides can be at most 32. Finally, partition decisions for the 64x64 block (i.e., a feature map of size lxlxl at the output) means that the multiplication of the kemels/strides can be at most 64. This general principle in a receptive field conformant CNN can be seen with reference to FIG. 10. Namely, given the input 1002, where the end is the feature map 1025, the feature map 1027, or the feature map 1029, the multiplication where the kernel size equals the stride value has at most 64/4, 64/2, or 64/1, or respectively 16, 32, or 64 for partitions of 16x16, 32x32, or 64x64. Similar determinations may be used for partitioning 128x128, 8x8, or 4x4, for example, using 128x128, 8x8, or 4x4 inputs.
[00173] This general principle may also be applied where, like in FIG. 13, the inputs 1302 include a (e.g., luma) input block having a size 65x65x1, and the output comprises partition decisions for 16x16 sub-blocks (i.e., a feature map of size 4x4x1 at the output), partition decisions for 32x32 sub-blocks (i.e., a feature map of size 2x2x1 at the output), and partition decisions for the 64x64 block (i.e., a feature map of size lxlxl at the output). As described above with regards to FIG. 10, if a partitioning of a 64x64 block is to be determined, then a block of size 65x65x1 can be used as input to the CNN 1300. That is, for example, the (e.g., left and top) neighboring pixels of the block for which a partitioning is to determined can be included in the input block to the CNN 1300. In such a case, and in order to preserve the receptive field property as described above, a first filter (e.g., a first filter in each branch of the feature extraction layers) can be of size 2k +1 and the stride can be 2k. Stated more specifically, the multiplication of the kemels/strides as shown in FIG. 13 (and in the variations described below) is such that the initial feature extraction layer 1310-0, 1312-0, 1314-0 in each branch 1308-A, 1308-B, 1308-C of the feature classification layers 1304 has a kernel size of 2k +1 and a stride value of 2k· Successive layers have kernel sizes equal to stride values such that 64 / (2k * product of remaining strides) = 4, 2, or 1. The same
determinations may be used for partitioning 128x128, 8x8, or 4x4 blocks, for example, using 129x129, 9x9, or 5x5 inputs.
[00174] FIG. 13 shows that an image block to be encoded is presented as inputs 1302 to the CNN 1300. The image block can be a one color-plane block, such as the luminance block 65x65x1. While a superblock of size 64x64 is used to describe the CNN 1300, the block can be of any size.
[00175] The feature extraction layers 1304 form three branches 1308-A, 1308-B, 1308-C. The number of branches in the feature extraction layer can be configurable to include more or fewer branches. Each of the branches can include one or more of the feature extraction layers 1304. At each layer, respective feature maps is extracted. In this example, the feature extraction layers comprise a number or cardinality of branches equal to a number or cardinality of possible quad-tree partition decisions for the block, but this is not required. In any event, each of the branches comprises at least one of the feature extraction layers.
[00176] The branch 1308-A extracts, in a first or initial layer 1310-0 of the branch 1308-A, features corresponding to 8x8 blocks of the block. The branch 1308-A convolves k filters with the block, each having a size 8x8. A stride that is equal to 2k + 1 is used. As a result, 16 feature maps, each of size 8x8, are extracted. The same convolution operation is performed at each first layer 1312-0, 1314-0 of the branches 1308-B, 1308-C.
[00177] At a second layer 1310-1 of the branch 1308-A, each of the 16 feature maps of the initial layer 1310-0 is convolved using filters having a size equal to a stride value to extract 32 feature maps, each of size 4x4. At a third layer 1310-2 of the branch 1308-A, each of the 32 feature maps of the second layer 1310-1 is convolved using filters having a size equal to a stride value to extract 32 feature maps, each of size 2x2. Finally, at a fourth layer 1310-3 of the branch 1308-A, each of the 32 feature maps of the third layer 1310-2 is convolved using filters having a size equal to a stride value to extract 64 feature maps, each of size lxl.
[00178] At a second layer 1312-1 of the branch 1308-B, each of the 16 feature maps of the initial layer 1312-0 is convolved using filters having a size equal to a stride value to extract 32 feature maps, each of size 4x4. At a third layer 1312-2 of the branch 1308-B, each of the 32 feature maps of the second layer 1312-1 is convolved using filters having a size equal to a stride value to extract 64 feature maps, each of size 2x2.
[00179] At a second layer 1314-1 of the branch 1308-C, each of the 16 feature maps of the initial layer 1314-0 is convolved using filters having a size equal to a stride value to extract 64 feature maps, each of size 4x4.
[00180] The CNN 1300 includes three classifiers, namely classifiers 1320-A, 1320-B, and 1320-C. Each of the classifiers 1320- A, 1320-B, 1320-C includes one or more classification layers and uses convolutions as further described below. In this example, the multiple classifiers comprise a respective classifier corresponding to a respective branch of the branches of the feature extraction layers. The feature maps received by the initial
classification layer of a respective classifier 1320-A, 1320-B, and 1320-C may be configured to infer the partition decisions for sub-blocks of size (aS)x(aS) of the block comprising a convolution of N feature maps having respective feature dimension (N/2P)x(N/2P), wherein b is an integer and b=0, . . ., (the number of branches - 1).
[00181] In the example of FIG. 13, each of the final layers of the feature extraction layers 1304 before the classifiers 1306, namely the fourth layer 1310-3 of the first branch 1308-A, the third layer 1312-1 of the second branch 1308-B, and the second layer 1314-1 of the third branch, include N feature maps. The dimension of the N feature maps in each branch is equal to the number of possible mode decisions (e.g., the quad-tree partitions) for the block size associated with the corresponding classifier of the classifiers 1306. That is, the classifier 1320-A receives 64 feature maps of size lxl from the layer 1310-3, and the classifier 1320-A infers (i.e., outputs) one partition decision for the 64x64 input block. The classifier 1320-B receives 64 feature maps of size 2x2 from the layer 1312-2, and the classifier 1320-B infers four partition decisions, one for each of the 32x32 sub-blocks. The classifier 1320-C receives 64 feature maps of size 4x4 from the layer 1314-1, and the classifier 1320-C infers sixteen partition decisions, one for each of the 16x16 sub-blocks. The decisions can be binary decisions. That is, the feature maps output from each of the classifiers 1320-A, 1320-B, 1320- C can be thought of as a matrix of binary decisions. For example, a zero (0) can correspond to a decision not to split a sub-block and a one (1) can correspond to a decision to split the sub block. The order of the output of a classifier can correspond to a raster scan order of the sub blocks of the input block. In another example, the decisions can correspond to probabilities (i.e., values that range from 0 to 1), or some other values, such as values that range from 0 to 100. When a decision is greater than a threshold that is appropriate for the range of the decision values (e.g., 0.9, 0.75, 90%, etc.), it can be considered to correspond to a binary decision of 1.
[00182] At the classifier 1320-A, a series of convolutions, at least one of which is a lxl convolution, is applied, successively, to gradually reduce the feature dimension size to 1. At the first or initial classification layer 1322-0 of the classifier 1320-A, convolutions are
applied to the 64 feature maps from the extraction layer 1310-3, resulting in 1x1x24 feature maps, to which convolutions are applied at the second classification layer 1322-1 of the classifier 1320-A, resulting in 1x1x8 feature maps, to which convolutions are applied at the third classification layer 1322-2 of the classifier 1320-A, resulting in a final lxlxl feature map.
[00183] At the classifier 1320-B, a series of convolutions, at least one of which is a lxl convolution, is applied, successively, to gradually reduce the feature dimension size to 1. At the first or initial classification layer 1324-0 of the classifier 1320-B, convolutions are applied to the 64 feature maps from the extraction layer 1312-2, resulting in 2x2x24 feature maps, to which convolutions are applied at the second classification layer 1324-1 of the classifier 1320-B, resulting in 2x2x8 feature maps, to which convolutions are applied at the third classification layer 1324-2 of the classifier 1320-B, resulting in a final 2x2x1 feature map.
[00184] At the classifier 1320-C, a series of convolutions, at least one of which is a lxl convolution, is applied, successively, to gradually reduce the feature dimension size to 1. At the first or initial classification layer 1326-0 of the classifier 1320-C, convolutions are applied to the 64 feature maps from the extraction layer 1312-2, resulting in 4x4x24 feature maps, to which convolutions are applied at the second classification layer 1326-1 of the classifier 1320-C, resulting in 4x4x8 feature maps, to which convolutions are applied at the third classification layer 1326-2 of the classifier 1320-C, resulting in a final 4x4x1 feature map.
[00185] In each of the classifiers 1306, (e.g., non-overlapping) convolutional filters of any size can be used as long the classifiers 1306 determine, as described, 4x4 16x16 partitions, 2x2 32x32 partitions, and lxl 64x64 partition.
[00186] Although not expressly shown in FIG. 13, a QP may be used as input to one or more of the feature extraction layers 1304 of the CNN 1300. The QP can be input as a non linear function f(QP). For example, the input function can be QP2 or QPn. The variable n may be a real number. The variable n may be a non- zero (e.g., positive) integer with an absolute value greater than 1.
[00187] FIG. 13 shows a CNN 1300 where inputs 1302 extend from luminance-only (luma) data of the block to luma and chrominance (chroma) data of the block. For example, where the luma input has dimensions of 64x64, two chroma channels with a half resolution in each channel can be included with the inputs. The chroma channels may input in dimensions 32x32x2 in an arrangement such as that in FIG. 10. In this case, the first kernel size may equal the stride value. In FIG. 13, the chroma data is included in the inputs 1302 with
dimensions of 33x33x2 by including the neighboring pixels (one row and one column in this example). In this example, the first kernel is 2k +1 with a stride value of 2k as described with respect to the luma channel. In either implementation, the kernel and stride relationships are those described with regards to the luma channel to obtain 16-4x4 partitions, except that the multiplication of kemels/strides can be at most 8 because 32/4 is equal to 8.
[00188] More specifically, and referring to FIG. 13, the branch 1308-A extracts, in a second kernel of the initial layer 1310-0, features corresponding to 4x4 blocks of the chroma blocks. The branch 1308-A convolves k filters with the chroma blocks, each having a size 4x4. A stride that is equal to 2k + 1 is used. As a result, 8 feature maps, each of size 4x4, are extracted. The same convolution operation is performed at each first layer 1312-0, 1314-0 of the branches 1308-B, 1308-C. Then, each of the 8 feature maps having the size 4x4 is convolved with the feature maps corresponding to the luma block for the additional feature extraction layers 1304.
[00189] The inclusion of chroma data is optional but may improve the inference performance of a CNN over using luma data alone. For example, in some videos, content may not be well captured in the luma channel. By including the additional data in chroma channels, additional features are available for feature extraction, such as in the feature extraction layers 1304, improving inference accuracy at the classifiers, such as the classifiers 1306.
[00190] Although FIG. 13 and its variations refer to luminance and chrominance data in the YUV color space, the CNN architectures described herein may be used with other color spaces, such as RGB or LUV, for example.
[00191] In these implementations, one column and one row of pixels is used for the (e.g., left and top) neighboring pixels in each of the luma and chroma inputs. In some
implementations, more than one row/column represented by the value v may be used.
Accordingly, the input would be (64 + v)x(64 + v)xl for a 64x64 luma block and (32 + v)x(32 + v)x2 for the two 32x32 chroma blocks, and the first kernel has a kernel size of 2k + v and a stride value of 2k. It is worth noting that these examples of luma and chroma block sizes refer to 4:2:0 format. If another format is used, the relative block sizes between the luma and chroma samples will differ from these examples.
[00192] In addition to adjusting the number of features on each convolution layer, the architecture of FIGS. 10 and 13, and their variations described herein can be designed so that a global feature adjustment rate r may be applied across all layers for reducing/expanding the
number of features. This feature adjustment rate can be a configurable hyper-parameter. After applying the feature adjustment rate, the number of features can be reduced/expended from n to n*r.
[00193] The flexible architecture described herein allows different classifiers to have distinct feature extraction as in the CNNs 1000, 1300, or share one or more feature extraction layers, such as shown in the example of FIG. 14. FIG. 14 is a block diagram of a third example of a CNN 1400 for a mode decision according to implementations of this disclosure. The CNN 1400 of FIG. 14 includes feature extraction layers 1404 and classifiers 1406. The feature extraction layers 1404 include one or more common feature extraction layers 1408 and additional feature extraction layers 1410, 1412, 1414. Specifically, the common feature extraction layers 1408 provide features maps to teach of additional 64x64 feature extraction layers 1410, additional 32x32 feature extraction layers 1412, and additional 16x16 feature extraction layers 1414. In this way, the common feature extraction layers 1408 are shared by one or more of the classifiers 1406. In the CNN 1400, the common feature extraction layers 1408 are shared by each of the 64x64 classifier layers 1416, the 32x32 classifier layers 1418, and the 16x16 classifier layers 1420. An advantage of not sharing common feature extraction layers is that such a CNN can maximize the inference accuracy through the design of specific extraction layers for each classifier. However, sharing common feature extraction layers between different classifiers has an advantage in that the sharing can effectively reduce model size. Sharing common feature extraction layers may be seen in additional detail in the example of FIG. 15.
[00194] FIG. 15 is a block diagram of a fourth example of a CNN 1500 for a mode decision according to implementations of this disclosure. The CNN 1500 of FIG. 15 modifies the architecture of the CNN 1300 by forcing the three classifiers 1306 to share some feature extraction layers. The classifiers 1306 (i.e., the layers of the classifiers 1306) in the CNN 1500 are unchanged from those in the CNN 1300 of FIG. 13. The inputs 1502 comprise luma data in the form of input 65x65x1 and chroma data in the form of inputs 33x33x2. Each of the three classifiers 1320-A, 1320-B, 1320-C share first/initial feature extraction layers 1510, 1512 of sizes 8x8x16 and 4x4x8, respectively. Subsequent layers branch therefrom. The classifiers 1320-A, 1320-B for 64x64 and 32x32 blocks, respectively, share a subsequent feature extraction layer 1514 having a size 4x4x32. As indicated more generally in FIG. 13, the CNN 1500 also includes additional feature extraction layers that are specific to each of the classifiers 1306. For example, the classifier 1320-C for 16x16 blocks receives input
through an additional feature extraction layer 1516 having a size 4x4x64. Similarly, the classifier 1320-B for 32x32 blocks receives input through an additional feature extraction layer 1518 having a size 2x2x64. The classifier 1320-A for 64x64 blocks receives input through two additional feature extraction layers 1520 and 1522, respectively having sizes 2x2x32 and 1x1x64. As mentioned, sharing common feature extraction layers can reduce model size. In addition, sharing common feature extraction layers can increase model robustness to noise.
[00195] The mode decision inferred by the CNNs described above for a block (e.g., a superblock) is the block partitioning. However, existing video codecs also use brute force approaches to decide the optimal prediction modes and transform unit sizes for compression of the block. These mode decisions are similar to the mode decision of block partitioning in the sense that they all use features based on the raw image content and a quantization value of the block. Further, the same receptive-field-conforming principle should be observed for the decisions of prediction mode (PM) and transform unit (TU) size and/or type on the block/sub block level. Accordingly, the teachings herein may be extended to include these mode decisions.
[00196] FIG. 16 is a block diagram of a fifth example of a CNN 1600 for a mode decision according to implementations of this disclosure. The CNN 1600 extends the architecture previously described to infer prediction (e.g., intra prediction) modes and transform unit sizes. As can be seen in FIG. 16, the classifiers for PM and TU at each sub-block size can share common feature extraction layers 1602 with the classifiers for the partition decisions, such as those described with regard to FIGS. 10 and 13. More specifically, the CNN 1600 indicates that the classifiers for each of the partition decisions for 64x64 blocks, 32x32 blocks, and 16x16 blocks comprises separate classifier layers 0 to m, where m is a positive integer. The layers conform to the rules described above with regards to the multiplication of the kernel sizes and stride values, and to the receptive field conformance. In this example, the classifiers for PM and TU at each sub-block size also share the first/initial classification layers 1604, 1606, 1608, respectively with its corresponding block partition classifiers. The initial classification layers 1604, 1606, 1608 provide respective inputs (e.g., feature maps) to an extra classifier for the PM and TU at each sub-block size, respectively classifiers 1610, 1612, 1614. The classifiers 1610, 1612, 1614 may each comprise one or more classifier layers. This arrangement may be desirable because block partitioning and TU/PM
determination can use similar image features for inferring decisions. More desirably, mode
decisions for TU/PM have their own classification layers to implement classification rules that are unique to those mode decisions.
[00197] For simplicity, the CNNs 1000, 1300, 1400, 1500, 1600 are described for determining a partitioning of a 64x64 block from a 64x64 partition down to whether each 16x16 sub-block should be further partitioned into 8x8 blocks. However, the disclosure herein is not so limited. A CNN architecture according to implementations of the disclosure can be generalized as follows.
[00198] A CNN for determining a mode decision in video coding, where the block is of size NxN (e.g., 64x64, 128x128) and where a smallest partition determined by the CNN is of size SxS (e.g., 4x4, 8x8), can include feature extraction layers, optionally a concatenation layer, and classifiers. The classifiers include all-convolutional layers. Other values of N and S can be possible. In some examples, N can be 32, 64, or 128, and S can be 4, 8, or 16.
[00199] When a concatenation layer is included as in the CNN 1000, the layer may receive, from the feature extraction layers, feature maps of the block. Each feature map may be of a defined size, such as 8x8.
[00200] Whether or not a concatenation layer is included, each of the classifiers includes one or more classification layers. Each classification layer receives feature maps having a respective feature dimension. For example, and referring to FIG. 10, the classifier 1018 includes 5 classification layers (illustrated by the 5 squares representing the feature maps of each layer), the classifier 1020 includes 4 classification layers, and the classifier 1022 includes 3 classification layers. For example, and referring to FIGS. 13 and 15, each of the classifiers 1320-A, 1320-B, 1320-C includes 3 classification layers (illustrated by the 3 squares representing the feature maps of each layer).
[00201] Each of the classifiers can be configured to infer a partition decision for sub blocks of a specific size. That is, a classifier can be configured to infer partition decisions for sub-blocks of size (aS)x(aS) of the block, where a is a power of 2 and a=2, . . ., N/S. As such, when N=64 and S=8, a can have any of the values 2, 4, and 8. For example, with respect to the classifiers 1018, 1320-C, a=2 and the classifiers 1018, 1320-C infer partition decisions for blocks of size (2x8)x(2x8)=l6xl6; with respect to the classifiers 1020, 1320-B, a=4 and the classifiers 1020, 1320-B infer partition decisions for blocks of size
(4x8)x(4x8)=32x32; and with respect to the classifiers 1022, 1320-A, a=8 and the classifiers 1022, 1320-A infer partition decisions for blocks of size (8x8)x(8x8)=64x64.
[00202] A classifier can infer partition decisions for sub-blocks of size (aS)x(aS) of the block by instructions that include applying, at each successive classification layer of the classification layers, a kernel of size lxl to reduce the respective feature dimension at least in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map. Using the classifier 1022 as an example where a=8, the classifier 1022 convolves the feature maps 1023 with 32 kernels, each of size lxl, thereby resulting in feature maps 1031, which have dimensions of 1x1x32. The feature map 1029 (which is of size N/(aS)xN/(aS)xl=64/(8x8)x64/(8x8)xl=lxlxl) corresponds to the decision whether the block of size NxN should be split or not. As can be seen from FIGS. 13 and 15, a classifier may convolve the feature maps of a previous layer with a different number of kernels to reduce the respective feature dimension by more than half (here, by 23).
[00203] In the example of FIG. 10, a first classifier 1018, can receive the first feature maps as an output of a final feature extraction layer of the feature extraction layers through the concatenation layer (e.g., the concatenation layer 1016), wherein a first non-overlapping convolution operation using a first 2x2 kernel is applied to reduce the first feature maps to a size of (S/2)x(S/2). For example, as described with respect to the classifier 1018, the first layer of the classifier 1018 receives the 8x8 feature maps from the concatenation layer 1016 and reduces them to the size of 4x4 (i.e., the feature maps 1019). In the example of the classifier 1018, the feature maps 1019 is shown as having a dimension of 256. However, that need not be the case so long as the dimension of the last layer of each of the classifiers is N/(aS)xN/(aS)xl in this example. The CNN may also include a second classifier 1022 that infers partition decisions for sub-blocks of size (b8)c(b8). In an example, b=8. The second classifier can receive third feature maps, each of size MxM, from a third classifier. The third classifier can be the classifier 1020. As such, M=2 and the third feature maps can be the features maps 1021. The second classifier can apply a second non-overlapping convolution operation using a second 2x2 kernel to reduce the third feature maps to a size of
(M/2)x(M/2). For example, the classifier 1022 receives the features maps 1021 from the classifier 1020 and applies a second non-overlapping convolution operation using a second 2x2 kernel to generate the feature maps 1023.
[00204] In contrast, in the CNN 1300 and its variations that do not include a concatenation layer, an initial classification layer of each classifier receives the feature maps as an output of a final feature extraction layer of the feature extraction layers, directly from the final feature extraction layer.
[00205] The feature maps received at an initial classification layer include specific feature dimensionalities for illustrative purposes. The number or cardinality of feature maps received at the first layer of each classifier can be configurable. Kernel sizes that obey the rule kernel = stride size = (2k, 2k), for some k, can be used in some examples.
[00206] In a case where v neighboring rows and v neighboring columns are included in input to the first feature extraction layer(s), such that the input block is of size (64+v)x(64xv), (128+nc128+n), etc., a kernel of size (2k+v, 2k+v) and a stride size of (2k, 2k) can be used to propagate the v left/top information and observe (e.g., preserve) the perception field.
[00207] While outputs of the classification layers are described as matrices of the form BxBxl (e.g., 4x4x1, 2x2x1, or lxlxl), it is to be understood that a classification layer outputs B*B=B2 values, such that each of the outputs B2 corresponds to a Cartesian location in the matrix. Each of the output values corresponds a block location and can be a value indicating whether a sub-block at that location should be partitioned or not. For example, a value of 0 can indicate that the sub-block is not to be partitioned and a value of 1 can indicate that the sub-block is to be partitioned. Other values are, of course, possible.
[00208] The first/initial feature extraction layer can apply a non-overlapping convolution filter to the block to generate feature maps of the block for the next feature extraction layer. Each successive feature extraction layer after the initial feature extraction layer can apply a non-overlapping convolution filter to the feature maps from the previous layer. A non overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size.
[00209] For example, a first feature extraction layer can apply a (N/S)x(N/S) non overlapping convolution filter to the block to generate a first subset (e.g., cardinality) of the feature maps of the block. This can be illustrated by example where the feature extraction layer 1003-A applies a (64/8)x(64/8)=8x8 non-overlapping convolutional filter to the block 1002 to generate the feature maps 1004. A second feature extraction layer can apply a MxM non-overlapping convolutional filter to the block to generate maps each of size
(N/M)x(N/M), where M is less than S, is greater than 1, and is a power of 2; and successively apply non-overlapping 2x2 convolutional filters to the maps to generate a second subset (e.g., cardinality) of the feature maps of the block. The feature extraction layers 1003-B and 1003- C can be examples of the second feature extraction layer.
[00210] As described above, a non-linear value of a quantization parameter (QP) can be used as an input to the CNN. In FIG. 10, the non-linear value of QP is shown as an input to
the concatenation layer 1016. However, that need not be the case and the non-linear value of the QP can be used as an input to other layers of the CNN. For example, the non-linear value of the QP can be used as the input to at least one of the classification layers.
[00211] A CNN that is configured as described above can be used by an encoder, such as the encoder 400 of FIG. 4, to infer mode decisions that include block partitioning and optionally other mode decisions. As such, the mode decisions are not derived by brute force methods as are known in the art. In an example, a CNN described herein can be used by the intra/inter-prediction stage 402. Subsequent to inferring the block partitioning, an encoder can predict the blocks of the partitions using known prediction techniques, such as inter prediction, intra-prediction, other techniques, or a combination thereof, or can obtain the prediction mode from the CNN. The resulting residual may be encoded as described with regard to FIG. 4, optionally where the transform type/size is also provided by the CNN.
[00212] FIG. 17 is a flowchart of a process 1700 for encoding, by an encoder, an image block according to implementations of this disclosure. The process 1700 trains, using input data, a machine-learning model to infer one or more mode decisions. The process 1700 then uses the trained machine-learning model to infer a mode decision for an image block, which is to be encoded. In an example, the mode decision can be a quad-tree partition decision of the image block. The image block can be a block of an image (e.g., a video frame) that is encoded using intra-prediction. In another example, the mode decision can be a partition that includes partitions described with respect to FIG. 20 described below. As further described below, some of the partitions of FIG. 20 include square and non-square sub-partition; and each of the square sub-partitions can be further partitioned according to one of the partitions of FIG. 20.
[00213] At 1702, the process 1700 trains the machine-learning (ML) model. The ML model can be trained using a training data 1712. Each training datum of the training data 1712 can include a video block that was encoded by traditional encoding methods (e.g., by an encoder such as described with respect to FIGS. 4 and 6-8); a QP used by the encoder; zero or more additional inputs corresponding to inputs used by the encoder in determining the mode decision (e.g., block partitioning and optionally prediction mode and/or transform unit size) for encoding the video block; and the resulting mode decision determined by the encoder. In the training phase, parameters of the ML model are generated such that, for at least some of the training data, the ML model can infer, for a training datum, the resulting mode decision of the training datum for a set of inputs that includes the video block, the value
corresponding to a quantization parameter, and zero or more additional inputs of the training datum.
[00214] As described above, the value corresponding to the QP has a non-linear relation to the QP. That is, the value is derived from the QP based on a non-linear function of the QP. In an example, the non-linear function can be an exponential function, a quadratic function, or some other non-linear function of the QP. For example, the non-linear function f(Q ) = cQP , where c is a constant, can be used. In an example, c = 1/3. The non-linear function f(QP ) = QPa, where is a integer that is not equal to 0 or 1 (i.e., a ¹ 0 and a ¹ 1), can be used. In an example, a = 2. In the general case, the non-linear function is of a same type as a function used by the encoder for determining a multiplier used in a rate-distortion calculation, as described above.
[00215] In the case that the ML model is used to infer a relationship between blocks and respective quad-tree partitioning of the blocks, the resulting mode decision determined by the encoder can be indicative of the quad-tree partition of the training block of the training datum. Many indications (e.g., representations) of the quad-tree partition are possible. In an example, a vector (e.g., sequence) of binary flags, as described with respect to the quadtree 704 can be used.
[00216] In the case that the ML model is used to infer a relationship between blocks that are intra-predicted and respective partitioning of the blocks, the zero or more additional inputs corresponding to inputs used by the second encoder in determining the mode decision for encoding the video block can include at least some of the samples (i.e., first samples) of the top neighboring block, at least some of the samples (i.e., second samples) of the left neighboring block of the input block, at least some of the samples of the top-left neighboring block, or a combination thereof. For brevity, and without loss of generality, the top-left neighboring block can be considered to be part of either the top neighboring block or the left neighboring block. As such, in an example, the first samples or the second samples can be considered to include samples from the top-left neighboring block.
[00217] During the training phase (i.e., at 1702), the ML model leams (e.g., trains, builds, derives, etc.) a mapping (i.e., a function) that accepts, as input, a block and a non-linear value of a quantization parameter (e.g., QP2 as shown in FIG. 10) and outputs at least a partitioning of the block.
[00218] During the training phase, and so that the learned function can be as useful as possible, it is preferable that the ML model be trained using a large range of input blocks and
a large range of possible QP values, such as QP values that are used in representative of real- world applications.
[00219] With respect to input blocks, if the training data set includes only dark (e.g., pixels having low intensity values) training blocks, then the ML model may well learn how to determine a mode decision for dark blocks but provide unreliable output when presented with non-dark blocks during the inference phase. If the encoder uses a discrete set of the QP values, then it is preferable that each of the QP values is well represented in the training data set. For example, if the QP value can vary from 0 to 1, then it is preferable that the training data include varying QP values in the range 0 to 1. If a QP value is not used (e.g., missed QP value) in the training data, then the ML model may misbehave (e.g., provide erroneous output) when the missed QP value is presented to the ML model during the inference phase. In another example, if a missed QP value (i.e., a QP value that is not used during the training phase) is used during the inference phase, the missed QP can be interpolated from QP values that are used during the training phase and the interpolated QP value can then be used during the inference phase.
[00220] The ML model can then be used by the process 1700 during an inference phase. The inference phase includes the operations 1704 and 1706. A separation 1710 indicates that the training phase and the inference phase can be separated in time. As such, the inferencing phase can be performed by a first encoder and the training data 1712 can be generated by a second encoder. In an example, the first encoder and the second encoder are the same encoder. That is, the training data 1712 can be generated by the same encoder that performs the inference phase. In either case, the inference phase uses a machine-learning model that is trained as described with respect to 1702.
[00221] At 1704, inputs are presented to ML module. That is, the inputs are presented to a module that incorporates, includes, executes, implements, and the like the ML model. The inputs include the image block and optionally a non-linear function of a value corresponding to a QP. As described above, the first value is derived (i.e., results) from the non-linear function using the QP as input to the non-linear function. The inputs can also include additional inputs, as described above with respect to the zero or more additional inputs.
[00222] In an example, the non-linear function can be approximated by linear segments. Approximating the non-linear function by piecewise linear segments is illustrated with respect to FIG. 18.
[00223] FIG. 18 is an example 1800 of approximating a non-linear function of a quantization parameter using linear segments according to implementations of this disclosure. A quadratic function is used to describe the non-linear function. However, other non-linear function types are possible. The example 1800 shows, as a dashed curve, a non-linear function 1802 of the quantization parameter. The non-linear function 1802 is QP2. In the example 1800, the QP values range from 0 to 1. The example 1800 illustrates splitting the range 0 to 1 into several segments; namely, segments 1804, 1806, and 1808. While three segments are illustrated, more or fewer, but more than 1, segments can be used.
[00224] The range 0 to 1 can be split into a first range that includes the QP values 0 to 0.25, a second range that includes the QP values 0.25 to 0.75, and a third range that includes the QP values 0.75 to 1. The segment 1804 corresponds to the function QPt = 0.25 QP the segment 1806 corresponds to the function QP2 = QP— 0.1875; and the segment 1808 corresponds to the function QP3 = 1.75 QP— 0.75. As such, which of functions QP1, QP3, or QP3 is used to derive the second value, which is used during the training phase, and the first value, which is used during the inferencing phase, depends on the respective QP value. For example, if the first quantization parameter is 0.8, then the function QP3 is used. For example, if the second quantization parameter is 0.2, then the function QPt is used.
[00225] The benefits of using a non-linear QP function may be seen by reference to FIG. 19, which is an example 1900 of a rate-distortion performance comparison of a first machine learning model 1916 that uses as input a non-linear QP function and a second machine learning model 1926 that uses a linear QP function. The peak signal-to-noise ratio (PSNR) is used as the distortion metric. That is, in the graphs 1910 and 1920, the x-axis indicates the data rate for encoding the sample video sequence measured in kbps, while the y-axis indicates PSNR quality measured in decibels (dB). The results of graphs 1910 and 1920 were obtained by experimentation as described below.
[00226] The first machine-learning model 1916 is a model of a CNN that has an architecture as described with respect to FIG. 10, by example. Whereas the fully connected layers of the second machine-learning model 1926 has 1.2 million parameters, the first machine-learning model 1916 (which is an all-convolutional model and does not include fully connected classification layers) is much smaller with only 300,000 parameters (using a feature compression rate of 0.5). As such, and due at least in part to the smaller model size, it is possible to perform inferring on a power- and/or capacity-constrained platform (e.g., a mobile device) using the first machine-learning model 1916. The reduced model size is due
in part to each of, or the combination of, using the non-linear value of the QP (in this example, QP2) and the CNN architecture, which has the receptive field conforming properties described herein.
[00227] The first machine-learning model 1916 and the second machine-learning model 1926 are depicted as generic machine-learning models with an input layer, internal layers, and an output layer. The first machine-learning model 1916 and the second machine-learning model 1926 are depicted only to illustrate that the first machine-learning model 1916 uses a non-linear function of QP, namely QP2, whereas the second machine-learning model 1926 uses a linear function of QP, namely the QP value itself.
[00228] A curve 1912 of graphs 1910 and 1920 depicts the rate-distortion performance of a (e.g., VP9) encoder, as described with respect to FIG. 4. That is the curve 1912 is generated based on brute-force encoding (i.e., encoding that is not based on a machine-learning model). A curve 1914 of the graph 1910 depicts the rate-distortion performance resulting from using the first machine-learning model 1916 to infer block partitions to be used in a VP9 software encoder. A curve 1924 of the graph 1920 depicts the rate-distortion performance resulting from using the second machine-learning model 1926 to infer block partitions to be used in a VP9 software encoder.
[00229] It is noted that in the graphs 1910 and 1920, higher QP values typically correspond to lower data rates. The graph 1920 shows that when using a linear function of QP, the PSNR performance degrades as the QP value increases. However, when using QP2, as shown in the graph 1910, a more consistent rate-distortion performance across various QP values is obtained.
[00230] The graphs show that, on average, higher rate-distortion performance can be achieved when using QP2. The performance, in RD-rate, is approximately 1.78% worse than brute-force encoding when using QP2; whereas using QP, the performance is approximately 3.6% worse than brute-force encoding.
[00231] Referring again to FIG. 17, at 1706 the process 1700 obtains one or more mode decisions from the machine-learning model. In an example, the process 1700 obtains the mode decisions as described with respect to FIGS. 10 and 13-16. That is, for example, the CNN can provide an output that is indicative of a quad-tree partition of an image block.
[00232] At 1708, the process 1700 encodes the image block using the one or more mode decisions. That is, and continuing with the example of inferring a block partitioning, for each of the sub-blocks (i.e., according to the output that is indicative of a quad-tree partition), the
process 2000 can intra-predict (or inter-predict) the block as described with respect to the intra/inter-prediction stage 402 of FIG. 4, and consistent with the description of FIG. 4, ultimately entropy encode, as described with respect to the entropy encoding stage 408, the image block in a compressed bitstream, such as the bitstream 420 of FIG. 4.
[00233] An encoder that uses a machine-learning model, such as one of the ML models described herein, to infer mode decision parameters for image block, can encode the mode decisions in a compressed bitstream, such as the bitstream 420 of FIG. 4. As mentioned above, the image block can be a superblock and the mode decision can be indicative of a quad-tree partition of the superblock.
[00234] As such, a decoder, such as the decoder 500 of FIG. 5, can decode the image block using the mode decisions received in the compressed bitstream. The process of decoding an image block can include receiving, in a compressed bitstream, such as the compressed bitstream 420 of FIG. 5, an indication of a partitioning of the image block into sub-blocks; and decoding the image block using the indication of the partitioning and optionally the prediction mode and transform unit size (or type) of the image block.
[00235] As is known in the art, a quad-tree, such as described with respect to FIG. 7, can be output in a compressed bitstream, such as the bitstream 420 of FIG. 4. A decoder, such as the decoder 500 of FIG. 5, can decode from the compressed bitstream the quad-tree in the process of decoding a block (i.e., a superblock). The quad-tree can be determined (e.g., inferred) in the encoder using a CNN that is configured as described above and output in the compressed bitstream. As such, a decoder decodes, from the compressed bitstream, the quad tree, which was inferred by the CNN that is configured as described with respect to any of FIGS. 10, 13, 14, 15, or 16.
[00236] While inferring a quad-tree partition of a block is described, a CNN according to implementations of this disclosure can be used to infer non-square partitions that may or may not be represented by a quad-tree. That is, for example, a non-square partition can correspond to an internal node of the quad-tree having a number or cardinality of children that is greater than or equal to two children. FIG. 20 is an example 2000 of non-square partitions of a block. Some encoders may partition a superblock, such as a super-block of size 64x64, 128x128, or any other size, of a square sub-block of the superblock, into one of the partitions of the example 2000.
[00237] A partition type 2002 (which may be referred to as the PARTITION_VERT_A) splits an NxN coding block into two horizontally adjacent square blocks 2002A, 2002B, each
of size N/2xN/2, and a rectangular prediction unit of size NxN/2. A partition type 2008 (which may be referred to as the PARTITION_VERT_B) splits an NxN coding block into a rectangular prediction unit of size NxN/2 and two horizontally adjacent square blocks 2008A, 2008B, each of size N/2xN/2.
[00238] A partition type 2004 (which may be referred to as the PARTITION_HORZ_A) splits an NxN coding block into two vertically adjacent square blocks 2004A, 2004B, each of size N/2xN/2, and a rectangular prediction unit of size N/2xN. A partition type 2010 (which may be referred to as the PARTITION_HORZ_B) splits an NxN coding block into a rectangular prediction unit of size N/2xN and two vertically adjacent square blocks 2010A, 2010B, each of size N/2xN/2.
[00239] A partition type 2006 (which may be referred to as the PARTITION_VERT_4) splits an NxN coding block into four vertically adjacent rectangular blocks, each of size NxN/4. A partition type 2012 (which may be referred to as the PARTITION_HORZ_4) splits an NxN coding block into four horizontally adjacent rectangular blocks, each of size N/4xN.
[00240] As is known, other partition types can be used by a codec. The example 2000 illustrates four partition types that may be available at an encoder. A partition type 2014 (also referred to herein as the PARTITIONS PLIT partition type and partition- split partition type) splits an NxN coding block into four equally sized square sub-blocks. For example, if the coding block 2014 is of size NxN, then each of the four sub-blocks of the
PARTITIONS PLIT partition type, such as a sub-block 2014A, is of size N/4xN/4.
[00241] A partition type 2016 (also referred to herein as the PARTITION_VERT partition type) splits the coding block into two adjacent rectangular prediction units 2016 A, 2016B, each of size NxN/2. A partition type 2018 (also referred to herein as the PARTITION_HORZ partition type) splits the coding block into two adjacent rectangular prediction units, each of size N/2xN. A partition type 2020 (also referred to herein as the PARTITIONNONE partition type and partition-none partition type) uses one prediction unit for the coding block such that the prediction unit has the same size (i.e., NxN) as the coding block.
[00242] The partition types 2014-2020 are referred to herein as basic partition types and the partitions 2002-2012 are referred to herein as extended partition types.
[00243] A partition can be represented by a tree. A tree can be represented by a vector. Let P denote the set of all valid partitions (or, equivalently, the respective representations of the partitions). Accordingly, a CNN can be trained to infer a mapping into the set P. Configuring
a CNN to infer the partitions described with respect to FIG. 20 includes defining an appropriate set P and using appropriate training data.
[00244] Assuming that there are N possible outcomes, then there are N*M possible decisions (in an example, M=2l and N=4) for simplicity, and each p(n, j) can be combined by a softmax function, so that sum(n in range(N)) p(n, j) == 1 for some any j.
[00245] For example, in the case of VP9, which uses a coding unit size of 64x64 and the four basic partition types, for quad- tree partitions only, there are 21 decisions corresponding to one 64x64, four 32x32, and 16 16x16 decisions (i.e., 1 + 4 + 16 = 21 decisions). In a case where a CNN is used to also determine non-quad-tree partitions, then there are 21 * 4 = 84 possible decisions, where 21 corresponds to the quad-tree partitions and four corresponds to the basic partition types; namely, PARTITION_SPLIT, PARTITION_VERT,
PARTITION_HORZ, and PARTITION_NONE.
[00246] For example, in the case of AV1, which uses a coding unit size of 128x128 and the basic and extended partitions types (for a total of 10 partition types), for quad-tree partitions only, there are 85 decisions (corresponding to one 128x128, four 64x64, 16 32x32, and 64 16x16 decisions) per partition type. In a case where a CNN is used to also determine non-quad-tree partitions, then there are 850 decisions (corresponding to 85 decisions multiplied by 10 partition types = 850 decisions).
[00247] More generally, non-square partitions can be composed by smaller square groups. Accordingly, the techniques described herein may be generalized to non-square partitions such as shown in FIG. 20.
[00248] For simplicity of explanation, the processes depicted and described herein are shown as a series of blocks, steps, or operations. However, the blocks, steps, or operations in accordance with this disclosure can occur in various orders and/or concurrently. Additionally, other steps or operations not presented and described herein may be used. Furthermore, not all illustrated steps or operations may be required to implement a technique in accordance with the disclosed subject matter.
[00249] The aspects of encoding and decoding described above illustrate some encoding and decoding techniques. However, it is to be understood that“encoding” and“decoding,” as those terms are used in the claims, could mean compression, decompression, transformation, or any other processing or change of data.
[00250] The words“example” or“implementation” are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as“example” or
“implementation” is not necessarily to be construed as being preferred or advantageous over other aspects or designs. Rather, use of the words“example” or“implementation” is intended to present concepts in a concrete fashion. As used in this application, the term“or” is intended to mean an inclusive“or” rather than an exclusive“or.” That is, unless specified otherwise or clearly indicated otherwise by the context,“X includes A or B” is intended to mean any of the natural inclusive permutations thereof. That is, if X includes A; X includes B; or X includes both A and B, then“X includes A or B” is satisfied under any of the foregoing instances. In addition, the articles“a” and“an” as used in this application and the appended claims should generally be construed to mean“one or more” unless specified otherwise or clear from the context to be directed to a singular form. Moreover, use of the term“an implementation” or“one implementation” throughout is not intended to mean the same embodiment or implementation unless described as such.
[00251] Implementations of the transmitting station 102 and/or the receiving station 106 (and the algorithms, methods, instructions, etc., stored thereon and/or executed thereby, including by the encoder 400 and the decoder 500) can be realized in hardware, software, or any combination thereof. The hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit. In the claims, the term“processor” should be understood as encompassing any of the foregoing hardware, either singly or in combination. The terms“signal” and“data” are used interchangeably. Further, portions of the transmitting station 102 and the receiving station 106 do not necessarily have to be implemented in the same manner.
[00252] Further, in one aspect, for example, the transmitting station 102 or the receiving station 106 can be implemented using a general-purpose computer or general-purpose processor with a computer program that, when executed, carries out any of the respective methods, algorithms, and/or instructions described herein. In addition, or alternatively, for example, a special-purpose computer/processor, which can contain other hardware for carrying out any of the methods, algorithms, or instructions described herein, can be utilized.
[00253] The transmitting station 102 and the receiving station 106 can, for example, be implemented on computers in a video conferencing system. Alternatively, the transmitting station 102 can be implemented on a server, and the receiving station 106 can be
implemented on a device separate from the server, such as a handheld communications
device. In this instance, the transmitting station 102, using an encoder 400, can encode content into an encoded video signal and transmit the encoded video signal to the
communications device. In turn, the communications device can then decode the encoded video signal using a decoder 500. Alternatively, the communications device can decode content stored locally on the communications device, for example, content that was not transmitted by the transmitting station 102. Other transmitting station 102 and receiving station 106 implementation schemes are available. For example, the receiving station 106 can be a generally stationary personal computer rather than a portable communications device, and/or a device including an encoder 400 may also include a decoder 500.
[00254] Further, all or a portion of implementations of the present disclosure can take the form of a computer program product accessible from, for example, a tangible computer- usable or computer-readable medium. A computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor. The medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.
[00255] Further implementations are summarized in the following examples:
[00256] Example 1 : A convolutional neural network (CNN) for determining a block partitioning in video coding, the block having an NxN size and a smallest partition determined by the CNN being of size SxS, the CNN comprising: feature extraction layers; a concatenation layer that receives, from the feature extraction layers, first feature maps of the block, wherein each first feature map is of size SxS; and classifiers, each classifier comprising classification layers, wherein each classification layer receives second feature maps having a respective feature dimension, the each classifier is configured to infer partition decisions for sub-blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S, by: applying, at some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
[00257] Example 2: The CNN of Example 1, wherein the classifiers includes a first classifier that infers partition decisions for sub-blocks of size (2S)x(2S), the first classifier is configured to: receive the first feature maps from the concatenation layer; and apply a first non-overlapping convolution operation using a first 2x2 kernel to reduce the first feature maps to a size of (S/2)x(S/2).
[00258] Example 3: The CNN of Example 2, wherein the classifiers includes a second classifier that infers partition decisions for sub-blocks of size (b8)c(b8), the second classifier being different from the first classifier and is configured to: receive third feature maps, each of size MxM, from a third classifier; and apply a second non-overlapping convolution operation using a second 2x2 kernel to reduce the third feature maps to a size of
(M/2)x(M/2).
[00259] Example 4: The CNN of any of Examples 1 to 3, wherein the feature extraction layers comprise: a first feature extraction layer that applies an (N/S)x(N/S) non-overlapping convolutional filter to the block to generate a first subset of the first feature maps of the block.
[00260] Example 5 : The CNN of Example 4, wherein the feature extraction layers further comprise: a second feature extraction layer that is configured to: apply an MxM non overlapping convolutional filter to the block to generate maps each of size (N/M)x(N/M), wherein M is less than S, is greater than 1, and is a power of 2; and successively apply non overlapping 2x2 convolutional filters to the maps to generate a second subset of the first feature maps of the block.
[00261] Example 6: The CNN of any of Examples 1 to 5, wherein N is 64 and S is 8.
[00262] Example 7: The CNN of any of Examples 1 to 5, wherein N is 128 and S is 8.
[00263] Example 8: The CNN of any of Examples 1 to 7, wherein a non-linear value of a quantization parameter (QP) is used as an input to the CNN.
[00264] Example 9: The CNN of Example 8, wherein the non-linear value of the QP is used as the input to the concatenation layer.
[00265] Example 10: The CNN of Example 8, wherein the non-linear value of the QP is used as the input to at least one of the classification layers.
[00266] Example 11 : A method of determining a block partitioning in video coding using a convolutional neural network (CNN), the block having an NxN size and a smallest partition determined by the CNN being of size SxS, comprising: extracting, using feature extraction layers of the CNN, first feature maps of the block, wherein each first feature map is of size SxS; concatenating, using a concatenation layer of the CNN, the first feature maps of the block; and inferring, by classifiers of the CNN each including classification layers, block partitioning, by: receiving, by each classification layer, second feature maps having a respective feature dimension; inferring, by the each classifier, partition decisions for sub blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S, by:
applying, at some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map.
[00267] Example 12: The method of Example 11, wherein the classifiers includes a first classifier that infers partition decisions for sub-blocks of size (2S)x(2S), and wherein inferring, by the each classifier, partition decisions for sub-blocks of size N/(aS)xN/(aS) of the block further comprises: receiving, by the first classifier, the first feature maps from the concatenation layer; and applying, by the first classifier, a first non-overlapping convolution operation using a first 2x2 kernel to reduce the first feature maps to a size of (S/2)x(S/2).
[00268] Example 13: The method of Example 12, wherein the classifiers includes a second classifier that infers partition decisions for sub-blocks of size N/(b8)cN/(b8), the second classifier being different from the first classifier, and further comprising: receiving, by the second classifier, third feature maps, each of size MxM, from a third classifier; and applying, by the second classifier, a second non-overlapping convolution operation using a second 2x2 kernel to reduce the third feature maps to a size of (M/2)x(m/2).
[00269] Example 14: The method of any of Examples 11 to 13, comprising: applying, by a first feature extraction layer of the feature extraction layers, an (N/S)x(N/S) non-overlapping convolutional filter to the block to generate a first subset of the first feature maps of the block.
[00270] Example 15: The method of Example 14, wherein the feature extraction layers further comprise a second feature extraction layer, further comprising: applying, by the second feature extraction layer, an MxM non-overlapping convolutional filter to the block to generate maps each of size (N/M)x(N/M), wherein M is less than S, is greater than 1, and is a power of 2; and successively applying non-overlapping 2x2 convolutional filters to the maps to generate a second subset of the first feature maps of the block.
[00271] Example 16: The method of any of Examples 11 to 15, wherein N is 64 and S is 8.
[00272] Example 17: The method of any of Examples 11 to 15, wherein N is 128 and S is 8.
[00273] Example 18: The method of any of Examples 11 to 17, further comprising: using a non-linear value of a quantization parameter (QP) as an input to the concatenation layer.
[00274] Example 19: An apparatus for decoding an image block, comprising: a processor that: receives, in a compressed bitstream, an indication of a quad-tree partitioning of the image block into sub-blocks, wherein an encoder determined the quad-tree partitioning of the
image block using a convolutional neural network (CNN) that includes: feature extraction layers; a concatenation layer that receives, from the feature extraction layers, first feature maps of the block, wherein each first feature map is of size SxS; and classifiers, each classifier comprising classification layers, wherein each classification layer receives second feature maps having a respective feature dimension, the each classifier is configured to infer partition decisions for sub-blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S, by: applying, at some of successive classification layers of the classification layers, a kernel of size lxl to reduce the respective feature dimension in half; and outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map; and decodes the image block using the indication of the quad tree partitioning of the image block.
[00275] Example 20: The apparatus of Example 19, wherein a non-linear value of a quantization parameter (QP) is used as an input to the CNN.
[00276] The above-described embodiments, implementations, and aspects have been described in order to allow easy understanding of the present disclosure and do not limit the present disclosure. On the contrary, the disclosure is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation as is permitted under the law so as to encompass all such modifications and equivalent arrangements.
Claims
1. A convolutional neural network for determining a mode decision for encoding a block in video coding, comprising:
feature extraction layers for extracting features of the block for determining the mode decision, wherein a non-overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size, the mode decision comprises a block partitioning of the block, the block has a NxN size, and a smallest partition output for the block has a SxS size; and
multiple classifiers for processing the features of the block, wherein:
each classifier comprises classification layers, each classification layer of the classification layers for receiving respective feature maps having a respective feature dimension,
each classifier is configured to infer partition decisions for sub-blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S, by:
applying, at some of successive classification layers of the
classification layers, a kernel of size lxl to reduce the respective feature dimension; and
outputting, by a last layer of the classification layers, an output corresponding to a N/(aS)xN/(aS)xl output map.
2. The convolutional neural network of claim 1 , wherein an input into an initial feature extraction layer of the feature extraction layers has a size (N+l)x(N+l)xl comprising the block and neighboring pixels of the block, and a first filter in a branch of the feature extraction layers has a kernel size of 2k +1 and a stride value equal to 2k, where k is an integer.
3. The convolutional neural network of claim 1 or 2, wherein a final layer of the feature extraction layers comprises a concatenation layer that receives, from the feature extraction layers, first feature maps of the block, wherein each of the first feature maps is of the SxS size.
4. The convolutional neural network of any of claims 1 to 3, wherein the feature extraction layers comprise a number of branches equal to a number of possible quad-tree partition decisions for the block, each of the branches comprising at least one of the feature extraction layers.
5. The convolutional neural network of claim 4, wherein the feature extraction layers are arranged so that each of the branches comprises distinct feature extraction layers that do not refer to the feature extraction layers of any other branch of the branches.
6. The convolutional neural network of claim 4 or 5, wherein the multiple classifiers comprise a respective classifier corresponding to a respective branch of the branches, the feature maps received by an initial classification layer of a respective classifier configured to infer the partition decisions for sub-blocks of size (aS)x(aS) of the block comprising a convolution of N feature maps having respective feature dimension
(N/2P)x(N/2P), wherein b is an integer and b=0, . . ., (the number of branches - 1).
7. The convolutional neural network of any of claims 1 to 4, wherein an initial feature extraction layer of the feature extraction layers comprises a first kernel for performing a first convolution operation using luma data of the block as input and a second kernel for performing a second convolution operation using chroma data of the block as input, the first kernel having a different kernel size from the second kernel, and wherein the block partitioning for the block comprises partitioning of the luma data of the block.
8. The convolutional neural network of claim 7, wherein a second filter extraction layer subsequent to the initial feature extraction layer outputs respective feature maps having a single kernel size.
9. The convolutional neural network of claim 7 or 8, wherein the feature extraction layers are arranged into multiple branches, each branch associated with a respective one of the multiple classifiers, and wherein each of the branches shares at least the initial feature extraction layer.
10. The convolutional neural network of any of claims 1 to 9, wherein the mode decision comprises a prediction mode of the block, the multiple classifiers comprise block
partition classifiers, and the convolutional neural network further comprises multiple prediction mode classifiers, wherein:
each multiple prediction mode classifier comprises at least one classification layer, each classification layer of the classification layers receiving respective feature maps having a respective feature dimension, and an initial classification layer of each classifier receiving the feature maps as one of the output of the final feature extraction layer of the feature extraction layers or an output of a classification layer of a respective multiple prediction mode classifier.
11. An encoder comprising the convolutional neural network of any of claim 1 to 10, wherein the encoder is configured to encode the block using one or more mode decisions indicated by the output of the last layer of the classification layers.
12. A method of determining a mode decision for encoding a block in video coding using a convolutional neural network, comprising:
extracting, using feature extraction layers of the convolutional neural network, features of the block for determining the mode decision, wherein a non-overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size, the mode decision comprises a block partitioning of the block, the block has a NxN size, and a smallest partition output for the block has a SxS size;
inferring, by multiple classifiers of the convolutional neural network that each include classification layers, the mode decision, by:
receiving, by each classification layer, respective feature maps having a respective feature dimension;
inferring, by a respective classifier of the multiple classifiers, partition decisions for sub-blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S, by:
applying, at some of successive classification layers of the
classification layers, a kernel of size lxl to reduce the respective feature dimension; and
outputting, by a last layer of the classification layers, an output corresponding to a N/(aS)xN/(aS)xl output map.
13. The method of claim 12, wherein extracting the features of the block comprises:
applying, using a first feature extraction layer, a (N/S)x(N/S) non-overlapping convolutional filter to the block to generate a first cardinality of feature maps of the block.
14. The method of claim 13, wherein extracting the features of the block comprises:
applying, using a second feature extraction layer, a MxM non-overlapping convolutional filter to an output of the first feature extraction layer to generate a second cardinality of feature maps each of size (N/M)x(N/M), wherein M is less than S, is greater than 1 , and is a power of 2.
15. The method of any of claims 12 to 14, further comprising:
encoding the block using the output from the last layer of the classification layers.
16. The method of any of claims 12 to 15, wherein the feature extraction layers comprise:
a first branch that provides feature maps to an initial classification layer of a first classifier that infers partition decisions for sub-blocks of size (2S)x(2S);
a second branch that provides feature maps to an initial classification layer of a second classifier that infers partition decisions for sub-blocks of size (4S)x(4S); and
a third branch that provides feature maps to an initial classification layer of a third classifier that infers partition decisions for sub-blocks of size (8S)x(8S); and wherein the first branch, the second branch, and the third branch share at least one of the feature extraction layers.
17. The method of claim 16, wherein inferring the mode decision comprises: applying, at the initial classification layer of the first classifier, a (N/S)x(N/S) non overlapping convolutional filter to reduce a number of features in the feature maps provided by the first branch;
applying, at the initial classification layer of the second classifier, a (N/S)x(N/S) non overlapping convolutional filter to reduce a number of features in the feature maps provided
by the second branch; and
applying, at the initial classification layer of the third classifier, a (N/S)x(N/S) non overlapping convolutional filter to reduce a number of features in the feature maps provided by the third branch.
18. The method of any of claims 12 to 17, wherein the multiple classifiers comprise multiple partition classifiers, the mode decision comprises a transform size of the block, and inferring the mode decision further comprises:
inferring, by a respective classifier of multiple transform mode classifiers of the convolutional neural network that each include classification layers, transform size decisions for sub-blocks of size (aS)x(aS) of the block using the feature maps from the final feature extraction layer of the feature extraction layers.
19. An apparatus for decoding an image block, comprising:
a processor configured to execute a method comprising:
receiving, in a compressed bitstream, an indication of a partitioning of the image block into sub-blocks, wherein an encoder determined the partitioning of the image block using a convolutional neural network that includes:
feature extraction layers for extracting features of the block for determining the partitioning, wherein a non-overlapping convolution operation is performed on input at at least one of the feature extraction layers by setting a stride value equal to a kernel size, the block has a NxN size, and a smallest partition output for the block has a SxS size; and
multiple classifiers, wherein:
each classifier comprises classification layers, each classification layer of the classification layers receiving respective feature maps having a respective feature dimension,
each classifier is configured to infer partition decisions for sub-blocks of size (aS)x(aS) of the block, wherein a is a power of 2 and a=2, . . ., N/S, by:
applying, at some of successive classification layers of the
classification layers, a kernel of size lxl to reduce the respective feature dimension; and
outputting by a last layer of the classification layers an output corresponding to a N/(aS)xN/(aS)xl output map; and
decoding the image block using the indication of the partitioning of the image block.
20. The apparatus of claim 19, where the multiple classifiers share at least one of the feature extraction layers.
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
EP19779332.6A EP3743855A1 (en) | 2018-09-18 | 2019-09-17 | Receptive-field-conforming convolution models for video coding |
CN201980017854.1A CN111837140A (en) | 2018-09-18 | 2019-09-17 | Video coded field consistent convolution model |
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/134,165 | 2018-09-18 | ||
US16/134,165 US10869036B2 (en) | 2018-09-18 | 2018-09-18 | Receptive-field-conforming convolutional models for video coding |
US16/289,149 US11025907B2 (en) | 2019-02-28 | 2019-02-28 | Receptive-field-conforming convolution models for video coding |
US16/289,149 | 2019-02-28 |
Publications (1)
Publication Number | Publication Date |
---|---|
WO2020061008A1 true WO2020061008A1 (en) | 2020-03-26 |
Family
ID=68073256
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
PCT/US2019/051458 WO2020061008A1 (en) | 2018-09-18 | 2019-09-17 | Receptive-field-conforming convolution models for video coding |
Country Status (3)
Country | Link |
---|---|
EP (1) | EP3743855A1 (en) |
CN (1) | CN111837140A (en) |
WO (1) | WO2020061008A1 (en) |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113378973A (en) * | 2021-06-29 | 2021-09-10 | 沈阳雅译网络技术有限公司 | Image classification method based on self-attention mechanism |
WO2021261950A1 (en) * | 2020-06-25 | 2021-12-30 | 인텔렉추얼디스커버리 주식회사 | Method and apparatus for compression and training of neural network |
US20220065981A1 (en) * | 2020-08-31 | 2022-03-03 | Qualcomm Incorporated | Data efficient learning and rapid domain adaptation for wireless positioning and tracking |
WO2022139617A1 (en) * | 2020-12-24 | 2022-06-30 | Huawei Technologies Co., Ltd. | Encoding with signaling of feature map data |
WO2022237427A1 (en) * | 2021-05-11 | 2022-11-17 | 北京字跳网络技术有限公司 | Video processing method and apparatus, device, and storage medium |
WO2023051653A1 (en) * | 2021-09-29 | 2023-04-06 | Beijing Bytedance Network Technology Co., Ltd. | Method, apparatus, and medium for video processing |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN115410585A (en) * | 2021-05-29 | 2022-11-29 | 华为技术有限公司 | Audio data encoding and decoding method, related device and computer readable storage medium |
Family Cites Families (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3331668B2 (en) * | 1993-03-22 | 2002-10-07 | 富士ゼロックス株式会社 | Nonlinear operation unit and information processing apparatus using the same |
EP1515270A1 (en) * | 2003-09-09 | 2005-03-16 | Semeion | An artificial neural network |
HUE051313T2 (en) * | 2009-10-01 | 2021-03-01 | Sk Telecom Co Ltd | Method and apparatus for encoding/decoding image using variable-sized macroblocks |
US9967558B1 (en) * | 2013-12-17 | 2018-05-08 | Google Llc | Adaptive motion search control for variable block size partitions in video coding |
CN104754357B (en) * | 2015-03-24 | 2017-08-11 | 清华大学 | Intraframe coding optimization method and device based on convolutional neural networks |
CN106162167B (en) * | 2015-03-26 | 2019-05-17 | 中国科学院深圳先进技术研究院 | Efficient video coding method based on study |
US10068171B2 (en) * | 2015-11-12 | 2018-09-04 | Conduent Business Services, Llc | Multi-layer fusion in a convolutional neural network for image classification |
KR102309910B1 (en) * | 2015-11-19 | 2021-10-08 | 한국전자기술연구원 | Optimal mode decision unit of video encoder and video encoding method using the optimal mode decision |
WO2017088093A1 (en) * | 2015-11-23 | 2017-06-01 | Mediatek Singapore Pte. Ltd. | On the smallest allowed block size in video coding |
CN105791826B (en) * | 2016-05-11 | 2019-03-08 | 南京大学 | A kind of HEVC interframe fast schema selection method based on data mining |
CN106408562B (en) * | 2016-09-22 | 2019-04-09 | 华南理工大学 | Eye fundus image Segmentation Method of Retinal Blood Vessels and system based on deep learning |
WO2018099579A1 (en) * | 2016-12-02 | 2018-06-07 | Huawei Technologies Co., Ltd. | Apparatus and method for encoding an image |
CN106713935B (en) * | 2017-01-09 | 2019-06-11 | 杭州电子科技大学 | A kind of HEVC block division fast method based on Bayesian decision |
US10382770B2 (en) * | 2017-02-06 | 2019-08-13 | Google Llc | Multi-level machine learning-based early termination in partition search for video encoding |
CN107392925B (en) * | 2017-08-01 | 2020-07-07 | 西安电子科技大学 | Remote sensing image ground object classification method based on super-pixel coding and convolutional neural network |
CN107742107B (en) * | 2017-10-20 | 2019-03-01 | 北京达佳互联信息技术有限公司 | Facial image classification method, device and server |
CN107704862A (en) * | 2017-11-06 | 2018-02-16 | 深圳市唯特视科技有限公司 | A kind of video picture segmentation method based on semantic instance partitioning algorithm |
CN108259897B (en) * | 2018-01-23 | 2021-08-27 | 北京易智能科技有限公司 | Intra-frame coding optimization method based on deep learning |
CN108366295B (en) * | 2018-02-12 | 2020-07-14 | 北京印刷学院 | Video classification feature extraction method, transcoding recompression detection method and storage medium |
CN108495129B (en) * | 2018-03-22 | 2019-03-08 | 北京航空航天大学 | The complexity optimized method and device of block partition encoding based on deep learning method |
-
2019
- 2019-09-17 EP EP19779332.6A patent/EP3743855A1/en active Pending
- 2019-09-17 CN CN201980017854.1A patent/CN111837140A/en active Pending
- 2019-09-17 WO PCT/US2019/051458 patent/WO2020061008A1/en unknown
Non-Patent Citations (4)
Title |
---|
FENG ZEQI ET AL: "HEVC Fast Intra Coding Based CTU Depth Range Prediction", 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC), IEEE, 27 June 2018 (2018-06-27), pages 551 - 555, XP033422234, DOI: 10.1109/ICIVC.2018.8492898 * |
MAI XU ET AL: "Reducing Complexity of HEVC: A Deep Learning Approach", IEEE TRANSACTIONS ON IMAGE PROCESSING., vol. 27, no. 10, 22 March 2018 (2018-03-22), US, pages 5044 - 5059, XP055647622, ISSN: 1057-7149, DOI: 10.1109/TIP.2018.2847035 * |
MENG XIANDONG ET AL: "A New HEVC In-Loop Filter Based on Multi-channel Long-Short-Term Dependency Residual Networks", 2018 DATA COMPRESSION CONFERENCE, IEEE, 27 March 2018 (2018-03-27), pages 187 - 196, XP033376056, DOI: 10.1109/DCC.2018.00027 * |
ZHANG YUN ET AL: "Machine Learning-Based Coding Unit Depth Decisions for Flexible Complexity Allocation in High Efficiency Video Coding", IEEE TRANSACTIONS ON IMAGE PROCESSING, IEEE SERVICE CENTER, PISCATAWAY, NJ, US, vol. 24, no. 7, 1 July 2015 (2015-07-01), pages 2225 - 2238, XP011578748, ISSN: 1057-7149, [retrieved on 20150413], DOI: 10.1109/TIP.2015.2417498 * |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2021261950A1 (en) * | 2020-06-25 | 2021-12-30 | 인텔렉추얼디스커버리 주식회사 | Method and apparatus for compression and training of neural network |
US20220065981A1 (en) * | 2020-08-31 | 2022-03-03 | Qualcomm Incorporated | Data efficient learning and rapid domain adaptation for wireless positioning and tracking |
WO2022139617A1 (en) * | 2020-12-24 | 2022-06-30 | Huawei Technologies Co., Ltd. | Encoding with signaling of feature map data |
WO2022237427A1 (en) * | 2021-05-11 | 2022-11-17 | 北京字跳网络技术有限公司 | Video processing method and apparatus, device, and storage medium |
CN113378973A (en) * | 2021-06-29 | 2021-09-10 | 沈阳雅译网络技术有限公司 | Image classification method based on self-attention mechanism |
CN113378973B (en) * | 2021-06-29 | 2023-08-08 | 沈阳雅译网络技术有限公司 | Image classification method based on self-attention mechanism |
WO2023051653A1 (en) * | 2021-09-29 | 2023-04-06 | Beijing Bytedance Network Technology Co., Ltd. | Method, apparatus, and medium for video processing |
Also Published As
Publication number | Publication date |
---|---|
CN111837140A (en) | 2020-10-27 |
EP3743855A1 (en) | 2020-12-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11310501B2 (en) | Efficient use of quantization parameters in machine-learning models for video coding | |
US11310498B2 (en) | Receptive-field-conforming convolutional models for video coding | |
US11025907B2 (en) | Receptive-field-conforming convolution models for video coding | |
US11689726B2 (en) | Hybrid motion-compensated neural network with side-information based video coding | |
US10848765B2 (en) | Rate/distortion/RDcost modeling with machine learning | |
US10382770B2 (en) | Multi-level machine learning-based early termination in partition search for video encoding | |
EP3743855A1 (en) | Receptive-field-conforming convolution models for video coding | |
EP3545679B1 (en) | Apparatus and method for encoding an image | |
US11956447B2 (en) | Using rate distortion cost as a loss function for deep learning | |
WO2020046434A1 (en) | Lossy image compression using palettization of locally mixed colors | |
CN116547969A (en) | Processing method of chroma subsampling format in image decoding based on machine learning | |
WO2018222239A1 (en) | Adaptation of scan order for entropy coding | |
US20230007284A1 (en) | Ultra Light Models and Decision Fusion for Fast Video Coding | |
EP4338416A1 (en) | End-to-end learning-based, eg neural network, pre-processing and post-processing optimization for image and video coding | |
US11854165B2 (en) | Debanding using a novel banding metric | |
WO2023043552A1 (en) | Filtering with side-information using contextually-designed filters |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
121 | Ep: the epo has been informed by wipo that ep was designated in this application |
Ref document number: 19779332Country of ref document: EPKind code of ref document: A1 |
|
ENP | Entry into the national phase |
Ref document number: 2019779332Country of ref document: EPEffective date: 20200825 |
|
NENP | Non-entry into the national phase |
Ref country code: DE |