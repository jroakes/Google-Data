JP2017126317A - Hotword detection on multiple devices - Google Patents
Hotword detection on multiple devices Download PDFInfo
- Publication number
- JP2017126317A JP2017126317A JP2016174371A JP2016174371A JP2017126317A JP 2017126317 A JP2017126317 A JP 2017126317A JP 2016174371 A JP2016174371 A JP 2016174371A JP 2016174371 A JP2016174371 A JP 2016174371A JP 2017126317 A JP2017126317 A JP 2017126317A
- Authority
- JP
- Japan
- Prior art keywords
- computing device
- signal
- voice command
- speech
- mobile computing
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000001514 detection method Methods 0.000 title abstract 2
- 238000000034 method Methods 0.000 claims abstract description 66
- 230000009471 action Effects 0.000 claims abstract description 27
- 230000008569 process Effects 0.000 claims description 29
- 230000004044 response Effects 0.000 claims description 8
- 238000002604 ultrasonography Methods 0.000 claims description 7
- 238000004590 computer program Methods 0.000 abstract description 6
- 230000015654 memory Effects 0.000 description 31
- 238000004891 communication Methods 0.000 description 18
- 238000004364 calculation method Methods 0.000 description 12
- 230000005540 biological transmission Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000005236 sound signal Effects 0.000 description 3
- 239000000872 buffer Substances 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000000712 assembly Effects 0.000 description 1
- 238000000429 assembly Methods 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000003139 buffering effect Effects 0.000 description 1
- 238000010411 cooking Methods 0.000 description 1
- 238000001914 filtration Methods 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/03—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the type of extracted parameters
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
- G10L2025/783—Detection of presence or absence of voice signals based on threshold decision
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
- G10L25/87—Detection of discrete points within a voice signal
Abstract
Description
本明細書は、一般に、人が話す言葉を認識する、発話認識とも称されるシステムおよび技術に関する。 The present specification relates generally to systems and techniques, also called speech recognition, that recognize words spoken by a person.
発話が可能な家または他の環境、すなわちユーザがクエリまたはコマンドを平常音で話すだけで、コンピュータベースのシステムがクエリに対応し回答する、かつ/またはコマンドを実行するということが現実になってきた。発話が可能な環境(たとえば、家、仕事場、学校など)は、その環境における様々な部屋またはエリアのいたるところに分布した接続されたマイクデバイスのネットワークを用いて実施することができる。そのようなマイクのネットワークを通じて、ユーザは、コンピュータまたは他のデバイスをユーザの正面または近傍にさえ有する必要なしに、システムに、原則的には環境のどこからでも口頭で質問する能力を有する。たとえば、ユーザは、台所で料理しながら、システムに、「3カップは何ミリリットル?」と聞いてもよく、それに応じて、システムから、たとえば合成した声の出力という形で、回答を受けてもよい。あるいは、ユーザは、システムに、「ここから一番近いガソリンスタンドは、いつ閉まりますか?」などの質問を聞いてもよく、また、家を出る準備が整うと、「今日はコートを着ていくべきですか?」などの質問を聞いてもよい。 It is becoming reality that a home or other environment where speech is possible, i.e. a user simply speaks a query or command in a normal tone, and a computer-based system responds to the query and answers and / or executes the command. It was. Speaking environments (eg, homes, workplaces, schools, etc.) can be implemented using a network of connected microphone devices distributed throughout various rooms or areas in the environment. Through such a microphone network, the user has the ability to interrogate the system, in principle, verbally from anywhere in the environment without having to have a computer or other device in front of or even near the user. For example, while cooking in the kitchen, the user may ask the system, “How many milliliters is 3 cups?” And respond accordingly from the system, eg in the form of a synthesized voice output. Good. Alternatively, the user may ask the system questions such as "When will the nearest gas station close from here?" And when ready to leave home, You may ask questions such as "Do you want to go?"
さらに、ユーザは、システムに質問をしてもよく、かつ/または、ユーザの個人情報に関するコマンドを発行してもよい。たとえば、ユーザは、システムに、「ジョンとの打ち合わせはいつですか?」と聞いてもよく、または、システムに、「家に着いたら、ジョンに電話するよう思い出させて」と命令してもよい。 In addition, the user may ask questions to the system and / or issue commands regarding the user's personal information. For example, the user may ask the system “When is the meeting with John?” Or the system may ask “Remind me to call John when I get home” Good.
発話が可能なシステムのために、システムとやり取りするユーザのやり方を、これに限定されないが、主として、声を入力することによるものとして設計する。その結果、システムに向けられていない発言を含む周囲の環境でなされたすべての発言を潜在的に拾い上げるシステムでは、たとえば、環境の中にいる個人に向けられた任意の所与の発言が行われたときに対して、システムに向けられた任意の所与の発言が行われたときを識別する何らかのやり方を備えなければならない。この識別を達成する1つのやり方として、システムの注意をひくために話される所定の語として、その環境にいるユーザ間の合意によって予約されたホットワード(hotword)を使用することがある。環境の一例で、システムの注意をひくのに使用されるホットワードは、「オーケー、コンピュータ」という語である。その結果、「オーケー、コンピュータ」という語が話されるたびに、それはマイクによって拾われる。そして、それは、ホットワードが話されたかどうかを判断する発話認識技術を行い、もし話されたのなら、確保されるコマンドまたはクエリを待機するシステムに伝えられる。したがって、システムに向けられた発言は、[ホットワード][クエリ]という一般的な形をとる。この例での「ホットワード」は、「オーケー、コンピュータ」であり、「クエリ」は、任意の質問、コマンド、宣言、または単独でもしくはネットワークを介してサーバと連動して、システムによって発話を認識し、解析し、実行することができる他の要求であり得る。 For a system that can speak, the user's way of interacting with the system is designed primarily by, but not limited to, inputting voice. As a result, a system that potentially picks up all statements made in the surrounding environment, including statements that are not directed to the system, for example, gives any given remarks directed at individuals in the environment. There must be some way of identifying when a given utterance directed at the system is made. One way to achieve this identification is to use a hotword reserved by agreement between users in the environment as the predetermined word spoken to draw the attention of the system. In one example environment, the hot word used to draw the attention of the system is the word “ok, computer”. As a result, whenever the word “ok, computer” is spoken, it is picked up by the microphone. It then performs an utterance recognition technique that determines whether a hot word has been spoken, and if so, is communicated to a system that waits for a reserved command or query. Thus, statements directed to the system take the general form of [hotword] [query]. The “hotword” in this example is “OK, computer”, and the “query” is any question, command, declaration, or the utterance recognized by the system either alone or in conjunction with the server over the network And other requests that can be analyzed and implemented.
本明細書に記載される主題の1つの革新的な態様によると、コンピューティングデバイスは、ユーザが話した発言を受信する。コンピューティングデバイスは、発言がホットワードを含む可能性を判定し、発言のラウドネススコアを演算する。近傍にある他のコンピューティングデバイスも、発言を受信し、発言がホットワードを含む可能性を判定し、発言のラウドネススコアを演算する。各コンピューティングデバイスはまた、遅延の長さがラウドネススコアに反比例するような、ラウドネススコアに基づいた遅延も計算する。発言源に最も近いコンピューティングデバイスは、通常、最も高いラウドネススコアを有するので、最も近いデバイスは、最も短い遅延を有する。所与のコンピューティングデバイスに関連付けられた遅延が経過した後で、それぞれのコンピューティングデバイスは、遅延期間中に通知信号を受信しない限り、他のコンピューティングデバイスに、通知信号を送信する。したがって、最も小さい遅延を有する(したがって、ユーザに最も近そうである)コンピューティングデバイスは、他のコンピューティングデバイスに信号を送信して、このコンピューティングデバイスがホットワードに続くさらなる音声を処理することを示す。この例では、送信を行うコンピューティングデバイスは、ホットワードに続くさらなる音声を処理する。もし、遅延中に、コンピューティングデバイスが、ホットワードに続くさらなる音声を別のデバイスが処理することを示す信号を他のデバイスの1つから受信すると、このコンピューティングデバイスは、音声の処理を中止する。 According to one innovative aspect of the subject matter described in this specification, a computing device receives a speech spoken by a user. The computing device determines the likelihood that the utterance includes a hot word and calculates the loudness score of the utterance. Other computing devices in the vicinity also receive the speech, determine the likelihood that the speech includes hot words, and compute the loudness score of the speech. Each computing device also calculates a delay based on the loudness score such that the length of the delay is inversely proportional to the loudness score. Since the computing device closest to the source of speech typically has the highest loudness score, the closest device has the shortest delay. After the delay associated with a given computing device has elapsed, each computing device transmits a notification signal to other computing devices unless it receives the notification signal during the delay period. Thus, the computing device with the smallest delay (and therefore most likely to be closest to the user) sends a signal to the other computing device to process further speech following the hot word. Indicates. In this example, the computing device performing the transmission processes additional speech following the hot word. If, during the delay, the computing device receives a signal from one of the other devices indicating that another device will process further audio following the hotword, the computing device will stop processing the audio. To do.
一般に、本明細書に記載される主題の別の革新的な面は、コンピューティングデバイスが発言に相当する音声データを受信するアクション、発言がホットワードを含む可能性を判定するアクション、音声データのラウドネススコアを判定するアクション、ラウドネススコアに基づいて遅延時間の量を判定するアクション、遅延時間の量が経過した後で、音声データの発話認識処理をコンピューティングデバイスが開始することを示す信号を送信するアクションを含む方法で具体化し得る。 In general, another innovative aspect of the subject matter described herein is that an action in which a computing device receives audio data corresponding to an utterance, an action to determine the likelihood that the utterance includes a hot word, Action to determine loudness score, action to determine the amount of delay time based on the loudness score, and send a signal indicating that the computing device will begin utterance recognition processing of voice data after the amount of delay time has elapsed Can be embodied in a way that includes actions to do.
これらの実施形態および他の実施形態は、各々、1つまたは複数の以下の特徴を、場合によっては含むことができる。アクションは、さらに、コンピューティングデバイスがさらなる発言に相当するさらなる音声データを受信するアクション、さらなる発言がホットワードを含む第2の可能性を判定するアクション、さらなる音声データの第2のラウドネススコアを判定するアクション、第2のラウドネススコアに基づいて第2の遅延時間の量を判定するアクション、ならびに遅延時間の量が経過する前に、(i)第2のコンピューティングデバイスが、さらなる音声データの発話認識処理を開始すること、および(ii)コンピューティングデバイスが、さらなる音声データの発話認識処理を開始すべきではないことを示す第2の信号を受信するアクションを含む。アクションは、第2の信号の受信に基づいて、コンピューティングデバイスの作動状態が活動停止状態であることを判定するアクションをさらに含む。 These and other embodiments can each optionally include one or more of the following features. The action further includes an action in which the computing device receives additional voice data corresponding to the further utterance, an action to determine a second possibility that the further utterance includes a hot word, and a second loudness score for the further voice data. An action to determine a second amount of delay based on a second loudness score, and (i) the second computing device utters further audio data before the amount of delay has elapsed. Initiating a recognition process, and (ii) an action in which the computing device receives a second signal indicating that the speech recognition process for further audio data should not be initiated. The action further includes an action of determining that the operational state of the computing device is inactive based on receipt of the second signal.
アクションは、信号の送信に基づいて、コンピューティングデバイスの作動状態が活動中状態であることを判定するアクションをさらに含む。信号は、超音波信号または短距離無線信号を含む。信号は、別のコンピューティングデバイスによって受信され、この別のコンピューティングデバイスに、音声データの発話認識処理を開始しないように示す。アクションは、発言がホットワードを含む可能性の判定に基づいて、音声データの発話認識処理を別のコンピューティングデバイスが開始することを示す信号を受信する準備をするアクションをさらに含む。ラウドネススコアは、遅延時間の量に比例する。ラウドネススコアが閾値を満たすとき、遅延時間はゼロである。音声データのラウドネススコアを判定するアクションは、発言がホットワードを含む可能性が、可能性閾値を満たすことを判定するアクションをさらに含む。 The action further includes an action of determining that the operational state of the computing device is active based on the transmission of the signal. The signal includes an ultrasonic signal or a short-range wireless signal. The signal is received by another computing device, indicating to the other computing device not to initiate the speech recognition process for voice data. The action further includes an action that prepares to receive a signal indicating that another computing device will begin utterance recognition processing of the voice data based on a determination that the utterance may include a hot word. The loudness score is proportional to the amount of delay time. When the loudness score meets the threshold, the delay time is zero. The action of determining the loudness score of the speech data further includes an action of determining that the likelihood that the utterance includes a hot word satisfies a likelihood threshold.
この態様の他の実施形態は、この方法の動作を行うように各々が構成された、対応するシステム、装置、およびコンピュータ記憶デバイスに記録されたコンピュータプログラムを含む。 Other embodiments of this aspect include computer programs recorded in corresponding systems, apparatuses, and computer storage devices, each configured to perform the operations of this method.
本明細書に記載される主題の特定の実施形態は、1つまたは複数の以下の利点を実現するように実施することができる。複数のデバイスが、ホットワードを検出することができ、1つのデバイスのみがホットワードに応答する。 Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. Multiple devices can detect a hot word and only one device responds to the hot word.
本明細書に記載される主題の1つまたは複数の実施形態の詳細は、添付の図面および以下の記載に記述する。主題の他の特徴、態様、および利点は、記載、図面、および特許請求の範囲から明らかになるであろう。 The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will be apparent from the description, drawings, and claims.
様々な図面の同じ参照番号および記号は、同じ要素を示す。 Like reference numbers and symbols in the various drawings indicate like elements.
本明細書は、ホットワードを聞くいくつものコンピューティングデバイスが、どのデバイスが応答すべきかを判定できるようにする、ラウドネスに基づいたプロトコルを記載する。有利には、プロトコルは、たとえば音声信号または短距離無線信号を用いて、ローカルなやり取りに基づいて実施することができ、いかなるサーバ側の通信も必要としないですむ。このことは、ローカルなやり取りはサーバと通信する待ち時間を発生させることなく応答の交渉を可能にすることができるので、有益である。 This document describes a loudness-based protocol that allows several computing devices listening to a hotword to determine which device should respond. Advantageously, the protocol can be implemented on the basis of local interactions, for example using voice signals or short-range radio signals, and does not require any server-side communication. This is beneficial because local interactions can allow responses to be negotiated without incurring latency to communicate with the server.
図1は、ホットワードを検出するシステム100の例を示す図である。一般に、システム100は、コンピューティングデバイス106、コンピューティングデバイス108、およびコンピューティングデバイス110のマイクによって検出される発言104を話すユーザ102を示す。コンピューティングデバイス106、コンピューティングデバイス108、およびコンピューティングデバイス110は、発言104を処理して、発言のラウドネススコアを判定するのに加え、発言104がホットワードを含む可能性を判定する。コンピューティングデバイス106、コンピューティングデバイス108、およびコンピューティングデバイス110は、ラウドネススコアに比例する遅延時間を計算する。コンピューティングデバイスの遅延時間の1つが経過すると、そのコンピューティングデバイスは、他のコンピューティングデバイスに信号を送信する。信号は、送信を行っているコンピューティングデバイスが、発言に相当する音声データの発話認識を行うことを、他のコンピューティングデバイスに示す。
FIG. 1 is a diagram illustrating an example of a
より詳細には、ユーザ102は、「オーケー、コンピュータ」という発言104を話し、コンピューティングデバイス106、コンピューティングデバイス108、およびコンピューティングデバイス110は、マイクなどのそれぞれの音声入力デバイスを通じて、発言104を受信する。コンピューティングデバイス106は、ユーザ102がユーザの手に持っている電話機である。コンピューティングデバイス108は、テーブル上に配置したタブレットである。コンピューティングデバイス110は、壁に配置したサーモスタットである。コンピューティングデバイス106は、ユーザに最も近く、コンピューティングデバイス108が次に近く、最後に、コンピューティングデバイス110がユーザから最も遠い。コンピューティングデバイス106、コンピューティングデバイス108、およびコンピューティングデバイス110はそれぞれ、マイク114、マイク116、およびマイク118を含む。それぞれのマイクは、それぞれの音声サブシステムに音声データを提供する。音声サブシステムは、マイクから受信した音声データを、バッファリングし、フィルタリングし、デジタル化する。いくつかの実装形態で、各コンピューティングデバイスは、音声データのエンドポインティングおよび話者識別も行い得る。いくつかの実装形態で、コンピューティングデバイス106、コンピューティングデバイス108、およびコンピューティングデバイス110は、図3に関して以下で記載するデバイスなどの、音声データを受信し処理することができる任意のデバイスであり得る。
More specifically, the
各音声サブシステムは、ホットワーダ(hotworder)120、ホットワーダ122、またはホットワーダ124などのホットワーダに、処理済みの音声データを提供する。それぞれのホットワーダは、既知のホットワードの発言を用いて知らされ得るまたは訓練され得る処理済みの音声データの分類処理を行う。それぞれのホットワーダは、発言104がホットワードを含む可能性を演算する。それぞれのホットワーダは、処理済みの音声データから、フィルタバンクエネルギーまたはメル周波数ケプストラム係数などの音声の特徴を抽出し得る。それぞれのホットワーダは、サポートベクターマシーンまたはニューラルネットワークを用いるなどしてこれらの音声の特徴を処理する分類ウィンドウを使用し得る。音声の特徴の処理に基づいて、それぞれのホットワーダは、発言104がホットワードを含む可能性を演算する。いくつかの実装形態で、可能性は、信頼性スコアに反映され得る。信頼性スコアは、1から100段階の評価に正規化され得る。信頼性スコアは、数値が大きいほど、発言104がホットワードを含む信頼性が高いことを示す。
Each audio subsystem provides processed audio data to a hot worder such as a hot worder 120,
いくつかの実装形態で、それぞれのホットワーダは、信頼性スコアを閾値と比較する。信頼性スコアが閾値を満たすと、コンピューティングデバイスは、音声データの処理を続行する。信頼性スコアが閾値を満たさないと、コンピューティングデバイスは、音声データの処理を中止する。たとえば、信頼性スコアが0.8で、閾値が0.7なら、コンピューティングデバイスは、音声データの処理を続行する。音声データが「オーケー、コンピュータ」に相当するとき、信頼性スコアは0.8であり得る。信頼性スコアが0.5で、閾値が0.7なら、コンピューティングデバイスは、音声データの処理を中止する。音声データが「夕食の時間」に相当するとき、信頼性スコアは0.5であり得る。 In some implementations, each hotworder compares the confidence score to a threshold value. If the confidence score meets the threshold, the computing device continues to process the audio data. If the reliability score does not meet the threshold, the computing device stops processing the audio data. For example, if the reliability score is 0.8 and the threshold is 0.7, the computing device continues to process the audio data. When the voice data corresponds to “OK, computer”, the reliability score may be 0.8. If the reliability score is 0.5 and the threshold is 0.7, the computing device stops processing the audio data. When the audio data corresponds to “dinner time”, the reliability score may be 0.5.
ホットワーダは、音声データをラウドネススコアラに提供する。ラウドネススコアラは、音声データのラウドネススコアを演算する。通常、ラウドネススコアは、各コンピューティングデバイスからのユーザの距離の反映である。たとえば、ラウドネススコアラ126が演算するコンピューティングデバイス106のラウドネススコアは、0.9であり得る。ラウドネススコアラ128が演算するコンピューティングデバイス108のラウドネススコアは、0.6であり得る。ラウドネススコアラ130が演算するコンピューティングデバイス110のラウドネススコアは、0.5であり得る。いくつかの実装形態で、音声サブシステムは、ラウドネススコアラに音声データを提供する。この例では、ホットワーダは、ラウドネススコアラがラウドネススコアを演算するのと並行して、発言104がホットワードを含む可能性を演算し得る。
The hot worder provides audio data to the loudness scorer. The loudness scorer calculates the loudness score of the audio data. Typically, the loudness score is a reflection of the user's distance from each computing device. For example, the loudness score of the
ラウドネススコアラは、たとえば以下の技術の1つまたは組合せである、ラウドネスに比例する値を生成する任意の適切な技術を用いて、音声データのラウドネスを演算し得る。1つの技術は、ユーザが発言をしたときのマイクが受信する最大音圧もしくは平均音圧、または最大音圧レベルもしくは平均音圧レベルを計算する技術であり得る。音圧または音圧レベルが高いほど、ラウドネスは大きい。第2の技術は、音声データの二乗平均平方根を計算する技術である。音声データの二乗平均平方根が大きいほど、ラウドネスは大きい。第3の技術は、音声データの音の強度を計算する技術である。音声データの音の強度が強いほど、ラウドネスは大きい。第4の技術は、音声データの音力を計算する技術である。音力が強いほど、ラウドネスは大きい。 The loudness scorer may compute the loudness of the audio data using any suitable technique that produces a value proportional to loudness, for example, one or a combination of the following techniques. One technique may be a technique that calculates the maximum or average sound pressure received by the microphone when the user speaks, or the maximum or average sound pressure level. The higher the sound pressure or sound pressure level, the greater the loudness. The second technique is a technique for calculating the root mean square of audio data. The loudness increases as the root mean square of the audio data increases. The third technique is a technique for calculating the sound intensity of audio data. The louder the sound data, the greater the loudness. The fourth technique is a technique for calculating the sound power of voice data. The louder the sound, the greater the loudness.
ラウドネススコアラは、遅延計算モジュールに、ラウドネススコアを提供する。ラウドネススコアに基づいて、モジュールは、音声データをさらに処理し他のコンピューティングデバイスにコンピューティングデバイスが音声データを処理することを通知する前にコンピューティングデバイスが待機するべき遅延時間を計算する。たとえば、ラウドネススコアラ126は、遅延計算モジュール132に、ラウドネススコア0.9を提供し、モジュール132は、遅延時間50ミリ秒を演算する。ラウドネススコアラ128は、遅延計算モジュール134に、ラウドネススコア0.6を提供し、遅延計算モジュール134は、遅延時間200ミリ秒を演算する。ラウドネススコアラ130は、遅延計算モジュール136に、ラウドネススコア0.5を提供し、遅延計算モジュール136は、遅延時間250ミリ秒を演算する。
The loudness scorer provides a loudness score to the delay calculation module. Based on the loudness score, the module calculates a delay time that the computing device should wait before further processing the audio data and notifying other computing devices that the computing device is processing the audio data. For example, the
次いで、タイマーは、遅延時間をカウントダウンし、遅延時間が経過すると、タイマーによって、コンピューティングデバイスは、コンピューティングデバイスのスピーカに、スピーカが通知信号を発するように信号を送信する。タイマーは、遅延計算モジュールに含まれてもよく、またはタイマーは、遅延計算モジュールから分離してもよい。通知信号は、たとえば、超音波、可聴音声信号、またはブルートゥース(登録商標)などの短距離無線信号であり得る。通知信号は、他のコンピューティングデバイスによって受信され、通知信号を発したコンピューティングデバイスが音声データの処理を扱うことを、他のコンピューティングデバイスに示す。たとえば、遅延計算モジュール132は、スピーカ138に通知信号を発するよう命令する前に、50ミリ秒待機する。コンピューティングデバイス108およびコンピューティングデバイス110は、それぞれのタイマーがそれぞれの遅延時間をカウントダウンし終える前に、通知信号を受信するので、コンピューティングデバイス108およびコンピューティングデバイス110は、音声データの処理を中止し、スピーカ140およびスピーカ142は、通知信号を発さない。いくつかの実装形態で、通知信号は、特定の周波数および/またはパターンを含み得る。たとえば、通知信号は、20キロヘルツであって、コンピューティングデバイスが音声データの発話認識を行うことを示し得る。
The timer then counts down the delay time, and when the delay time elapses, the timer causes the computing device to send a signal to the computing device speaker such that the speaker issues a notification signal. The timer may be included in the delay calculation module, or the timer may be separated from the delay calculation module. The notification signal can be, for example, an ultrasonic wave, an audible audio signal, or a short-range wireless signal such as Bluetooth. The notification signal is received by the other computing device to indicate to the other computing device that the computing device that issued the notification signal handles processing of the audio data. For example, the
いくつかの実装形態で、通知信号を発すると、コンピューティングデバイス106は、そのデバイス状況138も設定し得る。コンピューティングデバイス106は、音声データを処理するデバイスであるので、デバイス状況138は、活動中または「アウェイク」に設定される。コンピューティングデバイス108およびコンピューティングデバイス110のデバイス状況140およびデバイス状況142は、どちらのデバイスも音声データのさらなる処理を行っていないので、活動停止または「スリープ」に設定される。
In some implementations, when issuing a notification signal, the
いくつかの実装形態で、コンピューティングデバイスのデバイス状況は、影響を受けないこともある。最も大きい音声データを測定し、そして、通知信号を最も早く発するコンピューティングデバイスは、他のコンピューティングデバイスがアウェイクのままでいるまたはアウェイク状態を入力する一方で、音声データのさらなる処理を開始し得る。たとえば、ユーザ102は、コンピューティングデバイス108で映画を視聴しながら、ユーザの手にコンピューティングデバイス106を持ち得る。ユーザ102が、「オーケー、コンピュータ」と話すと、コンピューティングデバイス106は、音声データを検出し、50ミリ秒後に、通知信号を発して、コンピューティングデバイス106が音声データをさらに処理することを示す。コンピューティングデバイス108は、通知信号を受信し、映画の上映を続行する。
In some implementations, the device status of the computing device may not be affected. The computing device that measures the largest amount of audio data and issues the notification signal earliest may initiate further processing of the audio data while other computing devices remain awake or enter an awake state . For example, the
いくつかの実装形態で、コンピューティングデバイスは、通知信号を発する前または受信する前に、音声データの発話認識を開始し得る。通知信号を発する前または受信する前に認識を行うことによって、コンピューティングデバイスは、発言の発話認識を迅速に完了することができる。たとえば、発言が、「オーケー、コンピュータ、アリスを呼んで」であれば、コンピューティングデバイスは、コンピューティングデバイスが活動中状態のままであるならばより早く応答できるように、ユーザが「アリスを呼んで」と話したことの認識を開始することができる。コンピューティングデバイスのデバイス状況が活動停止なら、コンピューティングデバイスは、「アリスを呼んで」の発話認識を行ったという表示をしなくてもよい。図1で、この技術を用いると、コンピューティングデバイス106、コンピューティングデバイス108、およびコンピューティングデバイス110は、音声データおよび音声データに続く任意の音声データの発話認識を行う。スピーカ138が通知信号を送信するとき、コンピューティングデバイス106は、発話認識を続行し、結果を表示する。マイク116およびマイク118が通知信号を受信すると、コンピューティングデバイス108およびコンピューティングデバイス110は、発話認識を中止し、結果を表示しない。コンピューティングデバイス108およびコンピューティングデバイス110は活動停止状態のままであるようにユーザ102には見える。
In some implementations, the computing device may initiate speech recognition of voice data before issuing or receiving a notification signal. By performing the recognition before issuing or receiving the notification signal, the computing device can quickly complete the speech recognition of the speech. For example, if the utterance is “Okay, computer, call Alice”, the computing device can respond more quickly if the computing device remains active. You can start recognizing what you said. If the device status of the computing device is inactive, the computing device may not display that “call Alice” utterance recognition has been performed. In FIG. 1, using this technology,
図2は、ホットワードを検出するプロセス200の例を示す図である。プロセス200は、図1のコンピューティングデバイス108などのコンピューティングデバイスによって行われ得る。プロセス200は、(i)発言がホットワードを含む可能性、および(ii)発言に相当する音声データのラウドネススコアを演算する。プロセス200は、コンピューティングデバイスが音声データを処理することを他のコンピューティングデバイスに通知する前に、コンピューティングデバイスが待機する遅延時間を演算する。
FIG. 2 is a diagram illustrating an example of a process 200 for detecting hot words. Process 200 may be performed by a computing device such as
コンピューティングデバイスは、発言に相当する音声データを受信する(210)。ユーザは、発言を行い、コンピューティングデバイスのマイクは、発言の音声データを受信する。コンピューティングデバイスは、音声データをバッファリング、フィルタリング、エンドポインティング、およびデジタル化することによって、音声データを処理する。例として、ユーザは、「オーケー、コンピュータ」と発言してもよく、コンピューティングデバイスのマイクは、「オーケー、コンピュータ」に相当する音声データを受信する。コンピューティングデバイスの音声サブシステムは、コンピューティングデバイスによるさらなる処理を行うため、音声データをサンプリング、バッファリング、フィルタリング、およびエンドポインティングする。 The computing device receives audio data corresponding to the speech (210). The user speaks, and the microphone of the computing device receives the voice data of the speech. A computing device processes voice data by buffering, filtering, endpointing, and digitizing the voice data. As an example, a user may say “Okay, computer” and the microphone of the computing device receives audio data corresponding to “Okay, computer”. The audio subsystem of the computing device samples, buffers, filters, and endpoints the audio data for further processing by the computing device.
コンピューティングデバイスは、発言がホットワードを含む可能性を判定する(220)。コンピューティングデバイスは、発言の音声データをホットワードを含む一群の音声サンプルと比較することにより、および/または、発言の音声データの音声の特徴を分析することにより、発言がホットワードを含む可能性を判定する。いくつかの実装形態で、発言がホットワードを含む可能性は、信頼性スコアによって表され得る。信頼性スコアは、1から100段階の評価に正規化され得る。100は、発言がホットワードを含む可能性が最も高いことを示す。コンピューティングデバイスは、信頼性スコアと閾値を比較し得る。信頼性スコアが閾値を満たすなら、コンピューティングデバイスは、音声データの処理を続行する。信頼性スコアが閾値を満たさないなら、コンピューティングデバイスは、音声データの処理を中止する。いくつかの実装形態で、コンピューティングデバイスが処理を続行するには、信頼性スコアは閾値より高くあるべきである。たとえば、信頼性スコアが0.9であり、閾値が0.7であれば、コンピューティングデバイスは、音声データの処理を続行する。 The computing device determines the likelihood that the statement includes a hot word (220). A computing device may compare speech speech data to a group of speech samples that contain hot words and / or analyze speech features of speech speech data so that speech may contain hot words. Determine. In some implementations, the likelihood that a statement includes a hot word may be represented by a confidence score. The confidence score can be normalized to a 1 to 100 grade rating. 100 indicates that the statement is most likely to contain a hot word. The computing device may compare the confidence score with a threshold value. If the reliability score meets the threshold, the computing device continues processing the audio data. If the reliability score does not meet the threshold, the computing device stops processing the audio data. In some implementations, the reliability score should be above the threshold for the computing device to continue processing. For example, if the reliability score is 0.9 and the threshold is 0.7, the computing device continues processing the audio data.
いくつかの実装形態で、コンピューティングデバイスは、音声データの発話認識処理を別のコンピューティングデバイスが開始することを示す信号を受信する準備をする。信号を受信するのに、コンピューティングデバイスは、コンピューティングデバイスのマイクが活動中のままであること、短距離無線受信機が活動中であること、または、ローカルエリアワイヤレス無線などの別の無線が活動中であることを確保し得る。コンピューティングデバイスは、別のコンピューティングデバイスが結果を表示するとき、コンピューティングデバイスが音声データの発話認識から結果を表示しないように、信号を受信する準備をすることが必要であり得る。 In some implementations, the computing device prepares to receive a signal indicating that another computing device will begin the speech recognition process for voice data. To receive the signal, the computing device must have a computing device microphone active, a short-range radio receiver active, or another radio such as a local area wireless radio. You can ensure that you are working. A computing device may need to be prepared to receive a signal so that when another computing device displays the result, the computing device does not display the result from speech recognition of voice data.
コンピューティングデバイスは、音声データのラウドネススコアを判定する(230)。コンピューティングデバイスは、以下の技術の1つまたは組合せを用いて、音声データのラウドネススコアを計算し得る。1つの技術は、ユーザが発言をしているときにマイクから受信する音圧または音圧レベルを計算することであり得る。音圧または音圧レベルが高いほど、ラウドネスは大きい。第2の技術は、音声データの二乗平均平方根を計算することである。音声データの二乗平均平方根の値が大きいほど、ラウドネスは大きい。第3の技術は、音声データの音の強度を計算することである。音声データの音の強度が強いほど、ラウドネスは大きい。第4の技術は、音声データの音力を計算することである。音力が強いほど、ラウドネスは大きい。コンピューティングデバイスが受信する音声データのラウドネスは、コンピューティングデバイスと音声源の距離を反映し得る。直接経路での信号伝搬の場合、ラウドネスは、源と受信機の間の距離の2乗にほぼ反比例する。いくつかの実装形態で、コンピューティングデバイスは、発言がホットワードを含む可能性が閾値を満たす場合のみ、ラウドネススコアを演算する。発言がホットワードを含みそうにないなら、コンピューティングデバイスは、ラウドネススコアを演算しない。 The computing device determines a loudness score of the audio data (230). The computing device may calculate the loudness score of the audio data using one or a combination of the following techniques. One technique may be to calculate the sound pressure or sound pressure level received from the microphone when the user is speaking. The higher the sound pressure or sound pressure level, the greater the loudness. The second technique is to calculate the root mean square of audio data. The loudness increases as the root mean square value of the audio data increases. The third technique is to calculate the sound intensity of the audio data. The louder the sound data, the greater the loudness. The fourth technique is to calculate the sound power of audio data. The louder the sound, the greater the loudness. The loudness of the audio data received by the computing device may reflect the distance between the computing device and the audio source. In the case of signal propagation in the direct path, the loudness is approximately inversely proportional to the square of the distance between the source and the receiver. In some implementations, the computing device computes the loudness score only if the likelihood that the statement includes a hot word meets a threshold. If the statement is unlikely to contain a hot word, the computing device does not compute a loudness score.
コンピューティングデバイスは、ラウドネススコアに基づいて、遅延時間の量を判定する(240)。いくつかの実装形態で、遅延時間は、ラウドネススコアに反比例する。たとえば、ラウドネススコア0.9は、遅延時間50ミリ秒に相当し、ラウドネススコア0.6は、遅延時間200ミリ秒に相当し得る。いくつかの実装形態で、ラウドネススコアが閾値を満たさないなら、遅延時間はない。すなわち、コンピューティングデバイスは、音声信号の処理を中止し、いかなる時も通知を送信しない。いくつかの実装形態で、ラウドネスが閾値を超えるなら、遅延時間はゼロとなり、これはつまり、対応するコンピューティングデバイスは、音声信号の処理を続行し、他のデバイスに通知信号を直ちに送出することを意味する。これらの閾値は、たとえば実験によって経験的に、任意の適切な方法を用いて決定し得る。 The computing device determines the amount of delay time based on the loudness score (240). In some implementations, the delay time is inversely proportional to the loudness score. For example, a loudness score of 0.9 may correspond to a delay time of 50 milliseconds, and a loudness score of 0.6 may correspond to a delay time of 200 milliseconds. In some implementations, if the loudness score does not meet the threshold, there is no delay time. That is, the computing device stops processing the audio signal and does not send a notification at any time. In some implementations, if the loudness exceeds the threshold, the delay time is zero, which means that the corresponding computing device continues processing the audio signal and immediately sends a notification signal to other devices. Means. These thresholds may be determined using any suitable method, for example empirically, by experimentation.
コンピューティングデバイスは、遅延時間の量が経過した後で、音声データの発話認識処理をコンピューティングデバイスが開始することを示す通知信号を送信する(250)。コンピューティングデバイスがいったん遅延時間を演算すると、コンピューティングデバイスのタイマーは、遅延時間をカウントダウンする。遅延時間が経過すると、コンピューティングデバイスは、近くにある他のコンピューティングデバイスに、超音波、短距離無線信号、またはローカルエリアワイヤレス信号などの信号を送信して、コンピューティングデバイスが音声データの発話認識処理を開始していることを示す。たとえば、コンピューティングデバイスは、音声データを受信した後、活動中状態または「アウェイク」状態であり、他のコンピューティングデバイスは、信号を受信した後、活動停止状態または「スリープ」状態である。 After the amount of delay time has elapsed, the computing device transmits a notification signal indicating that the computing device starts speech recognition processing for voice data (250). Once the computing device calculates the delay time, the computing device timer counts down the delay time. When the delay time elapses, the computing device sends a signal, such as an ultrasound, a short-range radio signal, or a local area wireless signal, to other nearby computing devices, and the computing device utters voice data Indicates that the recognition process has started. For example, a computing device is in an active state or “awake” state after receiving voice data, and another computing device is in an inactive state or “sleep” state after receiving a signal.
いくつかの実装形態で、コンピューティングデバイスは、別のコンピューティングデバイスから、音声データの発話認識処理を別のコンピューティングデバイスが開始することを示す信号を受信する。この例では、コンピューティングデバイスは、タイマーが遅延時間をカウントダウンしている間に、信号を受信する。コンピューティングデバイスが信号を受信すると、コンピューティングデバイスは、音声データの発話認識を行わない、または、続行しない。たとえば、コンピューティングデバイスが遅延時間200ミリ秒を演算し、コンピューティングデバイスのタイマーが200ミリ秒のカウントダウンをしている間に、コンピューティングデバイスが、別のコンピューティングデバイスから通知信号を受信したなら、コンピューティングデバイスは、音声データの発話認識を行わない。コンピューティングデバイスは、信号を受信した後、活動停止状態または「スリープ」状態になり得る。 In some implementations, the computing device receives a signal from another computing device indicating that the other computing device initiates speech recognition processing for voice data. In this example, the computing device receives the signal while the timer counts down the delay time. When the computing device receives the signal, the computing device does not perform speech recognition of the voice data or does not continue. For example, if a computing device receives a notification signal from another computing device while the computing device computes a delay of 200 milliseconds and the computing device timer counts down to 200 milliseconds The computing device does not perform speech recognition of voice data. The computing device may be in an inactive state or “sleep” state after receiving the signal.
いくつかの実装形態で、コンピューティングデバイスは、近くにあり、ホットワードに応答できる他のコンピューティングデバイスを検出する。コンピューティングデバイスは、応答を要求する超音波または無線信号を周期的に送信し得る。たとえば、コンピューティングデバイスは、ホットワードを認識する近くのコンピューティングデバイスを検索するとき、20.5キロヘルツの超音波を送信し得る。それに応じて、20.5キロヘルツの超音波を受信するコンピューティングデバイスは、21キロヘルツの超音波で応答し得る。コンピューティングデバイスが、ホットワードを認識する近くのコンピューティングデバイスを検出しないとき、コンピューティングデバイスは、音声データの発話認識をする前に、ラウドネススコアおよび遅延時間の演算をしなくてもよい。 In some implementations, the computing device detects other computing devices that are nearby and can respond to the hotword. The computing device may periodically transmit ultrasound or wireless signals that require a response. For example, a computing device may transmit 20.5 kilohertz ultrasound when searching for nearby computing devices that recognize the hot word. Accordingly, a computing device that receives 20.5 kilohertz ultrasound may respond with 21 kilohertz ultrasound. When the computing device does not detect a nearby computing device that recognizes the hot word, the computing device may not calculate the loudness score and the delay time before performing speech recognition of the speech data.
いくつかの実装形態で、コンピューティングデバイスは、同一のユーザに属する他のコンピューティングデバイスを識別し得る。新しいコンピューティングデバイスを設定するとき、設定手順の一部に、同一のユーザに属する他のコンピューティングデバイスを識別することがあり得る。これは、ユーザがログインしている他のデバイスを検出することによって達成し得る。コンピューティングデバイスが、別のコンピューティングデバイスを識別すると、コンピューティングデバイス同士は、ホットワードを識別するときコンピューティングデバイス同士が交換できる超音波周波数パターンまたはビットストリームを示すデータを交換し得る。超音波周波数パターンは、スピーカを通じて送信され、ビットストリームは、無線を通じて送信され得る。たとえば、ユーザは、サーモスタットを設定してもよく、設定プロセスの一部は、近くにありホットワードに応答する他のコンピューティングデバイスを検索することである。サーモスタットは、ユーザがログインしている電話機およびタブレットを識別し得る。例として、サーモスタット、電話機、およびタブレットは、20.5キロヘルツで1ミリ秒、21キロヘルツで1ミリ秒、および21.5キロヘルツで1ミリ秒の傾斜周波数パターンを用いてデータを交換し得る。パターンを用いることにより、音声データの発話認識処理を開始するコンピューティングデバイスは、ユーザに属する他のコンピューティングデバイスに通知ができるようになり、別のユーザに属し得る他のデバイスでの発話認識をやめさせずにすむ。 In some implementations, the computing device may identify other computing devices that belong to the same user. When configuring a new computing device, part of the configuration procedure may identify other computing devices belonging to the same user. This may be accomplished by detecting other devices that the user is logged into. When a computing device identifies another computing device, the computing devices may exchange data indicating an ultrasonic frequency pattern or bitstream that the computing devices can exchange when identifying a hot word. The ultrasonic frequency pattern can be transmitted through a speaker and the bitstream can be transmitted over the air. For example, a user may set up a thermostat and part of the setup process is to search for other computing devices that are nearby and respond to hotwords. The thermostat may identify the phone and tablet that the user is logged into. By way of example, thermostats, phones, and tablets may exchange data using ramp frequency patterns of 20.5 kilohertz at 1 millisecond, 21 kilohertz at 1 millisecond, and 21.5 kilohertz at 1 millisecond. By using a pattern, a computing device that starts speech recognition processing of voice data can notify other computing devices that belong to the user, and can recognize speech recognition on other devices that can belong to another user. Don't stop.
図3は、本明細書に記載される技術を実施するのに用いることができるコンピューティングデバイス300および携帯型コンピューティングデバイス350の例を示す。コンピューティングデバイス300は、ラップトップ、デスクトップ、ワークステーション、携帯情報端末、サーバ、ブレードサーバ、メインフレーム、および他の適切なコンピュータなどの様々な形態のデジタルコンピュータを表すことを意図している。携帯型コンピューティングデバイス350は、携帯情報端末、携帯電話機、スマートフォン、および他の同様のコンピューティングデバイスなどの様々な形態の携帯型デバイスを表すことを意図している。本明細書で示す構成要素、それらの接続および関係、ならびにそれらの機能は、例としてのみ示されており、これらに限定されない。
FIG. 3 illustrates an example of a
コンピューティングデバイス300は、プロセッサ302、メモリ304、記憶デバイス306、メモリ304および複数の高速拡張ポート310に接続する高速インタフェース308、ならびに低速拡張ポート314および記憶デバイス306に接続する低速インタフェース312を含む。プロセッサ302、メモリ304、記憶デバイス306、高速インタフェース308、高速拡張ポート310、および低速インタフェース312はそれぞれ、様々なバスを用いて相互接続され、共通のマザーボード上にまたは適宜他のやり方で搭載され得る。プロセッサ302は、コンピューティングデバイス300内で実行する命令を処理することができる。命令は、メモリ304または記憶デバイス306に記憶されて、高速インタフェース308に結合するディスプレイ316などの外部入出力デバイスにGUIのグラフィカルな情報を表示する命令を含む。他の実装形態で、複数のプロセッサおよび/または複数のバスは、複数のメモリおよび複数の種類のメモリとともに、適宜使用され得る。また、複数のコンピューティングデバイスは、各デバイスが必要な動作部分を提供している状態で(たとえば、サーババンク、一群のブレードサーバ、またはマルチプロセッサシステムとして)、接続し得る。
The
メモリ304は、コンピューティングデバイス300内に情報を記憶する。いくつかの実装形態で、メモリ304は、1つまたは複数の揮発性メモリ装置である。いくつかの実装形態で、メモリ304は、1つまたは複数の不揮発性メモリ装置である。メモリ304は、磁気ディスクまたは光学ディスクなどの別の形式のコンピュータ可読媒体でもあり得る。
記憶デバイス306は、コンピューティングデバイス300に大容量記憶をもたらすことができる。いくつかの実装形態で、記憶デバイス306は、フロッピー(登録商標)ディスクデバイス、ハードディスクデバイス、光学ディスクデバイス、またはテープデバイス、フラッシュメモリまたは他の同様の半導体メモリデバイス、または、ストレージエリアネットワーク内または他の構成内のデバイスを含むデバイスの配列などのコンピュータ可読媒体であってもよく、またはそのようなコンピュータ可読媒体を含んでもよい。命令は、情報担体に記憶することができる。命令は、1つまたは複数の処理デバイス(たとえば、プロセッサ302)によって実行されると、上記の方法などの1つまたは複数の方法を行う。命令は、コンピュータ可読媒体または機械可読媒体(たとえば、メモリ304、記憶デバイス306、またはプロセッサ302のメモリ)などの1つまたは複数の記憶デバイスによっても記憶することができる。
高速インタフェース308は、コンピューティングデバイス300の帯域幅集中動作を管理し、低速インタフェース312は、低帯域幅集中動作を管理する。そのような機能の割当ては、ただの例にすぎない。いくつかの実装形態で、高速インタフェース308は、メモリ304、ディスプレイ316に(たとえば、グラフィックプロセッサまたはグラフィックアクセラレータを通じて)結合され、様々な拡張カード(図示せず)を受け入れ得る高速拡張ポート310に結合される。実装形態で、低速インタフェース312は、記憶デバイス306および低速拡張ポート314に結合される。様々な通信ポート(たとえば、USB、ブルートゥース(登録商標)、イーサネット(登録商標)、ワイヤレスイーサネット(登録商標))を含み得る低速拡張ポート314は、キーボード、ポインティングデバイス、スキャナなどの1つまたは複数の入出力デバイスに、または、たとえばネットワークアダプタを通じて、スイッチまたはルータなどのネットワークデバイスに結合され得る。
The
コンピューティングデバイス300は、図に示すように、いくつもの異なる形式で実施し得る。たとえば、コンピューティングデバイス300は、スタンダードサーバ320として、またはそのようなサーバのグループで複数倍で実施し得る。さらに、コンピューティングデバイス300は、ラップトップコンピュータ322などのパーソナルコンピュータで実施し得る。コンピューティングデバイス300は、ラックサーバシステム324の一部としても実施し得る。あるいは、コンピューティングデバイス300の構成要素は、携帯型コンピューティングデバイス350などの携帯型デバイス(図示せず)の他の構成要素と組み合わされ得る。そのようなデバイスは各々、1つまたは複数のコンピューティングデバイス300および携帯型コンピューティングデバイス350を含んでもよく、システム全体は、互いに通信する複数のコンピューティングデバイスから構成されてもよい。
The
携帯型コンピューティングデバイス350は、他の構成要素の中で、プロセッサ352、メモリ364、ディスプレイ354などの入出力デバイス、通信インタフェース366、およびトランシーバ368を含む。携帯型コンピューティングデバイス350は、さらなるストレージを提供するマイクロドライブデバイスまたは他のデバイスなどの記憶デバイスも備え得る。プロセッサ352、メモリ364、ディスプレイ354、通信インタフェース366、およびトランシーバ368の各々は、様々なバスを用いて相互接続され、いくつかの構成要素は、共通のマザーボード上にまたは適宜他のやり方で搭載され得る。
プロセッサ352は、メモリ364に記憶された命令を含む、携帯型コンピューティングデバイス350内の命令を実行することができる。プロセッサ352は、別個の複数のアナログプロセッサおよびデジタルプロセッサを含むチップのチップセットとして実施され得る。プロセッサ352は、たとえば、ユーザインタフェースの制御、携帯型コンピューティングデバイス350によって実行されるアプリケーションの制御、および携帯型コンピューティングデバイス350によるワイヤレス通信の制御などの、携帯型コンピューティングデバイス350の他の構成要素の協働を提供し得る。
The
プロセッサ352は、ディスプレイ354に結合された制御インタフェース358およびディスプレイインタフェース356を通じて、ユーザと通信し得る。ディスプレイ354は、たとえば、TFT(薄膜トランジスタ液晶ディスプレイ)ディスプレイまたはOLED(有機発光ダイオード)ディスプレイ、または他の適切なディスプレイ技術であり得る。ディスプレイインタフェース356は、ディスプレイ354を駆動する適切な回路を備え、ユーザにグラフィカルな情報および他の情報を提示し得る。制御インタフェース358は、ユーザからコマンドを受け、そのコマンドを、プロセッサ352に送信するために変換し得る。さらに、外部インタフェース362は、携帯型コンピューティングデバイス350が他のデバイスと近接エリア通信を行えるように、プロセッサ352との通信を提供し得る。外部インタフェース362は、たとえば、いくつかの実装形態で、有線通信を提供してもよく、他の実装形態で、ワイヤレス通信を提供してもよく、複数のインタフェースを使用してもよい。
The
メモリ364は、携帯型コンピューティングデバイス350内に情報を記憶する。メモリ364は、1つもしくは複数のコンピュータ可読媒体、1つもしくは複数の揮発性メモリ装置、または1つもしくは複数の不揮発性メモリ装置の内の1つまたは複数として実施することができる。拡張メモリ374も提供されて、たとえば、SIMM(シングルインラインメモリモジュール)カードインタフェースを含み得る拡張インタフェース372を通じて、携帯型コンピューティングデバイス350に接続し得る。拡張メモリ374は、携帯型コンピューティングデバイス350に余分のストレージ空間を提供してもよく、または、携帯型コンピューティングデバイス350のアプリケーションまたは他の情報も記憶してもよい。具体的には、拡張メモリ374は、上記の処理を実行または補完する命令を含んでもよく、またセキュア情報を含んでもよい。したがって、たとえば、拡張メモリ374は、携帯型コンピューティングデバイス350のセキュリティモジュールとして提供されてもよく、携帯型コンピューティングデバイス350の安全な使用を可能にする命令がプログラムされ得る。さらに、セキュアアプリケーションは、ハッキング不能なやり方でSIMMカードに識別情報を配置するなど、さらなる情報とともにSIMMカードを介して提供され得る。
メモリは、以下に述べるように、たとえばフラッシュメモリおよび/またはNVRAMメモリ(不揮発性ランダムアクセスメモリ)を含み得る。いくつかの実装形態で、命令は、情報担体に記憶される。命令は、1つまたは複数の処理デバイス(たとえば、プロセッサ352)によって実行されると、上記の方法などの1つまたは複数の方法を行う。命令は、1つまたは複数のコンピュータ可読媒体または機械可読媒体(たとえば、メモリ364、拡張メモリ374、またはプロセッサ352のメモリ)などの1つまたは複数の記憶デバイスによっても記憶することができる。いくつかの実装形態で、命令は、たとえばトランシーバ368または外部インタフェース362を介して、伝播された信号で受信することができる。
The memory may include, for example, flash memory and / or NVRAM memory (nonvolatile random access memory), as described below. In some implementations, the instructions are stored on an information carrier. The instructions, when executed by one or more processing devices (eg, processor 352), perform one or more methods, such as those described above. The instructions can also be stored by one or more storage devices, such as one or more computer-readable or machine-readable media (eg,
携帯型コンピューティングデバイス350は、デジタル信号処理回路を必要に応じて含み得る通信インタフェース366を通じてワイヤレスに通信し得る。通信インタフェース366は、他のモードまたはプロトコルの中で、GSM(登録商標)ボイスコール(グローバルシステムフォーモバイルコミュニケーションズ)、SMS(ショートメッセージサービス)、EMS(拡張メッセージングサービス)、またはMMSメッセージング(マルチメディアメッセージングサービス)、CDMA(符号分割多元接続)、TDMA(時分割多元接続)、PDC(パーソナルデジタルセルラー)、WCDMA(登録商標)(広帯域符号分割多元接続)、CDMA2000、またはGPRS(汎用パケット無線サービス)などの様々なモードまたはプロトコル下の通信を提供し得る。そのような通信は、たとえば、トランシーバ368を通じて、無線周波数を用いて行い得る。さらに、短距離通信は、ブルートゥース(登録商標)、WiFi、または他のそのようなトランシーバ(図示せず)を用いるなどして行い得る。さらに、GPS(全地球測位システム)受信機モジュール370は、携帯型コンピューティングデバイス350に、携帯型コンピューティングデバイス350で実行されるアプリケーションによって適宜使用され得るさらなるナビゲーション関連およびロケーション関連のワイヤレスデータを提供し得る。
携帯型コンピューティングデバイス350は、話された情報をユーザから受信し使用可能なデジタル情報に変換し得るオーディオコーデック360を用いて、可聴通信もし得る。オーディオコーデック360は、同様に、たとえば携帯型コンピューティングデバイス350のハンドセットにおけるスピーカを通じるなどして、ユーザに対して可聴音を生成し得る。そのような音は、音声電話コールからの音を含んでもよく、記録された音(たとえば、ボイスメッセージ、音楽ファイルなど)を含んでもよく、また携帯型コンピューティングデバイス350で作動するアプリケーションによって生成された音を含んでもよい。
The
携帯型コンピューティングデバイス350は、図に示すように、いくつもの異なる形式で実施され得る。たとえば、携帯電話380として実施され得る。また、スマートフォン382、携帯情報端末、または他の同様の携帯デバイスの一部としても実施され得る。
The
本明細書で記載するシステムおよび技術の様々な実装形態は、デジタル電子回路、集積回路、特別に設計されたASIC(特定用途向け集積回路)、コンピュータハードウェア、ファームウェア、ソフトウェア、および/またはそれらの組合せで実現することができる。これらの様々な実装形態は、記憶システム、少なくとも1つの入力デバイス、および少なくとも1つの出力デバイスとの間でデータおよび命令を送受信するように結合される、専用または汎用であり得る、少なくとも1つのプログラマブルプロセッサを含むプログラマブルシステム上で実行可能および/または解釈可能な1つまたは複数のコンピュータプログラムにおける実装形態を含むことができる。 Various implementations of the systems and techniques described herein include digital electronic circuits, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and / or their It can be realized in combination. These various implementations are at least one programmable, which may be dedicated or general purpose, coupled to send and receive data and instructions to and from the storage system, at least one input device, and at least one output device. Implementations in one or more computer programs executable and / or interpretable on a programmable system including a processor may be included.
これらのコンピュータプログラム(プログラム、ソフトウェア、ソフトウェアアプリケーション、またはコードとしても知られる)は、プログラマブルプロセッサのための機械命令を含み、高水準手続き型および/またはオブジェクト指向プログラミング言語で、ならびに/またはアセンブリ/機械言語で実施することができる。本明細書内で用いられるように、機械可読媒体およびコンピュータ可読媒体という用語は、機械命令を機械可読信号として受信する機械可読媒体を含む、プログラマブルプロセッサへ機械命令および/またはデータを提供するのに用いられる任意のコンピュータプログラム製品、装置、および/またはデバイス(たとえば、磁気ディスク、光学ディスク、メモリ、プログラマブルロジックデバイス(PLD))を指す。機械可読信号という用語は、プログラマブルプロセッサに、機械命令および/またはデータを提供するのに用いられる任意の信号を指す。 These computer programs (also known as programs, software, software applications, or code) contain machine instructions for programmable processors, in high-level procedural and / or object-oriented programming languages, and / or assemblies / machines Can be implemented in language. As used herein, the terms machine-readable medium and computer-readable medium are used to provide machine instructions and / or data to a programmable processor, including machine-readable media that receive machine instructions as machine-readable signals. Any computer program product, apparatus, and / or device used (eg, magnetic disk, optical disk, memory, programmable logic device (PLD)). The term machine readable signal refers to any signal used to provide machine instructions and / or data to a programmable processor.
ユーザとやり取りするために、本明細書で記載されるシステムおよび技術は、ユーザに情報を表示するディスプレイデバイス(たとえば、CRT(陰極線管)またはLCD(液晶ディスプレイ)モニタ)およびユーザがコンピュータに入力することができるキーボードおよびポインティングデバイス(たとえば、マウスまたはトラックボール)を有するコンピュータ上で実施することができる。他の種類のデバイスも同様に、ユーザとやり取りするのに用いることができ、たとえば、ユーザに提供されるフィードバックは、任意の形式の感覚フィードバック(たとえば、視覚的フィードバック、聴覚的フィードバック、または触覚的フィードバック)であることができ、ユーザからの入力は、音響入力、音声入力、または触覚入力を含む任意の形式で受信することができる。 To interact with the user, the systems and techniques described herein provide a display device (eg, a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) that displays information to the user and the user inputs to the computer. It can be implemented on a computer having a keyboard and pointing device (eg, mouse or trackball) that can be. Other types of devices can be used to interact with the user as well, for example, the feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, audio feedback, or tactile feedback). Feedback) and input from the user can be received in any form including acoustic input, voice input, or haptic input.
本明細書で記載されるシステムおよび技術は、バックエンドコンポーネント(たとえばデータサーバとして)を含むコンピューティングシステム、または、ミドルウェアコンポーネント(たとえば、アプリケーションサーバ)を含むコンピューティングシステム、または、フロントエンドコンポーネント(たとえば、ユーザが本明細書で記載するシステムおよび技術の実装とやり取りすることができるグラフィカルユーザインタフェースまたはウェブブラウザを有するクライアントコンピュータ)を含むコンピューティングシステム、またはそのようなバックエンドコンポーネント、ミドルウェアコンポーネント、またはフロントエンドコンポーネントの任意の組合せを含むコンピューティングシステムで実施することができる。システムの構成要素は、任意の形式のまたは媒体のデジタルデータ通信(たとえば、通信ネットワーク)によって、相互接続することができる。通信ネットワークの例として、ローカルエリアネットワーク(LAN)、ワイドエリアネットワーク(WAN)、およびインターネットを含む。 The systems and techniques described herein include a computing system that includes a back-end component (e.g., as a data server), or a computing system that includes a middleware component (e.g., an application server), or a front-end component (e.g., , A computing system comprising a graphical user interface or a client computer having a web browser) that allows a user to interact with the implementation of the systems and techniques described herein, or such back-end, middleware, or front It can be implemented in a computing system that includes any combination of end components. The components of the system can be interconnected by any form or medium of digital data communication (eg, a communication network). Examples of communication networks include a local area network (LAN), a wide area network (WAN), and the Internet.
コンピューティングシステムは、クライアントおよびサーバを含むことができる。クライアントおよびサーバは、一般に、互いから遠く離れており、通常、通信ネットワークを通じて、やり取りを行う。クライアントとサーバの関係は、それぞれのコンピュータ上で作動し互いにクライアントサーバ関係を有するコンピュータプログラムによって生じる。 The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship between the client and the server is generated by a computer program that runs on each computer and has a client-server relationship with each other.
以上、いくつかの実装形態を、詳細に記載したが、他の修正も可能である。たとえば、クライアントアプリケーションは、デリゲート(delegate)にアクセスするものとして記載されるが、他の実装形態で、デリゲートは、1つまたは複数のサーバ上で実行するアプリケーションなどの、1つまたは複数のプロセッサによって実施される他のアプリケーションによって採用され得る。さらに、図で示された論理のフローは、所望の結果を得るのに、特定の示された順番、または順序を必要としない。さらに、他のアクションが提供されてもよく、または、アクションが、記述されたフローから削除されてもよく、他の構成要素が、記述されたシステムに追加されてもよく、または取り除かれてもよい。したがって、他の実装形態は、以下の特許請求の範囲の範囲内にある。 Although several implementations have been described in detail above, other modifications are possible. For example, a client application is described as accessing a delegate, but in other implementations, the delegate is by one or more processors, such as an application running on one or more servers. It can be employed by other applications implemented. Further, the logic flow shown in the figures does not require a particular order shown or order to achieve the desired result. In addition, other actions may be provided, or actions may be removed from the described flow, and other components may be added to or removed from the described system. Good. Accordingly, other implementations are within the scope of the following claims.
100 システム
102 ユーザ
104 発言
106 コンピューティングデバイス
108 コンピューティングデバイス
110 コンピューティングデバイス
114 マイク
116 マイク
118 マイク
120 ホットワーダ
122 ホットワーダ
124 ホットワーダ
126 ラウドネススコアラ
128 ラウドネススコアラ
130 ラウドネススコアラ
132 遅延計算モジュール
134 遅延計算モジュール
136 遅延計算モジュール
138 スピーカ
138 デバイス状況
140 スピーカ
140 デバイス状況
142 スピーカ
142 デバイス状況
200 プロセス
100 system
102 users
104 remarks
106 Computing devices
108 computing devices
110 Computing devices
114 microphone
116 microphone
118 microphone
120 Hot Warder
122 Hot Warder
124 hotworder
126 Loudness Scorer
128 Loudness Scorer
130 Loudness Scorer
132 Delay calculation module
134 Delay calculation module
136 Delay calculation module
138 Speaker
138 Device status
140 Speaker
140 Device status
142 Speaker
142 Device status
200 processes
Claims (20)
(i)予め定義されたホットワードが先行する音声コマンドを処理するように構成され、(ii)同一の予め定義されたホットワードが先行する音声コマンドを処理するように構成された別のモバイルコンピューティングデバイスの近くにあり、(iii)前記別のモバイルコンピューティングデバイスより話者から遠いモバイルコンピューティングデバイスが、前記予め定義されたホットワードが先行する音声コマンドの前記話者による発言を表す音声入力を受信するステップと、
前記予め定義されたホットワードが先行する音声コマンドの前記話者による発言を表す音声入力の受信に応答して、前記別のモバイルコンピューティングデバイスに信号を送信し、前記音声コマンドの処理を回避するステップと、を備える方法。 A computer-implemented method comprising:
(i) another mobile computer configured to process a voice command preceded by a pre-defined hot word, and (ii) configured to process a voice command preceded by the same pre-defined hot word. (Iii) a voice input representing a speech by the speaker of a voice command preceded by the predefined hotword when a mobile computing device is near the mobile device and further from the speaker than the other mobile computing device Receiving the step,
In response to receiving a voice input representing the speech by the speaker of a voice command preceded by the predefined hot word, a signal is sent to the other mobile computing device to avoid processing the voice command. And a method comprising:
前記遅延時間が経過した後で、前記信号が送信される、請求項1に記載の方法。 Determining the amount of delay time in response to receiving a voice input representing the speech by the speaker of a voice command preceded by the predefined hot word;
The method of claim 1, wherein the signal is transmitted after the delay time has elapsed.
前記別のモバイルコンピューティングデバイスに信号を送信する間、前記音声コマンドの処理を回避し、前記モバイルコンピューティングデバイスの画面はブランクである、請求項1に記載の方法。 The screen of the mobile computing device is blank while receiving voice input representing the speech by the speaker of the voice command preceded by the predefined hot word;
The method of claim 1, wherein processing of the voice command is avoided while sending a signal to the another mobile computing device, and the screen of the mobile computing device is blank.
前記音声コマンドの処理が、前記別の信号に基づいて回避される、請求項1に記載の方法。 Receiving another signal from the another mobile computing device;
The method of claim 1, wherein processing of the voice command is avoided based on the another signal.
(i)予め定義されたホットワードが先行する音声コマンドを処理するように構成され、(ii)同一の予め定義されたホットワードが先行する音声コマンドを処理するように構成された別のモバイルコンピューティングデバイスの近くにあり、(iii)前記別のモバイルコンピューティングデバイスより話者から遠いモバイルコンピューティングデバイスが、前記予め定義されたホットワードが先行する音声コマンドの前記話者による発言を表す音声入力を受信することと、
前記予め定義されたホットワードが先行する音声コマンドの前記話者による発言を表す音声入力の受信に応答して、前記別のモバイルコンピューティングデバイスに信号を送信し、前記音声コマンドの処理を回避することと、を備えるシステム。 One or more computers and one or more storage devices that store instructions operable to cause the one or more computers to perform operations when executed by the one or more computers , Wherein the operation is
(i) another mobile computer configured to process a voice command preceded by a pre-defined hot word, and (ii) configured to process a voice command preceded by the same pre-defined hot word. A voice input representing a speech by the speaker of a voice command preceded by the predefined hotword, wherein the mobile computing device is near the mobile device and (iii) farther from the speaker than the other mobile computing device Receiving and
In response to receiving a voice input representing the speech by the speaker of a voice command preceded by the predefined hot word, a signal is sent to the other mobile computing device to avoid processing the voice command. A system comprising:
前記遅延時間が経過した後で、前記信号が送信される、請求項8に記載のシステム。 The operation further comprises determining an amount of delay time in response to receiving a voice input representative of the speech by the speaker of a voice command preceded by the predefined hot word;
9. The system of claim 8, wherein the signal is transmitted after the delay time has elapsed.
前記別のモバイルコンピューティングデバイスに信号を送信する間、前記音声コマンドの処理を回避し、前記モバイルコンピューティングデバイスの画面はブランクである、請求項8に記載のシステム。 The screen of the mobile computing device is blank while receiving voice input representing the speech by the speaker of the voice command preceded by the predefined hot word;
9. The system of claim 8, wherein processing of the voice command is avoided while sending a signal to the another mobile computing device, and the screen of the mobile computing device is blank.
前記音声コマンドの処理が、前記別の信号に基づいて回避される、請求項8に記載のシステム。 The operation further comprises receiving another signal from the another mobile computing device;
9. The system of claim 8, wherein processing of the voice command is avoided based on the another signal.
(i)予め定義されたホットワードが先行する音声コマンドを処理するように構成され、(ii)同一の予め定義されたホットワードが先行する音声コマンドを処理するように構成された別のモバイルコンピューティングデバイスの近くにあり、(iii)前記別のモバイルコンピューティングデバイスより話者から遠いモバイルコンピューティングデバイスが、前記予め定義されたホットワードが先行する音声コマンドの前記話者による発言を表す音声入力を受信することと、
前記予め定義されたホットワードが先行する音声コマンドの前記話者による発言を表す音声入力の受信に応答して、前記別のモバイルコンピューティングデバイスに信号を送信し、前記音声コマンドの処理を回避することと、を備える非一時的なコンピュータ可読媒体。 A non-transitory computer readable medium that stores software that is executable by one or more computers and that, when executed, comprises instructions that cause the one or more computers to perform an action, the action comprising:
(i) another mobile computer configured to process a voice command preceded by a pre-defined hot word, and (ii) configured to process a voice command preceded by the same pre-defined hot word. A voice input representing a speech by the speaker of a voice command preceded by the predefined hotword, wherein the mobile computing device is near the mobile device and (iii) farther from the speaker than the other mobile computing device Receiving and
In response to receiving a voice input representing the speech by the speaker of a voice command preceded by the predefined hot word, a signal is sent to the other mobile computing device to avoid processing the voice command. A non-transitory computer-readable medium comprising:
前記遅延時間が経過した後で、前記信号が送信される、請求項15に記載の非一時的なコンピュータ可読媒体。 The operation further comprises determining an amount of delay time in response to receiving a voice input representative of the speech by the speaker of a voice command preceded by the predefined hot word;
16. The non-transitory computer readable medium of claim 15, wherein the signal is transmitted after the delay time has elapsed.
前記別のモバイルコンピューティングデバイスに信号を送信する間、前記音声コマンドの処理を回避し、前記モバイルコンピューティングデバイスの画面はブランクである、請求項15に記載の非一時的なコンピュータ可読媒体。 The screen of the mobile computing device is blank while receiving voice input representing the speech by the speaker of the voice command preceded by the predefined hot word;
The non-transitory computer-readable medium of claim 15, wherein processing of the voice command is avoided while sending a signal to the another mobile computing device, and the screen of the mobile computing device is blank.
前記音声コマンドの処理が、前記別の信号に基づいて回避される、請求項15に記載の非一時的なコンピュータ可読媒体。 The operation further comprises receiving another signal from the another mobile computing device;
The non-transitory computer-readable medium of claim 15, wherein processing of the voice command is avoided based on the another signal.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201462061903P | 2014-10-09 | 2014-10-09 | |
US62/061,903 | 2014-10-09 | ||
US14/659,861 US9424841B2 (en) | 2014-10-09 | 2015-03-17 | Hotword detection on multiple devices |
US14/659,861 | 2015-03-17 |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2016549271A Division JP6261751B2 (en) | 2014-10-09 | 2015-09-29 | Hotword detection on multiple devices |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2017126317A true JP2017126317A (en) | 2017-07-20 |
JP6251343B2 JP6251343B2 (en) | 2017-12-20 |
Family
ID=54347818
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2016549271A Active JP6261751B2 (en) | 2014-10-09 | 2015-09-29 | Hotword detection on multiple devices |
JP2016174371A Active JP6251343B2 (en) | 2014-10-09 | 2016-09-07 | Hotword detection on multiple devices |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2016549271A Active JP6261751B2 (en) | 2014-10-09 | 2015-09-29 | Hotword detection on multiple devices |
Country Status (6)
Country | Link |
---|---|
US (6) | US9424841B2 (en) |
EP (3) | EP3100260B1 (en) |
JP (2) | JP6261751B2 (en) |
KR (2) | KR101819682B1 (en) |
CN (2) | CN105960673B (en) |
WO (1) | WO2016057269A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2019091005A (en) * | 2017-11-16 | 2019-06-13 | バイドゥ オンライン ネットワーク テクノロジー （ベイジン） カンパニー リミテッド | Multi apparatus interactive method, device, apparatus and computer readable medium |
Families Citing this family (123)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10032452B1 (en) * | 2016-12-30 | 2018-07-24 | Google Llc | Multimodal transmission of packetized data |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US11017428B2 (en) | 2008-02-21 | 2021-05-25 | Google Llc | System and method of data transmission rate adjustment |
US10013986B1 (en) | 2016-12-30 | 2018-07-03 | Google Llc | Data structure pooling of voice activated data packets |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US10417037B2 (en) | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US10776830B2 (en) | 2012-05-23 | 2020-09-15 | Google Llc | Methods and systems for identifying new computers and providing matching services |
US10152723B2 (en) | 2012-05-23 | 2018-12-11 | Google Llc | Methods and systems for identifying new computers and providing matching services |
US10735552B2 (en) | 2013-01-31 | 2020-08-04 | Google Llc | Secondary transmissions of packetized data |
US10650066B2 (en) | 2013-01-31 | 2020-05-12 | Google Llc | Enhancing sitelinks with creative content |
CN104969289B (en) | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US10748529B1 (en) | 2013-03-15 | 2020-08-18 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
EP2958010A1 (en) | 2014-06-20 | 2015-12-23 | Thomson Licensing | Apparatus and method for controlling the apparatus by a user |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11942095B2 (en) | 2014-07-18 | 2024-03-26 | Google Llc | Speaker verification using co-location information |
US11676608B2 (en) | 2021-04-02 | 2023-06-13 | Google Llc | Speaker verification using co-location information |
US9257120B1 (en) | 2014-07-18 | 2016-02-09 | Google Inc. | Speaker verification using co-location information |
US9318107B1 (en) | 2014-10-09 | 2016-04-19 | Google Inc. | Hotword detection on multiple devices |
US9424841B2 (en) * | 2014-10-09 | 2016-08-23 | Google Inc. | Hotword detection on multiple devices |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US20160378747A1 (en) | 2015-06-29 | 2016-12-29 | Apple Inc. | Virtual assistant for media playback |
US10740384B2 (en) | 2015-09-08 | 2020-08-11 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
KR20170034154A (en) * | 2015-09-18 | 2017-03-28 | 삼성전자주식회사 | Method and electronic device for providing contents |
US9542941B1 (en) * | 2015-10-01 | 2017-01-10 | Lenovo (Singapore) Pte. Ltd. | Situationally suspending wakeup word to enable voice command input |
US9747926B2 (en) * | 2015-10-16 | 2017-08-29 | Google Inc. | Hotword recognition |
JP6463710B2 (en) | 2015-10-16 | 2019-02-06 | グーグル エルエルシー | Hot word recognition |
US9928840B2 (en) | 2015-10-16 | 2018-03-27 | Google Llc | Hotword recognition |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10074364B1 (en) * | 2016-02-02 | 2018-09-11 | Amazon Technologies, Inc. | Sound profile generation based on speech recognition results exceeding a threshold |
US9779735B2 (en) | 2016-02-24 | 2017-10-03 | Google Inc. | Methods and systems for detecting and processing speech signals |
US20170294138A1 (en) * | 2016-04-08 | 2017-10-12 | Patricia Kavanagh | Speech Improvement System and Method of Its Use |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
US10091545B1 (en) * | 2016-06-27 | 2018-10-02 | Amazon Technologies, Inc. | Methods and systems for detecting audio output of associated device |
US10438583B2 (en) * | 2016-07-20 | 2019-10-08 | Lenovo (Singapore) Pte. Ltd. | Natural language voice assistant |
US10621992B2 (en) * | 2016-07-22 | 2020-04-14 | Lenovo (Singapore) Pte. Ltd. | Activating voice assistant based on at least one of user proximity and context |
US9972320B2 (en) | 2016-08-24 | 2018-05-15 | Google Llc | Hotword detection on multiple devices |
KR102241970B1 (en) | 2016-11-07 | 2021-04-20 | 구글 엘엘씨 | Suppressing recorded media hotword trigger |
US10276149B1 (en) * | 2016-12-21 | 2019-04-30 | Amazon Technologies, Inc. | Dynamic text-to-speech output |
US10559309B2 (en) | 2016-12-22 | 2020-02-11 | Google Llc | Collaborative voice controlled devices |
US10276161B2 (en) * | 2016-12-27 | 2019-04-30 | Google Llc | Contextual hotwords |
US10593329B2 (en) | 2016-12-30 | 2020-03-17 | Google Llc | Multimodal transmission of packetized data |
US10708313B2 (en) | 2016-12-30 | 2020-07-07 | Google Llc | Multimodal transmission of packetized data |
KR20180083587A (en) * | 2017-01-13 | 2018-07-23 | 삼성전자주식회사 | Electronic device and operating method thereof |
KR20180085931A (en) | 2017-01-20 | 2018-07-30 | 삼성전자주식회사 | Voice input processing method and electronic device supporting the same |
US9990926B1 (en) * | 2017-03-13 | 2018-06-05 | Intel Corporation | Passive enrollment method for speaker identification systems |
US10403276B2 (en) | 2017-03-17 | 2019-09-03 | Microsoft Technology Licensing, Llc | Voice enabled features based on proximity |
US10621980B2 (en) * | 2017-03-21 | 2020-04-14 | Harman International Industries, Inc. | Execution of voice commands in a multi-device system |
CN117577099A (en) | 2017-04-20 | 2024-02-20 | 谷歌有限责任公司 | Method, system and medium for multi-user authentication on a device |
DK180048B1 (en) | 2017-05-11 | 2020-02-04 | Apple Inc. | MAINTAINING THE DATA PROTECTION OF PERSONAL INFORMATION |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
KR102458806B1 (en) | 2017-05-16 | 2022-10-26 | 구글 엘엘씨 | Handling calls on a shared speech-enabled device |
US20180336892A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Detecting a trigger of a digital assistant |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10664533B2 (en) | 2017-05-24 | 2020-05-26 | Lenovo (Singapore) Pte. Ltd. | Systems and methods to determine response cue for digital assistant based on context |
US10395650B2 (en) | 2017-06-05 | 2019-08-27 | Google Llc | Recorded media hotword trigger suppression |
US10069976B1 (en) * | 2017-06-13 | 2018-09-04 | Harman International Industries, Incorporated | Voice agent forwarding |
US10636428B2 (en) | 2017-06-29 | 2020-04-28 | Microsoft Technology Licensing, Llc | Determining a target device for voice command interaction |
US20190065608A1 (en) * | 2017-08-29 | 2019-02-28 | Lenovo (Singapore) Pte. Ltd. | Query input received at more than one device |
KR102489914B1 (en) * | 2017-09-15 | 2023-01-20 | 삼성전자주식회사 | Electronic Device and method for controlling the electronic device |
US10276175B1 (en) * | 2017-11-28 | 2019-04-30 | Google Llc | Key phrase detection with audio watermarking |
EP3519936B1 (en) * | 2017-12-08 | 2020-04-08 | Google LLC | Isolating a device, from multiple devices in an environment, for being responsive to spoken assistant invocation(s) |
US10885910B1 (en) | 2018-03-14 | 2021-01-05 | Amazon Technologies, Inc. | Voice-forward graphical user interface mode management |
US10877637B1 (en) * | 2018-03-14 | 2020-12-29 | Amazon Technologies, Inc. | Voice-based device operation mode management |
US11127405B1 (en) | 2018-03-14 | 2021-09-21 | Amazon Technologies, Inc. | Selective requests for authentication for voice-based launching of applications |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10692496B2 (en) * | 2018-05-22 | 2020-06-23 | Google Llc | Hotword suppression |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10892996B2 (en) * | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
GB2574803B (en) * | 2018-06-11 | 2022-12-07 | Xmos Ltd | Communication between audio devices |
WO2020013946A1 (en) | 2018-07-13 | 2020-01-16 | Google Llc | End-to-end streaming keyword spotting |
JP7250900B2 (en) * | 2018-08-09 | 2023-04-03 | グーグル エルエルシー | Hot word recognition and passive assistance |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
CN109545207A (en) * | 2018-11-16 | 2019-03-29 | 广东小天才科技有限公司 | A kind of voice awakening method and device |
CN109243462A (en) * | 2018-11-20 | 2019-01-18 | 广东小天才科技有限公司 | A kind of voice awakening method and device |
CN109584876B (en) * | 2018-12-26 | 2020-07-14 | 珠海格力电器股份有限公司 | Voice data processing method and device and voice air conditioner |
CN109584878A (en) * | 2019-01-14 | 2019-04-05 | 广东小天才科技有限公司 | A kind of voice awakening method and system |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
DK201970509A1 (en) | 2019-05-06 | 2021-01-15 | Apple Inc | Spoken notifications |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
DK180129B1 (en) | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
DK201970510A1 (en) | 2019-05-31 | 2021-02-11 | Apple Inc | Voice identification in digital assistant systems |
US11227599B2 (en) | 2019-06-01 | 2022-01-18 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
CN112712803B (en) * | 2019-07-15 | 2022-02-25 | 华为技术有限公司 | Voice awakening method and electronic equipment |
CN110660390B (en) * | 2019-09-17 | 2022-05-03 | 百度在线网络技术（北京）有限公司 | Intelligent device wake-up method, intelligent device and computer readable storage medium |
KR102629796B1 (en) | 2019-10-15 | 2024-01-26 | 삼성전자 주식회사 | An electronic device supporting improved speech recognition |
CN110890092B (en) * | 2019-11-07 | 2022-08-05 | 北京小米移动软件有限公司 | Wake-up control method and device and computer storage medium |
KR20210069977A (en) * | 2019-12-04 | 2021-06-14 | 엘지전자 주식회사 | Method and apparatus for contolling device |
CN111312239B (en) * | 2020-01-20 | 2023-09-26 | 北京小米松果电子有限公司 | Response method, response device, electronic equipment and storage medium |
US11282527B2 (en) * | 2020-02-28 | 2022-03-22 | Synaptics Incorporated | Subaudible tones to validate audio signals |
US11061543B1 (en) | 2020-05-11 | 2021-07-13 | Apple Inc. | Providing relevant data items based on context |
US11043220B1 (en) | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11490204B2 (en) | 2020-07-20 | 2022-11-01 | Apple Inc. | Multi-device audio adjustment coordination |
US11438683B2 (en) | 2020-07-21 | 2022-09-06 | Apple Inc. | User identification using headphones |
CN112133302B (en) * | 2020-08-26 | 2024-05-07 | 北京小米松果电子有限公司 | Method, device and storage medium for pre-waking up terminal |
KR20220041413A (en) * | 2020-09-25 | 2022-04-01 | 삼성전자주식회사 | Electronic apparatus and control method thereof |
US11727925B2 (en) * | 2020-10-13 | 2023-08-15 | Google Llc | Cross-device data synchronization based on simultaneous hotword triggers |
US11557300B2 (en) * | 2020-10-16 | 2023-01-17 | Google Llc | Detecting and handling failures in other assistants |
US20210225374A1 (en) * | 2020-12-23 | 2021-07-22 | Intel Corporation | Method and system of environment-sensitive wake-on-voice initiation using ultrasound |
CN114115788A (en) * | 2021-10-09 | 2022-03-01 | 维沃移动通信有限公司 | Audio playing method and device |
US20230178075A1 (en) * | 2021-12-02 | 2023-06-08 | Lenovo (Singapore) Pte. Ltd | Methods and devices for preventing a sound activated response |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8340975B1 (en) * | 2011-10-04 | 2012-12-25 | Theodore Alfred Rosenberger | Interactive speech recognition device and system for hands-free building control |
WO2014107413A1 (en) * | 2013-01-04 | 2014-07-10 | Kopin Corporation | Bifurcated speech recognition |
Family Cites Families (54)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4363102A (en) | 1981-03-27 | 1982-12-07 | Bell Telephone Laboratories, Incorporated | Speaker identification system using word recognition templates |
JP3674990B2 (en) * | 1995-08-21 | 2005-07-27 | セイコーエプソン株式会社 | Speech recognition dialogue apparatus and speech recognition dialogue processing method |
SE511418C2 (en) | 1997-03-13 | 1999-09-27 | Telia Ab | Method of speech verification / identification via modeling of typical non-typical characteristics. |
US6076055A (en) | 1997-05-27 | 2000-06-13 | Ameritech | Speaker verification method |
US5897616A (en) | 1997-06-11 | 1999-04-27 | International Business Machines Corporation | Apparatus and methods for speaker verification/identification/classification employing non-acoustic and/or acoustic models and databases |
US6141644A (en) | 1998-09-04 | 2000-10-31 | Matsushita Electric Industrial Co., Ltd. | Speaker verification and speaker identification based on eigenvoices |
JP3357629B2 (en) * | 1999-04-26 | 2002-12-16 | 旭化成株式会社 | Equipment control system |
DE19939102C1 (en) * | 1999-08-18 | 2000-10-26 | Siemens Ag | Speech recognition method for dictating system or automatic telephone exchange |
US6567775B1 (en) | 2000-04-26 | 2003-05-20 | International Business Machines Corporation | Fusion of audio and video based speaker identification for multimedia information access |
US6826159B1 (en) | 2000-05-24 | 2004-11-30 | Cisco Technology, Inc. | System and method for providing speaker identification in a conference call |
EP1215658A3 (en) * | 2000-12-05 | 2002-08-14 | Hewlett-Packard Company | Visual activation of voice controlled apparatus |
US20030231746A1 (en) | 2002-06-14 | 2003-12-18 | Hunter Karla Rae | Teleconference speaker identification |
TW200409525A (en) | 2002-11-26 | 2004-06-01 | Lite On Technology Corp | Voice identification method for cellular phone and cellular phone with voiceprint password |
EP1429314A1 (en) * | 2002-12-13 | 2004-06-16 | Sony International (Europe) GmbH | Correction of energy as input feature for speech processing |
US7222072B2 (en) | 2003-02-13 | 2007-05-22 | Sbc Properties, L.P. | Bio-phonetic multi-phrase speaker identity verification |
US7571014B1 (en) | 2004-04-01 | 2009-08-04 | Sonos, Inc. | Method and apparatus for controlling multimedia players in a multi-zone system |
US8290603B1 (en) | 2004-06-05 | 2012-10-16 | Sonos, Inc. | User interfaces for controlling and manipulating groupings in a multi-zone media system |
US20070198262A1 (en) | 2003-08-20 | 2007-08-23 | Mindlin Bernardo G | Topological voiceprints for speaker identification |
US8517921B2 (en) | 2004-04-16 | 2013-08-27 | Gyrus Acmi, Inc. | Endoscopic instrument having reduced diameter flexible shaft |
US8214447B2 (en) | 2004-06-08 | 2012-07-03 | Bose Corporation | Managing an audio network |
US7720012B1 (en) | 2004-07-09 | 2010-05-18 | Arrowhead Center, Inc. | Speaker identification in the presence of packet losses |
US8412521B2 (en) | 2004-08-20 | 2013-04-02 | Multimodal Technologies, Llc | Discriminative training of document transcription system |
US8521529B2 (en) | 2004-10-18 | 2013-08-27 | Creative Technology Ltd | Method for segmenting audio signals |
US8709018B2 (en) | 2005-09-16 | 2014-04-29 | Applied Medical Technology, Inc. | Non-balloon low profile feed device with insertion/removal tool |
KR100711094B1 (en) * | 2005-11-29 | 2007-04-27 | 삼성전자주식회사 | Resource allocating method among mobile-stations in distribution communication network |
US7741962B2 (en) * | 2006-10-09 | 2010-06-22 | Toyota Motor Engineering & Manufacturing North America, Inc. | Auditory display of vehicular environment |
CN1996847B (en) | 2006-12-27 | 2010-05-19 | 中国科学院上海技术物理研究所 | Cooperative network based image and multi-media data communication and storage system |
US8099288B2 (en) | 2007-02-12 | 2012-01-17 | Microsoft Corp. | Text-dependent speaker verification |
US8838457B2 (en) | 2007-03-07 | 2014-09-16 | Vlingo Corporation | Using results of unstructured language model based speech recognition to control a system-level function of a mobile communications facility |
US20080252595A1 (en) * | 2007-04-11 | 2008-10-16 | Marc Boillot | Method and Device for Virtual Navigation and Voice Processing |
US8385233B2 (en) | 2007-06-12 | 2013-02-26 | Microsoft Corporation | Active speaker identification |
US8504365B2 (en) | 2008-04-11 | 2013-08-06 | At&T Intellectual Property I, L.P. | System and method for detecting synthetic speaker verification |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US8326637B2 (en) | 2009-02-20 | 2012-12-04 | Voicebox Technologies, Inc. | System and method for processing multi-modal device interactions in a natural language voice services environment |
US8209174B2 (en) | 2009-04-17 | 2012-06-26 | Saudi Arabian Oil Company | Speaker verification system |
CN101923853B (en) | 2009-06-12 | 2013-01-23 | 华为技术有限公司 | Speaker recognition method, equipment and system |
US8311838B2 (en) | 2010-01-13 | 2012-11-13 | Apple Inc. | Devices and methods for identifying a prompt corresponding to a voice input in a sequence of prompts |
US8626511B2 (en) * | 2010-01-22 | 2014-01-07 | Google Inc. | Multi-dimensional disambiguation of voice commands |
JP5411789B2 (en) * | 2010-04-19 | 2014-02-12 | 本田技研工業株式会社 | Communication robot |
KR101672212B1 (en) * | 2010-06-15 | 2016-11-04 | 엘지전자 주식회사 | Mobile terminal and operation method thereof |
US8719018B2 (en) | 2010-10-25 | 2014-05-06 | Lockheed Martin Corporation | Biometric speaker identification |
US9031847B2 (en) | 2011-11-15 | 2015-05-12 | Microsoft Technology Licensing, Llc | Voice-controlled camera operations |
US9711160B2 (en) * | 2012-05-29 | 2017-07-18 | Apple Inc. | Smart dock for activating a voice recognition mode of a portable electronic device |
JP6131537B2 (en) * | 2012-07-04 | 2017-05-24 | セイコーエプソン株式会社 | Speech recognition system, speech recognition program, recording medium, and speech recognition method |
US8983836B2 (en) | 2012-09-26 | 2015-03-17 | International Business Machines Corporation | Captioning using socially derived acoustic profiles |
US8996372B1 (en) | 2012-10-30 | 2015-03-31 | Amazon Technologies, Inc. | Using adaptation data with cloud-based speech recognition |
US8775191B1 (en) | 2013-11-13 | 2014-07-08 | Google Inc. | Efficient utterance-specific endpointer triggering for always-on hotwording |
CN103645876B (en) * | 2013-12-06 | 2017-01-18 | 百度在线网络技术（北京）有限公司 | Voice inputting method and device |
CN103730116B (en) * | 2014-01-07 | 2016-08-17 | 苏州思必驰信息科技有限公司 | Intelligent watch realizes the system and method that intelligent home device controls |
US8938394B1 (en) | 2014-01-09 | 2015-01-20 | Google Inc. | Audio triggers based on context |
US9424841B2 (en) | 2014-10-09 | 2016-08-23 | Google Inc. | Hotword detection on multiple devices |
US9812126B2 (en) | 2014-11-28 | 2017-11-07 | Microsoft Technology Licensing, Llc | Device arbitration for listening devices |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US10679629B2 (en) | 2018-04-09 | 2020-06-09 | Amazon Technologies, Inc. | Device arbitration by multiple speech processing systems |
-
2015
- 2015-03-17 US US14/659,861 patent/US9424841B2/en active Active
- 2015-09-29 EP EP15784808.6A patent/EP3100260B1/en active Active
- 2015-09-29 EP EP18213657.2A patent/EP3483877B1/en active Active
- 2015-09-29 CN CN201580006769.7A patent/CN105960673B/en active Active
- 2015-09-29 JP JP2016549271A patent/JP6261751B2/en active Active
- 2015-09-29 WO PCT/US2015/052870 patent/WO2016057269A1/en active Application Filing
- 2015-09-29 EP EP16193577.0A patent/EP3136381B1/en active Active
- 2015-09-29 KR KR1020167026606A patent/KR101819682B1/en active IP Right Grant
- 2015-09-29 CN CN201911273215.XA patent/CN111105784A/en active Pending
- 2015-09-29 KR KR1020167020950A patent/KR101819681B1/en active IP Right Grant
-
2016
- 2016-06-23 US US15/190,739 patent/US9990922B2/en active Active
- 2016-09-07 JP JP2016174371A patent/JP6251343B2/en active Active
-
2018
- 2018-04-23 US US15/959,508 patent/US10347253B2/en active Active
-
2019
- 2019-06-27 US US16/454,451 patent/US10665239B2/en active Active
-
2020
- 2020-04-28 US US16/860,419 patent/US11024313B2/en active Active
-
2021
- 2021-04-28 US US17/242,738 patent/US11955121B2/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8340975B1 (en) * | 2011-10-04 | 2012-12-25 | Theodore Alfred Rosenberger | Interactive speech recognition device and system for hands-free building control |
WO2014107413A1 (en) * | 2013-01-04 | 2014-07-10 | Kopin Corporation | Bifurcated speech recognition |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2019091005A (en) * | 2017-11-16 | 2019-06-13 | バイドゥ オンライン ネットワーク テクノロジー （ベイジン） カンパニー リミテッド | Multi apparatus interactive method, device, apparatus and computer readable medium |
US10482903B2 (en) | 2017-11-16 | 2019-11-19 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method, device and apparatus for selectively interacting with multi-devices, and computer-readable medium |
Also Published As
Publication number | Publication date |
---|---|
WO2016057269A1 (en) | 2016-04-14 |
KR101819681B1 (en) | 2018-01-17 |
US9990922B2 (en) | 2018-06-05 |
JP2017513037A (en) | 2017-05-25 |
KR101819682B1 (en) | 2018-01-17 |
US20160104483A1 (en) | 2016-04-14 |
KR20160105847A (en) | 2016-09-07 |
KR20160121585A (en) | 2016-10-19 |
EP3100260A1 (en) | 2016-12-07 |
CN105960673A (en) | 2016-09-21 |
US9424841B2 (en) | 2016-08-23 |
US20200258522A1 (en) | 2020-08-13 |
US10665239B2 (en) | 2020-05-26 |
EP3483877B1 (en) | 2021-12-22 |
US20180315424A1 (en) | 2018-11-01 |
JP6251343B2 (en) | 2017-12-20 |
US11024313B2 (en) | 2021-06-01 |
US20160300571A1 (en) | 2016-10-13 |
EP3136381A1 (en) | 2017-03-01 |
US10347253B2 (en) | 2019-07-09 |
US20190385604A1 (en) | 2019-12-19 |
CN105960673B (en) | 2019-12-31 |
US20210249016A1 (en) | 2021-08-12 |
US11955121B2 (en) | 2024-04-09 |
CN111105784A (en) | 2020-05-05 |
EP3100260B1 (en) | 2018-12-26 |
EP3483877A1 (en) | 2019-05-15 |
EP3136381B1 (en) | 2019-11-06 |
JP6261751B2 (en) | 2018-01-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6251343B2 (en) | Hotword detection on multiple devices | |
JP6893951B2 (en) | Hotword detection on multiple devices |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20170807 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20171011 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20171030 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20171124 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 6251343Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |