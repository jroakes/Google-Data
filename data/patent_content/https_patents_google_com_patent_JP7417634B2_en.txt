JP7417634B2 - Using context information in end-to-end models for speech recognition - Google Patents
Using context information in end-to-end models for speech recognition Download PDFInfo
- Publication number
- JP7417634B2 JP7417634B2 JP2021566525A JP2021566525A JP7417634B2 JP 7417634 B2 JP7417634 B2 JP 7417634B2 JP 2021566525 A JP2021566525 A JP 2021566525A JP 2021566525 A JP2021566525 A JP 2021566525A JP 7417634 B2 JP7417634 B2 JP 7417634B2
- Authority
- JP
- Japan
- Prior art keywords
- context
- fst
- utterance
- score
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 claims description 120
- 238000013518 transcription Methods 0.000 claims description 89
- 230000035897 transcription Effects 0.000 claims description 89
- 238000012545 processing Methods 0.000 claims description 68
- 238000012048 forced swim test Methods 0.000 claims description 63
- 230000015654 memory Effects 0.000 claims description 52
- 230000008569 process Effects 0.000 claims description 51
- 238000013138 pruning Methods 0.000 claims description 40
- 239000012634 fragment Substances 0.000 claims description 36
- 238000004891 communication Methods 0.000 claims description 13
- 230000007704 transition Effects 0.000 claims description 13
- 238000013528 artificial neural network Methods 0.000 claims description 12
- 238000013480 data collection Methods 0.000 claims description 12
- 230000000306 recurrent effect Effects 0.000 claims description 8
- 230000009471 action Effects 0.000 claims description 6
- 230000000694 effects Effects 0.000 claims description 6
- 230000036961 partial effect Effects 0.000 claims description 6
- 230000003213 activating effect Effects 0.000 claims description 5
- 238000011156 evaluation Methods 0.000 claims description 4
- 230000002441 reversible effect Effects 0.000 claims description 4
- XDNBJTQLKCIJBV-GOSISDBHSA-N diethoxy-[4-[(r)-methylsulfinyl]phenoxy]-sulfanylidene-$l^{5}-phosphane Chemical compound CCOP(=S)(OCC)OC1=CC=C([S@@](C)=O)C=C1 XDNBJTQLKCIJBV-GOSISDBHSA-N 0.000 claims 17
- 238000012549 training Methods 0.000 description 72
- 238000012360 testing method Methods 0.000 description 21
- 238000004458 analytical method Methods 0.000 description 19
- 230000006872 improvement Effects 0.000 description 16
- 238000003860 storage Methods 0.000 description 16
- 238000002474 experimental method Methods 0.000 description 12
- 230000004927 fusion Effects 0.000 description 11
- 238000009826 distribution Methods 0.000 description 10
- 238000010586 diagram Methods 0.000 description 9
- 230000015556 catabolic process Effects 0.000 description 8
- 238000004590 computer program Methods 0.000 description 8
- 238000006731 degradation reaction Methods 0.000 description 8
- 241000282326 Felis catus Species 0.000 description 7
- 230000009467 reduction Effects 0.000 description 7
- 239000013598 vector Substances 0.000 description 6
- 230000004044 response Effects 0.000 description 5
- 238000013519 translation Methods 0.000 description 5
- 230000008901 benefit Effects 0.000 description 4
- 238000003062 neural network model Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000013459 approach Methods 0.000 description 3
- 230000007423 decrease Effects 0.000 description 3
- 238000000605 extraction Methods 0.000 description 3
- 238000001914 filtration Methods 0.000 description 3
- 238000005457 optimization Methods 0.000 description 3
- 230000008859 change Effects 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 238000004821 distillation Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000000670 limiting effect Effects 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 238000002156 mixing Methods 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 238000011160 research Methods 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000006403 short-term memory Effects 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000002194 synthesizing effect Effects 0.000 description 2
- 241000251468 Actinopterygii Species 0.000 description 1
- 238000006424 Flood reaction Methods 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000002860 competitive effect Effects 0.000 description 1
- 230000001143 conditioned effect Effects 0.000 description 1
- 238000012790 confirmation Methods 0.000 description 1
- 238000013499 data model Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000006866 deterioration Effects 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000005065 mining Methods 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000001629 suppression Effects 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 230000000699 topical effect Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/211—Selection of the most significant subset of features
- G06F18/2113—Selection of the most significant subset of features by ranking or filtering the set of features, e.g. using a measure of variance or of feature cross-correlation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/193—Formal grammars, e.g. finite state automata, context free grammars or word networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/085—Methods for reducing search complexity, pruning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
Description
本開示は、音声認識のためのエンドツーエンドモデルでコンテキスト情報を使用することに関する。 This disclosure relates to using context information in an end-to-end model for speech recognition.
音声のコンテキストを認識することは、人々が話す可能性がある多種多様な単語、ならびにアクセントおよび発音における多くのバリエーションを考慮すると、自動音声認識(ASR: automated speech recognition)システムにとって困難である。多くの場合、人が話す単語およびフレーズのタイプは、その人が気がつくとそこにいるコンテキストに応じて異なる。 Recognizing the context of speech is difficult for automated speech recognition (ASR) systems, given the wide variety of words that people may speak, and the many variations in accent and pronunciation. The types of words and phrases that a person speaks often vary depending on the context in which the person finds himself or herself.
コンテキスト自動音声認識(ASR)は、ユーザ自身のプレイリスト、連絡先、地理的な場所の名前などの、所与のコンテキストに向けて音声認識をバイアスすることを伴う。コンテキスト情報は、通常、認識されるべき関連フレーズのリストを含み、このリストは、しばしば、トレーニングする際にまれに見られる珍しいフレーズまたは外来語さえ含む。コンテキストバイアスを実行するために、従来のASRシステムは、ときには、n-gram重み付き有限状態トランスデューサ(WFST: weighted finite state transducer)を使用して独立したコンテキスト言語モデル(LM: language model)においてコンテキスト情報をモデル化し、オンザフライ(OTF: on-the-fly)再スコアリングのためのベースラインLMを用いて独立したコンテキストLMを構成する。 Contextual automatic speech recognition (ASR) involves biasing speech recognition toward a given context, such as a user's own playlists, contacts, or names of geographic locations. Contextual information typically includes a list of related phrases to be recognized, and this list often includes rare phrases or even foreign words that are rarely seen during training. To perform context bias, traditional ASR systems sometimes use n-gram weighted finite state transducers (WFSTs) to collect context information in a separate context language model (LM). model and construct an independent context LM using the baseline LM for on-the-fly (OTF) rescoring.
最近、エンドツーエンド(E2E: end-to-end)モデルが、従来のオンデバイスモデルと比較して、改善された単語誤り率(WER: word error rate)およびレイテンシメトリックを示し、ASRに対して大きな期待を示している。音声からテキストへのマッピングを直接学習するために、音響モデル(AM: acoustic model)、発音モデル(PM: pronunciation model)、およびLMを単一のネットワークに畳み込むこれらのE2Eモデルは、個別のAM、PM、およびLMを有する従来のASRシステムと比較して競争力のある結果を示している。典型的なE2Eモデルは、単語ベースのコネクショニスト時間分類(CTC: connectionist temporal classification)モデル、リカレントニューラルネットワークトランスデューサ(RNN-T: recurrent neural network transducer)モデル、ならびにListen, Attend, and Spell(LAS)などの注意(attention)ベースのモデルを含む。 Recently, end-to-end (E2E) models have shown improved word error rate (WER) and latency metrics compared to traditional on-device models for ASR. It shows great expectations. These E2E models convolve an acoustic model (AM), a pronunciation model (PM), and a LM into a single network to directly learn the speech-to-text mapping. It shows competitive results compared to traditional ASR systems with PM and LM. Typical E2E models include word-based connectionist temporal classification (CTC) models, recurrent neural network transducer (RNN-T) models, and Listen, Attend, and Spell (LAS) models. Includes attention-based models.
E2Eモデルは、ビーム検索復号中に限られた数の認識候補を維持するので、コンテキストASRは、E2Eモデルにとって困難であり得る。E2Eの書記素のみのモデルおよびE2Eの単語断片のみのモデルにとって、語彙外(OOV: out-of-vocabulary)の単語、特に、トレーニングにおいてほとんど見られない単語のクラスを綴ることは、特に困難である可能性がある。 Contextual ASR can be difficult for E2E models because they maintain a limited number of recognition candidates during beam search decoding. For E2E grapheme-only and E2E word fragment-only models, spelling out-of-vocabulary (OOV) words, especially classes of words rarely seen in training, is particularly difficult. There is a possibility.
本明細書における実装形態は、発話が話された現在のコンテキストに最も関連する可能性が高い単語およびフレーズに向けて音声認識をバイアスするために様々なタイプのコンテキスト情報を使用するバイアス技法を適用する自動音声認識(ASR)システムに向けられている。コンテキストのうちの1つが検出されると、ASRシステムは、関連する単語のセットの認識を改善するためにこれらの単語を使用する。バイアス技法は、音響モデル、発音モデル、および言語モデルの機能を単一の統一されたモデルに組み込んだニューラルネットワークモデルなどの、エンドツーエンドモデルで使用することができる。本明細書で開示されている様々な技法は、エンドツーエンドモデルのコンテキストバイアスを改善することができ、エンドツーエンドモデルを用いてバイアスを実装する際に発生する問題に対処することができる。 Implementations herein apply biasing techniques that use various types of context information to bias speech recognition toward words and phrases that are most likely to be relevant to the current context in which the utterance is spoken. automatic speech recognition (ASR) systems. Once one of the contexts is detected, the ASR system uses these words to improve recognition of sets of related words. Biasing techniques can be used in end-to-end models, such as neural network models that incorporate the functionality of acoustic, pronunciation, and language models into a single unified model. Various techniques disclosed herein can improve contextual bias in end-to-end models and can address issues that occur when implementing bias with end-to-end models.
ユーザの曲名、アプリケーション名、または連絡先の名前などの特定のドメインへのコンテキストバイアスは、ASRシステムの精度を大幅に改善することができる。エンドツーエンドモデルは、しばしば、ビーム検索中に候補の小さいリストのみを保持しているので、コンテキストバイアスは、これらのモデルを使用することが困難である可能性がある。多くのエンドツーエンドモデルは、バイアスフレーズの最も一般的なソースである固有名詞を認識する際の精度も比較的低い。 Contextual bias toward specific domains, such as a user's song name, application name, or contact name, can significantly improve the accuracy of an ASR system. End-to-end models often maintain only a small list of candidates during beam search, so context bias can make these models difficult to use. Many end-to-end models also have relatively low accuracy at recognizing proper nouns, the most common source of bias phrases.
コンテキスト知識をエンドツーエンドモデルに組み込むことは、様々な理由のために困難である可能性がある。第1に、ニューラルネットワークモデルは、共同の音響、発音、および言語モデルを用いてトレーニングされるので、ニューラルネットワークモデルは、典型的には、従来の言語モデルよりもはるかに少ないテキストデータを用いてトレーニングされる。従来の言語モデルは、対応するオーディオデータが利用できない場合でも、任意のソースからのテキストを使用してトレーニングされ得るが、エンドツーエンドモデルは、一般に、純粋なテキストよりも変動が少なく、純粋なテキストよりも取得するのが困難な、ペアのテキストおよび対応するオーディオデータを用いて訓練される。これにより、モデルは、はるかに少ない固有名詞の例を用いて訓練されることになり、結果として固有名詞に対する精度が低くなる。特定のコンテキストに関連するn-gramの多くは、固有名詞であり、バイアスなしでは、モデルがこれらのコンテキストに関連するn-gramを予測することは、困難である。第2に、効率的なデコードのために、エンドツーエンドモデルは、一般に、ビーム検索の各ステップにおいて、候補のトランスクリプションのための検索空間を少数の候補にまで剪定しなければならない。結果として、従来のエンドツーエンドモデルでは、この剪定プロセスは、しばしば、まれな単語およびフレーズが早期に剪定され、したがって、候補トランスクリプションのセットから除外されることを引き起こす。 Incorporating contextual knowledge into end-to-end models can be difficult for various reasons. First, because neural network models are trained using joint acoustic, pronunciation, and language models, neural network models typically use much less text data than traditional language models. be trained. While traditional language models can be trained using text from any source even when corresponding audio data is not available, end-to-end models generally have less variation than pure text and It is trained using paired text and corresponding audio data, which is more difficult to obtain than text. This causes the model to be trained with far fewer examples of proper nouns, resulting in lower accuracy for proper nouns. Many of the n-grams associated with a particular context are proper names, and without bias it is difficult for the model to predict the n-grams associated with these contexts. Second, for efficient decoding, end-to-end models must generally prune the search space for candidate transcriptions to a small number of candidates at each step of beam search. As a result, in traditional end-to-end models, this pruning process often causes rare words and phrases to be pruned prematurely and thus excluded from the set of candidate transcriptions.
コンテキスト知識をエンドツーエンドモデルの出力を復号するプロセスに組み込むことは、エンドツーエンドモデルを使用する音声認識の精度を改善することができる。以下でより詳細に説明されているが、コンテキスト情報を復号プロセスに組み込むことは、コンテキスト情報とモデルからの出力(たとえば、可能な音声認識仮説にわたる確率分布)を選択的に格子に統合することを伴い得る。コンテキスト情報を使用するかどうか、およびどのようなコンテキスト情報を使用するかの判断は、音声が発話されたコンテキスト情報、または他の要因に基づいて行うことができる。 Incorporating contextual knowledge into the process of decoding the output of an end-to-end model can improve the accuracy of speech recognition using the end-to-end model. As explained in more detail below, incorporating context information into the decoding process involves selectively integrating context information and output from a model (e.g., a probability distribution over possible speech recognition hypotheses) into a lattice. It can be accompanied. The determination of whether and what context information to use may be made based on the context information in which the speech was uttered, or other factors.
単語全体に対してではなく、部分語単位レベルで(たとえば、個々の書記素または単語断片に対して)バイアスを実行することは、エンドツーエンドモデルによるコンテキストバイアスをさらに改善することができる。これは、単語が完成された後に単にバイアスするのではなく、ターゲット単語の最初の書記素または単語断片単位をバイアスすることによって、大幅な改善を精度にもたらすことができる。いくつかのエンドツーエンドモデルでは、モデルによって実行される復号プロセスは、最初の数個の書記素または単語断片のみを考慮した後、(たとえば、ビーム検索中に剪定することによって)固有名詞などの一般的でない単語を考慮から素早く外す。単語レベルのバイアスでは、この状況は、単語が完成される前に単語が考慮から外される可能性があるので、バイアスは、発生する機会がない。対照的に、単語を構成する部分語単位のスコアをブーストすることによって単語をバイアスすることは、バイアスによって提供されるブーストが単語を候補トランスクリプションとして維持するように、単語の最初の部分のスコアを高めることができる。さらに、ビーム剪定後とは対照的にビーム剪定前に音声認識スコアを調整することによって、復号プロセスにおいて早期にコンテキストバイアスを適用することは、バイアスプロセスが効果を生じる機会を有する前に、剪定によって削除されるのではなく、ターゲット単語が適切に考慮されることを保証するのを助けることができる。 Performing bias at the subword level (e.g., for individual graphemes or word fragments) rather than for whole words can further improve the context bias with end-to-end models. This can yield significant improvements in accuracy by biasing the first grapheme or word fragment unit of the target word, rather than simply biasing after the word is completed. In some end-to-end models, the decoding process performed by the model considers only the first few graphemes or word fragments, and then decodes them (e.g. by pruning during beam search), such as proper nouns. Quickly remove uncommon words from consideration. With word-level bias, this situation may cause the word to be removed from consideration before it is completed, so the bias has no chance to occur. In contrast, biasing a word by boosting the scores of the subword units that make up the word means that the boost provided by the bias retains the word as a candidate transcription. You can increase your score. Additionally, applying context bias early in the decoding process by adjusting speech recognition scores before beam pruning as opposed to after beam pruning allows the pruning to occur before the biasing process has a chance to take effect. It can help ensure that target words are properly considered rather than deleted.
全体的な認識精度をさらに改善するために、ASRシステムは、バイアスを実行するときに、接頭辞の単語およびフレーズも考慮に入れることができる。特定の単語またはフレーズは、しばしば、特定のタイプの単語を含むステートメントの前にある。たとえば、ユーザがデバイスにコマンドを提供する場合、「call(電話をかける)」という単語は、しばしば、人の名前の前に存在し、「play(再生する)」という単語は、しばしば、ユーザのコレクション内の曲などのメディアアイテムの名前の前にある。たとえば、既知の接頭辞の存在は、認識が特定のタイプの単語にバイアスされるべきであるという追加の確信を提供するので、ASRシステムは、認識精度を改善するために接頭辞の検出を使用することができる。加えて、いくつかの実装形態において、バイアスが一般的な単語の認識の精度を妨げたり低下させたりしないように、バイアスすることは、適切な接頭辞が検出された場合にのみ適用することができる。結果として、コンテキストに依存する方法で尤度がブーストされた特定の単語およびフレーズに対する認識精度をコンテキストバイアスが改善する場合でも、モデルの一般的な語彙について高い認識精度を維持することができる。 To further improve overall recognition accuracy, the ASR system can also take into account prefix words and phrases when performing bias. Certain words or phrases often precede statements containing certain types of words. For example, when a user provides commands to a device, the word "call" often precedes the person's name, and the word "play" often precedes the user's In front of the name of a media item, such as a song, in a collection. For example, ASR systems use prefix detection to improve recognition accuracy, since the presence of a known prefix provides additional confidence that recognition should be biased toward a certain type of word. can do. Additionally, in some implementations, biasing may only be applied when a suitable prefix is detected, so that the bias does not interfere with or reduce the accuracy of common word recognition. can. As a result, high recognition accuracy can be maintained for the model's general vocabulary even when context bias improves recognition accuracy for specific words and phrases whose likelihood has been boosted in a context-dependent manner.
固有名詞のモデル化を改善するために、様々な技法を使用することもできる。これらの技法のうちのいくつかは、テキストのみのデータの大規模なコーパスを活用することができる。たとえば、トレーニングのために、システムは、多数の固有名詞のテキストのみを取得または作成し、次いで、対応する音声を合成することができる。システムは、固有名詞を有するデータを保持するようにフィルタリングされた、オーディオおよび対応するテキストデータを示す大量の教師なしデータを活用することができる。別の例として、システムは、より多くの固有名詞の例を作成するために、トレーニングデータトランスクリプションの変形または「ファズ(fuzz)」を作成することができる。 Various techniques can also be used to improve modeling of proper names. Some of these techniques can exploit large corpora of text-only data. For example, for training, the system can acquire or create only the text of a large number of proper nouns and then synthesize the corresponding speech. The system can leverage large amounts of unsupervised data showing audio and corresponding text data filtered to retain data with proper nouns. As another example, the system may create a variation or "fuzz" of the training data transcription to create more examples of proper nouns.
本開示の一態様は、音声を転写するためにコンテキストバイアスを使用するための方法を提供する。方法は、データ処理ハードウェアにおいて、発話を符号化するオーディオデータを受信するステップと、データ処理ハードウェアにおいて、音声認識モデルを使用して、音声要素の音声認識スコアを生成するためにオーディオデータを処理するステップとを含む。方法は、データ処理ハードウェアによって、発話のコンテキストを示すコンテキストデータに基づいて音声要素のコンテキストスコアを決定するステップも含む。方法は、データ処理ハードウェアによって、音声認識スコアとコンテキストスコアとを使用して、発話に対する1つまたは複数の候補トランスクリプションを決定するためにビーム検索復号プロセスを実行するステップと、データ処理ハードウェアによって、1つまたは複数の候補トランスクリプションから発話に対するトランスクリプションを選択するステップとをさらに含む。 One aspect of the present disclosure provides a method for using context bias to transcribe speech. The method includes the steps of: receiving, at data processing hardware, audio data to encode an utterance; and, at the data processing hardware, using a speech recognition model to generate a speech recognition score for a speech element. and processing. The method also includes determining, by the data processing hardware, a context score for the speech element based on context data indicative of the context of the utterance. The method includes, by data processing hardware, performing a beam search decoding process to determine one or more candidate transcriptions for the utterance using the speech recognition score and the context score; selecting a transcription for the utterance from one or more candidate transcriptions by the ware.
本開示の実装形態は、以下のオプションの特徴のうちの1つまたは複数を含み得る。いくつかの実装形態において、ビーム検索復号プロセスの実行中、コンテキストスコアは、1つまたは複数の候補トランスクリプションのいずれかを評価から剪定する前に、1つまたは複数の候補トランスクリプションの尤度を調整するように構成される。追加の実装形態において、ビーム検索復号プロセスを実行するステップは、発話に対する1つまたは複数の候補トランスクリプションを決定するために、音声認識格子を通るパスを剪定するためにコンテキストスコアを使用するステップを含む。いくつかの例では、方法は、データ処理ハードウェアによって、コンテキストデータに基づいて、発話に関する特定のコンテキストを識別するステップも含む。これらの例では、音声要素のコンテキストスコアを決定するステップは、識別された特定のコンテキストに基づく。これらの例では、音声要素のコンテキストスコアを決定するステップは、音声要素のコンテキストスコアを決定するためにコンテキスト有限状態トランスデューサ(FST: finite-state transducer)を使用するステップであって、コンテキストFSTが特定のコンテキストに対応する、ステップをさらに含み得る。 Implementations of this disclosure may include one or more of the following optional features. In some implementations, during the beam search decoding process, the context score is calculated based on the likelihood of one or more candidate transcriptions before pruning any of the one or more candidate transcriptions from evaluation. configured to adjust the degree. In additional implementations, performing the beam search decoding process includes using the context scores to prune paths through the speech recognition grid to determine one or more candidate transcriptions for the utterance. including. In some examples, the method also includes identifying, by the data processing hardware, a particular context for the utterance based on the context data. In these examples, determining the context score for the audio element is based on the specific context identified. In these examples, determining the context score of the audio element includes using a context finite-state transducer (FST) to determine the context score of the audio element, where the context FST is may further include steps corresponding to the context of.
いくつかの実装形態において、方法は、発話を符号化するオーディオデータを受信する前に、データ処理ハードウェアによって、発話を話したユーザのパーソナライズされたデータコレクション内の単語またはフレーズの異なるセットを各々が表す複数のコンテキストFSTを生成するステップと、データ処理ハードウェアによって、データ処理ハードウェアと通信するメモリハードウェア内に複数のコンテキストFSTを記憶するステップとをさらに含む。ここで、複数のコンテキストFST内のコンテキストFSTは、異なる特定のコンテキストに対応する。これらの実装形態において、パーソナライズされたデータコレクションは、ユーザの連絡先のリスト、および/またはユーザのメディアライブラリ、および/またはユーザに関連付けられたユーザデバイス上にインストールされたアプリケーションのリストを含み得る。これらの実装形態において、方法は、複数のコンテキストFST内の少なくとも1つのコンテキストFSTの各々について、データ処理ハードウェアによって、対応するコンテキストFSTの異なる特定のコンテキストに各々が対応する1つまたは複数の接頭辞のセットを含む対応する接頭辞FSTを生成するステップと、データ処理ハードウェアによって、複数のコンテキストFST内の少なくとも1つのコンテキストFSTに対して生成された対応する接頭辞FSTを記憶するステップとをさらに含み得る。これらの実装形態において、方法は、データ処理ハードウェアによって、発話に対する予備的トランスクリプションが対応する接頭辞FSTの1つまたは複数の接頭辞のうちの1つを含むことを判定するステップと、データ処理ハードウェアによって、予備的トランスクリプション内に含まれる1つまたは複数の接頭辞のうちの1つを含む接頭辞FSTに対応するコンテキストFSTを選択的にアクティブ化するステップとをオプションで含み得る。ここで、音声要素のコンテキストスコアを決定するステップは、選択的にアクティブ化されたコンテキストFSTに基づく。 In some implementations, the method includes, prior to receiving the audio data encoding the utterance, each different set of words or phrases in the personalized data collection of the user who spoke the utterance, by data processing hardware. and storing the plurality of contexts FST by the data processing hardware in memory hardware in communication with the data processing hardware. Here, the contexts FSTs in the plurality of contexts FSTs correspond to different specific contexts. In these implementations, the personalized data collection may include a list of the user's contacts, and/or the user's media library, and/or a list of applications installed on the user device associated with the user. In these implementations, the method includes, for each of the at least one context FST in the plurality of contexts FST, one or more prefixes, each corresponding to a different specific context of the corresponding context FST, by the data processing hardware. and storing, by data processing hardware, the corresponding prefix FST generated for at least one context FST in the plurality of contexts FST. It may further include. In these implementations, the method includes determining by the data processing hardware that the preliminary transcription for the utterance includes one of the one or more prefixes of the corresponding prefix FST; selectively activating, by the data processing hardware, a context FST that corresponds to a prefix FST that includes one of the one or more prefixes included in the preliminary transcription. obtain. Here, determining the context score of the speech element is based on the selectively activated context FST.
いくつかの例では、データ処理ハードウェアは、発話を話したユーザに関連付けられたユーザデバイス上に存在し、音声認識モデルを実行する。いくつかの実装形態において、音声認識モデルは、エンドツーエンド音声認識モデルを含む。たとえば、エンドツーエンド音声認識モデルは、リカレントニューラルネットワークトランスデューサ(RNN-T: recurrent neural network-transducer)を含み得る。 In some examples, data processing hardware resides on a user device associated with the user who spoke the utterance and executes the speech recognition model. In some implementations, the speech recognition model includes an end-to-end speech recognition model. For example, an end-to-end speech recognition model may include a recurrent neural network-transducer (RNN-T).
いくつかの例では、方法は、データ処理ハードウェアによって、発話に対する予備的トランスクリプションが発話のコンテキストに対応する接頭辞要素を表す単語を含むことを判定するステップをさらに含む。これらの例では、音声要素のコンテキストスコアを決定するステップは、発話に対する予備的トランスクリプションが発話のコンテキストに対応する接頭辞要素を表す単語を含むという判定に基づく。追加の例では、コンテキストに基づいて音声要素のコンテキストスコアを決定するステップは、部分語単位を表す要素を使用してコンテキスト用語を表すコンテキストFSTを使用してコンテキストスコアを決定するステップを含む。これらの例では、コンテキストFSTは、コンテキスト用語の部分語単位間の各遷移をバイアスするように構成された遷移重みと、遷移重みのバイアス効果を元に戻すように構成されたオフセット重みを有するバックオフアーク(backoff arc)とを含み得る。いくつかの例では、音声要素は、単語断片または書記素を含む。 In some examples, the method further includes determining, by the data processing hardware, that the preliminary transcription for the utterance includes a word representing a prefix element corresponding to a context of the utterance. In these examples, determining the context score for the speech element is based on determining that the preliminary transcription for the utterance includes a word representing a prefix element that corresponds to the context of the utterance. In an additional example, determining a context score for a speech element based on context includes determining a context score using a context FST representing a context term using an element representing a subword unit. In these examples, the context FST has transition weights configured to bias each transition between subword units of the context term, and an offset weight configured to reverse the biasing effects of the transition weights. backoff arc. In some examples, the phonetic elements include word fragments or graphemes.
本開示の別の態様は、音声を転写するためにコンテキストバイアスを使用するためのシステムを提供する。システムは、データ処理ハードウェアと、データ処理ハードウェアと通信するメモリハードウェアとを含む。メモリハードウェアは、データ処理ハードウェアによって実行されると、データ処理ハードウェアに、発話を符号化するオーディオデータを受信する動作と、音声認識モデルを使用して、音声要素の音声認識スコアを生成するためにオーディオデータを処理する動作とを含む動作を実行させる命令を記憶する。動作は、発話のコンテキストを示すコンテキストデータに基づいて音声要素のコンテキストスコアを決定する動作も含む。動作は、音声認識スコアとコンテキストスコアとを使用して、発話に対する1つまたは複数の候補トランスクリプションを決定するためにビーム検索復号プロセスを実行する動作と、1つまたは複数の候補トランスクリプションから発話に対するトランスクリプションを選択する動作とをさらに含む。 Another aspect of the present disclosure provides a system for using context bias to transcribe speech. The system includes data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware, when executed by the data processing hardware, causes the data processing hardware to perform operations that encode speech, receive audio data, and generate speech recognition scores for speech elements using the speech recognition model. storing instructions for performing operations, including operations for processing audio data in order to perform operations; The operations also include determining a context score for the speech element based on context data indicative of the context of the utterance. The operations include: performing a beam search decoding process to determine one or more candidate transcriptions for the utterance using the speech recognition score and the context score; and selecting a transcription for the utterance from the utterance.
本開示の実装形態は、以下のオプションの特徴のうちの1つまたは複数を含み得る。いくつかの実装形態において、ビーム検索復号プロセスの実行中、コンテキストスコアは、1つまたは複数の候補トランスクリプションのいずれかを評価から剪定する前に、1つまたは複数の候補トランスクリプションの尤度を調整するように構成される。追加の実装形態において、ビーム検索復号プロセスを実行する動作は、発話に対する1つまたは複数の候補トランスクリプションを決定するために、音声認識格子を通るパスを剪定するためにコンテキストスコアを使用する動作を含む。動作は、コンテキストデータに基づいて、発話に関する特定のコンテキストを識別するステップも含み得、音声要素のコンテキストスコアを決定する動作は、識別された特定のコンテキストに基づく。さらに、音声要素のコンテキストスコアを決定する動作は、音声要素のコンテキストスコアを決定するためにコンテキスト有限状態トランスデューサ(FST)を使用する動作であって、コンテキストFSTが特定のコンテキストに対応する、動作を含み得る。 Implementations of this disclosure may include one or more of the following optional features. In some implementations, during the beam search decoding process, the context score is calculated based on the likelihood of one or more candidate transcriptions before pruning any of the one or more candidate transcriptions from evaluation. configured to adjust the degree. In additional implementations, the act of performing a beam search decoding process includes an act of using context scores to prune a path through a speech recognition grid to determine one or more candidate transcriptions for the utterance. including. The operations may also include identifying a particular context for the utterance based on the context data, and the operations determining a context score for the speech element are based on the identified particular context. Further, the act of determining a context score of a speech element is an act of using a context finite state transducer (FST) to determine a context score of a speech element, wherein the context FST corresponds to a particular context. may be included.
いくつかの例では、動作は、発話を符号化するオーディオデータを受信する前に、発話を話したユーザのパーソナライズされたデータコレクション内の単語またはフレーズの異なるセットを各々が表す複数のコンテキストFSTを生成する動作と、データ処理ハードウェアと通信するメモリハードウェア内に複数のコンテキストFSTを記憶する動作とをさらに含む。ここで、複数のコンテキストFST内のコンテキストFSTは、異なる特定のコンテキストに対応する。これらの例では、パーソナライズされたデータコレクションは、ユーザの連絡先のリスト、および/またはユーザのメディアライブラリ、および/またはユーザに関連付けられたユーザデバイス上にインストールされたアプリケーションのリストを含み得る。例では、動作は、複数のコンテキストFST内の少なくとも1つのコンテキストFSTの各々について、対応するコンテキストFSTの異なる特定のコンテキストに各々が対応する1つまたは複数の接頭辞のセットを含む対応する接頭辞FSTを生成する動作と、複数のコンテキストFST内の少なくとも1つのコンテキストFSTに対して生成された対応する接頭辞FSTを記憶する動作とをさらに含み得る。これらの例では、動作は、発話に対する予備的トランスクリプションが対応する接頭辞FSTの1つまたは複数の接頭辞のうちの1つを含むことを判定する動作と、予備的トランスクリプション内に含まれる1つまたは複数の接頭辞のうちの1つを含む接頭辞FSTに対応するコンテキストFSTを選択的にアクティブ化する動作とをさらに含み得る。ここで、音声要素のコンテキストスコアを決定する動作は、選択的にアクティブ化されたコンテキストFSTに基づく。 In some examples, the operation generates multiple context FSTs, each representing a different set of words or phrases within the user's personalized data collection for the user who spoke the utterance, before receiving audio data to encode the utterance. The method further includes an act of generating and storing the plurality of contexts FST in memory hardware in communication with the data processing hardware. Here, the contexts FSTs in the plurality of contexts FSTs correspond to different specific contexts. In these examples, the personalized data collection may include a list of the user's contacts, and/or the user's media library, and/or a list of applications installed on the user device associated with the user. In the example, the operation includes, for each of the at least one context FST in the plurality of contexts FST, a set of one or more prefixes, each corresponding to a different specific context of the corresponding context FST. The method may further include an act of generating a FST and an act of storing a corresponding prefix FST generated for at least one context FST in the plurality of contexts FST. In these examples, the acts include an act of determining that the preliminary transcription for the utterance includes one of the one or more prefixes of the corresponding prefix FST; and selectively activating a context FST corresponding to a prefix FST that includes one of the included one or more prefixes. Here, the operation of determining a context score for a speech element is based on a selectively activated context FST.
いくつかの実装形態では、データ処理ハードウェアは、発話を話したユーザに関連付けられたユーザデバイス上に存在し、音声認識モデルを実行する。音声認識モデルは、エンドツーエンド音声認識モデルを含み得る。たとえば、エンドツーエンド音声認識モデルは、リカレントニューラルネットワークトランスデューサ(RNN-T)を含み得る。 In some implementations, data processing hardware resides on a user device associated with the user who spoke the utterance and executes the speech recognition model. The speech recognition model may include an end-to-end speech recognition model. For example, an end-to-end speech recognition model may include a recurrent neural network transducer (RNN-T).
いくつかの例では、動作は、発話に対する予備的トランスクリプションが発話のコンテキストに対応する接頭辞要素を表す単語を含むことを判定する動作をさらに含む。これらの例では、音声要素のコンテキストスコアを決定する動作は、発話に対する予備的トランスクリプションが発話のコンテキストに対応する接頭辞要素を表す単語を含むという判定に基づく。追加の例では、コンテキストに基づいて音声要素のコンテキストスコアを決定する動作は、部分語単位を表す要素を使用してコンテキスト用語を表すコンテキストFSTを使用してコンテキストスコアを決定する動作を含む。これらの例では、コンテキストFSTは、コンテキスト用語の部分語単位間の各遷移をバイアスするように構成された遷移重みと、遷移重みのバイアス効果を元に戻すように構成されたオフセット重みを有するバックオフアークとを含み得る。いくつかの例では、音声要素は、単語断片または書記素を含む。 In some examples, the operations further include an operation of determining that the preliminary transcription for the utterance includes a word representing a prefix element that corresponds to a context of the utterance. In these examples, the act of determining a context score for a speech element is based on determining that the preliminary transcription for the utterance includes a word representing a prefix element that corresponds to the context of the utterance. In additional examples, the act of determining a context score for a speech element based on context includes an act of determining a context score using a context FST representing a context term using an element representing a subword unit. In these examples, the context FST has transition weights configured to bias each transition between subword units of the context term, and an offset weight configured to reverse the biasing effects of the transition weights. off-arc. In some examples, the phonetic elements include word fragments or graphemes.
本開示の1つまたは複数の実装形態の詳細は、添付図面および以下の説明において記載されている。他の態様、特徴、および利点は、説明および図面、ならびに特許請求の範囲から明らかになるであろう。 The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
様々な図面中の同様の参照記号は、同様の要素を示す。 Like reference symbols in the various drawings indicate similar elements.
高精度の音声認識を提供するために、音声認識システムは、音声が発生するコンテキストを考慮に入れるように構成することができる。コンテキストは、数ある中で、曲名、アプリケーション名、連絡先の名前、または場所などの、言語の特定のドメインまたはクラスを表し得る。たとえば、音声認識システムが、ユーザが電話をかけようとしている可能性が高いと判断した場合、音声認識システムがユーザの連絡先のリスト内の連絡先の名前に向けて認識処理をバイアスすると、音声認識品質が改善する可能性がある。 To provide highly accurate speech recognition, speech recognition systems can be configured to take into account the context in which speech occurs. A context may represent a particular domain or class of language, such as a song title, an application name, a contact name, or a location, among others. For example, if the speech recognition system determines that the user is likely about to make a phone call, the speech recognition system biases the recognition process toward the names of contacts in the user's list of contacts. Recognition quality may be improved.
音声認識器は、多くの異なるタイプの状況において使用され得る。たとえば、音声認識器は、ディスプレイのないモバイルデバイスによって、ディスプレイを有するモバイルデバイスによって、携帯電話上で実行されるデジタルアシスタントアプリケーションによって、限定はしないが、デスクトップコンピュータ、ラップトップコンピュータ、スマートスピーカ、スマート家電、スマートディスプレイ、スマートヘッドフォン、または他のウェアラブルデバイスなどのコンピューティングデバイス上で実行されるデジタルアシスタントアプリケーションによって、車両内での使用に適合され得る。 Speech recognizers can be used in many different types of situations. For example, voice recognizers can be used by mobile devices without a display, by mobile devices with a display, by digital assistant applications running on mobile phones, and without limitation, desktop computers, laptop computers, smart speakers, and smart home appliances. It can be adapted for use in vehicles by digital assistant applications running on computing devices, such as smart displays, smart headphones, or other wearable devices.
音声認識器は、多くの異なる目的のために使用され得る。たとえば、音声認識器は、デバイスが特定の曲、または特定のアーティストによる音楽アルバムを再生することを要求するために使用され得る。音声認識器は、連絡先に電話をかけるため、もしくはテキストを送るため、または電子メール、メモ、もしくはカレンダーイベントに入力するために音声を転写するために使用され得る。発話が発生する状況を示す様々な要因は、コンテキストと呼ばれる場合がある。異なるコンテキストは、関連性があり、ユーザによって話される可能性が高い用語の異なるセットを有し得る。一般に、異なるコンテキストは、異なる単語、話題、ユーザアクション、アプリケーションなどに関連し得る。結果として、音声認識システムが識別するコンテキストに応じて、音声認識システムは、ユーザの予想される音声によりよく一致するように用語の異なるセットをバイアスすることができる。これは、一般的に音声内にありそうもないが、特定のコンテキストまたは状況内では発生する可能性がはるかに高くなる可能性がある、固有名詞および他の用語を認識する精度を大幅に改善することができる。 Speech recognizers can be used for many different purposes. For example, a speech recognizer may be used to request that the device play a particular song, or a music album by a particular artist. Speech recognizers can be used to transcribe speech for calling or sending texts to contacts, or for entering into emails, notes, or calendar events. Various factors that indicate the circumstances in which an utterance occurs may be referred to as context. Different contexts may have different sets of terms that are relevant and likely to be spoken by users. Generally, different contexts may relate to different words, topics, user actions, applications, etc. As a result, depending on the context that the speech recognition system identifies, the speech recognition system can bias different sets of terms to better match the user's expected speech. This greatly improves the accuracy of recognizing proper nouns and other terms that are generally unlikely in speech, but can be much more likely to occur within a specific context or situation. can do.
いくつかの実装形態において、重み付き有限状態トランスデューサ(FST)として表される、独立してトレーニングされたコンテキストn-gram言語モデルが、コンテキスト情報を表す。ASRシステムは、音声格子の要素のスコアリングに影響を与える別の構成要素としてコンテキストn-gram言語モデルを組み込むことによって、特定のコンテキストの用語に向けて認識プロセスをバイアスする。以下で論じられているように、コンテキストバイアスは、単語格子または部分語(たとえば、書記素または音素)の格子の要素のスコアを調整することができる。したがって、コンテキストバイアス言語モデルの寄与は、剪定後に単に適用されるのではなく、ビーム検索復号中の剪定決定における要因となる可能性がある。 In some implementations, an independently trained context n-gram language model, represented as a weighted finite state transducer (FST), represents the context information. ASR systems bias the recognition process toward terms in a particular context by incorporating a context n-gram language model as another component that influences the scoring of elements of the phonetic lattice. As discussed below, context biases can modulate the scores of elements of a word grid or a grid of subwords (eg, graphemes or phonemes). Therefore, the contribution of the context-biased language model may be a factor in the pruning decision during beam search decoding, rather than simply being applied after pruning.
エンドツーエンド音声認識モデルは、従来の音響モデル、発音モデル、および言語モデルを単一のニューラルネットワークに組み合わせるが、コンテキスト情報/知識をこれらのエンドツーエンド音声認識モデルに組み込むことは、様々な理由のために困難である。たとえば、効率的な復号のために、エンドツーエンドモデルは、一般に、ビーム検索の各ステップにおいて少数の候補に剪定する。したがって、固有名詞などのまれな単語およびフレーズは、ビームから離れる可能性が高い。以下でより詳細に説明されているように、剪定決定前にコンテキストバイアスを格子に組み込むことによって、コンテキストバイアスは、現在のコンテキストにおいて使用されている可能性がより高い一般的でない用語に対してより大きい重みを与えることができ、それによって、これらの候補が早まって省略されないように、これらの候補をビーム検索復号プロセスのパス内に保持する。 End-to-end speech recognition models combine traditional acoustic, pronunciation, and language models into a single neural network, but incorporating contextual information/knowledge into these end-to-end speech recognition models is important for various reasons. It is difficult because of For example, for efficient decoding, end-to-end models are generally pruned to a small number of candidates at each step of beam search. Therefore, rare words and phrases such as proper nouns are likely to fall off the beam. As explained in more detail below, by incorporating context bias into the lattice before the pruning decision, the context bias is more A large weight can be given to keep these candidates in the path of the beam search decoding process so that they are not prematurely omitted.
ASRシステムは、多数の異なるコンテキストの各々に関連する用語を表すために、コンテキストFSTを作成し得る。これらのFSTは、ユーザのデバイス上に記憶されるか、またはユーザのデバイスによってアクセス可能な情報(たとえば、ユーザのカレンダー、ユーザの連絡先のリスト、ユーザのメディアライブラリなど)に基づいて、ユーザに対してパーソナライズすることができる。ASRシステムは、異なるコンテキストFSTの関連性を示す接頭辞のコレクションを表す接頭辞FSTを組み込むこともできる。たとえば、「open(開く)」、「start(開始する)」、および「close(閉じる)」という接頭辞は、話される次の用語がアプリケーション名であることを示す接頭辞であり得るので、それらの接頭辞のうちの1つの検出は、音声認識システムに、アプリケーション名のためのコンテキストFSTをアクティブにさせ得る。別の例として、「play(再生する)」という接頭辞は、メディアアイテムのためのコンテキストFSTが使用される可能性があることを示し得、「call(電話をかける)」、「text(テキスト)」、または「e-mail(電子メール)」という接頭辞は、連絡先の名前のためのコンテキストFSTが使用されるべきであることを示し得る。 An ASR system may create a context FST to represent terms associated with each of a number of different contexts. These FSTs are provided to the user based on information stored on or accessible by the user's device (e.g., the user's calendar, the user's list of contacts, the user's media library, etc.). It can be personalized. ASR systems may also incorporate prefix FSTs that represent a collection of prefixes that indicate the relevance of different context FSTs. For example, the prefixes "open," "start," and "close" can be prefixes that indicate that the next term spoken is the application name, so Detection of one of those prefixes may cause the speech recognition system to activate a context FST for the application name. As another example, the prefix "play" may indicate that the context FST for the media item may be used, "call", "text", etc. )", or the prefix "e-mail" may indicate that the context FST for the contact's name should be used.
書記素レベルではなく単語断片においてバイアスを実行することは、単語間をより正確に区別し、検索ビーム上に維持される必要がある候補の数を制限することによって、エンドツーエンドモデルによるコンテキストバイアスをさらに改善することができる。ビーム剪定前にコンテキストバイアスを適用することは、関連する単語断片を検索ビームから早まって削除することを回避し得る。 Performing the bias at word fragments rather than at the grapheme level helps to more accurately distinguish between words and eliminates context bias with end-to-end models by limiting the number of candidates that need to be maintained on the search beam. can be further improved. Applying context bias before beam pruning may avoid prematurely removing relevant word fragments from the search beam.
図1は、音声を転写するためにコンテキストバイアスを使用する例示的な音声認識器100を示す。いくつかの実装形態において、携帯電話、タブレットコンピュータ、スマートスピーカ、スマートディスプレイ、スマートヘッドフォン、スマート家電、ラップトップコンピュータ、ウェアラブルデバイス、デスクトップコンピュータなどのユーザデバイス110が、音声認識器100を記憶および実行する。これらの実装形態において、音声認識器100は、サーバまたは他のネットワークベースのリソースなどの他のデバイスとの通信、または他のデバイスからのサポートなしに、ユーザデバイス110上でローカルに実行されるように構成される。しかしながら、本開示の範囲から逸脱することなく、ユーザデバイス110と通信するリモートサーバまたは他のネットワークベースのリソースが、音声認識器100を記憶および実行し得る。いくつかの構成において、音声認識器100の機能は、ユーザデバイス110と、リモートサーバまたは他のネットワークベースのリソースとの間で分割される。
FIG. 1 shows an example speech recognizer 100 that uses context bias to transcribe speech. In some implementations, a
図示されている例では、ユーザ115は、ユーザデバイス110の1つもしくは複数のマイクロフォンおよび/またはユーザデバイス110と通信する少なくとも1つのマイクロフォンによって検出される発話120を話す。ユーザデバイス110は、発話120に対するトランスクリプション185を決定するために音声認識器100を使用する、この例では、ユーザ115は、会話型デジタルアシスタントへのコマンド、たとえば、要求を話す。トランスクリプション185が決定されると、ユーザデバイス110は、トランスクリプション185を解釈し、要求されたアクションを実行すること、検索エンジンにクエリを提供すること、トランスクリプションを表示すること、メッセージフィールド内にトランスクリプションを入力することなど、応答して適切なアクションをとることができる。
In the illustrated example,
ユーザ115が発話120を話す前に、ユーザデバイス110は、音声認識精度を改善する高速で正確なコンテキストバイアスを準備するためのステップをとる。これらのステップのうちの1つは、どの用語(たとえば、単語および/またはフレーズ)が異なるコンテキストまたは話題ドメインに関連するかを決定するために、ユーザ115および/またはユーザデバイス110に関連付けられた情報を取得および処理することである。ユーザデバイス110(たとえば、ユーザデバイス110のメモリハードウェア)は、ユーザの連絡先のリスト、ユーザのメディアライブラリ内のアイテムの名前、近くの場所の名前、およびインストールされたアプリケーションの名前などのコンテキスト情報のデータストア150を含む。データストア150またはその一部は、リモート記憶デバイス、たとえば、クラウドストレージ環境のリモートサーバ101上にも存在し得る。音声認識器100は、これらの用語のリストを取得するために、ユーザデバイス110のソフトウェアと通信することができる。これらの異なるコンテキストデータセット内の情報は、ユーザが連絡先を追加または削除したとき、ユーザのメディアライブラリが変更されたとき、ユーザが場所を変更したときなどに、随時変化する。音声認識器は、使用するコンテキスト情報を最新の情報に更新するために、データのリストに対して更新を定期的に要求することができる。これは、たとえば、地図サービス用のサーバ、ユーザのメディアライブラリに関する情報をホストするサーバなどから、ネットワークを介して情報を取得することを含み得る。
Before
音声認識器100は、コンテキストFST160、160a～nを生成/更新するために、データストア150からのコンテキスト情報を使用するように構成されたコンテキストFST生成モジュール155をさらに含む。コンテキストFST生成モジュール155は、システムが検出するように構成されたコンテキストごとに、異なるそれぞれ重み付けされたコンテキストFST160を決定することができる。コンテキストFST生成モジュール155は、各コンテキストFST160を生成するために、データストア150からのデータにアクセスする。たとえば、第1のコンテキストFST160aは、ユーザ115の連絡先のリストからの名前を表すことができ、第2のコンテキストFST160bは、ユーザ115のメディアライブラリ内のアイテムの名前を表すことができ、第3のコンテキストFST160cは、ユーザ1115の現在の場所の近くの場所の場所名を表すことができるなどである。コンテキストFST生成モジュール155は、データストア150内のデータにおける変化を検出したことなどに応答して、継続的に、たとえば、一定の間隔で、コンテキストFST160を更新することができる。
Speech recognizer 100 further includes a context
各コンテキストFST160は、他のコンテキストFST160に関連付けられたコンテキストとは異なるコンテキストに対応するので、すべてのコンテキストFST160が同時に使用するのに適切であるとは限らない。本明細書で使用される場合、各FST160が対応するコンテキストは、コンテキストドメインと呼ばれる場合がある。現在のコンテキストに一致しない他のコンテキストFST160をアクティブ化せずに、現在のコンテキストに一致する関連するコンテキストFST160を選択的にアクティブ化するために、様々な技法を使用することができる。コンテキストバイアスを現在のコンテキストに適合させる1つの方法は、異なるそれぞれのコンテキストに対応する接頭辞の異なるセットの出現を各々が表す接頭辞FST163を使用することである。たとえば、「call(電話をかける)」という接頭辞の出現は、連絡先の名前が次の単語である可能性が高いことを示すことができるので、この接頭辞のための接頭辞FST163は、音声認識器100に連絡先名コンテキストFST160を有効にさせることができる。別の例として、「play(再生する)」という接頭辞の出現は、メディアアイテム名が次の単語である可能性が高いことを示すことができるので、この接頭辞のための接頭辞FST163は、音声認識器にメディアアイテム名コンテキストFST160を有効にさせることができる。各コンテキストFST160は、コンテキストFST160内の用語の可能性が高いことを示すために、ユーザ入力ログの分析を通じて決定された1つまたは複数の接頭辞のセットを表す対応する接頭辞FST163をオプションで有することができる。いくつかの実装形態において、接頭辞FSTは、空の接頭辞オプション164を含み、これは、最初に接頭辞の初期の発話なしで、コンテキストバイアスの使用を可能にする。
Since each context FST 160 corresponds to a different context than the context associated with other context FSTs 160, not all context FSTs 160 may be suitable for use at the same time. As used herein, the context to which each FST 160 corresponds may be referred to as a context domain. Various techniques may be used to selectively activate related context FSTs 160 that match the current context without activating other contexts FSTs 160 that do not match the current context. One way to adapt the context bias to the current context is to use prefixes FST163, each representing occurrences of a different set of prefixes corresponding to different respective contexts. For example, the occurrence of the prefix "call" can indicate that the contact's name is likely to be the following word, so the prefix FST163 for this prefix is Speech recognizer 100 may have contact name context FST 160 enabled. As another example, the occurrence of the prefix "play" can indicate that the media item name is likely to be the following word, so the prefix FST163 for this prefix is , can cause the speech recognizer to enable the media item name context FST 160. Each context FST 160 optionally has a
いくつかの実装形態において、コンテキストFST160および接頭辞FST163は、音声認識プロセス中に使用するための単一のコンテキストFSTに構成される。代替的には、コンテキスト分析モジュール165などのモジュールは、どのコンテキストFST160が異なるコンテキストに対して適用されるかを選択するために、様々な要因を使用することができる。
In some implementations, context FST 160 and prefix
音声認識器100は、特徴抽出モジュール130と音声認識モデル200とを含む、音響情報を処理するための様々な要素を含む。発話120が話されると、ユーザデバイス110の1つまたは複数のマイクロフォンは、発話120の音響特性を表すオーディオデータ125を生成する。特徴抽出モジュール130は、オーディオデータ125を受信し、オーディオデータ125から音響特徴135(たとえば、対数メル特徴)を生成する。たとえば、モジュール130の出力は、オーディオデータ125のウィンドウまたはフレーム(たとえば、セグメント)ごとの音響特徴ベクトルとすることができ、ここで、音響特徴ベクトルは、異なる周波数帯域におけるエネルギーレベルなどの特徴を示す値を含む。
Speech recognizer 100 includes various elements for processing acoustic information, including
音声認識モデル200は、音響特徴135を入力として受信し、異なる音声要素が発生した尤度を表す音声認識スコア145を出力として計算する。音声要素は、単語断片または書記素を含み得る。たとえば、音声認識モデル200は、出力ターゲットのセット、たとえば、潜在的に出力される可能性がある異なる書記素または単語断片のセットにわたる確率分布を表すスコアのベクトルを出力し得る。したがって、音声認識モデル200からの音声認識スコア145の出力ベクトルは、様々な書記素または単語断片がオーディオデータ125の一部における音声を正確に記述する尤度を示すことができる。音声認識モデル200は、発音モデル、音響モデル、および言語モデルの機能を単一のモデル(たとえば、ニューラルネットワーク)に組み合わせたエンドツーエンドモデルを含むので、音声認識モデル200は、音響特徴125、または生のオーディオデータ125さえも受信し、それに応答して、正字法特徴(たとえば、書記素、単語断片、または単語)の尤度を示す出力スコアを提供し得る。
発話120を認識するとき、音声認識器100は、コンテキストスコア166を決定するために、発話120のコンテキストを示すデータも処理する。音声認識器100は、発話120のコンテキスト122に関する様々なタイプの情報を受信するコンテキスト分析モジュール165を含むことができる。たとえば、コンテキスト分析モジュール165は、デバイス110の場所、デバイス110を使用するユーザ115によって実行されているタスク、デバイス110上で開いているかまたはアクティブであるアプリケーションなどに関する情報を受信し得る。コンテキスト分析モジュール165は、ビーム検索プロセス中の部分的または予備的なトランスクリプション186内に含まれる、最近認識された書記素、単語断片、単語、またはフレーズなどの、最近の音声認識決定を示すデータを受信することもできる。このテキスト情報のいずれかまたはすべてを使用して、コンテキスト分析モジュール165は、コンテキストFST160の中から選択するか、またはコンテキストFST160に異なる重みを適用することができる。コンテキストFST160(たとえば、発話120の現在のコンテキストに適用可能であるように選択された1つまたは複数のコンテキストFST160)は、次いで、データストレージ150内の識別された用語またはフレーズに向けて認識プロセスをバイアスすることができるコンテキストスコア166を生成するために使用される。上記で論じられているように、コンテキストFST160ならびに接頭辞FST163および164は、以前に生成され、記憶されたので、それらは、発話120が話される前に利用可能である。
When recognizing the
コンテキスト分析モジュール165は、予備的トランスクリプション186において検出された所定の接頭辞またはキーワードの存在に基づいて、所与の発話120をバイアスするためにどのコンテキストFST160が使用されるかを制限することができる。コンテキスト分析モジュール165は、認識された音声を接頭辞または他のキーワードと比較するために任意の適切な方法を使用し得るが、プロセスは、接頭辞FST163および164を使用して効率的に行われ得る。接頭辞FST163および164の重みおよび遷移は、(たとえば、予備的トランスクリプション186において)既知の接頭辞の音声単位が認識されたとき、コンテキストFST160のうちのどれが最も関連する可能性が高いかと、それらのバイアスの影響がどのように重み付けされるべきかとを示すことができる。接頭辞163は、接頭辞FSTがコンテキストFST160のうちの1つまたは複数を使用するバイアスに直接つながるように、コンテキストFST160に連結またはリンクすることができる。
上記で論じられているように、接頭辞FST163は、一般的に使用される接頭辞のセットが前にある場合にのみ、コンテキストFSTと、対応するバイアスフレーズとをアクティブ化するために使用される。接頭辞が話されると、特定のフレーズが話される信頼度がより高くなる。接頭辞FST163をコンテキストFST160にリンクすることによって、コンテキスト分析モジュール165は、全体的なバイアスを改善するためにバイアス重みを増加させる。たとえば、「call(電話をかける)」という単語を話した場合、接頭辞FST163は、コンテキスト分析モジュール165に連絡先に対応するコンテキストFSTを有効にさせるので、認識は、連絡先の名前に向けてバイアスされることになる。したがって、連絡先の名前は、トランスクリプション185内に含まれている可能性が高くなる。
As discussed above, prefix FST163 is used to activate the context FST and the corresponding bias phrase only if it is preceded by a set of commonly used prefixes. . When a prefix is spoken, there is greater confidence that a particular phrase will be spoken. By linking
メディア名、連絡先、およびアプリケーション名などの多くのカテゴリに対してバイアスがアクティブである場合、標準的な単語の使用から気をそらす過剰なバイアスのため、認識品質は、低下する可能性がある。接頭辞FST163は、空の接頭辞オプション164を有し、これは、所定の接頭辞の出現なしにコンテキストFST160が使用されることを可能にするが、空の接頭辞オプション164は、過剰バイアスを防ぐために、接頭辞が話されない場合、より小さいバイアス重みを適用する。
If bias is active for many categories such as media names, contacts, and application names, recognition quality can be degraded due to excessive bias that distracts from the use of standard words. . Prefix FST163 has an
コンテキスト情報122、186に基づくコンテキストスコア166、および音響情報135に基づく音声認識スコア122、186は、発話120のトランスクリプション185を決定するために一緒に使用される。特に、コンテキストスコア166は、音声認識全体よりも現在のコンテキストにおける特定のユーザ115に関連する用語に向けて認識をバイアスする。いくつかの実装形態において、スコアコンバイナ170は、音声格子175において使用される結合されたスコア172を生成するために、音声認識スコア145をコンテキストスコア166と結合する。重要なことに、コンテキストスコア166のバイアスの影響は、格子175の剪定の前、およびビーム検索におけるビーム剪定の前に、音声格子175に適用される。結果として、コンテキストスコア166の影響により、そうでなければ剪定されていた可能性がある関連用語が、ビーム検索プロセスおよび関連する剪定を通じて維持される。「ビーム検索プロセス」は、「ビーム検索復号プロセス」と互換的に呼ばれる場合がある。
The context score 166 based on the
音声認識器100は、音声格子175を通る潜在的なパスを評価し、トランスクリプション185を決定するために、ビーム検索プロセスを実行する。格子175を通る各パスは、異なる候補トランスクリプションを表すことができる。一般に、各々がそれぞれの候補トランスクリプションに関連付けられているすべての可能な単語シーケンスを評価することは、効率的ではないか、または可能ですらない。したがって、コンテキストスコア166およびASRスコア145が計算され、結合されたスコア172として結合された後、ビーム剪定プロセス180が、最も有望な単語パスに検索を誘導する。ビーム剪定180は、可能性が高い単語または単語断片について検索される格子175の範囲を減少させることができる。各単語断片の確率が計算されると、最も可能性が高い経路のみが残るまで、可能性の低い検索パスが剪定される。
Speech recognizer 100 performs a beam search process to evaluate potential paths through
ビーム検索プロセスの出力は、ユーザの発話120のトランスクリプション185である。トランスクリプション185が決定されると、トランスクリプション185は、様々な方法のいずれかにおいて使用することができ、たとえば、ユーザ115に対して表示される、テキストフィールドに入力される、検索エンジンへのクエリまたはデジタル会話アシスタントへの要求として送信される、音声コマンドとして解釈されるなどである。たとえば、ユーザデバイス110は、テキストメッセージをまたは電子メールを書き込む、電話をかける、曲を再生するなど、ユーザデバイス110が実行するアクションを識別するために、トランスクリプション185を使用する。たとえば、トランスクリプションが「Call Jason(Jasonに電話をかける)」である場合、ユーザデバイス110は、Jasonという名前の連絡先に電話をかけることを開始し、「Calling Jason now(今、Jasonに電話をかけています)」などの合成された発話などの、ユーザ115への確認で応答し得る。
The output of the beam search process is a
図2を参照すると、音声認識モデル200は、対話型アプリケーションに関連するレイテンシ制約に準拠するE2E、RNN-Tモデル200を含み得る。RNN-Tモデル200は、小さい計算フットプリントを提供し、従来のASRアーキテクチャよりも少ないメモリ要件を利用し、RNN-Tモデルアーキテクチャを、(たとえば、リモートサーバとの通信が必要とされない)ユーザデバイス102上で完全に音声認識を実行するのに適したものにする。RNN-Tモデル200は、エンコーダネットワーク210と、予測ネットワーク220と、共同ネットワーク230とを含む。従来のASRシステムにおける音響モデル(AM)にほぼ類似しているエンコーダネットワーク210は、積層型長・短期記憶(LSTM: Long Short-Term Memory)層のリカレントネットワークを含む。たとえば、エンコーダは、d次元の特徴ベクトルのシーケンス(たとえば、音響フレーム110(図1))x=(x1,x2,...,xr)、ここで、
Referring to FIG. 2, a
を読み取り、各時間ステップにおいてより高次の特徴表現を生成する。このより高次の特徴表現は、 and generates a higher-order feature representation at each time step. This higher-order feature representation is
として示される。 is shown as
同様に、予測ネットワーク220も、LSTMネットワークであり、これは、言語モデル(LM)のように、これまでに最終ソフトマックス層240によって出力された非空白シンボルのシーケンスy0,...,yui-1を密な表現 Similarly, the prediction network 220 is also an LSTM network, which, like a language model (LM), represents the sequence of non-blank symbols y 0 ,...,y so far output by the final softmax layer 240 Dense representation of ui-1
に処理する。最後に、RNN-Tモデルアーキテクチャでは、エンコーダおよび予測ネットワーク210、220によって生成された表現は、共同ネットワーク230によって結合される。共同ネットワークは、次いで、次の出力記号にわたる分布である to be processed. Finally, in the RNN-T model architecture, the representations produced by the encoder and prediction networks 210, 220 are combined by a joint network 230. The joint network is then the distribution over the output symbols
を予測する。別の言い方をすれば、共同ネットワーク230は、各出力ステップ(たとえば、時間ステップ)において、可能な音声認識仮説にわたる確率分布を生成する。ここで、「可能な音声認識仮説」は、指定された自然言語における記号/文字を各々が表す出力ラベルのセットに対応する。したがって、共同ネットワーク230は、出力ラベルの所定のセットの各々の出現の尤度を表す値のセットを出力し得る。この値のセットは、ベクトルとすることができ、出力ラベルのセットにわたる確率分布を示すことができる。場合によっては、出力ラベルは、書記素(たとえば、個々の文字、ならびに潜在的には句読点および他の記号)であるが、出力ラベルのセットは、そのように制限されない。共同ネットワーク230の出力分布は、異なる出力ラベルの各々に対する事後確率値を含むことができる。したがって、異なる書記素または他の記号を表す100の異なる出力ラベルが存在する場合、共同ネットワーク230の出力yiは、出力ラベルごとに1つずつ、100の異なる確率値を含むことができる。確率分布は、次いで、中間トランスクリプション115を決定するための(たとえば、ソフトマックス層240による)ビーム検索プロセスにおいて、候補正字法要素(たとえば、書記素、単語断片、および/または単語)を選択し、スコアを割り当てるために使用することができる。
Predict. Stated another way, collaborative network 230 generates a probability distribution over possible speech recognition hypotheses at each output step (eg, time step). Here, a "possible speech recognition hypothesis" corresponds to a set of output labels, each representing a symbol/character in a specified natural language. Collaborative network 230 may therefore output a set of values representing the likelihood of occurrence of each of the predetermined set of output labels. This set of values can be a vector and can indicate a probability distribution over the set of output labels. In some cases, the output labels are graphemes (eg, individual letters, and potentially punctuation marks and other symbols), but the set of output labels is not so limited. The output distribution of collaborative network 230 may include posterior probability values for each of the different output labels. Thus, if there are 100 different output labels representing different graphemes or other symbols, the output y i of collaborative network 230 can include 100 different probability values, one for each output label. The probability distribution then selects candidate orthographic elements (e.g., graphemes, word fragments, and/or words) in a beam search process (e.g., by softmax layer 240) to determine
ソフトマックス層240は、対応する出力ステップにおいてモデル200によって予測される次の出力記号として、分布において最も高い確率を有する出力ラベル/記号を選択するために、任意の技法を用い得る。このようにして、RNN-Tモデル200は、条件付き独立仮定をせず、むしろ、各記号の予測は、音響だけでなく、これまでに出力されたラベルのシーケンスにも条件付けられる。RNN-Tモデル200は、出力記号が将来の音響フレーム110から独立していることを想定しており、これは、RNN-Tモデルがストリーミング方式で用いられることを可能にする。
Softmax layer 240 may use any technique to select the output label/symbol that has the highest probability in the distribution as the next output symbol predicted by
いくつかの例では、RNN-Tモデル200のエンコーダネットワーク210は、時間短縮層と、それに続く8つの2000次元LSTM層と、それらの各々に続く600次元投影層とを含む。予測ネットワーク220は、2つの2048次元LSTM層を有し得、その各々にも、640次元の投影層が続く。最後に、共同ネットワーク230は、640の隠れユニットも有し得、その後に4096の単語断片ソフトマックス出力が続く。デコーダは、層ごとに2000の隠れユニットと600次元の投影とを有する2つのLSTM層である。エンコーダおよびデコーダは、600の隠れユニットを有する共同ネットワークに供給される。共同ネットワークは、96ユニット(書記素実験用)または4096ユニット(単語断片用)のいずれかを有するソフトマックス層に供給される。すべてのRNN-Tモデルは、4096のバッチサイズを有する8×8テンソル処理ユニット(TPU: Tensor Processing Unit)スライス上のTensorFlowにおいてトレーニングされる。推論中、各発話は、コンテキストFST160を構築するために使用されるバイアスフレーズのセットと関連付けられる。図4を参照して以下で示されているように、バイアスFST160における各アークは、同じ重みを有する。この重みは、上記のテストセットにおけるパフォーマンスを最適化するために、カテゴリ(曲、連絡先など)ごとに独立して調整される。
In some examples, the encoder network 210 of the RNN-
一般に、音声認識モデル200などのASRモデルは、様々な方法を使用してトレーニングすることができ、プロセスは、大量のトレーニングデータを使用し得る。図1に戻って参照すると、音声認識モデル200(たとえば、RNN-T)は、教師なしデータ193、教師ありデータ197、および/または合成データ195を含むトレーニングデータにおいてトレーニングし得る。
In general, ASR models, such as
固有名詞の認識を改善するために、音声認識モデル200は、名詞のより多くの例を提供するために大量のテキストデータを用いてトレーニングされ得、したがって、モデルが曝される固有名詞の種類を増加させる。教師なしデータ193は、音声検索トラフィックから発話をマイニングすることによって収集することができる。これらの発話は、次いで、ASRモデルによって復号され、高い信頼度を有する発話のみがさらなるトレーニングのために使用される。トレーニングが大量の教師なしデータ193によって支配されないことを確実にするために、トレーニングの各ステップ中、教師なしデータ193は、正確に転写されていることが確認または検証された教師ありデータ194と組み合わされる。たとえば、各ステップ中、トレーニングバッチは、最良のパフォーマンスを与えることが経験的にわかっているので、80%の割合で教師ありデータ197で満たされ、20%の割合で教師なしデータ193で満たされ得る。トレーニング中の固有名詞の認識を強調するために、教師なしデータ193における自動的に生成されたトランスクリプションをフィルタリングするために、固有名詞タガープロセスを実行することができる。いくつかの実装形態において、固有名詞を含むとタグ付けされた例示的な発話のみがトレーニングにおいて使用される。具体的には、固有名詞タガーは、固有名詞ごとに固有名詞の音声表現が生成されるように、各発話に対して実行され得る。たとえば、「Caitlin」は、k eI t l @ nという音素によって表される。次に、同じ音素シーケンスを有するレキシコン内の代替単語、たとえば、「Kaitlyn」が観察される。グラウンドトゥルースおよび代替単語が与えられると、これらの単語は、トレーニング中にランダムにサンプリングされる。これは、トレーニング中にモデルにより多くの固有名詞を与える。モデル200がトレーニング中により多くの名前を綴ることができる場合、復号中にコンテキストFSTが使用されるときにこれらの名前を綴ることがより確実になり、単語がビームから離れない。
To improve recognition of proper nouns, the
いくつかの実装形態において、様々な固有名詞を有する文章を生成し、次いで、対応するオーディオデータを合成することによって、合成トレーニングデータ195を作成することができる。この技法は、トレーニング例として使用することができるオーディオ-テキストペアの数を大幅に増やすことができる。いくつかの実装形態において、生成されたテキストおよび合成されたオーディオは、特定のドメインまたはカテゴリに対して決定される。たとえば、例示的なテキストを生成するために、カテゴリ固有の接頭辞と固有名詞とを組み合わせることができる。次いで、生成されたテキストの合成音声を生成するために、テキスト音声化技法を使用することができる。このようにして、カテゴリごとに多くの発話を有するトレーニングデータセットを作成することができる。たとえば、テキストメッセージ、電子メールなどの通信を表すために、人工的な例を生成することができる。同様に、別のカテゴリにおいて、メディア要求を表すために、例を生成することができる。別のカテゴリにおいて、コマンド、アクション、またはアプリケーションに対する要求をシミュレートするために、例を生成することができる。合成データ195が生成されると、教師ありデータ197および合成データ195の組合せをトレーニングのために使用することができ、したがって、教師ありデータセットからのコア例を提供し、特定のドメインまたはカテゴリからの固有名詞または可能性の高い言語のモデルの処理をさらに改善するために、合成例を使用する。ルームシミュレータは、合成データおよび教師ありデータ195、197の一方または両方における発話の少なくともいくつかにノイズを追加し得る。トレーニング中、各バッチは、90%の割合で教師ありデータ197で満たされ、10%の割合で合成データ195で満たされ得る。
In some implementations,
図3は、音声認識スコア145(図1)とコンテキストスコア166(図1)とに基づいてスコアコンバイナ170(図1)によって出力された格子175上でビーム剪定プロセス(たとえば、図1の剪定180)を実行する音声認識器100の図300を示す。示されている例では、ユーザ115が、自動ペット給餌器を含む家庭用機器を制御することができるユーザデバイス110上のアプリケーションを起動する。アプリケーションの起動は、コンテキストFST生成モジュール155に、ペット給餌器などの家庭用機器に関連付けられた単語をバイアスするために1つまたは複数のコンテキストFST160を生成させる。ユーザ115は、「feed cat(猫に餌をあげて)」という単語を含む発話120をユーザデバイス110の1つまたは複数のマイクロフォンに話す。いくつかの例では、この発話120は、家庭用機器を制御するためのアプリケーションを起動するためにユーザデバイス110を呼び出す。他の例では、ユーザ115は、発話120を話す前に、アプリケーションを起動するために別の音声コマンドを話し得る。追加の例では、ユーザ115は、ユーザデバイス110が発話120を処理することができるように、ユーザデバイス110をスリープ状態から起こすためにホットワードを話し得る。
FIG. 3 illustrates a beam pruning process (e.g., pruning 180 of FIG. ) shows a diagram 300 of a speech recognizer 100 that performs. In the example shown,
いくつかの例では、コンテキスト分析モジュール165(図1)は、(たとえば、予備的トランスクリプション186において)「feed(餌をあげて)」という接頭辞の出現を、ホームオートメーションのカテゴリ/コンテキストに対して一般的に使用される接頭辞として認識し、それによって、「fish(魚)」、「dog(犬)」、および「cat(猫)」などの、「feed」に続く可能性が高い単語をバイアスする。すなわち、「feed」という接頭辞を含む対応する接頭辞FST163は、音声認識器100に、ホームオートメーションのカテゴリ/コンテキストに関連付けられたコンテキストFST160を有効にさせ得る。したがって、コンテキスト分析モジュール165は、コンテキストスコア166を計算するために、コンテキストFST160と接頭辞FST163とを参照し、同時に、音声認識モジュール200は、接頭辞FST163に対応する音声認識スコア145を計算するためにオーディオ特徴135を評価する。
In some examples, the context analysis module 165 (FIG. 1) assigns occurrences of the prefix "feed" (e.g., in the preliminary transcription 186) to the home automation category/context. recognized as a commonly used prefix for "feed," such as "fish," "dog," and "cat." Bias words. That is, a
ステップ1から3は、対応する発話120において「feed」という単語に続く「cat」という単語のオーディオ特徴135の異なるパート/部分を認識する際に音声認識器100が実行する連続する出力ステップを示す。上記の注釈に記載されているように、ホームオートメーションコンテキストFST160に対応する接頭辞FST163に関する接頭辞としてコンテキスト分析モジュール165によって認識された予備的トランスクリプション186における「feed」という単語の出現は、コンテキスト分析モジュール165に、ステップ1～3において行われる様々な書記素決定をバイアスする際に使用するためのコンテキストスコア166を生成させる。
ステップ1において、スコアコンバイナ170は、結合されたスコア172を計算するためにASRスコア145(「音声認識スコア」と互換的に呼ばれる)とコンテキストスコア166とを結合する。結合されたスコア172に基づいて、結合されたスコア172が1.1の「c」という書記素が最も可能性が高いと識別される。ASRスコア145およびコンテキストスコア166は、剪定プロセスの各ステップにおいて再計算される。
In
ステップ2において、スコアコンバイナ170は、結合されたスコア172を計算するために、現在のステップの出力分布における出力ラベルの各々について、ASRスコア145(「音声認識スコア」と互換的に呼ばれる)とコンテキストスコア166とを結合する。したがって、ステップ2において、結合されたスコア172が0.9の「a」という書記素が最も可能性が高いと識別される。
In
ステップ3において、0.6の最も高いASRスコア145は、「r」という書記素に対するものであり、これは、結果として生じるトランスクリプション185において「car(車)」という単語を誤って出力するように「t」という書記素を省略する剪定プロセスをもたらすが、0.8のコンテキストスコア166は、「t」という書記素について高い尤度を示す。したがって、「t」という書記素に対する0.3のASRスコア145は、ステップ3の格子175において最も高いASRスコア145ではないが、剪定プロセスは、ここで、1.0の結合されたスコア172により、「t」という書記素をステップ3の格子175における最も高い出力ラベルとして識別する。したがって、剪定プロセスは、ここでは、結果として生じるトランスクリプション185において「feed」という単語に続く「cat」という候補単語を正しく出力し得る。この例では、音声認識器100がビーム剪定180の後までコンテキストスコア166を考慮しなかった場合、ビーム剪定180は、コンテキストスコアを適用する機会なしに、「cat」という正しい候補単語をビームから早まって剪定したであろう。したがって、ユーザデバイス115上の開いている/起動されているアプリケーションから導出されたホームオートメーションの現在のコンテキスト122、および予備的トランスクリプション186における「feed」という接頭辞の認識は、協働して、音声認識器100に、音声認識モデル200が「c」および「a」という可能性のある書記素を識別した後に、「r」という書記素のコンテキストスコア166よりも高い「t」という書記素のコンテキストスコア166を計算させる。したがって、音声認識器100は、格子175における候補書記素または単語断片に対してビーム剪定180を適用する前に、コンテキストスコア166を生成し、コンテキストスコア166を音声認識モデル200から出力されたASRスコア145と結合することによって、トランスクリプション185の精度を改善することができる。音声認識器100の音声認識モデル200がRNN-Tを含む場合、音声認識器100は、書記素が各出力ステップにおいて個別に出力されるように、ストリーミング方式においてトランスクリプション185を出力し得る。いくつかの例では、音声認識器100は、トランスクリプション180において複数の書記素を含む単語断片または単語全体を出力するのを待ち得る。
In
いくつかの実装形態において、ニューラルネットワークコンテキストバイアスは、単語境界においてのみコンテキストスコア166を適用する。この手法は、コンテキストフレーズのリストが曲名または連絡先などの多くの固有名詞を含む場合には効果的でない場合がある。ニューラルネットワークモデルは、ビーム検索中に書記素または単語断片などの部分語単位を予測するので、バイアスされるべき単語がビーム内に存在しない場合、単語境界においてコンテキストスコアを適用することは、機能しない。 In some implementations, the neural network context bias applies context scores 166 only at word boundaries. This approach may not be effective if the list of context phrases contains many proper nouns, such as song names or contacts. Neural network models predict subword units such as graphemes or word fragments during beam search, so applying context scores at word boundaries will not work if the words to be biased are not present in the beam. .
図4は、単語の各部分語単位に重みを押し付けるオンザフライ(OTF)再スコアリング技法を示す概略図400である。具体的には、図4は、バックオフアークを有する部分語FSTを示し、ここで、バックオフアークは、早期にブーストされるが、フレーズ全体に一致しない接頭辞に重みを人為的に与えることを回避するために減算コストを提供するための負の重みに対応する。すなわち、部分語FSTは、現在の状態に到達する前に追加された重みを無効にするバックオフアークを含む。単語の各部分語単位に重みを押し付けることによって、図4のOTF再スコアリング技法は、単語をビーム上に維持するのを助けることを目的とする。したがって、部分語単位ごとにバイアスすることは、バイアス候補をビームから早まって剪定する可能性を減らすので、部分語単位ごとにバイアスすることは、各単語の末尾においてバイアスすることよりも効果的である。 FIG. 4 is a schematic diagram 400 illustrating an on-the-fly (OTF) rescoring technique that imposes weights on each subword unit of a word. Specifically, Figure 4 shows a subword FST with backoff arcs, where backoff arcs artificially give weight to prefixes that are boosted early but do not match the entire phrase. Corresponds to negative weights to provide subtraction costs to avoid. That is, the subword FST includes a backoff arc that negates the added weights before reaching the current state. By imposing weights on each subword unit of a word, the OTF rescoring technique of Figure 4 aims to help keep the word on the beam. Therefore, biasing by subword units is more effective than biasing at the end of each word because biasing by subword units reduces the chance of prematurely pruning bias candidates from the beam. be.
一般に、書記素をバイアスすることは、コンテキストFST160と一致する部分的な書記素を有する不必要な単語でビームを溢れさせる。たとえば、図4に示されている例では、バイアス単語が「cat」である場合、コンテキストFST生成モジュール155は、「c」、「a」、および「t」という書記素がバイアスされるように、対応するコンテキストFST160を生成する。最初に「c」という書記素がバイアスされ、したがって、「cat」および「car」は、ビーム上に乗せられ得る。しかしながら、バイアスが単語断片レベルにおいて適用される場合、関連する部分語の一致が疎らになり、したがって、より多くの関連する単語がビームに来る。前の例に従って、バイアスする単語断片が「cat」である場合、「car」は、ビームに来ない。バイアスがより長い単位において実行されるように、書記素から単語断片バイアスに切り替えることは、より多くの関連候補がビーム上に維持されるのを助け、パフォーマンスを改善する。
In general, grapheme biasing floods the beam with unnecessary words that have partial graphemes matching the context FST 160. For example, in the example shown in FIG. 4, if the bias word is "cat," the context
一般に、ユーザの曲名、アプリ名、および連絡先の名前などの特定のドメインへのコンテキストバイアスは、プロダクションレベルの自動音声認識(ASR)システムの重要な構成要素である。エンドツーエンドモデルは、典型的には、ビーム検索中に候補の小さいリストを維持し、バイアスフレーズの主要なソースである固有名詞においてうまく機能しないので、コンテキストバイアスは、エンドツーエンドモデルでは特に困難である。この議論は、エンドツーエンドモデルのための浅い融合(shallow-fusion)ベースのバイアスに対する様々なアルゴリズムおよび固有名詞の改善を提示する。様々なタスクにわたって、提案されている手法は、最先端の従来のモデルと同様のパフォーマンスを得る。 In general, context bias toward specific domains, such as a user's song names, app names, and contact names, is an important component of production-level automatic speech recognition (ASR) systems. Contextual bias is particularly difficult with end-to-end models because end-to-end models typically maintain a small list of candidates during beam search and do not perform well on proper names, which are the primary source of biased phrases. It is. This discussion presents various algorithms and improvements to shallow-fusion-based biases for end-to-end models. Across a variety of tasks, the proposed method obtains similar performance to state-of-the-art conventional models.
前述のように、エンドツーエンド音声認識モデルによるAM、PM、およびLMの単一のニューラルネットワークへの結合は、様々な理由のため、コンテキスト知識を組み込む能力を困難にする。第1に、エンドツーエンドモデルは、共同のAM、PM、およびLMを用いてトレーニングされるので、従来のLMと比較してはるかに少ないデータを用いてトレーニングされ、したがって、従来のモデルと比較して固有名詞においてより多くのエラーを生じさせる。コンテキストn-gramは、しばしば、固有名詞(「call Jason(Jasonに電話をかけて)」、「text Jane(Janeにテキストメッセージを送って)」)であるので、モデルがこれらのn-gramを予測することは困難である。第2に、効率的な復号のために、エンドツーエンドモデルは、ビーム検索の各ステップにおいて、少数の候補(約4～10)に剪定しなければならない。したがって、コンテキストn-gramaの場合のように、まれな単語およびフレーズは、ビームから離れてしまう可能性がある。 As mentioned above, the combination of AM, PM, and LM into a single neural network by an end-to-end speech recognition model makes the ability to incorporate contextual knowledge difficult for various reasons. First, because the end-to-end model is trained with joint AM, PM, and LM, it is trained with much less data compared to traditional LM, and thus compared to traditional models. and produce more errors in proper nouns. Contextual n-grams are often proper nouns (“call Jason,” “text Jane”), so the model It is difficult to predict. Second, for efficient decoding, the end-to-end model must be pruned to a small number of candidates (approximately 4-10) at each step of beam search. Therefore, rare words and phrases can fall off the beam, as in the case of context n-gramas.
独立したコンテキストn-gram LMをエンドツーエンド音声認識モデルのフレームワークに組み込む浅い融合の従来の技法は、バイアスが(エンドツーエンドモデルが予測する書記素/単語断片単位ではなく)単語の末尾において、ビームが各ステップにおいて剪定された後に発生するので、固有名詞がビームから離れることに依然として苦しんでいる。しばしば、オールニューラル最適化のテーマに沿って、エンドツーエンドモデル内でバイアスを実行することがより効率的である。しかしながら、オールニューラルバイアスに関する懸念の1つは、多数のn-gramまでスケールアップしたときに単語誤り率(WER)が悪化することである。別の懸念は、コンテキストバイアスは、適切でない場合であっても、常にアクティブである可能性があることであり、「アンチコンテキスト」と呼ばれる、バイアスされることが意図されない発話において、パフォーマンスが低下しないことが望ましい。 Traditional techniques of shallow fusion, which incorporate independent context n-gram LMs into the framework of end-to-end speech recognition models, have been found to be biased at the end of words (rather than on a grapheme/word fragment basis as predicted by end-to-end models). , occurs after the beam is pruned at each step, so we still suffer from proper names leaving the beam. Often, in keeping with the theme of all-neural optimization, it is more efficient to perform biasing within the end-to-end model. However, one concern with all-neural bias is that word error rate (WER) worsens when scaling up to a large number of n-grams. Another concern is that context bias may be active at all times, even when it is not appropriate, and performance does not degrade in utterances that are not intended to be biased, called "anti-context." This is desirable.
上記で論じられている実装形態は、単語レベルではなく部分語単位レベル(書記素、単語断片)においてバイアスを実行し、ビーム剪定後ではなくビーム剪定前にコンテキストFST160を適用し、アンチコンテキストに非常に役立つように接頭辞の共通のセット(たとえば、「call」、「text」)を有する接頭辞FST163を組み込み、固有名詞のモデル化を改善するためにテキストのみのデータのより大きいコーパスを活用することによって、コンテキストn-gramの早期の剪定に対処する。具体的には、固有名詞のモデル化は、(1)多数の固有名詞のテキストのみのクエリを作成し、対応する音声を合成することによって、合成トレーニングデータセット195を作成することと、(2)固有名詞を有するデータを維持するようにフィルタリングされた大量の教師なしオーディオ-テキストデータ(たとえば、教師なしトレーニングデータ193)を活用することと、(3)より多くの固有名詞を作成するために教師ありトレーニングデータ197のトランスクリプトをファジングすることとを含む。結果は、4つの異なるコンテキストテストセットにわたって報告される。FST構造への提案されている変更は、浅い融合ベースのバイアスにおける大幅な改善につながる。それに加えて、大量の教師なしデータを用いたトレーニングによるよりよい固有名詞のモデリングにより、パフォーマンスがさらに改善する。全体的に、エンドツーエンドバイアス解決策は、相対的に20～40%の曲を除くすべてのセットにわたって、従来の埋込みモデルよりも性能が優れている。
The implementation discussed above performs biasing at the subword unit level (graphemes, word fragments) rather than at the word level, applies context FST160 before beam pruning rather than after beam pruning, and applies the context FST160 to the anticontext Incorporating the prefix FST163 with a common set of prefixes (e.g., "call", "text") to help leverage a larger corpus of text-only data to improve modeling of proper nouns. This addresses early pruning of context n-grams. Specifically, proper noun modeling involves (1) creating a
従来のASRシステム(別個のAM、PM、およびLMを有する)は、バイアスフレーズのリストをn-gram有限状態トランスデューサ(FST)として表し、復号中にこのコンテキストFSTを用いてLMを構成することによってコンテキストバイアスを実行する。これは、認識結果をコンテキストFST内のn-gramに向けてバイアスするのに役立ち、特定のシナリオにおいてWERを低減する。同様の技法は、n-gramFSTを構築し、次いでn-gramFSTをエンドツーエンド復号フレームワークに組み込むために使用される。 Conventional ASR systems (with separate AM, PM, and LM) are configured by representing the list of bias phrases as an n-gram finite state transducer (FST) and constructing the LM using this context FST during decoding. Perform context bias. This helps bias the recognition results towards n-grams in the context FST, reducing the WER in certain scenarios. Similar techniques are used to construct n-gramFST and then incorporate n-gramFST into an end-to-end decoding framework.
音響観測のセットx=(x1,...,xK)が与えられると、エンドツーエンドモデルは、これらの観測が与えられた部分語単位のセットy=(y1,...,yL)に対する事後確率、すなわち、P(y|x)を提供する。浅い融合は、式(1)によって与えられるように、ビーム検索復号中にエンドツーエンドモデルからのスコアを外部コンテキストLMで補間する。 Given a set of acoustic observations x=(x 1 ,...,x K ), the end-to-end model is a set of subword units y=(y 1 ,..., We provide the posterior probability for y L ), ie, P(y|x). Shallow fusion interpolates the scores from the end-to-end model with the external context LM during beam search decoding, as given by Equation (1).
ここで、PC(y)は、コンテキストLMからのスコアであり、λは、ビーム検索中にコンテキストLMが全体的なモデルスコアにどれくらい影響を与えるかを制御する調整可能なハイパーパラメータである。 where P C (y) is the score from the context LM and λ is a tunable hyperparameter that controls how much the context LM influences the overall model score during beam search.
エンドツーエンドモデルのためのコンテキストLMを構築するために、単語レベルのバイアスフレーズのセットが事前に知られており、n-gram重み付き有限状態トランスデューサ(WFST)にコンパイルされる。この単語レベルのWFST、Gは、次いで、書記素/単語断片のシーケンスを対応する単語に変換する「スペラー」FST、Sで左合成される。部分語FSTは、コンテキストLM、min(det(S○G))として取得される。 To construct the context LM for the end-to-end model, a set of word-level bias phrases is known in advance and compiled into an n-gram weighted finite state transducer (WFST). This word-level WFST,G is then left-synthesized with a "speller" FST,S, which converts sequences of graphemes/word fragments into corresponding words. The partial word FST is obtained as the context LM, min(det(S○G)).
浅い融合によるエンドツーエンドモデルのバイアスは、コンテキストLMが従来のシステムによるバイアスと同様に単語境界においてのみ適用される方法を使用して最初に調査された。この手法は、コンテキストフレーズのリストが多くの固有名詞(たとえば、曲名または連絡先)を含む場合、効果的ではないことがわかった。エンドツーエンドモデルは、ビーム検索中に部分語単位ラベルy(書記素、単語断片)を予測するので、バイアスされるべき単語がビーム内に存在しない場合、単語境界においてコンテキストLMを適用することは、機能しない。この問題に対処するために、部分語FSTの重みは、各部分語単位に押し付けられる。部分語FSTを決定論的にするために、同じ重みは、すべての部分語単位について使用される。図3に示されているように、接頭辞に一致するがフレーズ全体には一致しない候補に人為的に重みを与えることを回避するために、失敗アークが含まれる。失敗アークが重みを担持しないn-gramFSTとは異なり、部分語FSTは、現在の状態に到達する前に追加された重みを無効にする失敗アークを含む。部分語単位のバイアスは、バイアス候補がビームから早期に剪定される機会を減らすので、各単語の末尾におけるバイアスよりも効果的であることがわかった。しかしながら、このアイデアは、書記素部分語単位を使用してのみ検討された。それに加えて、バイアスされることが意図されていないフレーズが過度にバイアスされないことを確実にするために、「アンチコンテキスト」を用いた結果は、調査されなかった。これらの懸念に対処するための浅い融合エンドツーエンドバイアスへの追加の改善について、以下で説明されている。 Biases in end-to-end models due to shallow fusion were first investigated using a method in which context LM is applied only at word boundaries, similar to biases due to traditional systems. We found that this technique is not effective when the list of context phrases contains many proper nouns (e.g., song names or contact information). Since the end-to-end model predicts subword unit labels y (graphemes, word fragments) during beam search, applying context LM at word boundaries is not possible if the word to be biased is not present in the beam. , doesn't work. To address this problem, the weight of the subword FST is imposed on each subword unit. To make the subword FST deterministic, the same weights are used for all subword units. As shown in Figure 3, failure arcs are included to avoid giving artificial weight to candidates that match the prefix but not the entire phrase. Unlike n-gram FST, where failure arcs do not carry weights, subword FST includes failure arcs that override added weights before reaching the current state. Biasing on a subword basis was found to be more effective than biasing at the end of each word because it reduces the chance that bias candidates are prematurely pruned from the beam. However, this idea has only been explored using grapheme subword units. In addition, the results of using "anti-context" were not investigated to ensure that phrases that were not intended to be biased were not unduly biased. Additional improvements to shallow fusion end-to-end bias to address these concerns are described below.
バイアスフレーズは、たとえば、バイアスフレーズの前に一般的に使用される接頭辞のセットがある場合にのみ、選択的にアクティブ化することができる。たとえば、連絡先の要求は、典型的には、「call」、「text」、または「message」を有するが、曲の要求は、しばしば、「play」という接頭辞を使用する。この技法は、従来のモデルによって使用され、2つの主な利点を有する。第1に、曲、連絡先、およびアプリのような要求ごとにバイアスがアクティブである場合、いかなるバイアスフレーズも含まない発話について認識品質が低下する、アンチコンテキストとして知られる問題がある。接頭辞を見た後のみにバイアスを制御することは、そのような過剰バイアスを回避するのを助ける。第2の利点は、接頭辞を見た後、特定のフレーズに向けてバイアスすることについての信頼度がより高くなることであり、これは、バイアス重みを増加させ、全体的なバイアスパフォーマンスを改善することを可能にする。 A bias phrase can be selectively activated, for example, only if there is a set of commonly used prefixes before the bias phrase. For example, contact requests typically have "call," "text," or "message," while song requests often use the prefix "play." This technique is used by traditional models and has two main advantages. First, when bias is active on a request-by-request basis, such as songs, contacts, and apps, there is a problem known as anti-context, where recognition quality degrades for utterances that do not contain any bias phrases. Controlling the bias only after looking at the prefix helps avoid such excessive bias. The second benefit is that you have more confidence in biasing towards a particular phrase after seeing the prefix, which increases the bias weight and improves the overall bias performance. make it possible to
一例において、コンテキストFST160は、対応するバイアスフレーズに先行するしきい値回数(たとえば、50回)よりも多く出現するすべての接頭辞が、対応するコンテキストFST160と各々連結される対応する接頭辞FST163において使用するために抽出されるように、接頭辞のリストで構成され得る。接頭辞をスキップするために空の接頭辞オプションが許可されるが、これは、アクティブ化接頭辞の意図された抑制効果をキャンセルする。したがって、より小さいバイアス重みλが、空の接頭辞のために使用され得る。
In one example, a context FST 160 includes a
Table 1(表1)は、提案されているアルゴリズムの改善を示し、ここで、実験E0およびE1は、それぞれバイアスなしの書記素ベースラインおよびWPMベースラインを示し、実験E2は、提案されている改善なしの書記素バイアス結果を示し、実験E3は、曲、連絡先-実際、連絡先 TTS、およびアプリに関連するコンテキストセットについて、モデルが悪い候補をビーム上に維持することを防ぐために減算コストを使用する。 Table 1 shows the improvement of the proposed algorithm, where experiments E0 and E1 represent the unbiased grapheme baseline and WPM baseline, respectively, and experiment E2 shows the proposed algorithm Showing grapheme bias results without improvement, experiment E3 shows that for song, contact-actual, contact TTS, and app-related context sets, subtraction costs are applied to prevent the model from keeping bad candidates on the beam. use.
バイアスがより長い単位において発生するように書記素バイアスからWPMバイアスへ切り替えること(E4)は、より多くの関連する候補をビーム上に維持するのを助け、パフォーマンスも改善する。最後に、以下では早期バイアスとして示されている、剪定の前にバイアスFSTを適用することは、よい候補が早期にビーム上に残ることを保証するのを助け、追加の改善につながる(E5)。全体的に、最良の浅い融合の設定は、減算コストと早期バイアスとを用いてWPMレベルにおいてバイアスすることである。 Switching from grapheme bias to WPM bias (E4) so that the bias occurs in longer units helps keep more relevant candidates on the beam and also improves performance. Finally, applying a bias FST before pruning, denoted below as early bias, helps ensure that good candidates remain on the beam early, leading to additional improvements (E5) . Overall, the best shallow fusion setting is to bias at the WPM level with a subtractive cost and an early bias.
動的クラスバイアスは、常にオンであるので、バイアスフレーズが発話内に存在しない場合、パフォーマンスを低下させないことが重要である。Table 2(表2)は、アンチコンテキストの結果を示す。E1は、ベースラインの無バイアスWPMである。このモデルをバイアスすること(E5)は、VSおよびIMEにおけるパフォーマンスにおける大幅な低下を与える。セクション2.3.3において論じられているように、従来のモデルは、接頭辞をバイアスFST内に含めることによってこの問題に対処する。接頭辞を無視するために重みなしで接頭辞が使用され、コンテキストがバイアスされる場合(E6)、VSおよびIMEにおける結果は、改善するが、バイアスセットにおいて品質が低下する。しかしながら、E7において、空の接頭辞を有するパスを含めることは、過剰バイアスを引き起こし、VSおよびIMEにおける劣化をもたらす。サーバと同様に、解決策は、空の接頭辞が前にある場合、コンテキストフレーズに対してより小さい重みを使用することである(E7)。この手法では、VSおよびIMEにおいて劣化がほとんど観察されず、バイアステストセットにおいてパフォーマンスも改善する(E8)。 Since dynamic class bias is always on, it is important not to degrade performance if the bias phrase is not present in the utterance. Table 2 shows the anti-context results. E1 is the baseline unbiased WPM. Biasing this model (E5) gives a significant drop in performance in VS and IME. As discussed in Section 2.3.3, traditional models address this problem by including the prefix within the bias FST. When the prefix is used without weights to ignore the prefix and the context is biased (E6), the results in VS and IME improve, but the quality decreases in the biased set. However, in E7, including paths with empty prefixes causes overbiasing and results in degradation in VS and IME. Similar to the server, the solution is to use a smaller weight for context phrases when preceded by an empty prefix (E7). With this method, little degradation is observed in VS and IME, and it also improves performance on the bias test set (E8).
さらに、Table 3(表3)は、固有名詞の知識を改善することによってバイアス数を改善することができることを示す。ここでのベースラインは、E8であり、3500万の教師ありVS発話においてトレーニングされたRNN-T WPMである。実験E9は、教師なしデータを用いてトレーニングした場合のすべてのバイアステストセットにわたる改善を示す。TTSデータ(E10)を用いたトレーニングは、教師なしデータ(E9)と比較してTTSテストセットにおいてより大きい改善を与えるが、実際のテストセット(Contacts-Real)においてより大きい劣化をもたらす。これは、TTSバイアスセットにおける改善が、固有名詞のより豊富な語彙を学習するのではなく、主にトレーニングとテストデータとの間の一致したオーディオ条件から来ていることを示す。最後に、トランスクリプトをファジングすること(E11)は、すべてのセットにおいて品質の劣化を示す。今後の分析は、E9(教師なしデータ)に基づく。 Furthermore, Table 3 shows that the bias number can be improved by improving the knowledge of proper nouns. The baseline here is E8, an RNN-T WPM trained on 35 million supervised VS utterances. Experiment E9 shows the improvement across all bias test sets when training with unsupervised data. Training with TTS data (E10) gives a larger improvement in the TTS test set compared to the unsupervised data (E9), but results in a larger degradation in the real test set (Contacts-Real). This indicates that the improvement in the TTS bias set primarily comes from the matched audio conditions between training and test data, rather than learning a richer vocabulary of proper nouns. Finally, fuzzing the transcripts (E11) shows quality degradation in all sets. Further analysis will be based on E9 (unsupervised data).
Table 4(表4)は、バイアスフレーズのエラー率(ERR)と、これらのフレーズにおけるOOVの総数が与えられた場合のバイアスフレーズにおけるOOVにおいて発生したエラーの割合とを示す。(OOV)。第1に、この表は、すべてのテストセットにわたって、教師なしデータがバイアスとは無関係にERRメトリックを大幅に改善することを示す。それに加えて、教師なしデータモデルは、トレーニングにおいてより多くの単語を見ているので、バイアスありのERRメトリックは、より優れている。第2に、バイアスなしでは、OOVメトリックは、95%を超えており、単語がOOVである場合、ほぼ確実に検出されないことを示す。しかしながら、教師なしデータを用いてトレーニングする場合、バイアスは、教師ありデータにおいてトレーニングするよりもOOVにおけるエラーを大幅に改善する。両方のメトリックは、教師なしデータが、モデルがより多くの固有名詞を見るのを助け、正しい単語をビームにもたらすより高い信頼性をモデルに与えることを示している。 Table 4 shows the error rate (ERR) of bias phrases and the percentage of errors that occur in OOVs in bias phrases given the total number of OOVs in these phrases. (OOV). First, the table shows that across all test sets, unsupervised data significantly improves the ERR metric, independent of bias. In addition to that, the unsupervised data model sees more words in training, so the biased ERR metric is better. Second, without bias, the OOV metric is above 95%, indicating that if a word is OOV, it will almost certainly not be detected. However, when training with unsupervised data, the bias improves the error in OOV significantly more than training on supervised data. Both metrics show that the unsupervised data helps the model see more proper names, giving it more confidence in bringing the correct word to the beam.
Table 5(表5)は、RNN-Tのバイアスパフォーマンスを、別個のPMおよびLMとともにコンテキストに依存しない音素ターゲットを用いてトレーニングされたCTC AMからなる同等のサイズ(130MB)の従来モデルと比較する。RNN-Tモデルは、おそらくは、他のカテゴリと比較してTable 1(表1)におけるより高いOOV率により、曲を除くすべてのカテゴリにおいて、埋込み型の従来のモデルよりも20%～40%優れている。 Table 5 compares the bias performance of RNN-T with a conventional model of comparable size (130MB) consisting of a CTC AM trained with context-independent phoneme targets along with separate PMs and LMs. . The RNN-T model outperforms the embedded traditional model by 20% to 40% in all categories except songs, probably due to the higher OOV rate in Table 1 compared to other categories. ing.
単一のオールニューラルエンドツーエンドシステムは、別個の音響モデル(AM)、発音モデル(PM)、および言語モデル(LM)を有する従来のモデルと比較して、はるかに単純でコンパクトな解決策を提供するので、エンドツーエンド(end-to-end)モデルは、音声認識における有望な研究動向である。しかしながら、エンドツーエンド最適化は、結合オーディオ-テキストペアを必要とし、従来のモデルにおけるLMを訓練するために使用される追加のレキシコンおよび大量のテキストのみのデータを利用しないので、ときには、テールワード(たとえば、単語使用分布の「ロングテール」における珍しい単語)および固有名詞において不十分に機能することが指摘されている。テキストのみのデータにおいてリカレントニューラルネットワーク言語モデル(RNN-LM)をトレーニングし、それをエンドツーエンドモデルに融合させることにおいて、多数の努力があった。この議論において、エンドツーエンドモデルをトレーニングすることに対するこの手法は、教師なし音声データから生成されたオーディオ-テキストペアと対比される。固有名詞の問題を具体的にターゲットにするために、品詞(POS: Part-of-Speech)タガーが、固有名詞を有する教師なしデータのみを使用するために教師なしデータをフィルタリングする。フィルタリングされた教師なしデータを用いたトレーニングは、単語誤り率(WER)において最大13%の相対的な低下を提供し、コールドフュージョン(cold-fusion)RNN-LMと組み合わせて使用されると、最大17%の相対的改善を提供する。 A single all-neural end-to-end system provides a much simpler and more compact solution compared to traditional models with separate acoustic models (AM), pronunciation models (PM), and language models (LM). End-to-end models are a promising research trend in speech recognition. However, since end-to-end optimization requires joint audio-text pairs and does not utilize the additional lexicon and large amount of text-only data used to train the LM in traditional models, sometimes tail words It has been noted that it performs poorly on (e.g. rare words in the "long tail" of the word usage distribution) and proper nouns. There have been numerous efforts in training recurrent neural network language models (RNN-LM) on text-only data and fusing it into end-to-end models. In this discussion, this approach to training end-to-end models is contrasted with audio-text pairs generated from unsupervised speech data. To specifically target the problem of proper nouns, a Part-of-Speech (POS) tagger filters the unsupervised data to use only unsupervised data with proper nouns. Training with filtered unsupervised data provides up to 13% relative reduction in word error rate (WER), and when used in combination with cold-fusion RNN-LM, up to Provides a relative improvement of 17%.
エンドツーエンドモデルは、自動音声認識(ASR)のための単純だが効果的な方法を提供する。従来、ASRシステムは、AM、PM、およびLMから構成されるが、エンドツーエンドモデルは、これらの3つの構成要素を、共同で最適化された単一のニューラルネットワークに畳み込む。Listen, Attend, and Spell(LAS)は、強力な従来のASRシステムと比較して有望な結果を示したそのようなエンドツーエンドモデルの1つである。しかしながら、従来のシステムにおけるLMは、大量のテキストのみのデータにおいて独立してトレーニングすることができるが、LASモデルをトレーニングすることは、収集するためにはるかに費用がかかり、規模がはるかに小さいオーディオ-テキストペアを必要とする。したがって、LASは、曲名、連絡先などのまれな単語またはフレーズを認識する際に、従来のモデルと比較して不十分に機能する。ペアでないテキストデータを使用してエンドツーエンドモデルのパフォーマンスを改善するために、多くの努力がなされてきた。人気のある研究動向の1つは、テキストのみのデータにおいてトレーニングされた外部LMをエンドツーエンドモデルと統合することに向いている。たとえば、エンドツーエンドモデルは、テキストのみのデータから事前にトレーニングされたLMを用いて初期化し、次いで、マルチタスクトレーニングを通じてLMと共同で最適化することができる。それに加えて、独立してトレーニングされたエンドツーエンドモデルとLMモデルとを、浅い融合を介して補間することは、ニューラル機械翻訳とASRの両方について検討されている。たとえば、テキストのみのデータにおいてトレーニングされたRNN-LMを、コールドフュージョンと深い融合の両方を介して、エンドツーエンドデコーダに共同で統合することが検討されている。全体として、テキストのみのデータを活用することは、ASRのWERにおいて3%から7%の相対的な改善を示している。単一言語のトレーニングデータを用いて機械翻訳を改善するために、逆翻訳が検討されている。これは、BLEUスコアを2.8～3.7改善した。このアイデアは、音声認識にも適用されており、エンドツーエンドモデルをトレーニングするためのオーディオ-テキストペアを拡張するために、ペアでないテキストデータから生成された合成オーディオが使用された。TTSデータの使用は、TTSテストセットにおける劇的な改善を与えるが、実際のテストセットにおいて劣化が観察されている。 The end-to-end model provides a simple but effective method for automatic speech recognition (ASR). Traditionally, ASR systems consist of AM, PM, and LM, but end-to-end models convolve these three components into a single jointly optimized neural network. Listen, Attend, and Spell (LAS) is one such end-to-end model that has shown promising results compared to powerful traditional ASR systems. However, while the LM in traditional systems can be trained independently on large amounts of text-only data, training a LAS model is much more expensive to collect and much smaller in size. - Requires text pairs. Therefore, LAS performs poorly compared to traditional models in recognizing rare words or phrases such as song titles, contacts, etc. Many efforts have been made to improve the performance of end-to-end models using unpaired text data. One popular research trend is towards integrating external LMs trained on text-only data with end-to-end models. For example, an end-to-end model can be initialized with an LM pre-trained from text-only data and then jointly optimized with the LM through multi-task training. In addition, interpolating independently trained end-to-end models and LM models via shallow fusion has been considered for both neural machine translation and ASR. For example, jointly integrating RNN-LMs trained on text-only data into end-to-end decoders via both cold fusion and deep fusion has been considered. Overall, leveraging text-only data shows a relative improvement of 3% to 7% in WER for ASR. Back translation is being considered to improve machine translation using monolingual training data. This improved the BLEU score by 2.8-3.7. This idea has also been applied to speech recognition, where synthetic audio generated from unpaired text data was used to augment audio-text pairs for training end-to-end models. Although the use of TTS data gives dramatic improvements in TTS test sets, degradation is observed in real test sets.
それに加えて、従来のASRシステムは、パフォーマンスを改善するために、ラベルのないオーディオデータを利用する。既存のASRシステムからの信頼度スコアは、より多くのデータを用いてトレーニングするための教師なしデータを選択するために一般的に使用される。たとえば、認識単語の信頼度スコアと、低リソース言語のためのMLPポステリオグラム(posteriorgram)ベースの音素出現信頼度との組合せを使用して、教師なし音声データを選択することができる。ビデオ音声キャプションタスクについて、WERパフォーマンスを改善するためにトレーニングデータの量を大幅に増加させるために、「信頼の島」手法が開発された。 In addition, traditional ASR systems utilize unlabeled audio data to improve performance. Confidence scores from existing ASR systems are commonly used to select unsupervised data for training with more data. For example, a combination of recognition word confidence scores and MLP posteriorgram-based phoneme occurrence confidence for low resource languages can be used to select unsupervised speech data. For the video-audio captioning task, an "island of trust" method was developed to significantly increase the amount of training data to improve WER performance.
LASなどのオールニューラルエンドツーエンドシステムをトレーニングすることは、AM、PM、およびLMを共同で学習するためにオーディオ-テキストペアを必要とする。この共同トレーニングは、潜在的によりよい最適化を可能にするが、ペアのオーディオ-テキストデータの使用に制限もされるので、エンドツーエンドモデルがまれな単語および固有名詞において不十分に機能するという結果をもたらす。この作業では、技法は、これらのテールワードにおいてエンドツーエンドモデルのパフォーマンスを改善するために、転写されていない音声データを利用する。 Training an all-neural end-to-end system such as LAS requires audio-text pairs to jointly learn AM, PM, and LM. Although this joint training potentially allows for better optimization, it is also limited to using paired audio-text data, which means that the end-to-end model performs poorly on rare words and proper nouns. bring results. In this work, the technique utilizes untranscribed speech data to improve the performance of the end-to-end model on these tail words.
教師なしオーディオ-テキストペアにおいて起こり得る不一致に対処するための別の方法は、並列WaveNetボコーダを有する単一話者TTSエンジンを使用して、テキスト仮説から合成オーディオを生成することである。これは、機械翻訳において使用される「逆翻訳」手法と類似している。この手法に関する潜在的な問題の1つは、実際の音声と合成オーディオとの間の音響的な違い、特に、限られた話者の特性とクリーンな話し方である。 Another way to address possible mismatches in unsupervised audio-text pairs is to use a single-speaker TTS engine with a parallel WaveNet vocoder to generate synthetic audio from text hypotheses. This is similar to the "back translation" technique used in machine translation. One potential problem with this approach is the acoustic differences between real speech and synthesized audio, particularly limited speaker characteristics and clean speaking styles.
この懸念に対処するために、LASモデルのエンコーダおよびデコーダをバックプロッピング(backpropping)することが、デコーダだけをバックプロッピングすることと比較される。エンコーダは、AMを表し、現実的な条件においてトレーニングされるべきである。しかしながら、デコーダは、LMに類似しており、あまり現実的でない条件を用いてトレーニングすることができる。したがって、デコーダのみをバックプロッピングすることは、おそらくTTSデータに関する非現実的なオーディオの懸念に対処することができる。 To address this concern, backpropping the encoder and decoder of the LAS model is compared to backpropping the decoder alone. The encoder represents AM and should be trained in realistic conditions. However, the decoder is similar to LM and can be trained using less realistic conditions. Therefore, backpropping the decoder only could probably address unrealistic audio concerns regarding TTS data.
10億を超える教師なし発話が利用可能である。これには、教師なしデータが多ければ、モデルがトレーニング中にはるかにより多くの語彙を見るという利点がある。しかしながら、データが多くなると、モデルのトレーニング時間がより長くなる。 Over 1 billion unsupervised utterances are available. This has the advantage that with more unsupervised data, the model sees a much larger vocabulary during training. However, the more data, the longer the model training time.
したがって、LASモデルをトレーニングするために、データのセブセットが選択される。具体的には、モデルは、固有名詞においてうまく機能しないので、これらの発話を含むように教師なしデータをフィルタリングすることは、すべてのデータを使用することに比べてより短いトレーニング時間で、教師なしデータに関する品質改善を生じ得る。発話が固有名詞を含むかどうかの判断は、テキスト仮説において品詞(POS)タガーを実行することによって行われる。実験は、人間が転写した教師ありトレーニングセットと、ラベルなしの教師なしトレーニングセットとにおいて行われる。3500万の英語の発話(約27500時間)で構成される教師ありトレーニングセット。これらの発話は、匿名化され、手書きで転写され、インターネットの音声検索および口述トラフィックを表す。これらの発話は、全体的なSNRが0dBと30dBとの間になり、平均SNRが12dBになるように、ルームシミュレータを使用し、様々な程度のノイズと残響とを追加して、さらに人為的に破壊される。ノイズ源は、インターネットビデオおよび日常生活の騒々しい環境録音からである。発話ごとに、25の異なるノイズバージョンがトレーニングのために生成される。 Therefore, a subset of data is selected to train the LAS model. Specifically, the model does not perform well on proper nouns, so filtering the unsupervised data to include these utterances requires less training time than using all the data, and Quality improvements regarding the data may occur. Determining whether an utterance contains a proper noun is done by running a part-of-speech (POS) tagger on the text hypothesis. Experiments are performed on a human-transcribed supervised training set and an unlabeled unsupervised training set. Supervised training set consisting of 35 million English utterances (approximately 27,500 hours). These utterances are anonymized, hand-transcribed, and represent Internet voice search and dictation traffic. These utterances were further artificialized using a room simulator and adding varying degrees of noise and reverberation such that the overall SNR was between 0 and 30 dB, with an average SNR of 12 dB. destroyed. Noise sources are from internet videos and noisy environmental recordings of daily life. For each utterance, 25 different noise versions are generated for training.
実験は、25マイクロ秒のウィンドウを用いて計算され、10ミリ秒ごとにシフトされた、80次元の対数メル特徴を使用する。各々の現在のフレームにおいて、これらの特徴は、左に3つの連続するフレームでスタックされ、次いで、30ミリ秒のフレームレートにダウンサンプリングされる。実験は、LASモデルを用いて行われる。具体的には、エンコーダネットワークは、10の単方向長・短期記憶(LSTM)層で構成され、各層は、2048の隠れユニットを有し、その後に384次元の投影層が続く。エンコーダネットワークの第2の層の後、各フレームは、隣接する左の隣接フレームと連結され、2つずつストライドされてから次の層に渡される。このスタッキング層は、フレームレートを60ミリ秒にさらに低下させる。トレーニングを安定させるために、エンコーダ層に対して層の正規化が採用される。4つのアテンションヘッドを有する追加のアテンションが使用される。デコーダネットワークは、2048の隠れ層と384の出力投影サイズとを有する4つの単方向LSTM層で構成される。LASモデルの出力は、16000の単語断片の語彙である。 The experiment uses an 80-dimensional log-mel feature computed with a 25 microsecond window and shifted every 10 milliseconds. In each current frame, these features are stacked on the left for three consecutive frames and then downsampled to a frame rate of 30 ms. Experiments are performed using the LAS model. Specifically, the encoder network is composed of 10 unidirectional long short-term memory (LSTM) layers, each layer having 2048 hidden units followed by a 384-dimensional projection layer. After the second layer of the encoder network, each frame is concatenated with its adjacent left neighbor frame and strided by two before being passed to the next layer. This stacking layer further reduces the frame rate to 60ms. To stabilize the training, layer normalization is employed for the encoder layer. Additional attention with four attention heads is used. The decoder network consists of four unidirectional LSTM layers with 2048 hidden layers and an output projection size of 384. The output of the LAS model is a vocabulary of 16000 word fragments.
モデルは、TensorFlowを使用してラベル平滑化とクロスエントロピー損失とを用いてトレーニングされる。実験は、4096のグローバルバッチサイズを有する8×8のテンソル処理ユニット(TPU)スライスを使用し、約200000ステップについてモデルをトレーニングする。 The model is trained using TensorFlow with label smoothing and cross-entropy loss. The experiments use 8x8 tensor processing unit (TPU) slices with a global batch size of 4096 and train the model for approximately 200000 steps.
教師ありトレーニングデータ(B0として示されている)のみを用いてトレーニングされたLASモデルのパフォーマンスは、Table 6(表6)において提示されている。弱蒸留(weak distillation)のための教師モデルとして使用されるフルスタックの従来のモデルのパフォーマンスも提示されている。教師モデルは、従来のコンテキスト依存の音素ベースの低フレームレート音響モデル、400万語の発音レキシコン、および5-gramの言語モデルである。このモデルは、B1と呼ばれる。教師モデルは、同じ教師ありトレーニングデータを使用してトレーニングされる。この表は、ほとんどのテストセットにおいてLASモデルが従来のモデルよりも優れていることを示している。しかしながら、従来のモデルは、実際には、検索空間を剪定するためにコンテキスト情報を使用し、これは、多くの固有名詞(曲、連絡先、アプリ)を有するセットにおいてWERを減らすのを助ける。コンテキストバイアスを用いる教師モデルのパフォーマンスは、Table 6(表6)においてB2として示されている。 The performance of the LAS model trained using only supervised training data (denoted as B0) is presented in Table 6. The performance of a full-stack conventional model used as a teacher model for weak distillation is also presented. The teacher models are a traditional context-sensitive phoneme-based low frame rate acoustic model, a 4 million word pronunciation lexicon, and a 5-gram language model. This model is called B1. The supervised model is trained using the same supervised training data. This table shows that the LAS model outperforms the traditional model on most test sets. However, traditional models actually use context information to prune the search space, which helps reduce WER in sets with many proper names (songs, contacts, apps). The performance of the teacher model using context bias is shown as B2 in Table 6.
認識された仮説において符号化された知識を抽出するために、B0は、10億の教師なしデータをトレーニングする。B2によって生成された仮説は、それらのトランスクリプトにおけるエラーに関係なく、参照トランスクリプトとして使用される。450000のステップについて10億の教師なしデータにおいてトレーニングすると(E0)、TTSセットのすべてにおいて良好な改善が得られるが、Voice Search(音声検索)およびContacts-Real(連絡先-実際)について劣化が見られる。TTSセットにおける勝利は、主に、データによってもたらされたより多くの単語のバリエーションによるものであるが、損失は、復号された仮説におけるエラーによるものである可能性が最も高い。Voice SearchおよびContacts-Realにおける劣化を減らすために、E0は、150000ステップについての教師ありデータを用いてさらに微調整される(E1)。それは、すべてのテストセットにおいてB0よりも改善する。 To extract the knowledge encoded in the recognized hypotheses, B0 trains on 1 billion unsupervised data. Hypotheses generated by B2 are used as reference transcripts, regardless of errors in those transcripts. Training on 1 billion unsupervised data for 450,000 steps (E0) yields good improvements in all of the TTS sets, but shows degradation for Voice Search and Contacts-Real. It will be done. The wins in the TTS set are mainly due to more word variations introduced by the data, while the losses are most likely due to errors in the decoded hypotheses. To reduce degradation in Voice Search and Contacts-Real, E0 is further fine-tuned (E1) using supervised data for 150000 steps. It improves over B0 on all test sets.
10億のデータを用いたトレーニングは、長い時間がかかる。この量のデータが必要かどうかを理解するために、教師なしデータは、それぞれ、5億と1億とにランダムにダウンサンプリングされる。モデルは、最初に教師なしデータのみ(E2およびE4)においてトレーニングし、次いで、教師ありデータ(E3およびE5)において微調整する。1億と5億の両方の教師ありデータでゲインが達成されるが、1億のデータを使用するほうが、わずかによいパフォーマンスを提供する。 Training with 1 billion pieces of data takes a long time. To understand whether this amount of data is needed, the unsupervised data is randomly downsampled to 500 million and 100 million, respectively. The model is first trained on unsupervised data only (E2 and E4) and then fine-tuned on supervised data (E3 and E5). Gains are achieved with both 100 million and 500 million supervised data, but using 100 million data provides slightly better performance.
図7における実験は、教師なしデータを用いてLASモデルをトレーニングした後、モデルは、再び教師ありデータを用いて微調整される必要があることを示した。トレーニング手順を簡略化するために、教師ありデータおよび教師なしデータは、トレーニング中に混合される。具体的には、トレーニング用の発話のバッチを作成するときはいつでも、2つのトレーニングセットから固定比率でランダムな選択が生じる。たとえば、8:2の混合比では、トレーニングバッチは、80%の割合で教師ありデータに由来し、20%の割合で教師なしデータに由来する。 The experiments in Figure 7 showed that after training the LAS model using unsupervised data, the model needs to be fine-tuned again using supervised data. To simplify the training procedure, supervised and unsupervised data are mixed during training. Specifically, whenever we create a batch of utterances for training, a fixed proportion of random selections occur from the two training sets. For example, with a mixing ratio of 8:2, the training batch will come from supervised data 80% of the time and unsupervised data 20% of the time.
Table 8(表8)における結果から、教師ありデータと教師なしデータとを混合することは、教師なしデータを利用する効果的な方法である。3つの異なる比率の中で、8:2は、わずかな差で、全体的に最高のパフォーマンスを与える。E8をE1と比較すると、Voice Searchにおけるゲインは、E1と比べて小さいが、より多くの固有名詞(アプリ、曲、連絡先)を有するテストセットにおいて、はるかに低いWERが達成される。 From the results in Table 8, mixing supervised and unsupervised data is an effective way to utilize unsupervised data. Among the three different ratios, 8:2 gives the best performance overall, by a small margin. Comparing E8 to E1, the gain in Voice Search is smaller compared to E1, but much lower WER is achieved in the test set with more proper nouns (apps, songs, contacts).
このセクションでは、教師なしデータを組み込むための別の手法が比較される。すべての実験は、実験の素早い方向転換のために、教師なしデータのランダムにサンプリングされた1億のサブセットを使用する。E9は、E8とまったく同じ方法であるが、より少ない教師なしデータを用いてトレーニングされる。 In this section, different techniques for incorporating unsupervised data are compared. All experiments use 100 million randomly sampled subsets of unsupervised data for quick turnaround of experiments. E9 is trained in exactly the same way as E8, but with less unsupervised data.
Table 9(表9)における結果は、教師なしデータが少ないほど、一般的なVoice Searchテストセットにおいてわずかによいパフォーマンスを生じるが、より多いテールワードを有するテストセットにおいて、より高いWERを生じることを示している。次に、教師なしトランスクリプトからオーディオが合成され、ここで、まれな単語テストセットを作成するために前述のTTSシステムが使用される。E9において使用された教師なしデータは、このTTSトレーニングセットに置き換えられ、結果は、Table 9(表9)においてE10として提示されている。すべてのTTSテストセットについて大きいWERの低下を達成したが、Contacts-Realにおいてパフォーマンスを低下させた。TTSセットにおけるこの大幅な減少は、主に、追加された教師なしデータとテストセットとの間の一致した音響に由来する。実際のデータと合成データとの間で可能性のある一致しないオーディオ条件を回避するために、エンコーダネットワークパラメータの更新は、無効にされ、LASモデルのデコーダネットワークのみがトレーニング中に更新される。結果(E11)は、E10と同様で、アプリにおいてわずかに劣化している。TTSセットにおける大きいエラー削減にもかかわらず、E9と比較してより現実的なテストセットにおける劣化が、本当のところを物語っている。したがって、E9は、E10およびE11よりも優先される。 The results in Table 9 show that less unsupervised data yields slightly better performance on the general Voice Search test set, but higher WER on test sets with more tail words. It shows. Audio is then synthesized from the unsupervised transcripts, where the previously described TTS system is used to create a rare word test set. The unsupervised data used in E9 was replaced with this TTS training set and the results are presented as E10 in Table 9. We achieved large WER reductions for all TTS test sets, but degraded performance in Contacts-Real. This significant reduction in the TTS set primarily comes from the matched acoustics between the added unsupervised data and the test set. To avoid possible mismatched audio conditions between real data and synthetic data, encoder network parameter updates are disabled and only the decoder network of the LAS model is updated during training. The results (E11) are similar to E10, with a slight deterioration in applications. Despite the large error reduction on the TTS set, the degradation on the more realistic test set compared to E9 is telling. Therefore, E9 has priority over E10 and E11.
教師なしデータを利用する別の方法は、LMをLASシステムに統合することである。具体的には、RNN-LMは、教師ありデータトランスクリプトおよび1億の教師なしデータトランスクリプトにおいてトレーニングされ、次いで、コールドフュージョンを使用してLASモデルのトレーニングに統合される。結果(E12)は、教師ありベースライン(B0)に対して2%～6%の相対的なWER低減を示すが、ゲインは、E9と比較してはるかに小さい。 Another way to utilize unsupervised data is to integrate LM into LAS systems. Specifically, the RNN-LM is trained on supervised and 100 million unsupervised data transcripts and then integrated into the training of the LAS model using cold fusion. The results (E12) show a relative WER reduction of 2% to 6% over the supervised baseline (B0), but the gain is much smaller compared to E9.
このセクションは、教師なしデータをよりよく利用する方法を探索する。第1に、1億の教師なし発話のランダムな選択(E9)の代わりに、教師なしデータは、固有名詞を有するものだけを使用するようにフィルタリングされ(Table 10(表10)におけるE13)、これは、LASモデルが十分に機能しない発話の選択を可能にする。この選択は、固有名詞タガーを用いて行われる。固有名詞に焦点を当てた1億の教師なしデータは、トレーニングのために同じ8:2の比率で教師ありトレーニングデータと混合される。同じ量のデータを用いて、固有名詞をフィルタリングした音声を用いたトレーニングは、ランダム選択を使用した4%～12%の相対的な低下と比較して、6%～13%の相対的なWERの低下を与える。 This section explores ways to better utilize unsupervised data. First, instead of a random selection of 100 million unsupervised utterances (E9), the unsupervised data is filtered to use only those with proper nouns (E13 in Table 10); This allows selection of utterances for which the LAS model does not perform well. This selection is done using a proper name tagger. 100 million unsupervised data focused on proper nouns are mixed with supervised training data in the same 8:2 ratio for training. Using the same amount of data, training with speech filtered for proper nouns resulted in a relative WER of 6% to 13%, compared to a relative decrease of 4% to 12% using random selection. Gives a drop.
最後に、フィルタリングのアイデアは、10億の教師なしトレーニングデータ全体に拡張され、固有名詞を有する約5億の発話を残す。フィルタリングされたデータを使用する弱蒸留は、コールドフュージョンと組み合わされる。最終結果(Table 10(表10)におけるE14)は、10億のデータすべてを使用するよりもはるかに優れており、4つのテストセットすべてにおけるベースラインシステムのWERをそれぞれ6%～17%低下させる。 Finally, the filtering idea is extended across the 1 billion unsupervised training data, leaving about 500 million utterances with proper nouns. Weak distillation using filtered data is combined with cold fusion. The final result (E14 in Table 10) is much better than using all 1 billion data and reduces the WER of the baseline system in all four test sets by 6% to 17%, respectively. .
教師なしデータによってもたらされる改善を理解するために、2つのシステムB0およびE14がこのセクションにおいて比較される。B0は、教師ありトレーニングデータのみを使用し、E14は、追加された教師なしトレーニングデータを使用する。テストセット内に現れるが、対応するトレーニングセット内には現れない一意の単語の割合として計算された語彙外(「OOV」)率がTable 11(表11)内に提示されている。教師なしデータでは、固有名詞セットにおけるOOV率は、最大84.0%低下する。これは、教師なしデータが実際により多くのテールワードをもたらすことを示唆している。 Two systems B0 and E14 are compared in this section to understand the improvement brought about by unsupervised data. B0 uses only supervised training data and E14 uses additional unsupervised training data. The out-of-vocabulary (“OOV”) rate, calculated as the percentage of unique words that appear in the test set but not in the corresponding training set, is presented in Table 11. In unsupervised data, the OOV rate in the proper noun set decreases by up to 84.0%. This suggests that unsupervised data actually yields more tail words.
ロングテールワードにおけるLASモデルのパフォーマンスを改善するための教師なし音声データの使用が調査された。大量の教師なしデータのためのトランスクリプトトゥルースとしてテキスト仮説を生成するために、コンテキストバイアスを用いる従来のASRシステムが教師モデルとして使用された。これらの機械がラベル付けしたデータは、エンドツーエンドLASモデルをトレーニングするために、人間がラベル付けしたデータと混合された。まれな単語におけるLASモデルの弱点に焦点を当てるために、固有名詞モデルベースのフィルタリングが教師なしデータに適用された。フィルタリングされたデータでは、実験結果は、教師なしデータを導入することによって最大17%の相対的なWERの低下を達成することができたことを示している。 The use of unsupervised speech data to improve the performance of LAS models on long-tail words was investigated. A traditional ASR system with context bias was used as a supervised model to generate text hypotheses as transcript truth for large amounts of unsupervised data. These machine-labeled data were mixed with human-labeled data to train an end-to-end LAS model. Proper noun model-based filtering was applied to the unsupervised data to focus on the weaknesses of the LAS model in rare words. For filtered data, experimental results show that a relative WER reduction of up to 17% could be achieved by introducing unsupervised data.
たとえば、「call」という接頭辞の出現は、連絡先の名前が次の単語である可能性が高いことを示すことができるので、この接頭辞に対する接頭辞FST163は、音声認識器100に連絡先名コンテキストFST160を有効にさせることができる。 For example, the occurrence of the prefix "call" can indicate that the contact's name is likely to be the following word, so the prefix FST163 for this prefix tells speech recognizer 100 to Name context FST160 can be enabled.
ソフトウェアアプリケーション(すなわち、ソフトウェアリソース)は、コンピュータデバイスにタスクを実行させるコンピュータソフトウェアを指す場合がある。いくつかの例では、ソフトウェアアプリケーションは、「アプリケーション」、「アプリ」、または「プログラム」と呼ばれる場合がある。例示的なアプリケーションは、限定はしないが、システム診断アプリケーション、システム管理アプリケーション、システムメンテナンスアプリケーション、ワード処理アプリケーション、スプレッドシートアプリケーション、メッセージングアプリケーション、メディアストリーミングアプリケーション、ソーシャルネットワーキングアプリケーション、およびゲームアプリケーションを含む。 A software application (ie, software resource) may refer to computer software that causes a computing device to perform a task. In some examples, a software application may be referred to as an "application," "app," or "program." Exemplary applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
非一時的メモリは、コンピューティングデバイスによって使用するために、プログラム(たとえば、命令のシーケンス)またはデータ(たとえば、プログラム状態情報)を一時的または永続的に記憶するために使用される物理デバイスであり得る。非一時的メモリは、揮発性および/または不揮発性のアドレス可能な半導体メモリであり得る。不揮発性メモリの例は、限定はしないが、フラッシュメモリおよび読み取り専用メモリ(ROM)/プログラマブル読み取り専用メモリ(PROM)/消去可能なプログラマブル読み取り専用メモリ(EPROM)/電気的消去可能なプログラマブル読み取り専用メモリ(EEPROM)(たとえば、典型的には、ブートプログラムなどのファームウェアのために使用される)を含む。揮発性メモリの例は、限定はしないが、ランダムアクセスメモリ(RAM)、ダイナミックランダムアクセスメモリ(DRAM)、スタティックランダムアクセスメモリ(SRAM)、相変化メモリ(PCM)、ならびにディスクまたはテープを含む。 Non-transitory memory is a physical device used to temporarily or permanently store programs (e.g., sequences of instructions) or data (e.g., program state information) for use by a computing device. obtain. Non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, Flash memory and Read Only Memory (ROM)/Programmable Read Only Memory (PROM)/Erasable Programmable Read Only Memory (EPROM)/Electrically Erasable Programmable Read Only Memory (EEPROM) (e.g., typically used for firmware such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM), and disk or tape.
図5は、音声を転写するためにコンテキストバイアスを使用する方法500のための動作の例示的な配置のフローチャートである。動作502において、方法500は、データ処理ハードウェア610(図6)において、発話120を符号化するオーディオデータ125を受信するステップを含む。いくつかの例では、データ処理ハードウェア610は、ユーザデバイス110上に存在する。他の実装形態において、データ処理ハードウェア610は、ユーザデバイス110と通信するリモートサーバ101上に存在する。ユーザデバイス110は、ユーザ115によって話された発話120をキャプチャし、発話120を符号化するオーディオデータ125を生成する1つまたは複数のマイクロフォンを含み得る。
FIG. 5 is a flowchart of an example arrangement of operations for a
動作504において、方法300は、データ処理ハードウェア610によって、音声認識モデル200を使用して、音声要素の音声認識スコア145を生成するためにオーディオデータ125を処理するステップを含む。音声認識スコア145は、異なる音声要素が発生した尤度を示し得る。音声要素の例は、単語断片または書記素を含み得る。音声認識モデル200は、発音モデル、音響モデル、および言語モデルの機能を単一のモジュール(たとえば、ニューラルネットワーク)に組み合わせたエンドツーエンドモデルを含むので、音声認識モデル200は、音響特徴125、または生のオーディオデータ125さえも受信し、それに応答して、正字法特徴(たとえば、書記素、単語断片、または単語)の尤度を示す出力スコアを提供し得る。
At
動作506において、方法500は、データ処理ハードウェア610によって、発話120のコンテキスト122を示すコンテキストデータに基づいて、音声要素のコンテキストスコア166を決定するステップも含む。音声認識器100は、発話120のコンテキスト122に関する様々なタイプの情報を受信するコンテキスト分析モジュール165を含むことができる。たとえば、デバイス110の場所、デバイス110を使用してユーザ115によって実行されているタスク、デバイス110上で開いているかまたはアクティブであるアプリケーションなどに関する情報を受信し得る。コンテキスト分析モジュール165は、ビーム検索プロセス中の部分的または予備的なトランスクリプション186内に含まれる、最近認識された書記素、単語断片、単語、またはフレーズなどの、最近の音声認識決定を示すデータを受信することもできる。順次発生するように示されているが、コンテキスト分析モジュール165は、コンテキストスコア166を計算し得、同時に、音声認識モジュール200は、音声認識スコア145を計算するためにオーディオ特徴135を評価する。
At
動作508において、方法500は、データ処理ハードウェア610によって、音声認識スコア145とコンテキストスコア166とを使用して、発話120に対する1つまたは複数の候補トランスクリプションを決定するためにビーム検索復号プロセスを実行するステップも含む。ビーム検索プロセスは、音声格子175を通る潜在的なパスを評価し、トランスクリプション185を決定し得る。格子175を通る各パスは、異なる候補トランスクリプションを表すことができる。ここで、コンテキストスコア166は、音声認識全体よりも現在のコンテキストにおける特定のユーザ115に関連する用語に向けて認識をバイアスする。いくつかの実装形態において、スコアコンバイナ170は、音声格子175において使用される結合されたスコア172を生成するために、音声認識スコア145をコンテキストスコア166と結合する。重要なことに、コンテキストスコア166のバイアスの影響は、格子175の剪定の前、およびビーム検索におけるビーム剪定の前に、音声格子175に適用される。結果として、コンテキストスコア166の影響により、そうでなければ剪定されていた可能性がある関連用語が、ビーム検索プロセスおよび関連する剪定を通じて維持される。
At act 508,
動作510において、方法500は、データ処理ハードウェア610によって、1つまたは複数の候補トランスクリプションから発話120に対するトランスクリプション185を選択するステップも含む。格子175を通る各パスは、異なる候補トランスクリプションを表すことができる。ビーム剪定180は、可能性のある単語または単語断片について検索される格子175の範囲を減少させることができる。各単語断片の確率が計算されると、最も可能性が高い経路のみが残るまで、可能性の低い検索パスが剪定される。トランスクリプション185が決定されると、トランスクリプション185は、様々な方法のいずれかにおいて使用することができ、たとえば、ユーザ115に対して表示される、テキストフィールドに入力される、検索エンジンへのクエリまたはデジタル会話アシスタントへの要求として送信される、音声コマンドとして解釈されるなどである。
At
図6は、この文書において説明されているシステムおよび方法を実装するために使用され得る例示的なコンピューティングデバイス600の概略図である。コンピューティングデバイス600は、ラップトップ、デスクトップ、ワークステーション、携帯情報端末、サーバ、ブレードサーバ、メインフレーム、および他の適切なコンピュータなどの様々な形態のデジタルコンピュータを表すことを意図している。ここに示されている構成要素、それらの接続および関係、ならびにそれらの機能は、例示のみであることを意図しており、この文書において説明および/または特許請求されている本発明の実装形態を限定することを意図していない。
FIG. 6 is a schematic diagram of an
コンピューティングデバイス600は、プロセッサ610と、メモリ620と、記憶デバイス630と、メモリ620および高速拡張ポート650に接続する高速インターフェース/コントローラ640と、低速バス670および記憶デバイス630に接続する低速インターフェース/コントローラ660とを含む。構成要素610、620、630、640、650、および660の各々は、様々なバスを使用して相互接続されており、共通のマザーボード上に、または必要に応じて他の方法で取り付けられ得る。プロセッサ610は、高速インターフェース640に結合されたディスプレイ680などの外部入力/出力デバイス上にグラフィカルユーザインターフェース(GUI)のためのグラフィカル情報を表示するために、メモリ620内または記憶デバイス630上に記憶された命令を含む、コンピューティングデバイス600内で実行するための命令を処理することができる。他の実装形態において、複数のメモリおよびメモリのタイプとともに、必要に応じて、複数のプロセッサおよび/または複数のバスが使用され得る。また、(たとえば、サーババンク、ブレードサーバのグループ、またはマルチプロセッサシステムとして)複数のコンピューティングデバイス600が接続され、各デバイスが必要な動作の一部を提供し得る。
メモリ620は、コンピューティングデバイス600内に情報を非一時的に記憶する。メモリ620は、コンピュータ可読媒体、揮発性メモリユニット、または不揮発性メモリユニットであり得る。非一時的メモリ620は、コンピューティングデバイス600によって使用するために、プログラム(たとえば、命令のシーケンス)またはデータ(たとえば、プログラム状態情報)を一時的または永続的に記憶するために使用される物理デバイスであり得る。不揮発性メモリの例は、限定はしないが、フラッシュメモリおよび読み取り専用メモリ(ROM)/プログラマブル読み取り専用メモリ(PROM)/消去可能なプログラマブル読み取り専用メモリ(EPROM)/電気的消去可能なプログラマブル読み取り専用メモリ(EEPROM)(たとえば、典型的には、ブートプログラムなどのファームウェアのために使用される)を含む。揮発性メモリの例は、限定はしないが、ランダムアクセスメモリ(RAM)、ダイナミックランダムアクセスメモリ(DRAM)、スタティックランダムアクセスメモリ(SRAM)、相変化メモリ(PCM)、ならびにディスクまたはテープを含む。
記憶デバイス630は、コンピューティングデバイス600のための大容量ストレージを提供することができる。いくつかの実装形態において、記憶デバイス630は、コンピュータ可読媒体である。様々な異なる実装形態において、記憶デバイス630は、フロッピーディスク、ハードディスクドライブ、光ディスクドライブ、もしくはテープドライブ、フラッシュメモリもしくは他の同様のソリッドステートメモリデバイス、またはストレージエリアネットワークもしくは他の構成におけるデバイスを含むデバイスのアレイであり得る。追加の実装形態において、コンピュータプログラム製品は、情報キャリア内に明白に具体化される。コンピュータプログラム製品は、実行されると、上記で説明されている方法などの、1つまたは複数の方法を実行する命令を含む。情報キャリアは、メモリ620、記憶デバイス630、またはプロセッサ610上のメモリなどの、コンピュータ可読媒体または機械可読媒体である。
高速コントローラ640は、コンピューティングデバイス600のための帯域幅を消費する動作を管理し、低速コントローラ660は、より帯域幅を消費しない動作を管理する。そのような役割の割り当ては、単なる例示である。いくつかの実装形態において、高速コントローラ640は、メモリ620、(たとえば、グラフィックスプロセッサまたはアクセラレータを介して)ディスプレイ680、および様々な拡張カード(図示せず)を受け入れ得る高速拡張ポート650に結合される。いくつかの実装形態において、低速コントローラ660は、記憶デバイス630および低速拡張ポート690に結合される。様々な通信ポート(たとえば、USB、Bluetooth、イーサネット、ワイヤレスイーサネット)を含み得る低速拡張ポート690は、キーボード、ポインティングデバイス、スキャナ、または、たとえば、ネットワークアダプタを介してスイッチもしくはルータなどのネットワーキングデバイスなどの、1つまたは複数の入力/出力デバイスに結合され得る。
A high-
コンピューティングデバイス600は、図に示されるように、いくつかの異なる形態において実装され得る。たとえば、それは、標準的なサーバ600aとして、もしくはサーバ600aのグループ内で複数回、ラップトップコンピュータ600bとして、またはラックサーバシステム600cの一部として実装され得る。
本明細書で説明されているシステムおよび技法の様々な実装形態は、デジタル電気および/もしくは光回路、集積回路、特別に設計されたASIC(特定用途向け集積回路)、コンピュータハードウェア、ファームウェア、ソフトウェア、ならびに/またはそれらの組合せにおいて実現することができる。これらの様々な実装形態は、記憶装置、少なくとも1つの入力デバイス、および少なくとも1つの出力デバイスからデータおよび命令を受信し、記憶装置、少なくとも1つの入力デバイス、および少なくとも1つの出力デバイスにデータおよび命令を送信するように結合された、専用または汎用であり得る少なくとも1つのプログラマブルプロセッサを含むプログラマブルシステム上で実行可能および/または解釈可能な1つまたは複数のコンピュータプログラムにおける実装形態を含むことができる。 Various implementations of the systems and techniques described herein may include digital electrical and/or optical circuits, integrated circuits, specially designed ASICs (Application Specific Integrated Circuits), computer hardware, firmware, software. , and/or a combination thereof. These various implementations receive data and instructions from a storage device, at least one input device, and at least one output device, and transmit data and instructions to the storage device, at least one input device, and at least one output device. may include implementation in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor, which may be special purpose or general purpose, coupled to transmit.
これらのコンピュータプログラム(プログラム、ソフトウェア、ソフトウェアアプリケーション、またはコードとしても知られる)は、プログラマブルプロセッサのための機械語を含み、高級手続き型および/もしくはオブジェクト指向言語、ならびに/またはアセンブリ/機械語において実装することができる。本明細書で使用される場合、「機械可読媒体」および「コンピュータ可読媒体」という用語は、機械命令を機械可読信号として受信する機械可読媒体を含む、プログラマブルプロセッサに機械命令および/またはデータを提供するために使用される任意のコンピュータプログラム製品、非一時的コンピュータ可読媒体、装置、および/またはデバイス(たとえば、磁気ディスク、光ディスク、メモリ、プログラマブルロジックデバイス(PLD))を指す。「機械可読信号」という用語は、機械命令および/またはデータをプログラマブルプロセッサに提供するために使用される任意の信号を指す。 These computer programs (also known as programs, software, software applications, or code) include machine language for programmable processors and may be implemented in high-level procedural and/or object-oriented languages, and/or assembly/machine language. can do. As used herein, the terms "machine-readable medium" and "computer-readable medium" include a machine-readable medium that receives machine instructions as a machine-readable signal to provide machine instructions and/or data to a programmable processor. Refers to any computer program product, non-transitory computer readable medium, apparatus, and/or device (e.g., magnetic disk, optical disk, memory, programmable logic device (PLD)) used to The term "machine readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
本明細書で説明されているプロセスおよび論理フローは、入力データに対して動作し、出力を生成することによって機能を実行するために1つまたは複数のコンピュータプログラムを実行する、データ処理ハードウェアとも呼ばれる1つまたは複数のプログラマブルプロセッサによって実行することができる。プロセスおよび論理フローは、専用論理回路、たとえば、FPGA(フィールドプログラマブルゲートアレイ)またはASIC(特定用途向け集積回路)によって実行することもできる。コンピュータプログラムの実行に適したプロセッサは、例として、汎用および専用の両方のマイクロプロセッサ、ならびに任意の種類のデジタルコンピュータの任意の1つまたは複数のプロセッサを含む。一般に、プロセッサは、読み取り専用メモリ、もしくはランダムアクセスメモリ、またはその両方から命令とデータとを受信する。コンピュータの本質的な要素は、命令を実行するためのプロセッサと、命令とデータとを記憶するための1つまたは複数のメモリである。一般に、コンピュータは、データを記憶するための1つまたは複数の大容量記憶デバイス、たとえば、磁気ディスク、光磁気ディスク、または光ディスクも含むか、あるいはそれらからデータを受信、もしくはそれらにデータを送信、またはその両方を行うように動作可能に結合される。しかしながら、コンピュータは、そのようなデバイスを有する必要はない。コンピュータプログラム命令とデータとを記憶するのに適したコンピュータ可読媒体は、例として、半導体メモリデバイス、たとえば、EPROM、EEPROM、およびフラッシュメモリデバイス、磁気ディスク、たとえば、内部ハードディスクまたはリムーバブルディスク、光磁気ディスク、ならびにCD ROMおよびDVD-ROMディスクを含むか、すべての形態の不揮発性メモリ、媒体、およびメモリデバイスを含む。プロセッサおよびメモリは、専用論理回路によって補完することができ、または専用論理回路の中に組み込むことができる。 The processes and logic flows described herein are also data processing hardware that executes one or more computer programs to perform functions by operating on input data and producing output. It can be executed by one or more programmable processors called processors. The processes and logic flows may also be performed by dedicated logic circuits, such as FPGAs (Field Programmable Gate Arrays) or ASICs (Application Specific Integrated Circuits). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor receives instructions and data from read-only memory and/or random access memory. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally, a computer also includes, or receives data from, or transmits data to, one or more mass storage devices for storing data, such as magnetic disks, magneto-optical disks, or optical disks. or operably coupled to do both. However, a computer does not need to have such a device. Computer readable media suitable for storing computer program instructions and data include, by way of example, semiconductor memory devices such as EPROM, EEPROM and flash memory devices, magnetic disks such as internal hard disks or removable disks, magneto-optical disks. , and includes CD ROM and DVD-ROM discs, or all forms of non-volatile memory, media, and memory devices. The processor and memory can be supplemented by or embedded within dedicated logic circuitry.
ユーザとの対話を提供するために、本開示の1つまたは複数の態様は、ユーザに情報を表示するための表示デバイス、たとえば、CRT(陰極線管)、LCD(液晶ディスプレイ)モニタ、またはタッチスクリーンと、オプションで、ユーザがコンピュータに入力を提供することができるキーボードおよびポインティングデバイス、たとえば、マウスまたはトラックボールとを有するコンピュータ上に実装することができる。他の種類のデバイスも同様にユーザとの対話を提供するために使用することができ、たとえば、ユーザに提供されるフィードバックは、任意の形態の感覚的フィードバック、たとえば、視覚的フィードバック、聴覚的フィードバック、または触覚的フィードバックとすることができ、ユーザからの入力は、音響的入力、音声入力、または触覚的入力を含む任意の形態で受信することができる。それに加えて、コンピュータは、ユーザによって使用されるデバイスにドキュメントを送信し、そのデバイスからドキュメントを受信することによって、たとえば、ユーザのクライアントデバイス上のウェブブラウザから受信された要求に応答して、ウェブブラウザにウェブページを送信することによって、ユーザと対話することができる。 To provide user interaction, one or more aspects of the present disclosure may include a display device, such as a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen, for displaying information to the user. and, optionally, a keyboard and pointing device, such as a mouse or trackball, that allow a user to provide input to the computer. Other types of devices may be used to provide user interaction as well, for example the feedback provided to the user may be any form of sensory feedback, e.g. visual feedback, auditory feedback. , or tactile feedback, and input from the user can be received in any form, including acoustic, vocal, or tactile input. In addition, the computer can send documents to and receive documents from the device used by the user, e.g., in response to requests received from a web browser on the user's client device, You can interact with the user by sending a web page to the browser.
いくつかの実装形態について説明してきた。それにもかかわらず、本開示の要旨および範囲から逸脱することなく、様々な変更が行われ得ることが理解されよう。したがって、他の実装形態は、以下の特許請求の範囲内にある。 Several implementations have been described. Nevertheless, it will be understood that various changes may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
100 音声認識器
101 リモートサーバ
110 ユーザデバイス、デバイス、音響フレーム
115 ユーザ、ユーザデバイス
120 発話
122 コンテキスト、コンテキスト情報
125 オーディオデータ
130 特徴抽出モジュール、モジュール
135 オーディオ特徴、音響特徴
145 音声認識スコア、ASRスコア
150 データスコア、データストレージ
155 コンテキストFST生成モジュール
160、160a～n コンテキストFST
163 接頭辞FST
164 空の接頭辞オプション、接頭辞FST
165 コンテキスト分析モジュール
166 コンテキストスコア
170 スコアコンバイナ
172 結合されたスコア
175 音声格子、格子
180 ビーム剪定プロセス、ビーム剪定、剪定、トランスクリプション
185 トランスクリプション
186 部分的または予備的なトランスクリプション、予備的トランスクリプション、コンテキスト情報
193 教師なしデータ、教師なしトレーニングデータ
195 合成データ、合成トレーニングデータ
197 教師ありデータ、教師ありトレーニングデータ
200 音声認識モデル、RNN-Tモデル、モデル
210 エンコーダネットワーク
220 予測ネットワーク
230 共同ネットワーク
240 最終ソフトマックス層、ソフトマックス層
300 図
400 概略図
600 コンピューティングデバイス
600a サーバ
600b ラップトップコンピュータ
600c ラックサーバシステム
610 プロセッサ、構成要素
620 メモリ、構成要素
630 記憶デバイス、構成要素
640 高速インターフェース/コントローラ、構成要素、高速インターフェース、高速コントローラ
650 高速拡張ポート、構成要素
660 低速インターフェース/コントローラ、構成要素、低速コントローラ
670 低速バス
680 ディスプレイ
690 低速拡張ポート
100 speech recognizer
101 remote server
110 User devices, devices, acoustic frames
115 User, user device
120 Utterance
122 Context, context information
125 Audio data
130 Feature extraction module, module
135 Audio characteristics, acoustic characteristics
145 Speech recognition score, ASR score
150 data score, data storage
155 Context FST generation module
160, 160a-n Context FST
163 Prefix FST
164 Empty prefix option, prefix FST
165 Context Analysis Module
166 Context Score
170 Score Combiner
172 Combined score
175 Phonetic grid, grid
180 beam pruning process, beam pruning, pruning, transcription
185 Transcription
186 Partial or Preliminary Transcription, Preliminary Transcription, Contextual Information
193 Unsupervised data, unsupervised training data
195 Synthetic data, synthetic training data
197 Supervised data, supervised training data
200 speech recognition models, RNN-T models, models
210 encoder network
220 Prediction Network
230 Collaborative Network
240 Final softmax layer, softmax layer
300 figures
400 Schematic diagram
600 computing devices
600a server
600b laptop computer
600c rack server system
610 Processor, Components
620 Memory, Components
630 Storage devices, components
640 High Speed Interface/Controller, Components, High Speed Interface, High Speed Controller
650 High Speed Expansion Port, Component
660 Low Speed Interface/Controller, Components, Low Speed Controller
670 low speed bus
680 display
690 Low Speed Expansion Port
Claims (36)
前記データ処理ハードウェア(610)によって、音声認識モデル(220)を使用して、音声要素の音声認識スコア(145)を生成するために前記オーディオデータ(125)を処理するステップと、
前記データ処理ハードウェア(610)によって、前記発話(120)のコンテキスト(122)を示すコンテキストデータに基づいて前記音声要素のコンテキストスコア(166)を決定するステップと、
前記データ処理ハードウェア(610)によって、前記音声認識スコア(145)と前記コンテキストスコア(166)とを使用して、前記発話(120)に対する1つまたは複数の候補トランスクリプションを決定するためにビーム検索復号プロセス(180)を実行するステップと、
前記データ処理ハードウェア(610)によって、前記1つまたは複数の候補トランスクリプションから前記発話(120)に対するトランスクリプション(185)を選択するステップと
を含む方法であって、
前記データ処理ハードウェア(610)によって、前記発話(120)に対する予備的トランスクリプション(186)が前記発話(120)の前記コンテキスト(122)に対応する接頭辞要素を表す単語を含むことを判定するステップをさらに含み、
前記音声要素の前記コンテキストスコア(166)を決定するステップが、前記発話(120)に対する前記予備的トランスクリプション(186)が前記発話(120)の前記コンテキスト(122)に対応する前記接頭辞要素を表す前記単語を含むという判定に基づく、
方法(500)。 receiving, at data processing hardware (610), audio data (125) encoding the utterance (120);
processing the audio data (125) by the data processing hardware (610) to generate a speech recognition score (145) for speech elements using a speech recognition model (220);
determining, by the data processing hardware (610), a context score (166) for the speech element based on context data indicating a context (122) of the utterance (120);
the data processing hardware (610) to determine one or more candidate transcriptions for the utterance (120) using the speech recognition score (145) and the context score (166); performing a beam search decoding process (180);
selecting, by the data processing hardware (610), a transcription (185) for the utterance (120) from the one or more candidate transcriptions, the method comprising:
determining by the data processing hardware (610) that a preliminary transcription (186) for the utterance (120) includes a word representing a prefix element corresponding to the context (122) of the utterance (120); further comprising the step of
The step of determining the context score (166) of the speech element includes the step of determining the prefix element for which the preliminary transcription (186) for the utterance (120) corresponds to the context (122) of the utterance (120). Based on the determination that it contains the word representing
Method(500).
前記データ処理ハードウェア(610)において、前記発話(120)を符号化するオーディオデータ(125)を受信するステップと、
前記データ処理ハードウェア(610)において、音声認識モデル(220)を使用して、音声要素の音声認識スコア(145)を生成するために前記オーディオデータ(125)を処理するステップと、
前記データ処理ハードウェア(610)によって、前記発話(120)のコンテキスト(122)を示すコンテキストデータに基づいて前記音声要素のコンテキストスコア(166)を決定するステップと、
前記データ処理ハードウェア(610)によって、前記音声認識スコア(145)と前記コンテキストスコア(166)とを使用して、前記発話(120)に対する1つまたは複数の候補トランスクリプションを決定するためにビーム検索復号プロセス(180)を実行するステップと、
前記データ処理ハードウェア(610)によって、前記1つまたは複数の候補トランスクリプションから前記発話(120)に対するトランスクリプション(185)を選択するステップと
を含む方法であって、
前記複数のコンテキストFST(160)内の少なくとも1つのコンテキストFST(160)の各々について、
前記データ処理ハードウェア(610)によって、前記対応するコンテキストFST(160)の前記異なる特定のコンテキスト(122)に各々が対応する1つまたは複数の接頭辞のセットを含む対応する接頭辞FST(163)を生成するステップをさらに含む、
方法(500)。 Data processing hardware (610) generates a plurality of context FSTs (160) each representing a different set of words or phrases within the personalized data collection (150) of the user (115) who spoke the utterance (120) a step in which each context FST (160) in the plurality of contexts FST (160) corresponds to a different specific context (122);
receiving, at the data processing hardware (610), audio data (125) encoding the utterance (120);
processing the audio data (125) in the data processing hardware (610) to generate a speech recognition score (145) for speech elements using a speech recognition model (220);
determining, by the data processing hardware (610), a context score (166) for the speech element based on context data indicating a context (122) of the utterance (120);
the data processing hardware (610) to determine one or more candidate transcriptions for the utterance (120) using the speech recognition score (145) and the context score (166); performing a beam search decoding process (180);
selecting, by the data processing hardware (610), a transcription (185) for the utterance (120) from the one or more candidate transcriptions, the method comprising:
For each of at least one context FST (160) in the plurality of contexts FST (160),
The data processing hardware (610) generates a corresponding prefix FST (163) comprising a set of one or more prefixes each corresponding to the different specific context (122) of the corresponding context FST (160). ),
Method(500).
前記音声要素の前記コンテキストスコア(166)を決定するステップが、前記識別された特定のコンテキスト(122)に基づく、
請求項1から4のいずれか一項に記載の方法(500)。 further comprising identifying, by the data processing hardware (610), a particular context (122) for the utterance (120) based on the context data;
determining the context score (166) of the audio element is based on the identified specific context (122);
A method (500) according to any one of claims 1 to 4.
前記データ処理ハードウェア(610)によって、前記データ処理ハードウェア(610)と通信するメモリハードウェア(620)内に前記複数のコンテキストFST(160)を記憶するステップ
をさらに含む、請求項2に記載の方法(500)。 prior to receiving the audio data (125) encoding the utterance (120);
3. The method of claim 2, further comprising storing, by the data processing hardware (610), the plurality of contexts FSTs (160) in memory hardware (620) in communication with the data processing hardware (610). method(500).
前記データ処理ハードウェア(610)によって、前記複数のコンテキストFST(160)内の前記少なくとも1つのコンテキストFST(160)に対して生成された前記対応する接頭辞FST(163)を記憶するステップ
をさらに含む、請求項7から10のいずれか一項に記載の方法(500)。 For each of at least one context FST (160) in the plurality of contexts FST (160),
further comprising storing the corresponding prefix FST (163) generated by the data processing hardware (610) for the at least one context FST (160) in the plurality of contexts FST (160). 11. The method (500) of any one of claims 7-10, comprising:
前記データ処理ハードウェア(610)によって、前記予備的トランスクリプション(186)内に含まれる前記1つまたは複数の接頭辞のうちの前記1つを含む前記接頭辞FST(163)に対応する前記コンテキストFST(160)を選択的にアクティブ化するステップと
をさらに含み、
前記音声要素の前記コンテキストスコア(166)を決定するステップが、前記選択的にアクティブ化されたコンテキストFST(160)に基づく、
請求項11に記載の方法(500)。 The data processing hardware (610) causes a preliminary transcription (186) (185) for the utterance (120) to be performed using one of the one or more prefixes of the corresponding prefix FST (163). a step of determining that the
The data processing hardware (610) causes the prefix FST (163) to include the prefix FST (163) to include the one of the one or more prefixes included in the preliminary transcription (186). selectively activating the context FST (160);
determining the context score (166) of the audio element is based on the selectively activated context FST (160);
12. The method (500) of claim 11.
前記発話(120)を話したユーザ(115)に関連付けられたユーザデバイス(110)上に存在し、
前記音声認識モデル(220)を実行する、
請求項1から12のいずれか一項に記載の方法(500)。 The data processing hardware (610)
residing on a user device (110) associated with the user (115) who spoke the utterance (120);
executing the speech recognition model (220);
13. A method (500) according to any one of claims 1 to 12.
コンテキスト用語の前記部分語単位間の各遷移をバイアスするように構成された遷移重みと、
前記遷移重みのバイアス効果を元に戻すように構成されたオフセット重みを有するバックオフアークと
を含む、請求項16に記載の方法(500)。 The context FST(160) is
a transition weight configured to bias each transition between the subword units of a context term;
and a backoff arc having an offset weight configured to reverse the biasing effect of the transition weight.
前記データ処理ハードウェア(610)と通信するメモリハードウェア(620)であって、前記メモリハードウェア(620)が、前記データ処理ハードウェア(610)上で実行されると、前記データ処理ハードウェア(610)に、
発話(120)を符号化するオーディオデータ(125)を受信する動作と、
音声認識モデル(220)を使用して、音声要素の音声認識スコア(145)を生成するために前記オーディオデータ(125)を処理する動作と
前記発話(120)のコンテキスト(122)を示すコンテキストデータに基づいて前記音声要素のコンテキストスコア(166)を決定する動作と、
前記音声認識スコア(145)と前記コンテキストスコア(166)とを使用して、前記発話(120)に対する1つまたは複数の候補トランスクリプションを決定するためにビーム検索復号プロセス(180)を実行する動作と、
前記1つまたは複数の候補トランスクリプションから前記発話(120)に対するトランスクリプション(185)を選択する動作と
を含む動作を実行させる命令を記憶する、メモリハードウェア(620)と
を備えるシステムであって、
前記動作が、
前記発話(120)に対する予備的トランスクリプション(186)が前記発話(120)の前記コンテキスト(122)に対応する接頭辞要素を表す単語を含むことを判定する動作をさらに含み、
前記音声要素の前記コンテキストスコア(166)を決定する動作が、前記発話(120)に対する前記予備的トランスクリプション(186)が前記発話(120)の前記コンテキスト(122)に対応する前記接頭辞要素を表す前記単語を含むという判定に基づく、
システム(100)。 data processing hardware (610);
memory hardware (620) in communication with the data processing hardware (610), the memory hardware (620) being executed on the data processing hardware (610); (610)
an act of receiving audio data (125) encoding the utterance (120);
an act of processing said audio data (125) to generate a speech recognition score (145) for a speech element using a speech recognition model (220); and context data indicating a context (122) of said utterance (120). an act of determining a context score (166) of the audio element based on;
performing a beam search decoding process (180) to determine one or more candidate transcriptions for the utterance (120) using the speech recognition score (145) and the context score (166); movement and
a system comprising: an act of selecting a transcription (185) for the utterance (120) from the one or more candidate transcriptions; and memory hardware (620) storing instructions for performing the act. There it is,
The said operation is
further comprising an act of determining that a preliminary transcription (186) for the utterance (120) includes a word representing a prefix element corresponding to the context (122) of the utterance (120);
The act of determining the context score (166) of the speech element includes the prefix element for which the preliminary transcription (186) for the utterance (120) corresponds to the context (122) of the utterance (120). Based on the determination that it contains the word representing
System (100).
前記データ処理ハードウェア(610)と通信するメモリハードウェア(620)であって、前記メモリハードウェア(620)が、前記データ処理ハードウェア(610)上で実行されると、前記データ処理ハードウェア(610)に、
発話(120)を話したユーザ(115)のパーソナライズされたデータコレクション(150)内の単語またはフレーズの異なるセットを各々が表す複数のコンテキストFST(160)を生成する動作であって、前記複数のコンテキストFST(160)内の各コンテキストFST(160)が異なる特定のコンテキスト(122)に対応する、動作と、
前記発話(120)を符号化するオーディオデータ(125)を受信する動作と、
音声認識モデル(220)を使用して、音声要素の音声認識スコア(145)を生成するために前記オーディオデータ(125)を処理する動作と
前記発話(120)のコンテキスト(122)を示すコンテキストデータに基づいて前記音声要素のコンテキストスコア(166)を決定する動作と、
前記音声認識スコア(145)と前記コンテキストスコア(166)とを使用して、前記発話(120)に対する1つまたは複数の候補トランスクリプションを決定するためにビーム検索復号プロセス(180)を実行する動作と、
前記1つまたは複数の候補トランスクリプションから前記発話(120)に対するトランスクリプション(185)を選択する動作と
を含む動作を実行させる命令を記憶する、メモリハードウェア(620)と
を備えるシステムであって、
前記動作が、前記複数のコンテキストFST(160)内の少なくとも1つのコンテキストFST(160)の各々について、
前記対応するコンテキストFST(160)の前記異なる特定のコンテキスト(122)に各々が対応する1つまたは複数の接頭辞のセットを含む対応する接頭辞FST(163)を生成する動作をさらに含む、
システム(100)。 data processing hardware (610);
memory hardware (620) in communication with the data processing hardware (610), the memory hardware (620) being executed on the data processing hardware (610); (610)
an act of generating a plurality of contexts FSTs (160) each representing a different set of words or phrases in a personalized data collection (150) of a user (115) who uttered an utterance (120); an operation in which each context FST (160) in the context FST (160) corresponds to a different specific context (122);
an act of receiving audio data (125) encoding the utterance (120);
an act of processing said audio data (125) to generate a speech recognition score (145) for a speech element using a speech recognition model (220); and context data indicating a context (122) of said utterance (120). an act of determining a context score (166) of the audio element based on;
performing a beam search decoding process (180) to determine one or more candidate transcriptions for the utterance (120) using the speech recognition score (145) and the context score (166); movement and
a system comprising: an act of selecting a transcription (185) for the utterance (120) from the one or more candidate transcriptions; and memory hardware (620) storing instructions for performing the act. There it is,
The operation includes, for each of at least one context FST (160) in the plurality of contexts FST (160),
further comprising an act of generating a corresponding prefix FST (163) including a set of one or more prefixes each corresponding to the different specific context (122) of the corresponding context FST (160);
System(100).
前記コンテキストデータに基づいて、前記発話(120)に関する特定のコンテキスト(122)を識別する動作をさらに含み、
前記音声要素の前記コンテキストスコア(166)を決定する動作が、前記識別された特定のコンテキスト(122)に基づく、
請求項19から22のいずれか一項に記載のシステム(100)。 The said operation is
further comprising an act of identifying a particular context (122) for the utterance (120) based on the context data;
the act of determining the context score (166) of the audio element is based on the identified specific context (122);
A system (100) according to any one of claims 19 to 22.
前記データ処理ハードウェア(610)と通信するメモリハードウェア(620)内に前記複数のコンテキストFST(160)を記憶する動作と
をさらに含む、請求項20に記載のシステム(100)。 before the act of receiving the audio data (125) encoding the utterance (120);
21. The system (100) of claim 20, further comprising an act of storing the plurality of contexts FSTs (160) in memory hardware (620) in communication with the data processing hardware (610).
前記複数のコンテキストFST(160)内の前記少なくとも1つのコンテキストFST(160)に対して生成された前記対応する接頭辞FST(163)を記憶する動作
をさらに含む、請求項25から28のいずれか一項に記載のシステム(100)。 The operation includes, for each of at least one context FST (160) in the plurality of contexts FST (160),
29. Any of claims 25 to 28, further comprising an act of storing the corresponding prefix FST (163) generated for the at least one context FST (160) in the plurality of contexts FST (160). The system described in paragraph 1 (100).
前記発話(120)に対する予備的トランスクリプション(186)が前記対応する接頭辞FST(163)の前記1つまたは複数の接頭辞のうちの1つを含むことを判定する動作と、
前記予備的トランスクリプション(186)内に含まれる前記1つまたは複数の接頭辞のうちの前記1つを含む前記接頭辞FST(163)に対応する前記コンテキストFST(160)を選択的にアクティブ化する動作と
をさらに含み、
前記音声要素の前記コンテキストスコア(166)を決定する動作が、前記選択的にアクティブ化されたコンテキストFST(160)に基づく、
請求項29に記載のシステム(100)。 The said operation is
an act of determining that a preliminary transcription (186) for the utterance (120) includes one of the one or more prefixes of the corresponding prefix FST (163);
selectively activating said context FST (160) corresponding to said prefix FST (163) including said one of said one or more prefixes contained within said preliminary transcription (186); further comprising an action of
the act of determining the context score (166) of the audio element is based on the selectively activated context FST (160);
A system (100) according to claim 29.
前記発話(120)を話したユーザ(115)に関連付けられたユーザデバイス(110)上に存在し、
前記音声認識モデル(220)を実行する、
請求項19から30のいずれか一項に記載のシステム(100)。 The data processing hardware (610)
residing on a user device (110) associated with the user (115) who spoke the utterance (120);
executing the speech recognition model (220);
A system (100) according to any one of claims 19 to 30.
コンテキスト用語の前記部分語単位間の各遷移をバイアスするように構成された遷移重みと、
前記遷移重みのバイアス効果を元に戻すように構成されたオフセット重みを有するバックオフアークと
を含む、請求項34に記載のシステム(100)。 The context FST(160) is
a transition weight configured to bias each transition between the subword units of a context term;
and a backoff arc having an offset weight configured to reverse the biasing effect of the transition weight.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962846192P | 2019-05-10 | 2019-05-10 | |
US62/846,192 | 2019-05-10 | ||
PCT/US2020/024404 WO2020231522A1 (en) | 2019-05-10 | 2020-03-24 | Using context information with end-to-end models for speech recognition |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2022531615A JP2022531615A (en) | 2022-07-07 |
JP7417634B2 true JP7417634B2 (en) | 2024-01-18 |
Family
ID=70286001
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021566525A Active JP7417634B2 (en) | 2019-05-10 | 2020-03-24 | Using context information in end-to-end models for speech recognition |
Country Status (6)
Country | Link |
---|---|
US (1) | US11545142B2 (en) |
EP (1) | EP3966808A1 (en) |
JP (1) | JP7417634B2 (en) |
KR (1) | KR20220008309A (en) |
CN (1) | CN113874935A (en) |
WO (1) | WO2020231522A1 (en) |
Families Citing this family (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109243428B (en) * | 2018-10-15 | 2019-11-26 | 百度在线网络技术（北京）有限公司 | A kind of method that establishing speech recognition modeling, audio recognition method and system |
US11430433B2 (en) * | 2019-05-05 | 2022-08-30 | Microsoft Technology Licensing, Llc | Meeting-adapted language model for speech recognition |
US11282500B2 (en) * | 2019-07-19 | 2022-03-22 | Cisco Technology, Inc. | Generating and training new wake words |
CN110648658B (en) * | 2019-09-06 | 2022-04-08 | 北京达佳互联信息技术有限公司 | Method and device for generating voice recognition model and electronic equipment |
EP4073789B1 (en) * | 2020-01-17 | 2023-11-08 | Google LLC | Alphanumeric sequence biasing for automatic speech recognition |
US20210343277A1 (en) * | 2020-04-29 | 2021-11-04 | Samsung Electronics Co., Ltd. | System and method for out-of-vocabulary phrase support in automatic speech recognition |
US11715461B2 (en) * | 2020-10-21 | 2023-08-01 | Huawei Technologies Co., Ltd. | Transformer-based automatic speech recognition system incorporating time-reduction layer |
CN113129870B (en) * | 2021-03-23 | 2022-03-25 | 北京百度网讯科技有限公司 | Training method, device, equipment and storage medium of speech recognition model |
CN117396879A (en) * | 2021-06-04 | 2024-01-12 | 谷歌有限责任公司 | System and method for generating region-specific phonetic spelling variants |
CN113516968B (en) * | 2021-06-07 | 2022-05-20 | 北京邮电大学 | End-to-end long-term speech recognition method |
CN113393833B (en) * | 2021-06-16 | 2024-04-02 | 中国科学技术大学 | Audio and video awakening method, system, equipment and storage medium |
US11893983B2 (en) * | 2021-06-23 | 2024-02-06 | International Business Machines Corporation | Adding words to a prefix tree for improving speech recognition |
CN113793597A (en) * | 2021-09-15 | 2021-12-14 | 云知声智能科技股份有限公司 | Voice recognition method and device, electronic equipment and storage medium |
US20230096821A1 (en) * | 2021-09-30 | 2023-03-30 | Ronny Huang | Large-Scale Language Model Data Selection for Rare-Word Speech Recognition |
GB2607131B (en) * | 2021-11-11 | 2023-11-08 | Intelligent Voice Ltd | Self-learning end-to-end automatic speech recognition |
US11908454B2 (en) | 2021-12-01 | 2024-02-20 | International Business Machines Corporation | Integrating text inputs for training and adapting neural network transducer ASR models |
US20230197064A1 (en) * | 2021-12-17 | 2023-06-22 | Snap Inc. | Speech to entity |
CN114495114B (en) * | 2022-04-18 | 2022-08-05 | 华南理工大学 | Text sequence recognition model calibration method based on CTC decoder |
CN114765025A (en) * | 2022-04-25 | 2022-07-19 | 脸萌有限公司 | Method for generating and recognizing speech recognition model, device, medium and equipment |
US20230360646A1 (en) * | 2022-05-05 | 2023-11-09 | Nuance Communications, Inc. | End-to-end automatic speech recognition system for both conversational and command-and-control speech |
CN114944148B (en) * | 2022-07-09 | 2023-08-22 | 昆明理工大学 | Streaming Vietnam voice recognition method integrating external language knowledge |
WO2024091427A1 (en) * | 2022-10-26 | 2024-05-02 | Google Llc | Contextual biasing with text injection |
CN117708568A (en) * | 2024-02-02 | 2024-03-15 | 智慧眼科技股份有限公司 | Feature extraction method and device for large language model, computer equipment and medium |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6574597B1 (en) * | 1998-05-08 | 2003-06-03 | At&T Corp. | Fully expanded context-dependent networks for speech recognition |
US7308404B2 (en) * | 2001-09-28 | 2007-12-11 | Sri International | Method and apparatus for speech recognition using a dynamic vocabulary |
GB2495222B (en) * | 2011-09-30 | 2016-10-26 | Apple Inc | Using context information to facilitate processing of commands in a virtual assistant |
US9449598B1 (en) * | 2013-09-26 | 2016-09-20 | Amazon Technologies, Inc. | Speech recognition with combined grammar and statistical language models |
US10540957B2 (en) * | 2014-12-15 | 2020-01-21 | Baidu Usa Llc | Systems and methods for speech transcription |
US10049668B2 (en) * | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10304444B2 (en) * | 2016-03-23 | 2019-05-28 | Amazon Technologies, Inc. | Fine-grained natural language understanding |
US11107461B2 (en) * | 2016-06-01 | 2021-08-31 | Massachusetts Institute Of Technology | Low-power automatic speech recognition device |
US9934777B1 (en) * | 2016-07-01 | 2018-04-03 | Amazon Technologies, Inc. | Customized speech processing language models |
US20180293221A1 (en) * | 2017-02-14 | 2018-10-11 | Microsoft Technology Licensing, Llc | Speech parsing with intelligent assistant |
US20180330718A1 (en) * | 2017-05-11 | 2018-11-15 | Mitsubishi Electric Research Laboratories, Inc. | System and Method for End-to-End speech recognition |
-
2020
- 2020-03-24 EP EP20719068.7A patent/EP3966808A1/en active Pending
- 2020-03-24 KR KR1020217040648A patent/KR20220008309A/en active Search and Examination
- 2020-03-24 WO PCT/US2020/024404 patent/WO2020231522A1/en unknown
- 2020-03-24 JP JP2021566525A patent/JP7417634B2/en active Active
- 2020-03-24 US US16/827,937 patent/US11545142B2/en active Active
- 2020-03-24 CN CN202080034633.8A patent/CN113874935A/en active Pending
Non-Patent Citations (1)
Title |
---|
Yanzhange He et al.，Streaming End-to-end Speech Recognition For Mobile Devices，arXiv:1811.06621，2018年11月15日，https://arxiv.org/abs/1811.06621 |
Also Published As
Publication number | Publication date |
---|---|
JP2022531615A (en) | 2022-07-07 |
KR20220008309A (en) | 2022-01-20 |
US20200357388A1 (en) | 2020-11-12 |
EP3966808A1 (en) | 2022-03-16 |
US11545142B2 (en) | 2023-01-03 |
CN113874935A (en) | 2021-12-31 |
WO2020231522A1 (en) | 2020-11-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7417634B2 (en) | Using context information in end-to-end models for speech recognition | |
JP7234415B2 (en) | Context Bias for Speech Recognition | |
US11804218B2 (en) | Scalable dynamic class language modeling | |
JP6550068B2 (en) | Pronunciation prediction in speech recognition | |
JP7200405B2 (en) | Context Bias for Speech Recognition | |
US20210210100A1 (en) | Voice command processing for locked devices | |
US20220238101A1 (en) | Two-pass end to end speech recognition | |
JP2023545988A (en) | Transformer transducer: One model that combines streaming and non-streaming speech recognition | |
JP2023547847A (en) | Cascading encoder for simplified streaming and non-streaming ASR | |
US20240087562A1 (en) | Interactive content output | |
JP2024510817A (en) | Efficient streaming non-recurrent on-device end-to-end model | |
US20220310061A1 (en) | Regularizing Word Segmentation | |
CN117711376A (en) | Language identification method, system, equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20220107 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20230118 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20230123 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20230421 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20230807 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20231106 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20231211 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20240105 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7417634Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |