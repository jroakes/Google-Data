CN112805779A - Reading progress estimation based on speech fuzzy matching and confidence interval - Google Patents
Reading progress estimation based on speech fuzzy matching and confidence interval Download PDFInfo
- Publication number
- CN112805779A CN112805779A CN201880098288.7A CN201880098288A CN112805779A CN 112805779 A CN112805779 A CN 112805779A CN 201880098288 A CN201880098288 A CN 201880098288A CN 112805779 A CN112805779 A CN 112805779A
- Authority
- CN
- China
- Prior art keywords
- data
- text source
- phoneme
- text
- phonological
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 113
- 238000012545 processing Methods 0.000 claims abstract description 64
- 238000004590 computer program Methods 0.000 claims description 9
- 230000002708 enhancing effect Effects 0.000 abstract description 8
- 230000000704 physical effect Effects 0.000 description 121
- 230000000694 effects Effects 0.000 description 88
- 230000009471 action Effects 0.000 description 58
- 238000005259 measurement Methods 0.000 description 23
- 230000004044 response Effects 0.000 description 18
- 230000000977 initiatory effect Effects 0.000 description 17
- 238000004458 analytical method Methods 0.000 description 16
- 230000003287 optical effect Effects 0.000 description 15
- 230000006870 function Effects 0.000 description 10
- 230000015654 memory Effects 0.000 description 10
- 238000010586 diagram Methods 0.000 description 9
- 230000008569 process Effects 0.000 description 9
- 238000005516 engineering process Methods 0.000 description 6
- 238000011524 similarity measure Methods 0.000 description 6
- 241000282472 Canis lupus familiaris Species 0.000 description 5
- 230000005540 biological transmission Effects 0.000 description 5
- 230000007812 deficiency Effects 0.000 description 5
- 239000013589 supplement Substances 0.000 description 5
- 230000001755 vocal effect Effects 0.000 description 5
- 238000013500 data storage Methods 0.000 description 4
- 230000001934 delay Effects 0.000 description 4
- 230000004048 modification Effects 0.000 description 4
- 238000012986 modification Methods 0.000 description 4
- 230000001151 other effect Effects 0.000 description 4
- 230000005236 sound signal Effects 0.000 description 4
- 238000013518 transcription Methods 0.000 description 4
- 230000035897 transcription Effects 0.000 description 4
- 238000004519 manufacturing process Methods 0.000 description 3
- 230000001360 synchronised effect Effects 0.000 description 3
- 230000000007 visual effect Effects 0.000 description 3
- 206010011469 Crying Diseases 0.000 description 2
- 238000013459 approach Methods 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000003780 insertion Methods 0.000 description 2
- 230000037431 insertion Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 230000036314 physical performance Effects 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 230000004936 stimulating effect Effects 0.000 description 2
- 210000003813 thumb Anatomy 0.000 description 2
- 230000007704 transition Effects 0.000 description 2
- 230000017105 transposition Effects 0.000 description 2
- 206010002953 Aphonia Diseases 0.000 description 1
- 206010011224 Cough Diseases 0.000 description 1
- 241000282412 Homo Species 0.000 description 1
- 241001465754 Metazoa Species 0.000 description 1
- 206010050467 Tongue biting Diseases 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000003416 augmentation Effects 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000001914 calming effect Effects 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000001816 cooling Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 235000019800 disodium phosphate Nutrition 0.000 description 1
- 230000005670 electromagnetic radiation Effects 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 210000003811 finger Anatomy 0.000 description 1
- 239000012634 fragment Substances 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000010438 heat treatment Methods 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 230000008447 perception Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000035807 sensation Effects 0.000 description 1
- 235000019615 sensations Nutrition 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 230000011273 social behavior Effects 0.000 description 1
- 230000002194 synthesizing effect Effects 0.000 description 1
- 230000001131 transforming effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09B—EDUCATIONAL OR DEMONSTRATION APPLIANCES; APPLIANCES FOR TEACHING, OR COMMUNICATING WITH, THE BLIND, DEAF OR MUTE; MODELS; PLANETARIA; GLOBES; MAPS; DIAGRAMS
- G09B17/00—Teaching reading
- G09B17/003—Teaching reading electrically operated apparatus or devices
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/187—Phonemic context, e.g. pronunciation rules, phonotactical constraints or phoneme n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N7/00—Computing arrangements based on specific mathematical models
- G06N7/02—Computing arrangements based on specific mathematical models using fuzzy logic
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
- G10L2015/025—Phonemes, fenemes or fenones being the recognition units
Abstract
The present disclosure provides a technique for enhancing the ability of a computing device to detect a current reading position in a text source while the text source is being read aloud. An example method includes determining phoneme data for a text source, the text source including a sequence of words; receiving audio data comprising spoken words associated with a text source; comparing, by the processing device, the phoneme data of the text source with the phoneme data of the audio data; and identifying a location in the sequence of word words based on the comparison phonological data.
Description
Technical Field
The present disclosure relates to the field of computer-based human speech recognition, and in particular to enhancing the ability of a computer device to identify a reading location in a text source when a user reads the text source aloud.
Background
The capabilities and use of virtual assistants are expanding rapidly. Conventional virtual assistants include some form of computer human-machine interface that enables a human being to interact with the virtual assistant and cause the virtual assistant to perform tasks or services. The virtual assistant will typically record and understand human speech and may respond by synthesizing a reply. The virtual assistant may be initiated in response to a touch or gesture based command, or may continuously analyze its environment to detect spoken commands. When the command is detected, the virtual assistant can respond to or perform one or more actions.
Disclosure of Invention
The following is a simplified summary of the disclosure in order to provide a basic understanding of some aspects of the disclosure. This summary is not an extensive overview of the disclosure. It is intended to neither identify key or critical elements of the disclosure nor delineate any scope of particular embodiments of the disclosure or any scope of the claims. Its sole purpose is to present some concepts of the disclosure in a simplified form as a prelude to the more detailed description that is presented later.
In a first aspect of the disclosure, a method is provided. The method may include determining (e.g., calculating, retrieving, or receiving) phonological data of a text source, the text source including a sequence of words. The method may include receiving audio data, the audio data including spoken words associated with a text source. The method may include comparing, by the processing device, the phoneme data of the text source and the phoneme data of the audio data. The method may include identifying a location in the word sequence based on the comparison of the phonological data.
Alternatively, the text source may be a book and the location may be a current reading location in the book. The phonological data of the text source may include a phoneme encoding of a sequence of words. The phoneme encoding may include one or more sequences of phoneme values. Comparing the phoneme data may include calculating a phoneme edit distance between the phoneme data of the audio data and the phoneme data of the text source. Comparing the phoneme data may include calculating a numerical value representing a degree of similarity between two or more sequences of phoneme values. Comparing the phoneme data may include performing a fuzzy match between the phoneme data corresponding to the audio data and the phoneme data of the text source. The comparing may include comparing the audio data to a text source without converting the audio data to text using speech recognition. Identifying a location in the sequence of words can include determining that a spoken word matches a word in the sequence of words based on phonological data of the text source; and may include selecting a location of the word based on the phonological data of the text source. The method may include accessing text data of a text source. The method may include generating phoneme data based on the text data. The method may include associating the phonological data with a text source.
According to a second aspect of the present disclosure, a system comprising a processing device. The processing device may be configured to determine phoneme data for a text source, the text source including a sequence of words. The processing device may be configured to receive phoneme data including spoken words associated with the text source. The processing device may be configured to compare the phoneme data of the text source with the phoneme data of the audio data. The processing device may be configured to identify a location in the sequence of word words based on the comparison of the phonological data.
The system may include a data store. The system may include a communication system for communicating over a network, such as a local area network and/or a wide area network. The system may be, may be included in, or may be configured to implement a virtual assistant. The system may be configured to implement the method of the first aspect.
According to a third aspect of the present disclosure, a computer program product is configured such that, when processed by a processing device, the computer program product causes the processing device to perform the method of the first aspect.
Individual features and/or combinations of features defined above in accordance with any aspect of the present disclosure or below in relation to any particular embodiment may be used individually, separately or in combination with any other defined features in any other aspect or embodiment. Furthermore, the present disclosure is intended to cover an apparatus configured to perform any of the features described herein with respect to a method and/or a method of using or producing, using or manufacturing any of the apparatus features described herein.
Drawings
The present disclosure is illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings.
FIG. 1 illustrates an example environment with one or more computing devices according to embodiments of the present disclosure.
FIG. 2 is a block diagram illustrating an example computing device having components and modules for comparing phonological data derived from user input and phonological data derived from a text source in accordance with embodiments of the present disclosure.
FIG. 3 is a block diagram illustrating an example computing device having components and modules for identifying locations in text sources based on a user's audio input in accordance with embodiments of the present invention.
FIG. 4 is a block diagram illustrating an example computing device having components and modules for providing physical effects to enhance one or more users' experiences, according to embodiments of the invention.
Fig. 5 is a flow chart illustrating an example of a method according to an embodiment of the present disclosure.
Fig. 6 is a flow chart illustrating an example of a method according to an embodiment of the present disclosure.
Fig. 7 is a flow chart illustrating an example of a method according to an embodiment of the present disclosure.
Fig. 8 is a flow chart illustrating an example of a method according to an embodiment of the present disclosure.
Fig. 9 is a block diagram illustrating another example of a computing device in accordance with an embodiment of the present disclosure.
The drawings may be better understood when viewed in conjunction with the following detailed description.
Detailed Description
Modern computing devices often provide features to detect and understand human speech. These features may be associated with a virtual assistant that is accessible via a resource-constrained computing device, such as a smart speaker, mobile phone, smart watch, or other user device. The computing device may be associated with a microphone that can record human speech and can analyze the human speech using a combination of local and remote computing resources. Analyzing voice is typically a resource intensive operation, and a computing device may be configured to perform some processing local to the computing device, and with some processing performed remotely at a server or via a cloud service. Many virtual assistants use some form of remote speech recognition service that treats audio data as input and converts the audio data to text, which is returned to the computing device.
A number of technical problems arise when a computing device attempts to follow up using conventional virtual assistant features when a user reads a text source aloud. Some problems arise because conventional virtual assistant features perform speech recognition to convert audio to text. Speech recognition typically involves an acoustic step of converting audio into phonemes and a linguistic step of converting phonemes into text (e.g., words). The language step typically waits for a subsequent spoken word to establish context before converting the spoken word to text. The language step introduces unnecessary time delays and consumes additional computing resources. In addition, performing a traditional text-based comparison with a text source using recognized text may be more error prone than performing a speech comparison (e.g., a phoneme comparison). This problem typically arises because the spelling of many words that sound the same or similar may vary widely and may produce false negatives when performing text comparisons. In addition, conventional text comparisons may not properly account for situations where a user may jump around while reading the text source. For example, portions of the text source may be skipped, repeated, or new content added. This can make it difficult to identify the current reading position within the text source and to correctly detect the reading speed.
Aspects and embodiments of the present technology address the above and other deficiencies by providing enhancements to enable a computing device to detect a current reading location in a text source when reading the text source aloud. In one example, the techniques may avoid the linguistic steps of traditional speech recognition by comparing phoneme data derived from audio with phoneme data derived from a text source. The text source may be a book, magazine, presentation, lecture, script, or other source that includes a sequence of words. The techniques may receive audio data including words spoken by a user, and may convert the audio data into phoneme data locally or with the assistance of a remote server (e.g., a cloud service). The phoneme data of the audio and text sources may then be compared via a speech comparison rather than a more traditional text comparison. The voice comparison may be accompanied by fuzzy matching to identify a location (e.g., a current reading location) within the word sequence.
The systems and methods described herein include techniques to enhance the technical field of computer-based human speech recognition. In particular, the disclosed techniques enhance the latency, accuracy, and computational resources required to identify the current reading position. This may be the result of modifying the speech analysis process (e.g., speech recognition) to avoid converting audio data into text (e.g., converting spoken words into text words). The techniques may use a speech analysis process that converts audio into phoneme data using an acoustic model, but may avoid the language step of converting phoneme data into text (e.g., into words) using a language model. Avoiding the language step reduces latency and reduces consumption of computing resources. Performing the phoneme comparison and using fuzzy matching may enhance the accuracy of identifying the current reading location because it may better compensate for non-linear reading of the text source (e.g., skipping, repeating, or adding content). The above-described process may also be beneficial for real-time identification of reading locations, and may facilitate providing physical effects such as audio, tactile, and/or visual effects at or near the appropriate time.
The techniques discussed below include a number of enhancements to computing devices with or without virtual assistant features. This enhancement may be used alone or together to optimize the computing device's ability to follow when reading text sources aloud, and to provide special effects to supplement the listening user's environment. In one example, the environment may include a parent reading a book aloud to one or more children. In another example, the environment may include one or more users providing a presentation, voice, or other performance to an audience. In either example, the technique can be used to enhance the environment with special effects based on an analysis of data associated with the text source. Special effects may be synchronized with specific portions of the text source, such as specific spoken words or flipping pages.
Fig. 1 illustrates an example environment 100 that includes a text source being read aloud and one or more devices that supplement the environment to enhance a user's listening experience, in accordance with one or more aspects of the present disclosure. The environment 100 may be a physical environment such as an indoor setting (e.g., bedroom, meeting room), an outdoor setting (park, field), or other location. The environment 100 may be referred to as a pervasive computing environment or a pervasive computing environment, and may include embedded computing functionality. Embedded computing functionality may provide environmental intelligence that is sensitive to and responsive to the presence of humans. In one example, environment 100 may include one or more users 110A and 110B, text source 120, one or more computing devices 130A and 130B, and one or more physical effects devices 140A-C.
The users 110A and 110B may include human users that are able to perceive the content of the text source. The user 110A may be an individual user who is reading the content of the text sources, or may be multiple users who are each reading a portion of one or more text sources. The user 110A may be referred to as a reader, presenter, announcer, actor, other term, or a combination thereof. User 110B may listen to the content of the text source being read aloud. User 110B may or may not read with user 110A. In one example, the user 110A may be a parent who is reading for the child user 110B. In another example, user 110A may include one or more presenter speaking with one or more users 110B as audience members. In either example, the content of the text source 120 can be announced for listening by one or more other users.
The text source 120 may be any source of content that can be interpreted and read aloud. The text source 120 can include content that includes numbers, characters, words, symbols, images, or combinations thereof. The content may be arranged in a sequence that can be spoken by a user when reading or after storage. The text source 120 may be a physical or electronic book, magazine, presentation, voice, script, memo, announcement, article, blog, post, message, other text arrangement, or a combination thereof. In the example of fig. 1, the text source 120 may be a children's book that includes a series of words and images that can be read aloud to a child.
The sensors 131A-C may be coupled to the computing device 130A and may enable the computing device to sense aspects of the environment 100. The sensors 131A-C may include one or more audio sensors (e.g., microphones), optical sensors (e.g., ambient light sensors, cameras), atmospheric sensors (e.g., thermometers, barometers, gravitometers), motion sensors (e.g., accelerometers, gyroscopes, etc.), location sensors (e.g., global positioning system sensors (GPS)), proximity sensors, other sensing devices, or combinations thereof. In the example shown in fig. 1, sensor 131A may be an audio sensor, sensor 131B may be an optical sensor, and sensor 131C may be a temperature sensor. One or more of the sensors 131A-C may be internal to the computing device 130A, external to the computing device 130A, or a combination thereof, and may be connected via a wired or wireless connection (e.g.,computing device 130A.
The physical effects devices 140A-C may be any computing device capable of causing or providing a physical effect. The physical effect may be perceived via the senses (e.g., hearing, sight, touch, smell, and taste) of the users 110A and 110B. Each of the physical effects devices 140A-C may produce one or more physical effects, and the computing device 130A may function as one or more of the physical effects devices 130A-C. The physical effects devices 140A-C may provide the physical effect 145 or may instruct another device to provide the physical effect 145. In one example, one or more physical effects devices 140A-C may be part of or integrated with a home automation system, or may be separate from the home automation system. As shown in fig. 1, the physical effects device 130A may comprise a speaker or other device capable of causing or emitting an acoustic effect. The physical effects device 130B may include one or more light sources (e.g., light bulbs, pixels) or other devices capable of altering the amount of light present in the environment 100 (e.g., motorized window shades or blinds). Physical effect device 130C may include one or more devices that can cause a haptic effect, and may include a vibration source (e.g., a massage chair), a fan that generates wind (e.g., a ceiling fan or air conditioner), a heating or cooling source (e.g., a thermostat), other devices, or combinations thereof.
The physical effect 145 may be any modification to the environment 100 that a user or computing device may perceive and may include an acoustic effect, a haptic effect, an optical effect, other effects, or a combination thereof. The acoustic effect may be a physical effect related to sound and may be propagated via sound waves. The sound effects may include human or animal sounds (e.g., speech or noise), atmospheric sounds (e.g., sounds of thunder, rain, wind, or other weather), musical sounds (e.g., musical instruments, background music, theme music), object sounds (e.g., knock doors, open doors, close windows, break glass, bump objects, drive a car), other sound effects, or a combination thereof. The haptic effect may be a physical effect related to the sense of touch of the user. The haptic effect may include a breeze, vibration, temperature change, other tactile sensation, or a combination thereof. The optical effect may be a physical effect related to light and may propagate via visible electromagnetic radiation. The optical effect may include an increase or decrease in ambient lighting, a flash, an animation, other changes in the amount of light, or a combination thereof. The optical effect may originate from a lamp (e.g., ceiling lamp, desk lamp), flash lamp (e.g., telephone lamp), curtain (e.g., blinds or curtains), projector, electronic display, holographic display, laser, other light source, or a combination thereof. Other effects may include effects related to smell or taste (e.g., olfactory effects).
The media items may correspond to physical effects, text sources, profile information, voice models, instructions, other data, or combinations thereof. Example media items include, but are not limited to, digital sound effects, digital music, digital animation, social media information, electronic books (e-books), electronic magazines, digital newspapers, digital audio books, digital videos, digital photographs, web site content, electronic periodicals, web blogs, Really Simple Syndication (RSS) feeds, electronic comics, software applications, and so forth. In some implementations, the media items can be referred to as content items and can be provided over the internet and/or via the computing device 130A (e.g., smart speakers). As used herein, "media," "media item," "digital media item," "content," and "content item" may include electronic files or records that may be loaded or executed using software, firmware, or hardware configured to present content to one or more users in the environment 100. In one implementation, computing device 130B may store the media items using one or more data stores and provide the media items to computing device 130A over network 150.
Network 150 may include one or more of a private network (e.g., a Local Area Network (LAN), a public network (e.g., the internet), a Wide Area Network (WAN)), a wired network (e.g., ethernet), a wireless network (e.g., a Wi-Fi or bluetooth connection), a cellular network (e.g., a Long Term Evolution (LTE) network), a router, a hub, a switch, a server computer, and/or combinations thereof.
In general, functions described as being performed by computing device 130A, computing device 130B, or physical effects devices 140A-C in one embodiment may be performed by one or more other devices in other embodiments. In addition, the functionality attributed to a particular component may be performed by different or multiple components operating together. Computing devices 130A and 130B may also be accessed as a service provided to other systems or devices through an appropriate application programming interface. Although embodiments of the present disclosure are discussed in terms of smart speakers, these embodiments may also incorporate one or more features of a cloud service or content sharing platform.
Where the systems discussed herein collect or may utilize personal information about a client device or user, the user may be provided with an opportunity to control whether the computing device is able to collect user information (e.g., information about the user's audio input, the user's preferences, the user's current location, social networks, social behaviors, activities, or professions) or to control whether and/or how more relevant content to the user is received from the computing device. In addition, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized where location information is obtained (such as to a city, zip code, or state level) such that a particular location of the user cannot be determined. Thus, the user may have control over how information is collected about the user and used by the computing device.
Fig. 2-4 depict block diagrams illustrating an example computing device 130 that may detect reading locations within a text source and supplement the environment with physical effects to enhance the listening experience, according to one or more aspects of the present disclosure. Computing device 130 may be the same as or similar to computing device 130A, computing device 130B, or a combination thereof. FIG. 2 discusses features that enable the computing device 130 to receive and compare the user's audio data with data of a text source. Fig. 3 discusses features that enable the computing device 130 to analyze data based on audio data and text source data to detect a reading position. FIG. 4 discusses features that enable the computing device 130 to provide physical effects to modify the environment of one or more listeners. The components and modules provided in fig. 2-4 are exemplary, and more or fewer components or modules may be included without loss of generality. For example, two or more components may be combined into a single component, or a feature of a component may be divided into two or more components. In one implementation, one or more components may reside on different computing devices (e.g., a client device and a server device).
Referring to fig. 2, the computing device 130 may include an audio analysis component 132, a text source analysis component 133, a comparison component 134, and a data store 240. The audio analysis component 132 may receive and access audio data extracted from the environment while the user reads the text source aloud. In one example, the audio analysis component 132 may include an audio data receiving module 212 and an acoustic modeling module 214.
The audio data reception module 212 may receive audio data 241 including one or more audible actions of the user. The audio data may include spoken words, pages, or other audible actions captured from the user environment. The audio data 241 may be received directly from one or more sensors in the form of audio signals, or may be received indirectly from the data store 240 or other computing device after the sensors store the audio data 241. The audio data 241 may be in any digital or analog format and may be accessed or received from one or more storage objects (e.g., files, database records), data streams (e.g., audio streams, video streams), data signals, other data transmission or storage protocols, or a combination thereof. The audio data 241 may be an audio recording and may be segmented into one or more durations (e.g., portions, blocks, or other units) before, during, or after analysis by the acoustic modeling module 214.
The acoustic modeling module 214 may analyze the audio data 241 using acoustic models to identify phoneme data 243A. The acoustic model may represent a known relationship between audible action and phonemes. A phoneme may be a unit of sound and may correspond to a sound pattern of an audible action (e.g., a spoken word). The phonemes may be linguistic units, non-linguistic units, other units, or combinations thereof. The acoustic modeling module 214 may convert the audio data into phonemes, which are stored as phoneme data 243A in the data storage 240.
The phoneme data 243A may include values representing one or more phonemes extracted from the audio data 241. The phoneme data 243A may represent a series of phonemes using standard or proprietary tokens. The token may include a particular arrangement of one or more bits, bytes, symbols, or characters representing a phoneme. In one example, the particular arrangement may include symbols placed next to or between one or more delimiters. The separators may include slashes, brackets, vertical lines, brackets, commas, tabs, spaces, linens, other separators, or combinations thereof. The phonemes may be arranged in a series of phonemes representing a portion of one or more audible actions.
The text source analysis component 133 can receive and analyze data related to the text source 120. The text source 120 may be determined in view of user input that is text-based, voice-based, touch-based, gesture-based, or otherwise. For example, the user may identify the text source 120 by speaking the name of the text source 120 (e.g., the title or author of the book), by typing and searching the text source, by selecting a displayed text source, other selection mechanisms, or a combination thereof. In the example shown in fig. 2, the text source analysis component 133 can include a data access module 222 and a phoneme determination module 224.
Text source data 242 can be any data associated with text source 120 and can be provided by or accessed from an author, an issuer, a distributor, a partner, a remote server, a third party service, other sources, or a combination thereof. The text source data 242 may include descriptive data, textual data, phonological data, other data, or a combination thereof. The descriptive data may indicate a title, abstract, source (e.g., author, publisher, distributor), catalog (e.g., section, page), index (e.g., phrase, page indicator), other data, or a combination thereof.
The text data may include one or more words of the text source 120. In one example, the words may be organized into a sequence of words 122 with or without one or more images 124. The text data may be a data structure in which words (e.g., a series of consecutive words) are arranged in the same or similar manner as the user reads them. The word sequence may be limited to only words that appear in the text source 120, or may be supplemented with words or data (e.g., illustrations, images, tables, formatting, paragraphs, pages) that indicate the presence or content of non-textual information. In another example, words may also or alternatively be arranged in an index data structure that indicates unique words that are present in the text source 120 but are not arranged consecutively in the manner spoken by the user. Any of the data structures may be supplemented with other information, which may include word locations within the text source (e.g., pages, lines, slides), times of occurrence, variations of words (e.g., tenses, multiples), other data, or combinations thereof. In one example, the text source 120 can be a physical book, and the text source data 242 can include words from a corresponding electronic book (e.g., an e-book), a third party service, other sources, or a combination thereof.
The phonological data of the text source 120 may be the same as or similar to the phonological data 243B and may be a phoneme encoding of the text source 120 in a format that is the same as or similar to phonological data derived from audio (e.g., the phonological data 243A). In the example discussed above, the phoneme data 243B of the text source 120 may be included as part of the text source data 242 and accessed by the phoneme determination module 224. In another example, the text source data 242 may not have phoneme data 243B and may be generated by the phoneme determination module 224.
The phoneme determination module 224 may determine the phoneme data for a particular text source 120. This may involve the phoneme determination module 224 accessing existing phoneme data 243B from a remote source, generating phoneme data 243B based on the text data, or a combination thereof. When generating the phoneme data 243B, the phoneme determination module 224 may access and analyze the text data of the text source data 242 and convert (e.g., derive, convert, transform, encode) the text data into the phoneme data 243B. The generated phonological data may then be associated with the text source 120 for future use by the computing device 130 or one or more other computing devices. In one example, the text data may include a sequence of words and the generated phonological data may include a phoneme encoding including a sequence of phoneme values representing the sequence of words. The same sequence of phoneme values may correspond to two words (e.g., homophones) that sound the same but are differently spelled. Likewise, different sequences of phoneme values may correspond to words that sound different (e.g., homomorphic heteronyms) even though the spelling is the same.
As described above, the phoneme data 243A and 243B may each include a phoneme sequence represented using a standard or proprietary notation. The tokens may be referred to as phonemic transcription (phonemic transcription) or phonemic transcription (phonemic transcription) and may include a particular arrangement of phoneme values representing the language segment. A language fragment may be any discrete unit that may be physically or audibly identified in a voice stream. The tone value may comprise one or more symbols, characters, bytes, bits, other values, or combinations thereof. In one example, the phoneme values may be represented by one or more Unicode characters, American Standard Code for Information Interchange (ASCII) characters, other characters, or a combination thereof. A sequence of phoneme values may represent a single word and each individual phoneme value may represent a portion of the word. For example, the first phoneme sequence may be/θΛm/, and represents the spoken word "thumb", while the second phoneme sequence may be/dΛm/, and represents the spoken word "dumb". In the examples discussed below, the phoneme data 243A and 243B may include a sequence of values, and each value may represent a phoneme of the phoneme vocabulary.
The phonemic vocabulary may include a collection of possible phonemic values for one or more languages. The phonemic vocabulary may be a phonetic alphabet system and may represent the speech quality of a portion of spoken language: phonemes, intonations, and word and syllable separations. PhonemeThe vocabulary may or may not represent the additional quality of the voice and variations in the voice prompt (e.g., tongue-biting, misreading, accent, dialect). The phonemic vocabulary may be the same or similar to phonemic letters, character sets, vocabularies, dictionaries, other variants, or combinations thereof. In one example, the phonemic vocabulary may be based on international phonetic symbols (IPA). The IPA symbol may be composed of one or more elements related to letters and diacritics. For example, English letters<t>May use a single letter t in IPA]Or by letters plus diacritical marks
The comparison component 134 can compare the audio of the user 110A with the content of the text source 120. The examples discussed below use phoneme data corresponding to audio and text sources, and compare the audio and text sources without converting the audio to text using speech recognition. Other examples may also or alternatively use textual data, descriptive data, audio data, other data, or a combination thereof. The comparison may be performed by the computing device 130, by a remote computing device (e.g., a cloud service), or a combination thereof. In one example, the comparison component 134 may select a sequence of phonemes derived from audio and compare it to a plurality of sequences of phonemes derived from a text source. In another example, the comparison component 134 may compare the sequence of phonemes of the text source to a plurality of sequences of phonemes derived from audio. In either example, the calculation of the similarity measure data may be based on the phoneme edit distance.
Phoneme edit distance module 232 may quantify the similarity of two phoneme sequences to each other by determining the minimum number of operations required to convert one phoneme sequence to an exact match of another phoneme sequence. The operation may include any modification of a phoneme value (e.g., symbol) within one of the phoneme sequences. Example operations may include original operations, such as phoneme removal, insertion, replacement, transposition, other operations, or a combination thereof. In the example discussed above, the first sequence of phonemes may be/θΛm/and represents "thumb" and the second sequence of phonemes may be/dΛm/and represents "dumb". Although two words differ by two letters, their phonological edit distance is a numerical value of 1, since converting a sequence to an exact match would involve the replacement of a single phonological (e.g., replacing θ with d). In one example, the phonological edit distance may be a linear edit distance that is the same as or similar to a Levenshtein (Levenshtein) distance. The Levenshtein distance may be based on the minimum number of removal, insertion, or replacement operations required to make two phoneme sequences equal. In other examples, the phoneme edit distance may also or alternatively include a transposition or other operation. In either example, the phoneme edit distance may be a numerical value used to determine the similarity measure data 244.
The similarity measurement module 234 may access the data of the phonological edit distance module to determine a degree of similarity or dissimilarity between the audio and text sources. The similarity measurement module 234 may analyze the phoneme edit distance module data to calculate similarity measurement data 244. The similarity measurement data 244 may represent the similarity between two or more phoneme sequences (e.g., phonemic representations of words or word sets) and may include numerical data, non-numerical data, other data, or combinations thereof. The similarity measurement data 244 may be based on edit distances of one or more phoneme sequences. In one example, the similarity measurement data 244 may include a numerical value of the phonological edit distance. In another example, the similarity measure data 244 may include probability values derived from numerical values of phoneme edit distances. For example, the similarity measure data may be a percentage, ratio, or other value based on one or more phoneme edit distances and one or more other values. Other values may be the number of phonemes in one or more of the sequence of phonemes or portions of the text source.
The data store 240 can be a memory (e.g., random access memory), a cache, a drive (e.g., a solid state drive, a hard drive, a flash drive), a database system, or another type of component or device capable of storing data. The data store 240 can also include multiple storage components (e.g., multiple drives or multiple databases) that can span one or more computing devices (e.g., multiple server computers).
Fig. 3 depicts a block diagram illustrating example components that enable the computing device 130 to analyze the data discussed above to determine a reading location or the absence of a reading location within a text source. As discussed above, portions of audio may not exactly match the text source, as the user may add, skip, repeat, or reorder the content of the text source while reading aloud. As a result, the phoneme data derived from the audio and the phoneme data derived from the text source may be challenging to compare and align. In the example shown in fig. 1, the computing device 130 can include a non-linear reading recognition component 135 that enables the computing device to determine a location within the text source that best aligns with the audio data. In one example, the non-linear reading recognition component 135 can include a fuzzy matching module 352, a location identification module 354, a reading speed module 356, and a reading interruption module 358.
The fuzzy matching module 352 may enable the computing device 130 to determine whether a match exists between the audio and text sources. The match may be the same as or similar to a probabilistic match, a best match, a closest match, or any match that may not be an exact match but satisfies a predetermined threshold. In one example, determining a match between audio and a text source may involve detecting that an audio snippet includes one or more words of the text source. A match may be detected even if the audio or text source contains other words, missing words, or variants of the contained words (e.g., mispronunciations, missing plural forms). This match may be referred to as a fuzzy match or an approximate match and may be detected using fuzzy match logic. Fuzzy matching logic may be used to compare sequences of phoneme values and may operate under syllable level segments, word level segments, phrase level segments, sentence level segments, other segments, or combinations thereof. In one example, fuzzy matching may be performed using an audio segment having a predetermined length. The predetermined length may be customizable and may be any duration (e.g., 3+ seconds) or any number of word tokens (e.g., 3-4 words). Having a predetermined length that is much smaller than the length of the text source may enhance accuracy and performance when considering non-linear reading.
The fuzzy matching module 352 may impose one or more constraints to determine a match. In one example, detecting a match may involve using one or more global unweighted costs. The global unweighted cost may be related to the total number of raw operations required to convert a candidate sequence of phonemes (e.g., candidate patterns from a text source) to a selected sequence of phonemes (e.g., patterns from audio). In another example, detecting a match may involve specifying each type of operand separately, while other operations set the total cost, but allow different weights to be assigned to different original operations. The fuzzy matching module 352 may also apply separate assignments of constraints and weights to individual phoneme values in the sequence.
The location identification module 354 may access data of the fuzzy matching module 352 to identify locations within the text source that correspond to audible actions of audio (e.g., spoken words). In one example, the text source may be a children's book, and the location may be a reading location within a word sequence of the book. In other examples, the location may be in a voice, presentation, script, other text source, or a combination thereof. In either example, the location may be a past, current, or future reading location within the text source, and may be stored as location data 345. The location data may be digital or non-digital data identifying one or more particular phonemes, words, paragraphs, pages, segments, chapters, tables, images, slides, other locations, or combinations thereof.
The location identification module 354 may determine that the audible action matches a plurality of different portions of the text source. This may occur when the same word or phrase (e.g., a sequence of phonemes) is repeated multiple times in the text source. The location identification module 354 may detect the spoken word by analyzing the phonological data and detect that the spoken word matches a plurality of candidate locations within the text source. The location identification module 354 may select one or more of the plurality of candidate locations based on the data of the fuzzy matching module 352. The location identification module 354 may further narrow down the candidate locations (e.g., expand a predetermined segment length or use adjacent segments) by selecting particular locations based on the phonological data of the audio before, during, or after the occurrence of the spoken word.
The reading speed module 356 may access and analyze the location data 345 to determine the reading speed of the user. Reading speed data may be determined in view of location data, text source data, audio data, other data, or a combination thereof, and may be stored as reading speed data 346. The reading speed may be based on a portion of the location data 345 that identifies at least two locations in the text source. The location may correspond to a particular time, and determining the reading speed may be based on the amount of words and the amount of time between two or more locations. In one example, the amount of words may be based on the content of the text source and may not take into account content that the user added, skipped, or repeated. In another example, the amount of words may be based on the content of the text source and also based on the content of the audio. This may be advantageous because the content of the audio may indicate that words are added, skipped, repeated, other actions, or a combination thereof. In either example, the reading speed module 356 may update the reading speed data to represent the reading speed of the user for one or more durations.
The reading interruption module 358 may access and analyze any of the data discussed above to detect whether the user has interrupted reading the text source or is still reading the text source. This can be challenging because the user may have stopped reading the text source, but is discussing concepts related to the text source. As a result, there may be overlap in the content of the spoken words and the text source. Detecting interruptions in reading can be important because it can enable the computing device to avoid recording private discussions. The reading interruption module 358 may determine whether the user has interrupted reading the text source by calculating one or more corresponding measurements.
The correspondence measure may indicate a degree of similarity or dissimilarity between the audio segment and the corresponding portion of the text source. The correspondence measure may be a probability value indicative of a probability that the audio segment corresponds to the location of the text source. The probability values may be numeric or non-numeric values, and may be the same or similar to percentages, ratios, decimal or other values, or combinations thereof. In one example, the value may be between 0 and 1 (e.g., 0.97), 0 and 100 (e.g., 98), or other range of values. One end of the range may indicate that the segment of audio absolutely corresponds to the location of the text source (e.g., 1.0 or 100), while another range may indicate that the segment of audio absolutely does not correspond to the location of the text source (e.g., a value of 0).
The correspondence measure may be based on or related to a plurality of similarity measures. For example, both measurements may be used to compare or contrast data derived from audio (e.g., the phonological data 243A) with data derived from a text source (e.g., the phonological data 243B). A similarity measure (e.g., a phonological edit distance) may be used to compare or contrast the written words of the text source with the spoken words, while a corresponding measure may be used to compare or contrast the set of written words with the set of spoken words over a duration of time. The duration of the audio (e.g., segment) may be any length of time, and may include a collection of words and one or more other audible actions (e.g., page turn, book close). In one example, the audio of the user may include a first duration and a second duration, and the reading interruption module 358 may calculate one or more corresponding measurements for the first duration and one or more corresponding measurements for the second duration. The corresponding measurements may be stored as corresponding measurement data 347. In other examples, the corresponding measure may also or alternatively take into account one or more signals, such as the absence of voice input for a duration, the recognition of story text, or the absence of recognition of a particular word or phrase that may indicate a stop. These words or phrases may include "let's stop reading", "let's finish tomorrow", "OK, I'm done", "let's pause", other phrases or combinations thereof.
The reading interruption module 358 may compare the corresponding measurement data 347 for each duration to one or more predetermined thresholds. In response to the corresponding measurement data 347 for the first duration not satisfying the threshold (e.g., above or below the threshold), the reading interruption module 358 may determine that the duration of the audio corresponds to the text source and the user audio data corresponds to the user reading the text source. In response to the corresponding measurement data 347 within the second duration satisfying a threshold (e.g., below or above a threshold), the reading interruption may determine that the duration of the audio does not correspond to the text source and that the user has stopped reading the text source. In one example, determining that the correspondence measure satisfies the threshold may indicate that the audio data does not match the data of the text source or that the audio data is different from the content of the text source.
The reading interruption module 358 may perform one or more actions in response to determining that the user has interrupted reading the text source. In one example, the reading interruption module 358 may transmit a signal to deactivate one or more microphones associated with the computing device to avoid capturing or recording additional audio data. In another example, the reading interruption module 358 may transmit a signal to stop analyzing the audio data (e.g., comparing the audio data to data of a text source). The latter example may record audio but not access or analyze the audio data. In yet another example, the reading interruption module may cause the computing device 130 to interact with the user before, during, or after transmitting the signal. For example, the computing device may interact with the user by providing prompts (e.g., audio, visual, or a combination thereof). The prompt may ask the user whether to exit the story time mode or may inform the user that the story time mode has exited and may or may not enable the user to re-enable the story time mode.
FIG. 4 depicts a block diagram illustrating exemplary components that enable the computing device 130 to provide physical effects to enhance the user's experience. As discussed above, the physical effect may modify the environment and may include an acoustic effect, a haptic effect, an optical effect, other effects, or a combination thereof. In the illustrated example, the computing device 130 may include a physical effects determination component 136, a predictive loading component 137, and an effects provision component 138.
The physical effect determination component 136 enables the computing device 130 to identify and provide physical effects corresponding to particular portions of the text source. In one example, physical effectiveness determination component 136 may include an audible action correlation module 462, a contextual data module 464, and an effectiveness selection module 466.
The audible action correlation module 462 may enable the computing device to correlate a particular physical effect with a particular audible action associated with the text source. The audible action correlation module 462 can determine a correlation based on the performance data 448 of the text source. The effects data 448 can indicate which physical effects correspond to which portions of the text source. The effects data 448 can relate a particular physical effect to a particular location in a text source, a particular audible action of a user, a particular trigger condition (discussed below), or a combination thereof. The location in the text source may be related to an audible action (e.g., a spoken word or a page turn) or unrelated to an audible action (e.g., a user viewing a graphical image). In one example, the effects data 448 can identify an audible action of a particular spoken word (e.g., a dog) that includes a text source, and the physical effect can involve initiating an acoustic effect (e.g., a barking sound) corresponding to the spoken word. In another example, the effects data 448 may identify an audible action (e.g., turning a page) and the physical effect may involve modifying an existing physical effect (e.g., readjusting ambient sound, light, or temperature).
The effects data 448 may be accessed by the computing device 130 or may be created by the computing device 130. In one example, the computing device 130 may access or receive the effects data directly or indirectly from an author, a publisher, a distributor, a partner, a third party service, other source, or a combination thereof. Effects data 448 may be included within text source data 242 or may be separate from text source data 242. In another example, computing device 130 may create effect data based on text source data 242. For example, the audible action correlation module 462 may analyze the text data or phonological data and identify a physical effect corresponding to a particular portion of the text source. In either example, the effects data 448 may be stored in the data store 240 for enhanced access by the computing device 130.
The effect selection module 466 may enable the computing device 130 to select and modify physical effects based on the effect data 448, contextual data 449, text source data 242, other data, or a combination thereof. The effect selection module 466 may be used to select a particular physical effect (e.g., an acoustic effect) or modify a property of a physical effect. The attribute may relate to the intensity, timing, tone, transition (e.g., fade in/out), other characteristics of the physical effect, or a combination thereof. The intensity may be related to the magnitude of the modification to the environment, and may be related to the volume (e.g., loudness) or luminance (e.g., brightness) of the physical effect. The timing may be related to the speed or duration of the physical effect. Computing device 130 may select a physical effect based on words of the text source and may update properties of the physical effect based on the context data. In one example, the contextual data may include sound data of the user's environment, and the physical effect may be an amount of acoustic effect based on the sound data. In another example, the contextual data may include light data of the user's environment, and the physical effect may be an optical effect that modifies the luminance of the light source (e.g., dims or brightens the light) based on the light data. In yet another example, the contextual data may include user profile data for a parent or child and indicate the age of the audience, and wherein the physical effects include acoustic effects selected based on the age of the user (e.g., more interesting dog barking for young children and more severe dog barking for older children).
The effect selection module 466 can use the context data to identify timing aspects related to reading of the text source. For example, time data or calendar data may be used to distinguish between text sources reading in the evening or reading in the morning. At night, the effect selection module 466 may select a more calming (e.g., less stimulating) physical effect to encourage the listener to prepare to go to bed. This may involve reducing the brightness and volume settings for sound and optical effects and/or selecting effects with lower pitch (e.g., softer impact effects or whisper as opposed to yelling). In the morning, the effect selection module 466 may select a more stimulating physical effect to encourage the user to prepare for the day. This may involve increasing the brightness and volume settings for the acoustic and optical effects. The calendar data may also indicate whether the reading time is associated with a weekend or a work day, or whether an appointment is coming (e.g., late in the day or early in the morning of the next day). Any of these may affect how quickly the user can read the text source and how long or how long the physical effect should be provided.
The predictive loading component 137 may enable the computing device 130 to predictively load content for a physical effect in advance of the content being desired. Predictive loading may accelerate the ability of the computing device 130 to provide a physical effect by loading the content of the physical effect before the physical effect is initiated. The speculative load may be the same or similar to prefetching, pre-caching, cache prefetching, other concepts, or a combination thereof. In one example, the predictive loading component 137 may include a prediction module 472, a trigger determination module 474, and a content loading module 476.
The prediction module 472 can enable the computing device 130 to predict a time at which a user will arrive at a particular portion of the text source. For example, the prediction module 472 may determine a time at which words of the text source will be spoken before the user speaks the words. The predicted time may be a future time and may be determined based on a reading speed of the user, a reading location of the text source, other data, or a combination thereof. In one example, the time may be calculated based on the user's reading speed (e.g., words per minute, pages per minute) and the difference between the current reading position and the target position in the text source (e.g., the number of words, paragraphs, or pages). In other examples, the prediction module 472 may use predictive models, machine learning, neural networks, or other techniques to enhance the prediction based on current data, historical data, or a combination thereof.
The determination of the trigger condition may be based on one or more factors related to the content, the computing device, the user, other aspects of the environment, or a combination thereof. Factors related to the content may include the amount of content (e.g., 1MB file size), the location of the content (e.g., remote storage), the format of the content (e.g., downloadable files, streaming blocks, or formats that require transcoding), the duration of the content (e.g., 2 second sound effect), other aspects of the content, or a combination thereof. The factors related to the computing device may correspond to the amount and/or availability of computing resources of the computing device 130 or other computing devices. The computing resources may include connection speed (e.g., networking bandwidth), storage space (e.g., available solid state storage), processing power (e.g., CPU speed or load), other computing resources, or a combination thereof. The factors related to the user may include the user's reading speed, current reading position, speech intelligibility, other aspects, or a combination thereof.
The trigger determination module 474 may use one or more factors to calculate the duration of time to load or provide the content of the physical effect. The duration associated with loading the content may be referred to as a predicted load time and may or may not include the duration of providing (e.g., playing) the content. In one example, the trigger determination module 474 may determine the duration of time to load the content of the physical effect based on the size of the content and the network bandwidth of the computing device 130. The trigger determination module 474 may use the predicted load time to identify a particular time or location of the trigger condition. In one example, the trigger condition may be set to a time greater than or equal to the predicted time of the audible action (e.g., spoken word) minus a predetermined load time (e.g., 5 seconds). In another example, the trigger condition may be set to a position within the text source that is equal to or prior to the position at which the physical effect is expected to align. This may involve selecting a location in the text source based on the predicted load time and reading speed. For example, if the user reads at a rate of 120 words per minute (i.e., 2 words per second) and the predicted load time is 5 seconds, the trigger position may be 10 or more words before the word with which the physical effect should align.
The effect providing component 138 may enable the computing device 130 to provide physical effects to modify the user's environment. The effect providing component 138 may be initiated after loading content for a physical effect and may be timed such that the physical effect is provided at a time aligned with an audible action intended to be aligned therewith. In one example, effects providing component 138 may include an instruction access module 482 and a physical effects initiation module 484.
Physical effect initiation module 484 can access instruction data and execute the instruction data to initiate a physical effect. Physical effect initiation module 484 can initiate instructions before, during, or after detecting an initiation trigger condition (e.g., an audible action) corresponding to a physical effect. In one example, the text source may include particular words, and initiating the physical effect may be in response to detecting that the audio data includes words (e.g., matching phonemes). In another example, physical effect initiation module 484 may determine an initiation trigger condition to initiate a physical effect. The process of determining the trigger condition for initiating the physical effect may be the same as or similar to the trigger condition for initiating the loading of the content of the physical effect. The instructions may cause the computing device 130 to provide a physical effect or may cause the computing device 130 to communicate with one or more physical effect devices to provide a physical effect. In either example, the computing device 130 may cause the physical effect to modify the user's environment, either directly or indirectly, to enhance the listening user's experience.
In one example, the physical effect initiation module 484 or the effect selection module 466 may select and/or initiate a physical effect using one or more confidence thresholds. The one or more confidence thresholds may be grouped into one or more confidence intervals that categorize a probability that the audio matches a particular location of the text source (e.g., that a spoken word matches a word of the text source). There may be any number of confidence intervals, and the first confidence interval may indicate that there is a low probability (e.g., > 50%) that audio matches the text source location, and each successive confidence interval may be higher (e.g., > 75%, > 95%, etc.). The effect data relating the physical effect to the location may also include a particular confidence threshold (e.g., a minimum confidence interval). For example, providing a sound effect may be associated with a higher confidence interval and then transitioning a background effect. The computing device 130 may determine whether a confidence threshold is met before selecting or initiating the physical effect. This may involve comparing corresponding measurement data, similarity measurement data, other data associated with fuzzy matches, or a combination thereof. In one example, a particular location in the text source may be associated with a plurality of different physical effects, and each may correspond to a different confidence interval associated with the current reading location. When the confidence interval is high (e.g., trusting that the current reading position is accurate), a particular sound effect may be initiated (e.g., the sound effect of a single dog barking at a higher volume), while when the confidence interval is low (e.g., without determining whether the current reading position is accurate), a different sound effect may be initiated (e.g., background noise of multiple dogs barking at a lower volume).
Fig. 5-8 depict flowcharts of respective methods 500, 600, 700, and 800, the methods 500, 600, 700, and 800 for enhancing the ability of a computing device to follow and provide special effects in real-time while reading text sources aloud, in accordance with one or more aspects of the present disclosure. The method 500 may involve estimating reading progress using phoneme data and fuzzy matching. The method 600 may optimize the ability of a computing device to detect when a user has stopped reading from a text source and is conducting private discussions. The method 700 may enable a computing device to provide a physical effect that takes into account a user's context and the user's environment. The method 800 may enable a computing device to pre-cache content of a physical effect to reduce latency and better synchronize the physical effect with audible actions associated with a text source.
Each of the methods of fig. 5-8 and individual functions, routines, subroutines, or operations thereof may be performed by one or more processors of a computer device executing the method. In some implementations, one or more of the methods may be performed by a single computing device. Alternatively, one or more of the methods may be performed by two or more computing devices, each computing device performing one or more separate functions, routines, subroutines, or operations of the methods. For simplicity of explanation, the methodologies of the present disclosure are depicted and described as a series of acts. However, acts in accordance with the present disclosure may occur in various orders and/or concurrently, and with other acts not presented and described herein. Moreover, not all illustrated acts may be required to implement a methodology in accordance with the disclosed subject matter. In addition, those skilled in the art will understand and appreciate that a methodology could alternatively be represented as a series of interrelated states via a state diagram or events. Additionally, it should be appreciated that the methodologies disclosed in this specification are capable of being stored on an article of manufacture to facilitate transporting and transferring such methodologies to computing devices. The term "article of manufacture" as used herein is intended to encompass a computer program accessible from any computer-readable device or storage media. In one embodiment, the method may be performed by one or more of the components in fig. 1-4.
Referring to fig. 5, the method 500 may be performed by a processing device of a client device (e.g., smart speakers), a server device (e.g., cloud services), other device, or a combination thereof, and may begin at block 502. At block 502, the processing device may determine phoneme data for a text source. The text source may include a sequence of words and the phoneme data may be a phoneme encoding of the sequence of words, the phoneme encoding including one or more sequences of phoneme values. Each phoneme value may correspond to a phoneme, and the sequence of phonemes may correspond to a spoken word. The same sequence of phoneme values may correspond to words that sound the same but that are spelled differently (e.g., homophones), while a different sequence of phoneme values may correspond to words that are spelled the same but that sound different (e.g., homophones).
The processing device may access the phonological data from a source of the text source or may generate the phonological data for the text source. The processing device may generate phoneme data by phoneme coding the word sequence. This may involve accessing text data of a text source and generating (e.g., converting, transforming, deriving) phoneme data based on the text data. The phoneme data may then be associated with phoneme data for future use.
At block 504, the processing device may receive audio data including spoken words associated with a text source. The audio data may include one or more audible actions of the user, and may include spoken words, flipping pages, or other audible actions captured from the user's environment. In one example, the processing device may receive audio data in the form of audio signals directly from one or more sensors. In another example, the processing device may receive audio data from a data store or another computing device. The audio data may be in any digital or analog format and may be accessed or received via one or more storage objects (e.g., files, database records), data streams (e.g., audio streams, video streams), data signals, other data transmission or storage protocols, or a combination thereof.
At block 506, the processing device may compare the phoneme data of the text source with the phoneme data of the audio data. The comparison of the audio data and the text source may occur without converting the audio data to text (e.g., recognized words) using speech recognition, and may involve comparing the phonological data corresponding to the audio data to the phonological data corresponding to the text source. The comparison may include calculating a numerical value representing a degree of similarity between the two or more sequences of phoneme values. The numerical value may be a phonological edit distance between the phonological data of the audio data and the phonological data of the text source. The comparison may also involve performing a fuzzy match between the phonological data corresponding to the audio data and the phonological data of the text source.
At block 508, the processing device may identify a location in the word sequence based on a comparison of the phoneme data of the text source and the phoneme data of the audio. The identification of the location may involve determining that the spoken word matches a word in a sequence of words of the text source. In one example, the text source may be a book and the location may be a current reading location in the book. The method may terminate (terminate) in response to completing the operations described above with reference to block 508.
Referring to fig. 6, method 600 may be performed by the same processing device as discussed above or a different processing device, and may begin at block 602. At block 602, the processing device may receive audio data including spoken words associated with a text source. The audio data may be segmented (e.g., tokenized, sliced, split, partitioned) into a first duration and a second duration. In one example, the text source may be a book, and the first portion of the audio data may correspond to content of the book (e.g., including spoken words of the book), while the second portion of the audio data may not correspond to content of the book (e.g., there may be no spoken words from the book).
At block 604, the processing device may compare the audio data to data of the text source. The data of the text source may include phoneme data, and comparing the audio data and the data of the text source may involve phoneme comparison. In one example, comparing the phoneme data may involve calculating a phoneme edit distance between the phoneme data of the text source and the phoneme data of the audio data.
At block 606, the processing device may calculate a correspondence measure between the second duration of the audio data and the data of the text source. Calculating the corresponding measure may include calculating the corresponding measure based on the plurality of phonological edit distances. In one example, the processing device may select a set of spoken words (e.g., 3, 4, 5+ words) and compare the set of spoken words to the content of the text source. The phonological edit distance may be determined for each word or combination of one or more words in the collection. The resulting values may then be weighted, aggregated, or modified to determine corresponding measurements.
At block 608, in response to determining that the corresponding measure satisfies the threshold, the processing device may transmit a signal to stop comparing the audio data to the data of the text source. Determining that the corresponding measurement meets the threshold may involve determining that the corresponding measurement is below or above the threshold. The determination may also be based on a duration of time for which the corresponding measurement meets or does not meet the threshold. Determining that the corresponding measure satisfies the threshold may indicate that the second duration of the audio data includes content that is different from the content of the text source and may or may not indicate that the audio data does not match the data of the text source. Transmitting the signal may involve transmitting a signal that deactivates one or more microphones that capture audio data. In one example, in response to determining that the second duration of audio data does not exist for the content of the text source, the processing device may cause the computing device to prompt the user to exit the story time mode. The prompt may be an audio prompt, a visual prompt, other prompt, or a combination thereof. In response to completing the operations described above with reference to block 608, the method may terminate.
Referring to fig. 7, method 700 may be performed by the same processing device as discussed above or a different processing device, and may begin at block 702. At block 702, the processing device may receive audio data including spoken words of a user. The spoken words may be associated with a text source that the user is reading aloud, and may include one or more other audible actions, such as turning pages, spoken words not within the text source, and other audible actions captured from the user's environment. In one example, the processing device may receive audio data directly from one or more sensors in the form of audio signals (e.g., for real-time use or real-time perception). In another example, the processing device may receive audio data from a data store or another computing device. The audio data may be in any digital or analog format and may be accessed or received from one or more storage objects (e.g., files, database records), data streams (e.g., audio streams, video streams), data signals, other data transmission or storage protocols, or a combination thereof.
At block 704, the processing device may analyze context data associated with the user. The contextual data may include sound data, light data, time data, weather data, calendar data, user profile data, other data, or a combination thereof. In some examples, the context data may be associated with a physical effect such that the processing device may provide the physical effect taking into account the context of the user and the environment of the user. In one example, the contextual data may include sound data of the user environment, and the physical effect may include an acoustic effect at a volume based on the sound data. In another example, the contextual data may include light data of the user environment, and the physical effect may include an optical effect that modifies a luminance of the light source based on the light data. In yet another example, the contextual data may include user profile data indicating an age of the child, and the physical effect may include an acoustic effect selected based on the age of the child.
At block 706, the processing device may determine a match between the audio data and the data of the text source. The processing device may identify the text source based on user input (e.g., audio data or touch data) and retrieve data of the text source. The data of the text source may include phoneme data, and determining a match may involve calculating a phoneme edit distance between the phoneme data of the text source and the phoneme data of the audio data. In one example, determining a match between the audio data and the data of the text source may involve using the phonological data of the text source to detect the audio data that includes words of the text source.
At block 708, the processing device may initiate a physical effect in response to determining a match. The physical effect may correspond to a text source and be based on the context data. The physical effect may modify the user's environment and may include at least one of an acoustic effect, an optical effect, and a haptic effect. The text source may include words, and initiating the physical effect may be in response to detecting that the audio data includes words. In one example, the processing device may select a physical effect based on words of the text source, and may update a property (e.g., volume or brightness) of the physical effect based on the context data. In response to completing the operations described above with reference to block 708, the method may terminate.
Referring to fig. 8, method 800 may be performed by a processing device of a server device or a client device and may begin at block 802. At block 802, the processing device may identify performance data for a text source, where the performance data relates a physical effect to an audible action of a user. The performance data may indicate a physical performance and indicate a location in the text source related to the audible action. The location may correspond to a word, paragraph, page, or other location of the text source. In one example, the audible action may be a spoken word of the text source, and the physical effect may be an acoustic effect corresponding to the spoken word. In another example, the audible action may include turning a page, and the physical effect may be a modification to an existing acoustic effect, optical effect, or haptic effect.
At block 804, the processing device may receive audio data comprising a plurality of audible actions. The plurality of audible actions may include one or more spoken words of the text source and one or more other audible actions, such as turning pages, spoken words not within the text source, and other audible actions captured from the user's environment. In one example, the processing device may receive audio data directly from one or more sensors in the form of audio signals (e.g., for real-time use or near/perceived real-time). In another example, the processing device may receive audio data from a data store or another computing device. The audio data may be in any digital or analog format and may be accessed or received from one or more storage objects (e.g., files, database records), data streams (e.g., audio streams, video streams), data signals, other data transmission or storage protocols, or a combination thereof.
At block 806, the processing device may determine a trigger condition based on the effect data and the text source. In one example, determining the trigger condition may involve determining a physical effect associated with a first location in the text source and selecting a second location in the text source that precedes the first location. The selection may be based on the reading speed and the load time associated with the physical effect, and the second location may be associated with at least one of a particular instance of a word, paragraph, page, or chapter of the text source. The processing device may then set the trigger condition to correspond to the second location in the text source. In another example, determining the trigger condition may involve calculating a duration of loading the content based on an amount of content and an amount of available computing resources of the physical effect. The computing resources may be related to one or more of networking bandwidth, storage space, or processing power, and the duration may be longer when the available computing resources are lower. In one example, determining a future time at which the audible action will occur may involve identifying a time at which to initiate loading based on the calculated duration and the determined time of the audible action, and initiating loading of the content at or before the identified time. In another example, determining the time includes calculating a future time based on the reading speed and a current reading position in the text source. In yet another example, determining the time includes predicting a time at which a word of the text source will be spoken before the word is spoken.
At block 808, the processing device may load content for the physical effect in response to the trigger condition being satisfied. The trigger condition may be satisfied before the audible action occurs.
At block 810, the processing device may provide a physical effect to modify the user's environment. In response to completing the operations described above with reference to block 810, the method may terminate.
The techniques discussed herein include various enhancements to computing devices with or without virtual assistant features. The following discussion includes a number of different enhancements that can be used separately or together to optimize the computing device's ability to follow when text sources are read aloud, and to provide special effects to supplement the user's environment. In one example, the environment may include a parent reading a book aloud to one or more children. In another example, the environment may include one or more users providing a presentation, voice, or other performance to an audience. In either example, the technique can be used to enhance the environment with special effects based on an analysis of data associated with the text source. Special effects may be synchronized with specific portions of the text source, such as specific spoken words or flipping pages.
In a first example, the enhancement may relate to reading progress estimation based on phoneme fuzzy matching and confidence intervals, and may relate to the field of computer-based human speech recognition, and in particular to enhancing the ability of the computer device to identify reading locations in text sources when the user reads them aloud. A number of technical problems arise when a computing device attempts to use a traditional virtual assistant function to follow when a user reads a text source aloud. Some problems arise because conventional virtual assistant functions perform speech recognition to convert audio into text/recognized words. Speech recognition typically involves an acoustic step to convert the audio into phonemes and a linguistic step to convert the phonemes into text/recognized words. The language step typically waits for subsequent spoken words to establish context before converting the spoken words to text. The language step introduces unnecessary time delays and consumes additional computing resources. Additionally, performing a traditional text-based comparison with a text source using recognized text may be more error prone than performing a phoneme comparison (e.g., a phonological comparison). This is often the case because many words that sound the same or similar may be spelled very differently and yield false negatives when the text is compared. In addition, conventional text comparisons may not properly address situations where a user may jump around while reading a text source. For example, portions of the text source may be skipped, repeated, or new content added. This can make it challenging to identify the current reading location within the text source and to correctly detect the reading speed. Aspects and embodiments of the present technology address the above and other deficiencies by providing enhancements to enable a computing device to detect a current reading location in a text source when reading the text source aloud. In one example, the techniques may avoid the linguistic step of traditional speech recognition by comparing the phonological data derived from audio with the phonological data derived from a text source. The text source may be a book, magazine, presentation, voice, script, or other source containing sequences of words. The techniques may receive audio data including words spoken by a user, and may convert the audio data into phonemic data locally or through an assistant or remote server (e.g., a cloud service). The phoneme data of the audio may then be compared to the text source via phoneme comparison rather than more traditional text comparison. The phoneme comparison may be accompanied by a fuzzy match to identify a location (e.g., a current reading location) within the word sequence. The systems and methods described herein include techniques to enhance the technical field of computer-based human speech recognition. In particular, the disclosed techniques enhance the latency, accuracy, and computational resources required to identify the current reading position. This may be the result of modifying the voice analysis process (e.g., voice recognition) to avoid converting the audio data into text/words. The technique may use a voice analysis process that uses an acoustic model to convert audio into phonological data, but may avoid the language step of using a language model to convert phonological data into text/words. Avoiding the language step reduces the time delay and consumption of computing resources. Performing a phoneme comparison and using fuzzy matching may enhance the accuracy of identifying the current reading position because it may better compensate for non-linear reading of the text source (e.g., skipping, repeating, or adding content).
In a second example, the enhancement may relate to algorithmic determinations that story readers interrupt reading, and may relate to the field of computer-based human speech recognition, and in particular to enhancing the ability of a computer device to determine that a user is no longer reading the content of a text source aloud. A number of technical problems arise when a computing device attempts to use a traditional virtual assistant function to follow when a user reads a text source aloud. Some problems arise because conventional virtual assistants may not be able to detect when the user has finished providing audio input if the user continues to talk about other things. This may result in the computing device continuing to record the user's audio, which may be problematic if the user transitions to discussing private things. When a user does not follow text and skips, repeats, or adds new content while reading the text source aloud, detecting when the user stops reading from the text source can be more challenging. Aspects and embodiments of the present technology address the above and other deficiencies by enhancing the ability of a computing device to detect when a user discontinues reading a text source. In one example, the techniques may enable the virtual assistant to more accurately detect that the user has left the source of reading text to take a break, and may deactivate the microphone to avoid capturing private audio content. This may involve receiving audio data including spoken words associated with the text source and comparing the audio data to the data of the text source. The techniques may calculate a correspondence measure between content of the audio data and content of the text source. The corresponding measure may be a probability value based on a comparison of the phonological data, the textual data, or other data, and may involve the use of fuzzy matching logic. When the correspondence measure satisfies a threshold (e.g., is below a minimum correspondence threshold), the technique may cause a signal to be transmitted that will stop analysis of subsequent audio data. The systems and methods described herein include techniques to enhance the technical field of computer-based human speech recognition. In particular, the techniques may solve the technical problem by avoiding inadvertent recordings of user private conversations using comparisons that may better compensate for non-linear reading of text sources (e.g., skipping, repeating, adding content). For example, the above techniques may facilitate more accurate and/or more rapid automatic control of a virtual assistant to record and/or process only relevant audio. The techniques may also enable the computing device to reduce power consumption by disabling an audio sensor (e.g., a microphone) and associated data processing when the computing device detects that the user has stopped reading text. Furthermore, the above techniques may enable a computing device to reduce utilization of computing resources such as processing capacity, network bandwidth, data storage, and the like that may otherwise be used to record and/or process audio data once a user has stopped reading text.
In a third example, the enhancement may relate to dynamic adjustment of a story time special effect based on the context data, and may relate to the field of virtual assistants, particularly to enhancing an ability of a virtual assistant to provide a special effect while reading text sources aloud. Modern computing devices may be configured to employ traditional virtual assistant features to provide sound effects to supplement the environment when a user reads a book aloud. For example, the computing device may provide a barking effect when the user reads the word "bark" aloud. The sound effects are typically provided by the same entity that provides the text source, and may correspond directly to a portion of the text source. As a result, special effects may be the same independent of the user or environment and may not be optimized for the user's particular reading environment. Aspects and embodiments of the present technology address the above and other deficiencies by enabling computing devices to provide a wide variety of special effects based on user environment. In one example, the techniques may enable a computing device to analyze context data of a user environment and select or customize special effects. A special effect may be a physical effect that alters the user's environment to include an acoustic effect (e.g., music, sound effect music), an optical effect (e.g., flashing lights, ambient light), a haptic effect (e.g., vibration, wind, temperature change), other effects, or a combination thereof. The techniques may involve receiving and analyzing context data associated with a user. The contextual data may relate to weather, lighting, time of day, user feedback, user profiles, other information, or a combination thereof. The techniques may select or modify a physical effect corresponding to the text source based on the context data. This may result in, for example, selecting or modifying volume, brightness, speed, pitch, or other attributes of the physical effect. The systems and methods described herein include techniques to enhance the technical fields of virtual assistants and home automation. In particular, the techniques may enable a computing device to optimize an environment by using contextual data about a user and the environment to add, remove, or modify physical effects to enhance the user's listening experience.
In a fourth example, the augmentation may be related to the detection of story reader progress to pre-cache special effects, and may be related to the realm of virtual assistants, particularly to augmenting the ability of the virtual assistant to pre-cache special effects for text sources that are being read aloud. In attempting to use conventional virtual assistant functionality to provide sound effects that are synchronized with the spoken content of the text source, a number of technical problems arise. Some problems arise because conventional virtual assistants perform speech recognition to convert audio to text and then make comparisons based on the text. Speech recognition typically involves an acoustic step to convert audio into phonemes and a linguistic step to convert phonemes into text. The language step typically waits for subsequent spoken words to establish context before translating the spoken words into text. The language step introduces time delays and consumes other computing resources on the computing device that may be resource constrained. The delay may be further exacerbated because the sound effect may be a large audio file downloaded from a remote data source. Traditional approaches may involve downloading sound effects in response to detecting a spoken word, but delays may result in special effects being provided long after the word is spoken. Another approach may involve downloading all sound effects when the text source is initially identified, but may be problematic when the computing device is a resource-constrained device (e.g., a smart speaker). Aspects and embodiments of the present technology address the above and other deficiencies by providing enhancements to a computing device that enhance its ability to conserve computing resources and still provide the special effect of synchronizing with text sources that are read aloud. This may be accomplished by using data from a text source (e.g., a book) to predict future audible actions and pre-fetching the associated physical effects before the corresponding audible actions occur. In one example, the techniques may enable a computing device to predict when a user will reach a word in a text source before speaking the word. This may involve identifying performance data for the text source that relates the physical performance to one or more audible actions of the user. The audible action may include words spoken by the user, or may be a page turn, a book close, or other action that generates an audible response. The technique may determine the trigger condition based on the current reading position, reading speed, other data, or a combination thereof. In response to detecting that the trigger condition is satisfied, the technique can cause the computing device to load content for the physical effect and then provide the physical effect to modify the user's environment. The systems and methods described herein include techniques in the art for enhancing pre-caching based on recognition of human speech. In particular, the disclosed technology can solve technical problems related to resource consumption when analyzing voice and downloading special effects. The techniques may also reduce the delay in providing special effects, thereby better synchronizing the special effects with human voice.
Fig. 9 depicts a block diagram of a computer system operating in accordance with one or more aspects of the present disclosure. In various illustrative examples, computing system 900 may correspond to computing device 130 of fig. 2-4. The computing system may be included within a data center that supports virtualization. In some embodiments, computer system 900 may be connected to other computer systems (e.g., via a network such as a Local Area Network (LAN), intranet, extranet, or the internet). The computer system 900 may operate in the capacity of a server or a client computer in a client-server environment, or as a peer computer in a peer-to-peer or distributed network environment. Computer system 900 may be provided by a Personal Computer (PC), a tablet PC, a set-top box (STB), a Personal Digital Assistant (PDA), a cellular telephone, a web appliance, a server, a network router, switch or bridge, or any device capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that device. Additionally, the term "computer" shall include any collection of computers that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies described herein.
In another aspect, computer system 900 may include a processing device 902, a volatile memory 904 (e.g., Random Access Memory (RAM)), a non-volatile memory 906 (e.g., Read Only Memory (ROM)), or an electrically erasable programmable ROM (eeprom)) and a data storage device 916, which may communicate with each other via a bus 908.
The processing device 902 may be provided by one or more processors, such as a general purpose processor (such as, for example, a Complex Instruction Set Computing (CISC) microprocessor, a Reduced Instruction Set Computing (RISC) microprocessor), a Very Long Instruction Word (VLIW) microprocessor, a microprocessor implementing other types of instruction sets, or a microprocessor implementing a combination of types of instruction sets), or a special purpose processor (such as, for example, an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Digital Signal Processor (DSP), or a network processor).
The computer system 900 may further include a network interface device 922. Computer system 900 may also include a video display unit 910 (e.g., an LCD), an alphanumeric input device 912 (e.g., a keyboard), a cursor control device 914 (e.g., a mouse), and a signal generation device 920.
The data storage 916 may include a non-transitory computer-readable storage medium 924 on which may be stored instructions 926 encoding any one or more of the methods or functions described herein, including instructions for implementing the method 500, 600, 700, or 800 and any of the components or modules in fig. 1-4.
The instructions 926 may also reside, completely or partially, within the volatile memory 904 and/or within the processing device 902 during execution thereof by the computer system 900, the volatile memory 904 and the processing device 902 thus also constituting machine-readable storage media.
While the computer-readable storage medium 924 is shown in an illustrative example to be a single medium, the term "computer-readable storage medium" should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of executable instructions. The term "computer-readable storage medium" shall also be taken to include a tangible medium that is capable of storing or encoding a set of instructions for execution by the computer and that cause the computer to perform any one or more of the methodologies described herein. The term "computer readable storage medium" shall include, but not be limited to, solid-state memories, optical media, and magnetic media.
The methods, components and features described herein may be implemented by discrete hardware components or may be integrated in the functionality of other hardware components such as ASICs, FPGAs, DSPs or similar devices. Furthermore, the methods, components and features may be implemented by firmware modules or functional circuits within hardware resources. Additionally, methods, components, and features may be implemented with any combination of hardware resources and computer program components or computer programs.
Unless specifically stated otherwise, terms such as "initiating," "transmitting," "receiving," "analyzing," or the like, refer to operations and processes performed or effected by a computer system that manipulates and transforms data represented as physical (electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices. In addition, the terms "first," "second," "third," "fourth," and the like, as used herein, refer to labels used to distinguish between different elements and may not have an ordinal meaning according to their numerical name.
Examples described herein also relate to an apparatus for performing the methods described herein. This apparatus may be specially constructed for performing the methods described herein, or it may comprise a general-purpose computer system selectively programmed by a computer program stored in the computer system. Such a computer program may be stored in a computer readable tangible storage medium.
The methods and illustrative examples described herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may be used with the teachings described herein, or it may prove convenient to construct a more specialized apparatus to perform each of the methods 500, 600, 700, 800, and/or individual functions, routines, subroutines, or operations thereof. Examples of the structure of various of these systems are set forth in the foregoing description.
The above description is intended to be illustrative, and not restrictive. While the present disclosure has been described with reference to specific illustrative examples and embodiments, it should be recognized that the present disclosure is not limited to the described examples and embodiments. The scope of the disclosure should be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled.
Claims (20)
1. A method, comprising:
determining phoneme data of a text source, wherein the text source comprises a word sequence;
receiving audio data, the audio data including spoken words associated with the text source;
comparing, by a processing device, the phoneme data of the text source and the phoneme data of the audio data; and
identifying a position in the sequence of words based on the comparison of the phonological data.
2. The method of claim 1, wherein the text source is a book and the location is a current reading location in the book.
3. The method of claim 1 or 2, wherein the phonological data of the text source includes a phoneme encoding of the sequence of words, the phoneme encoding including one or more sequences of phoneme values.
4. The method of any of claims 1-3, wherein comparing the phonological data includes calculating a phonological edit distance between the phonological data of the audio data and the phonological data of the text source.
5. The method of any of claims 1 to 4, wherein comparing phoneme data comprises calculating a numerical value representing a degree of similarity between two or more sequences of phoneme values.
6. The method of any of claims 1-5, wherein comparing the phonological data includes performing a fuzzy match between the phonological data corresponding to the audio data and the phonological data of the text source.
7. The method of any of claims 1-6, wherein the comparing comprises comparing the audio data and the text source without converting the audio data to text using speech recognition.
8. The method of any of claims 1-7, wherein identifying a position in the sequence of words comprises:
determining, based on phonological data of the text source, that the spoken word matches a word in the sequence of words; and
selecting a location of the word based on the phonological data of the text source.
9. The method of any of claims 1 to 8, further comprising:
accessing text data of the text source;
generating the phoneme data based on the text data; and
associating the phoneme data with the text source.
10. A system comprising a processing device configured to:
determining phoneme data of a text source, wherein the text source comprises a word sequence;
receiving audio data comprising spoken words associated with the text source;
comparing the phoneme data of the text source with phoneme data of the audio data; and is
Identifying a position in the sequence of words based on the comparison of the phonological data.
11. The system of claim 10, wherein the text source is a book and the location is a current reading location in the book.
12. The system of claim 10 or 11, wherein the phonological data of the text source includes a phoneme encoding of the sequence of words, the phoneme encoding including one or more sequences of phoneme values.
13. The system of any of claims 10 to 12, wherein the comparison of the phonological data includes calculating a phonological edit distance between the phonological data of the audio data and the phonological data of the text source.
14. The system of any of claims 10 to 13, wherein the comparison of the phoneme data comprises calculating a numerical value representing a degree of similarity between two or more sequences of phoneme values.
15. The system of any of claims 10 to 14, wherein the comparison of the phonological data includes performing a fuzzy match between the phonological data corresponding to the audio data and the phonological data of the text source.
16. The system of any of claims 10 to 15, wherein the comparing comprises comparing the audio data and the text source without converting the audio data to text using speech recognition.
17. The system of any of claims 10 to 16, wherein the identification of a location in the sequence of words comprises:
determining, based on phonological data of the text source, that the spoken word matches a word in the sequence of words; and
selecting a location of the word based on the phonological data of the text source.
18. The system of any of claims 10 to 17, further configured to:
accessing text data of the text source;
generating the phoneme data based on the text data; and
associating the phoneme data with the text source.
19. The system of any of claims 10 to 18, wherein the system is configured to implement a virtual assistant.
20. A computer program product, the computer program product configured to cause: when processed by a processing device, cause the processing device to perform the method of any one of claims 1 to 9.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2018/049401 WO2020050820A1 (en) | 2018-09-04 | 2018-09-04 | Reading progress estimation based on phonetic fuzzy matching and confidence interval |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112805779A true CN112805779A (en) | 2021-05-14 |
Family
ID=63684539
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880098288.7A Pending CN112805779A (en) | 2018-09-04 | 2018-09-04 | Reading progress estimation based on speech fuzzy matching and confidence interval |
Country Status (4)
Country | Link |
---|---|
US (1) | US11526671B2 (en) |
EP (1) | EP3837681A1 (en) |
CN (1) | CN112805779A (en) |
WO (1) | WO2020050820A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116052671A (en) * | 2022-11-21 | 2023-05-02 | 深圳市东象设计有限公司 | Intelligent translator and translation method |
CN117423260A (en) * | 2023-12-19 | 2024-01-19 | 杭州智慧耳朵科技有限公司 | Auxiliary teaching method based on classroom speech recognition and related equipment |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11301645B2 (en) * | 2020-03-03 | 2022-04-12 | Aziza Foster | Language translation assembly |
Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110153330A1 (en) * | 2009-11-27 | 2011-06-23 | i-SCROLL | System and method for rendering text synchronized audio |
US20110239107A1 (en) * | 2010-03-29 | 2011-09-29 | Phillips Michael E | Transcript editor |
US20120078630A1 (en) * | 2010-09-27 | 2012-03-29 | Andreas Hagen | Utterance Verification and Pronunciation Scoring by Lattice Transduction |
US20140040713A1 (en) * | 2012-08-02 | 2014-02-06 | Steven C. Dzik | Selecting content portions for alignment |
US8744856B1 (en) * | 2011-02-22 | 2014-06-03 | Carnegie Speech Company | Computer implemented system and method and computer program product for evaluating pronunciation of phonemes in a language |
US20150170648A1 (en) * | 2013-12-17 | 2015-06-18 | Google Inc. | Ebook interaction using speech recognition |
US20160062970A1 (en) * | 2014-09-02 | 2016-03-03 | Belleau Technologies | Method and System for Dynamic Speech Recognition and Tracking of Prewritten Script |
CN107305771A (en) * | 2016-04-25 | 2017-10-31 | 国家阅读方式研究院公司 | For carrying out visual system and method to connected speech |
Family Cites Families (27)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CA2088080C (en) * | 1992-04-02 | 1997-10-07 | Enrico Luigi Bocchieri | Automatic speech recognizer |
US7013276B2 (en) * | 2001-10-05 | 2006-03-14 | Comverse, Inc. | Method of assessing degree of acoustic confusability, and system therefor |
US7143033B2 (en) | 2002-04-03 | 2006-11-28 | The United States Of America As Represented By The Secretary Of The Navy | Automatic multi-language phonetic transcribing system |
WO2004093078A1 (en) * | 2003-04-18 | 2004-10-28 | Unisay Sdn. Bhd. | Process for adding subtitles to video content |
US7917364B2 (en) * | 2003-09-23 | 2011-03-29 | Hewlett-Packard Development Company, L.P. | System and method using multiple automated speech recognition engines |
KR20060054678A (en) | 2004-11-15 | 2006-05-23 | 김경희 | Apparatus and method for implementing character video synchronized with sound |
US8069397B2 (en) | 2006-07-10 | 2011-11-29 | Broadcom Corporation | Use of ECC with iterative decoding for iterative and non-iterative decoding in a read channel for a disk drive |
EP2135231A4 (en) * | 2007-03-01 | 2014-10-15 | Adapx Inc | System and method for dynamic learning |
US20100028843A1 (en) | 2008-07-29 | 2010-02-04 | Bonafide Innovations, LLC | Speech activated sound effects book |
US8568189B2 (en) | 2009-11-25 | 2013-10-29 | Hallmark Cards, Incorporated | Context-based interactive plush toy |
KR101832693B1 (en) | 2010-03-19 | 2018-02-28 | 디지맥 코포레이션 | Intuitive computing methods and systems |
US20120001923A1 (en) | 2010-07-03 | 2012-01-05 | Sara Weinzimmer | Sound-enhanced ebook with sound events triggered by reader progress |
US10672399B2 (en) | 2011-06-03 | 2020-06-02 | Apple Inc. | Switching between text data and audio data based on a mapping |
US10042603B2 (en) | 2012-09-20 | 2018-08-07 | Samsung Electronics Co., Ltd. | Context aware service provision method and apparatus of user device |
CN105359546B (en) | 2013-05-01 | 2018-09-25 | 乐盟交互公司 | Content for interactive video optical projection system generates |
US11250630B2 (en) | 2014-11-18 | 2022-02-15 | Hallmark Cards, Incorporated | Immersive story creation |
US10249205B2 (en) | 2015-06-08 | 2019-04-02 | Novel Effect, Inc. | System and method for integrating special effects with a text source |
US20190189019A1 (en) | 2015-06-08 | 2019-06-20 | Novel Effect, Inc. | System and Method for Integrating Special Effects with a Text Source |
US10928915B2 (en) | 2016-02-10 | 2021-02-23 | Disney Enterprises, Inc. | Distributed storytelling environment |
US20170262537A1 (en) | 2016-03-14 | 2017-09-14 | Amazon Technologies, Inc. | Audio scripts for various content |
US11544274B2 (en) | 2016-07-18 | 2023-01-03 | Disney Enterprises, Inc. | Context-based digital assistant |
US20180293221A1 (en) * | 2017-02-14 | 2018-10-11 | Microsoft Technology Licensing, Llc | Speech parsing with intelligent assistant |
US10586369B1 (en) | 2018-01-31 | 2020-03-10 | Amazon Technologies, Inc. | Using dialog and contextual data of a virtual reality environment to create metadata to drive avatar animation |
US11775479B2 (en) * | 2018-05-24 | 2023-10-03 | Luther Systems Us Incorporated | System and method for efficient and secure private similarity detection for large private document repositories |
WO2020046387A1 (en) | 2018-08-31 | 2020-03-05 | Google Llc | Dynamic adjustment of story time special effects based on contextual data |
EP3837597A1 (en) | 2018-09-04 | 2021-06-23 | Google LLC | Detection of story reader progress for pre-caching special effects |
US20200111491A1 (en) * | 2018-10-08 | 2020-04-09 | Alkira Software Holdings Pty Ltd. | Speech enabled user interaction |
-
2018
- 2018-09-04 WO PCT/US2018/049401 patent/WO2020050820A1/en unknown
- 2018-09-04 EP EP18778706.4A patent/EP3837681A1/en not_active Withdrawn
- 2018-09-04 US US16/610,475 patent/US11526671B2/en active Active
- 2018-09-04 CN CN201880098288.7A patent/CN112805779A/en active Pending
Patent Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110153330A1 (en) * | 2009-11-27 | 2011-06-23 | i-SCROLL | System and method for rendering text synchronized audio |
US20110239107A1 (en) * | 2010-03-29 | 2011-09-29 | Phillips Michael E | Transcript editor |
US20120078630A1 (en) * | 2010-09-27 | 2012-03-29 | Andreas Hagen | Utterance Verification and Pronunciation Scoring by Lattice Transduction |
US8744856B1 (en) * | 2011-02-22 | 2014-06-03 | Carnegie Speech Company | Computer implemented system and method and computer program product for evaluating pronunciation of phonemes in a language |
US20140040713A1 (en) * | 2012-08-02 | 2014-02-06 | Steven C. Dzik | Selecting content portions for alignment |
US20150170648A1 (en) * | 2013-12-17 | 2015-06-18 | Google Inc. | Ebook interaction using speech recognition |
US20160062970A1 (en) * | 2014-09-02 | 2016-03-03 | Belleau Technologies | Method and System for Dynamic Speech Recognition and Tracking of Prewritten Script |
CN107305771A (en) * | 2016-04-25 | 2017-10-31 | 国家阅读方式研究院公司 | For carrying out visual system and method to connected speech |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116052671A (en) * | 2022-11-21 | 2023-05-02 | 深圳市东象设计有限公司 | Intelligent translator and translation method |
CN117423260A (en) * | 2023-12-19 | 2024-01-19 | 杭州智慧耳朵科技有限公司 | Auxiliary teaching method based on classroom speech recognition and related equipment |
CN117423260B (en) * | 2023-12-19 | 2024-03-12 | 杭州智慧耳朵科技有限公司 | Auxiliary teaching method based on classroom speech recognition and related equipment |
Also Published As
Publication number | Publication date |
---|---|
WO2020050820A1 (en) | 2020-03-12 |
US11526671B2 (en) | 2022-12-13 |
EP3837681A1 (en) | 2021-06-23 |
US20210182488A1 (en) | 2021-06-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11501769B2 (en) | Dynamic adjustment of story time special effects based on contextual data | |
US11749279B2 (en) | Detection of story reader progress for pre-caching special effects | |
US11862192B2 (en) | Algorithmic determination of a story readers discontinuation of reading | |
CN107516511B (en) | Text-to-speech learning system for intent recognition and emotion | |
US11527174B2 (en) | System to evaluate dimensions of pronunciation quality | |
KR102582291B1 (en) | Emotion information-based voice synthesis method and device | |
CN111312231B (en) | Audio detection method and device, electronic equipment and readable storage medium | |
US11810471B2 (en) | Computer implemented method and apparatus for recognition of speech patterns and feedback | |
US10431188B1 (en) | Organization of personalized content | |
US11526671B2 (en) | Reading progress estimation based on phonetic fuzzy matching and confidence interval | |
CN112799630A (en) | Creating a cinematographed storytelling experience using network addressable devices | |
US11176943B2 (en) | Voice recognition device, voice recognition method, and computer program product | |
US20240135960A1 (en) | Algorithmic determination of a story readers discontinuation of reading | |
US11977816B1 (en) | Time-based context for voice user interface | |
Yan¹ et al. | Interplay Between Prosody and Syntax-Semantics: Evidence from the Prosodic Features of Mandarin Tag | |
CN116612740A (en) | Voice cloning method, voice cloning device, electronic device, and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |