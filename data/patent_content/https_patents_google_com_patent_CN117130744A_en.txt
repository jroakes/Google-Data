CN117130744A - Invoking an automated assistant through individual commands to perform multiple tasks - Google Patents
Invoking an automated assistant through individual commands to perform multiple tasks Download PDFInfo
- Publication number
- CN117130744A CN117130744A CN202310994057.7A CN202310994057A CN117130744A CN 117130744 A CN117130744 A CN 117130744A CN 202310994057 A CN202310994057 A CN 202310994057A CN 117130744 A CN117130744 A CN 117130744A
- Authority
- CN
- China
- Prior art keywords
- task
- automated assistant
- user
- spoken utterance
- command
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 66
- 230000004044 response Effects 0.000 claims description 66
- 230000003993 interaction Effects 0.000 claims description 48
- 230000008569 process Effects 0.000 claims description 12
- 230000015654 memory Effects 0.000 claims description 6
- 238000012790 confirmation Methods 0.000 claims description 3
- 230000000977 initiatory effect Effects 0.000 claims 14
- 230000002123 temporal effect Effects 0.000 claims 4
- 239000003795 chemical substances by application Substances 0.000 description 169
- 239000004615 ingredient Substances 0.000 description 22
- 238000012545 processing Methods 0.000 description 8
- 238000010586 diagram Methods 0.000 description 7
- 235000013312 flour Nutrition 0.000 description 6
- 230000009471 action Effects 0.000 description 5
- 239000000463 material Substances 0.000 description 5
- 230000006870 function Effects 0.000 description 4
- 235000013599 spices Nutrition 0.000 description 4
- 238000004891 communication Methods 0.000 description 3
- 230000006855 networking Effects 0.000 description 3
- 230000000007 visual effect Effects 0.000 description 3
- 230000008859 change Effects 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000000153 supplemental effect Effects 0.000 description 2
- 235000017060 Arachis glabrata Nutrition 0.000 description 1
- 244000105624 Arachis hypogaea Species 0.000 description 1
- 235000010777 Arachis hypogaea Nutrition 0.000 description 1
- 235000018262 Arachis monticola Nutrition 0.000 description 1
- 235000010676 Ocimum basilicum Nutrition 0.000 description 1
- 240000007926 Ocimum gratissimum Species 0.000 description 1
- 240000007594 Oryza sativa Species 0.000 description 1
- 235000007164 Oryza sativa Nutrition 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 239000000796 flavoring agent Substances 0.000 description 1
- 235000019634 flavors Nutrition 0.000 description 1
- 235000012041 food component Nutrition 0.000 description 1
- 239000005417 food ingredient Substances 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 235000020232 peanut Nutrition 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 239000000843 powder Substances 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 235000009566 rice Nutrition 0.000 description 1
- 235000013555 soy sauce Nutrition 0.000 description 1
- 235000021419 vinegar Nutrition 0.000 description 1
- 239000000052 vinegar Substances 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/48—Program initiating; Program switching, e.g. by interrupt
- G06F9/4806—Task transfer initiation or dispatching
- G06F9/4843—Task transfer initiation or dispatching by program, e.g. task dispatcher, supervisor, operating system
- G06F9/485—Task life-cycle, e.g. stopping, restarting, resuming execution
- G06F9/4856—Task life-cycle, e.g. stopping, restarting, resuming execution resumption being on a different machine, e.g. task migration, virtual machine migration
- G06F9/4862—Task life-cycle, e.g. stopping, restarting, resuming execution resumption being on a different machine, e.g. task migration, virtual machine migration the task being a mobile agent, i.e. specifically designed to migrate
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/48—Program initiating; Program switching, e.g. by interrupt
- G06F9/4806—Task transfer initiation or dispatching
- G06F9/4812—Task transfer initiation or dispatching by interrupt, e.g. masked
- G06F9/4831—Task transfer initiation or dispatching by interrupt, e.g. masked with variable priority
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/02—Reservations, e.g. for tickets, services or events
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/48—Program initiating; Program switching, e.g. by interrupt
- G06F9/4806—Task transfer initiation or dispatching
- G06F9/4843—Task transfer initiation or dispatching by program, e.g. task dispatcher, supervisor, operating system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/48—Program initiating; Program switching, e.g. by interrupt
- G06F9/4806—Task transfer initiation or dispatching
- G06F9/4843—Task transfer initiation or dispatching by program, e.g. task dispatcher, supervisor, operating system
- G06F9/4881—Scheduling strategies for dispatcher, e.g. round robin, multi-level priority queues
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/10—Office automation; Time management
- G06Q10/103—Workflow collaboration or project management
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Abstract
Invoking an automated assistant to perform a plurality of tasks by an individual command is disclosed. Methods, apparatus, systems, and computer readable media for engaging an automated assistant to perform a plurality of tasks through a multitasking command. The multitasking command may be a command that, when provided by a user, causes the automated assistant to invoke a plurality of different agent modules for executing tasks to complete the multitasking command. During execution of the multitasking command, a user may provide input that may be used by one or more proxy modules to perform their respective tasks. Further, feedback from one or more agent modules may be used by the automated assistant to dynamically alter tasks in order to more efficiently use resources available during completion of the multitasking command.
Description
Description of the division
The application belongs to a divisional application of Chinese application patent application No.201880038386.1 with the application date of 2018, 7 and 3.
Technical Field
The present application relates generally to invoking an automated assistant to perform a plurality of tasks by an individual command.
Background
An automated assistant (also referred to as a "personal assistant module," "mobile assistant," or "chat robot") may be interacted with by a user via various computing devices (such as a smart phone, tablet computer, wearable device, automobile system, standalone personal assistant device, etc.). The automated assistant receives input (e.g., typed and/or spoken natural language input) from the user and responds with response content (e.g., visual and/or audible natural language output).
The automated assistant may provide various functionalities through interactions with various local and/or third party agents. In order for a user to perform a particular function with an automated assistant, oftentimes the user must invoke the automated assistant (e.g., through a particular spoken phrase and/or a particular hardware input) and then provide a plurality of invocation phrases associated with the particular function. In some cases, a large number of dialog turns may be required for the user to ultimately cause the automated assistant to perform a particular task. This may result in an undesirably large amount of network traffic, for example, between the device used to receive the user input and one or more separate computing systems of the local or third party agents. This can also result in wasted time for the user and inefficient use of computing resources for the automated assistant because the automated assistant must process and respond to each input from the user.
Disclosure of Invention
The present disclosure relates generally to methods, apparatus, and computer-readable media (transitory and non-transitory) for engaging an automated assistant to perform multiple tasks through a single command. An automated assistant may be an application operating at a client device and/or a server device and having the ability to parse input from a user to determine what actions the user is requesting to perform. In some implementations, the automated assistant may interpret certain inputs as being associated with a plurality of different tasks. Each of the plurality of different tasks may be associated with an application or agent that may complete the task and/or provide information for completing the separate task. Tasks may be performed serially, in parallel, or a combination thereof, depending on input from a user. Generally, by facilitating the execution of multiple tasks via fewer commands, embodiments described herein reduce the number of user inputs to an automated assistant as compared to previous systems, and in doing so, reduce the burden on resources of client devices and data networks involved, for example, at least in providing an automated assistant. This is at least because the client device does not need to process as much user input as before, which may allow the client device to expend less battery power in implementing the assistant in addition to using less computing resources. For data networks, since fewer commands are received at the client device, network load may be reduced by fewer instantiations of network communications between the client device and other computing systems involved in facilitating performance of the task.
In some cases, a user may request an automated assistant to control various devices in their home in order to prepare the home before the user arrives. The user may do this by, for example, speaking an input to the assistant such as "assistant, please get my home ready". The automated assistant may receive input via an automated assistant interface of the client device and parse the input to determine an agent that should respond to the input. Specific inputs that prepare the user's home may cause the automated assistant to query the home climate control agent and weather agent in parallel. The weather agent may be used, for example, by an automated assistant to determine the current and/or forecasted weather for the next several hours. The home climate control agent may be used by an automated assistant to identify, for example, a current temperature inside the home using an intelligent thermostat inside the home. The automated assistant may optionally use the current weather and temperature inside the house to select an intelligent climate control system for how to control the home (e.g., based on whether the temperature of the home would remain within a desired temperature range in view of the current or forecasted weather), the assistant may cause the climate control system to input hot or cold air into the home to ensure that the temperature of the home is within the range when the user arrives at the home.
In some cases, the user may request the automated assistant to schedule an appointment for the user and others of interest by speaking an input such as, for example, "assistant, please schedule appointment night. The automated assistant may receive input via an automated assistant interface of the client device and parse the input to determine an agent that should respond to the input. Specific input of the scheduled appointment night may cause the automated assistant to query the calendar agent and the restaurant selection agent in parallel. The calendar agent may be used by the automated assistant to determine the available night number at which the user is free to have an appointment night. The restaurant selection agent may be used by the automated assistant to identify restaurants that are suitable for the appointment night. The automated assistant may optionally use available nights from the calendar agent to select restaurants (e.g., based on restaurants that were open at the available nights).
For example, as part of a request for a restaurant selection agent, an automated assistant may provide a value defining available night to thereby filter out particular restaurants that may not be hospitalized with a user during the available night. The automated assistant may then identify a reservation agent that may be utilized to reserve one of the selected restaurants for the available night. In this manner, the automated assistant is using parallel tasks (e.g., engaging a calendar agent and restaurant selection agent participation) to obtain further information for use by a separate agent module (e.g., reservation agent) to perform subsequent tasks to reserve the restaurant, thereby ultimately completing the original command to plan the appointment night. In some implementations, subtasks for executing commands to schedule appointments night may be performed serially, allowing each agent the opportunity to receive information serially from a previous agent.
In some implementations, the automated assistant can receive input associated with a plurality of different subtasks and query the user for feedback during execution of the plurality of different subtasks. For example, the user may request the automated assistant to plan a camping trip for the user by speaking an input such as "assistant, please plan a camping trip. The automated assistant may parse the input from the user and determine the agent that should respond to the input. For example, planning camping travel may involve at least the subtasks of identifying idle weekends, booking campsites, and obtaining camping equipment. The subtasks identifying the idle weekend and booking camping may involve the automated assistant invoking a calendar agent and a travel agent associated with the travel website. The data from the calendar agent may be used by the automated assistant to determine available weekends for booking camps. A travel agent associated with a travel website may be used to identify camping sites that may be reserved for open weekends identified by the calendar agent. In response to a suitable campsite being identified, the automated assistant may identify a campsite agent associated with the identified campsite. The automated assistant may determine, via output from the camping venue agent, that the camping venue provides camping equipment rentals. The automated assistant may compare the output from the campsite agent to the subtasks associated with the initial request to plan a camping trip. If there is a correlation between the output from the camping-site proxy and the at least one subtask based on the comparison, the automated assistant generates a query for the user. For example, the query may be a request to provide feedback from the user.
The request for feedback may help the automated assistant complete the subtasks of both booking camping and obtaining camping equipment. For example, an automated assistant may use a system such as "good, do you like to rent camping equipment? "such output requests feedback through the client device's automated assistant interface. If the user selects to rent camping equipment, the automated assistant may use the calendar agent's data and the request to rent camping equipment to continue to reserve camping land through the camping site agent. In this way, the automated assistant is able to complete the second subtask using both feedback from the user and data from the first subtask. Alternatively, the campsite agent may request feedback from the user via an automated assistant interface, where the automated assistant acts as an intermediary, for example, providing a GUI for the request for feedback or providing audio corresponding to the request for feedback. If camping equipment cannot be rented, the camping place agent may provide feedback back to the automated assistant, and the automated assistant may query the calendar agent to identify another free weekend to reserve camping.
In response to receiving feedback from the user, the automated assistant may interact with the camping place agent to book camping places on other idle weekends identified. Thereafter, the automated assistant may provide an output to the user such as, for example, "good, i complete booking your camping travel. If the user chooses not to rent camping equipment, the automated assistant may use the calendar agent's data to complete the reservation of the campsite and identify a shopping agent for helping the user purchase the camping equipment. If the user purchases camping equipment through the shopping agent, the automated assistant may consider the subtask of booking camping and obtaining camping equipment complete and indicate to the user that camping travel has been booked. In this way, multiple subtasks performed by the automated assistant may be dynamically affected by input from the user during execution of the subtasks.
The automated assistant may determine that the initial input from the user is associated with a plurality of different subtasks according to a variety of different methods. For example, a user may configure commands in multiple subtasks through a Graphical User Interface (GUI) for controlling an automated assistant. For example, if the user wants the automated assistant to know what the user means by "get my home ready" or "schedule appointment night," the user may open the automated assistant GUI and select the subtask to which the command "get my home ready" or "schedule appointment night" relates from among many subtasks. Alternatively, the user may engage in a human-machine conversation of the automated assistant to define how the commands should be executed. For example, a user may provide an input "assistant," i want you to do when i say 'plan appointment night', and thereafter describe the subtasks involved in planning appointment night (e.g., identify available night, book restaurants, and send invitations).
In some implementations, the automated assistant can automatically query the user regarding configuring the plurality of subtask commands. In other words, the automated assistant may ask the user if they would like to perform an action in the future in response to a single command that was performed as a result of a previous interaction between the user and the automated assistant. The previous interactions may be identified and grouped by the automated assistant according to one or more common conditions associated with the previous interactions. In some implementations, the automated assistant can determine a relevance between interactions based on interactions within a threshold time period of each other (e.g., each of the first interaction, the second interaction, and the third interaction are initialized within less than the threshold time period of each other). In other implementations, the automated assistant can determine the relevance between interactions based on interactions completed within a threshold session period (e.g., the first interaction, the second interaction, and the third interaction are all completed within less than the threshold session period). For example, after friday commuting, a user may query an automated assistant interface in their vehicle to learn what happens on weekends. Specifically, the user may ask a series of queries, such as, for example, "assistant can ask you to check my social network and tell me my friends what was done on the weekend? "
Upon receiving a series of queries from a user and identifying each query as being related to, for example, a weekend event, the automated assistant may ask the user if they would like to associate the series of queries with a single command. For example, after a user has provided a series of queries, the automated assistant may provide a response output, such as "in the future, whether you want this information in response to a single command such as" assistant, this weekend is boring me ". If the user chooses to have the automated assistant create a single command, the automated assistant may store the single command in association with a set of subtasks, each subtask corresponding to a series of queries from the user. Thereafter, when the user provides the input "assistant, this weekend is boring me chat," the automated assistant will collect weekend information from each of the social network, the agent module associated with the city website, and the networking application, and provide the collected weekend information to the user as a responsive output. This saves the user the trouble of repeating a series of queries and saves network and processing resources, as the automated assistant would no longer have to parse the series of queries individually or initiate an instance of communication with another computing system over the network for each of the series of queries. By facilitating replacement of a series of queries with a single query, embodiments described herein reduce the number of user inputs to an automated assistant as compared to previous systems, and in so doing reduce the burden on resources such as client devices and data networks involved at least in providing an automated assistant.
In some embodiments, a method implemented by one or more processors is set forth as comprising the steps of: determining content of the natural language input provided to the automated assistant interface includes commands corresponding to a plurality of sub-tasks to be performed by separate agent modules accessible to the automated assistant application. The automated assistant application may be configured to interact with the user via an automated assistant interface. The method may further comprise: invoking a first proxy module of the proxy modules to execute a first sub-task of the plurality of sub-tasks; and receiving a first proxy output from the first proxy module in response to invoking the first proxy module. The method may further comprise: and executing a second sub-task of the plurality of sub-tasks by calling a second one of the proxy modules with a parameter based on the first proxy output. In some implementations, the second sub-task relates to a processing parameter. The method may further comprise: receiving a second agent output from the second agent module in response to invoking the second agent module; and providing a response output to the user via the automated assistant interface based on the first agent output and the second agent output. In some implementations, determining that the content of the natural language input includes the command can include: the content is compared to entries in an index designating the stored command as a multitasking command.
In some implementations, each of the first and second agent modules may be a third party agent module controlled by a corresponding third party, the third party being distinct from the party controlling the automated assistant application. In still other embodiments, invoking the second proxy module with the parameters based on the first proxy outputs may include incorporating at least some of the first proxy outputs into one or more of the parameters and providing the parameters to the second proxy module through the automated assistance application.
In some implementations, the response output can correspond to a query to the user and the method can further include: input from the user is received at the automated assistant interface in response to the query, and the second agent module is caused to process the input from the user. The second agent module may provide feedback to the automated assistant application in response to processing the input from the user. In response to the feedback meeting one or more criteria, a third generation module may be invoked to perform a third sub-task of the plurality of sub-tasks using parameters based on the feedback.
In some implementations, the natural language input is a single natural language input, and the content of the single natural language input is different from the parameters that are incorporated into at least some of the first proxy outputs. In response to receiving the single natural language input, an automated assistant application can invoke a plurality of different proxy modules without requiring additional input from the user. For example, in response to receiving a single natural language input corresponding to a multitasking command, the automated assistant may invoke the first proxy module and the second module in parallel or serially without receiving additional input from the user in addition to the single natural language input corresponding to the multitasking command. In some implementations, invoking a first one of the proxy modules to perform a first one of the plurality of tasks includes communicating with a remote server that is separate from a client device that includes the automated assistant interface. Further, in some implementations, invoking a second one of the proxy modules to perform a second one of the plurality of tasks includes communicating with a different remote server that is separate from the client device. In some implementations, the command is a pre-configured multitasking command that was previously stored in response to the automated assistant application previously receiving a series of inputs from the user.
In some embodiments, a system is set forth comprising: one or more processors; and a memory storing instructions that, when executed by one or more of the processors, cause the one or more of the processors to perform steps comprising: determining that content of the natural language input provided to the automated assistant interface includes commands corresponding to a plurality of sub-tasks to be performed by the separate agent module. These steps may further include: invoking the first proxy module to perform the first subtask and invoking the second proxy module to perform the second subtask; and receiving an output from the first proxy module in response to the first proxy module performing the first subtask. These steps may also include: providing parameters associated with the output from the first proxy module to the second proxy module; and causing the second agent module to perform the second subtask using the parameters. The steps may further include, in response to at least the first subtask and the second subtask being performed, providing a response output to the user via the automated assistant interface.
In some implementations, the first subtask or the second subtask can be associated with subscribing to a subscription for the user. In other implementations, the command may be a pre-configured multitasking command previously stored in response to a previous interaction by the user via the automated assistant interface. In other embodiments, the first proxy module and the second proxy module are associated with separate third parties.
In some embodiments, the steps may further comprise: providing a query to the user via the automated assistant interface regarding the output; and receiving a subsequent user input from the user. Further, one or more of the parameters provided to the second agent module may be based at least in part on the subsequent user input received from the user.
In still other embodiments, a non-transitory computer-readable medium is set forth that stores instructions that, when executed by one or more processors, cause the one or more processors to perform steps comprising: determining content of the natural language input provided to the automated assistant interface includes commands corresponding to a plurality of sub-tasks to be performed by separate agent modules accessible to the automated assistant application. The automated assistant application may be configured to interface with a user via the automated assistant interface. These steps may further include: invoking a first proxy module of the proxy modules to execute a first sub-task of the plurality of sub-tasks; and receiving a first proxy output from the first proxy module in response to invoking the first proxy module. These steps may further include: and executing a second sub-task of the plurality of sub-tasks by calling a second one of the proxy modules with a parameter based on the first proxy output. The second sub-task may involve processing the parameter.
In some implementations, determining that the content of the natural language input includes the command can include comparing the content to an entry in an index that designates the stored command as a multitasking command. In other embodiments, each of the first agent module and the second agent module is a third party agent module controlled by a corresponding third party, the third party being distinct from the party controlling the automated assistant application.
In some implementations, invoking the second agent module with the parameters includes providing the parameters to the second agent module by the automated assistant application. In some embodiments, the second subtask may include purchasing an item. In some implementations, the steps can include identifying the first and second proxy modules via an index that associates the first and second proxy modules with the subtasks based on content of the natural language input. The plurality of subtasks may be associated with individual phrases previously provided by the user and stored by the automated assistant application.
Other embodiments may include a non-transitory computer-readable storage medium storing instructions executable by a processor (e.g., a Central Processing Unit (CPU) or a Graphics Processing Unit (GPU)) to perform a method such as one or more of the methods described above and/or elsewhere herein. Another embodiment may include a system of one or more computers and/or one or more robots including one or more processors operable to execute stored instructions to perform one or more such methods, such as those described below and/or elsewhere herein.
It should be appreciated that all combinations of the foregoing concepts and additional concepts described in more detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
Drawings
FIG. 1 illustrates a system diagram including an automated assistant capable of invoking a plurality of different agent modules in response to a command from a user.
FIG. 2 illustrates a method for providing commands to an automated assistant that causes the automated assistant to invoke a plurality of different agent modules to perform different tasks in order to complete the commands.
FIG. 3 illustrates a method for dynamically modifying tasks to be dispatched to one or more proxy modules based on feedback received from a user or the proxy modules.
FIG. 4 illustrates a method for configuring a multitasking command for invoking a plurality of different proxy modules via an automated assistant.
FIG. 5 provides a diagram illustrating an example of a user invoking an automated assistant in a multitasking command associated with a plurality of different agent modules.
FIG. 6 is a block diagram of an example computer system.
Detailed Description
Fig. 1 illustrates a system diagram 100 including an automated assistant 104 capable of invoking a plurality of different agent modules in response to a command from a user. The automated assistant 104 may be accessed by a user through a client device 102 connected to a remote device 116 (such as a server device), which remote device 116 may host the automated assistant 104. The automated assistant 104 may receive text or audio input from the client device 102 and interpret the input in order to perform actions to assist the user. The automated assistant 104 can use the speech-to-text engine 106 to convert the audio input into text or other media that can be further processed by the automated assistant 104. The automated assistant 104 may further include a text parser engine 108, which text parser engine 108 may process text input or text converted from audio input, and convert the input into instructions for execution by the automated assistant 104 and/or one or more proxy modules.
In some implementations, the text parser engine 108 can determine whether the input corresponds to a multitasking command. When the text parser engine 108 determines that the input corresponds to a multitasking command, the agent selection engine 110 may be employed to identify an agent module that should be invoked to complete the multiple tasks involved in executing the command. The proxy module may be an application accessible to the client device 102 over a network and associated with a native application on the client device 102, or a website accessible to the client device 102. In some implementations, the proxy module can be a third party application provided by an entity that is different from the entity that provides the operating system of the client device 102 or other software on the client device 102. Alternatively, the proxy module may be a first party application provided by an entity that also provides an operating system or other software for the client device 102. The automation assistant 104 may access an index that associates agent modules with various functions and use the index to determine agent modules that are suitable for completing a multitasking command provided by a user.
The proxy module may be managed by a separate remote server and the automated assistant 104 may access the remote server through the network 130. When the automated assistant 104 identifies an agent module suitable for completing a multitasking command, the agent interaction engine 112 may dispatch a task to each identified agent module. The automation assistant 104 may invoke each of the agent modules by sending a signal to each server device hosting the agent module via the network 130 to perform one or more of the dispatched tasks. For example, the automated assistant 104 may access the first server 118, the second server 120, and the nth server 122, each hosting a first proxy module 124, a second proxy module 126, and an nth proxy module 128, respectively.
The agent interaction engine 112 may dispatch tasks to agent modules serially or in parallel, depending on the multitasking commands provided from the user to the automated assistant 104. For example, the proxy interaction engine 112 may provide tasks serially by first dispatching the first task to the first proxy module 124. In response to the first agent module 124 providing output to the automated assistant 104, the automated assistant 104 may provide a second task to the second agent module 126. Thereafter, in response to the second agent module providing output to the automated assistant 104, the automated assistant 104 may provide an nth task to the nth agent module 128. This process may continue until each of the plurality of tasks corresponding to the input command from the user is completed.
The proxy interaction engine 112 may serve tasks in parallel by simultaneously assigning multiple tasks to multiple different proxy modules (e.g., the first proxy module 124 and the second proxy module 126). For example, a multitasking command provided by a user may be parsed to determine specific tasks for completing the multitasking command, and at least two of the tasks may be simultaneously dispatched to separate agent modules. In some implementations, outputs may be received from a separate agent module and used to dispatch another task to another agent module.
In some implementations, the output can be provided from one or more of these proxy modules and processed by the proxy interaction engine 112. The output may correspond to a task completion indicator that includes information related to the task or a request for more information to complete the task. For example, an agent module that has been tasked may query the automated assistant 104 for additional information, and the automated assistant 104 may determine whether to obtain the additional information from a user or from a separate agent module. If the automated assistant 104 determines that additional information should be obtained from the user, the automated assistant 104 can cause the request to be provided at the automated assistant interface 114 of the client device 102. The request may be an audible output or a text output that queries the user for additional information. When a user provides additional information to the automated assistant 104, the automated assistant 104 may treat the additional information as input that is processed and thereafter provided to the agent module that requested the information and/or any other agent module that may need the information. In some implementations, the proxy module may receive input from an automated assistant or another proxy module and invoke a separate proxy module with parameters for completing a particular sub-task. In this way, the agent module may at least temporarily "boot" the interaction, while the automated assistant acts as an intermediary.
In some implementations, the automated assistant 104 can determine that the separate agent module is better suited to provide additional information to the agent module requesting the additional information. In such a case, the agent selection engine 110 may identify the agent module most suitable for obtaining additional information therefrom. The identified agent module may then be queried by the automated assistant 104 for additional information and cause the identified agent module to send the additional information to the automated assistant 104 and/or to request the agent module.
FIG. 2 illustrates a method 200 for providing commands to an automated assistant, the method 200 causing the automated assistant to invoke a plurality of different agent modules to perform different tasks in order to complete the commands. The method 200 may be performed by a client device, a server device, and/or any other apparatus suitable for controlling an automated assistant. The method 200 may include a block 202 of determining that content of the natural language input provided to the automated assistant interface includes a multitasking command. For example, the command may be a request to order ingredients, such as "assistant, please order ingredients for my tai-type fried flour recipe". The command may be parsed by an automated assistant and the phrase "order ingredients" may be identified as a multitasking command. The multitasking command may be a command configured by a user at the automated assistant interface or by the automated assistant based on previous interactions between the user and the automated assistant. In some implementations, the multitasking command may be preconfigured based on interactions between another user and the automated assistant or between multiple users and multiple automated assistants. For example, an automated assistant may access historical interactions between another user and another automated assistant to identify multi-tasking commands that may be of interest to the user. The automated assistant may then associate the multitasking command with the user and allow the user to invoke the automated assistant to perform various tasks when the multitasking command is received.
In response to receiving the multitasking command, the automated assistant may identify one or more proxy modules suitable for performing a plurality of tasks associated with the multitasking command at block 204. For example, a multitasking command of "order ingredients" may be associated with a spice order agent module, a product order agent module, and a restaurant agent module. Each of the agent modules may be identified in an index that is accessible by the automated assistant and includes a correlation between the multitasking command and the agent module. Whenever a user makes a selection to have a set of commands stored as a single multitasking command understood by the automated assistant, the automated assistant can manage the index and add the multitasking command.
At block 206, at least one proxy module may be invoked for performing at least one task of the plurality of tasks. For example, a spice ordering agent that may be associated with a website for ordering spices may be invoked and queried by an automated assistant to identify food ingredients available through the spice ordering agent module. The agent module may respond to the automated assistant with an indication that certain ingredients (e.g., vinegar and soy sauce) are available, and the automated assistant may respond to a request to order the ingredients. The ordering of these ingredients may mark completion of at least some of the plurality of tasks, and the automated assistant may then identify any remaining tasks for completion at block 208. If there are no tasks left, at block 210, the automated assistant may provide an output indicating that the multitasking command has been completed by the automated assistant (e.g., "your Tai-style breading ingredient has been ordered").
If there are remaining tasks of the multitasking command to be completed, block 206 may be repeated and another proxy module may be invoked for executing one or more of the remaining tasks of the multitasking command. For example, the automated assistant may determine that there are remaining ingredients (e.g., basil, rice flour, peanut, etc.) to order and invoke the product ordering agent module for ordering the remaining ingredients. This may be a series of call procedures where the proxy module is called one after the other.
In some implementations, each agent module may be invoked simultaneously, and the responses from the agent modules may be used to determine how to continue to interact with the user and/or agent module. For example, each of the flavor order agent, product order agent, and restaurant agent may be invoked simultaneously and assigned the task of reporting whether they can provide all ingredients. If the proxy modules can cooperatively provide all of the ingredients of the Taiwan powder, each proxy module can be tasked with providing certain ingredients. However, if the agent module is unable to complete one or more tasks (e.g., an ingredient order), the automated assistant may query the user as to how to proceed or dynamically alter the task.
The tasks may be dynamically altered by the automated assistant in response to feedback from the agent module and/or the user. For example, if at least one of the ingredients is not available to the broker module, the automated assistant may change the task from ordering individual ingredients to ordering takeaway orders from the restaurant broker module. This decision by the automated assistant may be preconfigured by the user or based on the user's past activity (e.g., the user previously attempted to order the Taiwan fried flour ingredients via the automated assistant, but then defaults to order the Taiwan fried flour take-out). It should be noted that in some embodiments and/or situations, task changes and dispatches are performed in the background through the automated assistant and/or any agent modules invoked by the automated assistant. In this way, the user may simply provide the command "assistant, order the Taiwan fried flour ingredients" and receive output from the automated assistant, such as "good, ingredients are ordered" or "good, i ordered you for the Taiwan fried flour take-out because ingredients are not available. This eliminates the need for the user to review each ingredient and prevents the automated assistant from having to process multiple different commands for each ingredient, thereby conserving computing resources.
FIG. 3 illustrates a method 300 for dynamically modifying tasks to be dispatched to one or more proxy modules based on feedback received from a user or the proxy modules. The method 300 may be performed by a client device, a server device, and/or any other device suitable for controlling an automated assistant. The method 300 may include a block 302 of determining that content of natural language input provided by an automated assistant includes a multitasking command. For example, the natural language input may be a spoken phrase from the user, such as "assistant," please plan to go with my friends overnight. The automated assistant may convert the natural language input into text and recognize the multitasking command within the text. A multitasking command (e.g., "plan night out with my friends") may be a command cooperatively configured by a user and an automated assistant.
At block 304, the automated assistant may identify a proxy module suitable for performing a plurality of tasks associated with the multitasking command. The proxy module may be an application that is loaded onto a client device associated with the user or otherwise accessible to the automated assistant over a network (e.g., the internet). Each of the agent modules may be associated with a task to be performed in order to complete the multitasking command. For example, the multitasking command "plan with my friends overnight" may be associated with a social networking agent, a calendar agent, and/or a restaurant agent. The social network proxy module may be associated with a task that at least identifies friends of the user; the calendar agent module may be associated with a task that at least identifies when a friend is idle; and the restaurant agent module may be associated with a task that at least identifies restaurants to be excluded.
At block 306, at least one of the proxy modules may be invoked for performing at least one of the plurality of tasks. For example, a social networking agent module may be invoked and the automated assistant may use the agent module to identify a friend to invite to join a night out planned by the automated assistant. The automated assistant may query the social network proxy module for, for example, how many friends of the user live in the same city of the user. In response, the agent module may provide a list of friends of the user that live in the same city as the user. At block 308, a determination is made as to whether the output received from the proxy module is feedback. If the output (e.g., a list of friends) is not feedback, the method 300 may proceed to block 318. At block 318, a determination is made as to whether there are other tasks to be performed to complete the multitasking command. If there are no more tasks to complete, the method 300 may proceed to block 320 where an output confirming completion of the task is provided to the user by the automated assistant. However, if other tasks remain to be completed, block 306 may be repeated.
Block 306 may be repeated in order to perform another task (e.g., identify when a friend is idle) of the plurality of tasks associated with the plurality of tasks provided by the user (e.g., planning to go with the friend). For example, when the calendar agent module performs a task that identifies when a friend is idle, the calendar agent module may provide feedback indicating that all friends are available during the upcoming weekend, except for one friend. At block 308, a determination is made that feedback is provided from an agent module (e.g., calendar agent module). Feedback may be provided to the automated assistant and the automated assistant may determine whether a response should be provided from the user or another agent to the agent module. For example, when the calendar agent module communicates to the automated assistant that one friend from the group of friends identified by the social network proxy module is not idle, then the automated assistant may query the user for feedback at block 312. In particular, the automated assistant may query the user as to whether it is possible to proceed with planning a night out without including unoccupied friends.
Thereafter, at block 314, a response may be received from the user. The user may indicate in the response that it is not possible to proceed without inviting friends, and at block 316, an agent module (e.g., calendar agent module) for performing tasks associated with the user response may be identified. For example, the automated assistant may receive a response from the user and provide a supplemental task to the calendar agent module to identify a time when at least no friends will be idle. Block 310 may be repeated if the calendar agent module provides an output corresponding to the feedback. Otherwise, the method 300 may proceed to block 318 to determine whether other tasks are to be performed. If there are no other tasks to perform, at block 320, an output may be provided by the automated assistant to confirm completion of the command.
If there are other tasks (e.g., using the restaurant proxy module to identify restaurants to go to), the method 300 may proceed to block 306. At block 306, restaurant reservations may be made for friends at the dates provided by the calendar agent module. Thereafter, the method 300 may proceed to 308. If no other feedback is provided and no other tasks are to be performed, the method 300 may terminate at block 320, with an output to the user confirming completion of the command at block 320.
In some implementations, the method 300 enables a user and/or agent module to provide feedback to an automated assistant during execution of a multitasking command. Feedback may be provided from the agent module to the automated assistant, and the automated assistant may provide a response back to the same or a separate agent module. Alternatively, feedback may be provided from the agent module to the automated assistant, and the automated assistant may query the user for a response, which may be provided back to the same agent module or a separate agent module. In this way, the user does not have to personally identify each suitable agent module to the automated assistant and/or individually control each agent module. Rather, these steps may be performed by an automated assistant, which may conserve computing resources in view of less speech-to-text processing required when the user is providing fewer commands.
Fig. 4 illustrates a method 400 for configuring a multitasking command for invoking a plurality of different proxy modules via an automated assistant. The method 400 may be performed by a client device, a server device, and/or any other apparatus suitable for controlling an automated assistant. The method 400 may include a block 402 of identifying a plurality of different natural language commands received by at least one automated assistant interface. For example, natural language commands or text commands such as "book a table of nearby restaurants", "find places for dinner to drink", and "send invitation to my girlfriend" may be said. Each of these natural language inputs may be associated with a particular task undertaken by an automated assistant that may dispatch each task to the appropriate agent module.
At block 404, a query is provided to the user as to whether to associate a plurality of different natural language commands with the multitasking command. The multitasking command may be a natural language input, such as an audible or text word or phrase, that may be provided to the automated assistant interface to perform a number of different tasks. The multitasking command may be provided by a user or generated by an automated assistant. For example, the user may be operating a Graphical User Interface (GUI) corresponding to the automated assistant interface and typing each of a plurality of different natural language commands. The automated assistant may provide a query to the user as to whether the user would like to associate a plurality of different natural language commands with a multitasking command, which may also be provided by the user at the GUI. The multitasking command may also be configured through language interactions between the automated assistant and the user. For example, during a week, the user may provide various different natural language commands associated with an appointment that the user is planning at night. The automated assistant can identify commonalities between different natural language commands and, in response, provide a query to the user regarding associating the different natural language commands with the multitasking command. Commonalities may be the content of the natural language commands (e.g., mention of appointment nights in each command), the time or location associated with the natural language commands (e.g., mention of event time or location in each command), the time or location associated with the user when the commands are provided (e.g., the user plans appointment nights after work on a week), and/or any other commonalities that may be associated with the natural language commands. For example, the commonality may be that each natural language command is provided within a threshold time of each other. Alternatively, the commonality may be that all natural language commands are provided and parsed within a total threshold period of time.
At block 406, a response may be received from the user confirming that a plurality of different natural language commands should be associated with the multitasking command. The user may provide such confirmation through a GUI (e.g., by typing a multitasking command "schedule appointment night") or through a spoken command to an automated assistant. For example, the user may ask for an appointment night task and command by speaking "assistant: ' planning an appointment night ' associated ' to communicate the multitasking command to the automated assistant. In response, at block 408, the automated assistant may determine a proxy module associated with the plurality of different natural language commands to invoke to perform the task in response to receiving the multitasking command. For example, the previous user may have provided a command "book a table of nearby restaurants". The commands may be processed by the automated assistant and converted into tasks that are dispatched to the restaurant agent module. In a similar manner, the automated assistant may compile a list of tasks from a plurality of different natural language commands. Thereafter, at block 410, the automated assistant may store an identifier for the agent module and/or task in association with the multitasking command (e.g., "planning an appointment night"). In this way, the user can invoke multiple proxy modules via the automated assistant to perform different tasks. This may streamline various interactions between the user and the automated assistant, saving user time and conserving computing resources that may be utilized by the automated assistant.
FIG. 5 provides a diagram 500 illustrating an example of a user 502 invoking an automated assistant with a multitasking command associated with a plurality of different agent modules. In particular, diagram 500 illustrates an example in which a user 502 requests an automated assistant to plan a business trip using a multitasking command "assistant, plan my business trip". The multitasking command may be provided as spoken user input 508 to a client device (such as mobile device 504 or assistant device 506) and the client device may send the spoken user input 508 to a remote server hosting an automated assistant application over network 512. The automated assistant application may determine that the phrase "plan my business trip" corresponds to the multitasking command and identify an agent module associated with completing the multitasking command.
The automated assistant can access a storage device that includes an index that provides a correlation between the multitasking command and an agent module that can be used to complete the multitasking command. For example, the index may include an entry identifying the multitasking command "plan My business trip" and a corresponding entry identifying the agent module that may be employed to complete the subtasks of the multitasking command. The agent modules may include a calendar agent module 516, a rental car agent module 520, and a hotel agent module 524. The automated assistant may further identify, based on the index, tasks involved in completing the multitasking command. Such tasks may include: details of business travel, booking car rentals, and booking hotels are provided in a calendar managed by the user.
In some implementations, each task can be dispatched to each of the agent modules in parallel, serially, or a combination thereof. For example, the automated assistant may communicate with the first remote server 514 to dispatch tasks that use the calendar agent module to find details of business travel. In response to receiving details of the business trip from the calendar agent module 516, the automated assistant may serve the task of booking a car rental and booking a hotel. In particular, the automated assistant may communicate with the second remote server 518 to dispatch the task of booking a rental car to the rental car agency module 520, and with the third remote server 522 to dispatch the task of booking a hotel. Each of the tasks performed by the rental car agency module 520 and the hotel agency module 524 may be done simultaneously in order to save time.
In some implementations, the automated assistant can collect information from one agent module and provide the information to another agent module. For example, the calendar agent module may complete a task of providing details of a business trip and provide the details to an automated assistant. The automated assistant may parse the details and identify details that will be relevant to the remaining tasks. Details may include the destination of the business trip and the date of the business trip. When the automation assistant dispatches tasks to the rental car agency module 520 and the hotel agency module 524, the automation assistant may include a location and date. In this way, the user 502 need not be queried to provide such details, and the automated assistant can save computing resources by not having to process unnecessary natural language input from the user 502.
In some implementations, the automation assistant can use the environmental data (such as the current location of the user 502) to modify the tasks to be dispatched to the agent module. For example, the automated assistant may determine a distance between the current location of the user 502 and the destination of the business trip. The rental car agency module 520 may receive the distance information from the automated assistant and query the automated assistant as to whether the user would like to reserve an electric car because the distance is below a certain threshold. In response, the automated assistant may generate a query as output 510 for the user 502 (e.g., do you like to rent an electric car. Alternatively, the automated assistant may communicate a query from the rental car agency module 520 to the user, allowing the automated assistant to act as an intermediary between the user and the rental car agency module 520. If user 502 provides a response confirming an electric vehicle appointment (e.g., "yes," please), then the automated assistant may communicate to rental car agency module 520 that user 502 wants an electric vehicle. The rental car agency module 520 may then reserve the first type of electric car for the user 502 to travel to the destination of the business trip.
In some implementations, feedback from the agent module may be provided and used by the automated assistant to determine whether a previously performed task should be repeated. For example, the automated assistant may communicate to the hotel agent module 524 that the user 502 has subscribed to the first type of electric car. The first type of electric vehicle may include a charging outlet not supported by a charging station at the hotel reserved by the hotel agent module 524. In response to the hotel agent module 524 determining such incompatibility, the hotel agent module 524 may provide an indication to the automated assistant 104 of a first type that identifies the first type of electric vehicle as not supported by the charger at the hotel. In response, the automated assistant 104 may dispatch a supplemental task to the rental car agency module 520 to modify the reservation of the second type of electric car to reserve for charging station support at the hotel. In response to the rental car agency module 520 reserving the second type of electric car, the automated assistant can instruct the hotel agency module 524 to reserve the hotel and provide an output 510 to the user 502 indicating that the travel has been reserved. This process allows resolution of conflicts between agents to be performed by the agent module with little or no interaction with the user 502. In this way, the user 502 is able to perform other actions while the automated assistant coordinates completion of tasks in the background.
Fig. 6 is a block diagram 600 of an example computer system 610. Computer system 610 typically includes at least one processor 614 that communicates with a number of peripheral devices via a bus subsystem 612. These peripheral devices may include a storage subsystem 624 (including, for example, a memory subsystem 625 and a file storage subsystem 626), a user interface output device 620, a user interface input device 622, and a network interface subsystem 616. Input and output devices allow users to interact with computer system 610. Network interface subsystem 616 provides an interface to external networks and couples to corresponding interface devices in other computer systems.
The user interface input device 622 may include a keyboard, a pointing device such as a mouse, trackball, touch pad, or graphics tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computer system 610 or onto a communication network.
The user interface output device 620 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a viewable image. The display subsystem may also provide for non-visual display, for example, via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computer system 610 to a user, or to another machine or computer system.
Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, the storage subsystem 624 may include logic to perform selected aspects of the methods 200, 300, and/or 400 and/or to implement one or more of the automated assistant 104, the speech-to-text engine 106, the text parser engine 108, the agent selection engine 110, the agent interaction engine 1 12, a client device, a server device, a remote device, and/or any other apparatus or process discussed herein.
These software modules are typically executed by processor 614 alone or in combination with other processors. Memory 625 used in storage subsystem 624 may include a number of memories, including a main Random Access Memory (RAM) 630 for storing instructions and data during program execution and a Read Only Memory (ROM) 632 with fixed instructions stored. File storage subsystem 626 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive, and associated removable media, CD-ROM drive, optical drive, or removable media cartridge. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 626 in storage subsystem 624, or in other machines accessible by processor 614.
Bus subsystem 612 provides a mechanism for letting the various components and subsystems of computer system 610 communicate with each other as intended. Although bus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple buses.
Computer system 610 may be of various types including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computer system 610 depicted in FIG. 6 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of computer system 610 may have more or fewer components than the computer system depicted in FIG. 6.
In situations where the system described herein collects personal information about a user (or often referred to herein as a "participant") or may utilize personal information, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social actions or activities, profession, user preferences, or the user's current geographic location) or to control whether and/or how content that may be more relevant to the user is received from a content server. In addition, some data may be processed in one or more ways before it is stored or used so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized (such as to a city, zip code, or state level) where geographic location information is obtained such that a particular geographic location of the user cannot be determined. Thus, the user may control how information is collected and/or used with respect to the user.
Although several embodiments have been described and illustrated herein, various other means and/or structures for performing a function and/or obtaining a result and/or one or more of the advantages described herein may be utilized and each such change and/or modification is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is therefore to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. Furthermore, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, any combination of two or more such features, systems, articles, materials, kits, and/or methods is included within the scope of the present disclosure.
Claims (29)
1. A method implemented by one or more processors, the method comprising:
a correlation between the first spoken utterance and the second spoken utterance during a previous interaction between the user and the automated assistant via the automated assistant interface is determined,
wherein determining the correlation is based on a relationship between the first spoken utterance and the second spoken utterance in each of the previous interactions,
wherein the first spoken utterance causes the automated assistant to initiate performance of a first task, comprising: causing the automated assistant to initiate execution of the first task when provided during the previous interaction, and
wherein the second spoken utterance causes the automated assistant to initiate performance of a second task, comprising: cause the automated assistant to initiate execution of the second task when provided during the previous interaction;
based on determining the correlation between the first spoken utterance and the second spoken utterance, prompting the user with respect to a multitasking command,
wherein the multitasking command corresponds to the first task and the second task, and
wherein the prompting is performed without the user explicitly requesting the automated assistant to initiate generation of the multitasking command; and
After prompting the user with respect to the multitasking command:
determining that content of natural language input provided to the automated assistant interface identifies the multitasking command;
initiating execution of the first task and the second task in the multitasking command, wherein initiating is responsive to determining content of the natural language input to identify the multitasking command; and
providing a response output to the user based on the multitasking command.
2. The method of claim 1, wherein determining the correlation between the first spoken utterance and the second spoken utterance comprises:
determining that a temporal relationship between the first spoken utterance and the second spoken utterance indicates that the first spoken utterance and the second spoken utterance are provided within a threshold period of time.
3. The method of claim 1, wherein determining a correlation between the first spoken utterance and the second spoken utterance comprises:
determining that a geographic relationship between the first spoken utterance and the second spoken utterance indicates that the first spoken utterance and the second spoken utterance are provided at a particular location.
4. The method of claim 1, wherein the response output comprises a query for the user, and the method further comprises:
Receiving, at the automated assistant interface, additional input from the user in response to the query; and
causing a separate application to process the additional input from the user.
5. The method of claim 1, wherein determining the correlation between the first spoken utterance and the second spoken utterance is performed before the user provides the natural language input identifying the multitasking command.
6. The method according to claim 1,
wherein the natural language input is a single spoken natural language input and the content of the single spoken natural language input is different from other natural language content provided verbally to the automated assistant by the user during one or more of the previous interactions.
7. A non-transitory computer-readable storage medium configured to store instructions that, when executed by one or more processors included in a computing device, cause the computing device to perform operations comprising:
a correlation between the first spoken utterance and the second spoken utterance during a previous interaction between the user and the automated assistant via the automated assistant interface is determined,
Wherein determining the correlation is based on a relationship between the first spoken utterance and the second spoken utterance in each of the previous interactions,
wherein the first spoken utterance causes the automated assistant to initiate performance of a first task, comprising: causing the automated assistant to initiate execution of the first task when provided during the previous interaction, and
wherein the second spoken utterance causes the automated assistant to initiate performance of a second task, comprising: cause the automated assistant to initiate execution of the second task when provided during the previous interaction;
based on determining the correlation between the first spoken utterance and the second spoken utterance, recognition can be provided as a multitasking command to input to the automated assistant,
wherein the multitasking command corresponds to the first task and the second task, and
wherein the identifying is performed without the user explicitly requesting the automated assistant to initiate the identification of the multitasking command; and
after prompting the user with respect to the multitasking command:
determining that content of natural language input provided to the automated assistant interface identifies the multitasking command; and
The execution of the first task and the second task in the multitasking command is initiated, wherein initiating is responsive to determining content of the natural language input to identify the multitasking command.
8. The non-transitory computer-readable storage medium of claim 7, wherein determining the correlation between the first spoken utterance and the second spoken utterance comprises:
determining that a temporal relationship between the first spoken utterance and the second spoken utterance indicates that the first spoken utterance and the second spoken utterance are provided within a threshold period of time.
9. The non-transitory computer-readable storage medium of claim 7, wherein determining a correlation between the first spoken utterance and the second spoken utterance comprises:
determining that a geographic relationship between the first spoken utterance and the second spoken utterance indicates that the first spoken utterance and the second spoken utterance are provided at a particular location.
10. The non-transitory computer-readable storage medium of claim 7, wherein the operations further comprise:
providing a response output to the user based on the multitasking command.
11. The non-transitory computer-readable storage medium of claim 10, wherein the response output comprises a query for the user, and wherein the operations further comprise:
receiving another input from the user at the automated assistant interface in response to the query; and
causing a separate application to process the input from the user.
12. The non-transitory computer-readable storage medium of claim 7, wherein determining the correlation between the first spoken utterance and the second spoken utterance is performed before the user provides the natural language input identifying the multitasking command.
13. The non-transitory computer readable storage medium of claim 7,
wherein the natural language input is a single spoken natural language input and the content of the single spoken natural language input is different from other natural language content provided verbally to the automated assistant by the user during one or more of the previous interactions.
14. A computing device, comprising:
one or more processors, and
a memory configured to store instructions that, when executed by the one or more processors, cause the one or more processors to perform operations comprising:
A correlation between the first spoken utterance and the second spoken utterance during a previous interaction between the user and the automated assistant via the automated assistant interface is determined,
wherein determining the correlation is based on a relationship between the first spoken utterance and the second spoken utterance in each of the previous interactions,
wherein the first spoken utterance causes the automated assistant to initiate performance of a first task, comprising: causing the automated assistant to initiate execution of the first task when provided during the previous interaction, and
wherein the second spoken utterance causes the automated assistant to initiate performance of a second task, comprising: cause the automated assistant to initiate execution of the second task when provided during the previous interaction;
based on determining the correlation between the first spoken utterance and the second spoken utterance, prompting the user with respect to a multitasking command,
wherein the multitasking command corresponds to the first task and the second task, and
wherein the prompting is performed without the user explicitly requesting the automated assistant to initiate generation of the multitasking command; and
After prompting the user with respect to the multitasking command:
determining that content of natural language input provided to the automated assistant interface identifies the multitasking command;
initiating execution of the first task and the second task in the multitasking command, wherein initiating is responsive to determining content of the natural language input to identify the multitasking command, and
providing a response output to the user based on the multitasking command.
15. The computing device of claim 14, wherein determining the correlation between the first spoken utterance and the second spoken utterance comprises:
determining that a temporal relationship between the first spoken utterance and the second spoken utterance indicates that the first spoken utterance and the second spoken utterance are provided within a threshold period of time.
16. The computing device of claim 14, wherein determining a correlation between the first spoken utterance and the second spoken utterance comprises:
determining that a geographic relationship between the first spoken utterance and the second spoken utterance indicates that the first spoken utterance and the second spoken utterance are provided within a geographic region.
17. The computing device of claim 14, wherein determining a correlation between the first spoken utterance and the second spoken utterance comprises:
a plurality of different common conditions associated with both the first spoken utterance and the second spoken utterance are identified.
18. The computing device of claim 14, wherein the response output comprises a query for the user, and wherein the operations further comprise:
receiving additional input from the user at the automated assistant interface in response to the query; and
causing a separate application to process the additional input from the user.
19. The computing device of claim 14, wherein determining the correlation between the first spoken utterance and the second spoken utterance is performed before the user provides the natural language input identifying the multitasking command.
20. The computing device of claim 14,
wherein the natural language input is a single spoken natural language input and the content of the single spoken natural language input is different from other natural language content provided verbally to the automated assistant by the user during one or more of the previous interactions.
21. A method implemented by one or more processors, the method comprising:
identifying based on previous interactions between a user and an automated assistant via the automated assistant interface:
a first task occurrence, wherein during each of the first task occurrences, the automated assistant initiates execution of a first task based on a respective first input of the automated assistant by the user,
a second task occurrence, wherein during each of the second task occurrences, the automated assistant initiates execution of a second task based on a respective second input of the automated assistant by the user;
determining a correlation between the first task occurrence and the second task occurrence, wherein determining the correlation is based on a relationship between the first task occurrence and the second task occurrence in the previous interaction;
based on determining the correlation between the first task occurrence and the second task occurrence, prompting the user with respect to a multitasking command,
wherein the multitasking command, when initiated, causes execution of both the first task and the second task, and
Wherein the prompting is performed without the user explicitly requesting the automated assistant to initiate generation of the multitasking command;
receiving a confirmation input from the user in response to the prompt; and
assigning a multitasking command to both the first task and the second task in response to receiving the confirmation input from the user; and
after assigning the multitasking command to both the first task and the second task:
determining to initiate the multitasking command; and
in response to determining to initiate the multitasking command:
initiating execution of the multitasking command, comprising: and initiating execution of the first task and the second task in the multitasking command.
22. The method of claim 21, wherein determining to initiate the multitasking command comprises:
determining a spoken utterance provided via the automated assistant interface includes identifying natural language content of the multitasking command; and
responsive to determining that the spoken utterance includes identifying the natural language content of the multitasking command, determining to initiate the multitasking command.
23. The method of claim 21, wherein initiating execution of the multitasking command comprises: execution of the first task is initiated first, and then execution of the second task is initiated.
24. The method of claim 21, wherein initiating execution of the multitasking command comprises: execution of the first task is initiated in parallel with initiation of execution of the second task.
25. The method of claim 21, wherein determining the relationship based on which the correlation between the first and second task occurrences is based comprises a temporal relationship between the first and second task occurrences in the previous interaction.
26. The method of claim 25, wherein determining the relevance based on a relationship between the first task occurrence and the second task occurrence in the previous interaction comprises:
determining that in each of the previous interactions, a corresponding one of the first task occurrences and a corresponding one of the second task occurrences occur within a threshold time period of each other.
27. The method of claim 21, further comprising:
receiving a response output based on execution of the first task in response to initiating execution of the first task in the multitasking command; and
the response output is provided to the user.
28. The method of claim 27, wherein the response output comprises a query for the user, and further comprising:
receiving input from the user in response to the query via the automated assistant interface;
wherein initiating execution of the second task comprises: execution of the second task is initiated based on the input from the user.
29. The method of claim 21, further comprising:
receiving a response output based on execution of the first task in response to initiating execution of the first task in the multitasking command;
wherein initiating execution of the second task comprises: execution of the second task is initiated based on the response output.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/644,157 | 2017-07-07 | ||
US15/644,157 US10552204B2 (en) | 2017-07-07 | 2017-07-07 | Invoking an automated assistant to perform multiple tasks through an individual command |
PCT/US2018/040808 WO2019010236A1 (en) | 2017-07-07 | 2018-07-03 | Invoking an automated assistant to perform multiple tasks through an individual command |
CN201880038386.1A CN110720090B (en) | 2017-07-07 | 2018-07-03 | Invoking an automated assistant through individual commands to perform multiple tasks |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880038386.1A Division CN110720090B (en) | 2017-07-07 | 2018-07-03 | Invoking an automated assistant through individual commands to perform multiple tasks |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117130744A true CN117130744A (en) | 2023-11-28 |
Family
ID=63080516
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880038386.1A Active CN110720090B (en) | 2017-07-07 | 2018-07-03 | Invoking an automated assistant through individual commands to perform multiple tasks |
CN202310994057.7A Pending CN117130744A (en) | 2017-07-07 | 2018-07-03 | Invoking an automated assistant through individual commands to perform multiple tasks |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880038386.1A Active CN110720090B (en) | 2017-07-07 | 2018-07-03 | Invoking an automated assistant through individual commands to perform multiple tasks |
Country Status (4)
Country | Link |
---|---|
US (4) | US10552204B2 (en) |
EP (2) | EP3519955B1 (en) |
CN (2) | CN110720090B (en) |
WO (1) | WO2019010236A1 (en) |
Families Citing this family (28)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10845822B2 (en) | 2014-10-31 | 2020-11-24 | Clearpath Robotics Inc. | Systems and methods for utilizing fleets of robots |
EP3418881B1 (en) * | 2016-02-18 | 2020-04-01 | Sony Corporation | Information processing device, information processing method, and program |
US10585440B1 (en) | 2017-01-23 | 2020-03-10 | Clearpath Robotics Inc. | Systems and methods for using human-operated material-transport vehicles with fleet-management systems |
US20210312543A1 (en) | 2017-04-05 | 2021-10-07 | State Farm Mutual Automobile Insurance Company | Systems and methods for analyzing vehicle financing via blockchain |
US10552204B2 (en) | 2017-07-07 | 2020-02-04 | Google Llc | Invoking an automated assistant to perform multiple tasks through an individual command |
US10762161B2 (en) * | 2017-08-08 | 2020-09-01 | Accenture Global Solutions Limited | Intelligent humanoid interactive content recommender |
US11100922B1 (en) * | 2017-09-26 | 2021-08-24 | Amazon Technologies, Inc. | System and methods for triggering sequences of operations based on voice commands |
EP3474160A1 (en) * | 2017-10-17 | 2019-04-24 | Palantir Technologies Inc. | Data analytic systems |
WO2019079888A1 (en) * | 2017-10-23 | 2019-05-02 | Clearpath Robotics Inc. | Systems and methods for deploying groups of self-driving material-transport vehicles |
US10546584B2 (en) * | 2017-10-29 | 2020-01-28 | International Business Machines Corporation | Creating modular conversations using implicit routing |
US10802793B2 (en) * | 2017-11-22 | 2020-10-13 | Toyota Motor Engineering & Manufacturing North America, Inc. | Vehicle virtual assistance systems for expediting a meal preparing process |
US11348576B1 (en) * | 2017-12-06 | 2022-05-31 | Amazon Technologies, Inc. | Universal and user-specific command processing |
US11200760B2 (en) | 2018-01-22 | 2021-12-14 | Clearpath Robotics Inc. | Systems and methods for measuring fleets of self-driving industrial vehicles |
WO2019144222A1 (en) | 2018-01-24 | 2019-08-01 | Clearpath Robotics Inc. | Systems and methods for maintaining vehicle state information |
US11256270B2 (en) | 2018-02-07 | 2022-02-22 | Clearpath Robotics Inc. | Communication systems for self-driving vehicles, and methods of providing thereof |
US10832002B2 (en) * | 2018-05-08 | 2020-11-10 | International Business Machines Corporation | System and method for scoring performance of chatbots |
US20190348033A1 (en) * | 2018-05-10 | 2019-11-14 | Fujitsu Limited | Generating a command for a voice assistant using vocal input |
EP3682345B1 (en) | 2018-08-07 | 2021-11-24 | Google LLC | Assembling and evaluating automated assistant responses for privacy concerns |
US10629191B1 (en) * | 2019-06-16 | 2020-04-21 | Linc Global, Inc. | Methods and systems for deploying and managing scalable multi-service virtual assistant platform |
EP4037328A4 (en) * | 2019-09-27 | 2023-08-30 | LG Electronics Inc. | Display device and artificial intelligence system |
US11636438B1 (en) | 2019-10-18 | 2023-04-25 | Meta Platforms Technologies, Llc | Generating smart reminders by assistant systems |
US11567788B1 (en) | 2019-10-18 | 2023-01-31 | Meta Platforms, Inc. | Generating proactive reminders for assistant systems |
KR20210059978A (en) * | 2019-11-18 | 2021-05-26 | 엘지전자 주식회사 | Method for Processing User Input of Voice Assistant |
JP7287333B2 (en) * | 2020-04-06 | 2023-06-06 | トヨタ自動車株式会社 | Control device, program, and information processing method |
US11416290B2 (en) | 2020-05-28 | 2022-08-16 | Microsoft Technology Licensing, Llc | Semi-autonomous intelligent task hub |
US11488597B2 (en) * | 2020-09-08 | 2022-11-01 | Google Llc | Document creation and editing via automated assistant interactions |
CN113192490A (en) * | 2021-04-14 | 2021-07-30 | 维沃移动通信有限公司 | Voice processing method and device and electronic equipment |
US20230393872A1 (en) * | 2022-06-03 | 2023-12-07 | Apple Inc. | Digital assistant integration with system interface |
Family Cites Families (42)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5748191A (en) * | 1995-07-31 | 1998-05-05 | Microsoft Corporation | Method and system for creating voice commands using an automatically maintained log interactions performed by a user |
US7447637B1 (en) * | 1998-12-23 | 2008-11-04 | Eastern Investments, Llc | System and method of processing speech within a graphic user interface |
US6816837B1 (en) * | 1999-05-06 | 2004-11-09 | Hewlett-Packard Development Company, L.P. | Voice macros for scanner control |
US6895380B2 (en) * | 2000-03-02 | 2005-05-17 | Electro Standards Laboratories | Voice actuation with contextual learning for intelligent machine control |
US20020191754A1 (en) * | 2001-05-30 | 2002-12-19 | Liu Zhenyu L. | Remote voice interactive computer systems |
ATE410768T1 (en) * | 2003-08-29 | 2008-10-15 | Johnson Controls Tech Co | SYSTEM AND METHOD FOR OPERATING A VOICE RECOGNITION SYSTEM IN A VEHICLE |
TWI298844B (en) * | 2005-11-30 | 2008-07-11 | Delta Electronics Inc | User-defines speech-controlled shortcut module and method |
US20070124147A1 (en) * | 2005-11-30 | 2007-05-31 | International Business Machines Corporation | Methods and apparatus for use in speech recognition systems for identifying unknown words and for adding previously unknown words to vocabularies and grammars of speech recognition systems |
US20080109292A1 (en) | 2006-11-03 | 2008-05-08 | Sap Ag | Voice-enabled workflow item interface |
JP4321633B2 (en) * | 2007-07-12 | 2009-08-26 | 株式会社デンソー | Collision mitigation device |
US8165886B1 (en) * | 2007-10-04 | 2012-04-24 | Great Northern Research LLC | Speech interface system and method for control and interaction with applications on a computing system |
US8046226B2 (en) * | 2008-01-18 | 2011-10-25 | Cyberpulse, L.L.C. | System and methods for reporting |
US20100123834A1 (en) * | 2008-11-14 | 2010-05-20 | Apple Inc. | System and Method for Capturing Remote Control Device Command Signals |
US8407057B2 (en) * | 2009-01-21 | 2013-03-26 | Nuance Communications, Inc. | Machine, system and method for user-guided teaching and modifying of voice commands and actions executed by a conversational learning system |
US8656346B2 (en) | 2009-02-18 | 2014-02-18 | Microsoft Corporation | Converting command units into workflow activities |
US10540976B2 (en) * | 2009-06-05 | 2020-01-21 | Apple Inc. | Contextual voice commands |
US8943094B2 (en) * | 2009-09-22 | 2015-01-27 | Next It Corporation | Apparatus, system, and method for natural language processing |
US8812298B1 (en) * | 2010-07-28 | 2014-08-19 | Wolfram Alpha Llc | Macro replacement of natural language input |
WO2013101051A1 (en) * | 2011-12-29 | 2013-07-04 | Intel Corporation | Speech recognition utilizing a dynamic set of grammar elements |
US8635230B2 (en) * | 2012-01-26 | 2014-01-21 | International Business Machines Corporation | Display of information in computing devices |
US10417037B2 (en) * | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US9809185B2 (en) * | 2012-09-04 | 2017-11-07 | Ford Global Technologies, Llc | Method and apparatus for subjective command control of vehicle systems |
US9659298B2 (en) * | 2012-12-11 | 2017-05-23 | Nuance Communications, Inc. | Systems and methods for informing virtual agent recommendation |
US9922639B1 (en) * | 2013-01-11 | 2018-03-20 | Amazon Technologies, Inc. | User feedback for speech interactions |
US9875494B2 (en) * | 2013-04-16 | 2018-01-23 | Sri International | Using intents to analyze and personalize a user's dialog experience with a virtual personal assistant |
CN103324436B (en) * | 2013-05-24 | 2017-03-15 | 东莞宇龙通信科技有限公司 | A kind of task processing method and device |
US10162813B2 (en) * | 2013-11-21 | 2018-12-25 | Microsoft Technology Licensing, Llc | Dialogue evaluation via multiple hypothesis ranking |
US10534623B2 (en) * | 2013-12-16 | 2020-01-14 | Nuance Communications, Inc. | Systems and methods for providing a virtual assistant |
US20150212676A1 (en) * | 2014-01-27 | 2015-07-30 | Amit Khare | Multi-Touch Gesture Sensing and Speech Activated Radiological Device and methods of use |
US9582246B2 (en) * | 2014-03-04 | 2017-02-28 | Microsoft Technology Licensing, Llc | Voice-command suggestions based on computer context |
US9489171B2 (en) * | 2014-03-04 | 2016-11-08 | Microsoft Technology Licensing, Llc | Voice-command suggestions based on user identity |
US9966065B2 (en) * | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
TWI542155B (en) * | 2014-07-02 | 2016-07-11 | 瑞昱半導體股份有限公司 | Clock generator, communication device and sequential clock gating circuit |
FR3024267B1 (en) * | 2014-07-25 | 2017-06-02 | Redlime | METHODS FOR DETERMINING AND CONTROLLING A CONTROL EQUIPMENT, DEVICE, USE AND SYSTEM IMPLEMENTING SAID METHODS |
US9767794B2 (en) * | 2014-08-11 | 2017-09-19 | Nuance Communications, Inc. | Dialog flow management in hierarchical task dialogs |
US9965464B2 (en) * | 2014-12-05 | 2018-05-08 | Microsoft Technology Licensing, Llc | Automatic process guidance |
US9583097B2 (en) * | 2015-01-30 | 2017-02-28 | Google Inc. | Dynamic inference of voice command for software operation from help information |
US20180121215A1 (en) * | 2015-04-23 | 2018-05-03 | Fluent Systems, Inc. | Dynamic and customizable user interface platform |
US9674361B2 (en) * | 2015-06-29 | 2017-06-06 | Genesys Telecommunications Laboratories, Inc. | System and method for intelligent task management in a workbin |
KR102429260B1 (en) * | 2015-10-12 | 2022-08-05 | 삼성전자주식회사 | Apparatus and method for processing control command based on voice agent, agent apparatus |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10552204B2 (en) | 2017-07-07 | 2020-02-04 | Google Llc | Invoking an automated assistant to perform multiple tasks through an individual command |
-
2017
- 2017-07-07 US US15/644,157 patent/US10552204B2/en active Active
-
2018
- 2018-07-03 WO PCT/US2018/040808 patent/WO2019010236A1/en unknown
- 2018-07-03 CN CN201880038386.1A patent/CN110720090B/en active Active
- 2018-07-03 EP EP18749665.8A patent/EP3519955B1/en active Active
- 2018-07-03 EP EP22203942.2A patent/EP4145285A1/en active Pending
- 2018-07-03 CN CN202310994057.7A patent/CN117130744A/en active Pending
-
2019
- 2019-12-13 US US16/713,036 patent/US11494225B2/en active Active
-
2022
- 2022-11-02 US US17/979,556 patent/US11861393B2/en active Active
-
2023
- 2023-11-21 US US18/516,290 patent/US20240086230A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
EP3519955B1 (en) | 2022-12-07 |
WO2019010236A1 (en) | 2019-01-10 |
EP4145285A1 (en) | 2023-03-08 |
US11494225B2 (en) | 2022-11-08 |
CN110720090B (en) | 2023-08-25 |
US20230056213A1 (en) | 2023-02-23 |
EP3519955A1 (en) | 2019-08-07 |
US20240086230A1 (en) | 2024-03-14 |
US20190012198A1 (en) | 2019-01-10 |
CN110720090A (en) | 2020-01-21 |
US11861393B2 (en) | 2024-01-02 |
US10552204B2 (en) | 2020-02-04 |
US20200117502A1 (en) | 2020-04-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110720090B (en) | Invoking an automated assistant through individual commands to perform multiple tasks | |
US9798983B2 (en) | Enterprise resource search and reservation system | |
CN110892382B (en) | Systems, methods, and apparatus for restoring dialog sessions via an automated assistant | |
US11157879B2 (en) | System and methods for facilitating scheduling of event or meeting | |
JP6883050B2 (en) | Automatically generate prompts and analyze user responses to prompts to determine the entity for an action and to perform one or more compute actions related to the action and entity. | |
EP3622459B1 (en) | Method and apparatus for generating workflow | |
CN112136124A (en) | Dependency graph conversation modeling for human-machine conversation sessions with computer-implemented automated assistants | |
US20100174578A1 (en) | Interface for Project and Task Submission for Automated Delegation | |
US20240119933A1 (en) | Automated assistant invocation of second interactive module using supplemental data provided by first interactive module | |
US20130097095A1 (en) | Mobile Transport Tendering | |
US20110320235A1 (en) | Computer controlled meetings calendar scheduler based upon importance of meeting attributes | |
US20150199651A1 (en) | Integrated Online Time and Place Management | |
US20240007422A1 (en) | Chat bot control device, chat bot control method, and chat bot control device system | |
EP3942399B1 (en) | Automated assistant for generating, in response to a request from a user, application input content using application data from other sources | |
WO2010076774A1 (en) | System for dynamically configuring task-based environment and method of operation thereof | |
JPH1173335A (en) | Work flow management system and method for controlling acquisition of task information |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |