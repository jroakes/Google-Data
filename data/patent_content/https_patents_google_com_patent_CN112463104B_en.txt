CN112463104B - Automatic assistant with conference function - Google Patents
Automatic assistant with conference function Download PDFInfo
- Publication number
- CN112463104B CN112463104B CN202011124441.4A CN202011124441A CN112463104B CN 112463104 B CN112463104 B CN 112463104B CN 202011124441 A CN202011124441 A CN 202011124441A CN 112463104 B CN112463104 B CN 112463104B
- Authority
- CN
- China
- Prior art keywords
- automated assistant
- text
- conference
- meeting
- participants
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000012545 processing Methods 0.000 claims abstract description 76
- 238000000034 method Methods 0.000 claims description 48
- 230000004044 response Effects 0.000 claims description 39
- 230000015654 memory Effects 0.000 claims description 8
- 230000008859 change Effects 0.000 claims description 4
- 238000012544 monitoring process Methods 0.000 claims 5
- 230000009471 action Effects 0.000 description 34
- 230000000007 visual effect Effects 0.000 description 13
- 230000006870 function Effects 0.000 description 11
- 230000002452 interceptive effect Effects 0.000 description 11
- 230000007704 transition Effects 0.000 description 10
- 230000000694 effects Effects 0.000 description 8
- 230000008569 process Effects 0.000 description 8
- 238000004891 communication Methods 0.000 description 5
- 239000000463 material Substances 0.000 description 5
- 230000003993 interaction Effects 0.000 description 4
- 238000003058 natural language processing Methods 0.000 description 4
- 230000001755 vocal effect Effects 0.000 description 4
- 238000013475 authorization Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 235000013305 food Nutrition 0.000 description 2
- 238000002955 isolation Methods 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 239000000725 suspension Substances 0.000 description 2
- 241000282412 Homo Species 0.000 description 1
- 240000005561 Musa balbisiana Species 0.000 description 1
- 235000018290 Musa x paradisiaca Nutrition 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 230000003111 delayed effect Effects 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000005059 dormancy Effects 0.000 description 1
- 235000013399 edible fruits Nutrition 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000003203 everyday effect Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 235000012054 meals Nutrition 0.000 description 1
- 238000005065 mining Methods 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 238000013518 transcription Methods 0.000 description 1
- 230000035897 transcription Effects 0.000 description 1
Abstract
The present disclosure relates to automated assistants having conferencing functionality, and in particular to causing automated assistants to enter into a "conference mode" to "participate" in a conference between a plurality of human participants. In various implementations, an automated assistant implemented at least in part on one or more conference computing devices may be set to a conference mode in which the automated assistant performs speech-to-text processing on a plurality of different spoken utterances provided by a plurality of conference participants without explicitly invoking the automated assistant prior to each utterance. The automated assistant may perform semantic processing on a first text generated from the speech-to-text processing of one or more spoken utterances and generate data related to the first text based on the semantic processing. The data is output to the participant at one or more conference computing devices. The automated assistant may then determine that the meeting has ended and may be set to a non-meeting mode.
Description
Description of the division
The application belongs to a divisional application of Chinese application patent application 201880039481.3 with the application date of 2018, 10, 30.
Technical Field
The present disclosure relates to an automated assistant with conferencing functionality.
Background
People may engage in human-machine conversations using an interactive software application referred to herein as an "automated assistant" (also referred to as a "chat robot," "interactive personal assistant," "intelligent personal assistant," "personal voice assistant," "conversation agent," etc.). For example, humans (which may be referred to as "users" when they interact with an automated assistant, or "participants" in the context of a meeting) may use free-form natural language input, which may be spoken utterances that are converted into text and then processed, and/or provide commands, queries, and/or requests (collectively referred to herein as "queries") through typed free-form natural language input. Automatic assistants are typically invoked using a predetermined spoken utterance (e.g., "OK Assistant"), and various types of processing, such as speech-to-text processing, natural language processing, and/or semantic processing, are typically performed on only those spoken utterances that immediately follow a invoked phrase.
During conferences involving multiple human participants, there are often active or passive participants, sometimes also referred to as "secretaries," who take notes about the conference and share those notes (e.g., as an era of "action items" and/or "topics of discussion"). Additionally or alternatively, one or more conference participants may record their own notes during the conference. In either case, there may be some loss of information discussed during the meeting at the time of recording. Although stenographers can be invited to generate complete or as complete as possible text-word recordings of meetings, stenography can be expensive and/or impractical for everyday meetings or informal meetings.
During the meeting, participants also often operate computing devices to augment the meeting with information. In some cases, one or more participants may project or otherwise present a series of slides to guide the discussion. As another example, when a person presents a problem ("WHAT FLIGHTS ARE CHEAPEST (what is the cheapest flight). These searches may disrupt the flow of the meeting and/or cause the search participants to miss discussions while performing their study.
Disclosure of Invention
Described herein are techniques for enabling automated assistants to enter into a "meeting mode" in which they are able to "participate in" meetings among multiple human participants and perform the various functions described herein. In various implementations, an automated assistant configured with selected aspects of the present disclosure may operate at least in part on a device referred to herein as a "meeting computing device. The conference computing device may be any computing device capable of executing all or part of an automated assistant and capable of participating in a conference between a plurality of human participants using one or more input/output components, such as speakers, displays, and in particular microphones. A variety of computing devices may be particularly suitable for use as conference computing devices, such as stand-alone interactive speakers, video conference computing systems, vehicle computing systems, and the like. However, any computing device having a microphone and at least one output component (e.g., audio or video) may be used as the conference computing device. The term "conference" as used herein may refer to any verbal conversation between two or more human participants and does not imply that the interactions between those participants have commercial or administrative purposes.
An automatic assistant configured with selected aspects of the present disclosure may be set to a conference mode, for example, at the start of a multi-participant conference. In various embodiments, the start of the meeting may be detected based on calendar entries and/or in response to an explicit invocation of the meeting mode. During the conference, the automated assistant may perform speech-to-text processing on a plurality of different spoken utterances, particularly without explicitly invoking the automated assistant prior to each of the plurality of different spoken utterances. In some cases, the conference computing device may be a stand-alone interactive speaker or video conference computing system located in a room or area where some, if not all, conference participants are located. However, in other cases where multiple conference participants are geographically separated, an automated assistant running on multiple conference computing devices deployed at multiple locations may perform selected aspects of the present disclosure. Based on text generated from speech-to-text processing, the automated assistant may perform various functions to improve aspects of the meeting.
In some implementations, the automated assistant can perform semantic processing on text generated from one or more utterances using speech-to-text processing. Based on this semantic processing, the automated assistant may present (e.g., as audio and/or visual output) various information related to the discussion of the meeting. In some implementations, the automated assistant can perform semantic processing and/or present result information in response to an explicit request from the conference participant. Additionally or alternatively, in some implementations, the automated assistant can perform semantic processing and/or present information when the automated assistant detects a pause in the conference session.
The automated assistant may perform various forms of semantic processing to achieve various goals. In some implementations, semantic processing may be used to identify one or more topics of a conversation, for example, by a topic classifier. In some such implementations, these topics may be used, for example, to generate search queries (e.g., internet searches) to maintain "conference conversation context" associated with conference discussions (which may be used for various purposes, such as disambiguating participant utterances, filling in slots for tasks requested by automated assistants, etc.), and so forth. In embodiments in which the automated assistant generates a search query and performs a search based on the proposed subject matter (or more generally, based on semantic processing performed on the participant's utterances), the automated assistant may provide information responsive to the search query as audio and/or video output at the meeting computing device(s).
As a working example, assume that two participants are planning a ski trip, and one participant says that: "We should finalize our SKI TRIP IN Switzerland next weekend. Let's pick a resort (we should complete a ski trip in Switzerland on the next weekend. Let us choose a resort). After performing speech-to-text processing to generate text representing the spoken utterance, the automated assistant may perform semantic processing on the text to generate a search query that duplicates or at least summarizes the utterance. In some cases, the automated assistant may combine text generated from multiple utterances of multiple participants into a search query. The information in response to the search query may include, for example, a list of one or more ski resorts in switzerland. If the meeting computing device includes a display or has access to a display, these results may be automatically presented on the display, for example, as if one of the users had explicitly performed a search. Alternatively, if the conference computing device includes a speaker or has access to a speaker, data indicative of the results may be audibly output at the speaker, for example, during a conversation pause. It will be appreciated that in many embodiments where the available output component is a speaker, there may be less information output than if the output component was a display. This is because audio output may be more distracting and/or require more time to output than visual output, and may be beneficial to avoid interrupting conference flow.
In some implementations, the information output to the participants and/or the ongoing conference conversation context may be used to perform additional semantic processing on the subsequent utterances. For example, and continuing with this working example, one or more participants may ask some subsequent questions inspired by the presentation of the Swiss ski resort search query. Suppose a user asks: "how does it ski there? The problem may be too ambiguous to see "in isolation" because the term "there" cannot determine the target vacation resort. However, as described above, an automated assistant configured with selected aspects of the present disclosure may be configured to save a conference conversation context that saves one or more discussion topics and/or information that has been output by the automated assistant. In this example, the automated assistant may disambiguate "there," e.g., the highest ranked swiss ski resort previously presented. Or if the user says "Zermatt looks interesting, how does how look interesting how does how look like how does how IS THE SKIING THERE look interesting (Zermatt)? ", the target ski resort may instead be" Zermatt ". In any event, the automated assistant may then generate and submit a search query to find information about the skiing quality of the target vacation resort (e.g., skiing report, snow volume report, user comments, etc.). Thus, performing additional semantic processing on multiple utterances can resolve ambiguity for one or more of the utterances. This, in turn, can reduce the risk of errors in the operation of the automated assistant caused by such ambiguity (e.g., outputting erroneous or otherwise suboptimal data to the participant).
If the participant instead asks "WHAT WILL THE WEATHER be like? By way of example, similar techniques may be employed to generate suitable search queries. Likewise, such statements are seen in isolation as ambiguous, such that meaningful weather search queries cannot be generated. However, based on the reserved meeting dialogue context, the automated assistant can infer that the location used in the weather search query is the highest ranked resort previously presented, and that the time used in the weather search query is the "weekend". Thus, the automated assistant may search for the weather at the highest-ranked vacation resort for the next weekend and may present the results to the participant. This back-and-forth relationship between participants and/or automated assistants may continue to be used for other types of information, such as scheduling trips (e.g., enabling presentation of train schedules), purchasing ski tickets, and so forth.
In some implementations, an automated assistant configured with selected aspects of the present disclosure can be configured to generate a meeting summary based on a plurality of different utterances detected during the meeting. In various implementations, the meeting summary can take the form of a document (e.g., text and/or graphics) that includes a plurality of pieces of information, such as one or more topics detected by an automated assistant from the meeting discussion, one or more results of the meeting detected by the automated assistant from the meeting discussion, a text-to-text record of at least some of the plurality of different spoken utterances, information about participants in the meeting (e.g., some automated assistants may be able to match speech to a speech profile about a particular person), and so forth. The meeting summary may be stored, transmitted, and/or shared to one or more meeting participants, for example, by an automated assistant. In some implementations, meeting summary can be associated with calendar entries created to schedule the meeting.
In some implementations, an automated assistant configured with selected aspects of the present disclosure may be configured to perform various functions in a subsequent meeting (e.g., follow-up meeting) using the techniques described herein (utilizing information generated during a meeting, e.g., meeting conversation context, meeting summary). Suppose that the participants of the first meeting discuss a number of action items to be resolved, which are detected by the participating automated assistants and used to generate a meeting summary, for example, for the first meeting. In a second meeting that follows, the automated assistant may use information from the first meeting's era and/or information from the meeting conversation context stored in the first meeting to perform the various functions described above. For example, suppose a user asks: "OK, WHAT WERE THE action items from LAST MEETING (good, what was the action item of the last meeting)? The automated assistant may retrieve and output these action items, for example, as an audible list or on a display. In some implementations, the participant of the second meeting can indicate that one or more action items have been completed by the automated assistant, or the automated assistant can detect that one or more action items have been completed based on semantic processing of the participant's audible utterance during the meeting.
In some implementations, the automated assistant can automatically detect whether an action item has been completed since the first meeting. For example, assume that after a first meeting, one of the participants interacts with an automated assistant to address a certain action item (e.g., purchase disposable cutlery). In a second meeting, the automated assistant participating in the action may not present the action item because it has been resolved. Additionally or alternatively, the action item may be presented as "complete".
In various embodiments, the automated assistant may determine that two or more conferences are relevant in various ways (e.g., as an initial conference and a follow-up conference). In some implementations, participants may schedule meetings, for example, using electronic calendars, and may explicitly link meetings. Additionally or alternatively, in some implementations, the automated assistant may automatically detect that two or more conferences are related, e.g., based on a title to the conference, an overlap of participants in the conference, a document associated with the conference, etc. In some implementations in which a document (e.g., a calendar entry or its accompanying agenda) is associated with a meeting, the automated assistant may generate an initial meeting conversation context based on the associated document.
As described above, audio outputs generated by the automated assistant during the meeting may be more distracting than visual outputs, which may be ignored by the participants. Thus, in various implementations, the automated assistant may identify output modalities that are perceivable by the plurality of conference participants as being used by the one or more conference computing devices. The automated assistant may then output data related to a plurality of different spoken utterances during the meeting at a frequency selected based on the identified output modality. For example, if the meeting computing device is a stand-alone interactive speaker without a display, the automated assistant may provide output (e.g., search results, action item status, etc.) less frequently than if the meeting computing device included a display. As a specific example of an audio output in a vehicle in which the output modality is determined to be driven by a driver who is also one of the conference participants, the frequency of data relating to the presentation of a plurality of different spoken utterances by the automated assistant may be selected to avoid distraction of the driver.
In some embodiments, a method performed by one or more processors is provided, the method comprising: setting an automated assistant implemented at least in part on one or more conference computing devices to a conference mode in which the automated assistant performs speech-to-text processing on a plurality of different spoken utterances without explicitly invoking the automated assistant prior to each of the plurality of different spoken utterances, wherein the plurality of different spoken utterances are provided by a plurality of participants during a conference between the plurality of participants; automatically performing, by the automated assistant, semantic processing on first text generated from speech-to-text processing performed by one or more of the plurality of different spoken utterances, wherein the semantic processing is performed without explicit participant invocation; generating, by the automated assistant, data related to the first text based on semantic processing, wherein the data is output to the plurality of participants at the one or more conference computing devices while the automated assistant is in the conference mode; determining, by the automated assistant, that the meeting has ended; and based on the determination, setting the automated assistant to a non-conference mode in which the automated assistant needs to be invoked before speech-to-text processing is performed on the single spoken utterance.
These and other embodiments of the technology disclosed herein may optionally include one or more of the following features.
In various implementations, the data may be output to the plurality of participants as natural language output from the automated assistant via speakers of the one or more conference computing devices. In various embodiments, data may be output to multiple participants via one or more displays visible to the multiple participants.
In various embodiments, the determining includes: receiving, by the automated assistant, a verbal call from one of the plurality of participants indicating that the conference has ended; or to determine that the current time matches the scheduled end time of the meeting.
In various embodiments, the automated assistant may be set to the meeting mode in response to a verbal call indicating that the meeting has begun or an explicit command to enter the meeting mode. In various implementations, the method can further include performing additional semantic processing on the second text generated from the speech-to-text processing of one or more of the plurality of spoken utterances, wherein the additional semantic processing is performed based at least in part on the data related to the first text. In various embodiments, the additional semantic processing includes disambiguating one or more labels of the second text based on data related to the first text.
In various embodiments, the method may further comprise: a conference summary is generated by the automated assistant based on the plurality of different utterances, wherein the conference summary includes one or more topics detected by the automated assistant from the plurality of different spoken utterances while the automated assistant is in a conference mode. In various implementations, the meeting summary can also include one or more results of the meeting detected by the automated assistant from a plurality of different spoken utterances while the automated assistant is in the meeting mode. In various implementations, the meeting summary further includes a text word record of at least some of the plurality of different spoken utterances.
In various embodiments, the method may further comprise: determining that the meeting is related to a previous meeting; and identifying, by the automated assistant, additional data generated during the previous meeting and related to the current meeting based on information associated with the previous meeting, wherein the additional data is output to the plurality of participants on the one or more meeting computing devices while the automated assistant is in the meeting mode.
In various embodiments, the method may further comprise: identifying an output modality used by the one or more conference computing devices, the output modality being perceivable by the plurality of participants; and outputting data related to a plurality of different spoken utterances at a frequency related to the identified output modality. In various embodiments, the output modality includes audio output in a vehicle driven by a driver, which is also one of the participants, and the frequency of outputting data related to a plurality of different spoken voices is selected to avoid distraction of the driver.
Additionally, some implementations include one or more processors of one or more computing devices, wherein the one or more processors are operable to execute instructions stored in associated memory, and wherein the instructions are configured to cause performance of any of the foregoing methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by the one or more processors to perform any of the foregoing methods. Other embodiments include one or more transitory computer-readable storage media (e.g., signals such as optical, electrical, or electromagnetic signals) storing or embodying computer instructions capable of being executed by one or more processors to perform any of the aforementioned methods.
It should be appreciated that all combinations of the foregoing concepts and additional concepts described in more detail herein should be considered as part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure should be considered part of the subject matter disclosed herein.
Drawings
FIG. 1 is a block diagram of an example environment in which embodiments disclosed herein may be implemented.
Fig. 2A, 2B, 2C, and 2D depict one example of how the techniques described herein may be employed in particular situations, according to various implementations.
Fig. 3 illustrates another example of how the techniques described herein may be employed in another case, according to various embodiments.
Fig. 4 shows a flowchart of an example method according to embodiments disclosed herein.
FIG. 5 illustrates an example architecture of a computing device.
Detailed Description
Referring now to FIG. 1, an example environment is shown in which the techniques disclosed herein may be implemented. The example environment includes one or more client computing devices 106 1-N. Each client device 106 may execute a respective instance of the automated assistant client 118. One or more cloud-based automated assistant components 119, such as natural language processor 122, may be implemented on one or more computing systems (collectively referred to as "cloud" computing systems) communicatively coupled to client device 106 1-N via one or more local and/or wide area networks, indicated generally at 110 (e.g., the internet).
As described in the background, through interaction with one or more cloud-based automated assistant components 119, the instance of the automated assistant client 118 may form a logical instance of the automated assistant 120 that appears to a user to be a human-machine conversation with which the user may conduct. Two examples of such an automated assistant 120 are depicted in fig. 1. The first automated assistant 120A, enclosed by the dashed line, serves a first user (not shown) operating the first client device 106 1 and includes an automated assistant client 118 1 and one or more cloud-based automated assistant components 119. A second automated assistant 120B, surrounded by a two-dot chain line, serves a second user (not shown) operating another client device 106 N and includes an automated assistant client 118 N and one or more cloud-based automated assistant components 119. Thus, it should be appreciated that in some implementations, virtually every user interacting with the automated assistant client 118 executing on the client device 106 may interact with his or her own logical instance of the automated assistant 120. For brevity and simplicity, the term "automated assistant" used herein as "serving" a particular user will refer to a combination of an automated assistant client 118 executing on a client device 106 operated by the user with one or more cloud-based automated assistant components 119 (which may be shared among multiple automated assistant clients 118). It should also be appreciated that in some implementations, the automated assistant 120 may respond to requests from any user independent of whether the user is actually "served" by a particular instance of the automated assistant 120.
Client device 106 1-N may include, for example, one or more of the following: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a stand-alone interactive speaker, a smart appliance such as a smart television, and/or a user's wearable apparatus including a computing device (e.g., a user's watch with a computing device, a user's glasses with a computing device, a virtual reality or augmented reality computing device). Additional and/or alternative client computing devices may be provided.
In various implementations, each of the client computing devices 106 1-N can operate a variety of different applications, such as a corresponding one of the plurality of messaging clients 107, 1-N. The message exchange client 107 1-N may have various forms, and these forms may vary between client computing devices 106 1-N and/or may operate on a single one of the client computing devices 106 1-N. In some implementations, one or more of the messaging clients 107 1-N can be in the form of a short message service ("SMS") and/or multimedia message service ("MMS") client, an online chat client (e.g., instant messaging program, internet relay chat or "IRC," etc.), a messaging application associated with a social network, a personal assistant messaging service dedicated to conversations with the automated assistant 120, and so forth. In some implementations, one or more of the messaging clients 107 1-N can be implemented via a web page or other resource provided by a web browser (not depicted) or other application of the client computing device 106.
As described in greater detail herein, the automated assistant 120 conducts a human-machine conversation session with one or more users via user interface input and output devices of the one or more client devices 106 1-N. In some implementations, the automated assistant 120 can conduct a human-machine conversation with the user in response to user interface inputs provided by the user via one or more user interface input devices of one of the client devices 106 1-N. In some of those embodiments, the user interface input is specifically directed to the automated assistant 120. For example, one of the messaging clients 107 1-N may be a personal assistant messaging service dedicated to conversations with the automated assistant 120, and user interface inputs provided via the personal assistant messaging service may be automatically provided to the automated assistant 120. Further, for example, the user interface input may be explicitly directed to the automated assistant 120 in the one or more message exchange clients 107 1-N based on a particular user interface input indicating that the automated assistant 120 is to be invoked. For example, the specific user interface input may be one or more typed characters (e.g., @ auto-assistant), user interactions with hardware buttons and/or virtual buttons (e.g., tap, long press), verbal commands (e.g., "Hey Automated Assistant (he, auto-assistant)") and/or other specific user interface inputs.
In some implementations, the automated assistant 120 can participate in the conversation session in response to the user interface input even when the user interface input is not explicitly directed to the automated assistant 120. For example, the automated assistant 120 may examine the content of the user interface input and participate in a conversation session in response to certain terms present in the user interface input and/or based on other prompts. In some implementations, the automated assistant 120 can make an interactive voice response ("IVR") enabling the user to speak commands, search, etc., and the automated assistant can utilize natural language processing and/or one or more grammars to convert the utterance into text and respond to the text accordingly. In some implementations, the automated assistant 120 can additionally or alternatively respond to a spoken utterance without converting the utterance to text. For example, the automated assistant 120 can convert the speech input into an embedded, one or more entity representations (indicative of one or more entities present in the speech input), and/or other "non-textual" representations, and operate on such non-textual representations. Thus, embodiments described herein as operating based on text converted from speech input may additionally and/or alternatively operate directly on speech input and/or other non-textual representations of speech input.
Each of the client computing devices 106 1-N and one or more computing devices operating the cloud-based automated assistant component 119 can include one or more memories for storing data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. Operations performed by one or more client computing devices 106 1-N and/or by automated assistant 120 may be distributed across multiple computer systems. The automated assistant 120 may be implemented, for example, as a computer program running on one or more computers in one or more locations coupled to each other via a network.
As described above, in various embodiments, each of the client computing devices 106 1-N may operate the automated assistant client 118. In various implementations, each automatic assistant client 118 can include a corresponding speech capture/text-to-speech ("TTS")/STT module 114. In other implementations, one or more aspects of the speech capture/TTS/STT module 114 may be implemented separately from the automated assistant client 118.
Each speech capture/TTS/STT module 114 may be configured to perform one or more of the following functions: capturing the user's voice, for example, via a microphone (which may include presence sensor 105 in some cases); converting the captured audio to text (and/or other representations or embeddings); and/or converting text to speech. For example, in some implementations, because the client devices 106 may be relatively limited in terms of computing resources (e.g., processor cycles, memory, battery, etc.), the native voice capture/TTS/STT module 114 of each client device 106 may be configured to convert a limited number of different spoken phrases, particularly phrases that invoke the automated assistant 120, into text (or into other forms, such as lower-dimensional embedding). Other voice inputs may be sent to the cloud-based automated assistant component 119, which may include the cloud-based TTS module 116 and/or the cloud-based STT module 117.
The cloud-based STT module 117 may be configured to convert the audio data captured by the speech capture/TTS/STT module 114 to text (which may then be provided to the natural language processor 122) using nearly unlimited resources of the cloud. The cloud-based TTS module 116 may be configured to utilize virtually unlimited resources of the cloud to convert text data (e.g., natural language responses formulated by the automated assistant 120) into computer-generated speech output. In some implementations, TTS module 116 can provide computer-generated speech output to client device 106 for direct output, for example, using one or more speakers. In other implementations, text data (e.g., natural language responses) generated by the automated assistant 120 may be provided to the speech capture/TTS/STT module 114, and the speech capture/TTS/STT module 114 may then convert the text data into locally output computer-generated speech.
The automated assistant 120 (particularly the cloud-based automated assistant component 119) can include a natural language processor 122, the TTS module 116 described above, the STT module 117 described above, a dialog state tracker 124, a dialog manager 126, and a natural language generator 128 (which can be combined with the TTS module 116 in some implementations), and includes a conference engine 130 particularly relevant to the present disclosure. In some implementations, one or more of the engines and/or modules of the automated assistant 120 may be omitted, combined, and/or implemented in a separate component from the automated assistant 120.
In some implementations, the automated assistant 120 generates the response content in response to various inputs generated by a user of one of the client devices 106 1-N during a human-machine conversation session with the automated assistant 120. The automated assistant 120 may provide the responsive content (e.g., over one or more networks when separated from the user's client device) for presentation to the user as part of the conversation session. For example, the automated assistant 120 may generate the response content in response to free-form natural language input provided via one of the client devices 106 1-N. As used herein, free form input is input formulated by a user and is not limited to a set of options presented for selection by the user.
As used herein, a "conversation session" may include a logically independent exchange of one or more messages between a user and the automated assistant 120 (and in some cases, other human participants) and/or one or more responsive actions performed by the automated assistant 120. The automated assistant 120 may distinguish between multiple conversations with the user based on various signals such as time lapse between sessions, changes in user context between sessions (e.g., location, before/during/after a scheduled meeting, etc.), detection of one or more intervening interactions between the user and the client device other than conversations between the user and the automated assistant (e.g., the user opens an application for a while, the user walks away and then returns to a separate voice activated product), locking/dormancy of the client device between sessions, changes in the client device for interacting with one or more instances of the automated assistant 120, etc. As will be described in greater detail below, in some embodiments, the automated assistant 120 may facilitate a "conference conversation session," e.g., by the conference engine 130, in which the automated assistant 120 is converted to a "conference mode" in which various functions, such as natural language processing, may be performed without explicit invocation prior to each spoken utterance (or written statement).
The natural language processor 122 (alternatively referred to as a "natural language understanding engine") of the automated assistant 120 processes free-form natural language input generated by the user via the client device 106 1-N and, in some implementations, may generate annotation output for use by one or more other components of the automated assistant 120. For example, the natural language processor 122 may process natural language freeform input generated by a user through one or more user interface input devices of the client device 106 1. The generated annotation output may include one or more annotations of the natural language input and optionally one or more (e.g., all) terms of the natural language input.
In some implementations, the natural language processor 122 is configured to identify and annotate various types of grammar information in the natural language input. For example, the natural language processor 122 may include a portion of a voice markup (not depicted) configured to annotate terms with their grammatical roles. For example, a portion of a phonetic labeler may label each term with a portion of its voice (such as "noun," "verb," "adjective," "pronoun," etc.). Further, for example, in some implementations, the natural language processor 122 may additionally and/or alternatively include a dependency analyzer (not depicted) configured to determine syntactic relationships between terms in the natural language input. For example, the dependency analyzer may determine which terms modified other terms, subject and verbs of sentences, etc. (e.g., parse trees) -and may annotate these dependencies.
In some implementations, the natural language processor 122 may additionally and/or alternatively include an entity marker (not depicted) configured to annotate entity references in one or more segments, such as references to persons (including, for example, literature personas, celebrities, public personas, etc.), organizations, locations (real and fictional), and the like. In some implementations, data about an entity can be stored in one or more databases, such as in a knowledge graph (not depicted). In some implementations, the knowledge graph can include nodes representing known entities (and in some cases, entity attributes) and edges connecting the nodes and representing relationships between the entities. For example, a "banana" node may be connected (e.g., as a child node) to a "fruit" node, which in turn may be connected (e.g., as a child node) to a "produce (agricultural product)" and/or "food" node. As another example, a restaurant called "Hypothetical caf" may be represented by a node that also includes attributes such as its address, the type of food served, business hours, contact information, and so forth. In some embodiments, a "Hypothetical caf" node may be connected to one or more other nodes through edge connections (e.g., representing child-parent relationships), such as a "restaurant" node, a "business" node, a node representing the city and/or state in which the restaurant is located, and so forth.
The entity markers of the natural language processor 122 may annotate references to entities at a higher level of granularity (e.g., to enable identification of all references to entity categories such as people) and/or at a lower level of granularity (e.g., to enable identification of all references to specific entities such as specific people). The entity markers may rely on the content of the natural language input to parse a particular entity and/or may optionally communicate with a knowledge graph or other entity database to parse a particular entity.
In some implementations, the natural language processor 122 may additionally and/or alternatively include a co-reference parser (not depicted) configured to group or "cluster" references of the same entity based on one or more context cues. For example, in the natural language input "I liked Hypothetical CAFE LAST TIME WE ATE THERE (where me liked Hypothetical caf e last time we had a meal), the term" ther (there) "may be resolved to" Hypothetical caf e "using a co-reference resolver.
In some implementations, one or more components of the natural language processor 122 may rely on annotations from one or more other components of the natural language processor 122. For example, in some implementations, named entity markers may rely on annotations from a co-reference resolver and/or a dependency analyzer when annotating all aspects of a particular entity. In addition, for example, in some implementations, the co-reference resolver may rely on annotations from the dependency analyzer when clustering references to the same entity. In some implementations, one or more components of the natural language processor 122 may use related prior inputs and/or other related data beyond the specific natural language input to determine one or more annotations when processing the specific natural language input.
In some implementations, the dialog state tracker 124 may be configured to track "dialog states," including, for example, brief states of targets (or "intents") of one or more users during a human-machine dialog session procedure, across multiple dialog sessions, and/or during a conference dialog session. In determining dialog states, some dialog state trackers may attempt to determine the most likely value of one or more time slots instantiated in a dialog based on user and system utterances in the dialog session. Some techniques utilize a fixed body that defines a set of time slots and a set of values associated with those time slots. Additionally or alternatively, some techniques may be customized for individual timeslots and/or domains. For example, some techniques may require training a model for each slot type in each domain.
The dialog manager 126 may be configured to map the current dialog state, for example, provided by the dialog state tracker 124, to one or more "response actions" of the plurality of candidate response actions that are then performed by the automated assistant 120. Depending on the current dialog state, the response action may take a variety of forms. For example, initial and intermediate dialog states in response to a multi-round dialog session occurring prior to the last round (e.g., when performing an end user desired task) may be mapped onto various responsive actions including the automated assistant 120 outputting additional natural language dialogs. Such a response dialog may include, for example, requesting that the user provide parameters for certain actions (i.e., filling the time slot) that the dialog state tracker 124 deems the user to be performing. In some implementations, the responsive action may include actions such as "request" (e.g., find parameters for slot filling), "provide" (e.g., suggest actions or course of actions for the user), "select", "notify" (e.g., provide requested information to the user), "not match" (e.g., notify the user that the last input of the user was not understood), and so forth.
Conference engine 130 may be configured to facilitate a "conference mode" of automated assistant 120 that enables automated assistant 120 to "participate" in a conference among multiple human participants and perform various functions. In various implementations, the automated assistant 120 configured with selected aspects of the present disclosure may operate at least in part on a computing device that will be referred to herein as a "meeting computing device. The conference computing device may be any computing device that includes one or more client devices 106 that are capable of participating in a conference between multiple human participants using one or more input/output components, such as speakers, displays, and in particular microphones. A variety of computing devices may be particularly suitable for use as conference computing devices, such as stand-alone interactive speakers, video conference computing systems, vehicle computing systems, and the like. However, any computing device having a microphone and at least one output component (e.g., audio or video) may be used as the conference computing device.
In various implementations, the conference engine 130 can be configured to set the automated assistant 120 to the "conference mode" described above, such that the automated assistant 120 performs speech-to-text processing (e.g., via the STT 117) on a plurality of different spoken utterances without explicitly invoking the automated assistant 120 prior to each of the plurality of different spoken utterances. In many cases, multiple participants may provide multiple different spoken utterances during a meeting or conference between the multiple participants. By performing natural language processing and other processing of spoken user utterances without requiring explicit invocation at each time, the automated assistant 120 is able to perform various functions that may be helpful to the participants of the meeting. For example, avoiding the need to explicitly invoke an automated assistant before each utterance can reduce the risk that the automated assistant cannot process all or part of a particular utterance, which may occur if the automated assistant is in a non-conference mode (i.e., a mode in which speech-to-word processing is performed aperiodically) when an utterance occurs. Thereby improving the reliability of the automated assistant.
For example, in some embodiments, while in the meeting mode, the automated assistant 120 is free to provide information to the participant based on the participant's discussion. More specifically, in some implementations, the automated assistant 120 may automatically (i.e., without explicit command from the participant) perform semantic processing (e.g., by the natural language processor 122 and/or other cloud-based automated assistant component 119) on first text generated from speech-to-text processing of one or more of the plurality of different spoken utterances provided by the meeting participant. If the automated assistant 120 is not in meeting mode, it will not perform such semantic processing within explicit invocations. Based on the semantic processing, the automated assistant 120 may generate data related to the semantically processed text. For example, if text is generated from a user utterance that includes a question, the text may be used to generate a search query submitted by the automated assistant 120 to one or more databases. Data responsive to the search query may then be obtained by the automated assistant 120 and output to the plurality of conference participants at the one or more conference computing devices. An example of this will be described below.
Not every participant's utterance is worth responding by the automated assistant 120. For example, a participant may have an informal play during a meeting that they may not wish the automated assistant 120 to react to. Thus, in various embodiments, the automated assistant 120 may analyze various criteria to determine whether to inject its retrieved content into the meeting content based on semantic processing of the participant's discussion. In some implementations, the automated assistant 120 can determine a relevance score associated with information it obtains in response to the participant's utterance. If the retrieved information has a relevance score that meets some minimum relevance threshold, the automated assistant 120 can potentially incorporate the information into the discussion (e.g., subject to other constraints related to the modalities described below). On the other hand, if the retrieved information has a relevance score that does not meet such a threshold, the automated assistant 120 may avoid incorporating the information into the meeting discussion, as the information may not be useful or well accepted by the participant.
The automated assistant 120 may perform various other functions in the conference mode to assist conference participants. For example, the automated assistant 120 may provide an audio or visual output that provides information about the agenda, one or more documents, or other information associated with the meeting to the participant. It is assumed that the meeting is scheduled using an electronic/online calendar system and that the calendar entry comprises a meeting agenda prepared by one of the participants, for example. Such meeting agenda may include various information such as one or more topics discussed, action items and their related status (e.g., completed or unfinished), participant identities, agenda items to vote, the relationship of the current meeting to previous or future meetings, and so forth.
In some embodiments, such meeting agenda may be displayed during the meeting and/or continuously redisplayed and/or periodically displayed. For example, in some implementations, the automated assistant 120 may be configured with a topic classifier that identifies one or more topics presented from text generated by the participant utterance and/or identifies when discussions have been converted between different topics. Such topic classifier may employ known techniques of various topic classifications commonly used for document classification, such as expectation maximization, word frequency-inverse document frequency ("TF-IDF"), naive bayes classification, latent semantic indexing, support vector machines, artificial neural networks, decision trees, concept mining, and the like.
In some embodiments where the meeting agenda includes an action item, the automated assistant 120 may be configured to semantically process the utterance provided by the participant during the meeting to determine whether the action item has been resolved (e.g., resolved, delayed, modified, cancelled, etc.). The automated assistant 120 may modify the displayed information about the action item accordingly when displaying the agenda. An example of this is described below with respect to fig. 2C. In addition, in some implementations of presenting a sequence of slides, the automated assistant 120 may semantically process the utterances of the participants to automatically advance the slides through the sequence.
In some implementations, the automated assistant 120 can generate a meeting summary, for example, after transitioning from the meeting mode back to a non-meeting or "normal" mode that the automated assistant 120 needs to explicitly invoke before semantically processing the utterance. In some implementations, the meeting summary can be similar to the meeting agenda, except that the meeting summary can be annotated based on discussion content of meeting participants learned by semantic processing of the discussion of the meeting. Additionally or alternatively, and particularly where the meeting agenda is not prepared prior to the meeting, the automated assistant 120 may regenerate the meeting agenda based only on semantic processing of the participant's discussion. Thereby reducing or eliminating the need for one or all of the conference participants to take notes during the conference. In addition, errors and/or omissions in notes made by human participants can be reduced or avoided.
The meeting summary generated by the automated assistant 120 can include various other information. In addition to or in lieu of information that may also be included in the meeting agenda, meeting summaries generated using the techniques described herein may include subject matter in question (which may be detected at least in part by the subject matter classifier described above), action items created/resolved/modified, results of the meeting (e.g., reservation of a venue, purchase of a ticket, voting results, etc.), a partial or complete transcription of the utterances of some or all of the participants during the meeting, the next (or follow-up) meeting (if the participants discuss scheduling a meeting), and so forth.
In various implementations, the automated assistant 120 may determine when a meeting begins and/or ends, such as by the meeting engine 130, and thus use various hints to determine when the automated assistant 120 should transition between the meeting mode and the normal mode. In some implementations, the conference participant may issue an explicit command, such as "HEY ASSISTANT, let' S START THE MEETING (hey, assistant, let us start conference)", to cause the automated assistant 120 to transition into the conference mode. Additionally or alternatively, in some implementations, the automated assistant 120 can infer when to transition from the normal mode to the meeting mode based on the user utterance. For example, when one participant speaks something like "OK, let 'SGET STARTED (good we start)" or "Let's bring THIS MEETING to order" to another participant (not directly to the automated assistant 120), for example, the automated assistant 120 may transition from the normal mode to the conference mode. If the meeting is of a type that should follow a formal procedure, such as a public listening, non-profit board meeting, etc., then a phrase that is normally and/or officially issued to begin such a formal meeting may be detected and the automated assistant 120 may be caused to transition from the normal mode to the meeting mode. In some implementations, the automated assistant 120 may be configured to count tickets cast by participants on such meetings.
In some implementations, the automated assistant 120 can access one or more electronic calendar entries that indicate that a meeting is to be held at a particular time and/or location. In some such implementations, when the automated assistant 120 detects (e.g., using one or more microphones and/or cameras) that at least some of the participants are co-present at the designated meeting location, the automated assistant 120 may automatically transition into meeting mode at the scheduled start time of the meeting and/or at some point in time after the scheduled start time. Similarly, the automated assistant 120 may determine when to transition from the meeting mode back into the normal mode based on explicit user instructions (e.g., "HEY ASSISTANT, let 'S END THE MEETING (hey, assistant Let us end the meeting")), implicit user utterances (e.g., "Let' S CALL IT A DAY (today to this end")), and/or formal utterances (e.g., "THIS MEETING IS adjourned (meeting rest")). By automatically transitioning the automated assistant back to the non-conference mode, unnecessary speech-to-text processing (and subsequent processing) of the utterance that is not relevant to the conference can be avoided. This in turn can enable the automated assistant to take less computing resources than it would otherwise consume in the meeting mode.
Various challenges associated with the automated assistant 120 are automatically merging content into a meeting between multiple human participants. If human participants are talking to each other, rather than talking to the automated assistant 120, the automated assistant 120 providing content may be distracting when one participant desires feedback from another participant. If the automated assistant 120 is too fast to provide search results in response to an utterance of a speaker that includes a question (the automated assistant 120 may be submitted as a search query), the presentation of responsive content (especially if audible) may distract and/or interrupt one or more participants who are attempting to respond to the utterance of the speaker. Furthermore, if the automated assistant 120 provides responsive content for too many participant utterances, the participant may be distracted and/or inundated with too much information. In other words, the automated assistant 120 may become annoying.
Thus, in various embodiments, the automated assistant 120 may be configured to exercise various levels of judgment in outputting content to conference participants based on various cues (also referred to as "injecting content into discussion"). In some implementations, when the automated assistant 120 semantically processes the participant's utterance and has retrieved the responsive content, then the automated assistant 120 can wait for a pause in the conversation (e.g., a predetermined time interval, such as five seconds, etc.) before it provides the responsive content as output. In some such embodiments, if no such suspension occurs, for example, because the conference participants continue their discussions carefully, the automated assistant 120 may wait for suspension or discard responsive content, particularly if the automated assistant 120 determines that the context in question has changed (e.g., new discussion topics are detected). In some implementations, the automated assistant 120 may discard such responsive content if a predetermined time interval (such as one minute, five minutes, thirty seconds, etc.) is not paused in the conversation.
In some implementations, the automated assistant 120 can exercise a level of judgment when automatically injecting content into a discussion corresponding to the type of output modality available to the automated assistant 120. If the audible output provided by the client device 106, for example, in the form of a separate speaker or conference phone configured with selected aspects of the present disclosure, is presented too frequently, distraction may occur. In contrast, visual output may not be so distracting. Thus, if the automated assistant 120 is capable of providing visual output on a display (e.g., a video conference screen or even a single computer screen viewed by a participant), the automated assistant 120 may exercise a relatively low level of judgment in determining whether to output content and/or when to output content. On the other hand, if the automated assistant 120 is only able to provide audible output via one or more speakers, the automated assistant 120 may exercise a greater level of judgment in determining whether to output content and/or when to output content.
The examples described herein are primarily directed to a scenario in which multiple conference participants are physically co-located with a client device 106, such as a stand-alone interactive speaker and/or display operating an automated assistant 120 configured with selected aspects of the present disclosure, for example. However, this is not meant to be limiting. The techniques described herein are equally applicable where conference participants are not co-located. For example, suppose two or more participants are conducting a conference using a video conference, for example, in the case where each user sits in front of his or her own computer. In some implementations, the automated assistant 120 can provide the same output to each participant on their respective screen. In other implementations, the automated assistant 120 may provide different content to each participant on their screen, e.g., according to the preferences of the individual participants, the content of the individual participants (e.g., one participant may be in a public place and may not wish to display potentially sensitive information), etc. In the case where two conference participants are not co-located and operate the client device 106 in different output modalities (e.g., one is audio, one is video), the automated assistant 120 may provide (or "push") more content to participants with visual output capabilities than participants with audio output capabilities alone.
Fig. 2A-2D illustrate one example of a meeting between multiple participants 202 1-3, where the automated assistant 120 "participates" in the meeting by being executed at least in part on one or more client devices 206 1-2. In this example, the first client device 206 1 takes the form of a stand-alone interactive speaker with a microphone (not specifically depicted), while the second client device 206 2 takes the form of a smart television with display capabilities. For this example, it can be assumed that participant 202 1-3 has scheduled the meeting using the electronic calendar, and that there is an agenda defined by one of the participants, whether in the calendar entry or in a separate document attached to the calendar entry.
In fig. 2A, the first participant 202 1 begins the meeting by speaking an utterance of "OK, assant, let' S START THE MEETING (good, assistant, let us start meeting)". This is an example of an explicit command for the automated assistant 120 to transition from a non-conference mode or normal mode to the conference mode described above. The agenda for the meeting is displayed on second client device 206 2, e.g., as requested by automated assistant 120. The agenda includes two topics: "planning corporate activities" and "auditing budgets". In some implementations, the agenda may be displayed when the automated assistant 120 transitions to meeting mode.
In fig. 2B, the second participant 202 2 speaks: "We should plan the company EVENT AT THE ball park (we should plan corporate activities at the baseball field). "based on the semantic processing of the utterance, the automated assistant 120 can determine that she is referring to the first item on the meeting agenda (" planning corporate activity "). The automated assistant 120 may also determine, for example, through the entity markers discussed above, that "baseball field" is a reference to a particular venue associated with a particular sports team. Although not depicted in fig. 2B, in some implementations, the automated assistant 120 may cause the second client device 206 2 to display various information about the baseball field at this time, such as a picture, a link to its website, information about the sports team, and so on. Third participant 202 3 is asked "Good idea, what's bits schedule? (good idea, what is its schedule)? "in response to the statement of the second participant. For example, with the help of the previously described co-referenced parser, the automated assistant 120 may parse the word "its" into a previously identified team. The automated assistant 120 may then generate and submit a search query for the sports team's schedule, and may display the response data on the second client device 206 2, as shown in fig. 2B.
Fig. 2C depicts the same meeting of participant 202 1-3 at the next stage after the discussion of the corporate event has ended and the transition to the next topic. The first participant 202 1 says that: "Good, looks LIKE THE EVENT IS PLANNED (very Good, it appears that the activity has been scheduled)". The automated assistant 120 can semantically process the utterance and associate it with one of the meeting agenda items (e.g., the first activity item "scheduled corporate event"). In addition, the automated assistant 120 may determine that the particular agenda item has been resolved based on semantic processing. Thus, the automated assistant 120 may present (or re-present) the meeting agenda on the second client device 206 2, where the meeting agenda item "scheduled company event" is depicted as completed, for example, using a strikethrough or another visual indicator (e.g., check box, font, etc.) shown in fig. 2C. With the meeting agenda presented at this point in the discussion, participant 202 1-3 is reminded of the topic of the next discussion, in this case, the view budget, when participant 202 1-3 appears to be transitioning to a different topic. This helps focus the meeting and keeps the participants on the topic.
Fig. 2D depicts one example of a situation that may occur at the end of a meeting. Third participant 202 3 says that: "OK, let's get out of here (good, we Let's go)". As previously described, the automated assistant 120 may semantically process the utterance to infer that the meeting has ended. Thus, in fig. 2D, the automated assistant 120 may take a number of actions including displaying the meeting summary on the second client device 206 2 and transitioning from the meeting mode to a non-meeting or normal mode. In this example, the displayed meeting summary includes a list of topics in question, which may or may not be partially generated by the original meeting agenda. Here, the meeting summary includes the results of the meeting, including planning corporate activities and auditing budgets. In addition, the meeting summary includes action items discussed by the participants 202 1-3 during the meeting, such as action items regarding budget reviews, and is semantically detected by the automated assistant 120.
In some implementations, one or more meeting participants can be provided with meeting offerings such as depicted in fig. 2D, for example, by way of email or file sharing. In some embodiments where it is determined by the automated assistant 120 that a follow-up meeting has been planned (e.g., according to semantic processing discussed during the meeting or by linking to a new calendar entry of the original calendar entry), the meeting summary may be saved and presented on the follow-up meeting (e.g., as a meeting agenda). In some implementations, the automated assistant 120 can automatically detect when two meetings are related and can thus share an agenda and/or theme. For example, the automated assistant 120 may examine metadata (e.g., titles) associated with multiple conferences or determine that multiple conferences share participants. In some implementations, the automated assistant 120 can detect patterns in multiple conferences suggesting regularly scheduled conferences, and can "inherit" conference descriptions across multiple conferences.
In some implementations, the automated assistant 120 may identify conference participants in various ways, for example, with the purpose of pushing conference agenda and/or campaigns to those participants. As a simple example, a calendar entry may explicitly identify a meeting participant, and the automated assistant 120 may use the calendar entry to determine the email address of the participant. Additionally or alternatively, in some implementations, the automated assistant 120 may be configured to perform voice recognition to identify conference participants, and then may match the identified participants with known user profiles. As another example, in some implementations, for example, at the beginning of a meeting, participants may explicitly identify themselves as part of the presentation, and the automated assistant 120 may detect spoken names (and, for example, can add those names to the meeting summary).
In the example case of fig. 2A-2D, all conference participants are co-located. However, as noted above, this is not meant to be limiting. Fig. 3 depicts an example of a meeting occurring between a first participant (not depicted) operating a first client device 306 1 in the form of a desktop computer and a second participant (not depicted) driving a vehicle 340, the vehicle 340 comprising an on-board computing system forming a second client device 306 2. For this example, it can be assumed that the first participant is able to speak or type in free-form natural language input semantically processed by the automated assistant 120, but the second participant (due to driving) is limited to providing only spoken free-form natural language input. The automated assistant 120 can provide information visually and/or audibly at the first client device 306 1, but only audibly at the second client device 306 2 because the visual output may distract the participant who is driving.
Assume that a first participant at first client device 306 1 speaks some images during the meeting as "Do you want to go to Lexington THIS WEEKEND? (do you want to go to lek ston on this weekend), "and the second (driving) user operating client device 306 2 responds," Maybe, depends on THE WEATHER (perhaps bar, see weather) ". The automated assistant 120 may perform semantic processing on these utterances to generate one or more search queries and retrieve information about the weather of lekurd ston and the last of this week. Because the first participant is operating the first client device 306 1 with a display, the automated assistant 120 may exercise relatively little judgment in selecting the response information to be presented. This is because it is not known whether the first participant is engaged in an activity such as driving, and because the visual output may be least distracting. Thus, a large amount of response information is visually presented at first computing device 306 1, including other points of interest regarding the lekurd, itself, weather for the day of the week, and points of interest within one hour of the journey of lekurd.
In contrast, the automated assistant 120 is only able to push information to a second participant driving the vehicle 340 using the audio output. Thus, the automated assistant 120 may be more selective to the information it provides. For example, while participants are approximately in the location of the discussion of lekton, they do not explicitly query each other for information about points of interest. Thus, the relevance scores associated with the various points of interest displayed on the first client device 306 1 may not meet the minimum relevance score for the driving participant. Thus, when a first participant sees all information about london, a second participant driving the vehicle only hears the most relevant information, namely, the weather of london on the sunday.
Thus, it can be seen that in various embodiments, the automated assistant 120 can adjust the relevance threshold based on the context of the conference participant. As another example, assume that the first user in fig. 3 is operating first client device 306 1 to work (e.g., drafting a document, working in a spreadsheet, conducting a study, etc.). In this context, it may be undesirable to visually inundate or distract the first participant with information related to the conversation. Thus, the automated assistant 120 may adjust the relevance threshold associated with the first participant to be more closely aligned with a higher relevance threshold associated with the second driving, participant. For example, because the first participant uses the display for other purposes, the automated assistant 120 may choose to push information to the first participant in an audible rather than visual manner to avoid distracting the first participant.
Fig. 4 is a flow chart illustrating an example method 400 according to embodiments disclosed herein. For convenience, the operations of the flowcharts are described with reference to systems performing these operations. The system may include various components of various computer systems, such as one or more components of a computing system implementing the automated assistant 120. Furthermore, although the operations of method 400 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 402, the system may set an automated assistant 120 implemented at least in part on one or more conference computing devices to a conference mode in which the automated assistant performs speech-to-text processing on a plurality of different spoken utterances without explicitly invoking the automated assistant prior to each of the plurality of different spoken utterances. As described herein, in various embodiments, a plurality of different spoken utterances may be provided by a plurality of human participants during a conference between the plurality of participants.
At block 404, the system may automatically perform semantic processing on a first text generated from speech-to-text processing of one or more of the plurality of different spoken utterances. In particular, semantic processing may be performed without explicit participant invocation. And indeed, in various embodiments, the system may perform semantic processing on text generated from all participant utterances. If a particular participant utterance is not understood, speech may not be converted to text, in which case the automated assistant 120 takes no action. If a particular participant utterance is understandable, but when the semantic processing does not produce information related to the discussion of the meeting (e.g., the relevance score fails to meet the relevance threshold), the automated assistant 120 may take no action with respect to the retrieved information. However, at block 406, if the information retrieved based on the semantic processing meets certain criteria, such as a relevance threshold, the system may generate relevant data (e.g., natural language output) based on the information obtained as a result of the semantic processing and output (at block 408) the relevant data to one or more of the plurality of participants at one or more conference computing devices.
At block 410, the system may determine that the meeting has ended. As described above, this determination may be made in response to an explicit command from the participant ("OK Assistant, let's conclude THE MEETING (OK, assistant, we end meeting)") inferred from the user's utterance ("THIS MEETING IS adjourned (this meeting break)") or in response to other user input, such as clicking on a surface of a separate interactive speaker that is being used as a meeting computing device. In response to the determination of block 410, at block 412, the system may set the automated assistant 120 to a non-conference mode, wherein the automated assistant needs to invoke before performing speech-to-text processing on the respective spoken utterances.
At block 414, in some implementations, the system may generate a meeting summary based on, for example, semantic processing of a plurality of utterances provided by the meeting participants during the meeting. As described above, a meeting summary can include things such as the subject matter in question, action items (creation, resolution, modification, etc.), participants, and/or a partial or complete transcript of the meeting. In some implementations, the literal records may be annotated or otherwise annotated in a manner that includes not only the utterances of the participants, but also any information injected into the meeting by the automated assistant 120.
Fig. 5 is a block diagram of an example computing device 510 that may be used in alternative ways to perform one or more aspects of the techniques described herein. The computing device 510 generally includes at least one processor 514, the at least one processor 514 communicating with a plurality of peripheral devices via a bus subsystem 512. These peripheral devices may include a storage subsystem 524 (e.g., a memory subsystem 525 and a file storage subsystem 526), a user interface output device 520, a user interface input device 522, and a network interface subsystem 516. Input and output devices allow a user to interact with computing device 510. Network interface subsystem 516 provides an interface to external networks and is coupled to corresponding interface devices among other computing devices.
User interface input devices 522 may include a keyboard, a pointing device such as a mouse, trackball, touch pad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways of inputting information into computing device 510 or onto a communication network.
The user interface output device 520 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a cathode ray tube display (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for producing a viewable image. The display subsystem may also provide for non-visual displays, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices, as well as ways of outputting information from computing device 510 to a user or to another machine or computing device.
Storage subsystem 524 stores programs and data constructs that provide the functionality of some or all of the modules described herein. For example, storage subsystem 524 may include logic to perform selected aspects of the method of FIG. 4 and to implement the various components depicted in FIG. 1.
These software modules are typically executed by processor 514 alone or in combination with other processors. The memory 525 used in the storage subsystem 524 can include a plurality of memories including a main Random Access Memory (RAM) 530 for storing instructions and data during program execution, and a Read Only Memory (ROM) 532 for storing fixed instructions. File storage subsystem 526 is capable of providing persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive, and associated removable media, CD-ROM drive, optical disk drive, or removable media cartridge. Modules implementing the functionality of certain embodiments may be stored in storage subsystem 524 via file storage subsystem 526, or in other machines accessible by one or more processors 514.
Bus subsystem 512 provides a mechanism for allowing the various components and subsystems of computing device 510 to communicate with each other in a desired manner. Although bus subsystem 512 is shown schematically as a single bus, alternative embodiments of the bus subsystem may use multiple buses.
Computing device 510 can be of various types including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 510 depicted in FIG. 5 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of computing device 510 may have more or fewer components than the computing device depicted in fig. 5.
Certain embodiments discussed herein may provide a user with one or more opportunities to control whether to collect information, store personal information, use personal information, and how to use information about the user in the event that personal information about the user (e.g., user data extracted from other electronic communications, information about the user's social network, the user's location, the user's time, biometric information of the user, and activities and demographics of the user, relationships between users, etc.) is collected or used. That is, the systems and methods discussed herein collect, store, and/or use personal information of a user only after explicit authorization is received from an associated user.
For example, a user is provided with control over whether a program or component gathers user information about that particular user or other users associated with the program or component. Each user whose personal information is to be collected is provided with one or more options to allow control of the collection of information relating to that user to provide permission or authorization as to whether to collect the information and which portions of the information to collect. For example, one or more such control options can be provided to the user over a communications network. In addition, some data may be processed in one or more ways before it is stored or used, such that personal identification information is removed. As one example, the identity of the user may be processed such that personal identity information cannot be determined. As another example, the geographic location of a user may be generalized to a larger area such that a particular location of the user cannot be determined.
Although several embodiments have been described and illustrated herein, a variety of other means and/or structures for performing a function and/or obtaining results and/or one or more of the advantages described herein may be utilized and each such change and/or modification is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, any combination of two or more such features, systems, articles, materials, kits, and/or methods is included within the scope of the present disclosure.
Claims (20)
1. A method implemented by one or more processors for automated assistance, the method comprising:
Setting the automated assistant, implemented at least in part on one or more conference computing devices, to a conference mode in which the automated assistant performs speech-to-text processing on a plurality of different spoken utterances exchanged during a conversation between a plurality of participants without explicitly invoking the automated assistant prior to each of the plurality of different spoken utterances;
automatically performing, by the automated assistant, semantic processing on first text generated from the speech-to-text processing of one or more of the plurality of different spoken utterances, wherein the semantic processing is performed without explicit participant invocation;
Generating, by the automated assistant, data related to the first text based on the semantic processing;
monitoring, by the automated assistant, for pauses from the conversation;
aurally outputting the data related to the first text to the plurality of participants at one or more of the conference computing devices in response to detecting a pause in the conversation based on the monitoring while the automated assistant is in conference mode;
In response to determining that one or more criteria are met before the pause is detected, refraining from audibly outputting the data related to the first text.
2. The method of claim 1, further comprising: in response to determining that the one or more criteria are met before the pause is detected, visually outputting the data related to the first text on a display accessible to at least one of the plurality of participants.
3. The method of claim 1, wherein the one or more criteria comprise an elapse of a predetermined time interval since one or more of the plurality of different spoken utterances.
4. The method of claim 1, wherein the one or more criteria include detecting a new topic of the conversation.
5. The method of claim 1, wherein the one or more criteria comprise detecting a change in context of the conversation.
6. The method of claim 1, wherein the avoiding comprises discarding the data related to the first text.
7. The method of claim 1, further comprising: the data is output after the end of the conversation in response to determining that the one or more criteria are met before the pause is detected.
8. A system for an automated assistant comprising one or more processors and a memory storing instructions that, when executed by the one or more processors, cause the one or more processors to perform operations comprising:
Setting the automated assistant, implemented at least in part on one or more conference computing devices, to a conference mode in which the automated assistant performs speech-to-text processing on a plurality of different spoken utterances exchanged during a conversation between a plurality of participants without explicitly invoking the automated assistant prior to each of the plurality of different spoken utterances;
automatically performing, by the automated assistant, semantic processing on first text generated from the speech-to-text processing of one or more of the plurality of different spoken utterances, wherein the semantic processing is performed without explicit participant invocation;
Generating, by the automated assistant, data related to the first text based on the semantic processing;
monitoring, by the automated assistant, for pauses from the conversation;
In response to detecting a pause in the conversation while the automated assistant is in a conference mode, audibly outputting the data related to the first text to the plurality of participants at one or more of the conference computing devices;
In response to determining that one or more criteria are met before the pause is detected, refraining from audibly outputting the data related to the first text.
9. The system of claim 8, wherein the operations further comprise: in response to determining that the one or more criteria are met before the pause is detected, visually outputting the data related to the first text on a display accessible to at least one of the plurality of participants.
10. The system of claim 8, wherein the one or more criteria include the passage of a predetermined time interval since one or more of the plurality of different spoken utterances.
11. The system of claim 8, wherein the one or more criteria include detecting a new topic of the conversation.
12. The system of claim 8, wherein the one or more criteria comprise detecting a change in context of the conversation.
13. The system of claim 8, wherein the avoiding comprises discarding the data related to the first text.
14. The system of claim 8, wherein the operations further comprise: the data is output after the end of the conversation in response to determining that the one or more criteria are met before the pause is detected.
15. A non-transitory computer-readable medium comprising instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
Setting an automated assistant implemented at least in part on one or more conference computing devices to a conference mode in which the automated assistant performs speech-to-text processing on a plurality of different spoken utterances exchanged during a conversation between a plurality of participants without explicitly invoking the automated assistant prior to each of the plurality of different spoken utterances;
automatically performing, by the automated assistant, semantic processing on first text generated from the speech-to-text processing of one or more of the plurality of different spoken utterances, wherein the semantic processing is performed without explicit participant invocation;
Generating, by the automated assistant, data related to the first text based on the semantic processing;
monitoring, by the automated assistant, for pauses from the conversation;
aurally outputting the data related to the first text to the plurality of participants at one or more of the conference computing devices in response to detecting a pause in the conversation based on the monitoring while the automated assistant is in conference mode;
In response to determining that one or more criteria are met before the pause is detected, refraining from audibly outputting the data related to the first text.
16. The non-transitory computer-readable medium of claim 15, wherein the operations further comprise: in response to determining that the one or more criteria are met before the pause is detected, visually outputting the data related to the first text on a display accessible to at least one of the plurality of participants.
17. The non-transitory computer-readable medium of claim 15, wherein the one or more criteria include an elapse of a predetermined time interval since one or more of the plurality of different spoken utterances.
18. The non-transitory computer-readable medium of claim 15, wherein the one or more criteria include detecting a new topic of the conversation.
19. The non-transitory computer-readable medium of claim 15, wherein the one or more criteria include detecting a change in context of the conversation.
20. The non-transitory computer-readable medium of claim 15, wherein the avoiding comprises discarding the data related to the first text.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762580982P | 2017-11-02 | 2017-11-02 | |
US62/580,982 | 2017-11-02 | ||
US15/833,454 US10645035B2 (en) | 2017-11-02 | 2017-12-06 | Automated assistants with conference capabilities |
US15/833,454 | 2017-12-06 | ||
CN201880039481.3A CN110741601B (en) | 2017-11-02 | 2018-10-30 | Automatic assistant with conference function |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880039481.3A Division CN110741601B (en) | 2017-11-02 | 2018-10-30 | Automatic assistant with conference function |
Publications (2)
Publication Number | Publication Date |
---|---|
CN112463104A CN112463104A (en) | 2021-03-09 |
CN112463104B true CN112463104B (en) | 2024-05-14 |
Family
ID=
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2006330170A (en) * | 2005-05-24 | 2006-12-07 | Nhk Engineering Services Inc | Recording document preparation support system |
US7603413B1 (en) * | 2005-04-07 | 2009-10-13 | Aol Llc | Using automated agents to facilitate chat communications |
WO2014071152A1 (en) * | 2012-11-01 | 2014-05-08 | Gutman Ronald David | Teleconferencing for participants at different locations |
CN105447578A (en) * | 2014-09-24 | 2016-03-30 | 三星电子株式会社 | Conference proceed apparatus and method for advancing conference |
CN106471570A (en) * | 2014-05-30 | 2017-03-01 | 苹果公司 | Order single language input method more |
CN106486116A (en) * | 2015-08-26 | 2017-03-08 | 重庆西线科技有限公司 | A kind of online generation method of on-the-spot meeting summary |
CN107210045A (en) * | 2015-02-03 | 2017-09-26 | 杜比实验室特许公司 | The playback of search session and search result |
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7603413B1 (en) * | 2005-04-07 | 2009-10-13 | Aol Llc | Using automated agents to facilitate chat communications |
JP2006330170A (en) * | 2005-05-24 | 2006-12-07 | Nhk Engineering Services Inc | Recording document preparation support system |
WO2014071152A1 (en) * | 2012-11-01 | 2014-05-08 | Gutman Ronald David | Teleconferencing for participants at different locations |
CN106471570A (en) * | 2014-05-30 | 2017-03-01 | 苹果公司 | Order single language input method more |
CN105447578A (en) * | 2014-09-24 | 2016-03-30 | 三星电子株式会社 | Conference proceed apparatus and method for advancing conference |
CN107210045A (en) * | 2015-02-03 | 2017-09-26 | 杜比实验室特许公司 | The playback of search session and search result |
CN106486116A (en) * | 2015-08-26 | 2017-03-08 | 重庆西线科技有限公司 | A kind of online generation method of on-the-spot meeting summary |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110741601B (en) | Automatic assistant with conference function | |
US11552814B2 (en) | Proactive provision of new content to group chat participants | |
JP7419485B2 (en) | Proactively incorporating unsolicited content into human-to-computer dialogs | |
US11727220B2 (en) | Transitioning between prior dialog contexts with automated assistants | |
CN112463104B (en) | Automatic assistant with conference function | |
CN115605871A (en) | Recommending actions based on an entity or entity type |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant |