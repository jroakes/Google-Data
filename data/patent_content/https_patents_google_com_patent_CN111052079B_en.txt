CN111052079B - Systems/methods and apparatus for providing multi-function links for interacting with assistant agents - Google Patents
Systems/methods and apparatus for providing multi-function links for interacting with assistant agents Download PDFInfo
- Publication number
- CN111052079B CN111052079B CN201880039715.4A CN201880039715A CN111052079B CN 111052079 B CN111052079 B CN 111052079B CN 201880039715 A CN201880039715 A CN 201880039715A CN 111052079 B CN111052079 B CN 111052079B
- Authority
- CN
- China
- Prior art keywords
- application
- selectable
- computing device
- assistant agent
- user
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 title claims abstract description 52
- 230000003993 interaction Effects 0.000 claims abstract description 7
- 230000009471 action Effects 0.000 claims description 40
- 230000004044 response Effects 0.000 claims description 38
- 230000002093 peripheral effect Effects 0.000 claims description 14
- 230000015654 memory Effects 0.000 claims description 9
- 230000000153 supplemental effect Effects 0.000 claims description 7
- 230000000977 initiatory effect Effects 0.000 claims 3
- 239000003795 chemical substances by application Substances 0.000 description 135
- 230000006870 function Effects 0.000 description 10
- 230000008859 change Effects 0.000 description 6
- 239000000463 material Substances 0.000 description 5
- 238000004891 communication Methods 0.000 description 3
- 230000002452 interceptive effect Effects 0.000 description 3
- 230000008569 process Effects 0.000 description 3
- 238000001816 cooling Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000010438 heat treatment Methods 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 230000008901 benefit Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000012545 processing Methods 0.000 description 1
- 230000001737 promoting effect Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 239000002699 waste material Substances 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
- G06F16/90332—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
- G06F40/35—Discourse or dialogue representation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F8/00—Arrangements for software engineering
- G06F8/60—Software deployment
- G06F8/61—Installation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/445—Program loading or initiating
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/445—Program loading or initiating
- G06F9/44505—Configuring for program initiating, e.g. using registry, configuration files
- G06F9/4451—User profiles; Roaming
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72403—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72448—User interfaces specially adapted for cordless or mobile telephones with means for adapting the functionality of the device according to specific conditions
- H04M1/72454—User interfaces specially adapted for cordless or mobile telephones with means for adapting the functionality of the device according to specific conditions according to context-related or environment-related conditions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72469—User interfaces specially adapted for cordless or mobile telephones for operating the device by selecting functions from two or more displayed items, e.g. menus or icons
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W4/00—Services specially adapted for wireless communication networks; Facilities therefor
- H04W4/30—Services specially adapted for particular environments, situations or purposes
- H04W4/33—Services specially adapted for particular environments, situations or purposes for indoor environments, e.g. buildings
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W4/00—Services specially adapted for wireless communication networks; Facilities therefor
- H04W4/30—Services specially adapted for particular environments, situations or purposes
- H04W4/38—Services specially adapted for particular environments, situations or purposes for collecting sensor information
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W4/00—Services specially adapted for wireless communication networks; Facilities therefor
- H04W4/70—Services for machine-to-machine communication [M2M] or machine type communication [MTC]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/04817—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance using icons
Abstract
Methods, devices, systems, and computer-readable media are provided for introducing functionality of various applications to a user through interaction with an assistant agent. The assistant agent may correspond to an assistant application that may provide a plurality of selectable elements for the user interface, each selectable element may correspond to a separate application. When the user selects one of the selectable elements, the functionality of the application may be presented to the user so that the user may be more familiar with the functionality of the application. In some implementations, a portion of the selectable elements may be selected to cause information about the application to be presented to the user. This allows the user the option of trying or knowing the application before putting into computing resources by downloading and installing the entire application.
Description
Background
People may engage in human-machine conversations using an interactive software application referred to herein as "automated assistant" (also referred to as "digital agent," "chat robot," "interactive personal assistant," "assistant agent," "intelligent personal assistant," "conversation agent," etc.). For example, people (which may be referred to as "users" when they interact with an automated assistant) may interact with the automated assistant by providing commands using voice natural language input (i.e., utterances) that may be converted to text in some cases and then processed, and/or by providing text (e.g., typed) natural language input. Some automated assistants allow users to control certain peripheral devices through voice input, however, some users may not be aware that other applications may also be used to control the peripheral device. Furthermore, users may not want to waste computing resources to download and install applications that they are unfamiliar with and do not know how to operate. While some automated assistants may provide search query results to users regarding certain applications, users may still wish to experience certain functions of an application without having to download and install the entire application.
Disclosure of Invention
The present disclosure relates to systems, methods, and devices for introducing functionality of various applications to a user through interaction with an assistant agent. This may allow the user to be more familiar with the functionality of the application without causing consumption of computing resources by searching for information about the functionality and the application. The user may open the assistant agent application while interacting with a computing device, such as a cellular telephone, a tablet computer, a television, and/or any other type of computing device. The assistant agent may provide a home page or landing page that includes fields for receiving commands from the user. The home page may also include an icon for opening a user interface having a plurality of selectable elements, and each selectable element may correspond to a different application accessible to the assistant agent.
Each selectable element provided at the user interface may include a first selectable portion and a second selectable portion. When the user selects the first selectable portion, the assistant agent may cause the corresponding application to perform the function of the application. For example, the first selectable portion may include text describing commands that a user may provide to the assistant agent to cause the assistant agent to interact with the application to perform a particular action. When the user selects the second selectable portion, the assistant agent may cause application information to be presented to the user. The application information may relate to application functionality, ratings, links to websites, availability of applications, compatibility of applications, and/or any other information that may be associated with an application or device. In some implementations, the application information presented to the user can include other selectable elements that, when selected, cause the assistant agent to interact with the application to perform different actions.
In some implementations, the selectable element may include a first selectable portion that, when selected, causes a reduced version of the application to be installed at a separate computing device accessible to the computing device and/or the assistant agent. A thin application may require less disk space or memory to install than a non-thin application and may be downloaded using correspondingly less network bandwidth. The reduced-version application may include fewer functions or operations than a non-reduced-version application. For example, the assistant agent may identify a plurality of different devices associated with one or more accounts of the user. The account may be stored on a server device that manages a database of entries, and at least one of the entries may identify one or more devices owned by the user. The assistant agent may use the entry to identify a device, such as a facility, a computer, an automobile, an accessory, and/or any other device that may be controlled by the assistant application. The assistant agent may also identify an application associated with the identified device and present selectable elements corresponding to the application.
In some implementations, selecting any portion of the selectable element can invoke an application identified by the selectable element or otherwise corresponding to the selectable element. In some implementations, in response to selection of the selectable element, a dialog session between the application and the user can be initialized with an assistant agent that acts as an intermediary between the application and the user. The dialog session may be invoked based on the command phrase identified at the selectable element. For example, the command phrase may be provided to an application or assistant agent. The assistant agent may process the command phrase and/or generate a structured command corresponding to the command phrase. The structured command may then be provided to an application to cause the application to perform one or more actions corresponding to the structured command.
As an example, the assistant agent may identify a device, such as a smart light bulb (e.g., a light bulb connected to a WiFi network), and search for an application that can control the device. The assistant agent may then generate a selectable element that identifies the application (i.e., the IoT application), and the selectable element includes a first selectable portion and a second selectable portion. The first selectable part may include text describing or otherwise associated with an action that may be performed by the IoT application. For example, the text may include "decrease the brightness of an intelligent light bulb in my home". In response to the user selecting the first selectable portion of the selectable element, the assistant agent may send a request or command to the IoT application and/or the smart light bulb to cause the smart light bulb to decrease its brightness level. In response to the user selecting the second selectable portion, the assistant agent may generate a query sent over the internet or other network to identify information related to the IoT application. The identified information may include additional commands and/or other application details that may be presented to the user. The identified information may then be presented on a user interface to encourage the user to further interact with the assistant agent and/or IoT application. In some implementations, the action performed in response to the user selecting the first selectable portion may include downloading a condensed version of the IoT application in order to demonstrate one or more functions of the IoT application. Additionally, when the user selects the second selectable portion, the user may be presented with a link to download a non-condensed or full version of the IoT application. In this way, a user can more conveniently try out the functionality of an application through the assistant interface without having to download the entire application, thereby avoiding taking up disk space or memory and bandwidth resources, and without wasting time and resources searching for information how to use the application.
The above description is provided as an overview of some embodiments of the present disclosure. Additional descriptions of these and other embodiments are provided herein.
In some implementations, a method implemented by one or more processors is set forth as including steps such as receiving a selection of a first selectable element displayed at a first user interface of a computing device. The first user interface may be provided by an assistant agent accessible to the computing device. The method may further comprise the steps of: generating a command phrase associated with an application accessible to the computing device; and causing the assistant agent to provide a second user interface comprising a plurality of selectable elements associated with a plurality of different applications accessible to the computing device. A given selectable element of the plurality of selectable elements may be specific to an application and include (i) a first selectable portion including text corresponding to a command phrase for controlling the assistant agent, and (ii) a second selectable portion for accessing additional information associated with the application. When the first selectable portion is selected, the steps may include causing the assistant agent to initialize a dialog session with the application based on the command phrase, receiving response content from the application, and causing the response content to be graphically presented in a dialog interface provided by the assistant agent. When the second selectable portion is selected, the step may include causing a third user interface to be displayed at the computing device. The third user interface may identify additional information associated with the application.
In some implementations, the command phrase can be generated based on historical interactions between the user and the assistant agent. When the second selectable portion is selected, the assistant agent can bypass sending the command phrase to the application. The command phrase may be identified at a third user interface with other command phrases. The dialog interface may include additional selectable elements corresponding to different command phrases available to control the application. The application may be configured to control the peripheral device and when a command phrase is received at the application, the command phrase may cause the application to adjust settings of the peripheral device. The peripheral device may be separate from the computing device and connected to a network to which the computing device is connected.
In other embodiments, a system is set forth that includes one or more processors and memory configured to store instructions that, when executed by the one or more processors, cause the one or more processors to perform steps that include identifying one or more devices connected to a proxy assistant accessible to the one or more processors. The proxy assistant may be configured to control one or more settings of one or more devices. The method may also include determining one or more command phrases available for controlling the identified one or more devices via the proxy assistant. The one or more command phrases, when provided to the agent assistant, may cause the agent assistant to perform one or more actions. The method may further include causing a plurality of different selectable elements to be presented at a user interface of the display device. One selectable element of the plurality of different selectable elements may correspond to a device application associated with a device of the identified one or more devices, and the selectable element may include (i) a first selectable portion including text identifying a command phrase of the one or more command units, and (ii) a second selectable portion. When the first selectable portion is selected, this step may include causing the apparatus to perform operations in accordance with the identified command phrase. When the second selectable portion is selected, this step may include providing information related to the device application at the user interface and bypassing causing the device to perform operations. When the second selectable portion is selected, the information may be presented at a separate user interface that includes other selectable elements. Other selectable elements may correspond to command phrases that, when selected, cause the device to perform different operations. When the first selectable portion is selected, the user interface may be updated to include different selectable elements corresponding to different command phrases. The operations may include displaying content at a display of the device. The plurality of different selectable elements may correspond to different applications and include discrete text identifying different command phrases for controlling the different applications.
In yet other implementations, a non-transitory computer-readable medium is set forth that stores instructions that, when executed by one or more processors, cause the one or more processors to perform steps comprising causing a user interface to be presented at a display device of a computing device. The user interface may include a plurality of different selectable elements corresponding to a plurality of different applications accessible to the assistant agent. The selectable element of the plurality of different selectable elements may identify an application of the plurality of different applications and include a selectable portion. The step may further include generating a command phrase corresponding to the application. The command phrase may correspond to an action performed at a separate computing device in response to selection of the selectable element. When a selection of the selectable portion is received, the steps may include causing the discrete computing device to perform an action, wherein the action includes providing a physical response perceptible to a user and causing a display device of the computing device to present information associated with the application. The information may include supplemental command phrases that correspond to different actions performed by the application at the separate computing device. The discrete device may include a discrete display device, and the physical response may correspond to an output presented at the discrete display device. The output may correspond to content retrieved by a separate computing device. The command phrase may be processed by the assistant agent in response to selection of the selectable portion. The assistant agent may cause the discrete computing device to perform an action in response to selection of the selectable portion. The assistant agent may correspond to an assistant application that is hosted as a different device than the computing device and the separate computing device. The assistant application is accessible to each computing device and to a separate computing device.
Additionally, some implementations include one or more processors of one or more computing devices, wherein the one or more processors are operable to execute instructions stored in an associated memory, and wherein the instructions are configured to cause performance of any of the foregoing methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by the one or more processors to perform any of the foregoing methods.
It should be understood that all combinations of the foregoing concepts and additional concepts described in more detail herein are considered a part of the subject matter disclosed herein. For example, all combinations of the claimed objects appearing at the end of the present disclosure are considered part of the subject matter disclosed herein.
Drawings
FIG. 1 illustrates a system for providing an assistant agent that can provide multiple functional selectable elements for introducing to a user operations available from various applications.
Fig. 2A, 2B, and 2C illustrate an assistant agent that provides selectable elements for controlling a plurality of different applications and/or receiving more information about an application.
FIG. 3 provides a view of a user accessing an assistant agent to provide various selectable elements for operating different applications accessible to the assistant agent and/or computing device.
FIG. 4 illustrates a method for more efficiently allowing a user of a computing device to interact with applications available through the computing device.
Fig. 5 illustrates a method for controlling a peripheral or remote device according to a command phrase suggested to a user based on the accessibility of the device to an assistant agent.
Fig. 6 illustrates a method for controlling an application at a separate computing device while additional information about the application is provided at a different computing device.
FIG. 7 is a block diagram of an example computer system.
Detailed Description
Fig. 1 illustrates a system 100 for providing an assistant agent that can provide a multi-function selectable element for introducing to a user operations available from various applications. The assistant agent may operate as part of an assistant application 118, which assistant application 118 is disposed at the computing device 102 or a remote device 124, such as the server device 112. The user may interact with the assistant agent via assistant interface 110, which may be a microphone, a camera, a touch screen display, a user interface, and/or any other device capable of providing interaction between the user and the application. For example, a user may initialize an assistant agent by providing voice, text, or graphical input to the assistant interface 110 to cause the assistant agent to perform functions (e.g., provide data, control peripheral devices, access discrete agents, etc.). The computing device 102 may include a display device 108, which may be a display panel that includes a touch interface for receiving touch inputs and/or gestures to allow a user to control applications of the computing device 102 via the touch interface.
Although a single computing device 102 is shown in fig. 1, in various embodiments, multiple computing devices may interact with the assistant application 118 in performing the various techniques disclosed herein. For example, a plurality of different selectable elements 120 may be provided to the assistant interface 110 of the computing device 102, and responsive data may be presented audibly and/or graphically on a separate computing device in response to selection of at least one selectable element 120. A separate computing device may be linked or paired with computing device 102. For example, separate computing devices may be linked to computing device 102 based on using the same user account at two computing devices, based on two computing devices connected to the same secure network, based on two computing devices in direct peer-to-peer communication with each other, and so on. As another example, the optional element may cause the assistant application 118 to send command(s) that cause the state of one or more peripheral devices (e.g., ioT devices) to be changed. For example, selection of the selectable element may cause a command to be sent from the IoT device application to a networked "smart" lamp, which may cause the lamp to turn on or off, change its lumen output, change its light output color, etc. Also, for example, selection of the selectable element may cause the IoT device application to send a command that causes the "intelligent" thermostat to change a set temperature of a heating or cooling system of the home 130, turn the heating or cooling system on or off, and so forth. Also, for example, selection of the selectable element 120 may additionally or alternatively cause a command to be sent from the IoT device application to a networked "smart" garage door opener in the home 130 and cause the garage door to open or close.
The computing device 102 may communicate with a remote device 124 over a network 122, such as the internet. The computing device 102 may offload computing tasks to the server device 112 in order to save computing resources at the computing device 102. For example, the server device 112 may host the assistant application 118, and the computing device 102 may send the input received at the assistant interface 110 to the server device 112. However, in some implementations, the assistant application 118 may be hosted at the computing device 102. In various implementations, all or less than all aspects of the assistant application 118 may be implemented on the computing device 102. In some of those implementations, aspects of the assistant application 118 are implemented via a native assistant application of the computing device 102 and interact with the server device 112 that implements other aspects of the assistant. Server device 112 may optionally serve multiple users and their associated assistant applications via multiple threads. In implementations in which all or less than all aspects of the assistant application 118 are implemented via a local assistant application of the computing device 102, the local assistant application may be an application separate from (e.g., installed "on top of") the operating system of the computing device 102, or alternatively implemented directly by (e.g., an application that is considered to be an operating system, but integrated with) the operating system of the computing device 102.
In some implementations, the server device 112 can include a speech-to-text engine 116 that can process audio data received at the assistant interface 110 to identify text embodied in the audio data. The process for converting audio data to text may include a speech recognition algorithm that may employ a neural network and/or statistical model to identify the set of audio data corresponding to the word or phrase. The text converted from the audio data may be used by the assistant application 118 as text data that may be used to generate and/or identify command phrases.
In some implementations, a user can interact with the assistant agent through the assistant interface 110 and cause the assistant agent to present a user interface that includes a plurality of different selectable elements. Each selectable element may correspond to a separate application accessible to computing device 102, server device 112, assistant application 118, and/or any other device that may be associated with a user. In some implementations, the server device 112 can include an optional element engine 114 for generating the optional element 120 based on user data 106, account data 132, and/or any other data available to the assistant application 118. For example, when a user is browsing the internet from their computing device 102, the user data 106 may correspond to browser data generated and/or stored by the computing device 102. The user data 106 may include an entry indicating that the user has searched for movies to watch. The optional element engine 114 may compare the user data 106 to other data such as account data 132 available at the remote device 126 over the network 122.
The optional element engine 114 may determine from the account data 132 that the user owns the television and/or that the television is connected in the user's home 130. The selectable element engine 114 may use the correlation between the user data 106 (e.g., movie search history) and the account data 132 (e.g., user owned television) to generate selectable elements 120 to be presented on the display device 108. For example, selectable element 120 may include a first selectable portion that includes text such as "Show me my recently searched movies on my television (show me most recently searched movie on my television)". The user may select a first selectable portion of selectable element 120 to cause the assistant agent to present the most recently searched movie on a television in home 130. Alternatively, the user may select a second selectable portion of selectable element 120 to cause the assistant agent to present information related to the features of the assistant agent. For example, the assistant agent may cause a separate user interface to be opened at the computing device 102 and display various selectable commands for interacting with the assistant agent. The selectable commands may include commands that may be based on user data 106, such as commands that may cause an assistant agent to interact with a television or present information including browser search history data.
Fig. 2A-2C illustrate an assistant agent that provides optional elements for controlling a plurality of different applications and/or receiving more information about the applications. A user interface may be presented at the computing device in response to a selection made at the computing device. Fig. 2A may correspond to a first user interface 200 disposed at a display 202 of a computing device. The first user interface 200 may include a plurality of different selectable elements 204, 206, and 208 for controlling different applications and/or devices. Each of the different selectable elements may include an application icon 210 for identifying applications corresponding to selectable elements 204, 206, and 208. Each selectable element may also include text identifying or describing a command that may be provided by the user to the assistant agent to cause the corresponding application to perform an action. For example, the selectable element 206 may correspond to an IoT device application, and the text provided at the selectable element 206 may include the phrase "Turn down the lights (turn down light)". The phrase may correspond to a command that, when spoken by a user to an assistant interface of the computing device, may cause the assistant agent to interact with the IoT device application and cause the IoT device application to perform an action. Text may be provided at the first selectable portion 212 of the selectable element and when the user selects the first selectable portion 212, the assistant agent may take an action to cause the action to be performed.
The selectable elements 204, 206, and 208 may also include a second selectable portion 214. When the user selects the second selectable part 214, the second selectable part 214 may cause the assistant agent to provide additional information about the application associated with the selectable element. For example, when the user selects the second selectable portion 214 of the selectable element 206, the assistant agent may retrieve information about the respective application (e.g., ioT device application) and present the retrieved information on a separate user interface.
Fig. 2B illustrates a second user interface 216 that may be provided to a user at a computing device in response to the user selecting the first selectable portion 212 of the selectable element 206. The second user interface 216 may provide an indication 222 (e.g., "The lights have been turned down (light turned low)") via the IoT device application that an action has been performed in response to the user selecting the first selectable portion 212. The IoT device application may be loaded onto a computing device or may be accessed by an assistant agent. For example, the IoT device application may be an application that the user has not yet installed on his computing device, but is still able to attempt various functions of the IoT device application (e.g., by installing a reduced version using an assistant agent). In some implementations, the second user interface 216 can provide additional command recommendations 224 for controlling IoT device applications. Further, the second user interface 216 may include an interface icon 218 and/or a field 220 to indicate that a user may provide voice or typed commands to the assistant agent to cause the assistant agent to control the IoT device application. For example, the user may speak one of the additional commands recommended at the second user interface 216 (e.g., "change the lights' color (change color of light)") and in response, the assistant agent may provide the command to the IoT device application. The IoT device application may then perform an action (e.g., change the output color of the light) in response to receiving the command from the assistant agent.
In some implementations, by selecting either the first selectable part 212 or the second selectable part 214, the assistant agent can cause the dialog interface to open and cause the command phrase identified in the first selectable part 212 to be received by the assistant agent. Thereafter, the assistant agent can cause the application corresponding to the selectable element to perform an action associated with the command phrase. The dialog interface may include a field 220 for the assistant agent to receive other commands from the user. For example, a user may speak a command into a computing device, and the spoken command may be converted to text and then provided to an assistant agent. The assistant agent may then interact with the application to cause the application to perform actions associated with the voice command. In this way, the user can talk to the assistant agent, ultimately allowing the user to understand how to operate a particular application.
Fig. 2C illustrates a third user interface 226 that may be provided at the computing device 202 when the user selects the second selectable portion 214 of the selectable element 206. The third user interface 226 may include additional information regarding the application identified at the selectable element 206. For example, the third user interface 226 may identify a supplemental command 228 for interacting with the IoT device application. In some implementations, the third user interface 226 can provide supplemental commands 228 for retrieving additional information about IoT device applications. For example, the supplemental commands 228 may include, for example, "operate in a low power mode (operate in low power mode)", which may cause the assistant agent to provide commands to the IoT device application for causing one or more devices in the user's home to operate in low power mode when the user speaks "operate in a low power mode (operate in low power mode)". Alternatively, the user may select the selectable element corresponding to the command "tell me about my lights (tell me about my lights)" to cause the assistant agent to retrieve information about any lights associated with the user (e.g., associated with the user's account). The assistant agent may then present the information on the display of the computing device 202 and/or provide an audio output to the user describing the information about the light retrieval.
In some implementations, when the user selects either the first selectable part 212 or the second selectable part 214, the assistant agent can retrieve information related to the application corresponding to the selected selectable element. For example, the information may include a version of the application, compatibility of the application, ratings and/or comments by other users of the application, a shortcut and/or links to other information or commands for the application, settings for the application, and/or any other information that may be associated with the application.
In some implementations, the second selectable portion 214 can be a chat icon that, when selected, causes the second user interface 216 to appear on the computing device. Additionally, in response to the selected chat icon, the command phrase identified in the first selectable part 212 may be received by the assistant agent, and the assistant agent may cause the application to perform an action associated with the command phrase. Further, when the user selects any other portion of the selectable elements, a third user interface 226 may appear on the computing device in addition to the chat icon. The third user interface 226 may provide command recommendations and/or additional information about the application to the user. Further, the third user interface 226 may include links for downloading applications or otherwise trying out applications, and/or links for entering a conversation interface to chat with an assistant agent regarding applications. In this way, the user will be able to request specific information about the application from the assistant agent.
FIG. 3 provides a view 300 of a user 302 accessing an assistant agent to provide various selectable elements 308 and 310 for operating different applications accessible to the assistant agent and/or a computing device 304. In particular, the user 302 may access the assistant agent through the computing device 304, which may display a home page or landing page of the assistant agent on the first user interface 306. In particular, the first user interface 306 of the computing device 304 may provide selectable elements 308 and 310 generated by an assistant application corresponding to an assistant agent or by a selectable element engine. Each selectable element may include an identifier 320, the identifier 320 identifying the application to which the respective selectable element corresponds. Additionally, each selectable element may include a first selectable portion 326 and a second selectable portion 314.
When the user 302 selects the first selectable portion 326 of the selectable element 308, the assistant agent may interact with the application corresponding to the selectable element 308. The application may be an interactive educational application and provide a lecture for the user 302 to view. In some implementations, the educational application may not be installed at the computing device 304, but rather the assistant agent may cause the condensed version to be presented to the user 302 when the user selects the first selectable portion 326. In response to selection of the first selectable component 326, a command "play a select" may be executed by the assistant agent so that the assistant agent may cause the educational application to open a lecture video on a device associated with the user. For example, the device may include a television 324 separate from the computing device 304.
In some implementations, in response to user 302 selecting first selectable portion 326, discrete interface 312 may be opened at computing device 304 to provide additional information regarding the educational application. A separate interface 312 may be presented at the computing device 304 while the television 324 displays the educational application. In this way, the user 302 is able to use the application while also learning additional functions of the application. In some implementations, the application provided in response to selection of the first selectable portion 326 can be presented in the interface 316 of a separate device such as the television 324. Additionally, the assistant agent may simultaneously provide suggestions for other commands 322 that may be employed by the assistant agent to interact with the application. Other commands may correspond to actions that may be performed by the application through computing device 304 and/or a separate device. For example, the assistant agent may perform a query for identification commands that may be received by an application through both computing device 304 and a separate device (e.g., television 324). Thereafter, as a result of the query, the identified command may be presented at the computing device 304, a separate device, and/or both devices. For example, one of the other commands may include the phrase "ask a query" that when spoken by the user may cause a dialog box to open for receiving questions from the user 302 when the educational application is operated.
While the application is presented on a separate device, the user 302 may select the second selectable portion 314 of the selectable element 308 to cause additional information about the application to appear on the separate interface 312. For example, rather than relying on versions accessed through an assistant agent, the discrete interface 312 may include links for downloading a complete version of the application. Alternatively, the discrete interface 312 may provide the most reliable commands associated with the application corresponding to the selectable element 308. For example, the assistant agent or assistant application may execute a query to identify general commands used by different users of the application (e.g., educational application) and present the most common commands on the separate interface 312 and/or on the separate device.
Fig. 4 illustrates a method 400 for more efficiently allowing a user of a computing device to interact with applications available through the computing device. Method 400 may be performed by one or more computing devices and/or any other apparatus capable of interacting with an assistant agent. The assistant agent may be an application that is hosted, in whole or in part, at the computing device and/or the server device. The method 400 may include a block 402 of receiving a selection of a first selectable element displayed on a first user interface of a computing device. The first user interface may be provided by an assistant agent accessible to the computing device. For example, the first user interface may correspond to a landing page or home screen for interacting with the assistant agent. The first user interface may include a dialog element (e.g., a text field or a microphone icon) indicating that the assistant agent is capable of receiving text, language, and/or other gestures for controlling the assistant agent. Further, the first selectable element may correspond to a link to a separate user interface that provides information related to a plurality of different applications accessible to the computing device. For example, the computing device may access or include a music application configured to download streaming music from the internet. Alternatively, the computing device may access or include an internet of things (IoT) application for controlling IoT devices in the user's home.
The method 400 may include, at block 404, generating a command phrase associated with an application accessible to a computing device. The command phrase may be a utterable command that causes the assistant agent to interact with the application when the user utters the command phrase to an assistant interface (e.g., a microphone of a computing device). For example, when the application is a music application, the command phrase may be a utterable command such as "Assistant, please play my favorite song (Assistant, please play me favorite song)". The command phrase may cause the assistant agent to invoke the music application to identify a favorite song of the user using historical data corresponding to interactions between the user and the music application.
The method 400 may also include a block 406 that causes the assistant agent to provide a second user interface that includes a plurality of selectable elements associated with a plurality of different applications accessible to the computing device. The second user interface may be provided at the display of the computing device or a separate computing device. The selectable element of the plurality of selectable elements may be specific to a given application and include a first selectable portion including text corresponding to a command phrase for controlling the assistant agent. The selectable element may also include a second selectable portion for accessing additional information associated with the application. The additional information may correspond to links, ratings, reviews, availability, compatibility, and/or any other information that aids in understanding the application.
At block 408 of method 400, it is determined whether to select the first selectable portion or the second selectable portion of the selectable element. The user may make the selection by voice command (e.g., "Select the left button (select left button)"), by gesture (e.g., a pointing action toward the selectable portion), touch input, and/or any other user input that may identify the selectable portion. If the first selectable portion is selected, at block 410, the command phrase is provided to the application and, at block 412, response content is received from the application. For example, when the command phrase corresponds to a request to play a favorite song through a music application, then the response content may be song data downloaded from a server device over the internet. The command phrase may be provided to an assistant agent, which may interact with the application to complete an action (e.g., play a favorite song) corresponding to the command phrase.
If the second selectable portion is selected at block 408, information about the application is displayed on a computing device or an assistant agent and/or a separate device accessible to the application at block 414. In some implementations, the additional information may include a link for accessing additional commands for controlling the application. Alternatively, the additional information may include a link for installing the application at a separate device accessible to the computing device or the assistant agent.
Fig. 5 illustrates a method 500 for controlling a peripheral device or a remote device according to a command phrase suggested to a user based on accessibility of the device to an assistant agent. Method 500 may be performed by one or more computing devices and/or any other apparatus suitable for interacting with an assistant application. The method 500 may include a block 502 of identifying one or more devices connected to one or more processor-accessible assistant agents. The assistant agent may be configured to control one or more settings of one or more devices. For example, the assistant agent may receive command phrases from a user of the assistant agent for controlling various devices and applications accessible to the assistant agent. The device may include an IoT device connected to a home WiFi network available in a user's home. Further, the computing device with which the user interacts with the assistant agent may be an IoT device that includes a microphone for receiving voice commands and at least one speaker for providing responses. Further, ioT devices may include televisions, smart lights, robotic devices, thermostats, facilities such as refrigerators or stoves, and/or any other device capable of interacting with a computing device.
The method 500 may also include determining one or more command phrases that are available to control the identified one or more devices via the proxy assistant 504. When provided to the assistant agent by the user, the one or more command phrases may cause the agent assistant to perform one or more actions, such as, for example, initializing a music application to play a favorite song or changing a setting of the IoT device. The one or more command phrases may be determined using data available at a remote device storing commands for controlling the one or more devices. Alternatively, the assistant agent may access one or more devices and determine the commands available to control the devices by directly or indirectly interacting with the devices. For example, the assistant agent may access the device to identify an instruction file or configuration file available at the device in order to compile commands that may be used by the assistant agent to control the device. In some implementations, the command may be obtained from an entity responsible for promoting the device (e.g., a third party website or agent), and thus may recommend the command that is most convenient and/or reliable for the user to execute.
The method 500 may also include a block 506 that causes a plurality of different selectable elements to be presented at a user interface of a display device of the computing device. Each selectable element may be independently associated with a different application or device. Further, each of the different selectable elements may include a first selectable part and a second selectable part. In some implementations, the first selectable portion of the selectable element may include text associated with a command for controlling a device paired with, in communication with, or otherwise accessible to the assistant agent. For example, the device may be an IoT device that may include a music application, and the text may include the command phrase "Play my favorite song (play me favorite song)". Further, the selectable element may identify the music application and/or IoT device, thereby noticing that their selection of the selectable element will invoke a response or action at the IoT device and/or music application.
The method 500 may include a block 508 of determining whether to select a first selectable portion or a second selectable portion of the selectable element. If the first selectable portion of the selectable element is selected, the method 500 may proceed to block 510, which includes causing the apparatus to perform operations in accordance with the identified command phrase. For example, when the device is an IoT device that includes a music application, the assistant agent may determine to select the first selectable portion and provide one or more commands to the music application. The command may cause the music application to perform an operation identified by or associated with the command phrase of the first selectable portion. For example, the assistant agent may cause the music application to play songs liked by the user from an output device (e.g., speaker) of the IoT device.
When the user selects the second selectable portion of the selectable element, the method 500 may proceed to block 512, which includes providing information related to the device application at the user interface and bypassing causing the device to perform an operation. By providing information related to the device application without having the device perform operations, a user may interface with the device application without invoking the device application. Instead, further information may be provided to the user in order to make more informed decisions about the operations that the device application may perform. In some implementations, the provided information may include additional optional elements corresponding to different commands. The commands identified in the provided information may be invoked by selecting additional optional elements. This allows a user to control a device or device application through the assistant agent simply by navigating a page of information about the device application.
Fig. 6 illustrates a method 600 for controlling an application at a separate computing device while additional information about the application is provided at a different computing device. Method 600 may be performed by one or more computing devices and/or any other apparatus suitable for interacting with an assistant application. The method 600 may include a block 602 that causes a user interface to be presented at a display device of a computing device. The user interface may include a plurality of different selectable elements corresponding to a plurality of different applications accessible to the assistant agent. An assistant agent may be an application provided at or accessible at a computing device over a network connection (e.g., over the internet). The selectable element of the plurality of different selectable elements may identify an application of the plurality of different applications and include a selectable portion. For example, the application may be a streaming media application capable of streaming content from the internet at a different device. The content may include movies, shows, and/or music, and the assistant agent may control the content being streamed in response to commands received from the user. Other applications corresponding to the plurality of different applications may include gaming applications, ioT-related applications, social networking applications, productivity applications, and/or any other application accessible to the computing device.
The method 600 may also include generating a command phrase for controlling the application, block 604. The command phrase may correspond to an action performed at a separate computing device in response to selection of the selectable element. For example, the action may be initializing a dialog interface at a separate computing device. The dialog interface may be a medium through which a user may provide command phrases to the assistant agent to cause the assistant agent to control an application. The dialog interface may include a field for entering a text command into the assistant agent. For example, a user may speak a command into an assistant interface (e.g., a microphone) at a computing device and cause the command to be provided in text form at a field of a dialog interface.
The method 600 may also include a block 606 of receiving a selection of a selectable element. The selection may be made through a touch interface provided at a display device of the computing device. Alternatively, the selection of the selectable element may be made by speaking a command (e.g., "Assistant, perform an operation with the application (Assistant, perform operation on application)") or "Assistant, select the link corresponding to the application (Assistant, select link corresponding to application)") to the Assistant interface of the computing device. In some implementations, the assistant interface may be provided at a separate computing device and/or a computing device that displays the selectable elements.
The method 600 may also include a block 608 that causes the discrete computing device to perform actions. The separate computing device may be, for example, a television that is connected to a WiFi network to which the computing device is also connected. Further, the action may be to cause the television to turn on and present a streaming application that provides digital content downloaded from the internet. Further, the method 600 at block 610 may include causing a display device of the computing device to present information associated with the application. The information may include supplemental command phrases corresponding to different actions performed by the application at the separate computing device. For example, the different actions may correspond to a search function of the streaming media application, and the supplemental command phrase may be "Search for popular comedy movies (search for popular comedy movies)". In this way, the assistant agent can employ a number of different devices to teach the user how to use the application. Furthermore, since the action execution and the presentation of information are processed simultaneously on different devices, it is possible to reserve the computing resources of a single device.
Fig. 7 is a block diagram of an example computer system 710. The computer system 710 generally includes at least one processor 714 that communicates with a plurality of peripheral devices via a bus subsystem 712. These peripheral devices may include: storage subsystem 724, which includes, for example, memory 725 and file storage subsystem 726; a user interface output device 720; user interface input 722; and a network interface subsystem 716. Input and output devices allow users to interact with computer system 710. The network interaction subsystem 716 provides an interface to external networks and is coupled to corresponding interface devices in other computer systems.
User interface input devices 722 may include a keyboard, a pointing device such as a mouse, trackball, touch pad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information onto the computer system 710 or a communication network.
The user interface output device 720 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for forming a viewable image. The display subsystem may also provide for a non-visual display, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computer system 710 to a user or another machine or computer system.
Storage subsystem 724 stores programs and data structures that provide the functionality of some or all of the modules described herein. For example, the storage subsystem 724 may include logic to perform selected aspects of the method 400, the method 500, the method 600, and/or the computing device 102, the server device 112, the remote device, the IoT device, and/or any other device or apparatus discussed herein.
These software modules are generally executed by processor 714 alone or in combination with other processors. The memory 725 used in the storage subsystem 724 may include a plurality of memories including a main Random Access Memory (RAM) 730 for storing instructions and data during program execution and a Read Only Memory (ROM) 732 in which fixed instructions are stored. File storage subsystem 726 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive, and associated removable media, CD-ROM drive, optical disk drive, or removable media cartridge. Modules implementing the functions of certain embodiments may be stored by file storage subsystem 726 in storage subsystem 724, or in other machines accessible by processor 714.
Bus subsystem 712 provides a mechanism for allowing the various components and subsystems of computer system 710 to communicate with each other as intended. Although bus subsystem 712 is shown schematically as a single bus, alternative embodiments of the bus subsystem may use multiple buses.
Computer system 710 may be of various types including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computer system 710 illustrated in FIG. 7 is intended only as a specific example for the purposes of illustrating some embodiments. Many other configurations of computer system 710 are possible with more or fewer components than the computer system shown in FIG. 7.
Where the system described herein gathers personal information about a user (or what is often referred to herein as a "participant") or may utilize personal information, the user may be provided with an opportunity to control whether or not programs or features gather user information (e.g., information about the user's social network, social actions or activities, profession, user preferences, or the user's current geographic location) or control whether and/or how to receive content more relevant to the user from the content server. Moreover, some data may be processed in one or more ways before being stored or used so that personal identity information is deleted. For example, the identity of the user may be processed such that personal identity information cannot be determined for the user, or the geographic location of the user may be summarized at a location where geographic location information (such as to a city, zip code, or state level) is obtained such that a particular geographic location of the user cannot be determined. Thus, the user may control how information about the user and/or usage is collected.
Although several embodiments have been described and illustrated herein, a variety of other means and/or structures for performing functions and/or achieving results and/or one or more advantages described herein may be utilized and each such variation and/or modification is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are exemplary and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or application for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, any combination of two or more such features, systems, articles, materials, kits, and/or methods is included within the scope of the present disclosure.
Claims (19)
1. A method implemented by one or more processors, the method comprising:
receiving a selection of a first selectable element displayed at a first user interface of a computing device, the first user interface provided via an assistant agent accessible to the computing device;
generating a command phrase associated with an application accessible to the computing device, the application being a separate application from the assistant agent,
wherein the command phrase includes natural language content that, when provided as voice input to the computing device, causes the assistant agent to initiate execution of an operation via the application;
in response to receiving the selection of the first selectable element, causing the assistant agent to provide a second user interface including a plurality of selectable elements that control a plurality of different applications accessible to the computing device,
wherein a given selectable element of the plurality of selectable elements is specific to the application and includes:
a first selectable part including text embodying the natural language content of the command phrase for initiating execution of the operation, an
A second optional portion for accessing additional information available via the application, an
Wherein each selectable element of the plurality of selectable elements other than the given selectable element includes:
a corresponding further first selectable portion including corresponding further text embodying natural language content of a further corresponding command phrase for initiating execution of a further corresponding operation via a corresponding further application, wherein the corresponding further text is different from text embodying the command phrase for initiating execution of the operation of the given selectable element, and
a corresponding further second optional portion for accessing corresponding further information available via the further application;
when the first selectable portion of the given selectable element is selected:
in response to the first selectable portion being selected, causing the assistant agent to initiate execution of the operation via the application,
receiving response content from the application, and
Causing the response content to be graphically presented in a dialog interface provided by the assistant agent; and
when the second selectable portion of the given selectable element is selected:
in response to the second selectable portion being selected, causing a third user interface to be displayed at the computing device, wherein the third user interface identifies the additional information associated with the application.
2. The method of claim 1, wherein the command phrase is generated based on historical interactions between a user and the assistant agent.
3. The method of claim 1, further comprising:
when the second selectable portion is selected:
causing the assistant agent to bypass sending the command phrase to the application.
4. The method of claim 1, wherein the command phrase is rendered with other command phrases at the third user interface.
5. The method of claim 1, wherein the dialog interface includes additional selectable elements corresponding to text embodying natural language content that can be used to control different command phrases of the application when provided as voice input to the computing device.
6. The method of any of claims 1-5, wherein the application is configured to control a peripheral device and when the command phrase is received at the application via the voice input provided by a user, the command phrase causes the application to adjust a setting of the peripheral device.
7. The method of claim 6, wherein the peripheral device is separate from the computing device and connected to a network to which the computing device is connected.
8. A system, comprising:
one or more processors; and
a memory configured to store instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
identify one or more devices controllable via an assistant agent accessible to the one or more processors,
wherein the assistant agent controls one or more settings of the one or more devices;
based on identifying the one or more devices, determining one or more command phrases that are available to control the identified one or more devices via the assistant agent,
wherein the one or more command phrases, when provided as voice input to the assistant agent by a user, cause the assistant agent to perform one or more actions;
Based on determining the one or more command phrases available, causing a plurality of different selectable elements to be presented at a user interface of a display device, wherein:
each selectable element of the plurality of different selectable elements corresponds to a device application controlling one of the identified one or more devices, the device application being separate from the assistant agent, and
each selectable element includes:
a corresponding first selectable portion including text identifying a corresponding command phrase of the one or more command phrases, and wherein the text embodies natural language content that, when provided as voice input to the assistant agent, causes the assistant agent to perform a corresponding operation, and
a corresponding second selectable portion;
when the corresponding first selectable portion is selected:
responsive to the corresponding first selectable portion of a given selectable element being selected, causing the apparatus to perform the corresponding operation in accordance with the corresponding command phrase; and
when the corresponding second selectable portion of the given selectable element is selected:
providing information related to the device application at the user interface in response to the corresponding second selectable portion being selected, and
Bypassing the device to perform the corresponding operation.
9. The system of claim 8, wherein the information is presented at a separate user interface including other selectable elements when the second selectable portion of the given selectable element is selected.
10. The system of claim 9, wherein the other selectable elements include text presentations of command phrases that cause the apparatus to perform different operations when one or more of the other selectable elements are selected.
11. The system of any of claims 8 to 10, wherein the user interface is updated to include different selectable elements corresponding to different command phrases when the corresponding first selectable portion of the given selectable element is selected.
12. The system of any of claims 8 to 10, wherein the operations comprise displaying content at a display of the device.
13. A non-transitory computer-readable storage medium storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
causing a user interface to be presented at a display device of a computing device, the user interface including a plurality of different selectable elements that control a plurality of different applications accessible by an assistant agent,
Wherein each selectable element of the plurality of different selectable elements identifies an application of the plurality of different applications and includes a corresponding selectable portion;
generates a command phrase corresponding to the corresponding application of the given selectable element,
wherein the command phrase identifies an action performed at a separate computing device in response to selection of the corresponding selectable portion of the given selectable element;
causing the user interface to present text embodying natural language content of the command phrase, wherein the natural language content of the command phrase, when provided as voice input by a user to the assistant agent, causes the corresponding application to perform the action identified in the command phrase; and
when a selection of the corresponding selectable portion of the given selectable element is received:
causing the discrete computing device to perform the action identified in the command phrase,
wherein the actions include providing a physical response that is perceivable by a user, and
causing the display device of the computing device to present information provided by the application,
wherein the information includes text embodying natural language content of supplemental command phrases corresponding to different actions that can be performed by the application at the discrete computing device.
14. The non-transitory computer-readable storage medium of claim 13, wherein the discrete device comprises a discrete display device and the physical response corresponds to an output presented at the discrete display device.
15. The non-transitory computer-readable storage medium of claim 14, wherein the output comprises content retrieved by the discrete computing device.
16. The non-transitory computer-readable storage medium of claim 13, wherein the command phrase is processed by the assistant agent in response to selection of the corresponding selectable portion.
17. The non-transitory computer-readable storage medium of claim 13, wherein the assistant agent causes the discrete computing device to perform the action in response to selection of the corresponding selectable portion of the given selectable element.
18. The non-transitory computer-readable storage medium of claim 13, wherein the assistant agent is an assistant application hosted as a different device than the computing device and the separate computing device.
19. The non-transitory computer-readable storage medium of claim 18, wherein the assistant application is accessible to each of the computing device and the discrete computing device.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202410062922.9A CN117992141A (en) | 2017-10-03 | 2018-10-03 | Systems/methods and apparatus for providing multi-function links for interacting with assistant agents |
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762567733P | 2017-10-03 | 2017-10-03 | |
US62/567,733 | 2017-10-03 | ||
US15/835,835 US10896050B2 (en) | 2017-10-03 | 2017-12-08 | Systems, methods, and apparatus that provide multi-functional links for interacting with an assistant agent |
US15/835,835 | 2017-12-08 | ||
PCT/US2018/054120 WO2019070821A1 (en) | 2017-10-03 | 2018-10-03 | Systems, methods, and apparatus that provide multi-functional links for interacting with an assistant agent |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202410062922.9A Division CN117992141A (en) | 2017-10-03 | 2018-10-03 | Systems/methods and apparatus for providing multi-function links for interacting with assistant agents |
Publications (2)
Publication Number | Publication Date |
---|---|
CN111052079A CN111052079A (en) | 2020-04-21 |
CN111052079B true CN111052079B (en) | 2024-02-02 |
Family
ID=65896109
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202410062922.9A Pending CN117992141A (en) | 2017-10-03 | 2018-10-03 | Systems/methods and apparatus for providing multi-function links for interacting with assistant agents |
CN201880039715.4A Active CN111052079B (en) | 2017-10-03 | 2018-10-03 | Systems/methods and apparatus for providing multi-function links for interacting with assistant agents |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202410062922.9A Pending CN117992141A (en) | 2017-10-03 | 2018-10-03 | Systems/methods and apparatus for providing multi-function links for interacting with assistant agents |
Country Status (4)
Country | Link |
---|---|
US (4) | US10896050B2 (en) |
EP (1) | EP3545412A1 (en) |
CN (2) | CN117992141A (en) |
WO (1) | WO2019070821A1 (en) |
Families Citing this family (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10896050B2 (en) | 2017-10-03 | 2021-01-19 | Google Llc | Systems, methods, and apparatus that provide multi-functional links for interacting with an assistant agent |
US10990373B2 (en) * | 2018-05-18 | 2021-04-27 | Nutanix, Inc. | Service managers and firmware version selections in distributed computing systems |
US10812951B2 (en) * | 2018-07-26 | 2020-10-20 | Sap Se | Integration and display of multiple internet of things data streams |
CN113794800B (en) * | 2018-11-23 | 2022-08-26 | 华为技术有限公司 | Voice control method and electronic equipment |
WO2021015801A1 (en) * | 2019-07-19 | 2021-01-28 | Google Llc | Condensed spoken utterances for automated assistant control of an intricate application gui |
CA3230911A1 (en) | 2021-09-07 | 2023-03-16 | Yoky Matsuoka | Systems and methods for ingesting task data and generating tasks from a browser |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102792320A (en) * | 2010-01-18 | 2012-11-21 | 苹果公司 | Intelligent automated assistant |
CN106462617A (en) * | 2014-06-30 | 2017-02-22 | 苹果公司 | Intelligent automated assistant for tv user interactions |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040172456A1 (en) * | 2002-11-18 | 2004-09-02 | Green Mitchell Chapin | Enhanced buddy list interface |
US9344612B2 (en) * | 2006-02-15 | 2016-05-17 | Kenneth Ira Ritchey | Non-interference field-of-view support apparatus for a panoramic facial sensor |
US9858925B2 (en) * | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US8386929B2 (en) * | 2010-06-22 | 2013-02-26 | Microsoft Corporation | Personal assistant for task utilization |
US20140049697A1 (en) * | 2012-08-14 | 2014-02-20 | Kentec Inc. | Television device and method for displaying virtual on-screen interactive moderator |
FR2996399B3 (en) * | 2012-09-28 | 2015-05-15 | Samsung Electronics Co Ltd | IMAGE PROCESSING APPARATUS AND CONTROL METHOD THEREFOR, AND IMAGE PROCESSING SYSTEM |
US9410815B1 (en) * | 2014-03-26 | 2016-08-09 | Google Inc. | System and method for displaying dynamic text content with a digital map |
US9666185B2 (en) * | 2014-10-06 | 2017-05-30 | Nuance Communications, Inc. | Automatic data-driven dialog discovery system |
US10229678B2 (en) * | 2016-10-14 | 2019-03-12 | Microsoft Technology Licensing, Llc | Device-described natural language control |
US20180267774A1 (en) * | 2017-03-16 | 2018-09-20 | Cisco Technology, Inc. | Conference assistant device with configurable user interfaces based on operational state |
US10896050B2 (en) | 2017-10-03 | 2021-01-19 | Google Llc | Systems, methods, and apparatus that provide multi-functional links for interacting with an assistant agent |
-
2017
- 2017-12-08 US US15/835,835 patent/US10896050B2/en active Active
-
2018
- 2018-10-03 WO PCT/US2018/054120 patent/WO2019070821A1/en unknown
- 2018-10-03 CN CN202410062922.9A patent/CN117992141A/en active Pending
- 2018-10-03 EP EP18795855.8A patent/EP3545412A1/en active Pending
- 2018-10-03 CN CN201880039715.4A patent/CN111052079B/en active Active
-
2020
- 2020-12-15 US US17/122,720 patent/US11243789B2/en active Active
-
2021
- 2021-12-16 US US17/552,824 patent/US11556360B2/en active Active
-
2023
- 2023-01-12 US US18/096,468 patent/US11947984B2/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102792320A (en) * | 2010-01-18 | 2012-11-21 | 苹果公司 | Intelligent automated assistant |
CN106462617A (en) * | 2014-06-30 | 2017-02-22 | 苹果公司 | Intelligent automated assistant for tv user interactions |
Also Published As
Publication number | Publication date |
---|---|
US10896050B2 (en) | 2021-01-19 |
US11243789B2 (en) | 2022-02-08 |
CN111052079A (en) | 2020-04-21 |
US20190102203A1 (en) | 2019-04-04 |
US11556360B2 (en) | 2023-01-17 |
WO2019070821A1 (en) | 2019-04-11 |
US20220107823A1 (en) | 2022-04-07 |
CN117992141A (en) | 2024-05-07 |
US11947984B2 (en) | 2024-04-02 |
US20230266981A1 (en) | 2023-08-24 |
US20210096890A1 (en) | 2021-04-01 |
EP3545412A1 (en) | 2019-10-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11579749B2 (en) | Providing composite graphical assistant interfaces for controlling various connected devices | |
CN111052079B (en) | Systems/methods and apparatus for providing multi-function links for interacting with assistant agents | |
JP7297836B2 (en) | Voice user interface shortcuts for assistant applications | |
KR102313474B1 (en) | System, method and apparatus for resuming conversation session via automated assistant | |
KR102391387B1 (en) | Initializing a conversation with an automated agent via selectable graphical element | |
US20220035643A1 (en) | Initializing a conversation with an automated agent via selectable graphical element | |
US11960837B2 (en) | Fulfillment of actionable requests ahead of a user selecting a particular autocomplete suggestion for completing a current user input | |
KR20200124298A (en) | Mitigate client device latency when rendering remotely generated automated assistant content | |
CN111771189A (en) | System, method and apparatus for providing dynamic automated response at mediation assistance application | |
US20210406261A1 (en) | Rendering interactive subsidiary application(s) in response to a search request | |
US20240064363A1 (en) | Voice-based scene selection for video content on a computing device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |