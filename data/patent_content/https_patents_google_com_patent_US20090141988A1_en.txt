US20090141988A1 - System and method of object recognition and database population for video indexing - Google Patents
System and method of object recognition and database population for video indexing Download PDFInfo
- Publication number
- US20090141988A1 US20090141988A1 US11/949,258 US94925807A US2009141988A1 US 20090141988 A1 US20090141988 A1 US 20090141988A1 US 94925807 A US94925807 A US 94925807A US 2009141988 A1 US2009141988 A1 US 2009141988A1
- Authority
- US
- United States
- Prior art keywords
- cluster
- module
- video
- frame
- objects
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 title claims abstract description 74
- 230000001815 facial effect Effects 0.000 claims description 216
- 230000008569 process Effects 0.000 claims description 10
- 238000000638 solvent extraction Methods 0.000 claims description 4
- 238000005192 partition Methods 0.000 claims 1
- 238000012545 processing Methods 0.000 abstract description 30
- 238000000605 extraction Methods 0.000 description 19
- 238000001514 detection method Methods 0.000 description 9
- 238000010586 diagram Methods 0.000 description 8
- 230000015654 memory Effects 0.000 description 7
- 238000002372 labelling Methods 0.000 description 5
- 230000008859 change Effects 0.000 description 3
- 238000012015 optical character recognition Methods 0.000 description 3
- 210000001747 pupil Anatomy 0.000 description 3
- 230000008901 benefit Effects 0.000 description 2
- 238000010606 normalization Methods 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 238000012795 verification Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 241001465754 Metazoa Species 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 238000013473 artificial intelligence Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000013213 extrapolation Methods 0.000 description 1
- 210000000887 face Anatomy 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000005291 magnetic effect Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7837—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content
- G06F16/784—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content the detected or recognised objects being people
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/23—Clustering techniques
- G06F18/232—Non-hierarchical techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/762—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using clustering, e.g. of similar faces in social networks
- G06V10/763—Non-hierarchical techniques, e.g. based on statistics of modelling distributions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/172—Classification, e.g. identification
Definitions
- This application relates to a system and method for processing digital media.
- FIG. 1 is a block diagram showing architecture within which a method and system of object recognition and database population for video indexing are implemented, in accordance with an example embodiment
- FIG. 2 is a block diagram of a video processing system, in accordance with an example embodiment
- FIG. 3 is a block diagram showing interrelations between various components of the video processing system of FIG. 2 , in accordance with an example embodiment
- FIG. 4 is a block diagram of a facial image extraction module, in accordance with an example embodiment
- FIG. 5 is a flow chart of a method for video processing, in accordance with an example embodiment
- FIG. 6 is a block diagram of a facial image clustering module, in accordance with an example embodiment
- FIG. 7 is a flow chart of a method for facial image clustering, in accordance with an example embodiment
- FIG. 8 is a block diagram of an environment within which a facial image clustering module is implemented, in accordance with an example embodiment.
- FIG. 9 is a diagrammatic representation of an example machine, in the form of a computer system, within which a set of instructions for causing the machine to perform any one or more of the methodologies discussed herein is executed.
- the example embodiments described herein may be implemented in an operating environment comprising software installed on a computer, in hardware, or in a combination of software and hardware.
- the technique may be utilized to detect any category of objects (e.g., facial images), but the term “facial image” will be used throughout this description to provide a clearer explanation of how the technique may work.
- the detection of the facial images may use various algorithms described below.
- the detected facial images may be normalized according to various criteria, which facilitate organization of the facial mages into clusters. Each cluster may contain facial images of one person, however, there may be more than one cluster created per one person because the confidence level of the system may not be high enough, at this point, to determine whether or not the facial images belong to the same person as the facial images in an existing cluster.
- the facial images may be compared to reference facial images.
- An increased efficiency is achieved by utilizing certain representative facial images from each cluster of facial images to compare to the reference facial images.
- the reference facial images may include facial images of people known to the system. If the system determines that the facial images in the cluster cannot be identified because there are no similar reference facial images, a manual identification may be performed.
- the cluster data pertaining to the identified images may be stored to a database and utilized to search the video clips from which the facial images are extracted.
- the stored data may include, among other things, the name of the person associated with the facial images, the times of appearances of the person in the video, and the location of the facial images in the video frames of the video clip.
- the data stored to the database may be utilized to search the video clips for people by keywords (e.g., Madonna). Data usage provides users with a better video viewing experience. For example, such data usage allows users to determine times in the video where the facial image associated with the keyword appears, and also to navigate through the video by the facial image appearances.
- FIG. 1 shows an example environment 100 , within which a method and system of facial image recognition and database population for video indexing may be implemented.
- the example environment 100 may comprise a user system 110 , a video processing facility 120 , a network 130 , a third party content provider 140 , and a satellite 150 .
- the user system 110 may further comprise a video viewing application 112 and a satellite dish 114 .
- the user system 110 may be a general purpose computer, a television set (TV), a personal digital assistant (PDA), a mobile telephone, a wireless device, and any other device capable of visual presentation of images (including text) acquired, stored, or transmitted in various forms.
- the video viewing application 112 may be any application software that facilitates display of a video via the user system 110 .
- the video viewing application 112 may run at or be distributed across the user system 110 , third party content provider 140 , and the video processing facility 120 .
- the satellite dish 114 in one example embodiment, is a type of antenna designed for the specific purpose of transmitting signals to and/or receiving signals from satellites.
- the satellite dish 114 may be of varying sizes and designs, and may be used to receive and transmit any type of digital data to and from a satellite.
- the satellite dish 114 may be located at the video processing facility 120 . It should be noted that the satellite dish 114 is just one of many means to provide network connectivity, and other types of network connectivity may be used.
- the video processing facility 120 may comprise a satellite dish 154 and a video processing system 200 .
- the satellite dish 154 may be similar to the satellite dish 114 described above.
- the video processing facility 120 may represent fixed, mobile, or transportable structures, including installed electrical and electronic wiring, cabling, and equipment and supporting structures, such as utilities, ground networks, wireless networks, and electrical supporting structures.
- the video processing system 200 is described by a way of example with reference to FIG. 2 .
- the video processing system 200 may be a general-purpose computer processor or a type of processor designed specifically for the receiving, creation and distribution of digital media.
- the video processing system 200 may include various modules such as a facial image extraction module 204 that provides extraction of facial images, a facial image clustering module 206 that clusters the facial images, and a suggestion engine 208 that automatically identifies the facial images by comparing the facial images to reference facial images stored in a reference database. Further modules may include a manual labeling interface 214 for manual identification of the facial images and the index database (DB) 218 to store searchable indexes.
- An example embodiment of the facial image extraction module 204 including various modules is described by way of example with reference to FIG. 4 below.
- a method that may be used to process video utilizing the facial image extraction module 204 is described by way of example with reference to FIG. 5 below.
- the facial image clustering module 206 utilized to cluster facial images extracted from the video, may reside at the video processing system 200 . In some example embodiments, more than one cluster may be created per person.
- An example embodiment of the facial image clustering module 206 including various modules is described by way of example with reference to FIG. 5 below.
- a method that may be utilized to process video at the facial image clustering module 206 is described by a way of example with reference to FIG. 8 below.
- the third party content provider 140 may comprise a digital media content generator 142 and a satellite dish 184 .
- the third party content provider 140 may be an entity that owns or has the rights to digital media content such as digital videos.
- the third party content provider 140 may be a news service that provides reports to digital media broadcasters.
- the digital media content generator 142 may be a software application generating video content and transmitting the video content via the satellite dish 184 or the network 130 , to be received at the video processing facility 120 .
- the satellite dish 184 may be similar to the satellite dish 114 described above.
- the network 130 may be a network of data processing nodes that are interconnected for the purpose of data communication.
- the video processing system 200 comprises a video receiving module 202 , a facial image extraction module 204 , a facial image clustering module 206 , a suggestion engine 208 , a cluster cache 210 , a buffered frame sequence processor 212 , a manual labeling interface 214 , and a number of databases.
- the databases comprise a cluster database (DB) 216 , an index DB 218 , and a patterns DB 220 .
- the video receiving module 202 may be configured to receive video frames from the buffered frame sequence processor 212 .
- there may be a specific number of frames received each time for example, 15 frames.
- the video may be received in time intervals, for example, a one-minute interval.
- the facial image extraction module 204 may be configured to extract facial images from the video frames, which are received by the video receiving module 202 from the buffered frame sequence processor 212 . Some frames may contain more than one facial image or no facial images at all. The facial image extraction module 204 may be configured to extract all facial images appearing in a single frame. If a frame does not contain any facial images, the frame may be dropped. The facial image extraction module 204 , in some example embodiments, may normalize the extracted facial images, as shown in FIG. 5 .
- the facial image clustering module 206 may be configured to save the normalized facial images once they are extracted by the facial image extraction module 204 .
- a method for clustering extracted images is described below by way of example with reference to method 700 of FIG. 7 .
- the suggestion engine 208 may be configured to label the normalized facial images with suggested identities of the person associated with the facial images in the cluster. In order to label the clusters, the suggestion engine 208 may compare the normalized facial images to reference facial images, and based on the comparison, may suggest the identity of the person associated with the facial image.
- the cluster cache 210 in an example embodiment, may be configured to store the clusters created by the facial image clustering module 206 until the clusters are labeled by the suggestion engine 208 . Once the clusters are labeled in the cluster cache 210 , they may be saved to the cluster DB 216 .
- the buffered frame sequence processor 212 may be configured to process video feeds received from the third party content provider 140 .
- a video feed may be partitioned into video clips of certain time durations or into video clips having a certain number of frames.
- the processed video frames may be received by the facial image extraction module 204 .
- the facial image extraction module 204 in an example embodiment, may be configured to process frames received from the buffered frame sequence processor 212 in order to detect facial images contained in the video frames.
- the facial image extraction module 204 may extract textual content of the video frames and save the textual content for further processing. Subsequently, the saved textual content may be processed to extract text that suggests the identity of the person appearing in the video.
- the manual labeling interface 214 may be a graphical user interface configured to provide an operator with a facial image from the cluster cache 210 , along with a set of reference facial images likely to be associated with the same person. The operator may visually compare and select, from the set of reference facial images, a facial image viewed as being associated with the same person as the facial image from the cluster cache 210 .
- the cluster DB 216 may be a database configured to store clusters of facial images and associated metadata extracted from the video feed.
- the facial images in the clusters stored in cluster DB 216 may be identified facial images.
- the metadata associated with the facial images in the clusters may be updated when previously unknown facial images in the cluster are identified.
- the cluster metadata may also be updated manually by comparing the cluster images to known reference facial images using the manual labeling interface 214 .
- the index DB 218 in an example embodiment, may be a database populated with the indexed records of the identified facial images, each facial image's position in the video frame(s) in which it appears, and the number of times the facial image appears in the video. The relationship between various components of the video processing system 200 is described by way of example with reference to FIG. 3 .
- the facial image extraction module 204 previously discussed in reference to FIGS. 2 and 3 is shown to include several components that may be configured to perform various operations.
- the facial image extraction module 204 may comprise a detecting module 2042 , a partitioning module 2044 , a discovering module 2046 , an extrapolating module 2048 , a limiting module 2050 , an evaluating module 2052 , an identifying module 2054 , a saving module 2056 , and a searching module 2058 .
- Various operations performed by the components of the facial image extraction module 204 are described in greater detail by way of example with reference to method 500 of FIG. 5 .
- FIG. 5 is a flow diagram showing a method 500 for extracting a facial image, in accordance with an example embodiment.
- the method 500 may be performed by processing logic that may comprise hardware, software, or a combination of both.
- the processing logic resides at the facial image extraction module 204 illustrated in FIG. 2 .
- the method 500 may be performed by the facial image extraction module 204 shown in FIG. 4 . These modules may comprise processing logic.
- the method 500 commences with receiving a sequence of buffered frames at operation 502 .
- the frames may be partitioned into groups of about 15 frames each by the partitioning module 2044 .
- the detecting module 2042 may analyze the frames to determine whether a facial image is present in each frame.
- the detecting module 2042 samples frames without analyzing each frame individually, by detecting a scene change between frames.
- the first and the last frames of a frame subset may be analyzed for facial images and the analysis of intermediate frames may be performed only in areas close to where the facial images are found in the first and the last frames, as described in more detail below.
- facial images in the first and the last frames may be detected by an existing face-detecting algorithm (e.g., AdaBoost).
- AdaBoost face-detecting algorithm
- the facial images detected in these non-contiguous frames may be extrapolated.
- the extrapolating module 2048 may extrapolate across multiple frames positioned between the detected images and approximate positions of the facial images in the intermediary frames. Such an extrapolation may provide probable positions of a facial image in regions that are more likely to contain the facial image so that only these regions are scanned in order to detect the facial image. The regions that are less likely to contain the facial image, based on the approximation, may be excluded from face scanning to increase performance.
- the limiting module 2050 may limit scanning for facial images to extrapolated frame regions.
- the discovering module 2046 may scan the frames containing detected facial images for the presence of textual content.
- the textual content may be helpful in identifying the person associated with the facial images. Accordingly, the facial images where textual content was detected may be queued to be processed by an optical character recognition (OCR) processor.
- OCR optical character recognition
- the detecting module 2042 may proceed to detect eyes in the frames in which a facial image was detected. Detection of eye positions may be performed in two stages. At the first stage, a quick pass may be performed by means of the AdaBoost algorithm (P. Viola and M. Jones, “Robust real-time object detection,” In Proc. of IEEE Workshop on Statistical and Computational Theories of Vision, pp. 1-25, 2001) using information learned from a large pool of eye images. Then, a facial image position may be defined more precisely by detection of eye pupil centers using direct detection of eye pupils.
- the AdaBoost method may be used without having to first normalize the images to be in a frontal orientation. The methods used for a more precise pass may be based on direct detection of eye pupils and may be limited to detection of open eyes in frontally oriented facial images.
- the evaluating module 2052 may evaluate the normalized facial image to determine whether eyes are well detected and whether sufficient distance between eyes exists. If the evaluating module 2052 determines that the eyes are well detected and that sufficient distance exists between the eyes, the facial images may be preserved. If, on the other hand, the evaluating module 2052 determines that the eyes are not well detected or that sufficient distance does not exist between the eyes, the facial images may be discarded.
- the facial images may be normalized to position eyes in a horizontal orientation.
- the images may be normalized by light intensity, and at operation 520 , the images may be normalized by size so that the eye centers in the facial image are located within a certain number of pixels from each other.
- every image may be enlarged or reduced so that all images are of the same size (e.g., 104 by 104 pixels), thus ensuring a certain number of pixels between the eyes.
- the procedure described herein is specific to a human face, a person skilled in the art will understand that similar normalization procedures may be utilized to normalize images of any other object categories such as, for example, cars, buildings, animals, and helicopters.
- the face detection techniques described herein may also be utilized to detect other categories of objects.
- the facial images are processed to provide clustering by similarity.
- the normalized facial images may be clustered in a cluster cache 210 ( FIG. 3 ).
- Each facial image is added to an existing cluster if the facial image is similar to the facial images already present in the cluster. This typically may result in facial images associated with a certain person being stored to one or a few clusters.
- To determine whether the facial image belongs to a previously created cluster the distance between the facial image and the already clustered facial images is measured. If the distance is below a predetermined threshold, the facial image is assumed to belong to the same cluster and, accordingly, may be added to the same cluster.
- the distance is below a predetermined threshold, there may be no additional value in saving almost identical facial images in the cluster cache and, correspondingly, the facial image may be dropped. If, on the other hand, the difference between the facial images in the previously created cluster and the newly normalized facial image is greater than a predetermined threshold, the newly normalized image may belong to a different person, and accordingly, a new cluster may be started. In some example embodiments, there may be more than one cluster created for the facial images of a single person. As already mentioned above, when clusters increase in size, a distance between the facial images of the clusters may decrease below a predetermined threshold. This may indicate that such clusters belong to the same person and, accordingly, such clusters may be merged into a single cluster using the merging module 2074 (described below with respect to FIG. 6 ).
- each cluster in the cluster cache 210 may be labeled by the suggestion engine 208 with a list of probable person identities based on the facial images contained in the clusters. Confidence levels corresponding to each probable person identity may be assigned to the clusters and their facial images resulting from identification of the normalized facial images of the cluster by comparing the clusters to the patterns DB 220 . The identification of the normalized facial images is based on calculation of distances from the facial image to every reference image in the patterns DB 220 .
- the clusters in the cluster cache 210 may be saved to cluster DB 216 along with labels, face sizes, and screenshots after the facial images in the clusters are identified.
- Cluster cache information may be used for automatic or manual decision making as to which person facial images of the cluster belong to. Once the decision is made, the cluster cache may be utilized to create an index, saving it to the index DB 218 at operation 524 .
- the index db 218 may provide searching capabilities to users searching the videos for facial images identified in the index database.
- the facial image clustering module 206 is shown to include several components that may be configured to perform various operations.
- the facial image clustering module 206 may comprise an associating module 2062 , a comparing module 2064 , an assigning module 2066 , a populating module 2068 , a client module 2070 , a receiving module 2072 , and a merging module 2074 .
- Various operations performed by the facial image clustering module 206 are described by way of example with reference to method 700 of FIG. 7 .
- FIG. 7 is a flow diagram showing a method 700 for clustering facial images, in accordance with one example embodiment.
- the method 700 may be performed by processing logic that may comprise hardware (e.g., dedicated logic, programmable logic, microcode, etc.), software (such as that which is run on a general purpose computer system or a dedicated machine), or a combination of both.
- the processing logic resides at the video processing system 200 illustrated in FIG. 2 .
- the method 700 may be performed by the various modules discussed above with reference to FIG. 6 . These modules may comprise processing logic.
- method 700 commences with receiving the next video frame from the video receiving module 202 .
- the clustering process may be performed in the facial image clustering module 206 .
- the suggestion process may be started by the suggestion engine 208 . Operations of both modules are described in more detail below.
- a video frame it may be followed by detecting a facial image at operation 702 . This method of detecting a facial image is described in more detail above with reference to method 500 of FIG. 5 .
- decision block 704 it may be determined whether or not a facial image is detected in the frame. If no facial image is detected at operation 702 , the frame may be dropped.
- the comparing module 2064 may compare the detected facial image to the facial images in existing clusters at operation 708 .
- the clusters may initially be stored in cluster cache 210 . Once the clusters are formed, they may be saved to the cluster DB 216 .
- Clusters may have other metadata associated with them besides images.
- the metadata may be text obtained from audio associated with the facial images in the cluster, or text obtained from visual content of the video frames from which the facial images were extracted.
- the metadata may also include other information obtained from the video and other accompanying digital media near the point where the facial images in the cluster were extracted.
- the comparing module 2064 compares the facial image to the facial images in the existing clusters in the cluster cache 210 and determines whether the distance between the facial image and the facial images in the existing clusters is less than a first predetermined threshold. If the distance is less than the first predetermined threshold (e.g., there is a small change), it may indicate that the facial images are very similar and that there is no benefit in saving both facial images to the cluster cache. Accordingly, the facial image may be dropped at operation 712 .
- the first predetermined threshold e.g., there is a small change
- a decision may be made at decision block 714 that the facial image is associated with the same person as facial images in an existing cluster, and also that there is value in adding the facial image to the existing cluster due to the difference between the facial image and the facial images in the existing cluster. Accordingly, the facial image may be added to an existing cluster at operation 716 .
- a new cluster may be created.
- the facial image may be added to more than one cluster. This may typically indicate that the two clusters belong to the same person and such clusters may then be merged into a single cluster by the merging module 2074 .
- the next detected facial image in the video frame is fetched. If no more facial images are available in the video frame, the next video frame may be received for processing.
- a rough comparison by the comparing module 2064 may be performed to compare the facial images in the cluster to the reference facial images in the patterns DB 220 .
- the reference facial images in the patterns DB 220 may be high definition images.
- the rough comparison may be performed in order to quickly identify a set of possible reference facial images and exclude unlikely reference facial images from the slower, fine-pass identification.
- the rough comparison is intended to pre-select the reference facial images in the database.
- a fine comparison to the reference facial images pre-selected in the initial rough comparison may be performed. This fine comparison may allow one or very few reference facial images from the pre-selected set to be identified as being associated with the same person as the facial image from the cluster.
- the method 700 proceeds to either the manual or the automatic branch.
- the automatic branch utilizes suggestions made by a suggestion module.
- the comparing module 2064 may determine whether an acceptable suggestion is made based on the distance from the cluster facial image to the reference facial image associated at operation 722 . If, at operation 736 , the decision is made that the suggestion made by the comparing module 2064 is acceptable, the method 700 may proceed to operation 730 and may label the cluster with metadata identifying the cluster as being associated with a certain person. In some example embodiments, there may be a list containing a predetermined number of suggestions generated for every facial image. In some example embodiments, there may be more than one suggestion method utilized based on different recognition technologies.
- each algorithm will provide the comparing module 2064 with a distance between the facial image in the cluster and the reference facial images in the patterns DB 220 .
- the precision with which the facial image in the cluster cache is identified may depend on the size of the patterns DB 220 .
- the more reference data that is stored to the patterns DB 220 the better are the results of the automatic recognition.
- an operator may be provided with the facial image for a manual identification.
- the cluster DB 216 may be empty and accordingly there will be no suggestions generated, or the confidence level of the available suggestions may be insufficient as in a case of the cluster DB 216 being only partially populated with reference data. Thus, an operator may have to identify the clusters manually.
- the operator may utilize the client module 2070 .
- the operator may be provided with the reference facial images that are the closest matches to the facial image.
- the operator may be provided with several reference facial images which are not within the predetermined threshold of the facial image but, nevertheless, are sufficiently close to be likely candidates for the manual comparison.
- the operator may be supplied with information extracted from the video stream, which may be helpful in identification of the facial image. For example, names extracted from textual content of frames using OCR, persons' names from subtitles, names extracted using speech-to-text, electronic program guide, or a transcript of the video file may be supplied to the operator to increase the likelihood of correct identification.
- the operator may visually identify the facial image and update patterns DB 220 with a new reference facial image if the operator decides that no matching reference facial image exists in the patterns DB 220 .
- patterns DB 220 is updated with a new reference facial image
- the operator may either manually update the cluster cache 210 with the identifying information or may instruct the facial image clustering module 206 to repeat the rough comparison step. If, on the other hand, the operator identifies the facial image based on the comparison to the reference facial images from the database, the operator may proceed to label the cluster manually at operation 730 . After the cluster is labeled with the identifying data, at operation 732 , the cluster (currently in the cluster cache 210 ) may be saved to cluster DB 216 by the populating module 2068 . Based on the cluster DB 216 , searchable information in the index DB 218 is created at operation 738 .
- the index information stored in the index DB 218 may contain metadata related to the object identity, its location in the video stream, time of its every appearance, and spatial location in the frames. Other relevant information useful for viewing application may be stored in the index DB 218 . If, after an automatic labeling, too many clusters remain unlabeled with metadata, then manual verification may be performed at module 736 . If, on the contrary, it is determined that no manual verification is to be performed, the video metadata extraction is completed at operation 740 .
- facial image clustering module environment 800 is shown to include several components that may be configured to perform various operations.
- the facial image clustering module environment 800 illustrates how the buffered frame sequence processor 212 , the facial image clustering module 206 , and the cluster DB 216 may interact.
- the buffered frame sequence processor 212 may comprise video frames, each video frame extracted and analyzed for presence of facial images as described above with reference to example method 500 in FIG. 5 .
- the facial image clustering module 206 is discussed above with reference to FIG. 6 .
- FIG. 9 shows a diagrammatic representation of a machine in the example form of a computer system 900 , within which a set of instructions for causing the machine to perform any one or more of the methodologies discussed herein may be executed.
- the machine operates as a stand-alone device or may be connected (e.g., networked) to other machines.
- the machine may operate in the capacity of a server or a client machine in a server-client network environment, or as a peer machine in a peer-to-peer (or distributed) network environment.
- the machine may be a personal computer (PC), a tablet PC, a set-top box (STB), a personal digital assistant (PDA), a cellular telephone, a portable music player (e.g., a portable hard drive audio device such as an MP3 player), a web appliance, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine.
- PC personal computer
- PDA personal digital assistant
- STB set-top box
- portable music player e.g., a portable hard drive audio device such as an MP3 player
- web appliance e.g., a web appliance, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine.
- machine shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein.
- the example computer system 900 includes a processor 902 (e.g., a central processing unit (CPU), a graphics processing unit (GPU) or both), a main memory 904 , and a static memory 906 , which communicate with each other via a bus 908 .
- the computer system 900 may further include a video display unit 910 (e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)).
- a processor 902 e.g., a central processing unit (CPU), a graphics processing unit (GPU) or both
- main memory 904 e.g., a main memory 904
- static memory 906 e.g., a static memory 906 , which communicate with each other via a bus 908 .
- the computer system 900 may further include a video display unit 910 (e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)).
- LCD liquid crystal display
- the computer system 900 also includes an alphanumeric input device 912 (e.g., a keyboard), a cursor control device 914 (e.g., a mouse), a drive unit 916 , a signal generation device 918 (e.g., a speaker), and a network interface device 920 .
- an alphanumeric input device 912 e.g., a keyboard
- a cursor control device 914 e.g., a mouse
- drive unit 916 e.g., a drive unit 916
- a signal generation device 918 e.g., a speaker
- a network interface device 920 e.g., a network interface
- the drive unit 916 includes a machine-readable medium 922 on which is stored one or more sets of instructions and data structures (e.g., instructions 924 ) embodying or utilized by any one or more of the methodologies or functions described herein.
- the instructions 924 may also reside, completely or at least partially, within the main memory 904 and/or within the processor 902 during execution thereof by the computer system 900 .
- the main memory 904 and the processor 902 also constitute machine-readable media.
- the instructions 924 may further be transmitted or received over a network 926 via the network interface device 920 utilizing any one of a number of well-known transfer protocols (e.g., Hyper Text Transfer Protocol (HTTP)).
- HTTP Hyper Text Transfer Protocol
- machine-readable medium 922 is shown in an example embodiment to be a single medium, the term “machine-readable medium” should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions.
- the term “machine-readable medium” shall also be taken to include any medium that is capable of storing, encoding, or carrying a set of instructions for execution by the machine and that causes the machine to perform any one or more of the methodologies of the present application, or that is capable of storing, encoding, or carrying data structures utilized by or associated with such a set of instructions.
- machine-readable medium shall accordingly be taken to include, but not be limited to, solid-state memories, optical and magnetic media, and carrier wave signals. Such media may also include, without limitation, hard disks, floppy disks, flash memory cards, digital video disks, random access memory (RAMs), read only memory (ROMs), and the like.
- the example embodiments described herein may be implemented in an operating environment comprising software installed on a computer, in hardware, or in a combination of software and hardware.
Abstract
Description
- This application claims the benefit of priority under 35 U.S.C. 119(e) to U.S. Provisional Patent Application Ser. No. 60/986,236, filed on Nov. 7, 2007, which is incorporated herein by reference.
- This application relates to a system and method for processing digital media.
- Object detection and recognition in video content have proven to be difficult tasks in artificial intelligence.
- Embodiments are illustrated by way of example and not limitation in the figures of the accompanying drawings, in which like references indicate similar elements and in which:
-
FIG. 1 is a block diagram showing architecture within which a method and system of object recognition and database population for video indexing are implemented, in accordance with an example embodiment; -
FIG. 2 is a block diagram of a video processing system, in accordance with an example embodiment; -
FIG. 3 is a block diagram showing interrelations between various components of the video processing system ofFIG. 2 , in accordance with an example embodiment; -
FIG. 4 is a block diagram of a facial image extraction module, in accordance with an example embodiment; -
FIG. 5 is a flow chart of a method for video processing, in accordance with an example embodiment; -
FIG. 6 is a block diagram of a facial image clustering module, in accordance with an example embodiment; -
FIG. 7 is a flow chart of a method for facial image clustering, in accordance with an example embodiment; -
FIG. 8 is a block diagram of an environment within which a facial image clustering module is implemented, in accordance with an example embodiment; and -
FIG. 9 is a diagrammatic representation of an example machine, in the form of a computer system, within which a set of instructions for causing the machine to perform any one or more of the methodologies discussed herein is executed. - The example embodiments described herein may be implemented in an operating environment comprising software installed on a computer, in hardware, or in a combination of software and hardware.
- Disclosed herein is an efficient technique to detect objects in video clips and to identify the detected objects throughout the clips with minimal computing cost. The technique may be utilized to detect any category of objects (e.g., facial images), but the term “facial image” will be used throughout this description to provide a clearer explanation of how the technique may work. The detection of the facial images may use various algorithms described below. The detected facial images may be normalized according to various criteria, which facilitate organization of the facial mages into clusters. Each cluster may contain facial images of one person, however, there may be more than one cluster created per one person because the confidence level of the system may not be high enough, at this point, to determine whether or not the facial images belong to the same person as the facial images in an existing cluster.
- Once the facial images are organized into clusters, they may be compared to reference facial images. An increased efficiency is achieved by utilizing certain representative facial images from each cluster of facial images to compare to the reference facial images. The reference facial images may include facial images of people known to the system. If the system determines that the facial images in the cluster cannot be identified because there are no similar reference facial images, a manual identification may be performed.
- Once the images are identified by comparison to the reference facial images, the cluster data pertaining to the identified images may be stored to a database and utilized to search the video clips from which the facial images are extracted. The stored data may include, among other things, the name of the person associated with the facial images, the times of appearances of the person in the video, and the location of the facial images in the video frames of the video clip. The data stored to the database may be utilized to search the video clips for people by keywords (e.g., Madonna). Data usage provides users with a better video viewing experience. For example, such data usage allows users to determine times in the video where the facial image associated with the keyword appears, and also to navigate through the video by the facial image appearances.
-
FIG. 1 shows anexample environment 100, within which a method and system of facial image recognition and database population for video indexing may be implemented. As shown inFIG. 1 , theexample environment 100 may comprise a user system 110, avideo processing facility 120, anetwork 130, a thirdparty content provider 140, and asatellite 150. - The user system 110 may further comprise a
video viewing application 112 and asatellite dish 114. The user system 110 may be a general purpose computer, a television set (TV), a personal digital assistant (PDA), a mobile telephone, a wireless device, and any other device capable of visual presentation of images (including text) acquired, stored, or transmitted in various forms. Thevideo viewing application 112 may be any application software that facilitates display of a video via the user system 110. Thevideo viewing application 112 may run at or be distributed across the user system 110, thirdparty content provider 140, and thevideo processing facility 120. - The
satellite dish 114, in one example embodiment, is a type of antenna designed for the specific purpose of transmitting signals to and/or receiving signals from satellites. Thesatellite dish 114 may be of varying sizes and designs, and may be used to receive and transmit any type of digital data to and from a satellite. Thesatellite dish 114 may be located at thevideo processing facility 120. It should be noted that thesatellite dish 114 is just one of many means to provide network connectivity, and other types of network connectivity may be used. - The
video processing facility 120 may comprise asatellite dish 154 and avideo processing system 200. Thesatellite dish 154 may be similar to thesatellite dish 114 described above. Thevideo processing facility 120 may represent fixed, mobile, or transportable structures, including installed electrical and electronic wiring, cabling, and equipment and supporting structures, such as utilities, ground networks, wireless networks, and electrical supporting structures. Thevideo processing system 200 is described by a way of example with reference toFIG. 2 . - The
video processing system 200 may be a general-purpose computer processor or a type of processor designed specifically for the receiving, creation and distribution of digital media. Thevideo processing system 200 may include various modules such as a facialimage extraction module 204 that provides extraction of facial images, a facialimage clustering module 206 that clusters the facial images, and asuggestion engine 208 that automatically identifies the facial images by comparing the facial images to reference facial images stored in a reference database. Further modules may include amanual labeling interface 214 for manual identification of the facial images and the index database (DB) 218 to store searchable indexes. An example embodiment of the facialimage extraction module 204, including various modules is described by way of example with reference toFIG. 4 below. A method that may be used to process video utilizing the facialimage extraction module 204 is described by way of example with reference toFIG. 5 below. - The facial
image clustering module 206, utilized to cluster facial images extracted from the video, may reside at thevideo processing system 200. In some example embodiments, more than one cluster may be created per person. An example embodiment of the facialimage clustering module 206 including various modules is described by way of example with reference toFIG. 5 below. A method that may be utilized to process video at the facialimage clustering module 206 is described by a way of example with reference toFIG. 8 below. - The third
party content provider 140 may comprise a digitalmedia content generator 142 and asatellite dish 184. The thirdparty content provider 140 may be an entity that owns or has the rights to digital media content such as digital videos. As an example, the thirdparty content provider 140 may be a news service that provides reports to digital media broadcasters. The digitalmedia content generator 142 may be a software application generating video content and transmitting the video content via thesatellite dish 184 or thenetwork 130, to be received at thevideo processing facility 120. Thesatellite dish 184 may be similar to thesatellite dish 114 described above. Thenetwork 130 may be a network of data processing nodes that are interconnected for the purpose of data communication. - As shown in
FIG. 2 , thevideo processing system 200 comprises avideo receiving module 202, a facialimage extraction module 204, a facialimage clustering module 206, asuggestion engine 208, acluster cache 210, a bufferedframe sequence processor 212, amanual labeling interface 214, and a number of databases. The databases comprise a cluster database (DB) 216, anindex DB 218, and apatterns DB 220. - The video receiving
module 202, in an example embodiment, may be configured to receive video frames from the bufferedframe sequence processor 212. In some example embodiments, there may be a specific number of frames received each time, for example, 15 frames. In some example embodiments, the video may be received in time intervals, for example, a one-minute interval. - The facial
image extraction module 204 may be configured to extract facial images from the video frames, which are received by thevideo receiving module 202 from the bufferedframe sequence processor 212. Some frames may contain more than one facial image or no facial images at all. The facialimage extraction module 204 may be configured to extract all facial images appearing in a single frame. If a frame does not contain any facial images, the frame may be dropped. The facialimage extraction module 204, in some example embodiments, may normalize the extracted facial images, as shown inFIG. 5 . - The facial
image clustering module 206, in an example embodiment, may be configured to save the normalized facial images once they are extracted by the facialimage extraction module 204. A method for clustering extracted images is described below by way of example with reference tomethod 700 ofFIG. 7 . - The
suggestion engine 208, in an example embodiment, may be configured to label the normalized facial images with suggested identities of the person associated with the facial images in the cluster. In order to label the clusters, thesuggestion engine 208 may compare the normalized facial images to reference facial images, and based on the comparison, may suggest the identity of the person associated with the facial image. Thecluster cache 210, in an example embodiment, may be configured to store the clusters created by the facialimage clustering module 206 until the clusters are labeled by thesuggestion engine 208. Once the clusters are labeled in thecluster cache 210, they may be saved to the cluster DB 216. - The buffered
frame sequence processor 212, in an example embodiment, may be configured to process video feeds received from the thirdparty content provider 140. As an example, a video feed may be partitioned into video clips of certain time durations or into video clips having a certain number of frames. The processed video frames may be received by the facialimage extraction module 204. The facialimage extraction module 204, in an example embodiment, may be configured to process frames received from the bufferedframe sequence processor 212 in order to detect facial images contained in the video frames. The facialimage extraction module 204 may extract textual content of the video frames and save the textual content for further processing. Subsequently, the saved textual content may be processed to extract text that suggests the identity of the person appearing in the video. - The
manual labeling interface 214, in an example embodiment, may be a graphical user interface configured to provide an operator with a facial image from thecluster cache 210, along with a set of reference facial images likely to be associated with the same person. The operator may visually compare and select, from the set of reference facial images, a facial image viewed as being associated with the same person as the facial image from thecluster cache 210. - The cluster DB 216, in an example embodiment, may be a database configured to store clusters of facial images and associated metadata extracted from the video feed. The facial images in the clusters stored in cluster DB 216 may be identified facial images. The metadata associated with the facial images in the clusters may be updated when previously unknown facial images in the cluster are identified. The cluster metadata may also be updated manually by comparing the cluster images to known reference facial images using the
manual labeling interface 214. Theindex DB 218, in an example embodiment, may be a database populated with the indexed records of the identified facial images, each facial image's position in the video frame(s) in which it appears, and the number of times the facial image appears in the video. The relationship between various components of thevideo processing system 200 is described by way of example with reference toFIG. 3 . - Referring to
FIG. 4 of the drawings, the facialimage extraction module 204 previously discussed in reference toFIGS. 2 and 3 is shown to include several components that may be configured to perform various operations. The facialimage extraction module 204 may comprise a detectingmodule 2042, apartitioning module 2044, a discoveringmodule 2046, anextrapolating module 2048, a limitingmodule 2050, an evaluatingmodule 2052, an identifyingmodule 2054, asaving module 2056, and asearching module 2058. Various operations performed by the components of the facialimage extraction module 204 are described in greater detail by way of example with reference tomethod 500 ofFIG. 5 . -
FIG. 5 is a flow diagram showing amethod 500 for extracting a facial image, in accordance with an example embodiment. Themethod 500 may be performed by processing logic that may comprise hardware, software, or a combination of both. In one example embodiment, the processing logic resides at the facialimage extraction module 204 illustrated inFIG. 2 . Themethod 500 may be performed by the facialimage extraction module 204 shown inFIG. 4 . These modules may comprise processing logic. - Referring to both
FIGS. 4 and 5 , themethod 500 commences with receiving a sequence of buffered frames atoperation 502. In some example embodiments, as the frames are received atoperation 502, they may be partitioned into groups of about 15 frames each by thepartitioning module 2044. The detectingmodule 2042 may analyze the frames to determine whether a facial image is present in each frame. In some example embodiments, the detectingmodule 2042 samples frames without analyzing each frame individually, by detecting a scene change between frames. In some example embodiments, the first and the last frames of a frame subset may be analyzed for facial images and the analysis of intermediate frames may be performed only in areas close to where the facial images are found in the first and the last frames, as described in more detail below. - At
operation 504, facial images in the first and the last frames may be detected by an existing face-detecting algorithm (e.g., AdaBoost). In some example embodiments, the facial images detected in these non-contiguous frames may be extrapolated. Thus, atoperation 506, theextrapolating module 2048 may extrapolate across multiple frames positioned between the detected images and approximate positions of the facial images in the intermediary frames. Such an extrapolation may provide probable positions of a facial image in regions that are more likely to contain the facial image so that only these regions are scanned in order to detect the facial image. The regions that are less likely to contain the facial image, based on the approximation, may be excluded from face scanning to increase performance. Atoperation 508, the limitingmodule 2050 may limit scanning for facial images to extrapolated frame regions. - At
operation 510, the discoveringmodule 2046 may scan the frames containing detected facial images for the presence of textual content. The textual content may be helpful in identifying the person associated with the facial images. Accordingly, the facial images where textual content was detected may be queued to be processed by an optical character recognition (OCR) processor. - At
operation 512, the detectingmodule 2042 may proceed to detect eyes in the frames in which a facial image was detected. Detection of eye positions may be performed in two stages. At the first stage, a quick pass may be performed by means of the AdaBoost algorithm (P. Viola and M. Jones, “Robust real-time object detection,” In Proc. of IEEE Workshop on Statistical and Computational Theories of Vision, pp. 1-25, 2001) using information learned from a large pool of eye images. Then, a facial image position may be defined more precisely by detection of eye pupil centers using direct detection of eye pupils. The AdaBoost method may be used without having to first normalize the images to be in a frontal orientation. The methods used for a more precise pass may be based on direct detection of eye pupils and may be limited to detection of open eyes in frontally oriented facial images. - A determination may be made to preserve the frames if the distance between the eyes is greater than a predetermined threshold distance. For example, faces with the distance between eyes of less than 40 pixels may be suppressed and not used when identifying the facial image. At
operation 514, the evaluatingmodule 2052 may evaluate the normalized facial image to determine whether eyes are well detected and whether sufficient distance between eyes exists. If the evaluatingmodule 2052 determines that the eyes are well detected and that sufficient distance exists between the eyes, the facial images may be preserved. If, on the other hand, the evaluatingmodule 2052 determines that the eyes are not well detected or that sufficient distance does not exist between the eyes, the facial images may be discarded. - At
operation 516, the facial images may be normalized to position eyes in a horizontal orientation. Atoperation 518, the images may be normalized by light intensity, and atoperation 520, the images may be normalized by size so that the eye centers in the facial image are located within a certain number of pixels from each other. During the normalization, every image may be enlarged or reduced so that all images are of the same size (e.g., 104 by 104 pixels), thus ensuring a certain number of pixels between the eyes. It should be noted that even though the procedure described herein is specific to a human face, a person skilled in the art will understand that similar normalization procedures may be utilized to normalize images of any other object categories such as, for example, cars, buildings, animals, and helicopters. Furthermore, it should be noted that the face detection techniques described herein may also be utilized to detect other categories of objects. - At
operation 522, the facial images are processed to provide clustering by similarity. The normalized facial images may be clustered in a cluster cache 210 (FIG. 3 ). Each facial image is added to an existing cluster if the facial image is similar to the facial images already present in the cluster. This typically may result in facial images associated with a certain person being stored to one or a few clusters. To determine whether the facial image belongs to a previously created cluster, the distance between the facial image and the already clustered facial images is measured. If the distance is below a predetermined threshold, the facial image is assumed to belong to the same cluster and, accordingly, may be added to the same cluster. - In some example embodiments, if the distance is below a predetermined threshold, there may be no additional value in saving almost identical facial images in the cluster cache and, correspondingly, the facial image may be dropped. If, on the other hand, the difference between the facial images in the previously created cluster and the newly normalized facial image is greater than a predetermined threshold, the newly normalized image may belong to a different person, and accordingly, a new cluster may be started. In some example embodiments, there may be more than one cluster created for the facial images of a single person. As already mentioned above, when clusters increase in size, a distance between the facial images of the clusters may decrease below a predetermined threshold. This may indicate that such clusters belong to the same person and, accordingly, such clusters may be merged into a single cluster using the merging module 2074 (described below with respect to
FIG. 6 ). - Referring now to
FIGS. 2 , 3, and 5, each cluster in thecluster cache 210 may be labeled by thesuggestion engine 208 with a list of probable person identities based on the facial images contained in the clusters. Confidence levels corresponding to each probable person identity may be assigned to the clusters and their facial images resulting from identification of the normalized facial images of the cluster by comparing the clusters to thepatterns DB 220. The identification of the normalized facial images is based on calculation of distances from the facial image to every reference image in thepatterns DB 220. The clusters in thecluster cache 210 may be saved to cluster DB 216 along with labels, face sizes, and screenshots after the facial images in the clusters are identified. Cluster cache information may be used for automatic or manual decision making as to which person facial images of the cluster belong to. Once the decision is made, the cluster cache may be utilized to create an index, saving it to theindex DB 218 atoperation 524. Theindex db 218 may provide searching capabilities to users searching the videos for facial images identified in the index database. - Referring to
FIG. 6 of the drawings, the facialimage clustering module 206 is shown to include several components that may be configured to perform various operations. The facialimage clustering module 206 may comprise an associatingmodule 2062, a comparingmodule 2064, an assigningmodule 2066, apopulating module 2068, aclient module 2070, areceiving module 2072, and amerging module 2074. Various operations performed by the facialimage clustering module 206 are described by way of example with reference tomethod 700 ofFIG. 7 . -
FIG. 7 is a flow diagram showing amethod 700 for clustering facial images, in accordance with one example embodiment. Themethod 700 may be performed by processing logic that may comprise hardware (e.g., dedicated logic, programmable logic, microcode, etc.), software (such as that which is run on a general purpose computer system or a dedicated machine), or a combination of both. In one example embodiment, the processing logic resides at thevideo processing system 200 illustrated inFIG. 2 . Themethod 700 may be performed by the various modules discussed above with reference toFIG. 6 . These modules may comprise processing logic. - Referring to both
FIGS. 6 and 7 ,method 700 commences with receiving the next video frame from thevideo receiving module 202. Until all frames are received, the clustering process may be performed in the facialimage clustering module 206. When all frames are received and the clusters are formed, the suggestion process may be started by thesuggestion engine 208. Operations of both modules are described in more detail below. Thus, when a video frame is received, it may be followed by detecting a facial image atoperation 702. This method of detecting a facial image is described in more detail above with reference tomethod 500 ofFIG. 5 . Atdecision block 704, it may be determined whether or not a facial image is detected in the frame. If no facial image is detected atoperation 702, the frame may be dropped. If, on the contrary, a facial image is detected, the comparingmodule 2064 may compare the detected facial image to the facial images in existing clusters atoperation 708. In some example embodiments, the clusters may initially be stored incluster cache 210. Once the clusters are formed, they may be saved to the cluster DB 216. Clusters may have other metadata associated with them besides images. For example, the metadata may be text obtained from audio associated with the facial images in the cluster, or text obtained from visual content of the video frames from which the facial images were extracted. The metadata may also include other information obtained from the video and other accompanying digital media near the point where the facial images in the cluster were extracted. - At
decision block 710, the comparingmodule 2064 compares the facial image to the facial images in the existing clusters in thecluster cache 210 and determines whether the distance between the facial image and the facial images in the existing clusters is less than a first predetermined threshold. If the distance is less than the first predetermined threshold (e.g., there is a small change), it may indicate that the facial images are very similar and that there is no benefit in saving both facial images to the cluster cache. Accordingly, the facial image may be dropped atoperation 712. If the distance between the facial image and the facial images in the existing clusters is more than the first predetermined threshold but less than a second, larger predetermined threshold, a decision may be made atdecision block 714 that the facial image is associated with the same person as facial images in an existing cluster, and also that there is value in adding the facial image to the existing cluster due to the difference between the facial image and the facial images in the existing cluster. Accordingly, the facial image may be added to an existing cluster at operation 716. - If the distance between the facial image and the facial images in the existing cluster is above the second, larger predetermined threshold (i.e., there is a large change), the distance may indicate that the facial images are not associated with the same person. Accordingly, at operation 718 a new cluster may be created. During the addition of a facial image to an existing cluster, it may be determined that the facial image may be added to more than one cluster. This may typically indicate that the two clusters belong to the same person and such clusters may then be merged into a single cluster by the
merging module 2074. After a facial image is added to a cluster, the next detected facial image in the video frame is fetched. If no more facial images are available in the video frame, the next video frame may be received for processing. - If no more frames are available, at
operation 704, the suggestion process starts. Thus, at operation 720 a rough comparison by the comparingmodule 2064 may be performed to compare the facial images in the cluster to the reference facial images in thepatterns DB 220. In some example embodiments, the reference facial images in thepatterns DB 220 may be high definition images. The rough comparison may be performed in order to quickly identify a set of possible reference facial images and exclude unlikely reference facial images from the slower, fine-pass identification. Thus, the rough comparison is intended to pre-select the reference facial images in the database. Atoperation 722, a fine comparison to the reference facial images pre-selected in the initial rough comparison may be performed. This fine comparison may allow one or very few reference facial images from the pre-selected set to be identified as being associated with the same person as the facial image from the cluster. - At
block 724, depending on a mode of the identification, themethod 700 flow proceeds to either the manual or the automatic branch. Atoperation 736, the automatic branch utilizes suggestions made by a suggestion module. The comparingmodule 2064 may determine whether an acceptable suggestion is made based on the distance from the cluster facial image to the reference facial image associated atoperation 722. If, atoperation 736, the decision is made that the suggestion made by the comparingmodule 2064 is acceptable, themethod 700 may proceed to operation 730 and may label the cluster with metadata identifying the cluster as being associated with a certain person. In some example embodiments, there may be a list containing a predetermined number of suggestions generated for every facial image. In some example embodiments, there may be more than one suggestion method utilized based on different recognition technologies. For example, there may be several different algorithms performing recognition, and each algorithm will provide the comparingmodule 2064 with a distance between the facial image in the cluster and the reference facial images in thepatterns DB 220. The precision with which the facial image in the cluster cache is identified may depend on the size of thepatterns DB 220. The more reference data that is stored to thepatterns DB 220, the better are the results of the automatic recognition. - If, on the contrary, at
operation 724, the execution of themethod 700 precedes to the manual branch, at operation 726 an operator may be provided with the facial image for a manual identification. For example, the cluster DB 216 may be empty and accordingly there will be no suggestions generated, or the confidence level of the available suggestions may be insufficient as in a case of the cluster DB 216 being only partially populated with reference data. Thus, an operator may have to identify the clusters manually. - To perform the manual identification, the operator may utilize the
client module 2070. The operator may be provided with the reference facial images that are the closest matches to the facial image. For example, the operator may be provided with several reference facial images which are not within the predetermined threshold of the facial image but, nevertheless, are sufficiently close to be likely candidates for the manual comparison. In some example embodiments, the operator may be supplied with information extracted from the video stream, which may be helpful in identification of the facial image. For example, names extracted from textual content of frames using OCR, persons' names from subtitles, names extracted using speech-to-text, electronic program guide, or a transcript of the video file may be supplied to the operator to increase the likelihood of correct identification. Thus, atoperation 728, the operator may visually identify the facial image andupdate patterns DB 220 with a new reference facial image if the operator decides that no matching reference facial image exists in thepatterns DB 220. - Once
patterns DB 220 is updated with a new reference facial image, the operator may either manually update thecluster cache 210 with the identifying information or may instruct the facialimage clustering module 206 to repeat the rough comparison step. If, on the other hand, the operator identifies the facial image based on the comparison to the reference facial images from the database, the operator may proceed to label the cluster manually at operation 730. After the cluster is labeled with the identifying data, at operation 732, the cluster (currently in the cluster cache 210) may be saved to cluster DB 216 by thepopulating module 2068. Based on the cluster DB 216, searchable information in theindex DB 218 is created atoperation 738. The index information stored in theindex DB 218 may contain metadata related to the object identity, its location in the video stream, time of its every appearance, and spatial location in the frames. Other relevant information useful for viewing application may be stored in theindex DB 218. If, after an automatic labeling, too many clusters remain unlabeled with metadata, then manual verification may be performed atmodule 736. If, on the contrary, it is determined that no manual verification is to be performed, the video metadata extraction is completed atoperation 740. - Referring to
FIG. 8 of the drawings, facial imageclustering module environment 800 is shown to include several components that may be configured to perform various operations. The facial imageclustering module environment 800 illustrates how the bufferedframe sequence processor 212, the facialimage clustering module 206, and the cluster DB 216 may interact. The bufferedframe sequence processor 212 may comprise video frames, each video frame extracted and analyzed for presence of facial images as described above with reference toexample method 500 inFIG. 5 . The facialimage clustering module 206 is discussed above with reference toFIG. 6 . -
FIG. 9 shows a diagrammatic representation of a machine in the example form of acomputer system 900, within which a set of instructions for causing the machine to perform any one or more of the methodologies discussed herein may be executed. In various example embodiments, the machine operates as a stand-alone device or may be connected (e.g., networked) to other machines. In a networked deployment, the machine may operate in the capacity of a server or a client machine in a server-client network environment, or as a peer machine in a peer-to-peer (or distributed) network environment. The machine may be a personal computer (PC), a tablet PC, a set-top box (STB), a personal digital assistant (PDA), a cellular telephone, a portable music player (e.g., a portable hard drive audio device such as an MP3 player), a web appliance, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine. Further, while only a single machine is illustrated, the term “machine” shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein. - The
example computer system 900 includes a processor 902 (e.g., a central processing unit (CPU), a graphics processing unit (GPU) or both), amain memory 904, and astatic memory 906, which communicate with each other via abus 908. Thecomputer system 900 may further include a video display unit 910 (e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)). Thecomputer system 900 also includes an alphanumeric input device 912 (e.g., a keyboard), a cursor control device 914 (e.g., a mouse), adrive unit 916, a signal generation device 918 (e.g., a speaker), and anetwork interface device 920. - The
drive unit 916 includes a machine-readable medium 922 on which is stored one or more sets of instructions and data structures (e.g., instructions 924) embodying or utilized by any one or more of the methodologies or functions described herein. Theinstructions 924 may also reside, completely or at least partially, within themain memory 904 and/or within theprocessor 902 during execution thereof by thecomputer system 900. Themain memory 904 and theprocessor 902 also constitute machine-readable media. - The
instructions 924 may further be transmitted or received over a network 926 via thenetwork interface device 920 utilizing any one of a number of well-known transfer protocols (e.g., Hyper Text Transfer Protocol (HTTP)). - While the machine-
readable medium 922 is shown in an example embodiment to be a single medium, the term “machine-readable medium” should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions. The term “machine-readable medium” shall also be taken to include any medium that is capable of storing, encoding, or carrying a set of instructions for execution by the machine and that causes the machine to perform any one or more of the methodologies of the present application, or that is capable of storing, encoding, or carrying data structures utilized by or associated with such a set of instructions. The term “machine-readable medium” shall accordingly be taken to include, but not be limited to, solid-state memories, optical and magnetic media, and carrier wave signals. Such media may also include, without limitation, hard disks, floppy disks, flash memory cards, digital video disks, random access memory (RAMs), read only memory (ROMs), and the like. - The example embodiments described herein may be implemented in an operating environment comprising software installed on a computer, in hardware, or in a combination of software and hardware.
- Thus, a method and system of object recognition and database population for video indexing have been described. Although embodiments have been described with reference to specific example embodiments, it will be evident that various modifications and changes may be made to these example embodiments without departing from the broader spirit and scope of the present application. Accordingly, the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.
Claims (25)
Priority Applications (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US11/949,258 US8315430B2 (en) | 2007-11-07 | 2007-12-03 | Object recognition and database population for video indexing |
JP2010532077A JP2011504673A (en) | 2007-11-07 | 2008-11-06 | Object recognition and database porting |
PCT/US2008/012502 WO2009061420A1 (en) | 2007-11-07 | 2008-11-06 | Object recognition and database population |
US13/654,541 US8457368B2 (en) | 2007-11-07 | 2012-10-18 | System and method of object recognition and database population for video indexing |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US98623607P | 2007-11-07 | 2007-11-07 | |
US11/949,258 US8315430B2 (en) | 2007-11-07 | 2007-12-03 | Object recognition and database population for video indexing |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/654,541 Continuation US8457368B2 (en) | 2007-11-07 | 2012-10-18 | System and method of object recognition and database population for video indexing |
Publications (2)
Publication Number | Publication Date |
---|---|
US20090141988A1 true US20090141988A1 (en) | 2009-06-04 |
US8315430B2 US8315430B2 (en) | 2012-11-20 |
Family
ID=40626061
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US11/949,258 Active 2030-10-14 US8315430B2 (en) | 2007-11-07 | 2007-12-03 | Object recognition and database population for video indexing |
US13/654,541 Active US8457368B2 (en) | 2007-11-07 | 2012-10-18 | System and method of object recognition and database population for video indexing |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/654,541 Active US8457368B2 (en) | 2007-11-07 | 2012-10-18 | System and method of object recognition and database population for video indexing |
Country Status (3)
Country | Link |
---|---|
US (2) | US8315430B2 (en) |
JP (1) | JP2011504673A (en) |
WO (1) | WO2009061420A1 (en) |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080187231A1 (en) * | 2005-03-10 | 2008-08-07 | Koninklijke Philips Electronics, N.V. | Summarization of Audio and/or Visual Data |
US20100082585A1 (en) * | 2008-09-23 | 2010-04-01 | Disney Enterprises, Inc. | System and method for visual search in a video media player |
US20100266166A1 (en) * | 2009-04-15 | 2010-10-21 | Kabushiki Kaisha Toshiba | Image processing apparatus, image processing method, and storage medium |
US20110216179A1 (en) * | 2010-02-24 | 2011-09-08 | Orang Dialameh | Augmented Reality Panorama Supporting Visually Impaired Individuals |
US20140089799A1 (en) * | 2011-01-03 | 2014-03-27 | Curt Evans | Methods and system for remote control for multimedia seeking |
US20150067023A1 (en) * | 2013-08-27 | 2015-03-05 | Cisco Technology, Inc. | System and associated methodology for enhancing communication sessions between multiple users |
US20160125232A1 (en) * | 2014-11-04 | 2016-05-05 | Hewlett-Packard Development Company, L.P. | Dynamic face identification |
US20170092296A1 (en) * | 2015-09-24 | 2017-03-30 | Canon Kabushiki Kaisha | Sound processing apparatus, sound processing method, and storage medium |
Families Citing this family (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2010165052A (en) * | 2009-01-13 | 2010-07-29 | Canon Inc | Image processor and image processing method |
US8503739B2 (en) * | 2009-09-18 | 2013-08-06 | Adobe Systems Incorporated | System and method for using contextual features to improve face recognition in digital images |
GB2489675A (en) * | 2011-03-29 | 2012-10-10 | Sony Corp | Generating and viewing video highlights with field of view (FOV) information |
US9225936B2 (en) | 2012-05-16 | 2015-12-29 | International Business Machines Corporation | Automated collaborative annotation of converged web conference objects |
WO2015008432A1 (en) * | 2013-07-17 | 2015-01-22 | 日本電気株式会社 | Object tracking device, object tracking method, and object tracking program |
US9842111B2 (en) * | 2013-12-22 | 2017-12-12 | Varonis Systems, Ltd. | On-demand indexing |
CN104991906B (en) * | 2015-06-17 | 2020-06-02 | 百度在线网络技术（北京）有限公司 | Information acquisition method, server, terminal, database construction method and device |
AU2015234329A1 (en) | 2015-09-30 | 2017-04-13 | Canon Kabushiki Kaisha | Method, system and apparatus for processing an image |
WO2017089968A1 (en) * | 2015-11-24 | 2017-06-01 | Hazan Nir | Method and system for creating a comprehensive personal video clip |
CN109635775B (en) * | 2018-12-21 | 2023-05-16 | 上海创功通讯技术有限公司 | Method, device and storage medium for displaying face expansion attribute |
CN109743579A (en) * | 2018-12-24 | 2019-05-10 | 秒针信息技术有限公司 | A kind of method for processing video frequency and device, storage medium and processor |
CN109743580A (en) * | 2018-12-24 | 2019-05-10 | 秒针信息技术有限公司 | A kind of method for processing video frequency and device, storage medium and processor |
US10909700B2 (en) * | 2019-04-02 | 2021-02-02 | Samsung Electronics Co., Ltd. | Display apparatus and image processing method thereof |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010035907A1 (en) * | 2000-03-10 | 2001-11-01 | Broemmelsiek Raymond M. | Method and apparatus for object tracking and detection |
US6594751B1 (en) * | 1999-03-25 | 2003-07-15 | Progress Software Corporation | Method and apparatus for pointer relocation optimization for virtual memory mapping and transaction management in a database system |
US20030161500A1 (en) * | 2002-02-22 | 2003-08-28 | Andrew Blake | System and method for probabilistic exemplar-based pattern tracking |
US20040017933A1 (en) * | 2002-04-12 | 2004-01-29 | Canon Kabushiki Kaisha | Face detection and tracking in a video sequence |
US6754389B1 (en) * | 1999-12-01 | 2004-06-22 | Koninklijke Philips Electronics N.V. | Program classification using object tracking |
US20040223631A1 (en) * | 2003-05-07 | 2004-11-11 | Roman Waupotitsch | Face recognition based on obtaining two dimensional information from three-dimensional face shapes |
US20060165258A1 (en) * | 2005-01-24 | 2006-07-27 | Shmuel Avidan | Tracking objects in videos with adaptive classifiers |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7006881B1 (en) * | 1991-12-23 | 2006-02-28 | Steven Hoffberg | Media recording device with remote graphic user interface |
US6771303B2 (en) * | 2002-04-23 | 2004-08-03 | Microsoft Corporation | Video-teleconferencing system with eye-gaze correction |
US7515173B2 (en) * | 2002-05-23 | 2009-04-07 | Microsoft Corporation | Head pose tracking system |
WO2008068456A2 (en) * | 2006-12-06 | 2008-06-12 | Sony United Kingdom Limited | A method and an apparatus for generating image content |
US20080298643A1 (en) * | 2007-05-30 | 2008-12-04 | Lawther Joel S | Composite person model from image collection |
-
2007
- 2007-12-03 US US11/949,258 patent/US8315430B2/en active Active
-
2008
- 2008-11-06 WO PCT/US2008/012502 patent/WO2009061420A1/en active Application Filing
- 2008-11-06 JP JP2010532077A patent/JP2011504673A/en active Pending
-
2012
- 2012-10-18 US US13/654,541 patent/US8457368B2/en active Active
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6594751B1 (en) * | 1999-03-25 | 2003-07-15 | Progress Software Corporation | Method and apparatus for pointer relocation optimization for virtual memory mapping and transaction management in a database system |
US6754389B1 (en) * | 1999-12-01 | 2004-06-22 | Koninklijke Philips Electronics N.V. | Program classification using object tracking |
US20010035907A1 (en) * | 2000-03-10 | 2001-11-01 | Broemmelsiek Raymond M. | Method and apparatus for object tracking and detection |
US20030161500A1 (en) * | 2002-02-22 | 2003-08-28 | Andrew Blake | System and method for probabilistic exemplar-based pattern tracking |
US20040017933A1 (en) * | 2002-04-12 | 2004-01-29 | Canon Kabushiki Kaisha | Face detection and tracking in a video sequence |
US20040223631A1 (en) * | 2003-05-07 | 2004-11-11 | Roman Waupotitsch | Face recognition based on obtaining two dimensional information from three-dimensional face shapes |
US20060165258A1 (en) * | 2005-01-24 | 2006-07-27 | Shmuel Avidan | Tracking objects in videos with adaptive classifiers |
Cited By (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080187231A1 (en) * | 2005-03-10 | 2008-08-07 | Koninklijke Philips Electronics, N.V. | Summarization of Audio and/or Visual Data |
US20100082585A1 (en) * | 2008-09-23 | 2010-04-01 | Disney Enterprises, Inc. | System and method for visual search in a video media player |
US9165070B2 (en) * | 2008-09-23 | 2015-10-20 | Disney Enterprises, Inc. | System and method for visual search in a video media player |
US8239359B2 (en) * | 2008-09-23 | 2012-08-07 | Disney Enterprises, Inc. | System and method for visual search in a video media player |
US20130007620A1 (en) * | 2008-09-23 | 2013-01-03 | Jonathan Barsook | System and Method for Visual Search in a Video Media Player |
US20100266166A1 (en) * | 2009-04-15 | 2010-10-21 | Kabushiki Kaisha Toshiba | Image processing apparatus, image processing method, and storage medium |
US8428312B2 (en) * | 2009-04-15 | 2013-04-23 | Kabushiki Kaisha Toshiba | Image processing apparatus, image processing method, and storage medium |
US20110216179A1 (en) * | 2010-02-24 | 2011-09-08 | Orang Dialameh | Augmented Reality Panorama Supporting Visually Impaired Individuals |
US8605141B2 (en) | 2010-02-24 | 2013-12-10 | Nant Holdings Ip, Llc | Augmented reality panorama supporting visually impaired individuals |
US10535279B2 (en) | 2010-02-24 | 2020-01-14 | Nant Holdings Ip, Llc | Augmented reality panorama supporting visually impaired individuals |
US11348480B2 (en) | 2010-02-24 | 2022-05-31 | Nant Holdings Ip, Llc | Augmented reality panorama systems and methods |
US9526658B2 (en) | 2010-02-24 | 2016-12-27 | Nant Holdings Ip, Llc | Augmented reality panorama supporting visually impaired individuals |
US20140089799A1 (en) * | 2011-01-03 | 2014-03-27 | Curt Evans | Methods and system for remote control for multimedia seeking |
US8856638B2 (en) * | 2011-01-03 | 2014-10-07 | Curt Evans | Methods and system for remote control for multimedia seeking |
US11017488B2 (en) | 2011-01-03 | 2021-05-25 | Curtis Evans | Systems, methods, and user interface for navigating media playback using scrollable text |
US20150067023A1 (en) * | 2013-08-27 | 2015-03-05 | Cisco Technology, Inc. | System and associated methodology for enhancing communication sessions between multiple users |
US9954909B2 (en) * | 2013-08-27 | 2018-04-24 | Cisco Technology, Inc. | System and associated methodology for enhancing communication sessions between multiple users |
US9858679B2 (en) * | 2014-11-04 | 2018-01-02 | Hewlett-Packard Development Company, L.P. | Dynamic face identification |
US20160125232A1 (en) * | 2014-11-04 | 2016-05-05 | Hewlett-Packard Development Company, L.P. | Dynamic face identification |
US10109299B2 (en) * | 2015-09-24 | 2018-10-23 | Canon Kabushiki Kaisha | Sound processing apparatus, sound processing method, and storage medium |
US20170092296A1 (en) * | 2015-09-24 | 2017-03-30 | Canon Kabushiki Kaisha | Sound processing apparatus, sound processing method, and storage medium |
Also Published As
Publication number | Publication date |
---|---|
US8457368B2 (en) | 2013-06-04 |
US20130039545A1 (en) | 2013-02-14 |
JP2011504673A (en) | 2011-02-10 |
WO2009061420A1 (en) | 2009-05-14 |
US8315430B2 (en) | 2012-11-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8315430B2 (en) | Object recognition and database population for video indexing | |
US11693902B2 (en) | Relevance-based image selection | |
US8064641B2 (en) | System and method for identifying objects in video | |
US10824874B2 (en) | Method and apparatus for processing video | |
US20130148898A1 (en) | Clustering objects detected in video | |
EP2234024B1 (en) | Context based video finder | |
CN108319723B (en) | Picture sharing method and device, terminal and storage medium | |
US10831814B2 (en) | System and method for linking multimedia data elements to web pages | |
US7707162B2 (en) | Method and apparatus for classifying multimedia artifacts using ontology selection and semantic classification | |
US20150186405A1 (en) | System and methods for generation of a concept based database | |
US20140207778A1 (en) | System and methods thereof for generation of taxonomies based on an analysis of multimedia content elements | |
US10380267B2 (en) | System and method for tagging multimedia content elements | |
KR102258420B1 (en) | Animaiton contents resource service system and method based on intelligent information technology | |
CN111327955A (en) | User portrait based on-demand method, storage medium and smart television | |
CN114021577A (en) | Content tag generation method and device, electronic equipment and storage medium | |
KR20090020005A (en) | System and method for recommendation of moving video based on visual content | |
US20150373404A1 (en) | Information processing device and method, and program | |
US20200257724A1 (en) | Methods, devices, and storage media for content retrieval | |
CN112052352A (en) | Video sequencing method, device, server and storage medium | |
Liu et al. | Naming faces in broadcast news video by image google | |
CN111797765B (en) | Image processing method, device, server and storage medium | |
CN112069331A (en) | Data processing method, data retrieval method, data processing device, data retrieval device, data processing equipment and storage medium | |
CN117786137A (en) | Method, device and equipment for inquiring multimedia data and readable storage medium | |
CN117835004A (en) | Method, apparatus and computer readable medium for generating video viewpoints | |
CN113918803A (en) | Method, device, server and storage medium for searching live broadcast room |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: VIEWDLE, INC., NEW YORKFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KOVTUN, IVAN;ZHUKOV, OLEKSANDR;MUSATENKO, YURIY;AND OTHERS;REEL/FRAME:020560/0293Effective date: 20071128 |
|
AS | Assignment |
Owner name: VENTURE LENDING & LEASING V, INC., CALIFORNIAFree format text: SECURITY INTEREST;ASSIGNOR:VIEWDLE INC.;REEL/FRAME:022530/0447Effective date: 20090327Owner name: VENTURE LENDING & LEASING V, INC.,CALIFORNIAFree format text: SECURITY INTEREST;ASSIGNOR:VIEWDLE INC.;REEL/FRAME:022530/0447Effective date: 20090327 |
|
AS | Assignment |
Owner name: VIEWDLE INC., NEW YORKFree format text: CORRECTIVE ASSIGNMENT TO CORRECT THE ASSIGNEE SHOULD BE SPELLED AS "VIEWDLE INC." PREVIOUSLY RECORDED ON REEL 020560 FRAME 0293. ASSIGNOR(S) HEREBY CONFIRMS THE ASSIGNEE SHOULD BE SPELLED AS "VIEWDLE INC.";ASSIGNORS:KOVTUN, IVAN;ZHUKOV, OLEKSANDR;MUSATENKO, YURIY;AND OTHERS;REEL/FRAME:026844/0654Effective date: 20071128 |
|
AS | Assignment |
Owner name: VIEWDLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SCHLESINGER, MYKHAILO;REEL/FRAME:026852/0753Effective date: 20110718 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE TECHNOLOGY HOLDINGS LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA MOBILITY LLC;REEL/FRAME:035379/0116Effective date: 20141028 |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |