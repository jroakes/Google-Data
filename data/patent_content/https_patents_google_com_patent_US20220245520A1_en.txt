US20220245520A1 - Systems and Methods for Generating and Providing Suggested Actions - Google Patents
Systems and Methods for Generating and Providing Suggested Actions Download PDFInfo
- Publication number
- US20220245520A1 US20220245520A1 US17/622,465 US201917622465A US2022245520A1 US 20220245520 A1 US20220245520 A1 US 20220245520A1 US 201917622465 A US201917622465 A US 201917622465A US 2022245520 A1 US2022245520 A1 US 2022245520A1
- Authority
- US
- United States
- Prior art keywords
- context data
- user
- computing system
- suggested
- suggested action
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000009471 action Effects 0.000 title claims abstract description 277
- 238000000034 method Methods 0.000 title claims description 37
- 238000013473 artificial intelligence Methods 0.000 claims abstract description 89
- 230000004044 response Effects 0.000 claims abstract description 27
- 238000004883 computer application Methods 0.000 claims description 41
- 230000003993 interaction Effects 0.000 description 14
- 230000015654 memory Effects 0.000 description 14
- 238000012549 training Methods 0.000 description 13
- 238000013528 artificial neural network Methods 0.000 description 9
- 238000010586 diagram Methods 0.000 description 9
- 238000004891 communication Methods 0.000 description 8
- 230000000694 effects Effects 0.000 description 7
- 230000008569 process Effects 0.000 description 5
- 238000012545 processing Methods 0.000 description 5
- 230000006870 function Effects 0.000 description 4
- 230000000977 initiatory effect Effects 0.000 description 4
- 230000004931 aggregating effect Effects 0.000 description 3
- 230000008859 change Effects 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 238000012552 review Methods 0.000 description 3
- 230000004075 alteration Effects 0.000 description 2
- 239000003795 chemical substances by application Substances 0.000 description 2
- 230000010006 flight Effects 0.000 description 2
- 235000013305 food Nutrition 0.000 description 2
- 238000010801 machine learning Methods 0.000 description 2
- 238000007792 addition Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/004—Artificial life, i.e. computing arrangements simulating life
- G06N3/006—Artificial life, i.e. computing arrangements simulating life based on simulated virtual individual or collective life forms, e.g. social simulations or particle swarm optimisation [PSO]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/02—Knowledge representation; Symbolic representation
- G06N5/022—Knowledge engineering; Knowledge acquisition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
- G06N5/041—Abduction
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/04817—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance using icons
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Definitions
- the present disclosure relates generally to artificial intelligence systems. More particularly, the present disclosure relates to systems and methods for generating and providing suggested actions to a user of a computing device.
- Artificial intelligence and machine learning has been used to assist users of computing devices, for example by providing artificial intelligence agents and personal assistants. Such artificial intelligence agents and personal assistants, however, lack the ability to proactively assist users with remembering actions or items.
- One aspect of the present disclosure is directed to a computing system including at least one processor and an artificial intelligence system including one or more machine-learned models.
- the one or more machine-learned models can be configured to receive a model input that includes context data, and, in response to receipt of the model input, output a model output that describes one or more semantic entities referenced by the context data.
- the computing system can include at least one tangible, non-transitory computer-readable medium that stores instructions that, when executed by the at least one processor, cause the at least one processor to perform operations.
- the operations can include obtaining the context data during a first time interval; inputting the model input that comprises the context data into the one or more machine-learned models; receiving, as an output of the one or more machine-learned models, the model output that describes the one or more semantic entities referenced by the context data; storing the model output in the at least one tangible, non-transitory computer-readable medium; and providing, for display in a user interface during a second time interval that is after the first time interval, a suggested action with respect to the one or more semantic entities described by the model output.
- the method can include obtaining, by one or more computing devices, context data during a first time interval.
- the method can include inputting, by the one or more computing devices, a model input that includes the context data into one or more machine-learned models that are configured to receive the model input that comprises context data, and, in response to receipt of the model input, output a model output that describes one or more semantic entities referenced by the context data.
- the method can include receiving, by the one or more computing devices, as an output of the one or more machine-learned models, the model output that describes the one or more semantic entities referenced by the context data.
- the method can include storing, by the one or more computing devices, the model output in at least one tangible, non-transitory computer-readable medium.
- the method can include providing, by the one or more computing devices for display in a user interface of the one or more computing devices during a second time interval that is after the first time interval, a suggested action with respect to the one or more semantic entities described by the model output.
- FIG. 1A depicts a block diagram of an example computing system for generating and providing suggested actions to users of computing systems according to example embodiments of the present disclosure.
- FIG. 1B depicts a block diagram an example computing system for generating and providing suggested actions to users of computing systems according to example embodiments of the present disclosure.
- FIG. 1C depicts a block diagram of an example computing system for generating and providing suggested actions to users of computing systems according to example embodiments of the present disclosure.
- FIG. 2 depicts an example artificial intelligence system for generating and providing suggested actions according to example embodiments of the present disclosure.
- FIG. 3 depicts an example computing system for generating and providing suggested actions according to example embodiments of the present disclosure including one or more computer applications.
- FIG. 4 depicts an example suggested action according to aspects of the present disclosure.
- FIGS. 5A, 5B, and 5C depict additional example suggested actions according to aspects of the present disclosure.
- FIG. 6 depicts an example panel including multiple suggested actions being displayed in a lock screen of a computing device according to aspects of the present disclosure.
- FIG. 7 depicts computing device displaying an example notification panel displaying a suggested actions with notifications.
- FIG. 8 depicts a computing device in a first state in which a plurality of category labels corresponding with categorized suggested actions are displayed according to aspects of the present disclosure.
- FIG. 9 depicts the computing device of FIG. 8 in which one category label of the plurality of category labels has been selected and suggested actions corresponding with the selected category label are displayed according to aspects of the present disclosure.
- FIG. 10 depicts a suggested action in which the semantic entity has been selected and search is being performed in response to the semantic entity being selected according to aspects of the present disclosure.
- FIG. 11 depicts a computing system displaying a settings panel in which the user can select a default computer application for a type of suggested action.
- FIG. 12 depicts a flowchart of a method for generating and providing suggested actions to users of computing systems according to aspects of the present disclosure.
- the present disclosure is directed to an artificial intelligence system for identifying information of interest, storing the information, and providing suggested actions to users of computing systems at a later time based on the stored information.
- the artificial intelligence system can be configured to intelligently process information on behalf of the user, including, for example, visual and/or audio information that is displayed, played, and/or otherwise processed or detected by the computing device.
- the artificial intelligence system can capture information of interest as the computing device is used to perform tasks throughout the day.
- the artificial intelligence system can identify and store semantic entities while the user navigates between various computer applications and/or switches between different tasks or activities.
- the artificial intelligence system can identify and store semantic entities referenced or included in a surrounding environment of the user (e.g., through analysis of captured imagery, audio, or other data regarding the surrounding environment.
- the artificial intelligence system can capture and process information (e.g., to identify semantic entities) that is actively identified or emphasized by a user while in other instances the artificial intelligence system can capture and process information (e.g., to identify semantic entities) that is simply referenced by or included in the ambient environment of the user (e.g., information that is contained in the surrounding environment but not specifically actively identified or emphasized by the user).
- the artificial intelligence system can save or otherwise retain data associated with semantic entities as they are recognized over time. For example, the saved semantic entities can be ranked, sorted, categorized, prioritized etc. based on the user's preferences and/or the user's plans or schedule. As another example, the artificial intelligence system can generate one or more suggested actions for a user that are related to one or more of the identified semantic entities. For example, the suggested actions can include actions that can be taken by the artificial intelligence system and/or a computer application under direction of the artificial intelligence system for and/or on behalf of the user relative to the identified semantic entities.
- the suggested actions can include communications actions (e.g., emailing a certain contact), information retrieval actions (e.g., retrieving options to purchase or shop for a certain item, providing an opportunity to listen to a certain song, accessing geographic information such as the location of a point of interest), a booking action (e.g., requesting a ride share vehicle or purchasing a flight ticket), information storage (e.g., note-taking or inserting an item into a user's calendar), and/or many other suggested actions.
- communications actions e.g., emailing a certain contact
- information retrieval actions e.g., retrieving options to purchase or shop for a certain item, providing an opportunity to listen to a certain song, accessing geographic information such as the location of a point of interest
- a booking action e.g., requesting a ride share vehicle or purchasing a flight ticket
- information storage e.g., note-taking or inserting an item into a user's calendar
- the saved suggested actions can be provided for display.
- the suggested actions can be accessed by the user via a specific menu, can be provided in a notifications menu, can be automatically surfaced at later, contextually relevant times, and/or can be accessed in other manners.
- the suggested actions can include links or buttons to perform the suggested actions (e.g., with computer applications).
- the user can also optionally provide feedback and/or instructions to the artificial intelligence system to customize how the artificial intelligence system captures information and/or suggests actions. Alternatively, the artificial intelligence system can also learn the user's preferences based on how the user interacts with the suggested actions.
- the user can be provided with controls allowing the user to make an election as to both if and when systems, programs, or features described herein may enable collection of user information (e.g., ambient audio, text presented in the user interface, etc.).
- user information e.g., ambient audio, text presented in the user interface, etc.
- certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user's identity may be treated so that no personally identifiable information can be determined for the user.
- the user may have control over what information is collected about the user, how that information is used, and what information is provided to the user.
- aspects of the present disclosure are directed to an artificial intelligence system that operates over multiple different time periods to provide suggested actions that are contextually meaningful.
- the artificial intelligence system can identify the semantic entities over a first time interval, store the semantic entities, and then display the suggested actions during a second time interval that is after the first time interval.
- the suggested actions can act as reminders for the user during the second time interval to complete a task that the user started earlier.
- aggregating relevant information over a first time interval and then providing multiple suggested actions based on the information during a second, later time interval can be less disruptive to the user. This can also be more useful to the user as the user is more likely to have forgotten about the task after some time has passed.
- the artificial intelligence system can operate as a intelligent memory assistant which assist the user in remembering actions which they may want to take based on activities they engaged in earlier that day, week, month, etc.
- the user can take a “screenshot” of an item while shopping in a first time interval.
- the artificial intelligence system can generate and store a name or description of the item. Later, the user can suggest a specific meeting time and day during a phone call.
- the artificial intelligence system can generate and store a second semantic entity that can include the name of the person, the meeting time, etc.
- the artificial intelligence system can display suggested actions with respect to the each of the item from the screenshot and the suggested meeting.
- the suggested action for the item can include purchasing the item displayed in the screenshot, and the suggested action for the meeting can include creating a calendar event based on the information gathered during the phone call.
- the artificial intelligence system can generate the suggested action(s) based on a template (e.g., a predefined template). Utilizing a template can reduce the computing resources required to generate such suggested actions. Instead of training and utilizing a machine-learned model to generate the complete suggested action, keywords of the suggested action can be generated using the machine-learned model(s) and then assembled according to the template to generate the suggested action.
- the template can include a verb, a semantic entity described by a model output of the machine-learned model, and a computer application, for example as follows:
- the artificial intelligence system can select an appropriate verb and computer application to be inserted into the respective placeholders of the template.
- One example of a suggested action generated based on the above template is “Add Appt. with Dr. Sherrigan to Calendar.” It should be understood that a variety of templates can be employed.
- the artificial intelligence system can learn the user's preferences with respect to which templates to use and/or whether to use templates. Thus, the systems and methods described herein can employ one or more templates to generate the suggested actions.
- the same template or template(s) can be used to generate multiple suggested actions such that the multiple suggested actions have the same general look and feel to the user. As such, the user can quickly evaluate the suggested actions because they are presented in a known and predictable format to the user.
- the templates as described herein can facilitate greater use of the suggested actions.
- the artificial intelligence system can include one or more machine-learned models that are configured to receive a model input that includes context data (e.g., ambient audio, information displayed in a screen of the computing device, etc.).
- the computing system can be configured to obtain the context data and input the model input that includes the context data into the machine-learned model.
- the computing system can receive, as an output of the machine-learned model, model output that describes one or more semantic entities referenced by the context data.
- the computing system can store the model output in at least one tangible, non-transitory computer-readable medium.
- the computing system can provide a suggested action with respect to the one or more semantic entity.
- the context data discussed herein can include a variety of information, such as information currently displayed in the user interface, information previously displayed in the user interface, information gleaned from the user's previous actions (e.g., text written or read by the user, content viewed by the user, etc.), and/or the like.
- the context data can include user data that describes a preference or other information associated with the user and/or contact data that describes preferences or other information associated with a contact of the user.
- Example context data can include a message received by the computing system for the user, the user's previous interactions with one or more of the user's contacts (e.g., a text message mentioning a user preference for a restaurant or type of food), previous interactions associated with a location (e.g., going to a park, museum, other attraction, etc.), a business, etc. (e.g., posting a review for a restaurant, reading a menu of a restaurant, reserving a table at a restaurant, etc.), and/or any other suitable information about the user's preferences or user.
- the user's previous interactions with one or more of the user's contacts e.g., a text message mentioning a user preference for a restaurant or type of food
- previous interactions associated with a location e.g., going to a park, museum, other attraction, etc.
- a business e.g., posting a review for a restaurant, reading a menu of a restaurant, reserving a table at
- audio played or processed by the computing system can include audio detected by the computing system, information about the user's location (e.g., a location of a mobile computing device of the computing system), and/or calendar data.
- context data can include ambient audio detected by a microphone of the computing system and/or phone audio processed during a phone call.
- Calendar data can describe future events or plans (e.g., flights, hotel reservations, dinner plans etc.).
- Example semantic entities that can be described by the model output can include words or phrases recognized in the text and/or audio.
- Additional examples can includes information about the user's location, such as a city name, state name, street name, names of nearby attractions, and the like.
- multiple suggested actions can be displayed together in the second time interval after the context data is collected during the first time interval. More specifically, at least one additional suggested action can be displayed in the user interface with the suggested action.
- the additional suggested action(s) can be distinct from the suggested action.
- the additional suggested action(s) can also be generated and stored (e.g., during the first time) by the artificial intelligence system based on distinct semantic entities. For example, the artificial intelligence system can obtain additional context data that is distinct from the context data and input additional model input(s) that include the additional context data into the machine-learned model(s).
- the artificial intelligence system can receive, as an additional output of the machine-learned model(s), data descriptive of the additional suggested action(s) that is described by the additional suggested action(s).
- the artificial intelligence system can leverage the machine-learned model(s) to store multiple semantic entities over the first time interval and then display multiple suggested actions during the second time interval.
- the artificial intelligence system can rank, sort, categorize, and prioritize, etc. the suggested actions, for example, based on the user data.
- the user data can include user preferences, calendar data (e.g., the user's plans or schedule), and/or other information about the user or the computing device.
- the artificial intelligence system can rank the suggested actions and arrange the suggested actions within the user interface based on the ranking.
- the artificial intelligence system can prioritize the suggested actions and selectively display a group of the most important and/or relevant suggested actions to the user in the user interface.
- the artificial intelligence system can categorize the suggested actions with respect to a plurality of categories.
- Category labels can be displayed in the user interface corresponding with the categories such that the user can navigate between the categories of suggested actions using the category labels (e.g., in separate panels or pages).
- the computing system can detect a user touch action with respect to one category label. In response to detecting the user touch action, the computing system can display suggested actions that were categorized with respect to the selected suggested action.
- the artificial intelligence system can categorize the suggested actions and provide an intuitive way for the user to navigate between the categories of suggested actions for selection by the user.
- the computing system can display explanations with respect to the suggested actions.
- the explanations can describe information about obtaining the context data, including, as examples, a time when the context data was obtained, a location of the computing device when the context data was obtained, and/or a source of the context data.
- the explanation can indicate that the suggested action was generated based on audio from a phone conversation with a specific user contact that occurred at a specific time.
- the explanation can indicate that the suggested action was generated based on a user's shopping session with a specific shopping application at a specific time.
- the source(s) of the data can include a computer application that was being displayed when the context data was obtained; whether the context data was obtained from ambient audio, text, graphical information, etc.; and/or any other information associated with obtaining the context data.
- Such explanations can provide the user with a better understanding of the operations of the artificial intelligence system. As a result, the user may be more comfortable with or trusting of the artificial intelligence system operations, which can make the artificial intelligence system more useful.
- the computing system can provide the user with a way to view additional information associated with obtaining the context data in addition to the explanation(s). For example, the computing system can detect a user touch input that requests additional information with respect to the explanation. In response to detecting the user touch input, the computing system can display additional explanation information about obtaining the context data.
- the additional explanation information can include a time when the context data was obtained or a location of the computing device when the context data was obtained.
- the additional explanation information can include a source of the context data (if not already displayed).
- the additional information can include information about other times that context data was obtained in a similar way (e.g., from the same source, at similar times, etc.).
- the computing system can provide the user with a way to adjust preferences with respect to how the artificial intelligence system collects context data.
- the additional explanation information can also include preferences and/or rules with respect to when and how the artificial intelligence system can obtain the context data.
- the user can adjust the rules and/or preferences within the user interface.
- the computing system can display the suggested action(s) automatically or in response to a user request and in a variety of locations.
- a panel displaying the suggested action(s) can be displayed in a “lock screen” or “home screen” of the computing device.
- the panel can be accessible at a system level from a drop down panel, navigation bar, or the like.
- the panel can be automatically displayed at one or more regular times throughout the day.
- the artificial intelligence system can intelligently choose when to display the panel based on the user data (e.g., preferences) and/or context data.
- the artificial intelligence system can display the panel when the suggested actions would be most relevant to the user based on the content of the suggested actions.
- the computing system can be configured to interface with one or more computer applications to provide suggested actions that can be performed with the computer application(s).
- the computing system can provide data descriptive of the model output of the machine-learned model(s) that describes the suggested action(s) to the computer application(s).
- the computing system can receive one or more application outputs respectively from the computing application(s) via a pre-defined application programming interface.
- the suggested action can describe at least one of the application output(s).
- the user can select a portion of the suggested action (e.g., the semantic entity) to perform another action with respect to the selected portion of the suggested action that is distinct from the suggested action.
- the computing system in user response to detecting a user touch action directed towards the semantic entity of the suggested action, can display a panel including a search (e.g., a web search) of the semantic entity. Additional examples include editing the semantic entity, changing a computer application of the suggested action, editing details of the suggested action, and so forth.
- a suggested action can include shopping for a specific brand of a produce (e.g., grill) using a specific shopping application.
- the user can select the semantic entity (e.g., “Webster Classic Grill”) and manually edit the entity, for example to change the brand name or type of product.
- the user can change “Webster Classic Grill” to “Webster Classic Grill Cover” before selecting the suggested action to purchase or shop for the item using the shopping application.
- the user can change the shopping application.
- the systems and methods of the present disclosure can be included or otherwise employed within the context of an application, a browser plug-in, or in other contexts.
- the models of the present disclosure can be included in or otherwise stored and implemented by a user computing device such as a laptop, tablet, or smartphone.
- the models can be included in or otherwise stored and implemented by a server computing device that communicates with the user computing device according to a client-server relationship.
- the models can be implemented by the server computing device as a portion of a web service (e.g., a web email service).
- context data may be automatically acquired from one or more sources of the system, such as one or more of microphone(s), camera(s), webpages visited, location(s) of the system, and orientation(s) of the system during normal use of the system and without the user necessarily opening a particular application.
- the context data may be acquired so long as the system is switched on and/or when an appropriate passcode, password and/or biometric identifier is verified. The user is not required therefore to remember to initiate a particular function to acquire context data.
- the context data may automatically be stored on a system-level “clipboard.” The user may disable certain sources of the context data if desired.
- the systems and methods of the present disclosure may limit the amount of data that is stored on memory and/or which is allocated to such systems and methods.
- the systems and methods may automatically remove or overwrite acquired context data and/or suggested actions based on one or more rules.
- context data that is older than a predetermined period, e.g. which is one day or one week old, may be removed or overwritten automatically.
- the rules e.g. the predetermined period, may be user configurable.
- the predetermined period may be updated dynamically for particular context data and/or suggested actions based on prior user interactions. For example, prior user interactions may be user selections in relation to suggested actions.
- the context data which is linked to shopping actions may be deleted or overwritten sooner than those for calendar appointments.
- a maximum limit may be placed on the amount of data that is stored so as to avoid taking away storage resources of application data. Where suggested actions are ranked in a priority order, only the top N suggested actions may be preserved for output.
- the systems and methods of the present disclosure may provide one or more suggested actions linked or associated with one or more applications, with at least one selectable button or the like being displayed alongside or otherwise with the suggested action to permit a single or reduced number of taps, touches or swipe gestures to effect the suggested action.
- the number of physical user interactions to effect a suggested action such as adding an appointment to a calendar event, playing a music or video track, opening a shopping website, etc. may require fewer user interactions with the user interface and/or use less electrical energy and processing resources than opening a particular application and making selections and/or entering data manually.
- opening a shopping application or website for purchasing a particular product may be achieved using a single gesture, rather than opening the relevant shopping application or browser window, entering a search term and then selecting an item from a list of suggestions.
- the suggested action may be displayed in an associated portion of the user interface and, as well as having a selectable button or the like associated with initiation of the suggested action, one or more further buttons or the like may be presented in the same portion for single-touch initiation of related functions which ordinarily might require the application to be opened.
- one or more further buttons or the like associated with that application, might be presented, such as a “like” button or a “share” button, selection of which causes the associated action to be performed.
- a suggested action to create a calendar appointment if it is detected that the proposed appointment clashes with an existing one, one or more further buttons or the like may be provided to initiate cancelling and/or rescheduling the existing appointment.
- the artificial intelligence system intelligently chooses when to display the panel based on the user data (e.g., user preferences) and/or context data
- the artificial intelligence system can display the panel when the suggested actions would be most relevant, or less disturbing or intrusive to the user, based on the content of the suggested actions. For example, if the suggested actions comprise playing audio or video, then the artificial intelligence system may choose not to display such suggested actions during working hours, whereas “silent” suggested actions such as suggested appointments or suggested shopping actions may be displayed at said times. As mentioned, aggregating suggested actions until a later, second time interval, avoids overly disturbing the user and/or being intrusive.
- context data may be automatically acquired from one or more sources of the system, such as one or more of microphone(s), camera(s), webpages visited, location(s) of the system, and orientation(s) of the system during normal use of the system and without the user necessarily opening a particular application.
- the context data may be acquired so long as the system is switched on and/or when an appropriate passcode, password and/or biometric identifier is verified. The user is not required therefore to remember to initiate a particular function to acquire context data.
- the context data may automatically be stored on a system-level “clipboard.” However, the user may disable certain sources of the context data if desired.
- the systems and methods of the present disclosure may limit the amount of data that is stored on memory and/or which is allocated to such systems and methods.
- the systems and methods may automatically remove or overwrite acquired context data and/or suggested actions based on one or more rules.
- context data that is older than a predetermined period, e.g. which is one day or one week old, may be removed or overwritten automatically.
- the rules e.g. the predetermined period, may be user configurable.
- the predetermined period may be updated dynamically for particular context data and/or suggested actions based on prior user interactions. For example, prior user interactions may include user selections in relation to suggested actions.
- the context data which is linked to shopping actions may be deleted or overwritten sooner than those for calendar appointments.
- a maximum limit may be placed on the amount of data that is stored so as to avoid taking away storage resources of application data. Where suggested actions are ranked in a priority order, only the top N suggested actions may be preserved for output.
- the systems and methods of the present disclosure may provide one or more suggested actions linked or associated with one or more applications, with at least one selectable button or the like being displayed alongside or otherwise with the suggested action to permit a single or reduced number of taps, touches or swipe gestures to effect the suggested action.
- the number of physical user interactions to effect a suggested action such as adding an appointment to a calendar event, playing a music or video track, opening a shopping website, etc. may require fewer user interactions with the user interface and/or use less electrical energy and processing resources than opening a particular application and making selections and/or entering data manually.
- opening a shopping application or website for purchasing a particular product may be achieved using a single gesture, rather than opening the relevant shopping application or browser window, entering a search term and then selecting an item from a list of suggestions.
- the suggested action may be displayed in an associated portion of the user interface and, as well as having a selectable button or the like associated with initiation of the suggested action, one or more further buttons or the like may be presented in the same portion for single-touch initiation of related functions which ordinarily might require the application to be manually opened by the user.
- one or more further buttons or the like associated with that application, might be presented, such as a “like” button or a “share” button. The selection of such a button can cause the associated action to be performed.
- buttons, or the like may be provided to initiate cancelling and/or rescheduling the existing appointment.
- the artificial intelligence system intelligently chooses when to display the panel based on the user data (e.g., preferences) and/or context data
- the artificial intelligence system can display the panel when the suggested actions would be most relevant, or less disturbing or intrusive to the user, based on the content of the suggested actions. For example, if the suggested actions comprise playing audio or video, then the artificial intelligence system may choose not to display such suggested actions during working hours, whereas “silent” suggested actions such as suggested appointments or suggested shopping actions may be displayed at said times. As mentioned, aggregating suggested actions until a later, second time interval, avoids overly disturbing the user and/or being intrusive.
- FIG. 1A depicts a block diagram of an example computing system 100 for generating and providing suggested actions according to example embodiments of the present disclosure.
- the system 100 includes a user computing device 102 , a server computing system 130 , and a training computing system 150 that are communicatively coupled over a network 180 .
- the user computing device 102 can be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
- a personal computing device e.g., laptop or desktop
- a mobile computing device e.g., smartphone or tablet
- a gaming console or controller e.g., a gaming console or controller
- a wearable computing device e.g., an embedded computing device, or any other type of computing device.
- the user computing device 102 includes one or more processors 112 and memory 114 .
- the one or more processors 112 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 114 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 114 can store data 116 and instructions 118 which are executed by the processor 112 to cause the user computing device 102 to perform operations.
- the user computing device 102 can store or include one or more computer applications 119 .
- the computer application(s) 119 can be configured to perform various operations and provide application output(s) as described herein.
- the user computing device 102 can store or include an artificial intelligence system 120 .
- the artificial intelligence system 120 can perform some or all of the operations described herein.
- the artificial intelligence system 120 can be separate and distinct from the one or more computer applications 119 but can be capable of communicating with the one or more computer applications 119 .
- the user computing device 102 can store or include one or more machine-learned models 122 .
- the machine-learned models 122 can be or can otherwise include various machine-learned models such as neural networks (e.g., deep neural networks) or other multi-layer non-linear models.
- Neural networks can include recurrent neural networks (e.g., long short-term memory recurrent neural networks), feed-forward neural networks, or other forms of neural networks.
- Example machine-learned models 122 are discussed with reference to FIGS. 2 and 3 .
- the one or more machine-learned models 122 can be received from the server computing system 130 over network 180 , stored in the user computing device memory 114 , and the used or otherwise implemented by the one or more processors 112 .
- the user computing device 102 can implement multiple parallel instances of a single machine-learned model 122 (e.g., to perform parallel operations across multiple instances of the machine-learned 120 ).
- an artificial intelligence system 140 can be included in or otherwise stored and implemented by the server computing system 130 that communicates with the user computing device 102 according to a client-server relationship.
- the artificial intelligence system 140 can include a machine-learned model 142 .
- the machine-learned models 142 can be implemented by the server computing system 140 as a portion of a web-based service.
- one or more models 122 can be stored and implemented at the user computing device 102 and/or one or more models 142 can be stored and implemented at the server computing system 130 .
- the user computing device 102 can also include one or more user input component 124 that receives user input.
- the user input component 124 can be a touch-sensitive component (e.g., a touch-sensitive display screen or a touch pad) that is sensitive to the touch of a user input object (e.g., a finger or a stylus).
- the touch-sensitive component can serve to implement a virtual keyboard.
- Other example user input components include a microphone, a traditional keyboard, or other means by which a user can enter a communication.
- the server computing system 130 includes one or more processors 132 and a memory 134 .
- the one or more processors 132 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 134 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 134 can store data 136 and instructions 138 which are executed by the processor 132 to cause the server computing system 130 to perform operations.
- the server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances in which the server computing system 130 includes plural server computing devices, such server computing devices can operate according to sequential computing architectures, parallel computing architectures, or some combination thereof.
- the server computing system 130 can store or otherwise includes one or more machine-learned models 142 .
- the models 142 can be or can otherwise include various machine-learned models such as neural networks (e.g., deep recurrent neural networks) or other multi-layer non-linear models.
- Example models 142 are discussed with reference to FIGS. 2 and 3 .
- the server computing system 130 can train the models 142 via interaction with the training computing system 150 that is communicatively coupled over the network 180 .
- the training computing system 150 can be separate from the server computing system 130 or can be a portion of the server computing system 130 .
- the training computing system 150 includes one or more processors 152 and a memory 154 .
- the one or more processors 152 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 154 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 154 can store data 156 and instructions 158 which are executed by the processor 152 to cause the training computing system 150 to perform operations.
- the training computing system 150 includes or is otherwise implemented by one or more server computing devices.
- the training computing system 150 can include a model trainer 160 that trains the machine-learned models 142 stored at the server computing system 130 using various training or learning techniques, such as, for example, backwards propagation of errors.
- performing backwards propagation of errors can include performing truncated backpropagation through time.
- the model trainer 160 can perform a number of generalization techniques (e.g., weight decays, dropouts, etc.) to improve the generalization capability of the models being trained.
- the training data 162 can be obtained from the user computing device 102 (e.g., based on communications previously provided by the user of the user computing device 102 ).
- the model 122 provided to the user computing device 102 can be trained by the training computing system 150 on user-specific communication data received from the user computing device 102 . In some instances, this process can be referred to as personalizing the model.
- the model trainer 160 includes computer logic utilized to provide desired functionality.
- the model trainer 160 can be implemented in hardware, firmware, and/or software controlling a general purpose processor.
- the model trainer 160 includes program files stored on a storage device, loaded into a memory and executed by one or more processors.
- the model trainer 160 includes one or more sets of computer-executable instructions that are stored in a tangible computer-readable storage medium such as RAM hard disk or optical or magnetic media.
- the network 180 can be any type of communications network, such as a local area network (e.g., intranet), wide area network (e.g., Internet), or some combination thereof and can include any number of wired or wireless links.
- communication over the network 180 can be carried via any type of wired and/or wireless connection, using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
- FIG. 1A illustrates one example computing system that can be used to implement the present disclosure.
- the user computing device 102 can include the model trainer 160 and the training dataset 162 .
- the models 122 can be both trained and used locally at the user computing device 102 .
- the user computing device 102 can implement the model trainer 160 to personalize the models 122 based on user-specific data.
- FIG. 1B depicts a block diagram of an example computing device 10 that can be used to implement the present disclosure.
- the computing device 10 can be a user computing device or a server computing device.
- the computing device 10 includes a number of applications (e.g., applications 1 through N). Each application contains its own machine learning library and machine-learned model(s). For example, each application can include a machine-learned model.
- Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc.
- each application can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components.
- each application can communicate with each device component using an API (e.g., a public API).
- the API used by each application is specific to that application.
- FIG. 1C depicts a block diagram of an example computing device 50 that performs according to example embodiments of the present disclosure.
- the computing device 50 can be a user computing device or a server computing device.
- the computing device 50 includes a number of applications (e.g., applications 1 through N). Each application is in communication with a central intelligence layer.
- Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc.
- each application can communicate with the central intelligence layer (and model(s) stored therein) using an API (e.g., a common API across all applications).
- the central intelligence layer includes a number of machine-learned models. For example, as illustrated in FIG. 1C , a respective machine-learned model (e.g., a model) can be provided for each application and managed by the central intelligence layer. In other implementations, two or more applications can share a single machine-learned model. For example, in some implementations, the central intelligence layer can provide a single model (e.g., a single model) for all of the applications. In some implementations, the central intelligence layer is included within or otherwise implemented by an operating system of the computing device 50 .
- a respective machine-learned model e.g., a model
- two or more applications can share a single machine-learned model.
- the central intelligence layer can provide a single model (e.g., a single model) for all of the applications.
- the central intelligence layer is included within or otherwise implemented by an operating system of the computing device 50 .
- the central intelligence layer can communicate with a central device data layer.
- the central device data layer can be a centralized repository of data for the computing device 50 . As illustrated in FIG. 1C , the central device data layer can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a private API).
- an API e.g., a private API
- FIG. 2 depicts a block diagram of an example artificial intelligence system 200 according to example embodiments of the present disclosure.
- the artificial intelligence system 200 can include one or more machine-learned model(s) 202 that are trained to receive context data 204 and, as a result of receipt of the context data 204 , provide a model output 206 that describes one or more semantic entities referenced by the context data 204 .
- the context data 204 discussed herein can include a variety of information, such as information currently displayed in the user interface, information previously displayed in the user interface, information gleaned from the user's previous actions (e.g., text written or read by the user, content viewed by the user, etc.), and/or the like.
- the context data 204 can include user data that describes a preference or other information associated with the user and/or contact data that describes preferences or other information associated with a contact of the user.
- Example context data 204 can include a message received by the computing system for the user, the user's previous interactions with one or more of the user's contacts (e.g., a text message mentioning a user preference for a restaurant or type of food), previous interactions associated with a location (e.g., going to a park, museum, other attraction, etc.), a business, etc. (e.g., posting a review for a restaurant, reading a menu of a restaurant, reserving a table at a restaurant, etc.), and/or any other suitable information about the user's preferences or user.
- the user's previous interactions with one or more of the user's contacts e.g., a text message mentioning a user preference for a restaurant or type of food
- previous interactions associated with a location e.g., going to a park, museum, other attraction, etc.
- a business e.g., posting a review for a restaurant, reading a menu of a restaurant, reserving a table
- audio played or processed by the computing system can include audio detected by the computing system, information about the user's location (e.g., a location of a mobile computing device of the computing system), and/or calendar data.
- the context data 204 can include ambient audio detected by a microphone of the computing system and/or phone audio processed during a phone call.
- Calendar data can describe future events or plans (e.g., flights, hotel reservations, dinner plans etc.).
- Example semantic entities that can be described by the model output 206 can include words or phrases recognized in the text and/or audio. Additional examples can includes information about the user's location, such as a city name, state name, street name, names of nearby attractions, and the like.
- the model output 206 can more directly describe suggested actions.
- the machine-learned model(s) 202 can be trained to output data that describes text of suggested actions (e.g., including a verb, application, and the semantic entity), for example as described with reference to FIGS. 4 through 9 .
- a single machine-learned model 202 can be trained to receive the context data 204 and output such model output 206 .
- multiple machine-learned model(s) 202 can be trained (e.g., end-to-end) to produce such model output 206 .
- a first model of the machine-learned models 202 can output data that describes a semantic entity included in the context data 204 .
- a second model of the machine-learned model can receive the data that describes a semantic entity included in the context data 204 and output the model output 206 that describes the suggest action with respect to the semantic entity.
- the second machine-learned model(s) can additionally receive some or all of the context data 204 .
- One or ordinary skill in the art would understand that additional configurations are possible within the scope of the present disclosure.
- FIG. 3 depicts a block diagram of an example computing system 300 including an artificial intelligence system 301 .
- the artificial intelligence system 301 can include one or more machine-learned model(s) 302 according to example embodiments of the present disclosure.
- the machine-learned model(s) 302 can be trained to receive context data 304 and, as a result of receipt of the context data 304 , provide a model output 306 that describes one or more semantic entities referenced by the context data 304 .
- the computing system 300 can be configured to interface with one or more computer applications 308 to provide suggested actions that can be performed with the computer application(s) 308 .
- the computing system 300 can provide data descriptive of the model output 306 of the machine-learned model(s) 302 that describes the suggested action(s) to the computer application(s) 308 .
- the computing system 300 can receive one or more application outputs 310 respectively from the computing application(s) 308 via a pre-defined application programming interface.
- the suggested action can describe or correspond with at least one of the application output(s) 310 .
- FIG. 4 depicts an example suggested action 400 according to aspects of the present disclosure.
- the suggested action 400 can describe an available action that can be performed with a computer application.
- the suggested action 400 can include creating a calendar event with a calendar application.
- More specifically, in this example suggested action 400 includes the text “Add Appt. with Dr. Sherrigan to Calendar.”
- the computing system can be configured to perform this action can in response to receiving a user touch input requesting the same. For instance, the user can tap a button 401 , slide a slider bar, or otherwise interact with the suggested action 400 to request that the computing system perform the action.
- the artificial intelligence system can generate the suggested action(s) 400 based on a template (e.g., a predefined template). Utilizing a template can reduce the computing resources required to generate such suggested actions 400 . Instead of training and utilizing a machine-learned model to generate all text of the suggested action 400 , key words of the suggested action 400 can be generated using the machine-learned model(s) and then assembled according to the template to generate the suggested action 400 .
- the template can include respective placeholders for a verb 402 , a semantic entity 404 , and/or a computer application 406 .
- the semantic entity 404 can be described by the model output 206 of the machine-learned model(s) 202 , for example as described above with reference to FIG. 2 .
- the template may be arranged as follows:
- the artificial intelligence system can select an appropriate verb and/or computer application to be inserted into the respective placeholders of the template.
- the verb and/or computer application can be selected based on the context data and/or semantic entity. It should be understood that a variety of template variations can be employed within the scope of this disclosure.
- the artificial intelligence system can learn the user's preferences with respect to which templates to use and/or whether to use templates. Thus, the systems and methods described herein can employ one or more templates to generate the suggested actions.
- the same template or template(s) can be used to generate multiple suggested actions such that the multiple suggested actions have the same general look and feel to the user. As such, the user can quickly evaluate the suggested actions because they are presented in a known and predictable format to the user.
- the templates as described herein can facilitate greater use of the suggested actions.
- An explanation 410 can be displayed with respect to the suggested actions 400 .
- the explanation 410 can describe information about obtaining the context data, including, as examples, a time when the context data was obtained, a location of the computing device when the context data was obtained, and/or a source of the context data.
- the explanation 410 states that the context data was “saved at Home” at “8:06 AM.”
- the explanation 410 can indicate a source of the context data used to generate the suggested action 400 by displaying an icon.
- the explanation 410 can include a phone icon 412 to indicate that the context data was collected from audio during a phone call.
- the explanation 410 may encourage the user to be more comfortable with or trusting of the artificial intelligence system operations, which can make the artificial intelligence system more useful to the user.
- the computing system can provide the user with a way to view additional information associated with obtaining the context data in addition to the explanation 410 .
- the computing system can detect a user touch input that requests additional information with respect to the explanation 410 .
- the computing system in response to receiving a user touch input directed to “Details” 414 , the computing system can display additional explanation information about obtaining the context data.
- the additional explanation information can include a time when the context data was obtained or a location of the computing device when the context data was obtained.
- the additional explanation information can include a source of the context data (if not already displayed).
- the additional information can include information about other times that context data was obtained in a similar way (e.g., from the same source, at similar times, while the computing device was in the same location etc.).
- FIG. 5A depicts an additional example suggested action 500 according to aspects of the present disclosure.
- This example suggested action 500 describes an available action that can be performed with a shopping application.
- the suggested action 500 can include shopping for an item “Webster Classic Grill” using the shopping application.
- the suggested action 500 includes the text “Shop Weber Classic Grill on Amazon.” This text can be generated based on a predefined template as described above with reference to FIG. 4 .
- the template can include respective placeholders for a verb 502 , a semantic entity 504 , and a computer application 506 .
- the computing system can be configured to perform this action in response to receiving a user touch input requesting the same. For instance, the user can tap a button 501 , slide a slider bar, or otherwise interact with the suggested action 500 to request that the computing system perform the action.
- An explanation 510 can be displayed with respect to the suggested actions 500 that describes information about obtaining the context data, including, as examples, a time and location of the computing device when the context data was obtained and/or a source of the context data.
- the explanation 510 states that the context data was “saved at Home Depot” at “8:38 AM.”
- the explanation 510 can indicate a source of the context data used to generate the suggested action 500 , for example by displaying an icon 512 .
- the icon 512 can indicate that the context data was obtained from an image (e.g., a photograph or screenshot).
- the explanation 510 may encourage the user to be more comfortable with or trusting of the artificial intelligence system operations, which can make the artificial intelligence system more useful to the user.
- the computing system can be configured to detect a user touch input that requests additional information with respect to the explanation 510 .
- the computing system in response to receiving a user touch input directed to “Details” 514 , the computing system can display additional explanation information about obtaining the context data.
- FIG. 5B depicts an additional example suggested action 520 according to aspects of the present disclosure.
- the suggested action 520 can include shopping for an item using a shopping application: “Shop BKR water bottle on Amazon.” This text can be generated based on a predefined template as described above with reference to FIG. 4 .
- the template can include respective placeholders for a verb 522 , a semantic entity 524 , and a computer application 526 .
- the suggested action 520 can include a button 525 for performing the suggested action 520 and an explanation 530 .
- the explanation 530 indicates the location and time when the context data was saved (e.g., “saved at home ⁇ 8:06 AM.”).
- the explanation 530 can indicate a source of the context data used to generate the suggested action 520 by displaying an icon 532 .
- the icon 532 can indicate that the context data was obtained from audio (e.g., a voice memo or ambient audio).
- the explanation 530 may encourage the user to be more comfortable with or trusting of the artificial intelligence system operations, which can make the artificial intelligence system more useful to the user.
- the computing system can be configured to detect a user touch input that requests additional information with respect to the explanation 530 . In this example, in response to receiving a user touch input directed to “Details” 534 , the computing system can display additional explanation information about obtaining the context data.
- FIG. 5C depicts an additional example suggested action 540 according to aspects of the present disclosure.
- This example suggested action 540 describes an available action that can be performed with a music streaming application.
- the suggested action 540 can include listening to a particular song 544 by a particular artist 546 , which can correspond with the stored semantic entity.
- the user can tap a button 541 to perform the suggested action 540 .
- the suggested action 540 can include a save button 548 for saving the suggested action 540 for a later time and/or a share button 550 for sharing the suggested action, for example, via social media, text message, e-mail, etc.
- An explanation 552 can be displayed with respect to the suggested actions 540 that describes information about obtaining the context data, including, as examples, a time and location of the computing device when the context data was obtained and/or a source of the context data.
- the explanation 552 states that the context data was “saved at Linda's Tavern” at “11:06 PM.”
- a portion of the explanation 552 such as the location “Linda's Tavern” can include a link, for example to further information about the location (e.g., a web search or map application search).
- the explanation 552 can indicate a source of the context data used to generate the suggested action 540 by displaying an icon 554 .
- the icon 554 can indicate that the context data was obtained from ambient music (e.g., detected by a microphone of the computing device).
- the explanation 552 may encourage the user to be more comfortable with or trusting of the artificial intelligence system operations, which can make the artificial intelligence system more useful to the user.
- the computing system can be configured to detect a user touch input that requests additional information with respect to the explanation 552 . In this example, in response to receiving a user touch input directed to “Details” 556 , the computing system can display additional explanation information about obtaining the context data.
- FIG. 6 depicts computing device 600 displaying an example panel 602 including multiple suggested actions 604 , 606 , 608 being displayed in a lock screen according to aspects of the present disclosure.
- the lock screen can be displayed when the computing device 600 is locked and requires authentication to access a main menu or perform other operations.
- the multiple suggested actions 604 , 606 , 608 can be displayed together during a second time interval after the context data is collected during a first time interval. More specifically, at least one additional suggested action 606 , 608 can be displayed in the user interface with the suggested action 604 .
- the additional suggested action(s) 606 , 608 can be distinct from the suggested action 604 .
- the additional suggested action(s) 606 , 608 can also be generated and stored (e.g., during the first time interval) by the artificial intelligence system based on distinct semantic entities. For example, the artificial intelligence system can obtain additional context data that is distinct from the context data and input additional model input(s) that include the additional context data into the machine-learned model(s).
- the artificial intelligence system can receive, as an additional output of the machine-learned model(s), data descriptive of the additional suggested action(s) that is described by the additional suggested action(s), for example as described above with reference to FIGS. 2 and 3 .
- the artificial intelligence system can leverage the machine-learned model(s) to store multiple semantic entities over the first time interval (e.g., as the user goes about their day) and then display multiple suggested actions during the second time interval (e.g., at the end of the day).
- buttons can also be displayed in the panel 602 for the user to control or manipulate the suggested actions 604 , 606 , 608 and/or adjust settings of the artificial intelligence system.
- a settings icon 610 can be displayed in the panel 602 .
- the computing system can display a settings panel, for example as described below with respect FIG. 11 . The user can adjust the settings of the artificial intelligence system using the setting panel.
- a search icon 612 can be displayed in the panel 602 .
- the user can search through suggested actions that are not currently displayed in the panel 602 .
- a “view all” button 614 can be displayed in the panel 602 .
- the user can view additional suggested actions that are not currently displayed in the panel 602 .
- FIG. 7 depicts computing device 700 displaying an example notification panel 702 displaying a suggested action 704 with notifications 706 , 708 .
- the notification panel 702 can be displayed automatically or in response to user input requesting that the notification panel 702 be displayed.
- FIG. 8 depicts a computing device 800 in a first state in which a plurality of category labels 802 , 804 , 806 , 808 , 810 , 812 , 814 corresponding with categorized suggested actions are displayed according to aspects of the present disclosure.
- a plurality of suggested actions 816 , 818 can be displayed in a panel 820 with the plurality of category labels 802 , 804 , 806 , 808 , 810 , 812 .
- the artificial intelligence system can categorize the suggested actions 816 , 818 , 822 , 824 with respect to a plurality of categories corresponding with the plurality of category labels 802 , 804 , 806 , 808 , 810 , 812 , 814 .
- Category labels 802 , 804 , 806 , 808 , 810 , 812 , 814 can be displayed in the user interface.
- the category labels 802 , 804 , 806 , 808 , 810 , 812 , 814 can describe two of more of the plurality of categories such that the user can navigate between the categories of suggested actions 816 , 818 , 822 , 824 using the category labels 802 , 804 , 806 , 808 , 810 , 812 , 814 (e.g., in separate panels or pages).
- the computing system can detect a user touch action with respect to one category label 814 and display suggested actions that correspond with the selected category label 814 , for example as described below with FIG. 9 .
- FIG. 9 depicts the computing device 800 of FIG. 8 in which one category label 814 of the plurality of category labels has been selected.
- Suggested actions 822 , 824 corresponding with the selected category label 814 are displayed according to aspects of the present disclosure. More specifically, in response to detecting the user touch action, the computing system can display the suggested actions 822 , 824 that were categorized with respect to the selected category label 814 .
- the artificial intelligence system can categorize the suggested actions 816 , 818 , 822 , 824 and provide an intuitive way for the user to navigate between the categories of suggested actions 816 , 818 , 822 , 824 for selection by the user.
- the artificial intelligence system can rank, sort, categorize, and prioritize, etc. the suggested actions 816 , 818 , 822 , 824 for example, based on the user data.
- the user data can include user preferences, calendar data (e.g., the user's plans or schedule), and/or other information about the user or the computing device.
- the artificial intelligence system can rank the suggested actions 816 , 818 , 822 , 824 and arrange the suggested actions 816 , 818 , 822 , 824 within the user interface based on the ranking.
- the artificial intelligence system can prioritize the suggested actions 816 , 818 , 822 , 824 and selectively display a group of the most important and/or relevant suggested actions 816 , 818 , 822 , 824 to the user in the user interface.
- FIG. 10 depicts a computing system 1000 displaying a suggested action 1002 in which a semantic entity 1004 has been selected and a search is being performed in an overlaid panel 1006 in response to the semantic entity 1004 being selected according to aspects of the present disclosure.
- FIG. 11 depicts a computing system 1100 displaying a settings panel 1102 in which the user can select a default computer application 1104 for a type of suggested action (e.g., a suggested action including shopping).
- the settings panel 1102 can allow the user to adjust other settings associated with the artificial intelligence system.
- FIG. 12 depicts a flow chart diagram of an example method 1200 for identifying information of interest, storing the information, and providing suggested actions to users of computing systems at a later time based on the stored information.
- FIG. 12 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of the method 1200 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.
- a computing system can obtain the context data during a first time interval.
- the artificial intelligence system can capture information of interest as the computing device is used to perform tasks throughout the day during the first time interval. For instance, the artificial intelligence system can identify and store semantic entities while the user navigates between various computer applications and/or switches between different tasks or activities.
- the computing system can input the model input that includes the context data into the one or more machine-learned models, for example as described above with reference to FIGS. 2 and 3 .
- the computing system can receive, as an output of the one or more machine-learned models, the model output that describes the one or more semantic entities referenced by the context data, for example as described above with reference to FIGS. 2 and 3 .
- the computing system can store the model output in a tangible, non-transitory computer-readable medium.
- the computing system can provide, for display in a user interface during a second time interval that is after the first time interval, a suggested action with respect to the one or more semantic entities described by the model output.
- the computing system can display the suggested action at a time that is convenient for the user to review (e.g., after work, after dinner, at regularly scheduled time intervals etc.).
- the first time interval can be defined as a duration of a call associated with a particular business.
- the context data can include a payment date, payment amount, or other information that was discussed during the call.
- the suggested action can be displayed in response to an event (e.g., the second time interval can begin in response to the event).
- the computing system can provide a suggested action that includes scheduling a calendar event to the user at a certain time interval prior to the event (e.g., 7 days).
- the duration between obtaining the context data and providing the suggested action can be learned based on a user interaction (e.g., with prior suggested actions, with the application associated with suggested action to be provided, etc.).
- a sale or lowering of a price of an item could cause the computing system to provide a suggested action with respect to the item based on context data that includes the user interacting with the item at an earlier time.
- the suggested action(s) can be provided several hours, days, weeks, or even months after the context data was obtained.
- the suggested action(s) can be provided at least 4 hours (or 8 hours, or longer) after the context data was obtained such that the original event that prompted obtaining the context data may not be as fresh in the mind of the user.
- the suggested action may be more useful as a “reminder” to the user.
- the technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems.
- the inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components.
- processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination.
- Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
Abstract
Description
- The present disclosure relates generally to artificial intelligence systems. More particularly, the present disclosure relates to systems and methods for generating and providing suggested actions to a user of a computing device.
- Artificial intelligence and machine learning has been used to assist users of computing devices, for example by providing artificial intelligence agents and personal assistants. Such artificial intelligence agents and personal assistants, however, lack the ability to proactively assist users with remembering actions or items.
- Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or can be learned from the description, or can be learned through practice of the embodiments.
- One aspect of the present disclosure is directed to a computing system including at least one processor and an artificial intelligence system including one or more machine-learned models. The one or more machine-learned models can be configured to receive a model input that includes context data, and, in response to receipt of the model input, output a model output that describes one or more semantic entities referenced by the context data. The computing system can include at least one tangible, non-transitory computer-readable medium that stores instructions that, when executed by the at least one processor, cause the at least one processor to perform operations. The operations can include obtaining the context data during a first time interval; inputting the model input that comprises the context data into the one or more machine-learned models; receiving, as an output of the one or more machine-learned models, the model output that describes the one or more semantic entities referenced by the context data; storing the model output in the at least one tangible, non-transitory computer-readable medium; and providing, for display in a user interface during a second time interval that is after the first time interval, a suggested action with respect to the one or more semantic entities described by the model output.
- Another aspects of the present disclosure to directed to a computer-implemented method for generating and providing suggested actions. The method can include obtaining, by one or more computing devices, context data during a first time interval. The method can include inputting, by the one or more computing devices, a model input that includes the context data into one or more machine-learned models that are configured to receive the model input that comprises context data, and, in response to receipt of the model input, output a model output that describes one or more semantic entities referenced by the context data. The method can include receiving, by the one or more computing devices, as an output of the one or more machine-learned models, the model output that describes the one or more semantic entities referenced by the context data. The method can include storing, by the one or more computing devices, the model output in at least one tangible, non-transitory computer-readable medium. The method can include providing, by the one or more computing devices for display in a user interface of the one or more computing devices during a second time interval that is after the first time interval, a suggested action with respect to the one or more semantic entities described by the model output.
- Other aspects of the present disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
- These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate example embodiments of the present disclosure and, together with the description, serve to explain the related principles.
- Detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended figures, in which:
-
FIG. 1A depicts a block diagram of an example computing system for generating and providing suggested actions to users of computing systems according to example embodiments of the present disclosure. -
FIG. 1B depicts a block diagram an example computing system for generating and providing suggested actions to users of computing systems according to example embodiments of the present disclosure. -
FIG. 1C depicts a block diagram of an example computing system for generating and providing suggested actions to users of computing systems according to example embodiments of the present disclosure. -
FIG. 2 depicts an example artificial intelligence system for generating and providing suggested actions according to example embodiments of the present disclosure. -
FIG. 3 depicts an example computing system for generating and providing suggested actions according to example embodiments of the present disclosure including one or more computer applications. -
FIG. 4 depicts an example suggested action according to aspects of the present disclosure. -
FIGS. 5A, 5B, and 5C depict additional example suggested actions according to aspects of the present disclosure. -
FIG. 6 depicts an example panel including multiple suggested actions being displayed in a lock screen of a computing device according to aspects of the present disclosure. -
FIG. 7 depicts computing device displaying an example notification panel displaying a suggested actions with notifications. -
FIG. 8 depicts a computing device in a first state in which a plurality of category labels corresponding with categorized suggested actions are displayed according to aspects of the present disclosure. -
FIG. 9 depicts the computing device ofFIG. 8 in which one category label of the plurality of category labels has been selected and suggested actions corresponding with the selected category label are displayed according to aspects of the present disclosure. -
FIG. 10 depicts a suggested action in which the semantic entity has been selected and search is being performed in response to the semantic entity being selected according to aspects of the present disclosure. -
FIG. 11 depicts a computing system displaying a settings panel in which the user can select a default computer application for a type of suggested action. -
FIG. 12 depicts a flowchart of a method for generating and providing suggested actions to users of computing systems according to aspects of the present disclosure. - Generally, the present disclosure is directed to an artificial intelligence system for identifying information of interest, storing the information, and providing suggested actions to users of computing systems at a later time based on the stored information. The artificial intelligence system can be configured to intelligently process information on behalf of the user, including, for example, visual and/or audio information that is displayed, played, and/or otherwise processed or detected by the computing device. In other words, the artificial intelligence system can capture information of interest as the computing device is used to perform tasks throughout the day. For example, the artificial intelligence system can identify and store semantic entities while the user navigates between various computer applications and/or switches between different tasks or activities. Alternatively, the artificial intelligence system can identify and store semantic entities referenced or included in a surrounding environment of the user (e.g., through analysis of captured imagery, audio, or other data regarding the surrounding environment. Thus, the artificial intelligence system can capture and process information (e.g., to identify semantic entities) that is actively identified or emphasized by a user while in other instances the artificial intelligence system can capture and process information (e.g., to identify semantic entities) that is simply referenced by or included in the ambient environment of the user (e.g., information that is contained in the surrounding environment but not specifically actively identified or emphasized by the user).
- The artificial intelligence system can save or otherwise retain data associated with semantic entities as they are recognized over time. For example, the saved semantic entities can be ranked, sorted, categorized, prioritized etc. based on the user's preferences and/or the user's plans or schedule. As another example, the artificial intelligence system can generate one or more suggested actions for a user that are related to one or more of the identified semantic entities. For example, the suggested actions can include actions that can be taken by the artificial intelligence system and/or a computer application under direction of the artificial intelligence system for and/or on behalf of the user relative to the identified semantic entities. As examples, the suggested actions can include communications actions (e.g., emailing a certain contact), information retrieval actions (e.g., retrieving options to purchase or shop for a certain item, providing an opportunity to listen to a certain song, accessing geographic information such as the location of a point of interest), a booking action (e.g., requesting a ride share vehicle or purchasing a flight ticket), information storage (e.g., note-taking or inserting an item into a user's calendar), and/or many other suggested actions.
- At a later time, the saved suggested actions can be provided for display. For example, the suggested actions can be accessed by the user via a specific menu, can be provided in a notifications menu, can be automatically surfaced at later, contextually relevant times, and/or can be accessed in other manners. The suggested actions can include links or buttons to perform the suggested actions (e.g., with computer applications). The user can also optionally provide feedback and/or instructions to the artificial intelligence system to customize how the artificial intelligence system captures information and/or suggests actions. Alternatively, the artificial intelligence system can also learn the user's preferences based on how the user interacts with the suggested actions.
- Importantly, the user can be provided with controls allowing the user to make an election as to both if and when systems, programs, or features described herein may enable collection of user information (e.g., ambient audio, text presented in the user interface, etc.). In addition, certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, a user's identity may be treated so that no personally identifiable information can be determined for the user. Thus, the user may have control over what information is collected about the user, how that information is used, and what information is provided to the user.
- Aspects of the present disclosure are directed to an artificial intelligence system that operates over multiple different time periods to provide suggested actions that are contextually meaningful. In particular, the artificial intelligence system can identify the semantic entities over a first time interval, store the semantic entities, and then display the suggested actions during a second time interval that is after the first time interval. For example, the suggested actions can act as reminders for the user during the second time interval to complete a task that the user started earlier. In this manner, aggregating relevant information over a first time interval and then providing multiple suggested actions based on the information during a second, later time interval can be less disruptive to the user. This can also be more useful to the user as the user is more likely to have forgotten about the task after some time has passed. Thus, by storing suggested and contextually-derived actions for later use, the artificial intelligence system can operate as a intelligent memory assistant which assist the user in remembering actions which they may want to take based on activities they engaged in earlier that day, week, month, etc.
- As one example, the user can take a “screenshot” of an item while shopping in a first time interval. In response to this user action, the artificial intelligence system can generate and store a name or description of the item. Later, the user can suggest a specific meeting time and day during a phone call. The artificial intelligence system can generate and store a second semantic entity that can include the name of the person, the meeting time, etc. During the second time interval (e.g., after work, after dinner, etc.) the artificial intelligence system can display suggested actions with respect to the each of the item from the screenshot and the suggested meeting. The suggested action for the item can include purchasing the item displayed in the screenshot, and the suggested action for the meeting can include creating a calendar event based on the information gathered during the phone call.
- In some implementations, the artificial intelligence system can generate the suggested action(s) based on a template (e.g., a predefined template). Utilizing a template can reduce the computing resources required to generate such suggested actions. Instead of training and utilizing a machine-learned model to generate the complete suggested action, keywords of the suggested action can be generated using the machine-learned model(s) and then assembled according to the template to generate the suggested action. For example, the template can include a verb, a semantic entity described by a model output of the machine-learned model, and a computer application, for example as follows:
-
[Verb]+[Semantic Entity]+[Computer Application] - The artificial intelligence system can select an appropriate verb and computer application to be inserted into the respective placeholders of the template. One example of a suggested action generated based on the above template is “Add Appt. with Dr. Sherrigan to Calendar.” It should be understood that a variety of templates can be employed. Furthermore, the artificial intelligence system can learn the user's preferences with respect to which templates to use and/or whether to use templates. Thus, the systems and methods described herein can employ one or more templates to generate the suggested actions.
- In some implementations, the same template or template(s) can be used to generate multiple suggested actions such that the multiple suggested actions have the same general look and feel to the user. As such, the user can quickly evaluate the suggested actions because they are presented in a known and predictable format to the user. Thus, the templates as described herein can facilitate greater use of the suggested actions.
- The systems and methods herein can leverage one or more machine-learned models according to aspects of the present disclosure. More specifically, the artificial intelligence system can include one or more machine-learned models that are configured to receive a model input that includes context data (e.g., ambient audio, information displayed in a screen of the computing device, etc.). The computing system can be configured to obtain the context data and input the model input that includes the context data into the machine-learned model. The computing system can receive, as an output of the machine-learned model, model output that describes one or more semantic entities referenced by the context data. The computing system can store the model output in at least one tangible, non-transitory computer-readable medium. The computing system can provide a suggested action with respect to the one or more semantic entity.
- The context data discussed herein can include a variety of information, such as information currently displayed in the user interface, information previously displayed in the user interface, information gleaned from the user's previous actions (e.g., text written or read by the user, content viewed by the user, etc.), and/or the like. The context data can include user data that describes a preference or other information associated with the user and/or contact data that describes preferences or other information associated with a contact of the user. Example context data can include a message received by the computing system for the user, the user's previous interactions with one or more of the user's contacts (e.g., a text message mentioning a user preference for a restaurant or type of food), previous interactions associated with a location (e.g., going to a park, museum, other attraction, etc.), a business, etc. (e.g., posting a review for a restaurant, reading a menu of a restaurant, reserving a table at a restaurant, etc.), and/or any other suitable information about the user's preferences or user. Further examples include audio played or processed by the computing system, audio detected by the computing system, information about the user's location (e.g., a location of a mobile computing device of the computing system), and/or calendar data. For instance, context data can include ambient audio detected by a microphone of the computing system and/or phone audio processed during a phone call. Calendar data can describe future events or plans (e.g., flights, hotel reservations, dinner plans etc.). Example semantic entities that can be described by the model output can include words or phrases recognized in the text and/or audio. Additional examples can includes information about the user's location, such as a city name, state name, street name, names of nearby attractions, and the like.
- In some implementations, multiple suggested actions can be displayed together in the second time interval after the context data is collected during the first time interval. More specifically, at least one additional suggested action can be displayed in the user interface with the suggested action. The additional suggested action(s) can be distinct from the suggested action. The additional suggested action(s) can also be generated and stored (e.g., during the first time) by the artificial intelligence system based on distinct semantic entities. For example, the artificial intelligence system can obtain additional context data that is distinct from the context data and input additional model input(s) that include the additional context data into the machine-learned model(s). The artificial intelligence system can receive, as an additional output of the machine-learned model(s), data descriptive of the additional suggested action(s) that is described by the additional suggested action(s). Thus, the artificial intelligence system can leverage the machine-learned model(s) to store multiple semantic entities over the first time interval and then display multiple suggested actions during the second time interval.
- In some implementations, the artificial intelligence system can rank, sort, categorize, and prioritize, etc. the suggested actions, for example, based on the user data. The user data can include user preferences, calendar data (e.g., the user's plans or schedule), and/or other information about the user or the computing device. For example, the artificial intelligence system can rank the suggested actions and arrange the suggested actions within the user interface based on the ranking. Thus, the artificial intelligence system can prioritize the suggested actions and selectively display a group of the most important and/or relevant suggested actions to the user in the user interface.
- In some implementations, the artificial intelligence system can categorize the suggested actions with respect to a plurality of categories. Category labels can be displayed in the user interface corresponding with the categories such that the user can navigate between the categories of suggested actions using the category labels (e.g., in separate panels or pages). For example, the computing system can detect a user touch action with respect to one category label. In response to detecting the user touch action, the computing system can display suggested actions that were categorized with respect to the selected suggested action. Thus, the artificial intelligence system can categorize the suggested actions and provide an intuitive way for the user to navigate between the categories of suggested actions for selection by the user.
- In some implementations, the computing system can display explanations with respect to the suggested actions. The explanations can describe information about obtaining the context data, including, as examples, a time when the context data was obtained, a location of the computing device when the context data was obtained, and/or a source of the context data. As an example, the explanation can indicate that the suggested action was generated based on audio from a phone conversation with a specific user contact that occurred at a specific time. As another example, the explanation can indicate that the suggested action was generated based on a user's shopping session with a specific shopping application at a specific time. The source(s) of the data can include a computer application that was being displayed when the context data was obtained; whether the context data was obtained from ambient audio, text, graphical information, etc.; and/or any other information associated with obtaining the context data. Such explanations can provide the user with a better understanding of the operations of the artificial intelligence system. As a result, the user may be more comfortable with or trusting of the artificial intelligence system operations, which can make the artificial intelligence system more useful.
- In some implementations, the computing system can provide the user with a way to view additional information associated with obtaining the context data in addition to the explanation(s). For example, the computing system can detect a user touch input that requests additional information with respect to the explanation. In response to detecting the user touch input, the computing system can display additional explanation information about obtaining the context data. The additional explanation information can include a time when the context data was obtained or a location of the computing device when the context data was obtained. The additional explanation information can include a source of the context data (if not already displayed). The additional information can include information about other times that context data was obtained in a similar way (e.g., from the same source, at similar times, etc.).
- In some implementations, the computing system can provide the user with a way to adjust preferences with respect to how the artificial intelligence system collects context data. The additional explanation information can also include preferences and/or rules with respect to when and how the artificial intelligence system can obtain the context data. The user can adjust the rules and/or preferences within the user interface.
- The computing system can display the suggested action(s) automatically or in response to a user request and in a variety of locations. For example, a panel displaying the suggested action(s) can be displayed in a “lock screen” or “home screen” of the computing device. The panel can be accessible at a system level from a drop down panel, navigation bar, or the like. The panel can be automatically displayed at one or more regular times throughout the day. In other implementations, the artificial intelligence system can intelligently choose when to display the panel based on the user data (e.g., preferences) and/or context data. The artificial intelligence system can display the panel when the suggested actions would be most relevant to the user based on the content of the suggested actions.
- In some implementations, the computing system can be configured to interface with one or more computer applications to provide suggested actions that can be performed with the computer application(s). The computing system can provide data descriptive of the model output of the machine-learned model(s) that describes the suggested action(s) to the computer application(s). The computing system can receive one or more application outputs respectively from the computing application(s) via a pre-defined application programming interface. The suggested action can describe at least one of the application output(s).
- In some implementations, the user can select a portion of the suggested action (e.g., the semantic entity) to perform another action with respect to the selected portion of the suggested action that is distinct from the suggested action. As an example, in user response to detecting a user touch action directed towards the semantic entity of the suggested action, the computing system can display a panel including a search (e.g., a web search) of the semantic entity. Additional examples include editing the semantic entity, changing a computer application of the suggested action, editing details of the suggested action, and so forth. For instance, a suggested action can include shopping for a specific brand of a produce (e.g., grill) using a specific shopping application. The user can select the semantic entity (e.g., “Webster Classic Grill”) and manually edit the entity, for example to change the brand name or type of product. The user can change “Webster Classic Grill” to “Webster Classic Grill Cover” before selecting the suggested action to purchase or shop for the item using the shopping application. As another example, the user can change the shopping application.
- As one example, the systems and methods of the present disclosure can be included or otherwise employed within the context of an application, a browser plug-in, or in other contexts. Thus, in some implementations, the models of the present disclosure can be included in or otherwise stored and implemented by a user computing device such as a laptop, tablet, or smartphone. As yet another example, the models can be included in or otherwise stored and implemented by a server computing device that communicates with the user computing device according to a client-server relationship. For example, the models can be implemented by the server computing device as a portion of a web service (e.g., a web email service).
- For example, the systems and methods of the present disclosure may operate at the operating system level rather than that of one or more particular applications that require user selection to initiate their operations. For example, context data may be automatically acquired from one or more sources of the system, such as one or more of microphone(s), camera(s), webpages visited, location(s) of the system, and orientation(s) of the system during normal use of the system and without the user necessarily opening a particular application. The context data may be acquired so long as the system is switched on and/or when an appropriate passcode, password and/or biometric identifier is verified. The user is not required therefore to remember to initiate a particular function to acquire context data. The context data may automatically be stored on a system-level “clipboard.” The user may disable certain sources of the context data if desired.
- For example, the systems and methods of the present disclosure may limit the amount of data that is stored on memory and/or which is allocated to such systems and methods. For example, the systems and methods may automatically remove or overwrite acquired context data and/or suggested actions based on one or more rules. For example, context data that is older than a predetermined period, e.g. which is one day or one week old, may be removed or overwritten automatically. The rules, e.g. the predetermined period, may be user configurable. The predetermined period may be updated dynamically for particular context data and/or suggested actions based on prior user interactions. For example, prior user interactions may be user selections in relation to suggested actions. For example, if a user selects to create calendar appointments from suggested actions more frequently than opening a shopping application from suggested actions, the context data which is linked to shopping actions may be deleted or overwritten sooner than those for calendar appointments. A maximum limit may be placed on the amount of data that is stored so as to avoid taking away storage resources of application data. Where suggested actions are ranked in a priority order, only the top N suggested actions may be preserved for output.
- For example, the systems and methods of the present disclosure may provide one or more suggested actions linked or associated with one or more applications, with at least one selectable button or the like being displayed alongside or otherwise with the suggested action to permit a single or reduced number of taps, touches or swipe gestures to effect the suggested action. Here, the number of physical user interactions to effect a suggested action, such as adding an appointment to a calendar event, playing a music or video track, opening a shopping website, etc. may require fewer user interactions with the user interface and/or use less electrical energy and processing resources than opening a particular application and making selections and/or entering data manually. For example, opening a shopping application or website for purchasing a particular product may be achieved using a single gesture, rather than opening the relevant shopping application or browser window, entering a search term and then selecting an item from a list of suggestions.
- For example, the suggested action may be displayed in an associated portion of the user interface and, as well as having a selectable button or the like associated with initiation of the suggested action, one or more further buttons or the like may be presented in the same portion for single-touch initiation of related functions which ordinarily might require the application to be opened. For example, in relation to a suggested action that involves playing audio or video, as well as a suggested action to open a music or video application to play the audio or video, one or more further buttons or the like, associated with that application, might be presented, such as a “like” button or a “share” button, selection of which causes the associated action to be performed. For example, in relation to a suggested action to create a calendar appointment, if it is detected that the proposed appointment clashes with an existing one, one or more further buttons or the like may be provided to initiate cancelling and/or rescheduling the existing appointment.
- Where the artificial intelligence system intelligently chooses when to display the panel based on the user data (e.g., user preferences) and/or context data, the artificial intelligence system can display the panel when the suggested actions would be most relevant, or less disturbing or intrusive to the user, based on the content of the suggested actions. For example, if the suggested actions comprise playing audio or video, then the artificial intelligence system may choose not to display such suggested actions during working hours, whereas “silent” suggested actions such as suggested appointments or suggested shopping actions may be displayed at said times. As mentioned, aggregating suggested actions until a later, second time interval, avoids overly disturbing the user and/or being intrusive.
- For example, the systems and methods of the present disclosure may operate at the operating system level rather than that of one or more particular applications that require user selection to initiate their operations. For example, context data may be automatically acquired from one or more sources of the system, such as one or more of microphone(s), camera(s), webpages visited, location(s) of the system, and orientation(s) of the system during normal use of the system and without the user necessarily opening a particular application. The context data may be acquired so long as the system is switched on and/or when an appropriate passcode, password and/or biometric identifier is verified. The user is not required therefore to remember to initiate a particular function to acquire context data. The context data may automatically be stored on a system-level “clipboard.” However, the user may disable certain sources of the context data if desired.
- For example, the systems and methods of the present disclosure may limit the amount of data that is stored on memory and/or which is allocated to such systems and methods. For example, the systems and methods may automatically remove or overwrite acquired context data and/or suggested actions based on one or more rules. For example, context data that is older than a predetermined period, e.g. which is one day or one week old, may be removed or overwritten automatically. The rules, e.g. the predetermined period, may be user configurable. The predetermined period may be updated dynamically for particular context data and/or suggested actions based on prior user interactions. For example, prior user interactions may include user selections in relation to suggested actions. For example, if a user selects to create calendar appointments from suggested actions more frequently than opening a shopping application from suggested actions, the context data which is linked to shopping actions may be deleted or overwritten sooner than those for calendar appointments. A maximum limit may be placed on the amount of data that is stored so as to avoid taking away storage resources of application data. Where suggested actions are ranked in a priority order, only the top N suggested actions may be preserved for output.
- For example, the systems and methods of the present disclosure may provide one or more suggested actions linked or associated with one or more applications, with at least one selectable button or the like being displayed alongside or otherwise with the suggested action to permit a single or reduced number of taps, touches or swipe gestures to effect the suggested action. Here, the number of physical user interactions to effect a suggested action, such as adding an appointment to a calendar event, playing a music or video track, opening a shopping website, etc. may require fewer user interactions with the user interface and/or use less electrical energy and processing resources than opening a particular application and making selections and/or entering data manually. For example, opening a shopping application or website for purchasing a particular product may be achieved using a single gesture, rather than opening the relevant shopping application or browser window, entering a search term and then selecting an item from a list of suggestions.
- For example, the suggested action may be displayed in an associated portion of the user interface and, as well as having a selectable button or the like associated with initiation of the suggested action, one or more further buttons or the like may be presented in the same portion for single-touch initiation of related functions which ordinarily might require the application to be manually opened by the user. For example, in relation to a suggested action that involves playing audio or video, as well as a suggested action to open a music or video application to play the audio or video, one or more further buttons or the like, associated with that application, might be presented, such as a “like” button or a “share” button. The selection of such a button can cause the associated action to be performed. For example, in relation to a suggested action to create a calendar appointment, if it is detected that the proposed appointment clashes with an existing one, one or more further buttons, or the like, may be provided to initiate cancelling and/or rescheduling the existing appointment.
- Where the artificial intelligence system intelligently chooses when to display the panel based on the user data (e.g., preferences) and/or context data, the artificial intelligence system can display the panel when the suggested actions would be most relevant, or less disturbing or intrusive to the user, based on the content of the suggested actions. For example, if the suggested actions comprise playing audio or video, then the artificial intelligence system may choose not to display such suggested actions during working hours, whereas “silent” suggested actions such as suggested appointments or suggested shopping actions may be displayed at said times. As mentioned, aggregating suggested actions until a later, second time interval, avoids overly disturbing the user and/or being intrusive.
- With reference now to the Figures, example embodiments of the present disclosure will be discussed in further detail.
-
FIG. 1A depicts a block diagram of anexample computing system 100 for generating and providing suggested actions according to example embodiments of the present disclosure. Thesystem 100 includes auser computing device 102, aserver computing system 130, and atraining computing system 150 that are communicatively coupled over anetwork 180. - The
user computing device 102 can be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device. - The
user computing device 102 includes one ormore processors 112 andmemory 114. The one ormore processors 112 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. Thememory 114 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. Thememory 114 can storedata 116 andinstructions 118 which are executed by theprocessor 112 to cause theuser computing device 102 to perform operations. - The
user computing device 102 can store or include one ormore computer applications 119. The computer application(s) 119 can be configured to perform various operations and provide application output(s) as described herein. - The
user computing device 102 can store or include anartificial intelligence system 120. Theartificial intelligence system 120 can perform some or all of the operations described herein. Theartificial intelligence system 120 can be separate and distinct from the one ormore computer applications 119 but can be capable of communicating with the one ormore computer applications 119. - The
user computing device 102 can store or include one or more machine-learnedmodels 122. For example, the machine-learnedmodels 122 can be or can otherwise include various machine-learned models such as neural networks (e.g., deep neural networks) or other multi-layer non-linear models. Neural networks can include recurrent neural networks (e.g., long short-term memory recurrent neural networks), feed-forward neural networks, or other forms of neural networks. Example machine-learnedmodels 122 are discussed with reference toFIGS. 2 and 3 . - In some implementations, the one or more machine-learned
models 122 can be received from theserver computing system 130 overnetwork 180, stored in the usercomputing device memory 114, and the used or otherwise implemented by the one ormore processors 112. In some implementations, theuser computing device 102 can implement multiple parallel instances of a single machine-learned model 122 (e.g., to perform parallel operations across multiple instances of the machine-learned 120). - Additionally or alternatively, an artificial intelligence system 140 can be included in or otherwise stored and implemented by the
server computing system 130 that communicates with theuser computing device 102 according to a client-server relationship. For example, the artificial intelligence system 140 can include a machine-learnedmodel 142. For example, the machine-learnedmodels 142 can be implemented by the server computing system 140 as a portion of a web-based service. Thus, one ormore models 122 can be stored and implemented at theuser computing device 102 and/or one ormore models 142 can be stored and implemented at theserver computing system 130. - The
user computing device 102 can also include one or moreuser input component 124 that receives user input. For example, theuser input component 124 can be a touch-sensitive component (e.g., a touch-sensitive display screen or a touch pad) that is sensitive to the touch of a user input object (e.g., a finger or a stylus). The touch-sensitive component can serve to implement a virtual keyboard. Other example user input components include a microphone, a traditional keyboard, or other means by which a user can enter a communication. - The
server computing system 130 includes one ormore processors 132 and amemory 134. The one ormore processors 132 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. Thememory 134 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. Thememory 134 can storedata 136 andinstructions 138 which are executed by theprocessor 132 to cause theserver computing system 130 to perform operations. - In some implementations, the
server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances in which theserver computing system 130 includes plural server computing devices, such server computing devices can operate according to sequential computing architectures, parallel computing architectures, or some combination thereof. - As described above, the
server computing system 130 can store or otherwise includes one or more machine-learnedmodels 142. For example, themodels 142 can be or can otherwise include various machine-learned models such as neural networks (e.g., deep recurrent neural networks) or other multi-layer non-linear models.Example models 142 are discussed with reference toFIGS. 2 and 3 . - The
server computing system 130 can train themodels 142 via interaction with thetraining computing system 150 that is communicatively coupled over thenetwork 180. Thetraining computing system 150 can be separate from theserver computing system 130 or can be a portion of theserver computing system 130. - The
training computing system 150 includes one ormore processors 152 and amemory 154. The one ormore processors 152 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. Thememory 154 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. Thememory 154 can storedata 156 andinstructions 158 which are executed by theprocessor 152 to cause thetraining computing system 150 to perform operations. In some implementations, thetraining computing system 150 includes or is otherwise implemented by one or more server computing devices. - The
training computing system 150 can include a model trainer 160 that trains the machine-learnedmodels 142 stored at theserver computing system 130 using various training or learning techniques, such as, for example, backwards propagation of errors. In some implementations, performing backwards propagation of errors can include performing truncated backpropagation through time. The model trainer 160 can perform a number of generalization techniques (e.g., weight decays, dropouts, etc.) to improve the generalization capability of the models being trained. - In some implementations, if the user has provided consent, the
training data 162 can be obtained from the user computing device 102 (e.g., based on communications previously provided by the user of the user computing device 102). Thus, in such implementations, themodel 122 provided to theuser computing device 102 can be trained by thetraining computing system 150 on user-specific communication data received from theuser computing device 102. In some instances, this process can be referred to as personalizing the model. - The model trainer 160 includes computer logic utilized to provide desired functionality. The model trainer 160 can be implemented in hardware, firmware, and/or software controlling a general purpose processor. For example, in some implementations, the model trainer 160 includes program files stored on a storage device, loaded into a memory and executed by one or more processors. In other implementations, the model trainer 160 includes one or more sets of computer-executable instructions that are stored in a tangible computer-readable storage medium such as RAM hard disk or optical or magnetic media.
- The
network 180 can be any type of communications network, such as a local area network (e.g., intranet), wide area network (e.g., Internet), or some combination thereof and can include any number of wired or wireless links. In general, communication over thenetwork 180 can be carried via any type of wired and/or wireless connection, using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL). -
FIG. 1A illustrates one example computing system that can be used to implement the present disclosure. Other computing systems can be used as well. For example, in some implementations, theuser computing device 102 can include the model trainer 160 and thetraining dataset 162. In such implementations, themodels 122 can be both trained and used locally at theuser computing device 102. In some of such implementations, theuser computing device 102 can implement the model trainer 160 to personalize themodels 122 based on user-specific data. -
FIG. 1B depicts a block diagram of anexample computing device 10 that can be used to implement the present disclosure. Thecomputing device 10 can be a user computing device or a server computing device. - The
computing device 10 includes a number of applications (e.g.,applications 1 through N). Each application contains its own machine learning library and machine-learned model(s). For example, each application can include a machine-learned model. Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc. - As illustrated in
FIG. 1B , each application can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., a public API). In some implementations, the API used by each application is specific to that application. -
FIG. 1C depicts a block diagram of anexample computing device 50 that performs according to example embodiments of the present disclosure. Thecomputing device 50 can be a user computing device or a server computing device. - The
computing device 50 includes a number of applications (e.g.,applications 1 through N). Each application is in communication with a central intelligence layer. Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc. In some implementations, each application can communicate with the central intelligence layer (and model(s) stored therein) using an API (e.g., a common API across all applications). - The central intelligence layer includes a number of machine-learned models. For example, as illustrated in
FIG. 1C , a respective machine-learned model (e.g., a model) can be provided for each application and managed by the central intelligence layer. In other implementations, two or more applications can share a single machine-learned model. For example, in some implementations, the central intelligence layer can provide a single model (e.g., a single model) for all of the applications. In some implementations, the central intelligence layer is included within or otherwise implemented by an operating system of thecomputing device 50. - The central intelligence layer can communicate with a central device data layer. The central device data layer can be a centralized repository of data for the
computing device 50. As illustrated inFIG. 1C , the central device data layer can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a private API). -
FIG. 2 depicts a block diagram of an exampleartificial intelligence system 200 according to example embodiments of the present disclosure. Theartificial intelligence system 200 can include one or more machine-learned model(s) 202 that are trained to receivecontext data 204 and, as a result of receipt of thecontext data 204, provide amodel output 206 that describes one or more semantic entities referenced by thecontext data 204. - The
context data 204 discussed herein can include a variety of information, such as information currently displayed in the user interface, information previously displayed in the user interface, information gleaned from the user's previous actions (e.g., text written or read by the user, content viewed by the user, etc.), and/or the like. Thecontext data 204 can include user data that describes a preference or other information associated with the user and/or contact data that describes preferences or other information associated with a contact of the user.Example context data 204 can include a message received by the computing system for the user, the user's previous interactions with one or more of the user's contacts (e.g., a text message mentioning a user preference for a restaurant or type of food), previous interactions associated with a location (e.g., going to a park, museum, other attraction, etc.), a business, etc. (e.g., posting a review for a restaurant, reading a menu of a restaurant, reserving a table at a restaurant, etc.), and/or any other suitable information about the user's preferences or user. Further examples include audio played or processed by the computing system, audio detected by the computing system, information about the user's location (e.g., a location of a mobile computing device of the computing system), and/or calendar data. For instance, thecontext data 204 can include ambient audio detected by a microphone of the computing system and/or phone audio processed during a phone call. Calendar data can describe future events or plans (e.g., flights, hotel reservations, dinner plans etc.). Example semantic entities that can be described by themodel output 206 can include words or phrases recognized in the text and/or audio. Additional examples can includes information about the user's location, such as a city name, state name, street name, names of nearby attractions, and the like. - In some implementations, the
model output 206 can more directly describe suggested actions. For example, the machine-learned model(s) 202 can be trained to output data that describes text of suggested actions (e.g., including a verb, application, and the semantic entity), for example as described with reference toFIGS. 4 through 9 . A single machine-learnedmodel 202 can be trained to receive thecontext data 204 and outputsuch model output 206. - In some implementations, multiple machine-learned model(s) 202 can be trained (e.g., end-to-end) to produce
such model output 206. For instance, a first model of the machine-learnedmodels 202 can output data that describes a semantic entity included in thecontext data 204. A second model of the machine-learned model can receive the data that describes a semantic entity included in thecontext data 204 and output themodel output 206 that describes the suggest action with respect to the semantic entity. The second machine-learned model(s) can additionally receive some or all of thecontext data 204. One or ordinary skill in the art would understand that additional configurations are possible within the scope of the present disclosure. -
FIG. 3 depicts a block diagram of anexample computing system 300 including anartificial intelligence system 301. Theartificial intelligence system 301 can include one or more machine-learned model(s) 302 according to example embodiments of the present disclosure. The machine-learned model(s) 302 can be trained to receivecontext data 304 and, as a result of receipt of thecontext data 304, provide amodel output 306 that describes one or more semantic entities referenced by thecontext data 304. - The
computing system 300 can be configured to interface with one ormore computer applications 308 to provide suggested actions that can be performed with the computer application(s) 308. Thecomputing system 300 can provide data descriptive of themodel output 306 of the machine-learned model(s) 302 that describes the suggested action(s) to the computer application(s) 308. Thecomputing system 300 can receive one or more application outputs 310 respectively from the computing application(s) 308 via a pre-defined application programming interface. The suggested action can describe or correspond with at least one of the application output(s) 310. -
FIG. 4 depicts an example suggestedaction 400 according to aspects of the present disclosure. The suggestedaction 400 can describe an available action that can be performed with a computer application. In this example, the suggestedaction 400 can include creating a calendar event with a calendar application. More specifically, in this example suggestedaction 400 includes the text “Add Appt. with Dr. Sherrigan to Calendar.” The computing system can be configured to perform this action can in response to receiving a user touch input requesting the same. For instance, the user can tap abutton 401, slide a slider bar, or otherwise interact with the suggestedaction 400 to request that the computing system perform the action. - In some implementations, the artificial intelligence system can generate the suggested action(s) 400 based on a template (e.g., a predefined template). Utilizing a template can reduce the computing resources required to generate such suggested
actions 400. Instead of training and utilizing a machine-learned model to generate all text of the suggestedaction 400, key words of the suggestedaction 400 can be generated using the machine-learned model(s) and then assembled according to the template to generate the suggestedaction 400. For example, the template can include respective placeholders for averb 402, asemantic entity 404, and/or acomputer application 406. Thesemantic entity 404 can be described by themodel output 206 of the machine-learned model(s) 202, for example as described above with reference toFIG. 2 . The template may be arranged as follows: -
[Verb]+[Semantic Entity]+[Computer Application] - The artificial intelligence system can select an appropriate verb and/or computer application to be inserted into the respective placeholders of the template. The verb and/or computer application can be selected based on the context data and/or semantic entity. It should be understood that a variety of template variations can be employed within the scope of this disclosure. Furthermore, the artificial intelligence system can learn the user's preferences with respect to which templates to use and/or whether to use templates. Thus, the systems and methods described herein can employ one or more templates to generate the suggested actions.
- In some implementations, the same template or template(s) can be used to generate multiple suggested actions such that the multiple suggested actions have the same general look and feel to the user. As such, the user can quickly evaluate the suggested actions because they are presented in a known and predictable format to the user. Thus, the templates as described herein can facilitate greater use of the suggested actions.
- An explanation 410 can be displayed with respect to the suggested
actions 400. The explanation 410 can describe information about obtaining the context data, including, as examples, a time when the context data was obtained, a location of the computing device when the context data was obtained, and/or a source of the context data. In this example, the explanation 410 states that the context data was “saved at Home” at “8:06 AM.” The explanation 410 can indicate a source of the context data used to generate the suggestedaction 400 by displaying an icon. In this example, the explanation 410 can include aphone icon 412 to indicate that the context data was collected from audio during a phone call. The explanation 410 may encourage the user to be more comfortable with or trusting of the artificial intelligence system operations, which can make the artificial intelligence system more useful to the user. - In some implementations, the computing system can provide the user with a way to view additional information associated with obtaining the context data in addition to the explanation 410. For example, the computing system can detect a user touch input that requests additional information with respect to the explanation 410. In this example, in response to receiving a user touch input directed to “Details” 414, the computing system can display additional explanation information about obtaining the context data. The additional explanation information can include a time when the context data was obtained or a location of the computing device when the context data was obtained. The additional explanation information can include a source of the context data (if not already displayed). The additional information can include information about other times that context data was obtained in a similar way (e.g., from the same source, at similar times, while the computing device was in the same location etc.).
-
FIG. 5A depicts an additional example suggestedaction 500 according to aspects of the present disclosure. This example suggestedaction 500 describes an available action that can be performed with a shopping application. In this example, the suggestedaction 500 can include shopping for an item “Webster Classic Grill” using the shopping application. More specifically, in this example suggestedaction 500 includes the text “Shop Weber Classic Grill on Amazon.” This text can be generated based on a predefined template as described above with reference toFIG. 4 . As discussed above, the template can include respective placeholders for averb 502, asemantic entity 504, and acomputer application 506. The computing system can be configured to perform this action in response to receiving a user touch input requesting the same. For instance, the user can tap abutton 501, slide a slider bar, or otherwise interact with the suggestedaction 500 to request that the computing system perform the action. - An
explanation 510 can be displayed with respect to the suggestedactions 500 that describes information about obtaining the context data, including, as examples, a time and location of the computing device when the context data was obtained and/or a source of the context data. In this example, theexplanation 510 states that the context data was “saved at Home Depot” at “8:38 AM.” Theexplanation 510 can indicate a source of the context data used to generate the suggestedaction 500, for example by displaying anicon 512. In this example, theicon 512 can indicate that the context data was obtained from an image (e.g., a photograph or screenshot). Theexplanation 510 may encourage the user to be more comfortable with or trusting of the artificial intelligence system operations, which can make the artificial intelligence system more useful to the user. The computing system can be configured to detect a user touch input that requests additional information with respect to theexplanation 510. In this example, in response to receiving a user touch input directed to “Details” 514, the computing system can display additional explanation information about obtaining the context data. -
FIG. 5B depicts an additional example suggestedaction 520 according to aspects of the present disclosure. In this example, the suggestedaction 520 can include shopping for an item using a shopping application: “Shop BKR water bottle on Amazon.” This text can be generated based on a predefined template as described above with reference toFIG. 4 . As discussed above, the template can include respective placeholders for averb 522, asemantic entity 524, and acomputer application 526. The suggestedaction 520 can include a button 525 for performing the suggestedaction 520 and anexplanation 530. In this example, theexplanation 530 indicates the location and time when the context data was saved (e.g., “saved at home ⋅ 8:06 AM.”). Theexplanation 530 can indicate a source of the context data used to generate the suggestedaction 520 by displaying anicon 532. In this example, theicon 532 can indicate that the context data was obtained from audio (e.g., a voice memo or ambient audio). As indicated above, theexplanation 530 may encourage the user to be more comfortable with or trusting of the artificial intelligence system operations, which can make the artificial intelligence system more useful to the user. The computing system can be configured to detect a user touch input that requests additional information with respect to theexplanation 530. In this example, in response to receiving a user touch input directed to “Details” 534, the computing system can display additional explanation information about obtaining the context data. -
FIG. 5C depicts an additional example suggestedaction 540 according to aspects of the present disclosure. This example suggestedaction 540 describes an available action that can be performed with a music streaming application. In this example, the suggestedaction 540 can include listening to aparticular song 544 by aparticular artist 546, which can correspond with the stored semantic entity. The user can tap abutton 541 to perform the suggestedaction 540. Additionally, in this example, the suggestedaction 540 can include a save button 548 for saving the suggestedaction 540 for a later time and/or a share button 550 for sharing the suggested action, for example, via social media, text message, e-mail, etc. - An
explanation 552 can be displayed with respect to the suggestedactions 540 that describes information about obtaining the context data, including, as examples, a time and location of the computing device when the context data was obtained and/or a source of the context data. In this example, theexplanation 552 states that the context data was “saved at Linda's Tavern” at “11:06 PM.” A portion of theexplanation 552, such as the location “Linda's Tavern” can include a link, for example to further information about the location (e.g., a web search or map application search). - The
explanation 552 can indicate a source of the context data used to generate the suggestedaction 540 by displaying anicon 554. In this example, theicon 554 can indicate that the context data was obtained from ambient music (e.g., detected by a microphone of the computing device). Theexplanation 552 may encourage the user to be more comfortable with or trusting of the artificial intelligence system operations, which can make the artificial intelligence system more useful to the user. The computing system can be configured to detect a user touch input that requests additional information with respect to theexplanation 552. In this example, in response to receiving a user touch input directed to “Details” 556, the computing system can display additional explanation information about obtaining the context data. -
FIG. 6 depictscomputing device 600 displaying anexample panel 602 including multiple suggestedactions computing device 600 is locked and requires authentication to access a main menu or perform other operations. - The multiple suggested
actions action action 604. The additional suggested action(s) 606, 608 can be distinct from the suggestedaction 604. The additional suggested action(s) 606, 608 can also be generated and stored (e.g., during the first time interval) by the artificial intelligence system based on distinct semantic entities. For example, the artificial intelligence system can obtain additional context data that is distinct from the context data and input additional model input(s) that include the additional context data into the machine-learned model(s). The artificial intelligence system can receive, as an additional output of the machine-learned model(s), data descriptive of the additional suggested action(s) that is described by the additional suggested action(s), for example as described above with reference toFIGS. 2 and 3 . Thus, the artificial intelligence system can leverage the machine-learned model(s) to store multiple semantic entities over the first time interval (e.g., as the user goes about their day) and then display multiple suggested actions during the second time interval (e.g., at the end of the day). - Additional buttons can also be displayed in the
panel 602 for the user to control or manipulate the suggestedactions settings icon 610 can be displayed in thepanel 602. In response to a user touch action directed to thesettings icon 610, the computing system can display a settings panel, for example as described below with respectFIG. 11 . The user can adjust the settings of the artificial intelligence system using the setting panel. - As another example, a
search icon 612 can be displayed in thepanel 602. In response to a user touch action directed to thesearch icon 612, the user can search through suggested actions that are not currently displayed in thepanel 602. - As another example, a “view all”
button 614 can be displayed in thepanel 602. In response to a user touch action directed to the “view all”button 614, the user can view additional suggested actions that are not currently displayed in thepanel 602. -
FIG. 7 depictscomputing device 700 displaying anexample notification panel 702 displaying a suggestedaction 704 withnotifications notification panel 702 can be displayed automatically or in response to user input requesting that thenotification panel 702 be displayed. -
FIG. 8 depicts a computing device 800 in a first state in which a plurality of category labels 802, 804, 806, 808, 810, 812, 814 corresponding with categorized suggested actions are displayed according to aspects of the present disclosure. A plurality of suggestedactions panel 820 with the plurality of category labels 802, 804, 806, 808, 810, 812. The artificial intelligence system can categorize the suggestedactions actions category label 814 and display suggested actions that correspond with the selectedcategory label 814, for example as described below withFIG. 9 . -
FIG. 9 depicts the computing device 800 ofFIG. 8 in which onecategory label 814 of the plurality of category labels has been selected.Suggested actions category label 814 are displayed according to aspects of the present disclosure. More specifically, in response to detecting the user touch action, the computing system can display the suggestedactions category label 814. Thus, the artificial intelligence system can categorize the suggestedactions actions - In some implementations, the artificial intelligence system can rank, sort, categorize, and prioritize, etc. the suggested
actions actions actions actions actions -
FIG. 10 depicts acomputing system 1000 displaying a suggestedaction 1002 in which asemantic entity 1004 has been selected and a search is being performed in an overlaidpanel 1006 in response to thesemantic entity 1004 being selected according to aspects of the present disclosure. -
FIG. 11 depicts acomputing system 1100 displaying asettings panel 1102 in which the user can select adefault computer application 1104 for a type of suggested action (e.g., a suggested action including shopping). Thesettings panel 1102 can allow the user to adjust other settings associated with the artificial intelligence system. -
FIG. 12 depicts a flow chart diagram of anexample method 1200 for identifying information of interest, storing the information, and providing suggested actions to users of computing systems at a later time based on the stored information. AlthoughFIG. 12 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of themethod 1200 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure. - At (1202), a computing system can obtain the context data during a first time interval. The artificial intelligence system can capture information of interest as the computing device is used to perform tasks throughout the day during the first time interval. For instance, the artificial intelligence system can identify and store semantic entities while the user navigates between various computer applications and/or switches between different tasks or activities.
- At (1204), the computing system can input the model input that includes the context data into the one or more machine-learned models, for example as described above with reference to
FIGS. 2 and 3 . - At (1206), the computing system can receive, as an output of the one or more machine-learned models, the model output that describes the one or more semantic entities referenced by the context data, for example as described above with reference to
FIGS. 2 and 3 . - At (1208), the computing system can store the model output in a tangible, non-transitory computer-readable medium.
- At (1210), the computing system can provide, for display in a user interface during a second time interval that is after the first time interval, a suggested action with respect to the one or more semantic entities described by the model output. For example, the computing system can display the suggested action at a time that is convenient for the user to review (e.g., after work, after dinner, at regularly scheduled time intervals etc.). As an example implementation, the first time interval can be defined as a duration of a call associated with a particular business. The context data can include a payment date, payment amount, or other information that was discussed during the call.
- As another example, the suggested action can be displayed in response to an event (e.g., the second time interval can begin in response to the event). For example, the computing system can provide a suggested action that includes scheduling a calendar event to the user at a certain time interval prior to the event (e.g., 7 days). In some implementations the duration between obtaining the context data and providing the suggested action can be learned based on a user interaction (e.g., with prior suggested actions, with the application associated with suggested action to be provided, etc.). As a further example, a sale or lowering of a price of an item could cause the computing system to provide a suggested action with respect to the item based on context data that includes the user interacting with the item at an earlier time.
- In some implementations, the suggested action(s) can be provided several hours, days, weeks, or even months after the context data was obtained. For example, the suggested action(s) can be provided at least 4 hours (or 8 hours, or longer) after the context data was obtained such that the original event that prompted obtaining the context data may not be as fresh in the mind of the user. As such, the suggested action may be more useful as a “reminder” to the user.
- The technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components. For instance, processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination. Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
- While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Those skilled in the art, upon attaining an understanding of the foregoing, can readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment. Thus, it is intended that the present disclosure cover such alterations, variations, and equivalents.
Claims (20)
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/044900 WO2021025668A1 (en) | 2019-08-02 | 2019-08-02 | Systems and methods for generating and providing suggested actions |
Publications (1)
Publication Number | Publication Date |
---|---|
US20220245520A1 true US20220245520A1 (en) | 2022-08-04 |
Family
ID=67587975
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/622,465 Pending US20220245520A1 (en) | 2019-08-02 | 2019-08-02 | Systems and Methods for Generating and Providing Suggested Actions |
Country Status (4)
Country | Link |
---|---|
US (1) | US20220245520A1 (en) |
EP (1) | EP3973469A1 (en) |
CN (1) | CN114041145A (en) |
WO (1) | WO2021025668A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220272055A1 (en) * | 2021-02-25 | 2022-08-25 | Google Llc | Inferring assistant action(s) based on ambient sensing by assistant device(s) |
US20220413689A1 (en) * | 2021-06-28 | 2022-12-29 | Citrix Systems, Inc. | Context-based presentation of available microapp actions |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2023239638A1 (en) * | 2022-06-09 | 2023-12-14 | MagicX Inc. | Digital interface with user input guidance |
Citations (32)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060015201A1 (en) * | 1999-12-01 | 2006-01-19 | Silverbrook Research Pty Ltd | Retrieving audio data via a coded surface |
US20100268661A1 (en) * | 2009-04-20 | 2010-10-21 | 4-Tell, Inc | Recommendation Systems |
US7853888B1 (en) * | 2007-01-12 | 2010-12-14 | Adobe Systems Incorporated | Methods and apparatus for displaying thumbnails while copying and pasting |
US20120144285A1 (en) * | 2010-12-07 | 2012-06-07 | Microsoft Corporation | User interface form field expansion |
US20140149330A1 (en) * | 2012-11-29 | 2014-05-29 | Elon Kaplan | Contextual knowledge management system and method |
US20160092074A1 (en) * | 2014-09-26 | 2016-03-31 | Lenovo (Singapore) Pte. Ltd. | Multi-modal fusion engine |
US20160283519A1 (en) * | 2015-03-25 | 2016-09-29 | Dane Glasgow | Media discovery and content storage within and across devices |
US20170098197A1 (en) * | 2014-02-21 | 2017-04-06 | Rna Labs Inc. | Systems and Methods for Automatically Collecting User Data and Making a Real-World Action for a User |
US20180039385A1 (en) * | 2016-08-08 | 2018-02-08 | Microsoft Technology Licensing, Llc | Interacting with a Clipboard Store |
US20180188924A1 (en) * | 2016-12-30 | 2018-07-05 | Google Inc. | Contextual paste target prediction |
US20180233141A1 (en) * | 2017-02-14 | 2018-08-16 | Microsoft Technology Licensing, Llc | Intelligent assistant with intent-based information resolution |
US20180247648A1 (en) * | 2017-02-27 | 2018-08-30 | SKAEL, Inc. | Machine-learning digital assistants |
US20180260680A1 (en) * | 2017-02-14 | 2018-09-13 | Microsoft Technology Licensing, Llc | Intelligent device user interactions |
US20180314689A1 (en) * | 2015-12-22 | 2018-11-01 | Sri International | Multi-lingual virtual personal assistant |
US20180321949A1 (en) * | 2017-05-04 | 2018-11-08 | Dell Products L.P. | Information Handling System Adaptive and Automatic Workspace Creation and Restoration |
US20190129940A1 (en) * | 2017-11-01 | 2019-05-02 | International Business Machines Corporation | Cognitive copy and paste |
US20190205391A1 (en) * | 2017-12-29 | 2019-07-04 | Aiqudo, Inc. | Automated Document Cluster Merging for Topic-Based Digital Assistant Interpretation |
US20190213465A1 (en) * | 2018-01-09 | 2019-07-11 | Fuji Xerox Co., Ltd. | Systems and methods for a context aware conversational agent based on machine-learning |
US20190258461A1 (en) * | 2018-02-22 | 2019-08-22 | Midea Group Co., Ltd. | Machine generation of context-free grammar for intent deduction |
US10417567B1 (en) * | 2013-02-14 | 2019-09-17 | Verint Americas Inc. | Learning user preferences in a conversational system |
US20190325029A1 (en) * | 2018-04-18 | 2019-10-24 | HelpShift, Inc. | System and methods for processing and interpreting text messages |
US20190340201A1 (en) * | 2016-10-24 | 2019-11-07 | CarLabs, Inc. | Computerized domain expert |
US20190355270A1 (en) * | 2018-05-18 | 2019-11-21 | Salesforce.Com, Inc. | Multitask Learning As Question Answering |
US20190370629A1 (en) * | 2018-05-31 | 2019-12-05 | International Business Machines Corporation | Building a gossip group of domain-specific chatbots |
US10629191B1 (en) * | 2019-06-16 | 2020-04-21 | Linc Global, Inc. | Methods and systems for deploying and managing scalable multi-service virtual assistant platform |
US20200175019A1 (en) * | 2018-11-30 | 2020-06-04 | Rovi Guides, Inc. | Voice query refinement to embed context in a voice query |
US20200320134A1 (en) * | 2019-04-04 | 2020-10-08 | Verint Americas Inc. | Systems and methods for generating responses for an intelligent virtual |
US20200410392A1 (en) * | 2019-06-27 | 2020-12-31 | Adobe Inc. | Task-aware command recommendation and proactive help |
US20210011973A1 (en) * | 2019-07-10 | 2021-01-14 | International Business Machines Corporation | Multi-Lingual Action Identification |
US10922493B1 (en) * | 2018-09-28 | 2021-02-16 | Splunk Inc. | Determining a relationship recommendation for a natural language request |
US11151175B2 (en) * | 2018-09-24 | 2021-10-19 | International Business Machines Corporation | On-demand relation extraction from text |
US11544475B2 (en) * | 2019-03-22 | 2023-01-03 | Predictika Inc. | System and method for providing a model-based intelligent conversational agent |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9318108B2 (en) * | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
-
2019
- 2019-08-02 EP EP19752625.4A patent/EP3973469A1/en active Pending
- 2019-08-02 US US17/622,465 patent/US20220245520A1/en active Pending
- 2019-08-02 CN CN201980098094.1A patent/CN114041145A/en active Pending
- 2019-08-02 WO PCT/US2019/044900 patent/WO2021025668A1/en unknown
Patent Citations (33)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060015201A1 (en) * | 1999-12-01 | 2006-01-19 | Silverbrook Research Pty Ltd | Retrieving audio data via a coded surface |
US7853888B1 (en) * | 2007-01-12 | 2010-12-14 | Adobe Systems Incorporated | Methods and apparatus for displaying thumbnails while copying and pasting |
US20100268661A1 (en) * | 2009-04-20 | 2010-10-21 | 4-Tell, Inc | Recommendation Systems |
US20120144285A1 (en) * | 2010-12-07 | 2012-06-07 | Microsoft Corporation | User interface form field expansion |
US20140149330A1 (en) * | 2012-11-29 | 2014-05-29 | Elon Kaplan | Contextual knowledge management system and method |
US10417567B1 (en) * | 2013-02-14 | 2019-09-17 | Verint Americas Inc. | Learning user preferences in a conversational system |
US11295221B2 (en) * | 2013-02-14 | 2022-04-05 | Verint Americas Inc. | Learning user preferences in a conversational system |
US20170098197A1 (en) * | 2014-02-21 | 2017-04-06 | Rna Labs Inc. | Systems and Methods for Automatically Collecting User Data and Making a Real-World Action for a User |
US20160092074A1 (en) * | 2014-09-26 | 2016-03-31 | Lenovo (Singapore) Pte. Ltd. | Multi-modal fusion engine |
US20160283519A1 (en) * | 2015-03-25 | 2016-09-29 | Dane Glasgow | Media discovery and content storage within and across devices |
US20180314689A1 (en) * | 2015-12-22 | 2018-11-01 | Sri International | Multi-lingual virtual personal assistant |
US20180039385A1 (en) * | 2016-08-08 | 2018-02-08 | Microsoft Technology Licensing, Llc | Interacting with a Clipboard Store |
US20190340201A1 (en) * | 2016-10-24 | 2019-11-07 | CarLabs, Inc. | Computerized domain expert |
US20180188924A1 (en) * | 2016-12-30 | 2018-07-05 | Google Inc. | Contextual paste target prediction |
US20180260680A1 (en) * | 2017-02-14 | 2018-09-13 | Microsoft Technology Licensing, Llc | Intelligent device user interactions |
US20180233141A1 (en) * | 2017-02-14 | 2018-08-16 | Microsoft Technology Licensing, Llc | Intelligent assistant with intent-based information resolution |
US20180247648A1 (en) * | 2017-02-27 | 2018-08-30 | SKAEL, Inc. | Machine-learning digital assistants |
US20180321949A1 (en) * | 2017-05-04 | 2018-11-08 | Dell Products L.P. | Information Handling System Adaptive and Automatic Workspace Creation and Restoration |
US20190129940A1 (en) * | 2017-11-01 | 2019-05-02 | International Business Machines Corporation | Cognitive copy and paste |
US20190205391A1 (en) * | 2017-12-29 | 2019-07-04 | Aiqudo, Inc. | Automated Document Cluster Merging for Topic-Based Digital Assistant Interpretation |
US20190213465A1 (en) * | 2018-01-09 | 2019-07-11 | Fuji Xerox Co., Ltd. | Systems and methods for a context aware conversational agent based on machine-learning |
US20190258461A1 (en) * | 2018-02-22 | 2019-08-22 | Midea Group Co., Ltd. | Machine generation of context-free grammar for intent deduction |
US20190325029A1 (en) * | 2018-04-18 | 2019-10-24 | HelpShift, Inc. | System and methods for processing and interpreting text messages |
US20190355270A1 (en) * | 2018-05-18 | 2019-11-21 | Salesforce.Com, Inc. | Multitask Learning As Question Answering |
US20190370629A1 (en) * | 2018-05-31 | 2019-12-05 | International Business Machines Corporation | Building a gossip group of domain-specific chatbots |
US11151175B2 (en) * | 2018-09-24 | 2021-10-19 | International Business Machines Corporation | On-demand relation extraction from text |
US10922493B1 (en) * | 2018-09-28 | 2021-02-16 | Splunk Inc. | Determining a relationship recommendation for a natural language request |
US20200175019A1 (en) * | 2018-11-30 | 2020-06-04 | Rovi Guides, Inc. | Voice query refinement to embed context in a voice query |
US11544475B2 (en) * | 2019-03-22 | 2023-01-03 | Predictika Inc. | System and method for providing a model-based intelligent conversational agent |
US20200320134A1 (en) * | 2019-04-04 | 2020-10-08 | Verint Americas Inc. | Systems and methods for generating responses for an intelligent virtual |
US10629191B1 (en) * | 2019-06-16 | 2020-04-21 | Linc Global, Inc. | Methods and systems for deploying and managing scalable multi-service virtual assistant platform |
US20200410392A1 (en) * | 2019-06-27 | 2020-12-31 | Adobe Inc. | Task-aware command recommendation and proactive help |
US20210011973A1 (en) * | 2019-07-10 | 2021-01-14 | International Business Machines Corporation | Multi-Lingual Action Identification |
Non-Patent Citations (1)
Title |
---|
Federico Viticci; Shortcuts: A New Vision for Siri and iOS Automation; June 13, 2018; macstories.net; Pages 1-19. * |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220272055A1 (en) * | 2021-02-25 | 2022-08-25 | Google Llc | Inferring assistant action(s) based on ambient sensing by assistant device(s) |
US20220413689A1 (en) * | 2021-06-28 | 2022-12-29 | Citrix Systems, Inc. | Context-based presentation of available microapp actions |
Also Published As
Publication number | Publication date |
---|---|
WO2021025668A1 (en) | 2021-02-11 |
CN114041145A (en) | 2022-02-11 |
EP3973469A1 (en) | 2022-03-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR101213929B1 (en) | Systems and methods for constructing and using models of memorability in computing and communications applications | |
US7366990B2 (en) | Method and system for managing user activities and information using a customized computer interface | |
US20120259927A1 (en) | System and Method for Processing Interactive Multimedia Messages | |
US20150186366A1 (en) | Method and System for Displaying Universal Tags | |
US20090049405A1 (en) | System and method for implementing session-based navigation | |
US20120259926A1 (en) | System and Method for Generating and Transmitting Interactive Multimedia Messages | |
TW201602932A (en) | Search and locate event on calendar with timeline | |
US11275630B2 (en) | Task-related sorting, application discovery, and unified bookmarking for application managers | |
US20220245520A1 (en) | Systems and Methods for Generating and Providing Suggested Actions | |
CN110476162B (en) | Controlling displayed activity information using navigation mnemonics | |
EP3942490B1 (en) | Enhanced task management feature for electronic applications | |
US20140297350A1 (en) | Associating event templates with event objects | |
US11831738B2 (en) | System and method for selecting and providing available actions from one or more computer applications to a user | |
Pozzi et al. | Individuation and diversity: the need for idiographic HCI | |
WO2023239625A1 (en) | User interfaces for creating journaling entries | |
Pan | Interface Design for the Memory Machine | |
Waloszek | UI Design Blinks (2013) |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE, LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:WANTLAND, TIM;BARNHART, MELISSA LAUREN;JACKSON, BRIAN L.;REEL/FRAME:058472/0225Effective date: 20190806 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |