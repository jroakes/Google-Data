EP2831870A1 - Method and system for controlling imagery panning based on displayed content - Google Patents
Method and system for controlling imagery panning based on displayed contentInfo
- Publication number
- EP2831870A1 EP2831870A1 EP13769362.8A EP13769362A EP2831870A1 EP 2831870 A1 EP2831870 A1 EP 2831870A1 EP 13769362 A EP13769362 A EP 13769362A EP 2831870 A1 EP2831870 A1 EP 2831870A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- imagery
- pan
- viewport
- features
- computer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 title claims abstract description 71
- 238000004091 panning Methods 0.000 title claims description 18
- 230000004044 response Effects 0.000 claims description 26
- 230000006870 function Effects 0.000 claims description 15
- 238000012545 processing Methods 0.000 claims description 12
- 230000000977 initiatory effect Effects 0.000 claims description 10
- 239000013598 vector Substances 0.000 claims description 8
- 230000003247 decreasing effect Effects 0.000 claims description 7
- 230000007423 decrease Effects 0.000 claims description 4
- 230000001419 dependent effect Effects 0.000 claims description 4
- 230000005484 gravity Effects 0.000 abstract description 4
- 230000002452 interceptive effect Effects 0.000 description 9
- 230000009471 action Effects 0.000 description 6
- 230000001413 cellular effect Effects 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000004891 communication Methods 0.000 description 2
- 230000001186 cumulative effect Effects 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 230000008569 process Effects 0.000 description 2
- VYZAMTAEIAYCRO-UHFFFAOYSA-N Chromium Chemical compound [Cr] VYZAMTAEIAYCRO-UHFFFAOYSA-N 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000004075 alteration Effects 0.000 description 1
- 238000013479 data entry Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 1
Classifications
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G5/00—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators
- G09G5/34—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators for rolling or scrolling
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/0485—Scrolling or panning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3667—Display of a road map
- G01C21/367—Details, e.g. road map scale, orientation, zooming, illumination, level of detail, scrolling of road map or positioning of current position marker
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09B—EDUCATIONAL OR DEMONSTRATION APPLIANCES; APPLIANCES FOR TEACHING, OR COMMUNICATING WITH, THE BLIND, DEAF OR MUTE; MODELS; PLANETARIA; GLOBES; MAPS; DIAGRAMS
- G09B29/00—Maps; Plans; Charts; Diagrams, e.g. route diagram
- G09B29/003—Maps
- G09B29/005—Map projections or methods associated specifically therewith
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2320/00—Control of display operating conditions
- G09G2320/06—Adjustment of display parameters
- G09G2320/0613—The adjustment depending on the type of the information to be displayed
Definitions
- the present disclosure relates generally to navigating imagery, and more particularly to imagery panning based on content displayed in the imagery.
- Such services can include a user interface that includes a viewport displaying at least a portion of geographic imagery, such as map imagery, satellite imagery, oblique view imagery, and street level imagery, of a geographic area from various angles and/or viewpoints.
- User interfaces for interactive imagery systems typically include one or more navigation tools that allow a user to pan, tilt, rotate, and zoom the imagery in the viewport. For instance, a user can pan the imagery in the viewport by interacting with the imagery and dragging the imagery in various directions. Certain interactive systems allow a user to navigate the imagery by "throwing" the imagery in the viewport. As one example, a user interacting with a touch screen interface can throw the imagery in the viewport using a finger swipe. This causes the imagery to pan in the general direction of the finger swipe. As another example, a user can throw the imagery in the viewport by selecting the imagery and dragging a user manipulable cursor using a mouse or touchpad.
- the view of the imagery typically lands or stops panning at an arbitrary and unpredictable location. This requires the user to perform further manipulations to the view of the imagery to obtain a desired view, frustrating the interactive viewing experience of the user.
- certain interact systems have made the lowered the inertia of the imagery pan in response to the user input. This is undesirable as it removes the ability to easily and quickly pan the imagery long distances in the viewport through a simple gesture.
- One exemplary aspect of the present disclosure is directed to a computer- implemented method for displaying imagery.
- the method includes presenting a viewport displaying at least a portion of geographic imagery; receiving a user input initiating an imagery pan of the imagery in the viewport; and panning the imagery in response to the user input.
- the method further includes adjusting the motion of the imagery in the viewport during the imagery pan based at least in part on content displayed in the viewport.
- the method can include adjusting a pan rate of the imagery based at least in part on content displayed in the viewport.
- the method can include adjusting a pan direction based at least in part on content displayed in the viewport.
- FIG. 1 Other exemplary implementations of the present disclosure are directed to systems, apparatus, computer-readable media, devices, and user interfaces for adjusting the motion of imagery in a viewport based on content displayed in the viewport.
- FIG. 1 depicts an exemplary system for displaying imagery according to an exemplary embodiment of the present disclosure
- FIG. 2 depicts an exemplary computing device having a user interface presenting geographic imagery in a viewport according to an exemplary embodiment of the present disclosure
- FIGS. 3A-3D depict an exemplary imagery pan in response to a user input;
- FIG. 4 depicts a graphical representation of the pan rate of the imagery pan depicted in FIGS. 3A-3D;
- FIG. 5 depicts a flow diagram of an exemplary method according to an exemplary embodiment of the present disclosure
- FIG. 6 depicts a flow diagram of an exemplary method according to an exemplary embodiment of the present disclosure
- FIGS. 7A-7D depict an exemplary image pan in response to a user input according to an exemplary embodiment of the present disclosure
- FIG. 8 depicts a graphical representation of the pan rate of the imagery pan depicted in FIGS.7A-7D;
- FIGS. 9A-9D depict an exemplary image pan in response to a user input according to an exemplary embodiment of the present disclosure
- FIG. 10 depicts a graphical representation of the pan rate of the imagery pan depicted in FIGS. 9A-9D;
- FIG. 11 depicts a flow diagram according to an exemplary embodiment of the present disclosure.
- FIGS. 12A-12D depict an exemplary imagery pan in response to a user input according to an exemplary embodiment of the present disclosure.
- the present disclosure is directed to navigating imagery, such as geographic imagery.
- a user can initiate a pan of the imagery in a viewport presented on a display of a computing device by throwing the imagery in the viewport. For instance, a user can perform a finger swipe on a touch screen interface to throw the imagery in the viewport. After a throw action, the imagery can pan across the viewport at an initial pan rate that decays over time.
- the initial pan rate can be variable based on the user input. For instance, a faster user gesture, such as a relative fast finger swipe, can result in the initial pan rate being relatively high. A slower user gesture can result in the initial pan rate being relatively slow.
- the initial inertia of the imagery pan is loosened up in response to the throw action such that the imagery pans at a greater pan rate in the viewport upon user interaction with the imagery.
- the imagery pan can be controlled based on content displayed in the viewport such that the imagery pan is more likely to land at or near predominate features depicted in the viewport, facilitating a user's navigation of the imagery.
- an imagery pan in response to a throw action can be controlled or adjusted based on weights assigned to features depicted in the viewport or near the viewport.
- Predominate features in the imagery can be assigned greater weights than less predominate features in the imagery. For instance, a large city can have a higher weight than a smaller city. A busy neighborhood can have a higher weight than a sparse area. A road through a desert can have a higher weight than an empty section of the desert.
- the weights can be assigned to features using any suitable criteria. For instance, the weights can be assigned to features in the imagery based on rankings used to prioritize features for display in the imagery. In addition, the weights can be assigned based on personal information optionally provided by a user, such as favorite locations, most visited locations, most viewed locations, current location of the user, personal preferences, and other settings and/or information provided by the user.
- the panning of the imagery in response to the throw is adjusted based on the weights assigned to features displayed in the viewport.
- the weights assigned to features can act as "friction" on the imagery, slowing down the imagery pan as the features pass through the viewport.
- a feature having a higher weight will slow down the imagery pan more than a feature having a low weight.
- a feature with a relatively high weight can slow down a relatively fast imagery pan and can stop a relatively slow imagery pan.
- a feature with a relatively low weight may barely affect a relatively fast imagery pan, but could further slow a relatively slow imagery pan.
- the weights assigned to features displayed in the viewport can act as "gravity" on the imagery.
- the direction of the imagery pan in response to the throw action by the user can be adjusted based on the weights assigned to features depicted in the imagery. For instance, the direction of the imagery pan can be adjusted such that that imagery pans more towards features with higher weights than features with lower rates.
- the present disclosure provides for more convenient navigation of imagery when panning imagery in viewports.
- the loosening of the inertia of the imagery pan allows for convenient panning across large distances or portions of the imagery.
- the imagery is more likely to land or pass over features of interest to a user when the user initiates a throw action of the imagery.
- the user can experience a more user friendly and convenient navigation experience when interacting with the imagery.
- FIG. 1 depicts an exemplary interactive system 100 for displaying imagery according to an exemplary embodiment of the present disclosure.
- the present disclosure is discussed with reference to geographic imagery, such as map imagery, satellite imagery, oblique view imagery, street level imagery, and other geographic imagery.
- geographic imagery such as map imagery, satellite imagery, oblique view imagery, street level imagery, and other geographic imagery.
- system 100 includes a computing device 110 for displaying geographic imagery to a user.
- the computing device 110 can take any appropriate form, such as a personal computer, smartphone, desktop, laptop, PDA, tablet, or other computing device.
- the computing device 110 includes a display 118 for displaying the imagery to a user and appropriate input devices 115 for receiving input from the user.
- the input devices 115 can include, for instance a touch screen, a touch pad, data entry keys, a mouse, speakers, and/or a microphone suitable for voice recognition.
- a user can request imagery by interacting with an appropriate user interface presented on the display 118 of computing device 110.
- the computing device 110 can then receive imagery and associated data and present at least a portion of the imagery through a viewport on any suitable output device, such as through a viewport set forth in a browser presented on the display 118.
- An exemplary user interface having a viewport for presenting imagery will be discussed with reference to FIG. 2.
- the computing device 110 includes a processor(s) 112 and a memory 114.
- the processor(s) 112 can be any known processing device.
- Memory 114 can include any suitable computer-readable medium or media, including, but not limited to, RAM, ROM, hard drives, flash drives, or other memory devices. Memory 114 stores information accessible by processor(s) 112, including instructions that can be executed by processor(s) 112.
- the instructions can be any set of instructions that when executed by the processor(s) 112, cause the processor(s) 112 to provide desired functionality.
- the instructions can be software instructions rendered in a computer-readable form.
- any suitable programming, scripting, or other type of language or combinations of languages can be used to implement the teachings contained herein.
- the instructions can be implemented by hard- wired logic or other circuitry, including, but not limited to application-specific circuits.
- the computing device 110 can include a network interface 116 for accessing information over a network 120.
- the network 120 can include a combination of networks, such as cellular network, WiFi network, LAN, WAN, the Internet, and/or other suitable network and can include any number of wired or wireless communication links.
- computing device 110 can communicate through a cellular network using a WAP standard or other appropriate communication protocol.
- the cellular network could in turn communicate with the Internet, either directly or through another network.
- Computing device 110 can communicate with another computing device 130 over network 120.
- Computing device 130 can be a server, such as a web server, that provides information to a plurality of client computing devices, such as computing devices 110 and 150 over network 120.
- Computing device 130 receives requests from computing device 110 and locates information to return to computing devices 110 responsive to the request.
- the computing device 130 can take any applicable form, and can, for instance, include a system that provides mapping services, such as the Google Maps services provided by Google Inc.
- computing device 130 includes a processor(s) 132 and a memory 134.
- Memory 134 can include instructions 136 for receiving requests for geographic imagery from a remote client device, such as computing device 110, and for providing the requested information to the client device for presentation to the user.
- Memory 134 can also include or be coupled to various databases 138 containing information for presentation to a user.
- computing device 130 can communicate with other databases as needed.
- the databases can be connected to computing device 130 by a high bandwidth LAN or WAN, or could also be connected to computing device 130 through network 120.
- the databases, including database 138 can be split up so that they are located in multiple locales.
- Database 138 can store store map-related information, at least a portion of which can be transmitted to a client device, such as computing device 110.
- database 138 can store map tiles, where each tile is an image of a particular geographic area.
- the images can include street maps, satellite images, oblique view images, aerial images, or combinations of these.
- the various map tiles are each associated with geographical locations, such that the computing device 130 is capable of selecting, retrieving and transmitting one or more tiles in response to receipt of a geographical location.
- the locations can be expressed in various ways including but not limited to latitude/longitude positions, street addresses, points of interest on a map, building names, and other data capable of identifying geographic locations.
- the database 138 can also include points of interest.
- a point of interest can be any item that is interesting to one or more users that is associated with a geographical location.
- a point of interest can include a landmark, stadium, park, monument, restaurant, business, building, or other suitable point of interest.
- a point of interest can be added to the database 138 by professional map providers, individual users, or other entities.
- the database 138 can store weights associated with the points of interest that can be used to control the navigation of imagery in the viewport presented on a computing device, such as computing device 110.
- the computing device 130 can transmit the weights to a client device along with map tiles and other information during navigation of the imagery, such as during an imagery pan.
- the database 138 can also store street information.
- the street information can include the location of a street relative to a geographic area or other streets. For instance, it can store information indicating whether a traveler can access one street directly from another street. Street information can further include street names where available, and potentially other information, such as distance between intersections and speed limits.
- the database 138 can include user information that is optionally provided by a user to enhance the user's viewing and navigation experience.
- Exemplary user information can include favorite locations, most visited locations, current location, preferences, and/or settings provided by the user that can be used to enhance the interactive viewing experience of the user.
- the user information can be used to determine weights associated with features and/or points of interest such that the weights can be tailored to individual users.
- Computing device 130 can provide information, including geographic imagery weights, and other associated information, to computing device 110 over network 120.
- the information can be provided to computing device 110 in any suitable format.
- the information can include information in HTML code, XML messages, WAP code, Flash, Java applets, xhtml, plain text, voiceXML, VoxML, VXML, or other suitable format.
- the computing device 110 can display the information to the user in any suitable format.
- the information can be displayed within a browser, such as Google Chrome or other suitable browser.
- FIG. 2 depicts an exemplary computing device 110 having a user interface 200, such as a browser, presented on a display 118.
- the computing device 110 of FIG. 2 is illustrated as a tablet computing device.
- computing device 110 can be any suitable computing device.
- User interface 200 includes a viewport 210 that displays geographic imagery 220.
- the geographic imagery 220 depicted in FIG. 2 comprises street map imagery.
- Geographic imagery 220 can also include satellite imagery, oblique view imagery, aerial imagery, three-dimensional imagery, or other suitable imagery.
- a user can interact with geographic imagery 220 by interacting with various navigation tools 230. For instance, a user can pan, tilt, rotate and/or zoom the imagery 220 using navigation tools 230 to obtain different views of the geographic imagery.
- a user can pan the imagery (i.e. move the imagery in the viewport in different directions) by "throwing" imagery in the viewport.
- a user can initiate a throw of the imagery by swiping a finger across a touch screen. This will cause the imagery to pan across the viewport in the general direction of the finger swipe at an initial speed or pan rate that is based on the speed of the finger swipe. The pan rate of the imagery pan will decay over time until the imagery comes to rest in the viewport.
- Other user interactions can initiate a throw of the imagery in the viewport. For instance, a user can initiate a throw by dragging a mouse across the display, by dragging a finger across a touchpad, or through other suitable user interactions.
- FIGS. 3A-3D depict an exemplary imagery pan in a viewport 210 of a user interface 200 in response to a user input throwing the imagery across the viewport 210.
- the viewport displays geographic imagery 220 at a first location.
- a user that wishes to view another portion of the geographic imagery 220 can initiate a throw of the imagery by a finger swipe or other suitable gesture. This causes the imagery to pan in the general direction of the finger swipe such that feature 225 comes into view as shown in FIG. 3B.
- Feature 225 can be any suitable object, item, information, or other feature depicted in the imagery 220.
- feature 225 can be a city, town, neighborhood, street, body of water, building, monument, address, stadium, arena, or other suitable point of interest.
- the imagery 220 will continue to pan across the viewport 210 in response to the user input as shown in FIG. 3C until the imagery 220 comes to rest as shown in FIG. 3D.
- FIG. 4 provides a graphical representation of the pan rate associated with the imagery pan illustrated in FIGS. 3A-3D.
- FIG. 4 plots the pan rate (speed) of the imagery pan as a function of time in response to a user input initiating a throw of the imagery.
- the initial pan rate Po can be dependent on the user gesture, such as the speed of a finger swipe. For instance, a faster finger swipe can result in a higher initial pan rate. A slower finger swipe can result in a lower initial pan rate.
- curve 410 the pan rate steadily declines over time until the pan rate is zero where the imagery comes to rest. While curve 410 represents a linear decline in the pan rate, other suitable relationships can be used. For instance, the pan rate can decline exponentially until the imagery comes to rest.
- the slope of curve 410 represents the rate of decline in the pan rate of the imagery and provides an indication of the "inertia" of the throw.
- the "inertia" of the throw dictates how long it takes for the imagery to come to rest after a throw action. For instance, the inertia of the throw can be loosened up (i.e. increased) such that it takes longer for the imagery to come to rest after the user input throwing the imagery. This is illustrated by curve 420.
- the inertia of the throw can also be tightened (i.e. decreased) such that the imagery comes to rest in a shorter period of time as illustrated by curve 430.
- the inertia of a throw can be adjusted by settings associated with an interactive imagery system, such as settings input by a user.
- the imagery pan in response to a user input is controlled based on content displayed in the viewport such that it is more likely that the imagery comes to rest with relevant features displayed in the viewport.
- FIG. 5 depicts an exemplary computer-implemented method (500) for controlling the pan of imagery in response to user input according to an exemplary embodiment of the present disclosure.
- the exemplary method (500) can be implemented using any computing device, such as the computing device 110 of FIG. 1.
- FIG. 5 depicts steps performed in a particular order for purposes of illustration and discussion, the methods discussed herein are not limited to any particular order or arrangement.
- One skilled in the art, using the disclosures provided herein, will appreciate that various steps of the methods can be omitted, rearranged, combined and/or adapted in various ways.
- the method includes presenting imagery in the viewport.
- the computing device 110 can present geographic imagery, such as map imagery, satellite imagery, aerial imagery, and/or oblique view imagery, in viewport 210 of a user interface 200 presented on a display 118 of computing device 110.
- the method includes receiving a user input initiating a pan of the imagery in the viewport.
- the user input can be any suitable input from a user initiating a pan of the imagery.
- a user input such as a finger swipe or other gesture, can be received to throw the imagery in the viewport.
- the method pans the imagery in response to the user input at an initial pan rate and direction.
- the processor 112 of the computing device 110 can initiate a pan of the imagery in the viewport 210 in response to user input provided via a suitable input device 115.
- the initial pan rate and direction of the imagery pan can be based on the user input. For example, in the case of a finger swipe, the initial pan rate and direction of the imagery pan can be based on the speed and direction of the finger swipe.
- the method adjusts the motion of the imagery in the viewport based on content displayed in or near the viewport.
- the processor 112 of the computing device 110 can adjust characteristics of the imagery pan, such as pan rate and pan direction, based on one or more features displayed in or near the viewport 210.
- the features depicted in the viewport can affect the motion of the imagery pan in various ways.
- features depicted in the viewport can act as "friction" on the imagery pan, slowing the pan rate of the imagery pan as the imagery pans across the viewport.
- the friction applied by features depicted in the viewport facilitate navigation of the imagery by making the imagery more likely to land in an area where relevant features are presented to a user after a imagery throw.
- the features can also act as "gravity" on the imagery pan, adjusting the direction of the imagery pan such that more predominate features of the imagery are depicted in the viewport after the imagery pan comes to rest.
- FIG. 6 depicts one exemplary method (600) for adjusting the pan motion based on content displayed in the viewport according to an exemplary embodiment of the present disclosure.
- the method of FIG.6 can be implemented by any computing device, such as by the processor 112 of the computing device 110 of FIG. 1.
- FIG. 6 depicts steps performed in a particular order for purposes of illustration and discussion, the methods discussed herein are not limited to any particular order or arrangement.
- One skilled in the art, using the disclosures provided herein, will appreciate that various steps of the methods can be omitted, rearranged, combined and/or adapted in various ways.
- the method identifies features displayed in the viewport.
- the processor 112 of computing device 110 can identify features that are currently displayed in the viewport 210.
- the number of features displayed in the viewport can be dependent on the zoom level of the imagery. Imagery at a high zoom level will have fewer features than imagery at a lower zoom level.
- the features depicted in the viewport will affect the motion of the imagery as the imagery pans across the viewport.
- the method includes accessing weights assigned to the features depicted in the viewport.
- the computing device 110 can send a request to computing device 130 for imagery to be displayed during an imagery pan.
- the computing device 130 can provide, via a network, geographic imagery for display during the imagery pan as well as weights associated with features depicted in the viewport.
- the computing device 110 could access weights previously downloaded from the computing device 130 and stored in a local memory.
- the characteristics of the imagery pan can be adjusted, for instance by the processor 112 of the computing device 110, based on the weights associated with the features depicted in the viewport.
- the weights can be assigned to the features using any suitable criteria. For instance, the weights can be assigned based on information associated with the feature, such as population, size, popularity, number of searches associated with the feature, amount of information associated with the feature, current events associated with the feature, or other suitable criteria. In one embodiment, the weights can be assigned to the features using priority rankings used to prioritize features for display in the imagery. As is known, a different number of features in the imagery can be displayed depending on the zoom level of the imagery. Features of relatively higher importance are typically displayed in the imagery before features of relatively low importance. The weights assigned to the features can be based on similar criteria to determining rankings for prioritizing these features for display. For instance, the weights can be based on population, popularity, size, number of search queries related to the feature, or other suitable criteria.
- the weights assigned to particular features can also be based on user information optionally provided by a user, such as favorite locations, most visited locations, most viewed locations, settings, preferences or other information optionally provided by a user. This information can be used to personalize the navigation experience of a particular user such that features of interest to a particular user will have a greater effect on the motion of the imagery as it pans across the viewport.
- the method includes adjusting the pan rate based on the weights assigned to features depicted in the viewport.
- the processor 1 12 can adjust the pan rate of the imagery in the viewport 210 presented on the display 1 18 of computing device 1 10 based on features depicted in the imagery.
- the method can include slowing down the pan rate based on the weights of features depicted in the viewport.
- the method can include summing all of the weights depicted in the viewport and adjusting the pan rate of the imagery pan in proportion to the total weight of all features.
- the method can include summing all of the weights within a predefined perimeter or radius about the center of the viewport and adjusting the pan rate of the imagery pan in proportion to the total weight of these features.
- the perimeter of radius can be predefined and in certain cases can be set to encompass features that are not displayed in the viewport. In this manner, the present disclosure can adjust the motion of an imagery pan on features that are near or adjacent to the viewport, but are not displayed in the viewport.
- the pan rate can be adjusted based on the following:
- Pi is the adjusted pan rate
- Po is the initial pan rate
- ⁇ is a weighting factor determined as a function of the weights assigned to features in the viewport (such as the sum of all weights associated with features depicted in the viewport) and I is associated with the natural decay rate of the imagery pan in response to the throw.
- the weighting factor ⁇ can be determined in any suitable manner. For instance, the weighting factor ⁇ can be determined as a function of the sum of all weights assigned to features depicted in the viewport.
- the weighting factor ⁇ can be determined as function of all weights within a predefined radius or perimeter about the center of the viewport.
- the weighting factor ⁇ increases the deceleration rate of the imagery pan in proportion to the weights of features depicted in the viewport. The greater the weight of the features depicted in the imagery, the stronger the frictional force slowing down the imagery pan. In this manner, the features act as "friction" on the imagery pan, slowing down the imagery.
- the method includes panning the imagery at the adjusted pan rate. For instance, after determining an adjusted pan rate based on the features depicted in the viewport, the processor 112 of the computing device 110 can pan the imagery at the adjusted pan rate such that the motion of the imagery across the viewport during the imagery pan is altered. This process can then repeat itself based on additional content displayed in the imagery as the imagery pans across the viewport until the imagery comes to rest.
- the method determines at (510) whether the imagery pan has comes to a rest. If so, the method terminates as shown at (512). If the imagery has not come to a rest, the imagery pan continues across the viewport such that additional content is displayed in the viewport (512). For instance, the computing device 110 can send a request to the computing device 130 for additional imagery to be displayed in the viewport during the imagery pan. Upon receipt of this request, the computing device 130 can provide the additional imagery along with associated weights to the computing device 110. As shown in FIG. 5, the pan motion can then be further adjusted (508) based on the additional content until the imagery pan eventually comes to a rest.
- FIGS. 7A-7D depict an exemplary imagery pan in a viewport 210 of a user interface 200 in response to a user input throwing the imagery across the viewport 210 according to an exemplary embodiment of the present disclosure.
- the viewport displays geographic imagery 220 at a first location.
- a user that wishes to view another portion of the geographic imagery 220 can initiate a throw of the imagery by a finger swipe or other suitable gesture. This causes the imagery to pan in the general direction of the finger swipe at an initial pan rate such that feature 225 comes into view as shown in FIG. 7B.
- feature 225 can be any suitable object, item, information, or other feature depicted in the imagery 220.
- the present illustration will be discussed with reference to a single feature 225 for illustrative purposes. Those of ordinary skill in the art, using the disclosures provided herein, will understand that many features can be depicted in the viewport during the imagery pan and that each of these features can affect the motion of the imagery in a manner similar to the single feature 225.
- the pan rate of the imagery pan will slow down at a greater rate as illustrated in FIGS. 7C and 7D until the imagery 220 comes to rest.
- the feature 225 is depicted at or near the center of viewport 210 when the imagery 220 comes to rest. This is because the weight associated with feature 225 has slowed down the imagery pan, making it more likely for the imagery pan to come to rest with feature 225 displayed in the viewport 210.
- FIG. 8 graphically depicts the pan rate associated with the imagery pan illustrated in FIGS. 7A-7D.
- FIG. 8 plots the pan rate of the imagery pan as a function of time in response to a user input initiating a throw of the imagery.
- curve 440 the pan rate steadily declines from the initial pan rate Po over time until the feature 225 comes into view at time ti.
- the rate at which the pan rate decelerates is increased as a result of the feature 225 being depicted in the viewport.
- curve 442 shows the pan rate more rapidly declining until the pan rate reaches zero at about time t 2 .
- FIGS. 9A-9D illustrate another exemplary imagery pan in a viewport 210 of a user interface 200 in response to a user input throwing the imagery according to an exemplary embodiment of the present disclosure.
- the viewport displays geographic imagery 220 at a first location.
- a user that wishes to view another portion of the geographic imagery 220 can initiate a throw of the imagery by a finger swipe or other suitable gesture. This causes the imagery to pan in the general direction of the finger swipe at an initial pan rate such that feature 225 comes into view as shown in FIG. 9B.
- the feature 225 will act as friction on the imagery pan, slowing the pan rate of the imagery pan.
- an additional feature 227 comes into view.
- the additional feature 227 will also affect the imagery pan, further slowing the imagery pan until the imagery comes to rest at FIG. 9D. While FIGS. 9A-9D are discussed with reference to two features 225 and 227 for illustration purposes, those of ordinary skill in the art, using the disclosures provided herein, will understand that many features can be depicted in the viewport during the imagery pan and that each of these features can affect the motion of the imagery in a manner similar to the features 225 and 227.
- This imagery pan of FIGS. 9A-9D is graphically depicted in FIG. 10, which plots the pan rate of the imagery as a function of time.
- curve 450 the imagery pans at an initial pan rate Po that naturally decays over time.
- the feature 225 comes into view and causes the pan rate to decelerate at a greater rate as illustrated by curve 452.
- the additional feature 227 comes into view causing a further deceleration of the pan rate as illustrated by curve 454.
- the cumulative effect of the features 225 and 227 on the imagery pan cause the imagery to come to rest quicker when compared to the natural decay of the pan rate shown by curve 455.
- the imagery is more likely to come to rest with both features 225 and 227 displayed in the viewport, improving the navigation experience of the user.
- the imagery 220 in FIGS. 9A-9D is zoomed out relative to the imagery 220 depicted in FIGS. 7A-7D
- the imagery 220 of FIGS. 9A-9D will display more features when compared to imagery 220 of FIGS. 7A-7D.
- These additional features can have a greater cumulative effect on the motion of the imagery during the imagery pan. In certain cases this can be undesirable.
- the weights assigned to the features that are used to adjust the motion of the imagery pan can be reduced or increased based on the zoom level of the imagery.
- the initial pan rate of the imagery pan can be set higher or lower depending on the zoom level to achieve desired imagery pan characteristics.
- a user input initiating a throw of the imagery at a lower zoom level can cause a higher initial pan rate when compared to a user input initiating a throw of the imagery at a higher zoom level (less features depicted).
- FIG. 1 1 depicts an exemplary method (700) for adjusting the motion of the imagery pan based on content displayed in the viewport according to an exemplary embodiment of the present disclosure.
- the method of FIG.1 lean be implemented by any computing device, such as by the processor 112 of the computing device 110 of FIG. 1.
- FIG. 11 depicts steps performed in a particular order for purposes of illustration and discussion, the methods discussed herein are not limited to any particular order or arrangement.
- One skilled in the art, using the disclosures provided herein, will appreciate that various steps of the methods can be omitted, rearranged, combined and/or adapted in various ways.
- the method identifies features displayed in the viewport. .
- the processor 112 of computing device 110 can identify features that are currently displayed in the viewport 210.
- the method access weights assigned to the features.
- the features displayed in the viewport can be assigned weights based on any suitable criteria, such as rankings used to prioritize features for display and/or information optionally provided by a user.
- the method then adjusts the pan direction of the imagery pan based on weights assigned to the features (706).
- the processor 112 can adjust the pan direction of the imagery in the viewport 210 presented on the display 118 of computing device 110 based on weights associated with features depicted in the imagery.
- the initial pan direction can be adjusted such that a significant feature, is more likely to be displayed in the center of the imagery when the imagery comes to rest.
- the pan direction can be adjusted as a function of both weight assigned to a particular feature and a distance and direction of the feature from the center of the viewport.
- each feature can be assigned a vector having a value determined as a function of the weight assigned to the feature and the distance of the feature from the center of the viewport.
- the direction associated with the vector can be determined as a function of the direction of the feature relative to the center of the viewport.
- the adjusted pan direction can be determined by calculating the vector sum of all features depicted in the viewport.
- the adjusted pan direction can also take into account the direction of the features depicted in the viewport relative to the center of the viewport.
- the processor 112 of the computing device 110 can pan the imagery in the adjusted pan direction such that the motion of the imagery across the viewport during the imagery pan is altered. The process can then repeat itself based on additional content displayed in the imagery until the imagery comes to rest.
- FIGS. 12A-12D depict an exemplary image pan in accordance with the exemplary method (700) of FIG. 11.
- the imagery pan in FIGS. 12a-12d will be discussed with reference to a single feature 225 for illustration purposes.
- Those of ordinary skill in the art, using the disclosures provided herein, will understand that many features can be depicted in the viewport during the imagery pan and that each of these features can affect the motion of the imagery in a manner similar to the single feature 225.
- the viewport displays geographic imagery 220 at a first location.
- a user that wishes to view another portion of the geographic imagery 220 can initiate a throw of the imagery by a finger swipe or other suitable gesture.
- This causes the imagery to pan generally to the right in an initial pan direction such that feature 225 comes into view as shown in FIG. 12B.
- the feature 225 will affect the direction of the imagery pan such that it is more likely that the feature 225 will be displayed at the center of the imagery when the imagery comes to rest.
- FIG. 12C shows that the direction of the imagery pan has been slightly altered such that feature 225 has moved slightly to the right and downward.
- FIG. 12D illustrates that the direction of the imagery pans has been further altered such that feature 225 is displayed at or near the center of viewport 210.
Abstract
Description
Claims
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/432,042 US20130257742A1 (en) | 2012-03-28 | 2012-03-28 | Method and System for Controlling Imagery Panning Based on Displayed Content |
PCT/US2013/033802 WO2013148625A1 (en) | 2012-03-28 | 2013-03-26 | Method and system for controlling imagery panning based on displayed content |
Publications (3)
Publication Number | Publication Date |
---|---|
EP2831870A1 true EP2831870A1 (en) | 2015-02-04 |
EP2831870A4 EP2831870A4 (en) | 2015-11-18 |
EP2831870B1 EP2831870B1 (en) | 2019-05-08 |
Family
ID=49234228
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP13769362.8A Active EP2831870B1 (en) | 2012-03-28 | 2013-03-26 | Method and system for controlling imagery panning based on displayed content |
Country Status (4)
Country | Link |
---|---|
US (1) | US20130257742A1 (en) |
EP (1) | EP2831870B1 (en) |
DE (1) | DE202013012455U1 (en) |
WO (1) | WO2013148625A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9322665B2 (en) * | 2012-06-05 | 2016-04-26 | Apple Inc. | System and method for navigation with inertial characteristics |
US9959641B1 (en) * | 2014-07-17 | 2018-05-01 | SeeScan, Inc. | Methods and systems for seamless transitioning in interactive mapping systems |
US9684425B2 (en) * | 2014-08-18 | 2017-06-20 | Google Inc. | Suggesting a target location upon viewport movement |
WO2019027469A1 (en) * | 2017-08-04 | 2019-02-07 | Google Llc | Automatically providing navigation directions on interactive digital map |
US11262910B2 (en) * | 2018-01-11 | 2022-03-01 | Honda Motor Co., Ltd. | System and method for presenting and manipulating a map user interface |
Family Cites Families (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3624626B2 (en) * | 1997-05-28 | 2005-03-02 | ソニー株式会社 | Information processing apparatus and method, and recording medium |
US6337694B1 (en) * | 1999-09-07 | 2002-01-08 | International Business Machines Corporation | Method and system for variable speed scrolling within a data processing system |
US6693653B1 (en) * | 2000-09-19 | 2004-02-17 | Rockwell Collins, Inc. | Method of assisting cursor movement toward a nearby displayed target |
US20050177305A1 (en) * | 2004-02-06 | 2005-08-11 | Han Maung W. | Display method and apparatus for navigation system |
US7728821B2 (en) * | 2004-08-06 | 2010-06-01 | Touchtable, Inc. | Touch detecting interactive display |
US20060271281A1 (en) * | 2005-05-20 | 2006-11-30 | Myron Ahn | Geographic information knowledge systems |
US7831381B2 (en) * | 2005-08-04 | 2010-11-09 | Microsoft Corporation | Data engine for ranking popularity of landmarks in a geographical area |
US7933897B2 (en) * | 2005-10-12 | 2011-04-26 | Google Inc. | Entity display priority in a distributed geographic information system |
US8381121B2 (en) * | 2006-03-01 | 2013-02-19 | Microsoft Corporation | Controlling scroll speed to improve readability |
US7734412B2 (en) * | 2006-11-02 | 2010-06-08 | Yahoo! Inc. | Method of client side map rendering with tiled vector data |
US7990394B2 (en) * | 2007-05-25 | 2011-08-02 | Google Inc. | Viewing and navigating within panoramic images, and applications thereof |
US20090055087A1 (en) * | 2007-08-07 | 2009-02-26 | Brandon Graham Beacher | Methods and systems for displaying and automatic dynamic re-displaying of points of interest with graphic image |
EP2297704B1 (en) * | 2008-04-14 | 2014-04-09 | Google, Inc. | Panning using virtual surfaces |
US8194102B2 (en) * | 2008-10-06 | 2012-06-05 | Microsoft Corporation | Rendering annotations for images |
JP5088343B2 (en) * | 2009-03-30 | 2012-12-05 | 富士通株式会社 | Information browsing device |
JP2010243670A (en) * | 2009-04-02 | 2010-10-28 | Clarion Co Ltd | Map display, map image display method of map display |
US9021386B1 (en) * | 2009-05-28 | 2015-04-28 | Google Inc. | Enhanced user interface scrolling system |
US20110119578A1 (en) * | 2009-11-17 | 2011-05-19 | Schwartz Michael U | Method of scrolling items on a touch screen user interface |
JP2011127949A (en) | 2009-12-16 | 2011-06-30 | Clarion Co Ltd | Navigation apparatus and method of scrolling map image |
JP5381691B2 (en) | 2009-12-25 | 2014-01-08 | アイシン・エィ・ダブリュ株式会社 | Map display device, map display method and program |
JP5533254B2 (en) * | 2010-05-24 | 2014-06-25 | アイシン・エィ・ダブリュ株式会社 | Information display device, information display method, and program |
MX2012014258A (en) * | 2010-06-30 | 2013-01-18 | Koninkl Philips Electronics Nv | Zooming-in a displayed image. |
US8922499B2 (en) * | 2010-07-26 | 2014-12-30 | Apple Inc. | Touch input transitions |
JP2012093887A (en) * | 2010-10-26 | 2012-05-17 | Aisin Aw Co Ltd | Display device, display method and display program |
US10304066B2 (en) * | 2010-12-22 | 2019-05-28 | Facebook, Inc. | Providing relevant notifications for a user based on location and social information |
US8994755B2 (en) * | 2011-12-20 | 2015-03-31 | Alcatel Lucent | Servers, display devices, scrolling methods and methods of generating heatmaps |
-
2012
- 2012-03-28 US US13/432,042 patent/US20130257742A1/en not_active Abandoned
-
2013
- 2013-03-26 DE DE202013012455.5U patent/DE202013012455U1/en not_active Expired - Lifetime
- 2013-03-26 EP EP13769362.8A patent/EP2831870B1/en active Active
- 2013-03-26 WO PCT/US2013/033802 patent/WO2013148625A1/en active Application Filing
Also Published As
Publication number | Publication date |
---|---|
EP2831870A4 (en) | 2015-11-18 |
DE202013012455U1 (en) | 2016-11-25 |
US20130257742A1 (en) | 2013-10-03 |
WO2013148625A1 (en) | 2013-10-03 |
EP2831870B1 (en) | 2019-05-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR102196401B1 (en) | Electronic map interface | |
EP2786352B1 (en) | Method and system for displaying panoramic imagery | |
KR101804602B1 (en) | 3d layering of map metadata | |
US7461345B2 (en) | System and method for displaying information using a compass | |
US8874366B2 (en) | First waypoint distance | |
US9285234B1 (en) | Automatic display of trip information in a mapping application | |
US9361283B2 (en) | Method and system for projecting text onto surfaces in geographic imagery | |
US8754907B2 (en) | Range of focus in an augmented reality application | |
US8706415B2 (en) | Changing emphasis of list items in a map navigation tool | |
US9163951B2 (en) | Optional re-routing | |
JP2017536527A (en) | Providing in-navigation search results that reduce route disruption | |
EP2831870B1 (en) | Method and system for controlling imagery panning based on displayed content | |
TW200821874A (en) | Popularity based geographical navigation | |
JP2015503802A (en) | System and method for displaying information local to a selected area | |
JP2009515191A (en) | Local search and mapping for mobile devices | |
JP3660287B2 (en) | Map data distribution device, map data reception device, map data distribution method, and map data reception method | |
US20140285526A1 (en) | Apparatus and method for managing level of detail contents | |
JP2019527336A (en) | Providing navigation instructions | |
US20150290543A1 (en) | Device, game and methods therefor |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20141027 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAX | Request for extension of the european patent (deleted) | ||
RA4 | Supplementary search report drawn up and despatched (corrected) |
Effective date: 20151020 |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G09G 5/34 20060101ALN20151014BHEPIpc: G06F 3/0485 20130101AFI20151014BHEPIpc: G01C 21/36 20060101ALN20151014BHEP |
|
17Q | First examination report despatched |
Effective date: 20160704 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R079Ref document number: 602013055081Country of ref document: DEFree format text: PREVIOUS MAIN CLASS: G09G0005000000Ipc: G06F0003048500 |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06F 3/0485 20130101AFI20181001BHEPIpc: G09G 5/34 20060101ALN20181001BHEPIpc: G01C 21/36 20060101ALN20181001BHEPIpc: G06F 3/0488 20130101ALI20181001BHEP |
|
INTG | Intention to grant announced |
Effective date: 20181017 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EPRef country code: ATRef legal event code: REFRef document number: 1131233Country of ref document: ATKind code of ref document: TEffective date: 20190515 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602013055081Country of ref document: DERef country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20190508 |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG4D |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190908Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190808Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190809Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190808Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1131233Country of ref document: ATKind code of ref document: TEffective date: 20190508 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602013055081Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: TRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508 |
|
26N | No opposition filed |
Effective date: 20200211 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20200331 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20200326 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20200326Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20200331Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20200331 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20200331 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190508Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190908 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20230327Year of fee payment: 11 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230510 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: DEPayment date: 20240327Year of fee payment: 12Ref country code: GBPayment date: 20240327Year of fee payment: 12 |