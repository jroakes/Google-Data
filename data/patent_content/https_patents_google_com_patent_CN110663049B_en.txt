CN110663049B - Neural Network Optimizer Search - Google Patents
Neural Network Optimizer Search Download PDFInfo
- Publication number
- CN110663049B CN110663049B CN201880034697.0A CN201880034697A CN110663049B CN 110663049 B CN110663049 B CN 110663049B CN 201880034697 A CN201880034697 A CN 201880034697A CN 110663049 B CN110663049 B CN 110663049B
- Authority
- CN
- China
- Prior art keywords
- neural network
- controller
- output sequence
- training
- sub
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 201
- 238000012549 training Methods 0.000 claims abstract description 85
- 238000000034 method Methods 0.000 claims abstract description 63
- 238000003860 storage Methods 0.000 claims abstract description 10
- 230000006870 function Effects 0.000 claims description 34
- 230000008569 process Effects 0.000 claims description 25
- 238000012545 processing Methods 0.000 claims description 17
- 230000000306 recurrent effect Effects 0.000 claims description 15
- 238000009826 distribution Methods 0.000 claims description 7
- 230000002787 reinforcement Effects 0.000 claims description 6
- 238000005457 optimization Methods 0.000 claims description 5
- 238000011156 evaluation Methods 0.000 claims description 4
- 230000004044 response Effects 0.000 claims description 4
- 238000005070 sampling Methods 0.000 claims 1
- 238000004590 computer program Methods 0.000 abstract description 15
- 238000010200 validation analysis Methods 0.000 description 11
- 230000001537 neural effect Effects 0.000 description 9
- 230000009471 action Effects 0.000 description 6
- 238000004891 communication Methods 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 6
- 230000015654 memory Effects 0.000 description 6
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 238000005259 measurement Methods 0.000 description 3
- 238000007796 conventional method Methods 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000004913 activation Effects 0.000 description 1
- 238000001994 activation Methods 0.000 description 1
- 238000000137 annealing Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 230000000295 complement effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000000737 periodic effect Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000013518 transcription Methods 0.000 description 1
- 230000035897 transcription Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/0985—Hyperparameter optimisation; Meta-learning; Learning-to-learn
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/09—Supervised learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/092—Reinforcement learning
Abstract
Methods, systems, and apparatus, including computer programs encoded on computer storage media, for determining update rules for training a neural network. One of the methods comprises: generating a batch of output sequences using a controller neural network, each output sequence in the batch defining a respective update rule; for each output sequence in the batch: training respective instances of a sub-neural network using the update rules defined by the output sequence; evaluating the performance of the trained instance of the sub-neural network on the particular neural network task to determine a performance metric of the trained instance of the sub-neural network on the particular neural network task; and adjusting the current value of the controller parameter of the controller neural network using the performance metric of the trained instance of the sub-neural network.
Description
Technical Field
The present description relates to training neural networks.
Background
Neural networks are machine learning models that employ one or more layers of nonlinear units to predict output for a received input. Some neural networks include one or more hidden layers in addition to the output layer. The output of each hidden layer is used as input to the next layer in the network, the next hidden layer or output layer. Each layer of the network generates an output from the received input according to the current value of the corresponding set of parameters.
Some neural networks are recurrent neural networks. A recurrent neural network is a neural network that receives an input sequence and generates an output sequence from the input sequence. In particular, the recurrent neural network may use some or all of the internal states of the network that are relied upon from the previous time step in calculating the output at the current time step. An example of a recurrent neural network is a Long Short Term (LSTM) neural network that includes one or more LSTM memory blocks. Each LSTM memory block may include one or more cells, each cell including an input gate, a forget gate, and an output gate that allow the cell to store a previous state of the cell, e.g., for generating current activations or other components provided to the LSTM neural network.
Disclosure of Invention
The present specification describes a manner in which a system implemented as a computer program on one or more computers in one or more locations may use a controller neural network to determine update rules for training the neural network to perform particular neural network tasks.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
During training of the neural network, the values of the parameters of the neural network are updated at each training iteration using the iteratively calculated gradients. More conventional techniques use predetermined update rules to determine the manner in which to update parameter values using the current gradient. In another aspect, the described system determines update rules tailored to a particular task of training a neural network. In particular, by utilizing a recurrent neural network purportedly capable of predicting valid update rules and then evaluating the validity of these rule update predictions, the described system is able to efficiently determine update rules tailored to a particular task. By using the determined update rules during the actual training of the neural network, the training process becomes more efficient because the updates are applied more accurately. This may have the following effect: (i) The training process uses less computational resources, e.g., processing power and memory, because the process requires fewer iterations to complete; (ii) Because updates are applied more accurately, the training process creates a neural network with improved performance for a particular task; or (iii) both effects.
Thus, the system can efficiently and automatically, i.e., without requiring user intervention, select and update rules for neural networks that will result in high performance training for a particular task.
Because the system trains the controller neural network to determine the update rules through reinforcement learning, the system can efficiently explore a wide range of possible update rules to identify update rules that are applicable to a particular task.
In addition, since the update rules are defined by the output of the controller neural network in such a way that the controller neural network generates strings of a domain-specific language that define the update rules, the system can efficiently explore the range of possible update rules in a computationally efficient manner. In particular, because the string defines an update rule from the perspective of a set of primitive operations, i.e., is an operand that has been found to be an output of some primitive operations that are effective when used as part of a conventional update rule, the system can efficiently explore possible combinations of such primitive operations in a small number of iterations.
Furthermore, evaluation of the candidate update rules generated by the system may be parallelized, thereby reducing the time required to determine valid update rules for the task.
For example, different image classification tasks require different types of images to be separated into different types of object categories. Update rules that are valid for one image classification task may not be valid for another task. For example, one image classification task may be simpler and therefore learn efficiently with a large update, i.e., a large step size, at each training iteration that depends greatly on the current gradient, while another image classification task may be more complex, requiring a smaller update that depends on the moving average of the nearest gradient, rather than just on the current gradient. The described system is able to determine different update rules for two different image classification tasks, which ensures that the neural network can be trained efficiently for the tasks.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example neural optimizer search system.
Fig. 2 is a diagram of an example of a controller neural network that generates an output sequence.
FIG. 3 is a flow chart of an example process for updating a current value of a controller parameter.
Like reference symbols and designations in the various drawings indicate like elements.
Detailed Description
The present specification describes a system implemented as a computer program on one or more computers in one or more locations that uses a controller neural network to determine update rules for updating values of parameters of a particular neural network during training of the particular neural network to perform a particular neural network task. The update rule is also referred to as an optimizer for training the first neural network.
Specifically, to train the neural network to perform a particular neural network task, the training system uses an iterative training process. In each iteration of the training process, the training system determines a gradient relative to the neural network parameter value, uses the gradient to determine an update to the current value of the parameter using the update rule, then applies, i.e., adds or subtracts, the update to the current value of the parameter to generate an updated parameter value, and then uses the updated parameter value in the next iteration. The update rules determine how to use the gradient (and, in some cases, the gradient or other number determined in the previous iteration) with the current iteration to generate an update for the current iteration.
This specification describes techniques for determining update rules in a manner tailored to a particular neural network task.
The neural network to be trained may be configured to perform any suitable neural network task, i.e., to receive any kind of digital data input and generate any kind of score, classification, or regression output based on the input.
For example, if the input to the neural network is an image or a feature that has been extracted from an image, the output generated by the neural network for a given image may be a score for each of a set of object classes, each score representing an estimated likelihood that the image contains an image of an object belonging to that class.
As another example, if the input to the neural network is an internet resource (e.g., a web page), a document, or a portion of a document, or a feature extracted from an internet resource, a document, or a portion of a document, the output generated by the neural network for a given internet resource, document, or portion of a document may be a score for each topic in a set of topics, each score representing an estimated likelihood that the internet resource, document, or portion of a document is related to the topic.
As another example, if the input to the neural network is characteristic of the impression context of the particular advertisement, the output generated by the neural network may be a score that represents an estimated likelihood that the particular advertisement will be clicked.
As another example, if the input to the neural network is a characteristic of a personalized recommendation for the user, e.g., a characteristic that characterizes the context of the recommendation, e.g., a characteristic that characterizes a previous action taken by the user, the output generated by the neural network may be a score for each of a set of content items, each score representing an estimated likelihood that the user will respond favorably to the recommended content item.
As another example, if the input to the neural network is a sequence of text in one language, the output generated by the neural network may be a score for each of a set of text in another language, each score representing an estimated likelihood that the text in the other language is correctly translated into the input text in the other language.
As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may be a score for each text in a set of texts, each score representing an estimated likelihood that the text is a correct transcription of the utterance.
FIG. 1 illustrates an example neural optimizer search system 100. The neural optimizer search system 100 is an example of a system implemented as a computer program on one or more computers located at one or more locations where the systems, components, and techniques described below may be implemented.
The neural optimizer search system 100 is a system that receives a request to determine update rules for training a particular neural network to perform a particular task. As part of this request, the neural optimizer search system 100 obtains training data 102 for training the neural network to perform a particular task and a validation set 104 for evaluating the performance of the particular neural network on the particular task.
The neural optimizer search system 100 uses the training data 102 and the validation set 104 to determine optimization update rules tailored to train the neural network to perform a particular task. As described above, the update rule specifies how the gradient calculated during an iteration of the neural network training process is to be used to update the current value of the parameter of the neural network, i.e., to update the value of the parameter of the neural network for that iteration.
Typically, both training data 102 and validation set 104 include a set of neural network inputs, and for each network input, a respective target output that should be generated by a particular neural network to perform a particular task. For example, a larger training data set may have been randomly partitioned to generate training data 102 and validation set 104.
The system 100 may receive the training data 102 and the validation set 104 in any of a variety of ways. For example, the system 100 may receive training data as an upload from a remote user of the system over a data communication network, for example, by using an Application Programming Interface (API) provided by the system 100, and randomly divide the uploaded data into training data 102 and a validation set 104. As another example, the system 100 may receive input from a user specifying which data the system 100 has maintained should be used to train the neural network, and then divide the specified data into training data 102 and validation set 104.
The neural optimizer search system 100 includes a controller neural network 110, a training engine 120, and a controller parameter update engine 130.
The controller neural network 110 is a neural network having parameters, which are referred to herein as "controller parameters," and is configured to generate an output sequence from the controller parameters. Each output sequence generated by the controller neural network 110 defines a candidate update rule for training the neural network.
Specifically, each output sequence expresses a formula of the candidate update rule as a character string of the domain-specific language. That is, the output sequence is an ordered set of characters from a vocabulary of characters from a domain-specific language that collectively define the candidate update rules. In some embodiments, the string describes a plurality of operands and one or more functions to be applied to the operands in a domain specific language. How the domain-specific language and the output of the neural network define the candidate update rules will be described in more detail below with reference to fig. 2.
In general, the system 100 determines the optimized update rules by training the controller neural network 110 to adjust the values of the controller parameters.
Specifically, during iterations of the controller training process, the system 100 generates a batch of sequences 112 using the controller neural network 110 according to the current values of the controller parameters.
For each output sequence in the batch 112, the training engine 120 trains instances of the sub-neural network according to candidate update rules defined by the output sequences on the training data 102 and evaluates the performance of the trained instances on the validation set 104.
That is, for a given output sequence, the training engine 120 trains instances of the sub-neural network on the training data 102, and during training, updates values of parameters applied to the sub-neural network using candidate update rules defined by the given output sequence.
A sub-neural network is a neural network configured to perform a particular neural network task. In some embodiments, the sub-neural network is a particular neural network, i.e., has the same architecture as the particular neural network. However, in other embodiments, the sub-neural network is a neural network having fewer parameters than the neural network. That is, the system 100 may use a relatively computationally simple sub-neural network, i.e., a sub-neural network having fewer layers than a particular neural network, to determine the optimization update rules. This ensures that the process of determining an optimal update does not use excessive computing resources even if a large number of candidate update rules are evaluated before the optimal update rules are determined.
The training engine 120 trains each instance for a fixed number of training periods before evaluating the performance of the instance. Specifically, the training engine 120 trains a smaller number of time periods per instance relative to the number of time periods typically required to train a particular neural network to accomplish a particular task. For example, the training engine 120 may train five or ten time periods per instance. Such a number of epochs may be suitable for determining the validity of the candidate update rules while keeping training relatively computationally efficient. This also ensures that the process of determining an optimal update does not use excessive computing resources even if a large number of candidate update rules are evaluated before the optimal update rules are determined.
The controller parameter update engine 130 then uses the results of the evaluation of the output sequences in the batch 112 to update the current values of the controller parameters to improve the expected performance of the trained sub-neural network when trained in accordance with the update rules defined by the output sequences generated by the controller neural network 110 for a particular task. Evaluating the performance of the trained instance and updating the current values of the controller parameters is described in more detail below with reference to FIG. 3.
By repeatedly updating the values of the controller parameters in this manner, the system 100 can train the controller neural network 110 to generate an output sequence defining update rules that, when used in the training process, will result in a sub-neural network with improved performance for a particular task, i.e., maximizing the expected accuracy of the trained sub-network to the validation set 104.
Once the controller neural network 110 has been trained, the system 100 may select the update rule that performs best for the validation set 104 as the optimized update rule, or may generate a new output sequence based on the training values of the controller parameters and use the update rule defined by the new output sequence as the optimized update rule.
The neural network search system 100 may then output update rule data 150 specifying the optimized update rule. For example, the neural network search system 100 may output the updated rule data 150 to the user submitting the training data.
In some implementations, instead of or in addition to outputting the update rule data 150, the system 100 trains a particular neural network using optimized update rules, e.g., from scratch, or if the particular neural network to be trained has the same architecture as the sub-neural network, trims parameter values generated as a result of training instances of the sub-neural network using the optimized update rules, and then uses the trained neural network to process requests received by a user, e.g., through an API provided by the system. That is, the system 100 may receive an input to be processed, process the input using a trained neural network, and provide an output generated by the trained neural network or data derived from the generated output in response to the received input.
In some embodiments, the system 100 trains the controller neural network in a distributed manner. That is, the system 100 evaluates a plurality of candidate update rules distributed in parallel across a plurality of different worker computing units, the rules configured such that they may operate independently of one another. In some implementations, for example, because the computing units share some resources, only partial independence of the operations is achieved. The computing unit may be, for example, a computer, a core within a computer having multiple cores, or other hardware or software capable of independently performing the calculations required to evaluate the performance measurements within the computer.
In some of these embodiments in which training is distributed, each worker computing unit maintains a copy of the controller neural network and has a dedicated training engine that generates performance metrics for a batch output sequence of copy outputs and a dedicated controller parameter update engine that uses the performance metrics to determine updates to the controller parameters.
Once the controller parameter update engine determines the update, the controller parameter update engine may send the update to a central parameter update server that is accessible to all of the controller parameter update engines. The central parameter update server may update the values of the controller parameters maintained by the server and send the updated values to the controller parameter update engine. In some cases, each of the multiple copies and its corresponding training engine and parameter update engine may run asynchronously with each other training engine and parameter update engine set.
In other of these embodiments, the system 100 includes a single controller neural network, and each worker computing unit evaluates only the output sequence. Specifically, the system 100 maintains a queue of output sequences. When the controller neural network generates an output sequence, the system 100 adds the output sequence to the queue. When a worker computation unit becomes available, i.e., the evaluation of an existing output sequence is completed, system 100 uses the worker computation unit that has become available to evaluate a candidate sequence that is first in the queue. After the worker computing unit has completed evaluating the candidate sequence, the system 100 designates the worker computing unit as available again.
Fig. 2 is a diagram 200 of an example of a controller neural network 110 generating an output sequence.
Typically, each output sequence is a string in a domain-specific language. That is, the output sequence includes a respective character at each of a plurality of time steps. The characters of a given time step are selected from a set of possible characters of the time step according to a grammar defined by a domain-specific language. That is, each time step corresponds to a portion of the formula for updating the rule, and the grammar defines for each time step the characters that may be used by that time step, such that the string of characters defines the valid formula for updating the rule.
In the example of fig. 2, the syntax of the formula is such that in a given training iteration, the formula to be applied to the current value of the parameter, i.e. the update aw added to or subtracted from the current value of the parameter, can be expressed as:
Δw＝λ*b(u 1 (op 1 )，u 2 (op 2 ))
where λ is the learning rate (the value of which is not determined by the system but can be selected during training using conventional techniques), b is a binary function selected from a set of possible binary functions, u 1 Is a unitary function selected from a set of possible unitary functions, u 2 Is another unitary function selected from a group of possible unitary functions, op 1 And op (op) 2 Are operands of a first and a second unitary function, respectively, selected from a set of possible operands. The binary function, the unitary function, and the operands are each defined by a character at each position in the output sequence.
Thus, the update rule is represented by a string comprising: a first operand to be selected at time step 1), a second operand to be selected at time step 2), a unitary function to be applied to the first operand at time step 3), a unitary function to be applied to the second operand at time step 4), and a binary function to be applied to combine the outputs of the unitary functions at time step 5).
In some cases, the string includes a single iteration of these 5 time steps, and after the string defines the binary function to be applied, the grammar indicates that the output of the binary function multiplied by the learning rate generates an update to the current value of the parameter.
In other cases, the string includes multiple iterations of the 5 time steps, and the grammar defines adding the output of the binary function of one iteration to the set of possible operands for use by future iterations, and may be used as one of the operands in any future iteration. In these cases, the output of the binary function for the final iteration is multiplied by the learning rate to generate an update to the current value of the parameter.
The operands, unitary functions, and binary functions may include some or all of those specified in table 1 below (where g is the gradient in the current training iteration):
operand: drop (x, 0.5) and sign (x)
A univariate function that maps the input x to:
clip(x，10 -3 ) Drop (x, 0.1), drop (x, 0.3), drop (x, 0.5) and sign (x)
Binary function, which maps (x, y)And (3) shooting: x+y (addition), x-y (subtraction), x y (multiplication),(division), x y (exponentiation) and x (reservation)
Here the number of the elements is the number,g, g 2 And g 3 Moving averages of running indexes of (a), respectively having attenuation rates of beta 1 、β 2 And beta 3 Drop (|p) sets its input to 0, probability to p, and clip (|l) cuts its input to [ -l, l]. All operations are performed on an element basis.
TABLE 1
Adam and RMSProp are update rules used by Adam and RMSProp optimizers, respectively.
As can be seen from Table 1, the operands include the outputs of certain primitive operations that have been found to be useful in determining updates to parameters such as running exponent moving average and Adam and RMSProp update rules. By utilizing these primitive operations as part of a domain-specific language, the number of iterations required to train the controller neural network to generate an effective update rule may be reduced.
In some cases, the operands also include some or all of the decaying operands in table 2. The decaying operand is an operand that varies based on the training iteration, i.e. depending on how many iterations have been completed before the current iteration.
Linear attenuation:
periodic decay:
restart decay:push in loshchlov & Hutter (2017)Breaking of the wire
Annealing noise: e-shaped article t ～N(0，1/(1+t) 0.55 )
Where T is the current training complement step size, T is the total number of training steps and n is the hyper-parameter controlling the number of time periods in the period decay. Note that the number of the components to be processed, Corresponding to cosine decay, no restart (loshchlov&Hutter, 2017), abbreviated as cd.
TABLE 2
When the string comprises a plurality of iterations of the above-described time steps 1-5, the iteration operands after the first iteration further comprise the output of the binary function calculated in the previous iteration.
The diagram 200 depicts the processing performed by the controller neural network 110 for seven example time steps 202-214 during generation of an output sequence. As described above, each of the seven time steps 202-214 corresponds to a different portion of the formula updating the rule. That is, the value at each time step is a character in the domain-specific language used by the system to represent the formula of the update rule.
The controller neural network 110 is a recurrent neural network that includes an embedded layer, layer 220, and one or more recurrent neural network layers, such as layer 230.
The embedding layer is configured to receive as input data identifying a character selected at a previous time step in a given output sequence and process the input to generate an embedding of the character in an embedding space. Embedding is an ordered set of digital values, such as a vector of floating points or quantized floating point values. For example, the data identifying the character may be a one-hot encoding of the character, and the embedding may be a dense vector in a continuous embedding space.
One or more recurrent neural network layers receive the embeddings as input and process the input to update the current hidden state of the recurrent neural network. For example, the recursion layer in the controller neural network 110 may be a Long Short Term Memory (LSTM) layer or a Gated Recursion Unit (GRU) layer.
In the example of FIG. 2, under time step 208, layers 220 and 230 receive as input the characters from the previous time step 206 and update the hidden state of layer 230 from time step 206 to generate as output updated hidden state 232.
The controller neural network 110 also includes a respective output layer for each time step in the output sequence, such as output layers 242-254 for time steps 202-214, respectively. Each output layer is configured to receive output layer inputs including updated hidden states at time steps and to generate outputs for the time steps that define a fractional distribution of possible characters over the time steps. For example, each output layer may first project the output layer input to the appropriate dimension for the number of possible values of the time step, and then apply softmax to the projected output layer input to generate a respective score for each of the plurality of possible values of the character at the time step.
In the example of fig. 2, output layer 242 is configured to generate a respective score for each operand in the set of possible operands at time step 202, output layer 244 is configured to generate a respective score for each operand in the set of possible operands at time step 204, output layer 246 is configured to generate a respective score for each of the set of possible unitary functions at time step 206, output layer 248 is configured to generate a respective score for each of the set of possible unitary functions at time step 208, and output layer 250 is configured to generate a respective score for each of the set of possible binary functions at time step 210. Thus, the set of five time steps corresponds to a single iteration of time steps 1-5 described above. The next iteration begins at time step 212 and output layer 252 is configured to generate a respective score for each operand in the set of possible operands of time step 212, which will typically include the output of the binary function selected in time step 210.
Thus, to generate the value for a given time step in the output sequence, the system 100 provides the character value at the previous time step in the output sequence as an input to the controller neural network, and the controller neural network generates an output for the time step that defines a fractional distribution of possible characters over the time step. For the first time step in the output sequence, because there is no previous time step, the system 100 may instead provide a predetermined placeholder input. The system 100 then samples from the possible values according to the score distribution to determine the value of the character in the output sequence at the time step.
Typically, the number of characters to be included in each output sequence is fixed before the sequence is generated, i.e. the number of iterations of time steps 1-5 to be included in the character string is fixed before the sequence is generated. In some embodiments, each output sequence generated during training has the same number of characters.
In other embodiments, the system uses a schedule that increases the number of iterations in the output sequence as training progresses.
FIG. 3 is a flow chart of an example process 300 for updating current values of controller parameters. For convenience, process 300 is described as being implemented by a system of one or more computers located in one or more locations. For example, a neural optimizer search system, such as the neural optimizer search system 100 of FIG. 1, if properly programmed, may perform the process 300.
The system may repeat the process 300 to train the controller neural network, i.e., determine training values for the controller parameters from initial values for the controller parameters.
The system uses the controller neural network and generates a batch of output sequences from the current values of the controller parameters until the iteration (step 302). Each output sequence in the batch defines a respective candidate update rule. In particular, because, as described above, the system samples from the fractional distribution as it is generating each value in the output sequence, the sequences in the batch will typically differ even though they are all generated from the same controller parameter value. The batch typically includes a predetermined number of output sequences, such as four, five, ten, thirty, or sixty sequences.
For each output sequence in the batch, the system trains an instance of the sub-neural network using the update rules defined by the output sequence to perform a particular neural network task (step 304). That is, for each output sequence in the batch, the system trains the received training data to perform a particular neural network task using conventional machine learning training techniques appropriate for the task, for example, with back-propagation or random gradient descent over time. During training, the system updates the parameter values of the instances using the update rules defined by the output sequence. As described above, in some embodiments, the system parallelizes training of the sub-neural network to reduce the overall training time of the controller neural network.
For each output sequence in the batch, the system evaluates the performance of the particular neural network task for the corresponding training instance of the sub-neural network to determine a performance metric for the training instance for the particular neural network task (step 306). For example, the performance metric may be the accuracy of the trained instance to the validation set, as measured by appropriate accuracy measurements. For example, the accuracy may be a confusion measurement when the output is a sequence, or may be a classification error rate when the task is a classification task. As another example, the performance metric may be an average or maximum value of instance accuracy for each of the last two, five, or ten epochs of the instance training.
The system uses the performance metrics of the trained instance to adjust the current values of the controller parameters (step 308).
Specifically, the system trains the controller neural network using reinforcement learning techniques to generate output sequences that result in the sub-neural network having an increased performance metric to adjust the current value. More specifically, the system trains the controller neural network to generate an output sequence that maximizes the received rewards, which are determined based on the performance metrics of the trained instances. Specifically, rewards for a given output sequence are a function of performance metrics of the trained instance. For example, the reward may be one of: performance metrics, squares of performance metrics, cubes of performance metrics, square roots of performance metrics, and the like.
In some cases, the system trains the controller neural network using strategic gradient techniques to maximize the expected rewards. For example, the policy gradient technique may be a REINFORCE technique or a near-end policy optimization (PPO) technique. For any technique, the system may use the moving average of the indices of the previous rewards as a baseline to stabilize the training.
The present specification uses the term "configuration" in both systems and computer program components. For a system of one or more computers to be configured to perform particular operations or actions, it is meant that the system has installed thereon software, firmware, hardware, or a combination thereof that, when executed, would cause the system to perform the operations or actions. By one or more computer programs to be configured to perform particular operations or actions, it is meant that the one or more programs comprise instructions which, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments and functional operations of the subject matter described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on a manually-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all apparatus, devices, and machines for processing data, including: by way of example, a programmable processor, a computer, or a plurality of processors or computers. The device may also be or may further comprise a dedicated logic circuit, for example an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the device may optionally include code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program can be written in any form of programming language, and can also be referred to or described as a program, software application, app, module, software module, script, or code, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. The program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a markup language document, or in a single file dedicated to the program in question, or in multiple coordinated files, such as files that store one or more modules, sub-programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of data: the data need not be structured in any particular way or structured at all, and may be stored in storage at one or more locations. Thus, for example, an index database may include multiple data sets, each of which may be organized and accessed differently.
Similarly, in this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers at one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or combination of, special purpose logic circuitry, e.g., an FPGA or ASIC, and one or more programmed computers.
A computer suitable for executing a computer program may be based on a general purpose microprocessor or a special purpose microprocessor or both, or any other type of central processing unit. Typically, the central processing unit receives instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or executing instructions, and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such a device. In addition, the computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including: for example, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices, magnetic disks such as internal hard disks or removable disks, magneto-optical disks, CD-ROM disks, and DVD-ROM disks.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having: a display device for displaying information to a user, for example, a CRT (cathode ray tube) or LCD (liquid crystal display) monitor; and a keyboard and pointing device, such as a mouse or trackball, by which a user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form including acoustic input, speech input, or tactile input. In addition, the computer may interact with the user by sending documents to and receiving documents from a device used by the user, such as by sending web pages to a web browser on the user's device in response to requests received from the web browser. Moreover, the computer may interact with the user by sending text messages or other forms of messages to a personal device, such as a smart phone running a messaging application, and receiving response messages from the user.
The data processing apparatus for implementing the machine learning model may also include, for example, a dedicated hardware accelerator unit for processing the common and computationally intensive portions of the machine learning training or production (i.e., reasoning) workload.
The machine learning model may be implemented and deployed using a machine learning framework such as a TensorFlow framework, microsoft Cognitive Toolkit framework, apache Single framework, or Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data processor), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification), or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data, such as HTML pages, to the user device, such as for displaying data to and receiving user input from a user interacting with the device (acting as a client). Data generated at the user device, such as results of a user interaction, may be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, while a feature may have been described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination. And the required combinations may be directed to sub-combinations or variations of sub-combinations.
Also, although operations are illustrated in the drawings and described in the claims, it should not be construed that such operations need to be performed in the particular order described or in sequential order, or that all illustrated operations need be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the present subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying drawings do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (19)
1. A method of optimizing a particular neural network for performing a particular neural network task, wherein an input to the particular neural network is an image or a feature that has been extracted from the image, and an output of the particular neural network is a score for each object class in a set of object classes of the image, each score representing an estimated likelihood that the image contains an image of objects belonging to a class, the method comprising:
receiving a request to determine an update rule for updating a value of a parameter of the particular neural network during training of the particular neural network to perform the particular neural network task; and
determining the update rule includes:
generating a plurality of output sequences using a controller neural network having a plurality of controller parameters and in accordance with current values of the plurality of controller parameters, each generated output sequence defining a respective candidate update rule and formulating the candidate update rule into a string of a domain-specific language, wherein the controller neural network is a recurrent neural network comprising an embedding layer and one or more recurrent neural network layers, wherein the string of characters comprises a respective character at each of a plurality of time steps, and wherein the character at each time step is selected from a set of possible characters for the time step in accordance with a syntax defined by the domain-specific language;
For each generated output sequence:
training respective instances of the sub-neural network to perform the particular neural network task using training data for the particular neural network task by updating values of parameters of the instances of the sub-neural network according to the candidate update rules defined by the generated output sequence, an
Evaluating the performance of the trained instance of the sub-neural network on the particular neural network task to determine a performance metric of the trained instance of the sub-neural network on the particular neural network task; and
adjusting the current values of the plurality of controller parameters of the controller neural network using the performance metrics of the trained instances of the sub-neural network; and
based on the adjusted values of the plurality of controller parameters, a final output sequence is generated that defines a final update rule for training the particular neural network to perform the particular neural network task.
2. A method of optimizing a particular neural network for performing a particular neural network task, wherein an input to the particular neural network is a text sequence in a language, the method comprising:
Receiving a request to determine an update rule for updating a value of a parameter of the particular neural network during training of the particular neural network to perform the particular neural network task; and
determining the update rule includes:
generating a plurality of output sequences using a controller neural network having a plurality of controller parameters and in accordance with current values of the plurality of controller parameters, each generated output sequence defining a respective candidate update rule and formulating the candidate update rule into a string of a domain-specific language, wherein the controller neural network is a recurrent neural network comprising an embedding layer and one or more recurrent neural network layers, wherein the string of characters comprises a respective character at each of a plurality of time steps, and wherein the character at each time step is selected from a set of possible characters for the time step in accordance with a syntax defined by the domain-specific language;
for each generated output sequence:
training respective instances of the sub-neural network to perform the particular neural network task using training data for the particular neural network task by updating values of parameters of the instances of the sub-neural network according to the candidate update rules defined by the generated output sequence, an
Evaluating the performance of the trained instance of the sub-neural network on the particular neural network task to determine a performance metric of the trained instance of the sub-neural network on the particular neural network task; and
adjusting the current values of the plurality of controller parameters of the controller neural network using the performance metrics of the trained instances of the sub-neural network; and
based on the adjusted values of the plurality of controller parameters, a final output sequence is generated that defines a final update rule for training the particular neural network to perform the particular neural network task.
3. The method of claim 1 or 2, wherein the sub-neural network has the same architecture as the specific neural network.
4. The method of claim 1 or 2, wherein the sub-neural network is a neural network configured to perform the particular neural network task but having fewer parameters than the particular neural network.
5. The method of claim 1 or 2, wherein adjusting the current values of the plurality of controller parameters of the controller neural network using the performance metrics of the trained instance of the sub-neural network comprises:
Training the controller neural network using reinforcement learning techniques generates an output sequence that results in a sub-neural network having an increased performance metric.
6. The method of claim 5, wherein the reinforcement learning technique is a strategy gradient technique.
7. The method of claim 6, wherein the reinforcement learning technique is a REINFORCE technique.
8. The method of claim 6, wherein the reinforcement learning technique is a trust zone policy optimization technique.
9. The method of claim 8, wherein the trust zone policy optimization technique uses an exponential moving average of previous rewards as a baseline function.
10. The method of claim 1 or 2, wherein the string describes a plurality of operands and one or more functions applied to the plurality of operands in the domain-specific language.
11. The method of claim 10, wherein the operand is selected from a set of input primitives, and wherein the set of input primitives includes one or more primitives that depend on gradients relative to parameters determined during training.
12. The method of claim 10, wherein at least one of the functions takes as input an output of another of the functions.
13. The method of claim 1 or 2, wherein the embedding layer is configured to, for a given output sequence and at each time step, receive as input data identifying a character selected at a previous time step in the given output sequence and process the input to generate an embedding of the character in an embedding space, and wherein the one or more recurrent neural network layers are configured to, for the given output sequence and at the time step:
receiving the embedding as input and processing the embedding to update a current hidden state of the recurrent neural network; and
wherein the controller neural network further comprises a respective output layer for each time step, wherein each output layer is configured to, for the given output sequence:
an output layer input is received that includes updated hidden states at the time step, and an output is generated for the time step that defines a fractional distribution among possible characters for the time step.
14. The method of claim 13, wherein generating the plurality of output sequences comprises, for each output sequence and for each time step of the plurality of time steps:
Providing the characters in the output sequence at a preceding time step as input to the controller neural network to generate an output for the time step, the output defining a fractional distribution among possible characters for the time step; and
sampling from the possible characters according to the score distribution to determine the characters in the output sequence at the time step.
15. The method of claim 1 or 2, further comprising:
the final update rule is used to train the particular neural network to perform the particular neural network task.
16. The method of claim 1 or 2, wherein the sub-neural network trains less training periods than the particular neural network.
17. The method of claim 1 or 2, further comprising: for each output sequence:
adding the generated output sequence to a queue of output sequences;
determining that a worker computing unit of the plurality of worker computing units has become available;
determining that the generated output sequence is the first of the queues;
in response, removing the output sequence from the queue and performing the training and the evaluating on the output sequence using the worker computing unit that has become available; and
After the training and evaluation has been completed, the worker computing unit is designated as being available again.
18. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the operations of the respective method of any one of claims 1 to 17.
19. One or more computer-readable storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the respective method of any one of claims 1-17.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762492021P | 2017-04-28 | 2017-04-28 | |
US62/492,021 | 2017-04-28 | ||
PCT/US2018/030281 WO2018201151A1 (en) | 2017-04-28 | 2018-04-30 | Neural network optimizer search |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110663049A CN110663049A (en) | 2020-01-07 |
CN110663049B true CN110663049B (en) | 2023-12-26 |
Family
ID=62567722
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880034697.0A Active CN110663049B (en) | 2017-04-28 | 2018-04-30 | Neural Network Optimizer Search |
Country Status (4)
Country | Link |
---|---|
US (2) | US10922611B2 (en) |
EP (1) | EP3602419B1 (en) |
CN (1) | CN110663049B (en) |
WO (1) | WO2018201151A1 (en) |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11386327B2 (en) * | 2017-05-18 | 2022-07-12 | Salesforce.Com, Inc. | Block-diagonal hessian-free optimization for recurrent and convolutional neural networks |
CN108427671B (en) * | 2018-01-25 | 2021-06-25 | 腾讯科技（深圳）有限公司 | Information conversion method and apparatus, storage medium, and electronic apparatus |
US11308222B2 (en) * | 2018-03-22 | 2022-04-19 | Via Science, Inc. | Neural-network training using secure data processing |
CN110096647B (en) * | 2019-05-10 | 2023-04-07 | 腾讯科技（深圳）有限公司 | Method and device for optimizing quantization model, electronic equipment and computer storage medium |
CN111370074B (en) * | 2020-02-27 | 2023-07-07 | 北京晶泰科技有限公司 | Method and device for generating molecular sequence and computing equipment |
CN116368493A (en) * | 2020-10-15 | 2023-06-30 | 罗伯特·博世有限公司 | Method and apparatus for weight sharing neural network with random architecture |
CN112241123B (en) * | 2020-10-23 | 2022-05-03 | 南京航空航天大学 | Aeroengine acceleration control method based on deep reinforcement learning |
CN112308434A (en) * | 2020-11-03 | 2021-02-02 | 中国交通通信信息中心 | Traffic safety risk assessment method and system |
US20220202348A1 (en) * | 2020-12-31 | 2022-06-30 | X Development Llc | Implementing brain emulation neural networks on user devices |
CN113344283B (en) * | 2021-06-23 | 2023-11-28 | 国网黑龙江省电力有限公司 | Energy internet new energy consumption capability assessment method based on edge intelligence |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN103324886A (en) * | 2013-06-05 | 2013-09-25 | 中国科学院计算技术研究所 | Method and system for extracting fingerprint database in network intrusion detection |
CN105940395A (en) * | 2014-01-31 | 2016-09-14 | 谷歌公司 | Generating vector representations of documents |
CN106462801A (en) * | 2014-10-07 | 2017-02-22 | 谷歌公司 | Training neural networks on partitioned training data |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3218854B1 (en) * | 2014-11-14 | 2021-01-06 | Google LLC | Generating natural language descriptions of images |
WO2016123409A1 (en) * | 2015-01-28 | 2016-08-04 | Google Inc. | Batch normalization layers |
WO2016197054A1 (en) * | 2015-06-05 | 2016-12-08 | Google Inc. | Whitened neural network layers |
US9734436B2 (en) * | 2015-06-05 | 2017-08-15 | At&T Intellectual Property I, L.P. | Hash codes for images |
CN108027897B (en) * | 2015-07-24 | 2022-04-12 | 渊慧科技有限公司 | Continuous control with deep reinforcement learning |
KR20170118520A (en) * | 2016-04-15 | 2017-10-25 | 삼성전자주식회사 | Interface neural network |
WO2018053187A1 (en) * | 2016-09-15 | 2018-03-22 | Google Inc. | Deep reinforcement learning for robotic manipulation |
US20200104678A1 (en) * | 2018-09-27 | 2020-04-02 | Google Llc | Training optimizer neural networks |
-
2018
- 2018-04-30 EP EP18730464.7A patent/EP3602419B1/en active Active
- 2018-04-30 CN CN201880034697.0A patent/CN110663049B/en active Active
- 2018-04-30 WO PCT/US2018/030281 patent/WO2018201151A1/en unknown
-
2019
- 2019-10-24 US US16/662,924 patent/US10922611B2/en active Active
-
2021
- 2021-01-11 US US17/145,524 patent/US20210271970A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN103324886A (en) * | 2013-06-05 | 2013-09-25 | 中国科学院计算技术研究所 | Method and system for extracting fingerprint database in network intrusion detection |
CN105940395A (en) * | 2014-01-31 | 2016-09-14 | 谷歌公司 | Generating vector representations of documents |
CN106462801A (en) * | 2014-10-07 | 2017-02-22 | 谷歌公司 | Training neural networks on partitioned training data |
Non-Patent Citations (4)
Title |
---|
Evolution and Design of Distributed Learning Rules;Thomas Philip Runarsson等;《IEEE》;论文第59-63页 * |
Learning to Optimize Neural Nets;Ke Li等;《arXIV》;论文第1-10页 * |
Marcin Andrychowicz等.Learning to learn by gradient descent by gradient descent.《30th Conference on Neural Information Processing Systems (NIPS 2016),Barcelona, Spain.》.2016,论文第1-17页. * |
OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING;Sachin Ravi等;《Published as a conference paper at ICLR 2017》;论文第1-11页 * |
Also Published As
Publication number | Publication date |
---|---|
US20210271970A1 (en) | 2021-09-02 |
US20200057941A1 (en) | 2020-02-20 |
EP3602419B1 (en) | 2023-09-20 |
CN110663049A (en) | 2020-01-07 |
US10922611B2 (en) | 2021-02-16 |
EP3602419A1 (en) | 2020-02-05 |
WO2018201151A1 (en) | 2018-11-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110663049B (en) | Neural Network Optimizer Search | |
JP6790286B2 (en) | Device placement optimization using reinforcement learning | |
US11651259B2 (en) | Neural architecture search for convolutional neural networks | |
EP3711000B1 (en) | Regularized neural network architecture search | |
CN110366734B (en) | Optimizing neural network architecture | |
CN110503192A (en) | The effective neural framework of resource | |
JP7043596B2 (en) | Neural architecture search | |
US11488067B2 (en) | Training machine learning models using teacher annealing | |
US20220092416A1 (en) | Neural architecture search through a graph search space | |
US10679006B2 (en) | Skimming text using recurrent neural networks | |
CN111652378B (en) | Learning to select vocabulary for category features | |
WO2019101836A1 (en) | Population based training of neural networks | |
CN110462638B (en) | Training neural networks using posterior sharpening | |
CN118043818A (en) | Self-attention-based neural network for processing network metrics from multiple modalities |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |