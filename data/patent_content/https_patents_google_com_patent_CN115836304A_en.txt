CN115836304A - Updating trained voice robots using example-based voice robot development techniques - Google Patents
Updating trained voice robots using example-based voice robot development techniques Download PDFInfo
- Publication number
- CN115836304A CN115836304A CN202180048384.2A CN202180048384A CN115836304A CN 115836304 A CN115836304 A CN 115836304A CN 202180048384 A CN202180048384 A CN 202180048384A CN 115836304 A CN115836304 A CN 115836304A
- Authority
- CN
- China
- Prior art keywords
- voice robot
- robot
- training instances
- trained
- training
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/02—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail using automatic reactions or user delegation, e.g. automatic replies or chatbot-generated messages
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
- G06F40/35—Discourse or dialogue representation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/004—Artificial life, i.e. computing arrangements simulating life
- G06N3/006—Artificial life, i.e. computing arrangements simulating life based on simulated virtual individual or collective life forms, e.g. social simulations or particle swarm optimisation [PSO]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/10—Speech classification or search using distance or distortion measures between unknown speech and reference templates
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/225—Feedback of the input speech
Abstract
Embodiments relate to updating trained voice robots deployed for conducting conversations on behalf of third parties. Third party developers can interact with the voice robot development system, which enables the third party developers to train, update, validate and monitor the performance of the trained voice robots. In various embodiments, the trained voice robot may be updated by updating a corpus of training instances that were originally used to train the voice robot and updating the trained voice robot based on the updated corpus. In some embodiments, the corpus of training instances may be updated in response to identifying the occurrence of a behavioral error of the trained speech robot when conducting a conversation on behalf of a third party. In additional or alternative embodiments, the corpus of training instances may be updated in response to determining that the trained speech robot does not include the desired behavior.
Description
Background
Humans may conduct human-computer conversations via various computing devices with interactive software applications called "bots," "chat bots," "automatic assistants," "interactive personal assistants," "intelligent personal assistants," "session agents," and so on. As one example, these robots may initiate or answer an incoming phone call and converse with humans to perform actions on behalf of a third party. However, the functionality of these robots may be limited by the predefined intent patterns that the robot uses to perform the action. In other words, if a human being participating in a conversation with the robot provides a spoken utterance that includes an intent that is not defined by the predefined intent pattern, the robot will fail. Further, to update these robots, existing intent patterns may be modified or new intent patterns may be added. However, it may be desirable to define an almost infinite pattern of intent to make the robot robust to various nuances of human language. Such intent patterns need to be manually defined and/or manually refined by a large utilization of computing resources. Furthermore, even if a large number of intent patterns are defined, a large amount of memory is required to store and/or utilize the large number of intent patterns. Thus, the intent patterns are not practically extendable to the extent of learning nuances of human language.
Disclosure of Invention
Embodiments disclosed herein relate to updating a trained voice robot deployed for conducting a conversation on behalf of a third party via a voice robot development platform. The voice robot may correspond to one or more processors that utilize multiple Machine Learning (ML) layers of one or more ML models to conduct conversations on behalf of a third party. Third party developers may interact with the voice robot development platform to train, update, validate and monitor the performance of the trained voice robots. In various embodiments, the trained voice robot may be updated by updating a corpus of training instances that were originally used to train the voice robot and updating the trained voice robot based on the updated corpus. In some embodiments, the corpus of training instances may be updated in response to identifying the occurrence of a behavioral error of the trained speech robot when conducting a conversation on behalf of a third party. In additional or alternative embodiments, the corpus of training instances may be updated in response to determining that the trained speech robot does not include the desired behavior. In these and other ways, the voice robot development platform may guide third party developers by training, updating, and validating trained voice robots.
For example, assume that the third party associated with the trained voice robot is an imaginary restaurant entity named the Hypothetical cafe. Further assume that the trained voice robot has conducted multiple conversations with the corresponding human on behalf of the hypothetical cafe during an incoming call to the hypothetical cafe to at least make a restaurant reservation for the corresponding human. In this example, assume further that the trained voice robot confuses the value of the party-size parameter for a given restaurant reservation (e.g., "six" people) with the value of the time parameter for the given restaurant reservation (e.g., "six pm"). Such behavioral errors may result from one or more missing and/or mis-tagged features of the training instance used to train the trained voice robot associated with the hypothetical cafe. In this example, further assume that the trained voice robot fails to solicit a value for a name parameter associated with a restaurant reservation. Such behavioral errors may result from a lack of training examples for training trained voice robots that include name features. The voice robot development platform may identify the behavioral errors based on the processing session and may determine one or more actions related to correcting the behavioral errors.
In various embodiments, the sessions conducted by the trained voice robot, the session context associated with each of the sessions, the embedding generated during the corresponding session, and/or any other information associated with the corresponding session may be stored as voice robot activity in a voice robot activity database. The voice robot activities stored in the voice robot activities may be processed using multiple additional ML layers of one or more of the ML models to identify a given behavioral error of the trained voice robot. For example, one or more portions of a given corresponding session and a given session context for one or more portions of the given corresponding session can be processed using multiple additional ML layers of one or more of the ML models. For example, the response by the trained voice robot to a spoken utterance provided by a corresponding human may be processed to recognize that the trained voice robot erroneously interpreted the values of the party-scale feature of the restaurant reservation as the values of the time feature of the restaurant reservation. Furthermore, the behavioral error may be processed using additional ML layers of one or more of the ML models to classify the behavioral error as one or more different categories of behavioral errors. For example, if multiple training instances used to initially train the trained voice robot lack temporal or party-scale features, this behavioral error may be classified as a category associated with the missing feature error; if one or more of the plurality of training instances used to initially train the trained speech robot incorrectly labels a temporal feature or a party-scale feature, then this behavioral error may be classified as a category associated with the incorrectly labeled feature error; if the plurality of training instances used to initially train the trained speech robot lacks enough training instances with temporal or party-scale features, then this behavioral error may be classified as a category associated with sparsity errors; and/or this behavioral error may be classified as other categories of errors.
In some embodiments, one or more training instances predicted to cause a given behavioral error may be identified. One or more embeddings associated with a corresponding session for which a given behavioral error is identified may be obtained from a voice robot activity database. Further, a corresponding embedding associated with each of a plurality of training instances in a corpus used to initially train the trained speech robots can be obtained. One or more of the embeddings associated with the corresponding session may be compared in an embedding space with corresponding embeddings associated with each of the plurality of training instances to determine a corresponding distance metric associated with each of the plurality of training instances. The corresponding distance metric may be, for example, a cosine similarity distance, a euclidean distance, and/or other distance metrics between one or more of the embeddings and the corresponding embeddings. One or more of the training instances associated with the corresponding distance metric that satisfies the threshold may be identified as a predicted cause of the given behavioral error. For example, one or more embeddings associated with corresponding sessions in which the behavioral errors described above were identified (e.g., incorrectly interpreting values of a party-scale feature of restaurant reservations as values of a temporal feature of restaurant reservations) may be compared to corresponding embeddings associated with one or more of the training instances. Thus, training instances that include temporal features and/or party-scale features should be identified as the predicted cause of a given behavioral error.
In some embodiments, one or more actions may be determined that relate to correcting a given behavioral error. One or more actions may be determined based on one or more different categories to which a given behavior was misrecognized. For example, if a given behavioral error is classified as a category associated with a missing feature error, one or more actions related to correcting the given behavioral error may include one or more existing training instances in the markup corpus. As another example, if a given behavioral error is classified as a category associated with a mis-tagged feature error, one or more actions related to correcting the given behavioral error may include re-tagging one or more existing training instances in the corpus. As yet another example, if the given behavioral error is classified as a category associated with sparse errors, the one or more actions related to correcting the given behavioral error may include synthesizing one or more additional training instances in the corpus and/or requesting a third party developer to add additional training instances in the corpus.
In various embodiments, the voice robot development platform may automatically perform one or more of the actions related to correcting a given behavioral error of the trained voice robot. For example, in embodiments where a given behavioral error is associated with a missing feature error and/or a false labeled feature error, the speech robot development platform may label and/or re-label one or more of the training instances identified as predictive causes of the given behavioral error. For example, the speech robot development platform may perform one or more natural language understanding operations on one or more terms of the one or more recognized training instances to classify one or more of the terms as a particular category of terms (e.g., "6PM (6 PM)" associated with time) and may assign one or more tags (re-tags) to one or more of the terms to add temporal features. As another example, in an embodiment where a given behavioral error is associated with a sparsity error, the speech robot development platform may synthesize one or more additional training instances. The voice robot development platform may process voice robot activity to generate additional training instances based on conversations conducted by the trained voice robot and/or based on enhancing one or more of the training instances included in the corpus. For example, the speech robot development platform may convert one or more portions of a corresponding session and a session context of the corresponding session into one or more training instances, label features of the one or more portions of the corresponding session, and add those training instances to the corpus. Also, for example, the speech robot development platform may enhance existing training instances by changing values of features of a given training instance, changing a conversational context of the given training instance, and/or otherwise enhancing existing training instances, and adding those training instances to the corpus.
In additional or alternative embodiments, the voice robot development platform may generate a notification that prompts the third party developer to perform one or more of the actions and cause the notification to be rendered at the third party developer's client device. For example, the voice robot development platform may generate a notification that includes an indication of the given behavioral error, an indication of one or more training instances identified as predictive causes of the given behavioral error, an indication of one or more actions related to correcting the given behavioral error (and optionally whether the voice robot development platform automatically performed one or more of the actions), and/or other information related to the given behavioral error or related to correcting the given behavioral error. Third party developers can interact with the voice robot development platform to update the trained voice robot to correct a given behavioral error of the trained voice robot.
In various embodiments, and after performing one or more of the actions related to correcting the given behavioral error, the trained speech robot may be updated based on at least one or more modified training instances included in the corpus and/or one or more additional training instances added to the corpus. In particular, a plurality of ML layers of one or more of the ML models used by one or more of the processors to conduct conversations on behalf of third parties may be updated. In some embodiments, the weights of one or more of the plurality of ML models may be updated based on at least the training instances, while the weights of the one or more of the plurality of ML models may remain fixed. In some embodiments, the trained voice robot may be updated in response to receiving user input directed to updating the voice robot. In additional or alternative embodiments, the trained voice robot may be updated in response to determining that one or more conditions are satisfied. The one or more conditions may include, for example, determining a duration of time that has elapsed (e.g., nightly, weekly, and/or other durations), determining that no instances of the voice robot are implemented, determining that a number of changes to the corpus since the voice robot was last trained satisfies a number threshold, and/or other conditions. The updated voice bot may then be deployed for additional conversations on behalf of the third party.
In various embodiments, and after updating the trained voice robot and before deploying the updated voice robot, the updated voice robot may be validated. In some embodiments, the voice robot may be validated based on validation training instances that are prohibited from being utilized in the initial training and/or subsequent updating of the voice robot. In some additional or alternative embodiments, the voice bot may authenticate based on user input from a third party developer authenticating the updated voice bot directed to the voice bot development platform. An updated voice robot may be considered validated when one or more conditions are satisfied that reflect an improvement in the measurements of the updated voice robot as compared to the voice robot currently deployed for conducting a conversation on behalf of a third party. For example, the one or more conditions may include a validation of one or more of the updated plurality of ML layers or the plurality of additional ML layers when further training the voice robot, a convergence of one or more of the updated plurality of ML layers or the plurality of additional ML layers (e.g., zero loss or within a threshold range of zero loss), a determination that one or more of the plurality of ML layers or the plurality of additional ML layers performs better than an instance of the voice robot (if any) currently being utilized (e.g., with respect to accuracy and/or recall), an occurrence of training based on at least a threshold number of training instances, and/or a duration of training based on the training instances.
By using the techniques described herein, various technical advantages may be realized. As one non-limiting example, the voice robot development platform enables voice robot behavior to be easily added or modified by adding new training instances or modifying existing training instances. Thus, a voice robot trained using the voice robot development platform described herein is more scalable and reduces memory consumption since a large number of intent patterns need not be defined. In contrast, multiple ML layers of one or more of the ML models trained and utilized may have a smaller memory footprint and may be more robust and/or accurate. Furthermore, the voice robot trained using the voice robot development platform achieves a high level of accuracy and recall, enabling the session to be ended more quickly and efficiently, as the voice robot trained using the voice robot development platform is better able to understand nuances of human language and respond accordingly.
The above description is provided as an overview of only some embodiments disclosed herein. These and other embodiments are described herein in additional detail.
Drawings
Fig. 1 depicts a block diagram of an example environment in which aspects of the present disclosure may be implemented.
Fig. 2 depicts an example process flow for training a voice robot, in accordance with various embodiments.
3A, 3B, 3C, 3D, and 3E depict various non-limiting examples of user interfaces associated with a voice robot development platform, according to various embodiments.
Fig. 4 depicts a flowchart illustrating an example method of updating a trained voice robot, in accordance with various embodiments.
Fig. 5 depicts a flowchart illustrating an example method of updating a trained voice robot based on additional training instances added to a corpus of training instances used to train the voice robot, in accordance with various embodiments.
Fig. 6 depicts a flowchart illustrating an example method of updating a trained voice robot based on error labeled features and/or missing features of training examples included in a corpus of training examples used to train the voice robot, in accordance with various embodiments.
Fig. 7 depicts an example architecture of a computing device, according to various embodiments.
Detailed Description
Turning now to fig. 1, a block diagram of an example environment is depicted in which aspects of the present disclosure may be implemented and in which embodiments disclosed herein may be implemented. The client device 110 is illustrated in fig. 1 and includes, in various embodiments, a user input engine 111, a rendering engine 112, and a voice robot development system client 113. The client device 110 may be, for example, a standalone secondary device (e.g., with a microphone, speaker, and/or display), a laptop computer, a desktop computer, a tablet computer, a wearable computing device, a vehicle computing device, and/or any other client device capable of implementing the voice robot development system client 113.
The user input engine 111 may detect various types of user input at the client device 110. The user input detected at client device 110 may include spoken input detected via a microphone of client device 110, touch input detected via a user interface input device (e.g., a touch screen) of client device 110, and/or typed input detected via a user interface input device of client device 110 (e.g., via a virtual keyboard on a touch screen, a physical keyboard, a mouse, a stylus, and/or any other user interface input device of client device 110).
In various embodiments, a speech robot development systemSystem client 113 may include an Automatic Speech Recognition (ASR) engine 130A, a Natural Language Understanding (NLU) engine 140A1, and a text-to-speech (TTS) engine 150A1. In addition, the voice robot development system client 113 may be over one or more networks 199 1 (e.g., any combination of Wi-Fi, bluetooth, near Field Communication (NFC), local Area Network (LAN), wide Area Network (WAN), ethernet, the internet, and/or other networks) with the voice robot development system 120. From the perspective of a user interacting with the client device 110, the voice robot development system client 113 and the voice robot development system 120 form a logical instance of a voice robot development platform. Although the voice robot development system 120 is depicted in fig. 1 as being implemented remotely (e.g., via one or more servers) from the client device 110, it should be understood that this is for purposes of example and is not meant to be limiting. For example, the voice robot development system 120 may instead be implemented locally at the client device 110.
A third party developer (e.g., a user of the client device 110) can utilize the voice robot development platform to train a voice robot as described herein to be deployed for conducting conversations on behalf of a third party associated with the third party developer for a phone call associated with the third party. Notably, the voice robot development platform can be provided by a first party, and a third party developer can utilize the voice robot development platform to train a voice robot for a third party associated with the third party developer. As used herein, the term "first party" refers to an entity that publishes a voice robot development platform, while the term "third party" refers to an entity that is different from the entity associated with the first party and that does not publish a voice robot development system. Thus, a third party developer refers to a user that interacts with the voice robot development platform to train a voice robot associated with a third party.
The telephone calls described herein may be performed using various voice communication protocols, such as voice over internet protocol (VoIP), public Switched Telephone Network (PSTN), and/or other telephone communication protocols. As described herein, the synthesized speech may be rendered as part of the secondary phone call, which may include injecting the synthesized speech into the call such that it is perceivable by at least one of the participants of the secondary phone call. The synthesized speech may be generated and/or injected by the client device 110 as an endpoint of a given telephone call and/or may be generated and/or injected by a server connected to the telephone call (e.g., a server implementing the voice robot development system 120).
In various embodiments, the speech robot development system 120 includes an ASR engine 130A2, NLU engine 140A2, TTS engine 150A2, speech robot training engine 160, speech robot engine 170, error recognition engine 180, and session summarization engine 185. The voice robot training engine 160 may be used to train a voice robot to be deployed for conducting a conversation on behalf of a third party for a telephone call associated with the third party, and may include a training instance engine 161 and a training engine 162 in various embodiments. Further, the voice robot engine 170 can then utilize the trained voice robot to conduct a conversation on behalf of the third party for a phone call associated with the third party, and can include a response engine 171 and a Remote Procedure Call (RPC) engine 172 in various embodiments.
The training instance engine 161 may obtain a plurality of training instances for training the voice robot based on user input provided by third party developers and detected at the client device 110 via the user input engine 111. The plurality of training instances may be stored in training instance database 161A and associated with an indication of a voice robot to be trained based on the plurality of training instances. Each training instance of the plurality of training instances may include a training instance input and a training instance output. The training instance inputs may include one or more of the following: a portion of the corresponding session (e.g., audio data and/or a plurality of speech hypotheses corresponding thereto), a previous context associated with the corresponding session, an indication of an incoming telephone call, an action or command to initiate performance of an outgoing telephone call, an RPC inbound request, or one or more feature emphasized inputs. The training instance output may include one or more of the following: a ground truth response corresponding to a portion of a conversation (e.g., audio data and/or a plurality of speech hypotheses corresponding thereto), an introduction of an incoming telephone call, an initiation of an outgoing telephone call, or an RPC outbound request.
In some embodiments, one or more of the plurality of training instances may be obtained from a corpus of prior telephone calls based on user input. The third party developer may need to label one or more of the training instances from the corpus of prior telephone calls via user input. The previous phone call may include capturing audio data of a plurality of humans and/or a corresponding conversation between one human and a corresponding voice robot. Training instance engine 161 can process previous phone calls to generate one or more of the training instances. For example, assume that a previous phone call includes audio data capturing a corresponding conversation between a first type of human (e.g., a customer) and a second type of human (e.g., an employee). In this example, training instance engine 161 can identify audio data corresponding to a portion of a corresponding session associated with the customer and identify audio data corresponding to a corresponding response associated with the employee that is responsive to the portion of the corresponding session associated with the customer. The portion of the corresponding session associated with the customer may be used as part of the training instance input and the corresponding response associated with the employee may be used as part of the training instance output. Further, the previous context of the corresponding session may also be used as part of the training instance input. The previous context of the corresponding session may include previous audio data of the corresponding session (and/or a plurality of speech hypotheses corresponding thereto or recognized text corresponding thereto), metadata associated with the session (e.g., a location of the client, a time at which the corresponding telephone call was initiated, whether values for parameters have been solicited, etc.), and/or other contextual information associated with the previous telephone call.
In some versions of these embodiments, the previous phone call may be associated with a third party for whom the voice robot is being trained. For example, assume that the third party is a fictitious retail entity named fictitious market selling various products. The prior telephone call may include audio data capturing a corresponding session between a first type of human (e.g., a customer) and one or more second types of human (e.g., employees of the hypothetical market), a voice robot associated with the hypothetical market, or an Interactive Voice Response (IVR) system associated with the hypothetical market. In some additional or alternative versions of these embodiments, the previous phone call may be associated with one or more other third parties that are different from the third party for which the voice robot is being trained. In some further versions of these embodiments, previous phone calls associated with one or more other third parties obtained by training instance engine 161 may be limited to other third parties having the same type of entity as the third party for which the voice robot is being trained (e.g., a retailer entity, an airline entity, a restaurant entity, a school or university entity, a supplier entity, a shipper entity, a government entity, and/or any other type of person, place, or thing). Continuing with the above example, previous phone calls used to generate training instances of voice robots associated with a hypothetical market may be limited to those phone calls associated with other retailers and, optionally, other retailers selling the same or similar products.
In additional or alternative embodiments, one or more of the plurality of training instances may be obtained from a presentation session conducted based on user input. A presentation session may include audio data and/or text that captures (e.g., which may or may not include third party developers) a corresponding presentation session between one or more humans. For example, assume that the third party is a fictitious retail entity named fictitious market selling various products. In this example, a human may provide user input from the perspective of a customer of the hypothetical market to initiate the corresponding conversation, a human or another human may provide subsequent user input from the perspective of an employee of the hypothetical market, a human may provide additional subsequent user input from the perspective of an employee, a human or another human may provide other further subsequent user input from the perspective of a customer, and so on (e.g., as described with reference to fig. 3B). Training instance engine 161 can process the presentation session to generate one or more of the training instances in a similar manner as described above with respect to the corpus of training instances.
In some additional or alternative embodiments, one or more of the plurality of training instances may be obtained directly based on user input. For example, a third party developer may define at least a portion of a corresponding session to be used as a training instance input for a given training instance, and may define a ground truth response to the portion of the corresponding session to be used as a training instance output for the given training instance. In addition, the third party developer may optionally define a previous context for the corresponding session that is also to be used as part of the training instance input for a given training instance, or a session summary for the "previous" portion of the corresponding session. Notably, while the third party developer is defining these portions of the session, the third party developer may not need to define the entire session as a presentation session. Thus, the third party developer may define one or more training instances that relate to a particular portion of the session, such as particular values for parameters of a solicitation task (e.g., a restaurant reservation task, a flight change task, an inventory check task, and/or any other task that may be performed during a corresponding telephone call), perform RPCs, introduce, and/or other aspects of the corresponding session.
In various embodiments, one or more corresponding feature emphasis inputs may be associated with one or more of the plurality of training instances. The one or more corresponding feature emphasis inputs may be, for example, a natural language input (e.g., spoken and/or typed) that indicates why one or more portions of a particular training instance are important for training the voice robot, such as one or more portions of the training instance input including a time feature, a date feature, a name feature, an account feature, an email address feature, a phone number feature, a money feature, a quantity feature, a product name feature, a location feature, an RPC request feature, and/or any other feature of the training instance input or the training instance output for a given training instance. One or more corresponding feature emphasizing inputs can be included in the training instance input for the corresponding training instance, for biasing updates of the plurality of ML layers corresponding to the voice robot being trained, and/or used as input to the pointer network to recognize the one or more corresponding feature emphasizing inputs that cause the voice robot to pay attention to. Thus, when a third party subsequently deploys the voice robot for a conversation, the trained voice robot can notice the presence of these features.
In embodiments where one or more corresponding feature emphasized inputs are used as inputs to the pointer network, the one or more corresponding feature emphasized inputs and portions of the corresponding session (and optionally the previous context of the corresponding session) may be processed using the pointer network. One or more tokens corresponding to the portion of the conversation may be labeled with one or more values (e.g., probabilities, log-likelihoods, binary values, and/or other values) that indicate whether the one or more tokens corresponding to the portion of the conversation are included in the one or more corresponding feature emphasized inputs. For example, assume that the speech robot being trained is associated with a hypothetical market, assume that the training instance input includes at least a portion of a corresponding session corresponding to "I world like to purchase Product X if available," and assume that the one or more corresponding feature emphasis inputs indicate a Product feature and a usability feature. In this example, one or more tokens corresponding to "Product X" and "available" may be associated with values indicating that these features are meaningful to properly respond to portions of the corresponding session. Further, these values may be used as part of the training instance input, as labels associated with corresponding portions, as side inputs in processing the training instance input, and/or in biasing updates of multiple ML layers. In other words, by processing the corresponding feature emphasis input using the pointer network, the multiple ML layers can note these particular features of the portion of the corresponding session that was used as part of the training instance input.
Continuing with the hypothetical market example, assume further that the corresponding ground truth response indicates that "Product X" is actually "available" for sale. Based on this availability, the predicted response may correspond to "It is available", "It is available, who you like to purchase Product X? (is it available, do you want to buy product X. In this example, the third party developer may also provide user input indicating a corresponding ground truth response indicating how to respond when "Product X" (Product X) "not available" is for sale. Based on this unavailability, the third party developer may have the voice robot learn other predictive responses, such as "It is not available", "It is not available, who you like to purchase Product Y instance? (is it unavailable, do you want to buy product Y instead). Thus, third party developers can interact with the voice robot development system 120 such that the voice robot can learn multiple behaviors based on a single training instance that includes feature emphasized inputs. Thus, not only does the voice robot learn the particular features in the corresponding session that are important to the corresponding session, but the voice robot can also learn how to respond in scenarios where the particular features of the corresponding session of a given training instance are different from those included in the given training instance by interacting with the voice robot development system 120.
By emphasizing inputs using corresponding features described herein, various technical advantages may be realized. As one non-limiting example, the speech robot may achieve a given level of accuracy and/or robustness based on a given number of training instances by including corresponding feature emphasis inputs. If the corresponding feature emphasis input is not included, a greater number of training instances will be required to achieve a given level of accuracy and/or robustness, or will not be able to achieve a given level of accuracy and/or robustness. Accordingly, the voice robot may be trained in a faster and efficient manner, thereby saving computing resources and/or network resources of a client device used to train the voice robot in implementations where training instances, training losses, and/or other training data are transmitted over one or more networks.
In various embodiments, one or more of the plurality of training instances may be RPC training instances. As used herein, an RPC training instance includes having corresponding training that includes at least corresponding RPC inbound requestsInstance inputs and/or training instances that include at least corresponding training instance outputs corresponding to RPC outbound requests. The RPC outbound request included in the corresponding training instance output may indicate that the voice robot should generate an RPC request and via one or more networks 199 2 The RPC request is transmitted to one or more third party systems 190 (e.g., a reservation system, an inventory system, a status checking system, and/or any other third party system). The RPC inbound request included in the corresponding training instance input may indicate that the voice robot should go through one or more networks 199 2 A response to the RPC request is received from one or more of the third party systems 190 and the response is processed to generate an output based on the response. Although in FIG. 1 network 199 is shown 2 And network 199 1 Depicted separately, but it is to be understood that this is for clarity and is not meant to be limiting. For example, network 199 2 And network 199 1 May be the same network or a different combination of networks described herein. Because the RPC requests are not directly associated with the corresponding session on which the plurality of training instances for training the voice robot are generated (e.g., not captured directly in the spoken or typed input of the session), the third party developer may need to define the RPC outbound requests and the RPC inbound requests of the training instances, the particular third party system in the one or more third party systems 190 to which the RPC outbound requests should be directed, the format of the RPC requests, the format of the responses to the RPC requests, and/or any other information associated with the RPCs.
In implementations where user input engine 111 detects a user's spoken input via a microphone of client device 110 when obtaining a training instance as described above, audio data capturing the spoken input may be processed. In some embodiments, the ASR engine 130A1 of the client device 110 may process the audio data that captures the spoken input using the ASR model 130A. In additional or alternative embodiments, client device 110 may be over network 199 1 The audio data is transmitted to the speech robot development system 120, and the ASR engine 130A2 may process the audio data that captures the spoken input using the ASR model 130A. The speech recognition engines 130A1 and/or 130A2 may be based on voiceThe processing of the frequency data generates a plurality of speech hypotheses for the spoken input, and a particular speech hypothesis may optionally be selected as recognized text for the spoken input based on corresponding values (e.g., probability values, log-likelihood values, and/or other values) associated with each speech hypothesis of the plurality of speech hypotheses. In various embodiments, ASR model 130A is an end-to-end speech recognition model such that ASR engines 130A1 and/or 130A2 may generate multiple speech hypotheses directly using the model. For example, the ASR model 130A may be an end-to-end model for generating each of a plurality of speech hypotheses character-by-character (or otherwise token-by-token). One non-limiting example of such an end-to-end model for generating recognition text character by character is the recurrent neural network converter (RNN-T) model. The RNN-T model is a form of sequence-to-sequence model that does not employ an attention mechanism. In other embodiments, ASR model 130A is not an end-to-end speech recognition model, such that ASR engines 130A1 and/or 130A2 may instead generate predicted phonemes (and/or other representations). For example, the predicted phonemes (and/or other representations) may then be used by the ASR engines 130A1 and/or 130A2 to determine a plurality of speech hypotheses that conform to the predicted phonemes. In doing so, the ASR engines 130A1 and/or 130A2 may optionally employ decoding graphs, dictionaries, and/or other resources. In various embodiments, corresponding transcriptions may be rendered at the client device 110 (e.g., associated with training instance inputs, training instance outputs, corresponding feature emphasis inputs, presentation dialogs, and/or other aspects of a voice robot development platform).
In some versions of these embodiments, NLU engine 140A1 of client device 110 and/or NLU engine 140A2 of speech robot development system 120 may process recognized text generated by ASR engines 130A1 and/or 130A2 using NLU model 140A to determine an intent to include in the spoken input. For example, if the client device 110 detects a spoken input of "add training instance input of' do you have a reservation of two people at 6pm for two people" from a third party developer (e.g., as part of an independent spoken input defining the training instance input), the client device 110 may process audio data capturing the spoken input using the ASR models 130A1 and/or 130A2 to generate recognition text corresponding to the spoken input, and may process the recognition text using the NLU model 140A to at least determine an intent to add the training instance input (e.g., that may include the audio data and/or corresponding speech hypotheses for the spoken input).
In some versions of these embodiments, TTS engine 150A1 of client device 110 and/or TTS engine 150A2 of speech robot development system 120 may generate synthesized speech audio data that captures the synthesized speech. The synthesized speech may be rendered at the client device 110 using the rendering engine 112 and via speakers of the client device 110. The synthesized speech may capture any output generated by the voice robot development described herein, and may include, for example, an indication that a training instance has been added (repeating a particular training instance input, a training instance output, a feature emphasis input, etc.), a notification requesting a third party developer to add one or more additional training instances or a set of training instances (and optionally those associated with a particular feature), a notification requesting a third party developer to modify one or more existing training instances or a set of training instances (and optionally those associated with a particular feature), an instance that has initiated, completed training of the voice robot, or a status update regarding training of the voice robot, and/or any other information related to the voice robot or the voice robot development platform that may be audibly communicated to the third party developer.
The training engine 162 may train the voice robot (e.g., its ML layer) with a plurality of training instances captured by the training instance engine 161 (e.g., stored in the training instance database 161A). The voice robot may correspond to one or more processors that utilize multiple Machine Learning (ML) layers of one or more ML models (e.g., stored in ML layer database 170 A1) to conduct conversations on behalf of a third party for a phone call associated with the third party. The plurality of ML layers may correspond to those of the transformer ML model (e.g., input layer, encoding layer, decoding layer, feed-forward layer, attention layer, output layer, and/or other ML layers), unidirectional and/or bidirectional RNN models (e.g., input layer, hidden layer, output layer, and/or other ML layers), and/or other ML layers of other ML models.
For example, and referring to FIG. 2, an example process flow 200 for training a voice robot is depicted. In some embodiments, training instance engine 161 may obtain a given training instance from among multiple training instances associated with voice robots stored in training instance database 161A. In some embodiments, for a given training instance, the training instance input may include at least audio data 201 corresponding to a portion of the corresponding session and a session context 202 for the corresponding session. Further, for a given training instance, the training instance output may include a ground truth response 203 for the portion of the session. Audio data 201 may be processed by ASR engines 130A1 and/or 130A2 using ASR model 130A to generate a plurality of speech hypotheses 204. In other embodiments, the training example input may include a plurality of speech hypotheses 204 generated based on the audio data 201, but may not include the audio data 201 itself.
In some embodiments, encoding engine 162A1 may process the plurality of speech hypotheses 204 using a first ML layer of the plurality of ML layers stored in ML layer database 170A1 to generate a first encoding. Encoding engine 162A1 may process session context 202 using a first ML layer in a second ML layer of the plurality of ML layers stored in ML layer database 170A1 to generate a second encoding. Further, the concatenation engine 162A2 may concatenate the first encoding and the second encoding to generate a concatenated encoding. The concatenated coding may represent the current state of the corresponding session. For example, concatenated coding may encode the history of the session and the most recent portion of the session to encode the entire session as a whole.
By encoding the current state of the corresponding session, the dialog of the session can be tracked, thereby enabling the speech robot to model and/or learn the state of the corresponding session. Thus, the resulting trained voice robot may learn to solicit corresponding values for parameters associated with tasks performed via corresponding phone calls. For example, assume that the training example input includes at least a portion of a corresponding session corresponding to "Hello, do you have an of Product X available for sale". In this example, the voice robot is trained to understand that a human being is requesting an inventory check of product X. Further, by encoding the session context 202 for the corresponding session, the speech robot is also trained to understand: if product X is available, the human has not provided any corresponding value for the name parameter associated with purchasing or pausing product X, for the money parameter associated with purchasing product X, for the address parameter (if the human wishes to ship product X to his residence), and so on. Accordingly, the voice robot may be trained to subsequently prompt a human for corresponding values of one or more of these parameters by tracking the state of the conversation.
Further, the embedding engine 162A3 may process concatenated coding using one or more of the plurality of ML layers to generate predictive embedding associated with the predicted response 205 (e.g., performing RPCs with a third-party system, synthesized speech or text to be provided in response to the training instance input, answering an incoming telephone call, initiating an outgoing telephone call, and/or predicting other responses in response to the training instance input). The predicted response 205 may be selected from a plurality of candidate responses in the candidate response database 171A (e.g., including the ground truth response 203 and a plurality of additional candidate responses). In generating the prediction embedding, the size of the concatenated coding can be reduced to a fixed dimension. This enables the predicted embedding associated with the predicted response 205 to be easily compared in the embedding space with other embedding described with respect to the loss engine 162 A4.
In some versions of these embodiments and prior to processing the plurality of speech hypotheses 204, the training engine 162 may cause the plurality of speech hypotheses to be aligned. For example, assume that multiple speech hypotheses capture verbal input of "for 4PM (4 PM)". In this example, multiple speech hypotheses may be aligned as [ for, # empty,4PM; for,4, PM; four, four, PM ] so that each of the multiple aligned speech hypotheses may then be processed in combination with one another. In some further versions of these embodiments, the training engine 162 may further cause the plurality of aligned speech hypotheses to be annotated. Continuing with the above example, multiple aligned phonetic hypotheses may be annotated as [ for, # empty (@ null), 4PM (@ time); for,4 (@ time), PM (@ time); four (@ time), four (@ time), PM (@ time) ].
In embodiments where the training example input also includes audio data 201, in addition to or instead of the encoding generated based on the plurality of speech hypotheses, encoding engine 162A1 may generate an encoding associated with audio data 201. In these embodiments, the concatenation engine 162A2 may process the encoding associated with the audio data 201 and the encoding associated with the session context 202 to generate a concatenated encoding. Further, the embedding engine 162A3 may process concatenated coding using one or more of the plurality of ML layers to generate the predictive embedding associated with the predicted response 205. In various embodiments, although not depicted in fig. 2, the encoding engine 162A1 may additionally process one or more corresponding feature emphasized inputs associated with a given training instance along with the audio data 201 and/or the plurality of speech hypotheses 204.
Although the encoding engine 162A1, the concatenation engine 162A2, and the embedding engine 162A3 are described herein as performing particular functions in a particular order, it should be understood that the performance of these particular functions may be reordered and/or one or more of these engines may be omitted. For example, encoding engine 162A1 may be omitted, and embedding engine 162A3 may process the plurality of speech hypotheses 204 and the session context 202 using respective ML layers of the plurality of ML models to generate predictive embeddings associated with predictive responses to at least portions of corresponding sessions associated with the plurality of speech hypotheses.
Further, in various embodiments, loss engine 162A4 may compare the predicted embedding associated with predicted response 205 and the ground truth embedding associated with ground truth response 203 in an embedding space to generate one or more losses 206. The predicted embedding and the ground truth embedding may correspond to low-dimensional representations of the predicted response 205 and the corresponding ground truth response 203, respectively. The embedding space allows comparison of these low dimensional embeddings. Furthermore, the predicted embedding associated with the predicted response 205 should be close in embedding space to the corresponding ground truth embedding associated with the corresponding ground truth response 203. In other words, the system should predict a response similar to the actual response to at least part of the corresponding session when processing at least part of the corresponding session and the previous context of the session. For example, a distance metric (e.g., cosine similarity distance, euclidean distance, and/or other distance metric) between the predicted embedding in the embedding space and the corresponding ground truth embedding may be determined, and one or more of the losses 206 may be generated based on the distance metric.
In some embodiments, when training a speech robot based on a given training instance, a ground truth embedding associated with ground truth response 203 may be generated using different ML layers of different ML models (not depicted) that are different from the plurality of ML layers utilized in generating the predictive embedding (e.g., dot product architecture) associated with predictive response 205. The ground truth embedding may then be stored in the candidate response database 171A to serve as one of a plurality of candidate responses when inferred. Notably, the different ML layers can additionally or alternatively be updated based on one or more of the losses 206, such that the different ML layers learn respective portions of the embedding space to allocate to ground truth embedding. Further, the corresponding embedding associated with one or more responses that are incorrect responses to portions of the corresponding session may additionally or alternatively be used as a counter-example to further distinguish the correct embedding of portions of the corresponding session in the embedding space. After updating the different ML layers, a plurality of additional candidate responses may be processed using the updated different ML layers to generate corresponding candidate response embeddings. These candidate response inlays and corresponding candidate responses may also be stored in candidate response database 171A, even though they are not used to train the voice robot. Thus, at inference, different ML layers may be omitted, as candidate response embedding and corresponding candidate responses are known. In additional or alternative embodiments, the ground truth embeddings may be stored in the training instance database 161A in association with the ground truth responses of a given training instance.
The update engine 162A5 may cause one or more of the plurality of ML layers to be updated based on one or more of the losses 206. For example, the update engine 162A5 may back-propagate one or more of the losses 206 across one or more of the plurality of ML layers to update the respective weights of the one or more of the plurality of ML layers. In some embodiments, the update engine 162A5 may bias updating one or more of the plurality of ML layers with one or more of the corresponding feature emphasized inputs for a given training instance. For example, the loss engine 162A4 or the update engine 162A5 may weight one or more of the losses before updating the respective weights of one or more of the plurality of ML layers. Thus, one or more of the multiple ML layers may note the subsequent occurrence of one or more of the corresponding feature emphasized inputs.
One or more of the plurality of ML layers may be further updated based on additional training instances obtained by training instance engine 161 in the same or similar manner as described above. In some embodiments, a voice robot may be obtained in this manner until one or more conditions are satisfied. The one or more conditions may include, for example, validation of one or more of the plurality of ML layers or the updated plurality of additional ML layers, aggregation of one or more of the plurality of ML layers or the updated plurality of additional ML layers (e.g., with zero loss or within a threshold range of zero loss), a determination that one or more of the plurality of ML layers or the plurality of additional ML layers performs better than an instance of the voice robot (if any) currently being utilized (e.g., with respect to precision and/or recall), an occurrence of training based on at least a threshold number of training instances, and/or a duration of training based on the training instances.
Although the voice robot is described as being trained in a particular manner and using a particular architecture, it is to be understood that this is for the sake of example and is not meant to be limiting. For example, when training a voice robot associated with a fictitious market, a voice robot associated with a fictitious restaurant named fictitious cafe may be used as a reference voice robot. In this example, one or more transfer learning techniques (e.g., meta-learning) may be used to adapt the voice robots associated with the fictitious cafe (or the output generated based on those training instances) to the voice robots associated with the fictitious market. For example, the training instance inputs may include additional inputs indicating that the voice robot associated with the fictitious market is being trained for different retail related purposes, while the original voice robot associated with the fictitious cafe is being trained for restaurant purposes.
Referring briefly back to fig. 1 and after training the voice robot, the voice robot engine 170 can then utilize the trained voice robot to conduct a conversation on behalf of a third party for a phone call associated with the third party, and can include a response engine 171 and a Remote Procedure Call (RPC) engine 172 in various embodiments. The trained voice robot can make a telephone call with a human or additional voice robot that originates an incoming telephone call or answers an outgoing telephone call via the respective additional client device 195. The telephone call may be over one or more networks 199 using a voice communication protocol (e.g., voice over internet protocol (VoIP), public Switched Telephone Network (PSTN), and/or other telephone communication protocols) 3 The process is carried out. Notably, the calls can be cloud-based phone calls such that the client device 110 used to train the voice robot is not an endpoint of the corresponding phone call. Instead, the voice robot development system 120 (e.g., one or more servers) may act as an endpoint for a telephone call with one of the additional client devices 195.
For example, suppose a third party for whom a voice robot is trained is a fictitious retail entity named fictitious market selling various products. Further assume that a human provides user input at a respective one of the additional client devices 195 to initiate a telephone call with the fictitious market, the voice robot answers the incoming telephone call initiated by the human, and causes synthesized speech audio data (e.g., generated using the TTS model 150A) that captures synthesized speech corresponding to the particular introduction of the voice robot to be audibly rendered at the respective one of the additional client devices 195 such that the human is perceptible to the synthesized speech via a speaker of the respective one of the additional client devices 195. Further assume that in response to an audible rendering of the synthesized speech audio data, a human provides a spoken utterance of "Hello, do you have an of Product X available for sale". The spoken utterance may be captured in audio data transmitted over one or more networks to the voice robot development system 120 of fig. 1.
ASR engine 130A2 may process the audio data using ASR model 130A to generate a plurality of speech hypotheses corresponding to the spoken utterance. The voice robot engine 170 may optionally align and/or annotate multiple voice hypothesis pairs. Further, the response engine 171 can process one or more of a plurality of voice hypotheses, a session context of an incoming telephone call initiated by a human (e.g., stored in the voice activity database 170 A2), and/or audio data using the plurality of ML layers stored in the ML layer database 170A1 to generate the response embedding. In some embodiments, in response to determining that the spoken utterance provided by the user is complete, the audio data may be processed only by the ASR engine 130A2 and/or the plurality of speech hypotheses may be processed only by the response engine 171. For example, the voice robot development system 120 may process the audio data using an endpoint model that is trained to detect when a human has finished providing a spoken utterance to determine that the human has finished providing the spoken utterance after speaking the word "sell".
In some embodiments, response engine 171 may compare the response embedding to a plurality of candidate response embeddings associated with a plurality of candidate responses stored in candidate response database 171A. Further, based on a distance metric in the embedding space between the response embedding and one or more of the plurality of candidate response embeddings associated with the plurality of candidate responses, the response engine 171 can select a given one of the plurality of candidate responses as the response to the spoken utterance. For example, candidate responses associated with corresponding distance metrics that satisfy a distance threshold may be selected as responses. The response may be processed by TTS engine 150A2 using TTS model 150A to generate synthesized speech audio data that captures the response. Further, the synthesized speech audio data may be audibly rendered at a respective one of the additional client devices 195.
In some embodiments, the response engine 171 can determine that an RPC request is required to respond to a spoken utterance captured in the audio data. In some versions of these embodiments, the RPC engine 172 may generate RPC outbound requests and transmit the RPC outbound requests to one or more third party systems 190. Continuing with the above example, the response engine 171 may determine that an RPC request is required to determine whether the hypothetical market has any inventory of "Product X (Product X)" available for sale. Thus, the RPC engine 172 may generate a structured request (e.g., inventory = product X, intent = sale) as an RPC outbound request that is transmitted to the inventory third party system 190. The RPC engine 172 may receive RPC inbound requests in response to RPC outbound requests. For example, the RPC inbound request may indicate "Product X (Product X)" is available or unavailable for sale via the hypothetical market. In embodiments where the response engine 171 determines that an RPC request is required, one or more instances of synthesized speech audio data associated with an RPC outbound request (e.g., "hold on a second while I check" and/or one or more instances of synthesized speech audio data associated with an RPC inbound request (e.g., "yes, we have Product X available for sale, we want to purchase)") may be rendered at a respective one of the additional client devices 195 in the same or similar manner as described above.
This process may be repeated to generate a corresponding response to the spoken utterance provided by the human until the telephone call is completed. Telephone calls with humans may be stored in the voice activity database 170 A2. For example, for a given telephone call, the voice activity database 170A2 may include audio data corresponding to a spoken utterance of a human, synthesized voice audio data corresponding to synthesized speech of the voice call, results of the given telephone call, duration of the given telephone call, time and/or data associated with the given telephone call, and/or other information derived from the given telephone call. In some embodiments, the voice robot may solicit consent from the human to interact with the voice robot prior to participating in the conversation. In embodiments where humans agree to participate in a conversation during a telephone call using voice, the voice robot may participate in a conversation with the user. In embodiments where humans are not intended to participate in the conversation during the telephone call using voice, the voice bot may end the telephone call or a request that another human associated with the third party join the telephone call.
In various embodiments, the error recognition engine 180 may process the voice robot activity stored in the voice robot activity database 170A2 using multiple ML layers of the ML model stored in the ML layer database 180A (or multiple rules stored in the ML layer database 180A) to recognize any behavioral errors of the voice robot. For example, the error recognition engine 180 can process one or more portions of the corresponding conversation conducted by the trained voice robot associated with the third party and/or the corresponding conversation context of one or more portions of the corresponding conversation. For a corresponding session, behavioral errors of the trained voice robot may be identified based on processing one or more portions of the corresponding session and/or a corresponding session context.
In some embodiments, the identified behavioral errors may be classified into one or more different categories of errors based on output generated using a plurality of ML layers that indicate the behavioral errors. The behavioral errors may include, for example, the voice robot prematurely terminating the call, the voice robot failing to provide a response and timeout, the voice robot failing to solicit corresponding values for parameters needed to complete a desired action of the human, the voice robot failing to recognize corresponding values for parameters provided by the human to complete the desired action of the human, the voice robot failing to perform an RPC when needed, the voice robot performing an RPC with an incorrect third-party system, and/or any other behavioral errors of the voice robot that may occur during a corresponding phone call. Further, these behavioral errors may be processed using the ML model or additional ML layers of additional ML models stored in the ML layer database 180A to classify a given behavioral error as one or more different categories of errors. For example, the voice robot may be classified as a category associated with sparse errors as failing to solicit corresponding values of parameters needed to complete a desired action of a human being, may be classified as a category associated with missing or error-tagged feature errors as failing to recognize corresponding values of parameters provided by a human being to complete a desired action of a human being, may be classified as failing to perform RPCs when needed or performing RPCs with an incorrect third-party system as a category associated with RPC errors, and so forth. In other words, if the root cause of the identified behavioral errors is the same, the identified behavioral errors may be classified as one or more of the same different categories of errors. Identifying and classifying behavioral errors of a voice robot is described below (e.g., with respect to fig. 3C, 3D, and 3E).
In some embodiments, the error recognition engine 180 may cause the speech robot development system 120 to automatically perform one or more actions to correct these recognized behavioral problems. The one or more actions may include, for example, synthesizing additional training instances to be added to the corpus of training instances for updating the speech robot and/or modifying existing training instances in the corpus to update the speech robot by tagging missing features and/or re-tagging mistagged features. For example, the misidentification engine 180 may determine that the voice robot has confused multiple features included in portions of the corresponding session, but that there is a sparsity problem in training instances that include these features that prevents the voice robot from being able to consistently distinguish the multiple features. In this example, the error recognition engine 180 may cause the voice robot development system 120 to generate a synthetic training instance that includes one or more of the plurality of features to solve the sparsity problem and cause the voice robot to update based on a plurality of training instances associated with the voice robot (including the generated synthetic training instance). As another example, the error recognition engine 180 may cause the speech robot development system 120 to additionally or alternatively re-label one or more existing training instances to further differentiate one or more of the plurality of features. The following (e.g., with respect to fig. 3C, 3D, 3E, 4, 5, and 6) describes causing the voice robot development system 120 to automatically perform one or more actions related to correcting the recognized behavioral errors of the trained voice robot. In some versions of these embodiments, the error recognition engine 180 may cause the voice robot development system 120 to generate a notification to be presented to a third party developer that at least indicates that one or more actions are to be automatically performed to correct any recognized behavioral errors of the trained voice robot.
In some additional or alternative embodiments, the error recognition engine 180 may cause the voice robot development system 120 to generate a notification to be presented to the third party developer requesting that the third party developer should perform one or more actions directed to correcting any recognized behavioral errors of the trained voice robot. The notification may be rendered along with a summary of the session for the corresponding call, or via a separate interface (e.g., pop-up notification, notification interface, voice robot behavior error interface, etc.). The one or more actions may include, for example, adding additional training instances to the corpus to update the voice robot (and optionally training instances of a particular type or having particular features) and/or modifying existing training instances to update the voice robot. For example, the error recognition engine 180 may cause the voice robot development system 120 to present one or more training instances and prompt a third party developer to recognize one or more corresponding feature-emphasized inputs for one or more of the plurality of training instances, add more training instances that include a particular feature or are a particular type of training instance, re-label training instances that may include one or more incorrect labels, and/or any other action that may correct the root cause of the identified behavioral error (e.g., with respect to fig. 3C, 3D, 3E, 4, 5, and 6).
The session summary engine 185 may generate a corresponding session summary for each phone call made by the voice robot based on the voice robot activity stored in the voice robot activity database 170 A2. The corresponding session summary may be rendered at a user interface of the client device 110 using the rendering engine 112. In some embodiments, the corresponding session summary may include, for example, a natural language summary of each of the corresponding telephone call, a duration of the corresponding telephone call, an outcome or result of the corresponding telephone call, monetary information associated with the corresponding telephone call, and/or other information associated with the telephone call. Continuing with the hypothetical market example, the corresponding session digest may be, for example, "user's call to acquire out availability of Product X, I-call to make sure Product X is available, the user's purchased Product X for $100 (user call asks for availability of Product X, I checked to determine that Product X is available, and user purchased Product X in $ 100)". In some additional or alternative embodiments, the corresponding session summary, when selected, may cause a transcription of the corresponding phone call to be rendered at a user interface of the client device 110 using the rendering engine 112. The corresponding session summary is described below (e.g., with respect to fig. 3C).
Thus, the voice robot development platform described herein enables third party developers associated with third parties to train voice robots, monitor the performance of the voice robots, and then update the voice robots based on any identified behavioral errors of the voice robots. Notably, the voice robot development platform is example-based in that the voice robot is trained based on portions of the conversation, and the voice robot is updated based on adding more examples or modifying existing examples. Thus, third party developers need not have any extensive knowledge of ML or how to define various intent patterns, which may be required for developing rule-based voice robots. However, the various techniques described herein may also be used to train, monitor, and update rule-based voice robots.
By using the techniques described herein, various technical advantages may be realized. As one non-limiting example, the voice robot development platform enables voice robot behavior to be easily added or modified by adding new training instances or modifying existing training instances. Thus, a voice robot trained using the voice robot development platform described herein is more scalable and reduces memory consumption since a large number of intent patterns need not be defined. In contrast, multiple ML layers of one or more of the ML models trained and utilized may have a smaller memory footprint and may be more robust and/or accurate. Furthermore, a voice robot trained using a voice robot development platform achieves a high level of accuracy and recall, enabling sessions to end more quickly and efficiently because a voice robot trained using a voice robot development platform is more able to understand nuances of human language and respond accordingly.
Although the voice robot is described herein as being subsequently deployed for conducting a conversation on behalf of a third party for a telephone call associated with the third party, it should be understood that this is for purposes of example and is not meant to be limiting. For example, the voice robots described herein may be deployed in any scenario in which a human may participate in a human-machine conversation with a given voice robot. For example, a given voice robot may be trained to converse with a human via a car shuttle restaurant system at a car shuttle restaurant, as an automated assistant via a client device of the human to converse with the human, and/or in any other domain beyond that in which the human may participate in a phone call with a human-machine conversation with the given voice robot. Thus, it should be understood that the behavior of these voice robots may be based on training examples used to train the corresponding voice robots.
Turning now to fig. 3A, 3B, 3C, 3D, and 3E, various non-limiting examples of a user interface 300 associated with a voice robot development platform are depicted. The third party developer may interact with the voice robot development platform using a client device that includes a voice robot development system client or a voice robot development system (e.g., client device 110 of fig. 1). By interacting with the voice robot development platform, third party developers can train voice robots that, when deployed, can converse on behalf of third parties associated with the third party developers for incoming telephone calls directed to the third parties and/or outgoing telephone calls initiated on behalf of the third parties. For purposes of example, throughout fig. 3A, 3B, and 3C, it is assumed that a third party developer is creating a new voice robot to converse for phone calls and/or auto shuttle restaurants associated with a hypothetical cafe (fictitious restaurant).
Referring specifically to fig. 3A, a home screen or login page of the voice robot development platform is depicted as being visually rendered on the user interface 300. In various embodiments, various graphical elements may be presented to third party developers on a home screen or landing page. For example, the user interface 300 may include a voice robot graphical element 310 that provides a snippet of any unique identifier associated with a voice robot developed by a third party developer and/or any voice robot associated with a third party (e.g., a hypothetical cafe). When creating a new voice robot, the third party developer may provide a unique identifier in text entry field 318 that will be associated with the new voice robot being developed. For example, as shown in fig. 3A, the third party developer may provide a typed input of "hypothetical cafe" in text input field 318, or capture a spoken input of audio data corresponding to "hypothetical cafe" (and optionally in response to a user selection of microphone interface element 350). In some embodiments, the third party developer may select a "view more" graphical element as shown in fig. 3A, to extend the snippet of the voice bot (if any other exists) to include additional voice bots, or to launch the voice bot interface on user interface 300.
Additionally, the user interface 300 may additionally or alternatively include a training instance graphical element 320 that provides a snippet of multiple training instances utilized in creating the new speech robot. Each training instance of the plurality of training instances may include a training instance input and a training instance output. The training instance input can include, for example, at least a portion of the corresponding session and a previous context of the corresponding session, and the training instance output can include, for example, a corresponding ground truth response to at least the portion of the corresponding session. For example, a plurality of training instances may be obtained from an existing phone call corpus associated with a hypothetical cafe (or another restaurant entity), from a presentation session between one or more human beings (e.g., which may or may not include developers), and/or from one or more other spoken utterances of one or more human beings (e.g., which may or may not include developers) corresponding to a segment of the session (e.g., collectively referred to as the "hypothetical cafe corpus in fig. 3A"). Obtaining a plurality of training examples and training a voice robot associated with a hypothetical cafe are described below (e.g., with respect to fig. 3B). In some embodiments, a third party developer may select an "add training examples" graphical element as shown in fig. 3A to add training examples for training a voice robot associated with a hypothetical cafe, or to launch a training example interface on user interface 300.
Further, the user interface 300 may additionally or alternatively include a voice robot activity graphical element 330 that provides a segment of voice robot activity associated with the trained voice robot (e.g., referred to as "phantom cafe activity" in fig. 3A). The voice robot activity may include information related to each corresponding phone call made by the trained voice robot on behalf of the hypothetical cafe. For example, the voice robot activity may include a time and/or date associated with each corresponding phone call, a duration of each corresponding phone call, a summary associated with each corresponding phone call, a transcription associated with each corresponding phone call, and/or any other information related to each corresponding phone call made by the trained voice robot on behalf of the hypothetical cafe. In some embodiments, the voice robot activity may be generated during and/or after each corresponding phone call. Voice robot activity enables third party developers to monitor the performance of voice robots. Voice robot activity is described below (e.g., with respect to fig. 3C). In some embodiments, the third party developer may select a "view more" graphical element as shown in fig. 3A, to extend the segment of the voice robot activity (if any exists) to include additional voice activity, or to launch a voice robot activity interface on the user interface 300.
Even further, the user interface 300 may additionally or alternatively include a voice robot behavior error graphical element 330 that provides a snippet of recognized voice robot behavior errors associated with the trained voice robot (e.g., referred to as a "hypothetical cafe behavior error" in fig. 3A). The voice robot behavior errors may include errors made by the trained voice robot during a corresponding telephone call placed on behalf of the hypothetical cafe. These voice robot behavioral errors may include, for example, an unavailable time to accept or suggest restaurant reservations, an incorrect business time to offer, an order to accept unavailable food, a failure to solicit values for parameters of tasks being performed during the corresponding session, and/or any other errors corresponding to incorrect behavior of the trained voice robot. The voice robot behavioral errors enable the voice robot development platform to identify corresponding root causes of the voice robot behavioral errors. In some embodiments, the voice robot development platform may take one or more actions to automatically correct the corresponding root cause, such as re-labeling one or more of the plurality of training instances used to retrain the voice robot, adding one or more feature-emphasized inputs to one or more of the plurality of training instances used to retrain the voice robot, and/or any other action that may be taken by the voice robot training platform to correct the corresponding root cause of the recognized voice robot behavior error. In additional or alternative embodiments, the voice robot development platform may generate one or more notifications to notify third party developers of the root cause of the identified voice robot behavior error. The notifications may optionally include an indication of one or more actions that, when performed by the third party developer, may correct the corresponding root cause of the identified voice robot behavior error, such as requesting the third party developer to re-label one or more of the plurality of training instances for retraining the voice robot, requesting the third party developer to add one or more feature-emphasis inputs to one or more of the plurality of training instances for retraining the voice robot, requesting the third party developer to add one or more additional training instances for retraining the voice robot (and optionally having one or more specific labels or specific feature-emphasis inputs), and/or any other action that may be taken by the third party developer to correct the corresponding root cause of the identified voice robot behavior error. In some embodiments, the third party developer may select a "view more" graphical element as shown in fig. 3A to expand the fragment of the recognized voice robot behavior error (if any exists) or to launch the voice robot behavior error interface on the user interface 300.
The third party developer may navigate the home or login page of the voice robot shown in fig. 3A to create a new voice robot, train the voice robot, monitor the performance of the voice robot, and/or subsequently update the voice robot. For example, assume that a third party developer provides the unique identifier of the voice robot, "hypothetical cafe," in text entry field 318 and chooses to navigate to user interface 300 to the training example interface as shown in fig. 3B. A third party developer may interact with the training instance interface to define a training instance for training the voice robot.
In some embodiments, the training examples may be obtained from a corpus of training examples. The corpus of training examples may include, for example, one or more previous sessions between a user (e.g., employee) associated with the hypothetical cafe and an additional user (e.g., customer) during a corresponding previous phone call, one or more previous sessions between other users not associated with the hypothetical cafe during a corresponding previous phone call (e.g., phone call associated with another restaurant entity), and/or other sessions in which training examples may be generated. For example, in response to receiving user input directed to training instance corpus interface element 380, a third party developer may access the corpus of training instances to select a portion of the corresponding session to use as training instance input 321A for a given training instance (and any previous context of the corresponding session), and a corresponding response to the portion of the corresponding session to use as training instance output 322A for the given training instance. User input directed to training example corpus interface element 380 may be, for example, touch input detected via a touch screen or via a user interface input device (e.g., a mouse or stylus), and/or spoken input detected via a microphone of a client device (and optionally in response to user input directed to speech interface element 350). In various embodiments, a third party developer may optionally define the feature emphasis input 323A for a given training instance.
In some additional or alternative embodiments, the training examples may be obtained from user input received at a training example interface presented to the user via user interface 300. The user input received at the training example interface may be, for example, touch or typing input detected via a touch screen or via a user interface input device (e.g., mouse, stylus, keyboard, etc.) and/or spoken input detected via a microphone of the client device (and optionally in response to user input directed at the voice interface element 350). For example, the user may provide user input that includes one or more of training instance input 321A and training instance output 322A (and optionally feature emphasis input 323A) in the table of training instances shown in fig. 3B.
In some additional or alternative embodiments, a training instance may be obtained from the presentation session 352B. The presentation session 352B may then be used to generate a plurality of training instances for training the voice robot associated with the hypothetical cafe. For example, as illustrated by the presentation session in fig. 3B, the third party developer (and optionally another human) may act according to different roles to simulate an actual session between a human (e.g., employee) and another human (e.g., client) associated with the hypothetical cafe by providing user input (e.g., typed input or spoken input) for the presentation session. For example, a third-party developer may select the employee graphical element 362A and provide user input 352B1? (what i could help you in your good, fictitious cafe) ", selecting the customer graphic element 362B and providing a user input 354b1 hi, i would like to book a table for four year at 6pm tonight (hi, i would like to book a four-person table this evening)" selecting the employee graphic element 362A and providing a user input 352B2"" Let me check ", followed by a user input of i 2b3 l 'i'm soy, 8 only have a four-person table for 8 pm", selecting the customer graphic element B and providing a user input 354b 7pm w. (what name?). The voice robot development platform may automatically generate a plurality of training instances based on the presentation session 352B. However, the third party developer may need to specify any feature-emphasized input for the training instance generated based on the presentation session 352B.
For example, assume that training instance input 321A1 generated based on presentation session 352B includes an indication that there is an incoming telephone call, and that training instance output 322A1 includes a corresponding response to the incoming call, such as answering the incoming telephone call and providing an output corresponding to user input 352B 1. In this example, the feature emphasis input 323A1 may correspond to an introductory feature of the incoming telephone call. The referral for the incoming telephone call may be user input 352B1, options presented via an Interactive Voice Response (IVR) system, and/or other referrals that a third party developer may desire voice robot learning. Notably, since there is no previous portion of presentation session 352B, there is no previous session context for training instance input 321A 1. Thus, a voice robot trained on this training example can learn how to answer an incoming telephone call. In embodiments where the training example input and/or the training example output is based on user input, the user input may correspond to audio data capturing the user input, a plurality of speech hypotheses generated based on processing the audio data, and/or text corresponding to the user input.
As another example, assume that training instance input 321A2 generated based on presentation session 352B includes a portion of presentation session 352B corresponding to user input 354B1 and a previous session Context as indicated by "$ Context" (e.g., user input in presentation session 352B that occurred before user input 354B1 and/or metadata associated with presentation session 352B), and that training instance output 322A1 includes a corresponding response to user input 354B1, such as user input 352B2 and an indication to initiate a Remote Procedure Call (RPC) outbound request for availability. In this example, the feature emphasis input 323A2 may correspond to features of the user input 354B1, such as a party scale feature (e.g., "four people" as indicated by the dashed box of the training instance input 321 A2), a time feature (e.g., "6PM (6 PM)", as also indicated by the dashed box of the training instance input 321 A2), and an RPC outbound request feature. Notably, training instance output 322A2 also includes an indication of an RPC outbound request that initiated availability. This training instance may be considered an RPC training instance, and the type of RPC training instance may be an RPC outbound request training instance.
The RPC outbound request for availability may include, for example, generating a structured request to query restaurant reservation availability at the requested time for the particular party size (e.g., availability: [ party size ] =4; [ time ] =6PM, or any other form of structured request), and transmitting the structured request to a third party system associated with managing restaurant reservations for the hypothetical cafe. Although RPC outbound requests are not explicitly included in presentation session 352B, the third party developer may add or inject RPC outbound requests for availability into training instance output 322 A2. Further, although the RPC outbound request is transmitted to a third-party system (and not to a "client" in presentation session 352B), the voice robot may still be trained to generate and transmit an available RPC outbound request during presentation session 352B based on the training instance input 321A2 requesting availability of restaurant reservations being an RPC outbound request training instance. Further, while the RPC outbound request is described as being associated with restaurant reservation availability, it should be understood that this is for purposes of example and is not meant to be limiting. For example, RPC outbound requests may be associated with food/inventory availability, business hours inquiries, telephone call transfers, and/or any other function that requires interaction with one or more third party systems during a telephone call. Thus, a voice robot trained on this RPC outbound request training instance can learn when and how to initiate RPC outbound requests.
Further, when obtaining a training instance with training instance input 321A2 and training instance output 322A2, the voice robot development platform may generate a notification 390B that requests a third party developer to "add more training instances with temporal features," as shown in FIG. 3B. Although notification 390B is depicted in fig. 3B as requesting a third party developer to add a training instance with temporal characteristics, it should be understood that this is for purposes of illustration. For example, the voice robot development platform may request third party developers to add training instances of a particular type (e.g., RPC training instances) and/or training instances that include particular features (e.g., temporal features, party-scale features, name features, and/or any other features). In some embodiments, the voice robot development platform may generate a notification requesting a third party developer to add training instances of a particular type and/or including particular features until a threshold number of those training instances are obtained and/or the performance of the voice robot with respect to processing a particular session associated with those training instances has been verified.
As yet another example, assume that training instance input 321A3 generated based on presentation session 352B includes a portion of presentation session 352B corresponding to an RPC inbound request and a previous session Context (e.g., user input in presentation session 352B and/or metadata associated with presentation session 352B that occurred prior to the RPC inbound request) as indicated by "$ Context", and that training instance output 322A3 includes a corresponding response to the RPC inbound request, such as user input 352B3. In this example, the feature emphasized input 323A3 may correspond to an RPC inbound request feature. Notably, training instance output 322A2 also includes an indication to initiate an RPC outbound request for availability. This training instance may be considered an RPC training instance, and the type of RPC training instance may be an RPC inbound request training instance.
The RPC inbound request for availability may include, for example, receiving a structured response that includes an indication of whether there are any restaurant reservations that satisfy the parameters of the reservation request (e.g., party size 4 and time 6 PM) and, optionally, one or more alternate times or time ranges that satisfy the alternatives to the parameters of the reservation request. In some embodiments, one or more affinity features may be generated based on the current state of presentation session 352B. For example, assume that the requested time of a restaurant reservation is 6PM as included in presentation session 352B, and that the requested time is available. In this example, one or more affinity features may be generated that indicate that the requested time is available. Instead, assume that the requested time of restaurant reservation is not available. In this example, the one or more affinity features that indicate that the requested time is not available also correlate the requested time to an alternate time (e.g., one hour after the requested time and two hours after the requested time instead of 6PM of the requested time if the availability corresponds to 7PM and 8 PM).
Similar to the RPC outbound requests described above, although RPC inbound requests are not explicitly included in the presentation session 352B, the third party developer may add or inject RPC inbound requests regarding availability into the training instance input 321 A3. Further, although the RPC inbound request is received from a third-party system (rather than a "client" in the presentation session 352B), the voice robot may still be trained to receive RPC inbound requests for availability during the presentation session 352B based on the training instance input 321A3 that includes the availability of restaurant reservations being an RPC inbound request training instance. Further, while the RPC inbound request is described as being associated with restaurant reservation availability, it should be understood that this is for purposes of example and is not meant to be limiting. For example, the RPC inbound requests can be based on corresponding RPC outbound requests transmitted to one or more of the third party systems. Thus, a voice robot trained on this RPC inbound request training instance may learn how to process RPC inbound requests and respond based on the context of the session and the data included in the RPC inbound request.
By training the voice robot based on the RPC training examples described herein, various technical advantages may be achieved. As one non-limiting example, the voice robot may learn how and/or when to transmit requests to third party systems, and how to utilize responses to those requests in addressing the task of the voice robot's ongoing phone conversation. Thus, the task may be solved by the voice robot during the conversation and may be efficiently solved without requiring additional human participation in the conversation. Furthermore, the utilization of RPC training instances enables a reduction in the number of RPC requests because there are fewer RPC requests that make errors, thereby saving computational resources that would otherwise be consumed in generating RPC requests and/or network resources that would otherwise be consumed in transmitting RPC requests over one or more networks.
As yet another example, assume that training instance input 321A4 generated based on presentation session 352B includes a portion of presentation session 352B corresponding to user input 354B3 and a previous session Context as indicated by "$ Context" (e.g., user input in presentation session 352B that occurred before user input 354B3 and/or metadata associated with presentation session 352B), and that training instance output 322A4 includes a corresponding response to the incoming call, e.g., a response corresponding to user input 352B 5. In this example, the feature emphasis input 323A4 may correspond to a name feature (e.g., "John Smith" as indicated by the dashed box of the training instance input 321 A4). Notably, in the presentation session 352B, the "client" does not provide the name of the restaurant reservation until prompted to do so by the "employee". Thus, if the "customer" does not provide a value for the name parameter (e.g., john Smith), the voice robot trained on this training instance may learn to be eligible for the value of the name parameter when making restaurant reservations.
In various embodiments and after defining the training instance, a voice robot associated with the hypothetical cafe may be trained. For example, in response to receiving user input directed to training voice robot graphical element 381, the voice robot may be trained based on the training instances. The voice robot associated with the hypothetical cafe may correspond to one or more processors utilizing multiple layers of one or more ML models (e.g., RNN models, transformer models, LSTM models, and/or other ML models). Notably, when training the speech robot, one or more of the multiple layers of the ML model may note the corresponding feature emphasized input associated with one or more of the multiple training instances. For example, the voice robot may note at least a party-scale feature, a time feature, a name feature, an RPC outbound request feature, an RPC inbound request feature, and/or other features defined by the corresponding feature emphasis inputs of the various training instances.
In various embodiments, and after deploying the voice robot, the third party developer may monitor the performance of the voice robot while conducting a session on behalf of the fictitious cafe. For example, in response to receiving user input directed to voice robot activity interface element 383, user interface 300 may present a voice robot activity interface as shown in fig. 3C. Additionally or alternatively, in response to receiving a user input directed to the main interface element 386, the user interface 300 may return to a home page or login page as shown in fig. 3A, and the user interface 300 may present a voice robot activity interface as shown in fig. 3C in response to a selection of the voice robot activity graphical element 330 (or the corresponding "view more" graphical element described above with reference to fig. 3A) from a third party developer. The third party developer may interact with the voice robot activity interface to view voice robot activity of the voice robot associated with the fictitious cafe. The voice robot activity may be stored in one or more databases (e.g., voice activity database 170A2 of fig. 1) accessible by the client device.
For example, the user may view a session summary 331A of a phone call made by the trained voice robot on behalf of the hypothetical cafe. In some embodiments, the third party developer may view all voice robot activity of the voice robot, as indicated by 330A. In some additional or alternative embodiments, the third party developer may switch between viewing all voice robot activities and viewing only voice robot activities that include recognized behavioral errors of the voice robot as indicated by 330B and as shown in fig. 3C. The text associated with 330A and 330B may be selectable and enable third party developers to switch between these different views of the voice robot activity. In various embodiments, a third party developer may search a voice robot activity log. For example, a third party developer may enter one or more terms into the text entry field 330C to search for voice robot activity. Further, in various embodiments, one or more ranking criteria may be used to rank the session digests 331A presented to the user. The one or more ranking criteria may include, for example, a recency of the corresponding session, a recency since a third party developer reviewed the corresponding session summary, and/or any other ranking criteria.
In some embodiments, the session summary provides a natural language interpretation of the corresponding telephone calls made by the voice bot on behalf of the hypothetical cafe. For example, session summary 331A1 indicates "the user made a call to make a reservation, made a reservation for an incorrect time and not performing an RPC, and did not request a name associated with the reservation". In this example, session summary 331A1 indicates a telephone call similar to presentation session 352B of fig. 3B. The session summary 331A1 may additionally or alternatively include an indication that RPC is performed to check availability of the requested time and identify an alternate time and/or other information associated with the corresponding telephone call, such as the time and date the corresponding telephone call was made, the duration of the corresponding telephone call, monetary information associated with the corresponding telephone call (e.g., the cost of a take out order), subscription information associated with the corresponding telephone call, and/or any other information derived from the corresponding telephone call.
In some additional or alternative embodiments, the transcription associated with the corresponding phone call made by the voice robot on behalf of the hypothetical cafe may be accessed from the voice call activity interface (and optionally only when the human talking to the voice robot agrees to monitor the voice robot activity for the corresponding phone call). For example, the session 352C portion of the voice robot activity graphical user interface includes a transcription of the session 352C for which a session summary 331A1 is generated. For example, assume that the voice robot answers an incoming telephone call directed to a hypothetical cafe, and that the incoming telephone call is initiated by a human via a respective client device. Further assume that upon answering an incoming telephone call, the speech robot renders captured synthesized speech 352c1' hello, hypothertical Caf, how may I help you? (you good, imagine a cafe, what i am able to serve you). Further assume that the speech robot processes audio data capturing the spoken utterance 354c1"hi, i woould like to make a reservation at 6pm" (and the previous context of the conversation 352C), and generates a captured synthetic speech 352c2"okay, for how many people peoples? (good, reserve bits of several people.
Notably and in contrast to the presentation session 352B of fig. 3B, the voice robot failed to execute an RPC to determine if "6PM" is available for reservation, the voice robot reserved "7PM" instead of "6PM" without confirming the reservation time, and the voice robot failed to request the name of the reservation. The session digests 331A2 and 331A3 provide other corresponding natural language digests that indicate other behavioral errors of the voice robot. For example, session summary 331A2 specifies that take-away orders will be placed even if food is not available, and session summary 331A3 specifies that the voice robot does not know how to handle the human asking the hypothetical cafe whether or not a terrace seat is provided.
In various embodiments and after deploying the voice robot, the third party developer may correct the behavioral errors of the voice robot. For example, in response to receiving user input directed to behavioral error interface element 384, user interface 300 may present a voice robot behavior error interface as shown in fig. 3D and 3E. Additionally or alternatively, in response to receiving a user input directed to the main interface element 386, the user interface 300 may return to a home page or login page as shown in fig. 3A, and the user interface 300 may present a voice robot behavior error interface as shown in fig. 3D and 3E in response to a selection of the voice robot behavior error graphical element 340 (or the corresponding "view more" graphical element described above with reference to fig. 3A) from a third party developer. The third party developer may interact with the voice robot behavior fault interface to view the recognized voice robot behavior fault, view one or more actions performed by the voice robot development platform to correct the recognized voice robot behavior fault, perform one or more actions to correct the recognized voice robot behavior fault, update the voice robot, and/or validate the voice robot.
In some embodiments, speech robot acquisitions of a hypothetical cafe (e.g., described above with respect to fig. 3C) may be processed using multiple ML layers of the ML model to identify one or more behavioral errors. Further, one or more behavioral errors may be handled using multiple ML layers of ML models or additional multiple ML layers of additional ML models to classify a given behavioral error as one or more different classes of behavioral errors that may or may not be mutually exclusive. In some further versions of these embodiments, the voice robot development platform may obtain the corresponding embedding generated during the corresponding session based on at least one or more portions of the corresponding session and the corresponding previous context of the corresponding session.
Further, the speech robot development platform may identify one or more training instances included in a corpus of training instances used to train the speech robot based on corresponding embeddings generated during corresponding sessions. For example, the voice robot development platform may compare, in the embedding space, respective embeddings generated during training of the voice robot associated with the synthetic cafe and based on one or more training instances used to train the voice robot associated with the synthetic cafe to corresponding embeddings generated during corresponding sessions. Further, the voice robot development platform may identify one or more training instances associated with respective embeddings associated with corresponding distance metrics (e.g., cosine similarity distance, euclidean distance, and/or other distance metrics) that satisfy a threshold distance. For example, training a voice robot based on one or more aspects of one or more training instances may result in a behavioral error of the voice robot if the respective embedding associated with those training instances is within a threshold distance of the corresponding embedding generated during the corresponding session for which the behavioral error was identified. Thus, those training instances may be identified as being associated with behavioral errors.
For example, and with respect to the session 352C and the session summary 331A1 of fig. 3C, the voice robot development platform may process the spoken utterance 354C2 and/or the response 354C3 (as well as the previous session context) to identify RPC errors. In other words, the voice robot development system may determine that the voice robot associated with the hypothetical cafe should have generated an RPC outbound request to the third party booking system in response to receiving the spoken utterance 354C2 (e.g., availability: [ party size ] =5; [ time ] =6PM, or any other form of structured request), and that the response 352C3 should be based on the RPC inbound request generated by the third party booking system (e.g., an indication of whether the requested time is available and optionally other alternative booking times). Thus, this behavioral error may be classified as category 3 340A3 associated with the behavioral error caused by RPC as indicated by 340 A3A. Further, category 3 340A3 may also include an indication of one or more training instances identified as being likely to be the cause of the behavioral error, as indicated by 340 A3B. For example, the one or more training instances as indicated by 340A3B may include RPC training instances associated with RPC requests to an incorrect third party system, RPC training instances that did not emphasize input indicia with corresponding features and/or emphasized input false indicia with incorrect features. Also, for example, and as indicated by notification 390D, the voice robot development platform may provide additional information to correct behavioral errors caused by RPCs (e.g., "adding more RPC training instances would likely solve this problem"). Thus, the third party developer may select the RPC graphical element 340A3C to correct an existing RPC training instance, and/or add additional RPC training instances and/or the training instance graphical element 382 to return to the voice robot training instance shown in fig. 3B. It is noted that this error may also be classified into one or more other different categories.
Additionally or alternatively, and with respect to the session 352C and the session summary 331A1 of fig. 3C, the voice robot development platform may process the spoken utterance 354C2 and/or the response 354C3 (as well as the previous session context) to identify missing feature errors and/or error-flagged feature errors. In other words, the voice robot development system may determine that the voice robot associated with the hypothetical cafe should have solicited the value of the name parameter of the subscription prior to the subscription, and that the response 352C3 should have actually solicited the value of the name parameter. Thus, this behavioral error may be classified as category 1 340A1 associated with a behavioral error caused by a missing name feature as indicated by 340 A1A.
Further, category 1 340A1 may also include an indication of one or more training instances identified as likely to be the cause of the behavioral error, as indicated by 340A 1B. For example, the one or more training instances as indicated by 340A1B may include a training instance that includes a name feature, but does not label the name feature as a name feature or does not include a corresponding feature-emphasized input that indicates a name feature. Further, this behavior error may be classified as a category 4 340A4 associated with the behavior error caused by the mislabeled name feature as indicated by 340 A4A. Further, category 4 340A4 may also include an indication of one or more training instances identified as likely to be the cause of the behavioral error, as indicated by 340 A4B. For example, the one or more training instances as indicated by 340A4B may include a training instance that includes a name feature, but the name feature is mis-labeled as a party-scale feature or other feature. Thus, the third party developer may select the tagged feature graphical elements 340A1C and/or 340A4C to tag or re-tag one or more training instances as indicated by 340A1B and/or 340A4C, respectively, and/or the training instance graphical element 382 to return to the voice robot training instance interface shown in FIG. 3B to tag and/or re-tag one or more of the training instances.
Further, in embodiments where the recognized behavioral errors of the voice robot include missing feature errors (e.g., classified as category 1 340A1 of fig. 3D), the voice robot development platform may generate a notification requesting a third party developer to add one or more missing features to the one or more training instances. For example, the voice robot development platform may identify a first training instance and a second training instance, and may include the first training instance and the second training instance in a notification with a request for a third party developer to define one or more features of the training instance. For example, assume that both the first training instance and the second training instance include the term "six". However, further assume that the term "six" in the first training example refers to party-scale characteristics of restaurant reservations, while the term "six" in the second training example refers to time characteristics of restaurant reservations. In response to the notification being presented to the user, the third party developer may flag these missing features accordingly. Thus, the updated voice bot may then be able to better distinguish these features in view of the dialog context of the respective corresponding session.
Additionally or alternatively, and with respect to session 352C and session summary 331A1 of fig. 3C, the voice robot development platform may process spoken utterance 354C2 and/or response 354C3 (as well as previous session context) to identify sparsity errors. In other words, the voice robot development system may determine that the voice robot associated with the hypothetical cafe should have solicited the value of the name parameter of the reservation before making the reservation and/or should have performed an RPC with the third party restaurant reservation system, and that the response 352C3 should have actually solicited the value of the name parameter and/or should have responded to the RPC request. However, the speech robot development platform may not have training instances and/or RPC training instances that have name features or corresponding features that indicate name features to emphasize input. Thus, this behavior error may be classified as a category 2 340A2 associated with a behavior error caused by sparsity of training instances having a particular feature (e.g., name feature) and/or sparsity of a particular type of training instance (e.g., RPC training instance), as indicated by 340 A2A.
Further, category 2 340A2 may also include one or more training specific features and/or an indication of a particular type of training instance that caused the sparsity error of the behavioral error as indicated by 340 A2B. For example, the one or more training instances as indicated by 340A2B may include a request to include a name feature and to add additional training instances having the name feature, training instances to add requests for additional RPC training instances, and/or other training instances having particular features or other types of training instances. Thus, the third party developer may choose to add a training instance graphical element 340A2C1 to add one or more training instances 340A2B, and/or a training instance graphical element 382 to return to the voice robot training instance interface shown in FIG. 3B to add additional training instances to correct sparsity errors.
In additional or alternative embodiments, the voice robot development platform may automatically perform one or more actions related to correcting any identified behavioral errors. For example, the voice robot development system may synthesize one or more training instances in response to receiving a selection of the synthesized graphical element 340AC 2. One or more synthetic training instances may be generated based on corresponding sessions from voice robot activity and/or augmenting existing training instances. For example, the voice robot development platform may synthesize one or more synthetic training instances in the same or similar manner as described above with respect to generating training instances based on the presentation session 352B of fig. 3B. Further, the speech robot development platform may request the developer to provide one or more corresponding feature emphasized inputs for one or more synthetic training instances. Also, for example, the speech robot development platform may synthesize one or more synthetic training instances (e.g., based on different responses to the modified portion of the corresponding session and/or the modified context of the session) by modifying training instance inputs (e.g., modifying a portion of the corresponding session or the context of the session) and/or training instance outputs for a given training instance. As another example, the voice robot development platform may automatically label or re-label one or more features of one or more training instances. One or more training instances may be tagged and/or re-tagged based on NLUs or other data associated with one or more tags associated with one or more terms of a portion of a corresponding session or ground truth responses of one or more training instances. For example, if the portion of the corresponding session includes one or more items indicating time (e.g., "AM" or "PM"), the term indicating time may be tagged with a time feature.
In some versions of these embodiments, and as shown in fig. 3E, the voice robot development platform may generate a notification 390E and cause the notification to be presented to the user upon recognition of one or more actions that correct the recognized behavioral errors of the voice robot. For example, the notification 390E may include an indication 390E1 that notifies a third party developer that one or more actions have been performed to correct the recognized behavioral errors of the voice robot, such as synthesizing one or more training instances, tagging missing features of one or more training instances, and/or re-tagging features of one or more training instances. Further, the notification 390E2 can additionally or alternatively include an indication 390E2 that notifies the third party developer of the identified behavioral error corrected by the action expectation, and the notification 390E2 additionally or alternatively includes an indication 390E3 that notifies the third party developer of one or more actions automatically performed by the voice robot development system. These indications 390E2 and/or 390E3 may be selectable to enable third party developers to view behavioral errors and actions related to correcting the behavioral errors.
In various embodiments, and after one or more actions involving correcting one or more behavioral errors of the voice robot, the voice robot may be further trained based at least in part on any added or synthesized training instances and/or any modified existing training instances. The voice robot may be further trained based at least in part on these training instances in the same or similar manner as described above with respect to fig. 2 and 3B. Notably, upon further training the voice robot, respective weights of one or more of the plurality of ML layers implemented by the at least one processor corresponding to the voice robot associated with the hypothetical cafe may be updated, while respective weights of one or more other layers may remain fixed. In some embodiments, the voice robot may be further trained in response to selection of one or more graphical elements (e.g., 381 and/or 390E 4). In additional or alternative embodiments, the speech robot may be further trained in response to determining that one or more conditions are satisfied. The one or more conditions may include, for example, determining a duration of time that has elapsed (e.g., nightly, weekly, and/or other durations), determining that no instances of the voice robot were implemented, determining that a number of changes to the corpus since the voice robot was last trained satisfies a number threshold, and/or other conditions.
In various embodiments, and after the voice robot is further trained based at least in part on any added or synthesized training instances and/or any modified existing training instances, the voice robot may be validated. In some embodiments, the voice robot may be validated based on validation training instances that are prohibited from being utilized in training the voice robot. In some additional or alternative embodiments, the voice robot may be authenticated based on user input directed to the voice robot development platform. For example, a third party developer may direct input to the graphical element 350 to initiate the verification session 352D. The updated voice robot may provide a response 352D1 and the session may continue as indicated by 354D1, 352D2, 352D3, 352D4, 354D2, and 352D 5. Notably, the session flow for authentication session 352D follows the same session flow as session 352C of fig. 3C. However, in contrast to the dialog 352C described with respect to fig. 3C, which was conducted by the voice robot before being updated, the updated voice robot correctly performs RPC, correctly solicits the value of the name parameter, and makes the reservation with the correct time. In some embodiments, the voice robot may be validated in response to selection of one or more graphical elements (e.g., 387). In additional or alternative embodiments, the voice robot may be validated in response to the voice robot being further trained. In addition, the voice robot performance metrics may be presented to third party developers. The voice robot performance metrics may include one or more measurement improvements (e.g., response accuracy, response time, and/or other performance metrics) of the voice robot.
While fig. 3C, 3D, and 3E are described herein with respect to particular errors classified as particular categories of errors, it is to be understood that this is for purposes of example and is not meant to be limiting. Furthermore, while only a single instance of a voice robot associated with a hypothetical cafe is described with respect to fig. 3A, 3B, 3C, 3D, and 3E, it is to be understood that this is for purposes of example and is not meant to be limiting. For example, multiple instances of a voice robot may be simultaneously implemented by one or more computing devices (e.g., client devices, servers, and/or other computing devices) such that the multiple instances of the voice robot may simultaneously participate in a conversation with a corresponding human (or other voice robot). Each instance of the voice robot may include a corresponding processor that utilizes a corresponding instance of the plurality of ML layers of the voice robot. Further, while the voice robot associated with the hypothetical cafe is described with respect to fig. 3A, 3B, 3C, 3D, and 3E as answering an incoming telephone call, it is to be understood that this is for purposes of example and is not meant to be limiting. For example, the voice bot may additionally or alternatively be trained to initiate outgoing telephone calls to various entities. For example, a training instance input for a given training instance may include an action or command to initiate a corresponding telephone call with a particular entity to perform a particular task, and a training instance output for the given training instance may include a corresponding ground truth response associated with initiating the outgoing telephone call. In this manner, the voice robot associated with the hypothetical cafe can be used to initiate and place outgoing telephone calls, to order more inventory from suppliers, to query software questions from information technology, to verify restaurant reservations with customers, and/or to perform any other function for which the voice robot is trained.
Referring now to FIG. 4, a flow diagram is depicted illustrating an example method 400 of updating a trained voice robot. For convenience, the operations of method 400 are described with reference to a system performing the operations. This system of method 400 includes at least one processor, at least one memory, and/or other components of a computing device (e.g., client device 110 of fig. 1, voice robot development platform 120 of fig. 1, and/or computing device 710 of fig. 7, a server, and/or other computing devices). Further, while the operations of method 400 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 452, the system identifies a trained voice robot associated with a third party and a corpus of training instances used to train the trained voice robot. The system can identify a trained voice robot and a corpus of training instances for training the trained voice robot based on user input directed to a voice robot development platform interface. The user input may be touch input, typed input, and/or spoken input received from a third party developer associated with the third party via a respective client device.
At block 454, the system obtains voice robot activity for the trained voice robot. The voice robot activity may include, for example, a corresponding session conducted by the trained voice robot on behalf of the third party, a corresponding session summary of the one or more corresponding sessions, behavioral errors associated with the one or more corresponding sessions, monetary information associated with the corresponding sessions, and orders or purchases associated with the corresponding sessions, and/or other information derived from the corresponding sessions. The voice robot activity may be stored in one or more databases (e.g., voice robot activity database 170A2 of fig. 1).
At block 456, the system identifies a given behavioral error for the trained voice robot based on processing the voice robot activity. By processing the voice robot activity using multiple ML layers of one or more ML models to identify a given behavioral error, the system can identify the given behavioral error of the trained voice robot. Further, the system can process the given behavioral error using multiple ML layers of the ML model or additional multiple ML layers of additional ML models to classify the given behavioral error as one or more different classes of behavioral errors. In particular, the system may process at least one or more corresponding portions of the corresponding conversation to identify the conversation that includes some misbehavior of the trained voice robot, and may further process the misbehavior to classify it as one or more different categories of behavioral errors. One or more different classes of behavioral errors may be associated with, for example, a missing feature error class, an error labeled feature error class, a feature-specific sparsity error class, a particular type of sparsity error class of training instances, an RPC outbound request error class, an RPC inbound request error class, and/or other classes of behavioral errors of the trained voice robot.
In some embodiments, the frame 456 may include a sub-frame 456A. If so, at sub-box 456A, the system identifies one or more training instances in the corpus that are associated with the given behavioral error. In embodiments that include the sub-box 456A, the system may obtain one or more correspondence inlays generated during a given correspondence session for which a voice robot behavioral error is identified. One or more corresponding embeddings may have been generated during a given corresponding session based on at least one or more of the portions of the given corresponding session and a corresponding previous context of the given corresponding session for each of the one or more portions of the given corresponding session. Further, the system can identify, based on the one or more corresponding embeddings, one or more training instances included in a corpus of training instances used to train the trained speech robot that are associated with the given behavioral error. For example, the system may: obtaining respective embeddings associated with one or more training instances generated when training the trained speech robot; comparing respective embeddings associated with the one or more training instances to one or more corresponding embeddings generated during the session for which the voice robot behavioral error was identified in the embedding space; and identifying one or more training instances associated with respective inlays that satisfy a distance criterion relative to the one or more corresponding inlays. These training instances, their mis-tagged features, their missing features, and/or the sparsity of these training instances may be the root cause of a given behavioral error of the trained speech robot.
At block 458, the system determines an action involving correcting the given behavioral error of the trained voice robot based on the given behavioral error of the trained voice robot. In various embodiments, an action directed to correcting a given behavioral error of a trained voice robot may be determined based on one or more different classes of behavioral errors into which the given behavioral error of the trained voice robot is classified. In some embodiments, block 458 may include a subframe 458A. If so, at sub-box 458A, the actions determined by the system may include modifying one or more training instances in the corpus for training the trained voice robot. For example, in embodiments where a given behavioral error is classified into a category associated with a missing feature error, the action involving correcting the given behavioral error may be a tagging action associated with tagging the missing features of one or more existing training instances. As another example, in embodiments that classify a given behavior error into a category associated with a false mark feature error, the action that involves correcting the given behavior error may be a relabeling action associated with relabeling one or more false mark features of one or more existing training instances.
In some additional or alternative embodiments, the block 458 may additionally or alternatively include a subframe 458B. If so, at sub-box 458B, the actions determined by the system may include adding one or more additional training instances to the corpus used to train the trained voice robot. For example, in implementations where a given behavior error is classified as a class associated with a sparse error, the action related to correcting the given behavior error may be a training instance action associated with adding one or more additional training instances to a corpus of training instances. The one or more additional training instances may include specific features requested by the system, and/or training instances of a specific type requested by the system. In some embodiments, one or more additional training instances may be defined by third party developers (e.g., as described with respect to fig. 3B), while in additional or alternative embodiments, one or more additional training instances may be synthesized by the system (e.g., as described with respect to fig. 3D and 3E).
At block 460, the system generates a notification to be presented to a third party developer associated with the third party based on the action involving correcting the given behavioral error of the trained voice robot. At block 462, the system causes a notification to be presented to the third party developer. The notification can be presented to the user via a user interface associated with the speech development platform, and can be presented to the user visually and/or audibly via a client device of the third party developer. In some embodiments, the notification may include an indication of a given behavioral error of the trained voice robot and an indication of a request by a third party developer to perform an action involving correcting the behavioral error of the voice robot. In some additional or alternative embodiments, the notification may include an indication of a given behavioral error of the trained voice robot and an indication that the system automatically performed an action involving correcting the given behavioral error of the voice robot.
At block 464, the system determines whether to update the voice robot based on an updated corpus of training instances, the updated corpus including at least one or more modified training instances (e.g., as described with respect to sub-box 458A) and/or one or more additional training instances (e.g., as described with respect to sub-box 458B). In some embodiments, the system may determine to update the voice robot in response to receiving user input related to updating the voice robot (e.g., selection of training voice robot selectable element 381 of fig. 3D and 3E). In additional or alternative embodiments, the system may determine to update the voice robot in response to determining that one or more conditions are satisfied. The one or more conditions may include, for example, determining a duration of time that has elapsed (e.g., nightly, weekly, and/or other durations), determining that no instances of the voice robot are implemented, determining that a number of changes to the corpus since the voice robot was last trained satisfies a number threshold, and/or other conditions. If at the iteration of block 464, the system determines not to update the voice robot, the system may continue to monitor when to update the voice robot at block 464. When the system monitors when to update the speech robot at block 464, the system may implement one or more additional instances of the operations of blocks 452 through 462 to identify additional behavioral errors and determine corresponding actions directed to correcting any additional behavioral errors identified. If at the iteration of block 464, the system determines to update the speech robot, the speech robot may be updated based on the updated corpus of training instances, and the system may proceed to block 466.
At block 466, the system causes the updated voice bot to be deployed for a conversation on behalf of a third party. For example, an updated voice robot may be deployed to answer incoming and/or originate outgoing telephone calls on behalf of third parties and conduct conversations with corresponding humans during the telephone calls, interact with humans at a car shuttle restaurant, act as an automated assistant, and/or be deployed to conduct any conversations on behalf of the third parties for human-machine conversations conducted on behalf of the third parties. Although not depicted in fig. 4 for simplicity, it should be understood that the updated voice bot may be validated before being deployed for conducting a session on behalf of a third party. In some embodiments, the voice robot may be validated based on a validation training instance that is prohibited from being utilized in the initial training and/or subsequent updating of the voice robot. In some additional or alternative embodiments, the voice robot may be verified based on user input directed to the voice robot development platform (e.g., as described with respect to fig. 3E). In embodiments where the updated voice robot is verified before being deployed, the updated voice robot may not be deployed unless verification of the updated voice robot indicates some measurement improvement over the updated voice robot that is currently deployed for conducting a conversation on behalf of a third party.
Turning now to fig. 5, a flow diagram is depicted illustrating an example method 500 of updating a trained voice robot based on additional training instances added to a corpus of training instances used to train the voice robot. For convenience, the operations of method 500 are described with reference to a system that performs the operations. This system of method 500 includes at least one processor, at least one memory, and/or other components of a computing device (e.g., client device 110 of fig. 1, voice robot development platform 120 of fig. 1, and/or computing device 710 of fig. 7, a server, and/or other computing devices). Further, while the operations of method 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 552, the system identifies a trained voice robot associated with a third party and a corpus of training instances used to train the trained voice robot. At block 554, the system obtains voice robot activity for the trained voice robot. The operations of blocks 552 and 554 may be performed in the same or similar manner as described above with respect to blocks 452 and 454, respectively.
At block 556, the system determines that the trained voice robot does not include the desired behavior. The system may determine that the trained voice robot does not include the desired behavior based on processing the voice robot activity. For example, assume that a trained voice robot is associated with a fictitious airline named hypothetical airline and that the voice should be able to alter existing flight reservations. However, further assume that the voice robot associated with the hypothetical airline never requests the frequent flyer number when any existing flight reservations are modified, but the frequent flyer number is the value of a parameter associated with modifying an existing flight reservation. In this example, requesting a frequent flyer number may be a desired behavior, but the voice robot associated with the hypothetical airline does not include such a desired behavior.
At block 558, the system determines whether to synthesize one or more additional training instances associated with the desired behavior. The system may determine whether to synthesize one or more additional training instances based on whether a third party developer enabled the system to synthesize one or more training instances. One or more synthetic training instances may be associated with a desired behavior (e.g., associated with a requesting frequent traveler number, having a corresponding feature emphasis input associated with a frequent traveler number feature, receiving a frequent traveler number value, etc.). Further, one or more synthetic training instances may be synthesized based on corresponding sessions from voice robot activity and/or augmenting existing training instances (e.g., as described with respect to fig. 3D). If at the iteration of block 558, the system determines to synthesize one or more additional training instances associated with the desired behavior, the system synthesizes one or more of the additional training instances and proceeds to block 566. Block 566 is described in more detail below. If, at an iteration of block 558, the system determines not to synthesize one or more additional training instances associated with the desired behavior, the system refrains from synthesizing one or more of the additional training instances and proceeds to block 560.
At block 560, the system generates a notification requesting a third party developer associated with the third party to add one or more additional training instances associated with the desired behavior to the corpus. At block 562, the system causes a notification to be presented to the third party developer. The notification can be presented to the user via a user interface associated with the speech development platform, and can be presented to the user visually and/or audibly via a client device of the third party developer. In some embodiments, the notification may request a threshold number of training instances associated with the desired behavior and an indication that the third party developer performed the request to add one or more training instances associated with the desired behavior. In some additional or alternative implementations, the notification may include an indication that one or more training instances associated with the desired behavior were synthesized by the system, and optionally a selectable element that, when selected, allows third party developers to view those training instances.
At block 564, the system determines whether any user input including one or more additional instances associated with the desired behavior was received from the third party developer in response to the notification being presented to the third party developer. The user input may be directed to a client device associated with a third party developer that is being used to update the voice bot. If, at an iteration of block 564, the system determines that no user input including one or more additional instances associated with the desired behavior has been received, the system may continue to monitor for user input at block 564. If at the iteration of block 564 the system determines that user input is received that includes one or more additional instances associated with the desired behavior, the system may proceed to block 566.
At block 566, the system causes the voice robot to be updated based on the one or more additional training instances. The voice robot may be updated based at least in part on one or more of the synthetic training instances and/or one or more of the training instances added by the third party developer in the same or similar manner as described above with respect to fig. 2 and 3B. Although not depicted in fig. 5 for simplicity, it should be understood that the system may determine whether to update the voice robot before the voice robot is updated (e.g., as described with respect to block 464 of fig. 4).
At block 568, the system determines whether to validate the updated voice bot. In some embodiments, the voice robot may be validated based on a validation training instance that is prohibited from being utilized in the initial training and/or subsequent updating of the voice robot. In some additional or alternative embodiments, the voice robot may be verified based on user input directed to the voice robot development platform (e.g., as described with respect to fig. 3E). The system may determine that the updated voice robot is validated when one or more conditions reflecting an improvement in measurements of the updated voice robot as compared to the voice robot currently deployed for conducting a conversation on behalf of the third party are satisfied. For example, the one or more conditions may include a validation of one or more of the updated plurality of ML layers or the plurality of additional ML layers when further training the voice robot, a convergence of one or more of the updated plurality of ML layers or the plurality of additional ML layers (e.g., with zero loss or within a threshold range of zero loss), a determination that one or more of the plurality of ML layers or the plurality of additional ML layers performs better than an instance (if any) of the voice robot currently being utilized (e.g., with respect to accuracy and/or recall), an occurrence of training based on at least a threshold number of training instances, and/or a duration of training based on the training instances. If at the iteration of block 568, the system determines that the updated speech robot is not verified, the system may return to block 558 and determine whether to synthesize one or more additional training instances and repeat the operations of blocks 558 through 566 to further update the trained speech robot. If at the iteration of block 568, the system determines that the updated voice robot is verified, the system may proceed to block 570.
At block 570, the system can cause the updated voice bot to be deployed for a conversation on behalf of a third party. For example, an updated voice robot may be deployed to answer incoming and/or originating outgoing telephone calls on behalf of a third party and to conduct conversations with a corresponding human during the telephone calls, to interact with the human at a car shuttle restaurant, to act as an automated assistant, and/or to conduct any conversations on behalf of the third party for human-machine conversations conducted on behalf of the third party.
Turning now to fig. 6, a flow diagram is depicted illustrating an example method 600 of updating a trained voice robot based on mis-tagged and/or missing features of training instances included in a corpus of training instances used to train the voice robot. For convenience, the operations of method 600 are described with reference to a system performing the operations. This system of method 600 includes at least one processor, at least one memory, and/or other components of a computing device (e.g., client device 110 of fig. 1, voice robot development platform 120 of fig. 1, and/or computing device 710 of fig. 7, a server, and/or other computing devices). Further, while the operations of method 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 652, the system identifies a trained voice robot associated with the third party and a corpus of training instances used to train the trained voice robot. At block 654, the system obtains voice robot activity for the trained voice robot. At block 656, the system identifies a given behavioral error for the trained voice robot based on processing the voice robot activity. The operations of blocks 652, 654, and 656 may be performed in the same or similar manner as described above with respect to blocks 452, 454, and 456, respectively.
At block 658, the system determines whether the given behavioral error is caused by one or more mis-tagged features of one or more corresponding training instances in the corpus and/or one or more missing features of one or more corresponding training instances in the corpus. The system may analyze a given behavioral error identified at block 656 to determine whether it is associated with one or more different classes of behavioral errors indicative of being caused by one or more training instances having one or more mis-tagged features or one or more missing features included in the corpus of training instances identified at block 652. If, at the iteration of block 658, the system determines that the given behavioral error was not caused by one or more incorrectly labeled features and/or missing features of one or more corresponding training instances in the corpus, the system may proceed to block 664. Block 664 is described below. If, at the iteration of block 658, the system determines that the given behavioral error is caused, at least in part, by one or more false tokens and/or missing features of one or more corresponding training instances in the corpus, the system may proceed to block 660.
At block 660, the system determines whether one or more missing features may be automatically flagged and/or whether one or more false flag features may be automatically relabeled. The system may determine whether one or more training instances having missing features may be automatically tagged and/or whether mistagged features may be relabeled based on NLU data or other data associated with one or more tags associated with one or more terms included in one or more training instances identified as having one or more mistagged features. For example, if the portion of the corresponding session on which the given training instance is generated includes one or more terms indicating time (e.g., "AM (AM)" or "PM (afternoon)"), the system may determine that one or more training instances may be flagged (or re-flagged if the one or more terms are associated with another different feature). If at an iteration of block 660 the system determines that one or more of the missing and/or mis-tagged features may be automatically flagged and/or re-flagged, the system may automatically re-flag one or more of the mis-tagged features and proceed to block 666. Block 666 is described below. If at the iteration of block 660 the system determines that one or more of the missing and/or false mark features may be automatically marked and/or relabeled, the system may proceed to block 662.
At block 662, the system causes a notification to be presented to the user requesting third party developers associated with the third party to re-label one or more mis-labeled features of corresponding training instances in the corpus and/or to add one or more missing features to corresponding training instances in the corpus. The notification can be presented to the user via a user interface associated with the speech development platform and can be presented to the user visually and/or audibly via a client device of the third party developer. In some embodiments, the notification may include an indication of the behavioral error identified at block 656, an indication that one or more training instances need to be marked and/or relabeled, an indication that one or more portions of one or more training instances need to be marked and/or relabeled, an indication of whether the system automatically marked and/or relabeled one or more training instances, and/or other information.
At block 664, the system determines whether user input is received from a third party developer who relabels one or more of the mis-tagged features and/or adds one or more tags of missing features. User input may be automatically received at block 664 until one or more error marked features are relabeled and/or one or more missing features are marked. When the third party developer completes relabeling one or more of the mis-labeled features and/or adding labels for one or more missing features, the voice robot may be updated based at least on the corresponding training instances with the labeled and/or relabeled features, and the system may proceed to block 666. The voice robot may be updated based at least in part on one or more of the synthetic training instances and/or one or more of the training instances added by the third party developer in the same or similar manner as described above with respect to fig. 2 and 3B.
At block 666, the system determines whether to authenticate the updated voice robot. When one or more conditions are satisfied, the system may determine that the updated voice robot is verified. The voice robot may be authenticated in the same or similar manner as described with respect to block 568 of FIG. 5. If, at the iterations of block 666, the system determines that the updated speech robot is not validated, the system may return to block 658 to identify one or more additional missing and/or mislabeled features in the corpus and repeat the operations of blocks 658 through 664 to further update the trained speech robot. If at the iteration of block 666, the system determines that the updated voice robot is verified, the system may proceed to block 668.
At block 668, the system may cause the updated voice bot to be deployed for a conversation on behalf of a third party. For example, an updated voice robot may be deployed to answer incoming and/or originating outgoing telephone calls on behalf of a third party and conduct conversations with a corresponding human during the telephone calls, interact with the human at a car shuttle restaurant, act as an automated assistant, and/or be deployed to conduct any conversations on behalf of the third party for human-machine conversations conducted on behalf of the third party.
Fig. 7 is a block diagram of an example computing device 710 that may optionally be used to perform one or more aspects of the techniques described herein. In some embodiments, one or more of the client device, cloud-based automated assistant component, and/or other components may comprise one or more components of the example computing device 710.
The user interface input devices 722 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or tablet, a scanner, a touch screen incorporated into the display, an audio input device such as a voice recognition system, microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 710 or onto a communication network.
User interface output devices 720 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide a non-visual display, such as via an audio output device. In general, use of the term "output device" is intended to include the various possible types of devices and ways to output information from computing device 710 to a user or to another machine or computing device.
These software modules are typically executed by processor 714, alone or in combination with other processors. Memory 725 used in storage subsystem 724 may include a plurality of memories including a main Random Access Memory (RAM) 730 for storing instructions and data during program execution and a Read Only Memory (ROM) 732 in which fixed instructions are stored. File storage subsystem 726 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical disk drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 726 in storage subsystem 724 or in other machines accessible by processor 714.
Where the systems described herein collect or otherwise monitor personal information about a user or information that may be personal and/or monitored may be utilized, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social behavior or activity, profession, the user's preferences, or the user's current geographic location) or whether and/or how to receive content from a content server that may be more relevant to the user. Also, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is purged. For example, the identity of the user may be processed such that no personally identifiable information for the user can be determined, or the geographic location of the user may be summarized (e.g., at a city, zip code, or state level) if geographic location information is obtained such that a particular geographic location of the user cannot be determined. Thus, the user may control how information is collected about the user and/or how the information is used.
In some embodiments, a method is provided that is implemented by one or more processors and includes: identifying, via a robot development system, a trained voice robot associated with a third party and a corpus of training instances for training the trained voice robot; and obtaining voice robot activity of the trained voice robot via the voice robot development system. The voice robot activity includes a plurality of previously conducted sessions between a trained voice robot on behalf of a third party and a corresponding human, and each of the previously conducted sessions includes at least a corresponding session and a corresponding previous context of one or more portions of the corresponding session. The method further comprises: identifying, via the voice robot development system and based on processing the plurality of previously conducted sessions, a given behavioral error of the trained voice robot; determining, via the voice robot development system and based on the given behavioral error of the trained voice robot, an action involving correcting the given behavioral error of the trained voice robot; and causing a notification to be presented to a third party developer via a user interface of the robot development system based on the action involving correcting the given behavioral error of the trained voice robot, wherein the third party developer is associated with the trained voice robot.
These and other embodiments of the technology disclosed herein may optionally include one or more of the following features.
In some embodiments, identifying a given behavioral error of the trained voice robot may include: processing one or more of the portions of a given corresponding session of the plurality of previously conducted sessions and a given corresponding previous context for the given corresponding session of the one or more of the portions of the given corresponding session using a plurality of Machine Learning (ML) layers of an ML model to identify a given behavioral error; and processing the given behavioral error using the plurality of ML layers of the ML model or an additional plurality of ML layers of the additional ML model to classify the given behavioral error into one or more different classes of behavioral errors.
In some versions of these embodiments, the method may further comprise: obtaining, via the voice robot development system, an embedding generated during a given corresponding session based at least on one or more of the portions of the given corresponding session and a given corresponding previous context of the given corresponding session; and identifying, via the speech robot development system and based on the embedding, one or more of the training instances included in the corpus of training instances used to train the trained speech robot that are associated with the given behavioral error =.
In some further versions of these embodiments, identifying one or more training instances associated with the given behavioral error based on the embedding may include: obtaining, via the voice robot development system, a corresponding embedding associated with each of the one or more training instances; comparing, via the voice robot development system and in the embedding space, the embedding generated during the given corresponding session to the corresponding embedding associated with each of the one or more training instances; determining, based on the comparison, a corresponding distance metric between the embedding generated during the given corresponding session and the corresponding embedding associated with each of the one or more training instances; and identifying, via the voice robot development system and based on the corresponding distance metric, one or more training instances associated with the given behavioral error.
In some versions of these embodiments, the act of determining a given behavioral error relating to correcting the trained voice robot may be based on a given category of the one or more different categories into which the given behavioral error of the trained voice robot may be classified.
In some further versions of these embodiments, the given category may be associated with a category of incorrectly labeled features indicative of one or more of the training instances with incorrectly labeled features included in the corpus used to train the trained speech robot. In other further versions of these embodiments, the action directed to correcting the given behavioral error of the trained voice robot may include relabeling one or more features of one or more of the training instances, wherein the relabeling of the one or more of the features of the one or more of the training instances is performed automatically by the voice robot development system, and wherein the notification presented to the third party developer includes an indication of: a given behavioral error identified by the voice robot development system, and an action involving correcting the given behavioral error of the trained voice robot, the given behavioral error being automatically performed by the voice robot development system. In even further versions of these embodiments, the notification presented to the third party developer may include an indication of: the method further includes identifying a given behavioral error identified by the voice robot development system, one or more of the training instances, and performing, by a third party developer, a request to relabel one or more of the features of the one or more of the training instances. In even other further versions of these embodiments, a notification may be presented to the developer in response to determining that the speech robot development platform is unable to relabel one or more of the features of one or more of the training instances.
In additional or alternative further versions of these embodiments, a given class may be associated with a missing feature class that indicates one or more of the training instances with missing features included in the corpus used to train the trained speech robots. In other additional versions of these embodiments, the notification presented to the third party developer may include an indication of: the method further includes identifying a given behavioral error identified by the voice robot development system, one or more of the training instances, and a request for a third party developer to label one or more of the features of the one or more of the training instances.
In additional or alternative further versions of these embodiments, a given class may be associated with a sparsity class indicating that one or more additional training instances having particular features need to be added to the corpus used to train the trained speech robot. In other further versions of these embodiments, the action directed to correcting the given behavioral error of the trained speech robot may include synthesizing one or more of the additional training instances. Synthesizing one or more of the additional training instances may be performed automatically by the voice robot development system, and the notification presented to the third party developer includes an indication of: a given behavioral error recognized by the voice robot development system, and an action that involves correcting the given behavioral error of the trained voice robot, the given behavioral error being automatically performed by the voice robot development system. In additional or alternative versions of these other additional embodiments, the notification presented to the third party developer may include an indication of: a given behavioral error identified by the voice robot development system, a particular feature, and a request to add one or more of additional training instances including the particular feature to a third party developer.
In some embodiments, the action directed to correcting the given behavioral error may include updating a corpus of training instances used to train the trained speech robot to generate an updated corpus. Updating a corpus of training instances used to train the trained speech robot to generate an updated corpus may include: modifying one or more training instances included in a corpus of training instances used to train trained speech robots; or adding one or more additional training instances to a corpus of training instances used to train the trained speech robot. In some versions of these embodiments, the method may further comprise, after updating the corpus of training instances used to train the trained speech robot: causing the trained voice robot to be updated based on the updated corpus of training instances to generate an updated voice robot; and causing the updated voice bot to be deployed for additional sessions on behalf of the third party.
In some further versions of these embodiments, causing the trained voice robot to be updated based on the updated corpus of training instances to generate an updated voice robot may be in response to receiving user input via a user interface of the voice robot development system that updates the trained voice robot. In some additional or alternative further versions of these embodiments, the method may further include after causing the trained voice robot to be updated and before causing the updated voice robot to be deployed, validating the updated voice robot to determine whether the given behavioral error has been corrected. Causing the updated voice robot to be deployed may be in response to determining that a given behavioral error has been corrected.
In some embodiments, the notification presented to the third party developer includes an indication of: a given behavioral error identified by the voice robot development system; and the third party developer performing a request for an action involving correcting the given behavioral error of the trained voice robot. In some versions of these embodiments, an indication of a request by the third party developer to perform an action related to correcting the given behavioral error of the trained voice robot may be included in the notification in response to determining that the voice robot development system is unable to automatically perform the action related to correcting the given behavioral error of the trained voice robot. In some additional or alternative versions of these embodiments, the method may further include receiving user input from the third party developer via a user interface of the voice robot development system and in response to the notification being presented to the third party developer to perform an action directed to correcting the given behavioral error of the trained voice robot.
In some embodiments, determining the action that involves correcting the given behavioral error of the trained voice robot may be in response to determining that a number of occurrences of the given behavioral error satisfies a number threshold.
In some embodiments, a method is provided that is implemented by one or more processors and includes: identifying, via a robot development system, a trained voice robot associated with a third party and a corpus of training instances for training the trained voice robot; and obtaining voice robot activity of the trained voice robot via the voice robot development system. The voice robot activity includes a plurality of previously conducted sessions between a trained voice robot on behalf of a third party and a corresponding human, and each of the previously conducted sessions includes at least a corresponding session and a corresponding previous context of one or more portions of the corresponding session. The method further comprises: determining, via the voice robot development system and based on a plurality of previously conducted sessions, that the trained voice robot does not include the desired behavior; generating, via a speech robot development platform, a notification indicating that one or more additional training instances need to be added to a corpus of training instances to generate an updated corpus of training instances, the one or more additional training instances associated with a desired behavior; causing a notification to be presented to a third party developer associated with a third party via a user interface of the robotic development system; and after obtaining one or more additional training instances: causing the trained voice robot to be updated based on the updated corpus of training instances to generate an updated voice robot; and causing the updated voice robot to replace the trained voice robot for the additional conversation on behalf of the third party.
These and other embodiments of the technology disclosed herein may optionally include one or more of the following features.
In some embodiments, the notification indicating that one or more additional training instances need to be added to the corpus of training instances may include a request by a third party developer to provide one or more of the additional training instances. In some versions of these embodiments, obtaining the one or more additional training instances may include receiving, via the speech robot development platform, user input defining one or more of the additional training instances associated with the desired behavior, the user input received in response to the notification. The one or more additional training instances may include feature emphasized inputs that were not previously defined in training instances of the corpus.
In some embodiments, the trained speech robot may correspond to one or more processors utilizing multiple Machine Learning (ML) layers of one or more ML models. In some versions of these embodiments, causing the trained speech robot to be updated based on the updated corpus of training instances to generate an updated speech robot may include: causing corresponding weights of one or more of the plurality of ML layers to be updated based on processing the one or more additional training instances; and causing the additional corresponding weight of one or more of the plurality of ML layers to be fixed.
In some embodiments, a method is provided that is implemented by one or more processors and includes: identifying, via a robot development system, a trained voice robot associated with a third party and a corpus of training instances for training the trained voice robot; and obtaining voice robot activity of the trained voice robot via the voice robot development system. The voice robot activity includes a plurality of previously conducted sessions between a trained voice robot on behalf of a third party and a corresponding human, and each of the previously conducted sessions includes at least a corresponding session and a corresponding previous context of one or more portions of the corresponding session. The method further includes identifying, via the voice robot development system and based on processing the plurality of previously conducted sessions, a given behavioral error of the trained voice robot; determining, via the speech robot development system and based on a given behavioral error of the trained speech robot, that one or more features of one or more corresponding training instances included in the corpus are missing or mislabeled; and causing, via a user interface of the robotic development system, a notification to be presented to the third-party developer, the notification indicating that one or more of the features of one or more of the corresponding training instances included in the corpus are missing or mislabeled.
These and other embodiments of the technology disclosed herein can optionally include one or more of the following features.
In some embodiments, the notification indicating that one or more features of one or more corresponding training instances are missing or mislabeled may include a request for a third party developer to add a label or re-label for the one or more corresponding training instances.
Additionally, some embodiments include one or more processors (e.g., central Processing Units (CPUs), graphics Processing Units (GPUs), and/or Tensor Processing Units (TPUs)) of one or more computing devices, wherein the one or more processors are operable to execute instructions stored in an associated memory, and wherein the instructions are configured to perform any of the above-described methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the above-described methods. Some embodiments also include a computer program product comprising instructions executable by one or more processors to perform any of the above methods.
It is to be understood that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the beginning of this disclosure are contemplated as being part of the subject matter disclosed herein.
Claims (31)
1. A method implemented by one or more processors, the method comprising:
identifying, via a robot development system, a trained voice robot associated with a third party and a corpus of training instances for training the trained voice robot;
obtaining, via the voice robot development system, voice robot activities of the trained voice robot, wherein the voice robot activities include a plurality of previously conducted sessions between the trained voice robot on behalf of the third party and a corresponding human, and wherein each of the previously conducted sessions includes at least a corresponding session and a corresponding previous context for one or more portions of the corresponding session;
identifying, via the voice robot development system and based on processing the plurality of previously conducted sessions, a given behavioral error of the trained voice robot;
determining, via the voice robot development system and based on the given behavioral error of the trained voice robot, an action involving correcting the given behavioral error of the trained voice robot; and
causing a notification to be presented to a third party developer via a user interface of the robot development system based on the action directed to correcting the given behavioral error of the trained voice robot, wherein the third party developer is associated with the trained voice robot.
2. The method of claim 1, wherein identifying the given behavioral error of the trained voice robot comprises:
processing one or more of the portions of a given corresponding session of the plurality of previously conducted sessions and a given corresponding previous context for the given corresponding session of one or more of the portions of the given corresponding session using a plurality of ML layers of a machine-learned ML model to identify the given behavioral error; and
processing the given behavioral error using the plurality of ML layers of the ML model or additional ML models to classify the given behavioral error as one or more different categories of behavioral errors.
3. The method of claim 2, further comprising:
obtaining, via the voice robot development system, an embedding generated during the given corresponding session based at least on one or more of the portions of the given corresponding session and the given corresponding previous context of the given corresponding session; and
identifying, via the speech robot development system and based on the embedding, one or more of the training instances included in the corpus of training instances used to train the trained speech robot that are associated with the given behavioral error.
4. The method of claim 3, wherein identifying the one or more training instances associated with the given behavioral error based on the embedding comprises:
obtaining, via the voice robot development system, a corresponding embedding associated with each of the one or more training instances;
comparing, via the voice robot development system and in an embedding space, the embedding generated during the given corresponding session with the corresponding embedding associated with each of the one or more training instances;
determining, based on the comparison, a corresponding distance metric between the embedding generated during the given corresponding session and the corresponding embedding associated with each of the one or more training instances; and
identifying, via the voice robot development system and based on the corresponding distance metric, the one or more training instances associated with the given behavioral error.
5. The method of any of claims 2 to 4, wherein determining the action related to correcting the given behavioral error of the trained voice robot is based on a given category of the one or more different categories into which the given behavioral error of the trained voice robot is classified.
6. The method of claim 5, wherein the given category is associated with a false labeled feature category indicative of one or more of the training instances with false labeled features included in the corpus used to train the trained speech robot.
7. The method of claim 6, wherein the action directed to correcting the given behavioral error of the trained voice robot comprises relabeling one or more features of one or more of the training instances, wherein the relabeling of one or more of the features of one or more of the training instances is performed automatically by the voice robot development system, and wherein the notification presented to the third party developer comprises an indication of:
the given behavioral error recognized by the voice robot development system, an
The actions performed automatically by the voice robot development system.
8. The method of claim 6, wherein the notification presented to the third party developer includes an indication of:
the given behavioral error recognized by the voice robot development system,
one or more of the training instances, an
Performing the re-labeling request for one or more of the features of one or more of the training instances for the third party developer.
9. The method of claim 8, wherein the notification is presented to the developer in response to determining that the voice robot development system cannot relabel one or more of the features of one or more of the training instances.
10. The method of claim 5, wherein the given class is associated with a missing feature class that indicates one or more of the training instances with missing features included in the corpus used to train the trained speech robots.
11. The method of claim 10, wherein the notification presented to the third party developer includes an indication of:
the given behavioral error recognized by the voice robot development system,
one or more of the training instances, an
A request for the third party developer to label one or more of the features of one or more of the training instances.
12. The method of claim 5, wherein the given class is associated with a sparsity class indicating that one or more additional training instances with particular features need to be added to the corpus used to train the trained speech robot.
13. The method of claim 12, wherein the action directed to correcting the given behavioral error of the trained voice robot includes synthesizing one or more of additional training instances, wherein synthesizing one or more of the additional training instances is performed automatically by the voice robot development system, and wherein the notification presented to the third party developer includes an indication of:
the given behavioral error recognized by the voice robot development system, an
The actions performed automatically by the voice robot development system.
14. The method of claim 12, wherein the notification presented to the third party developer includes an indication of:
the given behavioral error recognized by the voice robot development system,
the specific features, and
a request to the third party developer to add one or more of the additional training instances that include the particular feature.
15. The method of claim 1, wherein the action directed to correcting the given behavioral error includes updating the corpus of training instances used to train the trained voice robot to generate an updated corpus, and wherein updating the corpus of training instances used to train the trained voice robot to generate the updated corpus comprises:
modifying one or more training instances included in the corpus of training instances used to train the trained speech robot; or
Adding one or more additional training instances to the corpus of training instances used to train the trained speech robot.
16. The method of claim 15, further comprising:
after updating the corpus of training instances used to train the trained speech robot:
causing the trained voice robot to be updated based on the updated corpus of training instances to generate an updated voice robot; and
causing the updated voice robot to be deployed for additional conversations on behalf of the third party.
17. The method of claim 16, wherein causing the trained voice robot to be updated based on the updated corpus of training instances to generate the updated voice robot is in response to receiving user input via a user interface of the voice robot development system that updates the trained voice robot.
18. The method of claim 16 or claim 17, further comprising:
after causing the trained voice robot to be updated and before causing the updated voice robot to be deployed:
validating the updated voice robot to determine whether the given behavioral error has been corrected; and
wherein causing the updated voice robot to be deployed is in response to determining that the given behavioral error has been corrected.
19. The method of claim 1, wherein the notification presented to the third party developer includes an indication of:
the given behavioral error recognized by the voice robot development system, an
The third party developer performs a request for the action involving correcting the given behavioral error of the trained voice robot.
20. The method of claim 19, wherein the indication of the request by the third party developer to perform the action related to correcting the given behavioral error of the trained voice robot is included in the notification in response to determining that the voice robot development system cannot automatically perform the action related to correcting the given behavioral error of the trained voice robot.
21. The method of claim 19 or claim 20, further comprising:
receiving user input from the third party developer via a user interface of the voice robot development system and in response to the notification being presented to the third party developer to perform the action involving correcting the given behavioral error of the trained voice robot.
22. The method of any preceding claim, wherein determining the action involving correcting the given behavioral error of the trained voice robot is in response to determining that a number of occurrences of the given behavioral error satisfies a number threshold.
23. A method implemented by one or more processors, the method comprising:
identifying, via a robot development system, a trained voice robot associated with a third party and a corpus of training instances for training the trained voice robot;
obtaining, via the voice robot development system, voice robot activity of the trained voice robot, wherein the voice robot activity comprises a plurality of previously conducted sessions between the trained voice robot on behalf of the third party and a corresponding human, and wherein each of the previously conducted sessions comprises at least a corresponding session and a corresponding previous context of one or more portions of the corresponding session;
determining, via the voice robot development system and based on the plurality of previously conducted sessions, that the trained voice robot does not include a desired behavior;
generating, via the speech robot development system, a notification indicating that one or more additional training instances need to be added to the corpus of training instances to generate an updated corpus of training instances, the one or more additional training instances associated with the desired behavior;
causing the notification to be presented to a third party developer associated with the third party via a user interface of the robotic development system; and
after obtaining the one or more additional training instances:
causing the trained voice robot to be updated based on the updated corpus of training instances to generate an updated voice robot; and
causing the updated voice robot to replace the trained voice robot for additional conversations on behalf of the third party.
24. The method of claim 23, wherein the notification indicating that one or more additional training instances need to be added to the corpus of training instances comprises a request by the third party developer to provide one or more of the additional training instances.
25. The method of claim 23 or claim 24, wherein obtaining the one or more additional training instances comprises:
receiving, via the speech robot development system, user input defining one or more of the additional training instances associated with the desired behavior, the user input received in response to the notification, wherein the one or more additional training instances include feature emphasis input not previously defined in the training instances of the corpus.
26. The method of claim 23, wherein the trained speech robot corresponds to one or more processors utilizing one or more machine-learned ML layers of an ML model.
27. The method of claim 26, wherein causing the trained voice robot to be updated based on the updated corpus of training instances to generate the updated voice robot comprises:
causing corresponding weights for one or more of the plurality of ML layers to be updated based on processing the one or more additional training instances; and
causing additional corresponding weights of one or more of the plurality of ML layers to be fixed.
28. A method implemented by one or more processors, the method comprising:
identifying, via a robot development system, a trained voice robot associated with a third party and a corpus of training instances for training the trained voice robot;
obtaining, via the voice robot development system, voice robot activity of the trained voice robot, wherein the voice robot activity comprises a plurality of previously conducted sessions between the trained voice robot on behalf of the third party and a corresponding human, and wherein each of the previously conducted sessions comprises at least a corresponding session and a corresponding previous context of one or more portions of the corresponding session;
identifying, via the voice robot development system and based on processing the plurality of previously conducted sessions, a given behavioral error of the trained voice robot;
determining, via the speech robot development system and based on the given behavioral error of the trained speech robot, that one or more features are missing or mislabeled for one or more corresponding training instances included in the corpus; and
causing, via a user interface of the robotic development system, a notification to be presented to a third party developer indicating that one or more of the features correspond to one or more of the corresponding training instances included in the corpus are missing or mislabeled.
29. The method of claim 28, wherein the notification indicating that one or more features are missing or mislabeled for one or more corresponding training instances comprises a request for the third party developer to add or re-label one or more corresponding training instances.
30. A system, comprising:
one or more processors; and
a memory storing instructions that, when executed, cause one or more of the processors to perform any of the methods of any of claims 1-28.
31. A non-transitory computer-readable storage medium storing instructions that, when executed, cause one or more processors to perform any of the methods of any of claims 1-28.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/170,300 US11902222B2 (en) | 2021-02-08 | 2021-02-08 | Updating trained voice bot(s) utilizing example-based voice bot development techniques |
US17/170,300 | 2021-02-08 | ||
PCT/US2021/060360 WO2022169495A1 (en) | 2021-02-08 | 2021-11-22 | Updating trained voice bot(s) utilizing example based voice bot development techniques |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115836304A true CN115836304A (en) | 2023-03-21 |
Family
ID=78918779
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180048384.2A Pending CN115836304A (en) | 2021-02-08 | 2021-11-22 | Updating trained voice robots using example-based voice robot development techniques |
Country Status (4)
Country | Link |
---|---|
US (2) | US11902222B2 (en) |
EP (1) | EP4147153A1 (en) |
CN (1) | CN115836304A (en) |
WO (1) | WO2022169495A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220366896A1 (en) * | 2021-05-11 | 2022-11-17 | AskWisy, Inc. | Intelligent training and education bot |
US11875785B2 (en) * | 2021-08-27 | 2024-01-16 | Accenture Global Solutions Limited | Establishing user persona in a conversational system |
Family Cites Families (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US1069897A (en) | 1912-05-27 | 1913-08-12 | Frederick W Nolte | Eyeglass-mounting. |
US6925452B1 (en) | 2000-05-22 | 2005-08-02 | International Business Machines Corporation | Method and system for recognizing end-user transactions |
US20150278706A1 (en) | 2014-03-26 | 2015-10-01 | Telefonaktiebolaget L M Ericsson (Publ) | Method, Predictive Analytics System, and Computer Program Product for Performing Online and Offline Learning |
CN114357128A (en) * | 2016-04-18 | 2022-04-15 | 谷歌有限责任公司 | Automated assistant invocation of appropriate agents |
US10949748B2 (en) | 2016-05-13 | 2021-03-16 | Microsoft Technology Licensing, Llc | Deep learning of bots through examples and experience |
CN107590153B (en) * | 2016-07-08 | 2021-04-27 | 微软技术许可有限责任公司 | Conversational relevance modeling using convolutional neural networks |
CN108229686B (en) | 2016-12-14 | 2022-07-05 | 阿里巴巴集团控股有限公司 | Model training and predicting method and device, electronic equipment and machine learning platform |
WO2019028261A1 (en) | 2017-08-02 | 2019-02-07 | [24]7.ai, Inc. | Method and apparatus for training of conversational agents |
US10884598B2 (en) | 2017-09-29 | 2021-01-05 | Oracle International Corporation | Analytics for a bot system |
US11024294B2 (en) * | 2017-12-29 | 2021-06-01 | DMAI, Inc. | System and method for dialogue management |
US11386326B2 (en) | 2018-06-11 | 2022-07-12 | The Regents Of The University Of California | Training a machine learning model with limited training data |
US10861456B2 (en) | 2018-09-17 | 2020-12-08 | Adobe Inc. | Generating dialogue responses in end-to-end dialogue systems utilizing a context-dependent additive recurrent neural network |
US10853577B2 (en) | 2018-09-21 | 2020-12-01 | Salesforce.Com, Inc. | Response recommendation system |
US11113475B2 (en) | 2019-04-15 | 2021-09-07 | Accenture Global Solutions Limited | Chatbot generator platform |
US11461311B2 (en) * | 2019-04-26 | 2022-10-04 | Oracle International Corporation | Bot extensibility infrastructure |
US11206229B2 (en) | 2019-04-26 | 2021-12-21 | Oracle International Corporation | Directed acyclic graph based framework for training models |
US11651033B2 (en) * | 2019-04-26 | 2023-05-16 | Oracle International Corporation | Insights into performance of a bot system |
US10691897B1 (en) * | 2019-08-29 | 2020-06-23 | Accenture Global Solutions Limited | Artificial intelligence based virtual agent trainer |
US20210157989A1 (en) | 2019-11-22 | 2021-05-27 | Genesys Telecommunications Laboratories, Inc. | Systems and methods for dialog management |
US11379227B2 (en) * | 2020-10-03 | 2022-07-05 | Microsoft Technology Licensing, Llc | Extraquery context-aided search intent detection |
US11804211B2 (en) | 2020-12-04 | 2023-10-31 | Google Llc | Example-based voice bot development techniques |
US11854540B2 (en) * | 2021-01-08 | 2023-12-26 | Accenture Global Solutions Limited | Utilizing machine learning models to generate automated empathetic conversations |
US11373131B1 (en) * | 2021-01-21 | 2022-06-28 | Dell Products L.P. | Automatically identifying and correcting erroneous process actions using artificial intelligence techniques |
-
2021
- 2021-02-08 US US17/170,300 patent/US11902222B2/en active Active
- 2021-11-22 CN CN202180048384.2A patent/CN115836304A/en active Pending
- 2021-11-22 WO PCT/US2021/060360 patent/WO2022169495A1/en unknown
- 2021-11-22 EP EP21827490.0A patent/EP4147153A1/en active Pending
-
2024
- 2024-01-03 US US18/403,401 patent/US20240146668A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20220255885A1 (en) | 2022-08-11 |
EP4147153A1 (en) | 2023-03-15 |
WO2022169495A1 (en) | 2022-08-11 |
US11902222B2 (en) | 2024-02-13 |
US20240146668A1 (en) | 2024-05-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR102337820B1 (en) | User programmable automation assistant | |
US11488601B2 (en) | Dependency graph conversation modeling for use in conducting human-to-computer dialog sessions with a computer-implemented automated assistant | |
US11804211B2 (en) | Example-based voice bot development techniques | |
US11868727B2 (en) | Context tag integration with named entity recognition models | |
JP2023530423A (en) | Entity-Level Data Augmentation in Chatbots for Robust Named Entity Recognition | |
US20240146668A1 (en) | Updating trained voice bot(s) utilizing example-based voice bot development techniques | |
CN116802629A (en) | Multi-factor modeling for natural language processing | |
CN116583837A (en) | Distance-based LOGIT values for natural language processing | |
CN116547676A (en) | Enhanced logic for natural language processing | |
CN116490879A (en) | Method and system for over-prediction in neural networks | |
CN116635862A (en) | Outside domain data augmentation for natural language processing | |
US20220058347A1 (en) | Techniques for providing explanations for text classification | |
US20230419964A1 (en) | Resolving unique personal identifiers during corresponding conversations between a voice bot and a human | |
US20220180858A1 (en) | Example-based voice bot development techniques | |
KR20230006900A (en) | Example-based voice bot development techniques |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |