JP7247352B2 - Systems, User Interfaces, and Methods for Interactive Negative Explanation of Machine Learning Localization Models in Medical Applications - Google Patents
Systems, User Interfaces, and Methods for Interactive Negative Explanation of Machine Learning Localization Models in Medical Applications Download PDFInfo
- Publication number
- JP7247352B2 JP7247352B2 JP2021540194A JP2021540194A JP7247352B2 JP 7247352 B2 JP7247352 B2 JP 7247352B2 JP 2021540194 A JP2021540194 A JP 2021540194A JP 2021540194 A JP2021540194 A JP 2021540194A JP 7247352 B2 JP7247352 B2 JP 7247352B2
- Authority
- JP
- Japan
- Prior art keywords
- image
- machine learning
- user
- display
- regions
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G16—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR SPECIFIC APPLICATION FIELDS
- G16H—HEALTHCARE INFORMATICS, i.e. INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR THE HANDLING OR PROCESSING OF MEDICAL OR HEALTHCARE DATA
- G16H30/00—ICT specially adapted for the handling or processing of medical images
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/033—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor
- G06F3/0354—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor with detection of 2D relative movements between the device, or an operating part thereof, and a plane or surface, e.g. 2D mice, trackballs, pens or pucks
- G06F3/03543—Mice or pucks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04883—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures for inputting data by handwriting, e.g. gesture or text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/0002—Inspection of images, e.g. flaw detection
- G06T7/0012—Biomedical image inspection
-
- G—PHYSICS
- G16—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR SPECIFIC APPLICATION FIELDS
- G16H—HEALTHCARE INFORMATICS, i.e. INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR THE HANDLING OR PROCESSING OF MEDICAL OR HEALTHCARE DATA
- G16H50/00—ICT specially adapted for medical diagnosis, medical simulation or medical data mining; ICT specially adapted for detecting, monitoring or modelling epidemics or pandemics
- G16H50/20—ICT specially adapted for medical diagnosis, medical simulation or medical data mining; ICT specially adapted for detecting, monitoring or modelling epidemics or pandemics for computer-aided diagnosis, e.g. based on medical expert systems
-
- G—PHYSICS
- G16—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR SPECIFIC APPLICATION FIELDS
- G16H—HEALTHCARE INFORMATICS, i.e. INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR THE HANDLING OR PROCESSING OF MEDICAL OR HEALTHCARE DATA
- G16H50/00—ICT specially adapted for medical diagnosis, medical simulation or medical data mining; ICT specially adapted for detecting, monitoring or modelling epidemics or pandemics
- G16H50/30—ICT specially adapted for medical diagnosis, medical simulation or medical data mining; ICT specially adapted for detecting, monitoring or modelling epidemics or pandemics for calculating health indices; for individual health risk assessment
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10056—Microscopic image
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10072—Tomographic images
- G06T2207/10081—Computed x-ray tomography [CT]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10072—Tomographic images
- G06T2207/10088—Magnetic resonance imaging [MRI]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10116—X-ray image
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20084—Artificial neural networks [ANN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30004—Biomedical image processing
- G06T2207/30024—Cell structures in vitro; Tissue sections in vitro
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30004—Biomedical image processing
- G06T2207/30041—Eye; Retina; Ophthalmic
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30004—Biomedical image processing
- G06T2207/30068—Mammography; Breast
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30004—Biomedical image processing
- G06T2207/30096—Tumor; Lesion
Description
優先権
本出願は、2019年1月11日に出願された米国特許出願第16/246,156号の優先権の利益を主張する。
PRIORITY This application claims the priority benefit of US Patent Application No. 16/246,156, filed January 11, 2019.
本開示は、機械学習位置特定モデルによって生成された陰性予測を対話式に評価するためのシステム、ユーザインターフェース、および方法に関する。本開示の教示は、顕微鏡スライド検査、マンモグラムでの乳癌や他の放射線モダリティ(例えばX線、CT、MRI)での他のタイプの癌の診断、写真画像(皮膚科学)、さらに他のものなどの機械学習医療用途を含む、さまざまな分野に応用される。本教示は、冶金、部品検査、半導体製造、およびその他など、他の分野にも用途を有し、その場合、機械学習位置特定モデルは、入力画像データセットに基づいて予測を行うことになり、予測が陰性であり、ユーザはモデルにさらに問合せしようと試みる。 The present disclosure relates to systems, user interfaces, and methods for interactively evaluating negative predictions generated by machine learning localization models. The teachings of the present disclosure are useful for diagnosing breast cancer with microscopic slide examination, mammograms and other types of cancer with other radiological modalities (e.g., X-ray, CT, MRI), photographic imaging (dermatology), and others. It has applications in a variety of fields, including machine learning medical applications in The present teachings also have applications in other areas, such as metallurgy, parts inspection, semiconductor manufacturing, and others, where the machine learning localization model will make predictions based on the input image dataset, The prediction is negative and the user attempts to query the model further.
いくつかの医療用途への機械学習モデルの使用については、特許文献および技術文献に記載されている。一例では、そのようなモデルは、病理医が患者のまたは患者から得られた標本の2Dまたは3Dボリューム画像中の疾患の存在を特定するのを支援するために開発されている。例えば、病理医は、例えば生体組織検査から取得されたリンパ節組織、乳癌組織または前立腺癌組織などの組織の拡大デジタル画像中に腫瘍細胞(すなわち癌)が存在するかどうかを判定しようと試みていることがある。別の例として、機械学習モデルは、放射線科医がマンモグラムまたは胸部X線画像中の癌性細胞を検出するのを支援することが可能である。機械学習モデルは、典型的には畳み込みニューラルネットワークまたは当技術分野で知られる他の分類法を使用して、トレーニングデータセット(画像セット)から癌性の細胞または組織を認識するようにトレーニングされる。 The use of machine learning models for several medical applications is described in the patent and technical literature. In one example, such models have been developed to assist pathologists in identifying the presence of disease in 2D or 3D volumetric images of patients or of specimens obtained from patients. For example, pathologists attempt to determine whether tumor cells (ie, cancer) are present in magnified digital images of tissue, such as lymph node tissue, breast cancer tissue, or prostate cancer tissue, obtained from a biopsy. sometimes As another example, machine learning models can help radiologists detect cancerous cells in mammograms or chest X-ray images. Machine learning models are trained to recognize cancerous cells or tissues from a training data set (image set), typically using convolutional neural networks or other classification methods known in the art. .
「モデル説明」問題に対処するさまざまな技法およびツールが知られている。モデル説明は、人間が読み取れる様式で、機械学習モデルがある特定の推奨をした(例えば患者を癌と診断した)理由を正当化するプロセスである。深層学習モデル予測は、説明が困難であることがよく知られている。これは、YouTube（登録商標）ビデオランキングなどの使用事例では許容できるが、医学などの影響力の強い用途における使用事例にとっては完全に容認できない。病理医および他の医療専門家は、予測を信頼するために、モデル予測がどんなものであるかのみならず、モデル予測がなぜそうなっているのかについても知りたいと思う。 Various techniques and tools are known to address the "model explanation" problem. Model explanation is the process of justifying, in a human-readable fashion, why a machine learning model made a particular recommendation (eg diagnosed a patient with cancer). Deep learning model predictions are notoriously difficult to explain. While this is acceptable for use cases such as YouTube® video rankings, it is completely unacceptable for use cases in high impact applications such as medicine. Pathologists and other medical professionals want to know not only what the model predictions are, but also why they are the way they are, in order to trust the predictions.
本譲受人に関わる研究者らは、モデル予測について説明するためのいくつかの基本的な方法を開発してきた。例えば、サンプルまたは画像が「陽性」(例えば癌がある、または癌の可能性が高い)と診断された場合、次の方法が使用されてきた。(1)検出モデルによってもたらされ、後に分類モデルによって分類された、疑わしい病変の周りのバウンディングボックスが、ユーザに提示される。例を図1Aに示す、(2)典型的には特定の領域または画素が癌性である可能性の信頼度を示すカラーコーディングを施した、「ヒートマップ」が提示される。例を図1Bに示す。https://ai.googleblog.com/2017/03/assisting-pathologists-in-detecting.htmlを参照されたい、(3)アテンションマスクがユーザに提示される。その例を図1Cに示す。 Researchers associated with the present assignee have developed several basic methods for describing model predictions. For example, if a sample or image is diagnosed as "positive" (eg, cancer or probable cancer), the following methods have been used. (1) The user is presented with a bounding box around the suspected lesion produced by the detection model and later classified by the classification model. An example is shown in FIG. 1A, (2) a "heat map" is presented, typically color-coded to indicate the confidence that a particular region or pixel is likely to be cancerous. An example is shown in FIG. 1B. See https://ai.googleblog.com/2017/03/assisting-pathologists-in-detecting.html, (3) an attention mask is presented to the user. An example is shown in FIG. 1C.
これらの進歩にもかかわらず、所見の欠如(例えば癌がないこと)について説明するオプションは限られており、というのも、陰性を証明することは困難なためである。大多数のコンピュータ支援検出システムでは、ある特定の関心領域に疾患の疑いがあると考える医療専門家/病理医/放射線科医は、陰性予測をもたらすモデルがその領域を見落としたのか、それともモデルはその領域を検査して、それを正常/良性と分類したのかを知るすべがない。限られた計算リソースのため、この分野における機械学習のいくつかの実装形態では、疑わしい癌性組織を見いだすために検出モデルが最初に使用され、検出モデルによって見いだされた領域のみが、後に分類モデルを用いて分類される。したがって、潜在的に癌性のエリアを検出モデルが見落としていた可能性があり、そのため「陰性」という全体結果予測が正しくない可能性があるという、いくらかのリスクがある。 Despite these advances, the options for explaining the absence of findings (eg, no cancer) are limited, as it is difficult to prove a negative. In the vast majority of computer-aided detection systems, a medical professional/pathologist/radiologist who believes a particular region of interest to be suspected of having disease will be asked whether the model that yields a negative prediction has overlooked that region, or whether the model has There is no way of knowing if the area was examined and classified as normal/benign. Due to limited computational resources, in some implementations of machine learning in this field, a detection model is first used to find suspicious cancerous tissue, and only the regions found by the detection model are later used by the classification model. are classified using Thus, there is some risk that potentially cancerous areas may have been missed by the detection model and thus the overall outcome prediction of "negative" may be incorrect.
「陰性」予測という文脈におけるこのモデル説明問題により、市場に存在する多くのコンピュータ支援検出/診断(CAD)システムは、改善された結果をもたらすことができていなかった。例えば、マンモグラフィCADシステムは、特異度を下げることが示されており、というのも、一部には、そのようなシステムが、放射線科医に多数の所見をもって注意喚起する一方で、放射線科医がそれら自体を疑わしいと特定した所見が機械学習モデルでは良性であると見なされたことを放射線科医に確信させることのできないユーザインターフェースを用いているためである。本開示は、この満たされていない必要性に対処するものである。 This model explanation problem in the context of "negative" prediction has prevented many computer-aided detection/diagnosis (CAD) systems on the market from yielding improved results. For example, mammography CAD systems have been shown to reduce specificity, in part because such systems alert radiologists with a large number of findings, while radiologists This is because it uses a user interface that does not convince radiologists that findings that have identified themselves as suspicious are considered benign by the machine learning model. The present disclosure addresses this unmet need.
一態様では、患者の病状(disease state)に関してその患者のまたはその患者から取得されたサンプルの2Dまたは3Dイメージ、例えばX線、CTスキャン、病理標本から予測を行う機械学習モデルを評価するための方法が開示される。機械学習モデルは、2Dイメージまたは3Dイメージ、例えば癌性、良性、石灰化、病変などから予測を行うようにトレーニングされる。本方法は、a)予測に関連するリスクスコアまたは分類とともに画像を提示するステップであって、画像が、機械学習モデルによってもたらされる予測に影響を及ぼした、画像中の1つまたは複数の領域を示す強調表示でさらに拡張されている、ステップと、b)画像の1つまたは複数の領域を強調表示するためのユーザインターフェースツールを提供するステップと、c)画像の1つまたは複数の領域を強調表示したユーザ入力を受領するステップと、d)強調表示された1つまたは複数の領域を、機械学習モデルによる推論にかけるステップと、e)その1つまたは複数の領域に対する推論の結果を、ディスプレイを介してユーザに提示するステップとを含む。 In one embodiment, for evaluating a machine learning model that makes predictions about a patient's disease state from 2D or 3D images of the patient or samples obtained from the patient, such as X-rays, CT scans, pathological specimens. A method is disclosed. Machine learning models are trained to make predictions from 2D or 3D images, such as cancerous, benign, calcified, lesions, and the like. The method comprises the steps of: a) presenting an image with a risk score or classification associated with the prediction, wherein the image identifies one or more regions in the image that influenced the prediction provided by the machine learning model; b) providing a user interface tool for highlighting one or more regions of the image; and c) highlighting the one or more regions of the image. d) subjecting the highlighted region or regions to inference by a machine learning model; and e) displaying the results of the inference for the one or more regions. and presenting to the user via.
別の態様では、2Dイメージまたは3Dイメージから患者の予測を行う機械学習モデルを評価するように構成されたワークステーションについて説明する。ワークステーションは、a)患者のまたは患者から取得されたサンプルの画像を、予測に関連するリスクスコアまたは分類とともに表示するためのディスプレイであって、画像が、機械学習モデルによってもたらされる予測に影響を及ぼした、画像中の1つまたは複数の領域を示す強調表示でさらに拡張されている、ディスプレイと、b)ユーザが、病状の疑いがあるとユーザが見なす、画像の1つまたは複数の領域を、ディスプレイ上で強調表示することのできるユーザインターフェースツールであって、ユーザがツールを起動し、それにより、1つまたは複数の領域を強調表示する、ユーザインターフェースツールとを備える。ディスプレイは、ユーザによって強調表示された1つまたは複数の領域に対して機械学習モデルによって実施された推論の結果を提示するようにさらに構成されている。 In another aspect, a workstation configured to evaluate a machine learning model for patient prediction from 2D or 3D images is described. The workstation comprises: a) a display for displaying images of the patient or samples obtained from the patient, along with risk scores or classifications associated with the predictions, such that the images influence the predictions made by the machine learning model; b) a display further enhanced with a highlighting showing one or more regions in the image that the user has affected; , a user interface tool capable of being highlighted on a display, wherein a user activates the tool, thereby highlighting one or more regions. The display is further configured to present results of inferences performed by the machine learning model on one or more regions highlighted by the user.
一態様では、患者の病状に関してその患者のまたはその患者から取得されたサンプルの2Dまたは3D画像から予測を行う機械学習モデルを評価する、すなわちその機械学習モデルの人間による理解を容易にするための方法が開示される。2D画像の例は、胸部X線画像やマンモグラムなどの放射線画像、または病理標本の拡大デジタル画像である。3Dボリューム画像は、CTスキャン画像、核磁気共鳴スキャン画像、またはその他の形をとることができる。一態様では、本開示は、機械学習モデルが、2Dまたは3D画像中に癌細胞が存在する状態で、画像から病状の陰性予測、例えば「良性」または「低信頼度」という予測をもたらす状況下での、モデル解釈性に関する。予測が「陰性」であると見なされるときのしきい値は特に重要ではなく、しきい値は、モデルの感度またはユーザの嗜好のような事柄に応じて変わることがある。以下の議論では、癌スコアまたは局所領域スコアの数値は、仮定的なものであり、本開示の核となる概念を示すために例として提供されているにすぎず、所与の患者サンプルおよび機械学習モデルの実際のスコアリング方式(scoring regime)を反映していることも、していないこともある。 In one aspect, to evaluate a machine learning model that makes predictions from 2D or 3D images of the patient or of a sample obtained from the patient regarding a patient's medical condition, i.e., to facilitate human comprehension of the machine learning model. A method is disclosed. Examples of 2D images are radiological images such as chest X-ray images and mammograms, or magnified digital images of pathological specimens. 3D volumetric images can take the form of CT scan images, nuclear magnetic resonance scan images, or other forms. In one aspect, the present disclosure provides that a machine learning model, in the presence of cancer cells in a 2D or 3D image, provides a negative prediction of pathology from the image, e.g. on model interpretability. The threshold at which a prediction is considered "negative" is not particularly important, and may vary depending on things such as model sensitivity or user preferences. In the discussion that follows, numerical values for cancer scores or locoregional scores are hypothetical and are provided only as examples to illustrate the core concepts of the present disclosure, given patient samples and machine It may or may not reflect the actual scoring regime of the learning model.
本方法は、ワークステーションのディスプレイ上に、患者のまたは患者から取得されたサンプルの画像(例えばマンモグラム、または拡大組織画像)を、予測に関連するリスクスコアまたは分類とともに提示する第1のステップを含む。画像はさらに、画像中の、機械学習モデルによってもたらされる予測に影響を及ぼした、細胞集塊などの1つまたは複数の領域を示す強調表示で拡張されている。この例が図2に、限定としてではなく例示として提供されており、図2は、リスクスコア12(この例では癌スコア=0.6%、おそらく良性)と、機械学習モデルの診断予測に影響を及ぼした領域を識別する矩形14の形をとるオーバーレイとの両方を含む、患者のマンモグラム10である。この特定の例では、機械学習モデルは、おそらく癌である候補(likely cancer candidate)を検出するようにトレーニングされた検出モデルを含み、バウンディングボックスまたは矩形14が候補を取り囲む。機械学習モデルは、検出モデルによって検出された集塊または細胞に対して推論を実施しており、これらは、「低レベル」かつおそらく癌性ではないとスコアリングされており、したがって、このサンプルについての全体評価は、この例では0.65パーセントというスコアが示す「陰性」となっている。
The method includes a first step of presenting on a workstation display an image of the patient or of a sample obtained from the patient (eg, a mammogram, or an enlarged tissue image) along with a risk score or classification associated with the prediction. . The image is further augmented with a highlighting showing one or more regions in the image, such as cell clumps, that influenced the predictions made by the machine learning model. An example of this is provided in Figure 2, by way of illustration and not limitation, which influences the risk score of 12 (cancer score = 0.6%, likely benign in this example) and the diagnostic prediction of the machine learning model. 1 is a
本方法は、画像の、例えば病状の疑いがあるとユーザが見なす、または機械学習モデルにユーザが問合せしたいと思う、1つまたは複数の領域をユーザがそれによって強調表示することのできるユーザインターフェースツールを提供し、1つまたは複数の領域を強調表示した入力を受領するステップを含む。ツールは単に、ディスプレイに関連付けられたマウスから構成され得る。別法として、ディスプレイがタッチセンシティブであり、ツールが、既知のグラフィックス処理ソフトウェアの形をとり、そのグラフィックス処理ソフトウェアが、ディスプレイ上の、ユーザによって(例えばペンを用いて直接的または間接的に)接触された場所を記録し、そのような場所を画像中の位置に変換する。ユーザが1つまたは複数の領域を強調表示する様式は特に重要ではなく、変わることがある。このステップの例が図3に示されており、図3は、図2のマンモグラムの図であり、ただし、ユーザがマウスを用いてマンモグラム上に、ユーザが疑わしいと気付き、追加の所見を要求する、領域20を描いた図である。この領域は、カーソルを使って描くこともでき、あるいはマンモグラムがタッチセンシティブディスプレイ上に提示されている場合は指を使って描くこともでき、領域が特定される様式は特に重要ではない。関心エリアの周りにバウンディングボックスを描くための可能な一方法が、本発明の譲受人に譲渡された、Christopher Gammageらの米国特許第10,013,781号に記載されている。
The method is a user interface tool by which a user can highlight one or more regions of an image that the user deems suspicious for a medical condition, for example, or that the user wishes to query a machine learning model. and receiving input highlighting one or more regions. A tool may simply consist of a mouse associated with a display. Alternatively, the display is touch-sensitive and the tool takes the form of known graphics processing software that can be manipulated by a user (e.g., directly or indirectly with a pen) on the display. ) records touched locations and converts such locations to locations in the image. The manner in which the user highlights one or more regions is not particularly important and may vary. An example of this step is shown in FIG. 3, which is a view of the mammogram of FIG. 2, except that the user uses the mouse to indicate on the mammogram that the user finds it suspicious and requests additional findings. , depicting
本方法は、強調表示された1つまたは複数の領域(図3の20)を、機械学習モデルによる推論にかけるステップを続ける。このステップでは、機械学習モデルが、ユーザによって強調表示された領域内の画素データを処理し、出力、典型的には、モデルがどのように構成されているかに応じて分類結果、または予測、またはスコアを生成する。この例では、機械学習モデルは、新たな(これまでに見たことのない)例を正しく分類またはスコアリングするために数千の健康なマンモグラムおよび癌性のマンモグラムからトレーニングされた畳み込みニューラルネットワークの形をとってよい。本明細書において説明するタイプの深層畳み込みニューラルネットワークパターン認識器は、パターン認識およびマシンビジョンの技術分野で広く知られており、したがって、その詳細な説明は、簡単のため省略する。可能な一実装形態であるInception-v3深層畳み込みニューラルネットワークアーキテクチャについては、科学文献に記載されている。その内容が参照により本明細書に組み込まれている、次の参照文献を参照されたい。C. Szegedyら、Going Deeper with Convolutions、arXiv:1409.4842 [cs.CV] (2014年9月)、C. Szegedyら、Rethinking the Inception Architecture for Computer Vision、arXiv:1512.00567 [cs.CV] (2015年12月)。2015年8月28日出願のC. Szegedyらの「Processing Images Using Deep Neural Networks」という米国特許出願第14/839,452号も参照されたい。Inception-v4として知られる第4世代は、代替アーキテクチャと見なされる。C. Szegedyら、Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning、arXiv:1602.0761 [cs.CV] (2016年2月)を参照されたい。2016年12月30日出願のC. Vanhouckeの「Image Classification Neural Networks」という米国特許出願第15/395,530号も参照されたい。これらの論文および特許出願における畳み込みニューラルネットワークについての記載は、参照により本明細書に組み込まれている。「アテンション」モデル、および勾配積分法(integrated gradients)などの関連技法の使用については、科学文献および特許文献に記載されている。D. Bahdanauら、Neural Machine Translation by Jointly Learning to Align and Translate、2014年1月(arXiv:1409.0473[cs.CL])。Choiら、GRAM: Graph-based attention model for Healthcare Representation Learning、arXiv:1611.07012v3 [cs.LG] 2017年4月、Choiら、RETAIN: an Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism、arXiv:1608.05745v3[cs.GL] 2017年2月。M. Sundararajanら、Axiomatic Attribution for Deep Networks、arXiv:1703.01365 [cs.LG] (2017年7月)。いくつかの論文は、深層学習を用いてCTスキャン画像から肺小結節を検出および診断することを対象としており、これらの論文には次のものが含まれる。Xiaojie Huangら、Lung Nodule Detection in CT Using 3D Convolutional Neural Networks、The 2017 IEEE International Symposium on Biomedical Imaging、2017年4月、Francesco Ciompiら、Towards automatic pulmonary nodule management in lung cancer screening with deep learning、Scientific Reports 7巻、記事番号46479、2017年4月17日、Wenqing Sunら、Computer Aided lung cancer diagnosis with deep learning、Medical Imaging 2016、Proc. of SPIE 9785巻(2016年3月)、Albert Chonら、Deep Convolutional Neural Networks for Lung Cancer Detection、Stanford University Reports (2017)、www.cs231n.stanford.edu/reports/2017/pdfs/518.pdf、およびWafaa Alakwaaら、Lung Cancer Detection and Classification with 3D Convolutional Neural Network (3D-CNN)、International Journal of Advanced Computer Science and Applications、8巻、8号、409～417頁(2017)。 The method continues with subjecting the highlighted region or regions (20 in FIG. 3) to inference by a machine learning model. In this step, the machine learning model processes the pixel data within the region highlighted by the user and outputs, typically a classification result, or a prediction, or Generate a score. In this example, the machine learning model uses a convolutional neural network trained from thousands of healthy and cancerous mammograms to correctly classify or score novel (never-before-seen) examples. take shape. Deep convolutional neural network pattern recognizers of the type described herein are widely known in the pattern recognition and machine vision arts, and therefore a detailed description thereof is omitted for brevity. One possible implementation, the Inception-v3 deep convolutional neural network architecture, is described in the scientific literature. See the following references, the contents of which are incorporated herein by reference: C. Szegedy et al., Going Deeper with Convolutions, arXiv:1409.4842 [cs.CV] (September 2014), C. Szegedy et al., Rethinking the Inception Architecture for Computer Vision, arXiv:1512.00567 [cs.CV] (Dec. 2015) Month). See also US patent application Ser. No. 14/839,452, entitled "Processing Images Using Deep Neural Networks," to C. Szegedy et al., filed Aug. 28, 2015. The fourth generation, known as Inception-v4, is considered an alternative architecture. See C. Szegedy et al., Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, arXiv:1602.0761 [cs.CV] (February 2016). See also C. Vanhoucke, US patent application Ser. The descriptions of convolutional neural networks in these papers and patent applications are incorporated herein by reference. The use of the "attention" model and related techniques such as integrated gradients is described in the scientific and patent literature. D. Bahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate, January 2014 (arXiv:1409.0473[cs.CL]). Choi et al., GRAM: Graph-based attention model for Healthcare Representation Learning, arXiv:1611.07012v3 [cs.LG] April 2017, Choi et al., RETAIN: an Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism, arXiv:1608.05745v3 [cs.GL] February 2017. M. Sundararajan et al., Axiomatic Attribution for Deep Networks, arXiv:1703.01365 [cs.LG] (July 2017). Several papers have been directed to using deep learning to detect and diagnose lung nodules from CT scan images, these papers include: Xiaojie Huang et al., Lung Nodule Detection in CT Using 3D Convolutional Neural Networks, The 2017 IEEE International Symposium on Biomedical Imaging, April 2017, Francesco Ciompi et al., Towards automatic pulmonary nodule management in lung cancer screening with deep learning, Scientific Reports vol.7 , Article No. 46479, April 17, 2017, Wenqing Sun et al., Computer Aided lung cancer diagnosis with deep learning, Medical Imaging 2016, Proc. of SPIE Vol. 9785 (March 2016), Albert Chon et al., Deep Convolutional Neural Networks for Lung Cancer Detection, Stanford University Reports (2017), www.cs231n.stanford.edu/reports/2017/pdfs/518.pdf, and Wafaa Alakwaa et al., Lung Cancer Detection and Classification with 3D Convolutional Neural Network (3D-CNN) , International Journal of Advanced Computer Science and Applications, Vol. 8, No. 8, pp. 409-417 (2017).
本方法は、その1つまたは複数の領域(図3の20)に対する推論の結果を、ディスプレイを介してユーザに提示するステップを続ける。この例が図4に示されており、図4は、図3のマンモグラムの図であり、ユーザによって強調表示された図3の領域20に対して機械学習モデルが推論を実施した後の図である。ディスプレイには、推論の結果が22に、ユーザにより特定された領域20についての分類(「良性の石灰化」)に局所領域リスクスコア、この場合には癌スコア0.02%を加えた形をとるものとして示されている。推論の結果は、局所領域スコア、分類特性、回帰値、および/または領域20の解釈に有用な他の特徴を含んでよく、それらがユーザに提示される。任意選択で、結果22は、24に示す症例全体についての更新されたリスクスコアとともに提示される。この特定の例では、症例全体リスクスコアは、0.65%から0.67%に上昇した。
The method continues with presenting to the user via a display the results of the inference for the one or more regions (20 in FIG. 3). An example of this is shown in Figure 4, which is a view of the mammogram of Figure 3 after the machine learning model has performed inference on
注意:この例では、症例レベル確率が、非ゼロリスクの病変が見いだされるたびに上がり、エリアが検査されるたびに下がる(本発明者らは、既知の未知については、より低いペナルティを適用することができる)。これらの影響のどちらがより強いかは、どれだけ広いエリアが検査されたか、またどれだけ深刻な病変が発見されたかに応じて決まる。図3および図4に代わる数値例は、以下の通りである。
- モデルが、1つの病変、癌スコア0.6%、(綿密に検査されていなかった領域内の不確実性を見込んだ)症例全体スコア1.1%を見いだした、
- オペレータが別の病変について懸念し、それを強調表示し、それに対して推論を実施するようにモデルに求める、
- モデルがその病変を、癌スコアが0.2%の良性と分類し、その領域が先には考慮されていなかったため、本症例の癌スコアを1.2%に上昇させるものの、本症例は引き続き良性と考えられる。症例スコアの上昇は、直感的に分かるように、検査された領域の量に伴って減少する必要があることに留意されたく、というのも、そうでない場合、たくさんの領域が検査されると、どの症例も最終的には「陽性」になってしまうためである。
Note: In this example, the case-level probability rises each time a non-zero-risk lesion is found and falls each time an area is examined (we apply a lower penalty for known unknowns). be able to). Which of these effects is stronger depends on how large an area was examined and how severe the lesions were found. Numerical examples in place of FIGS. 3 and 4 are as follows.
- The model found 1 lesion, a cancer score of 0.6%, and an overall case score of 1.1% (allowing for uncertainty in regions that were not scrutinized),
- the operator is concerned about another lesion and asks the model to highlight it and perform inferences on it;
- The model classified the lesion as benign with a cancer score of 0.2%, and since the area was not previously considered, it raises the cancer score for this case to 1.2%, but the case is still considered benign be done. Note that the increase in case score should, intuitively, decrease with the amount of regions examined, because otherwise when many regions are examined, This is because every case will eventually become "positive."
図3のようにユーザが新たな領域を特定し、図4のように推論を実施し、結果を提示するプロセスは、ユーザによる必要に応じて継続することができる。 The process of the user identifying new regions as in FIG. 3, making inferences as in FIG. 4, and presenting results can continue as desired by the user.
先に述べたように、本開示の方法は、組織病理サンプル、例えば前立腺組織の拡大デジタル画像の形をとる画像データとともに使用するのに適している。その方法についてこの文脈の中で図5A～図5Cに関連して説明する。具体的には、図5Aは、リスクスコア(この例では癌スコア0.40%、良性)と、機械学習モデルの診断予測に影響を及ぼした細胞集塊を識別する矩形30の形をとるオーバーレイとの両方を含む、拡大組織画像の図である。この例では、標本全体が機械学習モデルによってリスクスコア0.40%の良性と予測された。このリスクスコアおよびバウンディングボックス30は、図示のように、ワークステーションのディスプレイ上でユーザに提示される。
As previously mentioned, the methods of the present disclosure are suitable for use with image data in the form of histopathological samples, such as magnified digital images of prostate tissue. The method is described in this context with reference to Figures 5A-5C. Specifically, FIG. 5A shows the risk score (0.40% cancer score, benign in this example) and an overlay in the form of
病理用ワークステーションのディスプレイ上でスライドの拡大デジタル画像を観察している病理医が、スライド内に、バウンディングボックス30内にはない他の潜在的関心エリアがあると判定し、したがって、そのような他のエリアが潜在的に癌性であるかどうかを知りたいと思うことがある。したがって、図5Bに示すように、ユーザはユーザインターフェースツール(例えばマウスおよびグラフィックス処理ソフトウェア)を用いて組織画像上に、ユーザが疑わしいと見なし、機械学習モデルに追加の所見を要求する、新たな領域40を描いている。領域40に関連する画素データが入力として機械学習モデルに提供される。ユーザによって描かれた領域40に対して実施された推論の結果は、42に示すようにディスプレイ上に提示される。これらの結果は、局所領域スコア(0.20%)、分類結果(陰性)、および任意選択で回帰値または他の情報を含む。このサンプルについての更新された全体癌スコアが生成および表示され、この例では0.42%である。この例では、ボックス40内の追加の細胞集塊を分類したことにより、全体癌スコアが0.40パーセントから0.42パーセントに上昇している。
A pathologist viewing a magnified digital image of the slide on the display of the pathology workstation determines that there are other areas of potential interest within the slide that are not within the
この例では、ユーザは、組織標本のさらに別のエリアを、機械学習モデルによるさらなるスコアリング/推論を得るために、強調表示することを選択している。この例では、図5Cに示すように、ユーザは画像上に、疑わしいと見なした別の新たな領域50を描いている。この新たな領域に対して実施された推論の結果は、52に示すようにディスプレイ上に提示される。
In this example, the user has chosen to highlight yet another area of the tissue specimen for further scoring/inference by the machine learning model. In this example, as shown in FIG. 5C, the user has drawn another
この例では、所与の組織画像中での新たな位置へのズームやパンなどの中間ステップがある可能性があるものの、上で図5A～図5Cにおいて説明したプロセスは本質的には変わらないことに留意されたい。図5Cに示す新たなエリアのユーザ選択は、言うまでもなく、ユーザがその組織標本についての陰性モデル予測に満足するまで継続することができる。 In this example, there may be intermediate steps such as zooming and panning to new locations within a given tissue image, but the process described above in FIGS. 5A-5C remains essentially the same. Please note that User selection of new areas shown in FIG. 5C can, of course, continue until the user is satisfied with the negative model predictions for that tissue specimen.
次に図6を参照すると、本開示の方法は、患者の病状に関してその患者のまたはその患者から取得されたサンプルの2Dまたは3D画像から予測を行う機械学習モデルの人間による理解を容易にするように構成されたワークステーション100内に実装することができる。機械学習モデルは画像から、例えばユーザに提示される分類結果またはスコアとして、病状の陰性予測をもたらす。ワークステーションは、a)患者のまたは患者から取得されたサンプルの画像を、予測に関連するリスクスコアまたは分類とともに表示するためのディスプレイ102を含む。画像はさらに、画像中の、機械学習モデルによってもたらされる陰性予測に影響を及ぼした1つまたは複数の領域を示す強調表示で拡張されている(図3および図5Bを参照されたい)。ワークステーションは、画像の、疾患の疑いがあるとユーザが見なす1つまたは複数の領域をユーザがそれによってディスプレイ上で強調表示することのできるユーザインターフェースツール、例えばマウス104またはタッチセンシティブディスプレイを含む。ユーザはツール、例えばマウスを起動し、それにより、1つまたは複数の領域を強調表示する。ディスプレイはさらに、先に説明したように、ユーザによって強調表示された1つまたは複数の領域に対して機械学習モデルによって実施された推論の結果を提示するように構成されている。ユーザが、例えばマウス104を使用して関心領域を強調表示すると、ユーザは、ディスプレイ上の「モデル適用」アイコンをクリックすることによって、またはキーボード106を使用して適切なコマンドを入力することによってなど、ディスプレイ上の適切なメニューを介してコマンドを入力することによって、機械学習モデルを起動することができる。
Referring now to FIG. 6, the methods of the present disclosure are designed to facilitate human understanding of machine learning models that make predictions about a patient's medical condition from 2D or 3D images of that patient or of samples obtained from that patient. can be implemented within a workstation 100 configured for A machine learning model yields a negative prediction of the disease state from the image, eg, as a classification result or score presented to the user. The workstation includes a) a
機械学習モデル110は、ワークステーション100内に常駐していてもよく、あるいはより典型的には、それをコンピュータネットワーク108上のコンピューティングリソース106によって実装することもできる。可能な一構成では、いくつかの機械学習モデルが利用可能である。組織病理学の状況においては、ユーザは標本を比較的高倍率、例えば40×で観察し、その倍率で新たな領域を指定し(例えば図5Bの領域40)、その倍率で癌細胞を識別するようにトレーニングされた特定の機械学習モデルを起動してよいが、ユーザは、低倍率、例えば10×で標本を観察し、細胞の領域を強調表示してもよく(例えば図5Cの領域50)、その推論タスクには低倍率(10×)機械学習モデルが使用される。
Machine learning model 110 may reside within workstation 100 or, more typically, it may be implemented by computing
図7は、図6のコンピューティング環境において実施される、本方法を実施するための処理の一実施形態を示すフローチャートである。機械学習モデルは、患者のまたは患者から取得されたサンプルの2Dまたは3D画像データセットに対して推論を実施する。モデルは、患者の病状に関して予測を行い、一例では、病状の陰性予測を行う。200に示す方法は、以下のステップを含む。 FIG. 7 is a flowchart illustrating one embodiment of a process for performing the method, implemented in the computing environment of FIG. A machine learning model performs inference on a sample 2D or 3D image dataset of a patient or obtained from a patient. The model makes a prediction regarding the patient's medical condition, in one example, a negative prediction of the medical condition. The method shown at 200 includes the following steps.
ステップ202:ワークステーションのディスプレイ上に、患者のまたは患者から取得されたサンプルの画像を、予測に関連するリスクスコアまたは分類とともに提示する。画像はさらに、画像中の、機械学習モデルによってもたらされる予測に影響を及ぼした1つまたは複数の領域を示す強調表示で拡張されている。 Step 202: Presenting on a workstation display an image of the patient or of a sample obtained from the patient along with a risk score or classification associated with the prediction. The image is further augmented with a highlighting showing one or more regions in the image that influenced the prediction provided by the machine learning model.
ステップ204:画像の、病状の疑いがあるとユーザが見なす1つまたは複数の領域をユーザがそれによって強調表示することのできるツールを提供する。ユーザはツールを起動し、それにより、1つまたは複数の領域を強調表示し、そのような入力がワークステーションによって受領される。 Step 204: Provide a tool by which the user can highlight one or more areas of the image that the user deems suspicious of a medical condition. A user activates a tool, thereby highlighting one or more regions, and such input is received by the workstation.
ステップ206:強調表示された1つまたは複数の領域に対して、機械学習モデルを用いて推論を実施する。 Step 206: Perform inference using a machine learning model for the highlighted region or regions.
ステップ208:その1つまたは複数の領域に対する推論の結果を、ディスプレイを介してユーザに提示する。 Step 208: Present the inference results for the one or more regions to the user via the display.
任意選択のステップ210:2D画像/3Dボリューム全体についての更新されたリスクスコアを提示する。 Optional step 210: Present updated risk score for the entire 2D image/3D volume.
このプロセスは、ステップ214に示すようにループバックすることができ、ステップ204、206、および208、ならびに210は、反復することができる。このループは、図5A～図5Cで説明した、ユーザが追加の領域を指定した状況に該当する。
This process may loop back as shown in
図2～図6から明らかなように、患者の病状に関してその患者のまたはその患者から取得されたサンプルの2Dまたは3D画像から予測を行う機械学習モデルの人間による理解を容易にするように構成されたワークステーション用のインターフェースについて説明されたことが明らかであろう。機械学習モデルは画像から病状の予測、例えば「良性」などの陰性予測をもたらす。インターフェースは、患者のまたは患者から取得されたサンプルの画像を、予測に関連するリスクスコアまたは分類とともに表示するためのディスプレイ(102、図6)を含む。画像はさらに、画像中の、機械学習モデルによってもたらされる予測に影響を及ぼした1つまたは複数の領域を示す強調表示で拡張されている。インターフェースは、画像の1つまたは複数の領域、例えば病状の疑いがあるとユーザが見なす領域をユーザがそれによってディスプレイ上で強調表示することのできるツール(例えばマウス104)であって、図3、図4、および図5A～図5Cに関連して説明したように、ユーザがツールを起動し、それにより、1つまたは複数の領域を強調表示する、ツールを含む。 As can be seen from Figures 2-6, it is configured to facilitate human comprehension of machine learning models that make predictions about a patient's medical condition from 2D or 3D images of that patient or of samples obtained from that patient. It will be apparent that an interface for a workstation has been described. Machine learning models yield predictions of disease states from images, such as negative predictions such as "benign." The interface includes a display (102, FIG. 6) for displaying an image of the patient or of a sample obtained from the patient along with a risk score or classification associated with the prediction. The image is further augmented with a highlighting showing one or more regions in the image that influenced the prediction provided by the machine learning model. The interface is a tool (e.g. mouse 104) by which the user can highlight on the display one or more regions of the image, e.g. As described in connection with FIGS. 4 and 5A-5C, includes tools that a user activates to highlight one or more regions.
さらなる考慮事項
本方法を使用して、機械学習モデルがすでに標本を陽性と分類した後であっても画像をさらに検査することができる。オペレータは、(より深刻なまたはそれほど深刻でない)報告する価値のある別の病変があるのではないかと疑って、モデルにそれを明示的に検査して欲しいと思うことがある。したがって、本方法は、追加の領域を強調表示するステップと、モデルの推論を開始するステップと、次いで推論の結果を生成し、それをワークステーションディスプレイ上に提示するステップとを続ける。
Further Considerations The method can be used to further inspect the image even after the machine learning model has already classified the specimen as positive. The operator may suspect that there is another lesion (more serious or less serious) worth reporting and want the model to explicitly test it. Accordingly, the method continues with the steps of highlighting additional regions, initiating inference of the model, and then generating inference results and presenting them on the workstation display.
上の説明は、分類+位置特定問題に焦点を当てているが、同じ方法を別様に、例えばセグメンテーション問題および回帰問題において使用することもできる。 Although the above description focuses on the classification + localization problem, the same method can be used differently, eg in segmentation and regression problems.
A.セグメンテーション
例えば、超音波画像がワークステーション上に提示され、機械学習モデルが使用されて、超音波画像上で前立腺が識別される。画像を観察しているオペレータが、前立腺を取り囲むセグメンテーションマスク輪郭を見て、マスク内にマークされていなかった何らかの組織も前立腺に属していたのではないかと疑うことがある。ユーザは、この追加のエリアを強調表示し、キーボードまたはマウス操作でモデル推論を開始する。次いで、モデルはこの領域が実際に、例えば尿道であることを説明するか、またはモデルはその領域をセグメンテーションマスクに追加することに「同意」する。
A. Segmentation For example, an ultrasound image is presented on a workstation and a machine learning model is used to identify the prostate on the ultrasound image. An operator viewing the image may see the segmentation mask outline surrounding the prostate and suspect that some tissue that was not marked in the mask also belonged to the prostate. The user highlights this additional area and initiates model inference with keyboard or mouse manipulation. The model then either explains that this region is in fact, say, the urethra, or the model "agreees" to add that region to the segmentation mask.
B.回帰問題
例えば、機械学習モデルは、「x線で撮像した患者の骨年齢はいくつか」などの回帰問題に回答するように構成されてよい。骨のX線は、モデルの予測とともに、ワークステーションディスプレイ上に提示される。オペレータは、ある特定の領域がより高い年齢を示しているのではないかと疑い、その領域を強調表示し、推論を開始し、モデルがそれに応じてその予測を更新する。この一般的な手順は、言うまでもなく、他のタイプの回帰問題にも適用することができ、骨年齢の例は、限定としてではなく例示として提供されたものである。
B. Regression Problem For example, a machine learning model may be configured to answer a regression problem such as "What is the bone age of a patient on x-rays?" Bone X-rays are presented on the workstation display along with the model's predictions. The operator suspects that a particular region indicates a higher age, highlights that region, initiates inference, and the model updates its predictions accordingly. This general procedure can, of course, be applied to other types of regression problems, and the bone age example is provided as an illustration and not as a limitation.
本教示は、冶金、部品検査、半導体製造、およびその他など、他の分野にも応用され、その場合、機械学習位置特定モデルは、入力画像データセットに基づいて物体に関して予測を行うことになる。例えば、予測が陰性であり(例えば、冶金サンプル(metallurgical sample)中に欠陥がない、または望ましくない不純物がない)、ユーザはモデルにさらに問合せしようと試みる。本方法は、上述したのと同じ基本的手法、すなわち、
a)ワークステーションのディスプレイ上に、物体の画像を、予測に関連するスコアまたは分類とともに提示する。画像はさらに、画像中の、機械学習モデルによってもたらされる予測に影響を及ぼした1つまたは複数の領域を示す強調表示で拡張されている、
b)画像の、予測に対してさらに関心のあるものとユーザが見なす1つまたは複数の追加の領域(例えば潜在的な欠陥、不純物など)をユーザがそれによって強調表示することのできるツールであって、ユーザがツールを起動し、それにより、1つまたは複数の領域を強調表示する、ツールを提供する、
c)強調表示された1つまたは複数の領域を、機械学習モデルによる推論にかける、
d)その1つまたは複数の領域に対する推論の結果を、ディスプレイを介してユーザに提示する、
に従う。
The present teachings also have application in other fields, such as metallurgy, parts inspection, semiconductor manufacturing, and others, where machine learning localization models will make predictions about objects based on input image datasets. For example, if the prediction is negative (eg, no defects or unwanted impurities in the metallurgical sample), the user attempts to query the model further. The method uses the same basic procedure as described above, i.e.
a) Presenting an image of the object on the workstation display with a score or classification associated with the prediction. The image is further augmented with a highlighting showing one or more regions in the image that influenced the prediction provided by the machine learning model.
b) a tool by which the user can highlight one or more additional regions (e.g. potential defects, impurities, etc.) of the image that the user deems more interesting for prediction; provide a tool that allows the user to launch the tool, thereby highlighting one or more regions;
c) subjecting the highlighted region or regions to inference by a machine learning model;
d) presenting to the user via a display the results of the reasoning for the one or more regions;
obey.
添付の特許請求の範囲は、開示した方法、ワークステーション、およびユーザインターフェースについてのさらなる説明として提供されている。 The appended claims are provided as further descriptions of the disclosed methods, workstations and user interfaces.
10 マンモグラム
12 リスクスコア
14 バウンディングボックスまたは矩形
20 領域
22 推論の結果
30 矩形、バウンディングボックス
40 領域、ボックス
50 領域
100 ワークステーション
102 ディスプレイ
104 マウス
106 キーボード、コンピューティングリソース
108 コンピュータネットワーク
110 機械学習モデル
10 mammograms
12 Risk Score
14 bounding box or rectangle
20 areas
22 Inference results
30 rectangle, bounding box
40 areas, boxes
50 areas
100 workstations
102 displays
104 mouse
106 keyboards, computing resources
108 computer network
110 machine learning models
Claims (20)
a)前記予測に関連するリスクスコアまたは分類とともに画像を提示するステップであって、前記画像が、前記機械学習モデルによってもたらされる前記予測に影響を及ぼした、前記画像中の1つまたは複数の領域を示す強調表示でさらに拡張されている、ステップと、
b)前記画像の1つまたは複数の領域を強調表示するためのユーザインターフェースツールを提供するステップと、
c)前記画像の1つまたは複数の領域を強調表示したユーザ入力を受領するステップと、
d)前記強調表示された1つまたは複数の領域を、前記機械学習モデルによる推論にかけるステップと、
e)前記1つまたは複数の領域に対する前記推論の結果を、ディスプレイを介してユーザに提示するステップと
を含む、方法。 1. A computer-implemented method for evaluating a machine learning model that makes predictions from 2D or 3D images about a medical condition of a patient, comprising:
a) presenting an image with a risk score or classification associated with said prediction, wherein said image is one or more regions in said image that influenced said prediction provided by said machine learning model; The step, which is further enhanced with a highlighting indicating the
b) providing a user interface tool for highlighting one or more regions of said image;
c) receiving user input highlighting one or more regions of said image;
d) subjecting the highlighted region or regions to inference by the machine learning model;
e) presenting the results of said inference for said one or more regions to a user via a display.
a)前記患者のまたは前記患者から取得されたサンプルの前記画像を、前記予測に関連するリスクスコアまたは分類とともに表示するためのディスプレイであって、前記画像が、前記機械学習モデルによってもたらされる前記予測に影響を及ぼした、前記画像中の1つまたは複数の領域を示す強調表示でさらに拡張されている、ディスプレイと、
b)ユーザが前記画像の1つまたは複数の領域を前記ディスプレイ上で強調表示することのできるユーザインターフェースツールであって、前記ユーザが前記ツールを起動し、それにより、前記1つまたは複数の領域を強調表示する、ユーザインターフェースツールと
を備え、
前記ディスプレイが、前記ユーザによって強調表示された前記1つまたは複数の領域に対して前記機械学習モデルによって実施された推論の結果を提示するようにさらに構成されている、
ワークステーション。 A workstation configured to evaluate a machine learning model that makes predictions from 2D or 3D images of the patient or of a sample obtained from the patient regarding a patient's medical condition, wherein the machine learning model predicts the patient's condition in the image. from which the workstation provides the prediction of the medical condition from
a) a display for displaying said image of said patient or of a sample obtained from said patient with a risk score or classification associated with said prediction, said image being said prediction produced by said machine learning model; a display further enhanced with a highlighting showing one or more regions in said image that affected the
b) a user interface tool that enables a user to highlight one or more regions of said image on said display, said user activating said tool thereby causing said one or more regions; and a user interface tool for highlighting
the display is further configured to present results of inferences performed by the machine learning model on the one or more regions highlighted by the user;
Work station.
前記患者のまたは前記患者から取得されたサンプルの前記画像を、前記予測に関連するリスクスコアまたは分類とともに表示するためのディスプレイであって、前記画像が、前記機械学習モデルによってもたらされる前記予測に影響を及ぼした、前記画像中の1つまたは複数の領域を示す強調表示でさらに拡張されている、ディスプレイと、
ユーザが、病状の疑いがあるとユーザが見なす、前記画像の1つまたは複数の領域を、前記ディスプレイ上で強調表示することのできるユーザインターフェースツールであって、ユーザが前記ツールを起動し、それにより、前記1つまたは複数の領域を強調表示する、ユーザインターフェースツールと
を備える、インターフェース。 An interface for a workstation configured to evaluate a machine learning model that makes predictions from 2D or 3D images of the patient or of a sample obtained from the patient regarding a medical condition of the patient, the machine learning model , resulting in a prediction of the medical condition from the image, the interface comprising:
A display for displaying the image of the patient or of a sample obtained from the patient along with a risk score or classification associated with the prediction, wherein the image influences the prediction provided by the machine learning model. a display further enhanced with a highlighting showing one or more regions in said image that affected
A user interface tool that enables a user to highlight on the display one or more regions of the image that the user deems to be suspicious of a medical condition, wherein the user activates the tool and and a user interface tool for highlighting the one or more regions.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/246,156 | 2019-01-11 | ||
US16/246,156 US10936160B2 (en) | 2019-01-11 | 2019-01-11 | System, user interface and method for interactive negative explanation of machine-learning localization models in health care applications |
PCT/US2019/055629 WO2020146024A1 (en) | 2019-01-11 | 2019-10-10 | System, user interface and method for interactive negative explanation of machine-learning localization models in health care applications |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2022516805A JP2022516805A (en) | 2022-03-02 |
JP7247352B2 true JP7247352B2 (en) | 2023-03-28 |
Family
ID=68425285
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021540194A Active JP7247352B2 (en) | 2019-01-11 | 2019-10-10 | Systems, User Interfaces, and Methods for Interactive Negative Explanation of Machine Learning Localization Models in Medical Applications |
Country Status (6)
Country | Link |
---|---|
US (2) | US10936160B2 (en) |
EP (1) | EP3909059A1 (en) |
JP (1) | JP7247352B2 (en) |
CN (1) | CN113272915B (en) |
AU (1) | AU2019420099A1 (en) |
WO (1) | WO2020146024A1 (en) |
Families Citing this family (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11132529B2 (en) | 2016-11-16 | 2021-09-28 | Ventana Medical Systems, Inc. | Convolutional neural networks for locating objects of interest in images of biological samples |
US11334994B2 (en) * | 2019-05-24 | 2022-05-17 | Lunit Inc. | Method for discriminating suspicious lesion in medical image, method for interpreting medical image, and computing device implementing the methods |
WO2021138087A1 (en) | 2020-01-03 | 2021-07-08 | PAIGE.AI, Inc. | Systems and methods for processing electronic images for generalized disease detection |
US11532084B2 (en) * | 2020-05-11 | 2022-12-20 | EchoNous, Inc. | Gating machine learning predictions on medical ultrasound images via risk and uncertainty quantification |
US11083403B1 (en) * | 2020-10-11 | 2021-08-10 | Cortery AB | Pulmonary health assessment system |
WO2022213119A1 (en) * | 2021-04-02 | 2022-10-06 | Andode Ip Llc | Systems and methods to process electronic medical images for diagnostic interventional use |
US20230342920A1 (en) * | 2021-04-23 | 2023-10-26 | Fcuro Inc. | Information processing apparatus, information processing method, information processing program, and information processing system |
WO2023228230A1 (en) * | 2022-05-23 | 2023-11-30 | 日本電気株式会社 | Classification device, learning device, classification method, learning method, and program |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2009086765A (en) | 2007-09-27 | 2009-04-23 | Fujifilm Corp | Medical report system, medical report creating device, and medical report creating method |
JP2009516551A (en) | 2005-11-23 | 2009-04-23 | ザ メディパターン コーポレイション | Quantitative and qualitative computer-aided analysis method and system for medical images |
US20160364862A1 (en) | 2015-06-12 | 2016-12-15 | Merge Healthcare Incorporated | Methods and Systems for Performing Image Analytics Using Graphical Reporting Associated with Clinical Images |
WO2018189551A1 (en) | 2017-04-12 | 2018-10-18 | Kheiron Medical Technologies Ltd | Malignancy assessment for tumors |
Family Cites Families (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6266435B1 (en) | 1993-09-29 | 2001-07-24 | Shih-Ping Wang | Computer-aided diagnosis method and system |
US7418119B2 (en) * | 2002-10-31 | 2008-08-26 | Siemens Computer Aided Diagnosis Ltd. | Display for computer-aided evaluation of medical images and for establishing clinical recommendation therefrom |
US20040100476A1 (en) * | 2002-11-25 | 2004-05-27 | Morita Mark M. | Method and apparatus for viewing computer aided detection and diagnosis results |
US7912528B2 (en) * | 2003-06-25 | 2011-03-22 | Siemens Medical Solutions Usa, Inc. | Systems and methods for automated diagnosis and decision support for heart related diseases and conditions |
EP1636731A2 (en) * | 2003-06-25 | 2006-03-22 | Siemens Medical Solutions USA, Inc. | Systems and methods for automated diagnosis and decision support for breast imaging |
US20120053446A1 (en) * | 2007-11-21 | 2012-03-01 | Parascript Llc | Voting in image processing |
US20100063947A1 (en) * | 2008-05-16 | 2010-03-11 | Burnside Elizabeth S | System and Method for Dynamically Adaptable Learning Medical Diagnosis System |
WO2010035161A1 (en) * | 2008-09-26 | 2010-04-01 | Koninklijke Philips Electronics, N.V. | System and method for fusing clinical and image features for computer-aided diagnosis |
US8737715B2 (en) * | 2009-07-13 | 2014-05-27 | H. Lee Moffitt Cancer And Research Institute, Inc. | Methods and apparatus for diagnosis and/or prognosis of cancer |
US10282840B2 (en) * | 2010-07-21 | 2019-05-07 | Armin Moehrle | Image reporting method |
KR20140138501A (en) * | 2013-05-24 | 2014-12-04 | 삼성전자주식회사 | Lesion classification apparatus, and method for modifying a lesion classification data |
US10049447B2 (en) * | 2013-11-06 | 2018-08-14 | H. Lee Moffitt Cancer Center and Research Insititute, Inc. | Pathology case review, analysis and prediction |
CN110110843B (en) | 2014-08-29 | 2020-09-25 | 谷歌有限责任公司 | Method and system for processing images |
US10909675B2 (en) * | 2015-10-09 | 2021-02-02 | Mayo Foundation For Medical Education And Research | System and method for tissue characterization based on texture information using multi-parametric MRI |
WO2017142629A1 (en) | 2016-02-18 | 2017-08-24 | Google Inc. | Image classification neural networks |
US20170249739A1 (en) * | 2016-02-26 | 2017-08-31 | Biomediq A/S | Computer analysis of mammograms |
US10025902B2 (en) * | 2016-08-12 | 2018-07-17 | Verily Life Sciences Llc | Enhanced pathology diagnosis |
US10748277B2 (en) * | 2016-09-09 | 2020-08-18 | Siemens Healthcare Gmbh | Tissue characterization based on machine learning in medical imaging |
EP3570753A4 (en) | 2017-02-23 | 2020-10-21 | Google LLC | Method and system for assisting pathologist identification of tumor cells in magnified tissue images |
US20200085382A1 (en) * | 2017-05-30 | 2020-03-19 | Arterys Inc. | Automated lesion detection, segmentation, and longitudinal identification |
US10650520B1 (en) * | 2017-06-06 | 2020-05-12 | PathAI, Inc. | Systems and methods for training a statistical model to predict tissue characteristics for a pathology image |
US10013781B1 (en) | 2017-06-13 | 2018-07-03 | Google Llc | Sewing machine-style polygon drawing method |
GB201709672D0 (en) * | 2017-06-16 | 2017-08-02 | Ucl Business Plc | A system and computer-implemented method for segmenting an image |
KR20200003407A (en) | 2017-07-28 | 2020-01-09 | 구글 엘엘씨 | Systems and methods for predicting and summarizing medical events from electronic health records |
CN107644419A (en) * | 2017-09-30 | 2018-01-30 | 百度在线网络技术（北京）有限公司 | Method and apparatus for analyzing medical image |
WO2019075410A1 (en) * | 2017-10-13 | 2019-04-18 | Ai Technologies Inc. | Deep learning-based diagnosis and referral of ophthalmic diseases and disorders |
US20190122073A1 (en) * | 2017-10-23 | 2019-04-25 | The Charles Stark Draper Laboratory, Inc. | System and method for quantifying uncertainty in reasoning about 2d and 3d spatial features with a computer machine learning architecture |
JP7069359B2 (en) * | 2018-06-18 | 2022-05-17 | グーグル エルエルシー | Methods and systems for improving cancer detection using deep learning |
GB201812050D0 (en) * | 2018-07-24 | 2018-09-05 | Dysis Medical Ltd | Computer classification of biological tissue |
US10818386B2 (en) * | 2018-11-21 | 2020-10-27 | Enlitic, Inc. | Multi-label heat map generating system |
-
2019
- 2019-01-11 US US16/246,156 patent/US10936160B2/en active Active
- 2019-10-10 EP EP19797441.3A patent/EP3909059A1/en active Pending
- 2019-10-10 JP JP2021540194A patent/JP7247352B2/en active Active
- 2019-10-10 WO PCT/US2019/055629 patent/WO2020146024A1/en unknown
- 2019-10-10 US US17/422,356 patent/US11934634B2/en active Active
- 2019-10-10 AU AU2019420099A patent/AU2019420099A1/en active Pending
- 2019-10-10 CN CN201980088777.9A patent/CN113272915B/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2009516551A (en) | 2005-11-23 | 2009-04-23 | ザ メディパターン コーポレイション | Quantitative and qualitative computer-aided analysis method and system for medical images |
JP2009086765A (en) | 2007-09-27 | 2009-04-23 | Fujifilm Corp | Medical report system, medical report creating device, and medical report creating method |
US20160364862A1 (en) | 2015-06-12 | 2016-12-15 | Merge Healthcare Incorporated | Methods and Systems for Performing Image Analytics Using Graphical Reporting Associated with Clinical Images |
WO2018189551A1 (en) | 2017-04-12 | 2018-10-18 | Kheiron Medical Technologies Ltd | Malignancy assessment for tumors |
Also Published As
Publication number | Publication date |
---|---|
US20200225811A1 (en) | 2020-07-16 |
JP2022516805A (en) | 2022-03-02 |
EP3909059A1 (en) | 2021-11-17 |
US11934634B2 (en) | 2024-03-19 |
AU2019420099A1 (en) | 2021-07-29 |
US20220121330A1 (en) | 2022-04-21 |
CN113272915B (en) | 2022-12-20 |
WO2020146024A1 (en) | 2020-07-16 |
CN113272915A (en) | 2021-08-17 |
US10936160B2 (en) | 2021-03-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7247352B2 (en) | Systems, User Interfaces, and Methods for Interactive Negative Explanation of Machine Learning Localization Models in Medical Applications | |
Wang et al. | AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system | |
Papanastasopoulos et al. | Explainable AI for medical imaging: deep-learning CNN ensemble for classification of estrogen receptor status from breast MRI | |
EP1751550B1 (en) | Liver disease diagnosis system, method and graphical user interface | |
JP2021528751A (en) | Methods and systems for improving cancer detection using deep learning | |
Xu et al. | Quantifying the margin sharpness of lesions on radiological images for content‐based image retrieval | |
Chang | Bayesian analysis revisited: a radiologist's survival guide | |
Chen et al. | Multimodality Attention-Guided 3-D Detection of Nonsmall Cell Lung Cancer in 18 F-FDG PET/CT Images | |
US20240112338A1 (en) | Systems and methods to process electronic images to produce a tissue map visualization | |
Zhao et al. | Diagnosis of thyroid disease using deep convolutional neural network models applied to thyroid scintigraphy images: a multicenter study | |
Pham et al. | Identifying an optimal machine learning generated image marker to predict survival of gastric cancer patients | |
Kang et al. | Towards a quantitative analysis of class activation mapping for deep learning-based computer-aided diagnosis | |
Paliwal et al. | A Comprehensive Analysis of Identifying Lung Cancer via Different Machine Learning Approach | |
Angelica et al. | Impact of computer vision with deep learning approach in medical imaging diagnosis | |
Carolus et al. | Automated detection and segmentation of mediastinal and axillary lymph nodes from CT using foveal fully convolutional networks | |
Han et al. | Automatic cancer detection and localization on prostatectomy histopathology images | |
Ravi et al. | Abnormality Classification in PET Images of Prostate Tumour using Neural Network Methods | |
AU2021100932A4 (en) | Machine learning & deep learning for disease detection | |
JP7450410B2 (en) | Medical information processing device, medical information processing method, and medical information processing program | |
Li et al. | Radiomic features derived from pre-operative multi-parametric MRI of prostate cancer are associated with decipher risk score | |
Ahila et al. | Identification of Malignant Attributes in Breast Ultrasound using a Fully Convolutional Deep Learning Network and Semantic Segmentation | |
Goswami et al. | Application of Deep Learning in Cytopathology and Prostate Adenocarcinoma Diagnosis | |
Safari | A State-of-the-Art Survey of Deep Learning Techniques in Medical Pattern Analysis and IoT Intelligent Systems | |
Kober et al. | Convolutional auto-encoder to extract local features of 2D images | |
Sorin et al. | Deep Learning for Contrast Enhanced Mammography-a Systematic Review |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20210908 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20210908 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20221028 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20221031 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20230127 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20230213 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20230315 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7247352Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |