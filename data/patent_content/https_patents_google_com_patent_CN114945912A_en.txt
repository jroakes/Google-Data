CN114945912A - Automatic enhancement of streaming media using content transformation - Google Patents
Automatic enhancement of streaming media using content transformation Download PDFInfo
- Publication number
- CN114945912A CN114945912A CN202080046312.XA CN202080046312A CN114945912A CN 114945912 A CN114945912 A CN 114945912A CN 202080046312 A CN202080046312 A CN 202080046312A CN 114945912 A CN114945912 A CN 114945912A
- Authority
- CN
- China
- Prior art keywords
- content
- media content
- visual data
- data
- media
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/432—Query formulation
- G06F16/433—Query formulation using audio data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/48—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/483—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/538—Presentation of query results
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
Abstract
A method includes receiving media content including audio data for distribution by a content distribution platform that requires the media content to include video content, transforming the audio data into textual content, determining, based on a search of a searchable database, characteristics of the textual content of the audio data matching visual data in the searchable database, in response to determining that the textual content of the audio data matches the characteristics of the visual data, integrating the visual data with the media content having the matching characteristics to create an enhanced content stream, and distributing the enhanced content stream by the content distribution platform that requires the media content to include video content.
Description
Technical Field
This description relates to data processing, transforming data from streaming media to detect characteristics of streaming media, and matching detected characteristics with other media.
Background
Media is distributed in various ways. For example, some media is streamed by a streaming device (stream), and the media may include audio, video, or a combination of audio and video. Some streaming services require that the media include certain types of content in order to be distributed by the streaming service platform.
Disclosure of Invention
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include receiving media content including audio data for distribution by a content distribution platform that requires the media content to include video content, transforming the audio data into textual content, determining that the textual content of the audio data matches a characteristic of visual data in a searchable database based on a search of the searchable database, integrating visual data with the media content having the matching characteristic to create an enhanced content stream in response to determining that the textual content of the audio data matches the characteristic of the visual data, and distributing the enhanced content stream by the content distribution platform that requires the media content to include video content.
These and other implementations can optionally include one or more of the following features. In some implementations, the method further includes detecting an annotation located at a particular temporal location within the media content, and wherein integrating the visual data with the media content to create the enhanced content stream includes overlaying the visual data with the media content at the particular temporal location within the media content based on the annotation. In some implementations, the annotation specifies one or more visual data characteristics.
In some implementations, integrating the visual data with the media content to create the enhanced content stream further includes editing the visual data based on one or more visual data characteristics.
In some implementations, the annotation specifies that the visual data cannot be overlaid with the media content at a particular temporal location.
In some implementations, the method includes determining a first context of the media content based on the text content of the audio data, determining a second context of the visual data based on characteristics of the visual data in the searchable database, and wherein determining that the text content of the audio data matches the characteristics of the visual data in the searchable database includes determining that the first context matches the second context. In some implementations, the method includes identifying a particular temporal location within the media content based on a first context of the media content, and wherein integrating the visual data with the media content to create the enhanced content stream includes superimposing the visual data with the media content at the particular temporal location within the media content.
Other embodiments of this aspect include corresponding systems, apparatus, and computer programs configured to perform the actions of the methods encoded on computer storage devices.
Content creators are generally limited to providing content in a format that creates the content, which may limit the platforms on which their content can be published and the audience to which they can reach. If a content creator generates an audio file, such as a podcast, the podcast (or other audio file) may not be eligible for distribution through a platform that requires visual components (e.g., image or video aspects). In such a scenario, the podcast will be blocked from distribution and potential listeners interested in the podcast cannot find the podcast using a platform that requires the inclusion of a visual component (e.g., video content).
Text formatted data is typically used to identify media content requested and consumed by a user, and existing systems are customized to use text-based characteristic data to analyze and select additional content for distribution with the media content. Current methods of matching and selecting additional content for non-text-based media depend on contextual data (e.g., in the form of textual metadata) that is often manually provided by the creator of the additional content. However, text-based systems do not utilize the actual audio/video media content itself to match and/or select media content. This may lead to wasted resources (e.g., when the contextual data assigned to the content does not accurately describe the content or fails to provide a sufficient description of the content) as this may lead to distribution of the content that is not useful in the context of streaming media content. In other words, text-based analysis using metadata provided by the content creator, without analyzing the audio or video aspects of the content, may result in an inefficient system that is unable to identify content related to the content the user is looking for. This results in additional time spent and requires additional network calls and additional queries to be submitted that may drain the battery of the client device, additional searches of the database that require more processing power, and additional content delivered to the client device using more network bandwidth and consuming the battery power of the client device.
The following description discusses various techniques and systems for improving a mechanism for controlling whether and the type of content that is sent over a network by transforming streaming media content into a form that enables information derived from the streaming media content itself to be used in addition to or instead of using metadata provided for the streaming media content to select content for presentation with the streaming media content.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The system may transform the streaming media content to include content in a different format or type, thereby making it suitable for analysis by existing systems. Further, the system allows for the use of non-textual content in a text matching mechanism to enable selection of audio content to be presented to a user with a streaming media content item based on a determination that the non-textual content corresponds to the audio content. Further, the techniques enable transforming a particular type of media content (e.g., audio content) into a different type of media content (e.g., mixed media, such as audio and video content), enabling distribution of the media content through a platform that distributes content including only the different types of media (e.g., a video distribution platform). Furthermore, and as will be appreciated, a greater amount of information may be transmitted over the same time frame using mixed media than non-mixed media.
The techniques discussed herein may enhance the transformation of streaming media content (e.g., audio content such as podcasts) based on various audio characteristics other than the words spoken within the streaming media content. For example, the techniques discussed herein may distinguish between speakers to prevent semantic drift that may occur if no distinction is made between speakers. Moreover, the techniques discussed herein provide an improvement over conventional systems by emphasizing transformations of spoken words in streaming media content (e.g., podcasts) based on audio characteristics other than speaker recognition. For example, the volume of some words relative to other words may be used to embed (or assign) an emphasis (or other indicator of importance) to words detected in the streaming media content, thereby enhancing information used to select additional content (e.g., digital components) for presentation with the streaming media content. In some scenarios, the emphasis assigned to a word may be proportional (e.g., directly or inversely) to the volume of the word, the change in pitch relative to other spoken words, or the amount of silence detected around the word. This provides richer information about the audio of the streaming media content that can be input to a text matching mechanism to provide a more accurate match between the subject matter of the streaming media content and the selected additional content for presentation.
Existing techniques that require manual selection by the creator of a particular media content item and integration of additional non-textual content into the particular media content item are time consuming and limited to additional content that the creator can access or know about. By transforming non-textual content into a format that can be processed and matched by a text-based system, the described techniques allow access to a more diverse and comprehensive selection of additional content as well as improved and more customized selection of content. In addition, the described system allows for dynamic content that can be updated and customized for each user's preferences. The described techniques reduce the resources required to train and improve the content matching and selection process because existing infrastructure and systems can be used, and allow non-textual content systems to leverage the accumulated knowledge available to existing systems for text-based content matching and selection. Furthermore, the described approach allows for more narrowly tailored content by analyzing the entirety of the content, as compared to other ways that would only use user-defined context data that is provided by the content creator, that is only manually entered.
By integrating with an existing database of different types of content than the original content, the system expands the hierarchy (universe) by which the system can pick available content for distribution to users. For example, audio content may be matched with image or video content, and new content may be created from the original content and the matched content. The process provides a simplified method for creating new content or content in a different format. Because the resulting content generated from the original content can be customized as needed, the resulting content can be dynamically updated/modified based on the fact that many versions of the content, which only require storage of a single instance of the original input content, can be created using the same original input content when the original content is requested or personalized depending on the requesting user's profile. This reduces the overall memory requirements, as only a single instance of the original input content need be stored when enabling the distribution of many different versions, relative to having to store instances of each separate version. This allows a single content creator to customize and customize their content for a wider audience without requiring additional memory capacity or network services (e.g., network storage or processing resources). Furthermore, the original content may be enhanced with additional content without disturbing the quality of the original content. Thus, the content being generated is dynamic and can be refreshed and personalized each time the original content is requested.
Further, using context to identify and select additional content can prevent distribution of inappropriate content, thereby reducing wasted resources. This approach reduces the amount of resources expended to distribute content that is inappropriate and should not be distributed and more efficiently provides content across the network-the approach prevents distribution of content to entities that do not actually consume (listen to and/or view) the content. In other words, using these resources to distribute content that should not be distributed does not waste computing resources such as network bandwidth, processor cycles, and/or allocated memory.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example environment for content transformation.
FIG. 2 depicts a data flow of an example content transformation method for improved content matching and selection.
FIG. 3 depicts an example content transformation method for improved content matching and selection.
FIG. 4 is a flow diagram of an example process including content transformation of matches and selections.
FIG. 5 is a block diagram of an example computer system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This document discusses techniques and systems to convert a media item containing a particular type of media (e.g., audio) into a new media item including at least one different type of media (e.g., video). As discussed in detail below, the techniques may transform non-textual content into a form that enables a text-based matching system to select non-textual content for presentation and create the content in a new form. For example, as discussed in more detail below, a podcast (e.g., an audio program with a moderator and typically a guest) can be analyzed using the techniques discussed herein, and a transformation of the audio aspect of the podcast can be created to enable selection of other content or digital components for presentation with and/or insertion into the podcast. In a particular example, video content corresponding to a topic being discussed in a podcast may be selected and combined with audio content of the podcast to convert the podcast from an audio file to an audio/video file that can be distributed using a video distribution platform that may not distribute audio-only content.
In some implementations, other content and/or digital components (collectively "additional content") that are available for delivery with the original content are in a non-textual format. A textual transcription of audio with additional content of audio data or a textual description of additional content with visual data may be stored in a text searchable database. Original content including non-text content is also obtained and transformed into text content. The transformation of the non-text content may be encoded (or emphasized) in various ways to reflect various characteristics of the media content. For example, as discussed in more detail below, if the original content is audio data, the transformation of the audio data may reflect characteristics of the audio content other than the spoken word. The transformation of the original content is used to search a searchable database to find a match between the original content and the additional content.
Additional content is integrated with the original content to create an enhanced content stream and delivered to the user who has requested the original content. The additional content may be integrated with the original content to create a new content stream. In some implementations, additional content is inserted into the original content stream. In some implementations, the additional content is overlaid (overlay) with the original content stream. The techniques discussed herein may be performed when a user requests original content, such that searches of a searchable database may be augmented (augmented) with additional information (e.g., user-specific information, current event information, or other information) when the original content is presented, thereby providing dynamic media items that may be modified for each individual user and/or based on information that changes over time.
Throughout this document, controls may be provided for users (e.g., end users, content generators or content providers, among other types of users), that allow users to make selections as to whether to send content or communications from a server to the user, and whether and when the systems, programs, or features described herein may enable the collection of user information (e.g., information about the user's social network, social actions or activities, profession, the user's preferences, or the user's current location). Further, certain data may be processed (processed) in one or more ways before being stored or used, such that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information of the user cannot be determined, or the geographic location of the user may be generalized to a location where location information (such as a city, zip code, or state level) is obtained such that a particular location of the user cannot be determined. Thus, the user may have control over what information is collected about the user, how the information is used, and what information is provided to the user.
FIG. 1 is a block diagram of an example environment 100 for privacy preserving data collection and analysis. The example environment 100 includes a network 102, such as a Local Area Network (LAN), a Wide Area Network (WAN), the Internet, or a combination thereof. The network 102 connects an electronic document server 104 ("electronic document server"), user devices 106, and a digital content distribution system 110 (also referred to as a DCDS 110). The example environment 100 may include many different electronic document servers 104 and user devices 106.
The user device 106 is an electronic device capable of requesting and receiving resources (e.g., electronic documents) over the network 102. Example user devices 106 include personal computers, wearable devices, smart speakers, tablet devices, mobile communication devices (e.g., smart phones), smart appliances, gaming systems, and other devices that can send and receive data over the network 102. In some implementations, the user device may include a speaker that outputs audible information to the user, and a microphone that accepts audible input (e.g., spoken word input) from the user. The user device may also include a digital assistant that provides an interactive voice (voice) interface for submitting input and/or receiving output provided in response to the input. The user device may also include a display to present visual information (e.g., text, images, and/or video). The user device 106 typically includes a user application, such as a web browser, to facilitate sending and receiving data over the network 102, although a native application executed by the user device 106 may also facilitate sending and receiving data over the network 102.
The user device 106 includes software such as a browser or operating system. In some implementations, the software allows a user to access information over a network, such as network 102, retrieve information from a server, and display the information on a display of user device 106. In some implementations, software manages the hardware and software resources of the user device 106 and provides common services for other programs on the user device 106. The software may act as an intermediary between the program and the hardware of the user device 106.
An electronic document is data that presents a collection of content at a user device 106. Examples of electronic documents include web pages, word processing documents, Portable Document Format (PDF) documents, images, videos, search result pages, and feed sources. Native applications (e.g., "apps"), such as applications installed on mobile, tablet, or desktop computing devices, are also examples of electronic documents. An electronic document 105 ("electronic file") may be provided by the electronic document server 104 to the user device 106. For example, the electronic document server 104 may include a server hosting a publisher's website. In this example, the user device 106 may initiate a request for a given publisher web page, and the electronic document server 104 hosting the given publisher web page may respond to the request by sending machine hypertext markup language (HTML) code that initiates presentation of the given web page at the user device 106.
The electronic document may include various contents. For example, the electronic document 105 may include static content (e.g., text or other specified content) that is within the electronic document itself and/or that does not change over time. The electronic document may also include dynamic content that may change over time or on a request basis. For example, a publisher of a given electronic document may maintain a data source for populating portions of the electronic document. In this example, the given electronic document may include a tag or script that causes the user device 106 to request content from a data source when the given electronic document is processed (e.g., rendered or executed) by the user device 106. The user device 106 integrates content obtained from the data source into the presentation of the given electronic document to create a composite electronic document that includes the content obtained from the data source. The media content referred to herein is one type of digital content.
In some cases, a given electronic document may include a digital content tag or digital content script that references the DCDS 110. In these cases, the digital content tag or digital content script is executed by the user device 106 when a given electronic document is processed by the user device 106. Execution of the digital content tag or digital content script configures the user device 106 to generate a request 108 for digital content that is communicated to the DCDS110 over the network 102. For example, a digital content tag or digital content script may enable user device 106 to generate a packetized data request including a header and payload data. The request 108 may include data, such as the name (or network location) of the server from which the digital content is being requested, the name (or network location) of the requesting device (e.g., user device 106), and/or information that the DCDS110 may use to select the digital content to provide in response to the request. The request 108 is transmitted by the user device 106 to a server of the DCDS110 over the network 102 (e.g., a telecommunications network).
The request 108 may include data specifying characteristics of the electronic document and the location where the renderable digital content is located. For example, data specifying a reference (e.g., a URL) to an electronic document (e.g., a web page) in which digital content is to be presented, an available location (e.g., a digital content slot) of the electronic document that is available for presenting the digital content, a size of the available location, a location of the available location within the presentation of the electronic document, and/or a media type that is eligible (eligible) for presentation in the location may be provided to DCDS 110. Similarly, keywords designated as being assigned for selection of an electronic document ("document keywords") or data of an entity (e.g., a person, place, or thing) referenced by the electronic document may also be included in the request 108 (e.g., as payload data) and provided to the DCDS110 to facilitate identifying digital content items that are eligible for presentation with the electronic document.
The request 108 may also include data related to other information, such as information that the user has provided, geographic information indicating the state or region from which the request was submitted, or other information that provides context of the environment in which the digital content is to be displayed (e.g., the type of device that is to display the digital content, such as a mobile device or tablet device). The user-provided information may include demographic data of the user device 106. For example, demographic information may include geographic location, occupation, hobbies, social media data, and whether a user owns a particular project, among other characteristics.
Data specifying characteristics of the user device 106 may also be provided in the request 108, such as information identifying a model of the user device 106, a configuration of the user device 106, or a size (e.g., display size or resolution) of an electronic display (e.g., touchscreen or desktop monitor) on which the electronic document is presented. The request 108 may be sent over a packet network, for example, and the request 108 itself may be formatted as packet data having a header and payload data. The header may specify the destination of the packet and the payload data may include any of the information discussed above.
The DCDS110 selects digital content to be presented with a given electronic document in response to receiving the request 108 and/or using information included in the request 108. The DCDS110 may select additional content to be presented with a given electronic document in response to receiving the request 108, the additional content defined as follows: digital components that are available for delivery with the original content and content other than the original content.
In some implementations, the DCDS110 is implemented in a distributed computing system (or environment) that includes, for example, a server and a collection of multiple computing devices interconnected and responsive to the request 108 to identify and distribute digital content. The collection of multiple computing devices operate together to identify a collection of digital content that is eligible for presentation in an electronic document from among a corpus of millions or more of available digital content. Millions or more of available digital content may be indexed, for example, in the digital component database 112. Each digital content index entry may reference the corresponding digital content and/or include distribution parameters (e.g., selection criteria) that affect the distribution of the corresponding digital content.
The identification of eligible digital content may be segmented into a plurality of tasks that are then dispatched among the computing devices within the set of multiple computing devices. For example, different computing devices may each analyze different portions of the digital component database 112 to identify various digital content having distribution parameters that match the information included in the request 108.
The DCDS110 aggregates results received from a set of multiple computing devices and uses information associated with the aggregated results to select one or more instances of digital content to be provided in response to the request 108. In turn, the DCDS110 may generate and send back reply data 114 (e.g., digital data representing a reply) over the network 102 that enables the user device 106 to aggregate the set of selected digital content into a given electronic document such that the set of selected digital content and the content of the electronic document are presented together on the display of the user device 106.
The text-to-speech system 120 is a content transformation system that can transform textual content into audio content and vice versa. In the context of the present description, audio content is not constrained to be audio-only content. For example, in some embodiments, audio content may include video content, which may be referred to as multimedia content, and for purposes of this document will still be considered audio content. The text-to-speech system 120 may perform content transformation using techniques for text-to-speech and speech-to-text transformation. For example, the text-to-speech system 120 may transcribe the audio-only file into plain text (playlist). The text-to-speech system 120 may also transform text-only files into audio files that may include corresponding image files. The output of the text-to-speech system 120 may be an audio-only file, a video file with audio and visual data, an image-only file, or a text-only file, among other formats. The text-to-speech system 120 detects words within the audio file and outputs the words, transcribing the content of the audio file. The DCDS110 stores the results of its analysis in a data store such as the content analysis and mapping database 116. The content analysis and mapping database 116 stores relationships between content items, content characteristics, entities and events, and other types of data.
The matching and selection system 130 performs a content matching and selection process for the DCDS 110. For example, the matching and selection system 130 can perform analysis of the output of the text to speech system 120 or any received content to determine specific characteristics of the content itself, such as topics or categories of content, entities mentioned or suggested by the content, and/or frequency of mentioning topics or entities, among other characteristics. The matching and selection system 130 may also perform analysis to determine characteristics of the audio data, such as volume, emphasis, pitch, and other characteristics. For example, the matching and selection system 130 may determine a topic or intent of the media content.
The matching and selection system 130 may be implemented as two separate systems. In some implementations, the matching and selection system 130 can perform matching and selection separately for other content (such as image or video data to be overlaid with the original content) and digital components (such as links to related websites to be inserted into the content or into composite content created using the original content and the other content).
For example, the subsystem of the matching and selection system 130 that performs matching and selection of other content may operate independently of the subsystem of the matching and selection system 130 that performs matching and selection for digital components and have different selection and matching algorithms and criteria than the subsystem of the matching and selection system 130 that performs matching and selection for digital components. In some implementations, requests for other content and requests for digital components may be sent as separate requests to separate subsystems. Separate subsystems of the matching and selection system 130 may each return a response with a winning candidate content item (any of the other content or digital components) based on their own selection process.
The matching and selection system 130 may use the transformed media content as input to a content or digital component selection process. For example, the matching and selection system 130 may perform a content selection auction according to the prior art and using the transformed media content as input.
The matching and selection system 130 can perform content matching by matching the content with characteristics of the content files themselves, for example, using artificial intelligence and machine learning techniques.
The matching and selection system 130 may use statistical and/or machine learning models that accept user-provided information and media content as input. The machine learning model may use any of a variety of models, such as decision trees, models based on generating confrontation networks, deep learning models, linear regression models, logistic regression models, neural networks, classifiers, support vector machines, inductive logic programming, integration of models (ensembles) (e.g., using techniques such as bagging, lifting, random forests, etc.), genetic algorithms, bayesian networks, etc., and may be trained using a variety of methods, such as deep learning, association rules, inductive logic, clustering, maximum entropy classification, learning classification, etc. In some examples, the machine learning model may use supervised learning. In some examples, the machine learning model uses unsupervised learning.
The content delivery system 140 performs content packaging and delivery. For example, the content delivery system 140 may format content, combine media content, and deliver the content to user devices. The content delivery system 140 transforms specific types of media content into different types of media content. For example, the content delivery system 140 may add visual data to audio data to create a mixed media, such as a video with audio data. The content delivery system 140 creates new media items from the original content and additional content from other sources to allow media content to be distributed on platforms that only distribute content that includes different types of media.
In some implementations, the temporal annotation may indicate the following numeric content slots: in the digital content slot, the digital content is expected to be delivered within media content delivered to a user device (such as user device 106).
In some implementations, the content delivery system 140 may provide additional audio content within the media content stream within the defined slot. For example, in some implementations, the content delivery system 140 may insert a pause that serves as a defined slot within the audio file at the marker where the additional audio content may be integrated.
In some implementations, the content delivery system 140 can pause delivery of the media content stream upon detecting the marker, separately deliver the additional audio content, and then resume delivery of the media content.
FIG. 2 depicts a data flow 200 of an example content transformation method for improved content matching and selection in the example environment of FIG. 1. The operations of data flow 200 are performed by various components of system 100. For example, the operations of the data flow 200 may be performed by the DCDS110 in communication with the user equipment 106.
The method includes automatically transforming non-textual content into text so that the content can be analyzed and the analyzed text can be used to match related content from other sources. For example, an original podcast file that includes only audio is first transformed into text so that the content of the audio can be analyzed and matched with additional content. The original podcast file is then combined with the additional content and provided to the requesting user. For example, where additional content is to be provided with the original content, the additional content may be requested by referencing the additional content from the data store, and the additional content may be combined with the original content as part of a data stream provided to the requesting user. The additional content may also be combined with the original content prior to transmission to the requesting user.
The flow begins at step a, where non-textual media content, referred to as "original content," is generated and uploaded to a content provider location. For example, a content creator, podcast host, Podrick Castlerock, may generate and upload audio-only media content 202 and upload the content 202 to a video hosting platform, ExampleVideoPlatform, where he regularly uploads his episodes of general interest podcasts, where he discusses the cool, new security features of the car and also talks about the loved dog. In some implementations, the entire media content is uploaded at once. In some implementations, the media content is uploaded in real-time (e.g., as the content is to be distributed and/or presented).
In this particular example, the non-textual media content 202 is a podcast episode called "guest Leuy Hamster talk about New Advances in F1 Security and his dog" that includes only audio data. In other examples, the non-textual media content may include video files with audio and image data or video files with only image data, as well as other types of non-textual media content. For example, the non-textual media may be a television program or a video game. In some examples, the non-textual media content may be a simple video having audio content and still images, and such video may simply be in a video file format but include audio content having placeholder images or video content. The placeholder image/video content may be replaced.
When a content creator uploads the content 202, the content creator may include annotations within the content 202 to mark temporal and/or visual locations where additional content may be included. For example, when a podcast episode 202 is created by a podcast castle, he may include temporal annotations where other content (such as complementary image/video content) may be integrated with the podcast episode 202 and temporal annotations where digital content may be integrated with the podcast episode 202. In some implementations, if the content creator uploads a video file that includes audio content and placeholder image/video content, the content creator can include a temporal annotation to mark the content as a file with audio content and alternative video/image content. Temporal annotations may also indicate, for example, the beginning and/or end of a particular segment of the original content. In some implementations, the temporal annotation may indicate the beginning and/or end of a period in which additional content may be displayed or integrated into the original content 202. For example, temporal annotations may be used to calculate time periods during which additional content may be displayed or integrated by determining temporal distances between the annotations. In one example, the Podrick Castlerock may include temporal annotations to the podcast episode 202 to indicate where the interruption occurred, when the topic changed, what the topic was, and so forth.
The flow continues with step B, where the audio content is provided to the digital content distribution system. In this example, the audio content 202 may be provided to the DCDS110 by a Podrick Castlerock. In some implementations, the audio content 202 may be uploaded to or retrieved by the DCDS110, among other ways obtained by the DCDS 110.
The described system is particularly efficient because audio content may be provided to the DCDS110, which may use improved techniques for performing portions of the described techniques to access a uniquely large amount of information and models. As depicted in fig. 2, the DCDS110 accesses the digital component database 112 and the electronic document server 104. The DCDS110 may match and select digital content, such as related image/video content to be integrated with the original content. In some implementations, the DCDS110 may match and select digital components to be integrated with the original content in addition to the digital content to be overlaid with the original content.
The flow continues with step C, where the text-to-speech system processes the audio content received by the DCDS110 and outputs text data. For example, the text-to-speech system 120 may process audio data received by the DCDS110 to generate output text data 204. The original form of the media content and the text of the media content may be stored or cached for future requests for the media content. For example, the original form and text of the media content may be stored in a database such as the content analysis and mapping database 116 or a different remote database. Databases in which the original form and/or text of the media content is stored may be indexed and/or easily searchable. The media content includes both non-textual media content from the content creator and media content to be provided as additional audio content integrated with the non-textual media content.
In addition, the text-to-speech system 120 may analyze the audio content to perform functions such as enhancing the original content by adding subtitles, performing audio analysis for beat patterns, generating audio signatures, performing music recognition and analysis, and performing language detection, accent detection, topic and entity detection, among other functions. In some implementations, when the original content 202 is a video file, the text-to-speech system can analyze the audio content and compare the complexity of the audio content to the accompanying visual content to determine whether the visual content is placeholder content or actually related to the audio.
In some implementations, the media content is uploaded in real-time and steps A, B and C are performed in real-time. For example, the upload may occur while recording the media content. In some implementations, the non-textual media content is uploaded in its entirety, and steps A, B and C may be performed once the content is received. In some implementations, the media content is fully uploaded and steps A, B and C may be performed when the media content is requested to be provided to the user device.
The text-to-speech system 120 performs content transformation by analyzing the characteristics of the content itself and the characteristics of the stream. While other text-to-speech or speech-to-text systems may be required to transcribe each word, the text-to-speech system 120 need not transcribe each word to determine the intent, category, topic, volume, etc. of the media content. Thus, the system described herein is particularly efficient because it only requires that a portion of the media content be transcribed. Furthermore, the system is able to exclude low confidence transcriptions and rely only on high confidence transcriptions for performing content matching and selection. In some implementations, the text-to-speech system 120 may automatically exclude any transcriptions below a confidence threshold level. In some implementations, the text-to-speech system 120 may provide the entire transcription to the matching and selection system 130 even if it is not sufficient to perform content matching and selection. In some implementations, the text-to-speech system 120 may instead provide a message to the matching and selection system 130 that insufficient content may be transformed.
In some implementations, the additional audio content is received as text-based content and may be transformed by the text-to-speech system 130 into the same type of media content as that presented to the user device 106. For example, additional audio content may be received as multiple lines of text and may be transformed into audio-only content to match the format of podcast episode of podcast castle "guest Leuy Hamster talk about new advances in F1 security and his dog".
In some implementations, the additional audio content is received as non-text content and may be transcribed into text by the text-to-speech system 130. The text of the additional audio content may then be stored in a database, such as the digital component database 112. The original form of the additional audio content may also be stored so that matching can be performed with the text of the additional audio content and the additional audio content itself can be provided upon request.
In some implementations, the audio content may include multiple speakers. For example, the podcast 202 may include two speakers: podrick Castlerock and his guest Leuy Hamster. The text-to-speech system 120 can distinguish between the voices of the moderator Podrick Castlerock and the guest Leuy Hamster. In some implementations, the text-to-speech system 120 may indicate which speaker speaks what text within its text output 204. For example, the text-to-speech system 120 may flag each portion of text with a speaker of each portion of text. The text-to-speech system 120 may distinguish between the voices of different speakers based on one or more characteristics of the audio. For example, text-to-speech system 120 may compare differences (e.g., frequency differences) between two speech sounds, match characteristics of a particular speech sound to a particular speech profile (profile) indicating a unique combination of frequency and/or pitch characteristics, and/or learn and develop a speech profile for each speaker, among other techniques.
Audio analysis of audio characteristics, topics, entities, etc. are stored within the data store of the DCDS 110. For example, the data store may be a relational SQL database, a document-oriented database, a key-value store, or a file system, among other systems. The DCDS110 may manage the database by performing analysis on the stored data to combine attributes of the data and enrich the available data. For example, the DCDS110 may perform analysis on stored data using algorithms to correlate dates, times, and locations to identify relevant events and entities, such as people, from the data. These identified events and entities may then be stored in a data store and mapped to a combination of the analyzed data. For example, if the textual data 204 of the podcast episode 202 indicates that podick Castlerock says "23 mo joule d/8/2003", the text-to-speech system 120 may detect that the date and location are related to "formula one race (F1)", "2003 hungarian jackpot" and "Fernando all who won that jackpot, and that is the first race win of Fernando all in F1. The DCDS110 may perform this analysis using data from the data store and data available through sources such as a search engine archive.
The flow continues with step D, where the DCDS110 receives a request from the user device for content including various information, such as information about the user of the user device. For example, the DCDS110 may receive a request 108 from a user of the user device 106 that includes profile information of the user. The request may include more than one request or may be parsed into sub-requests by the DCDS 110. For example, the request may include a request for additional content to be superimposed on the audio of the podcast episode 202 and a request for a digital component to be inserted into the combined content resulting from the combination of the podcast episode 202 and any additional content superimposed with the podcast episode 202. In some implementations, when the podcast episode 202 is requested, the request is generated by the user device 106. For example, when the podcast episode 202 is streamed to the user device 106, the user device 106 may generate one or more requests for additional content. The request may include one or more temporal annotations for context of the requested additional content, length of time the additional content was requested, display size of the requested additional content, file size of the requested additional content, and the like.
In some implementations, the DCDS110 receives each request to be generated for the podcast episode 202 throughout the entire episode length. For example, if there are four slots for digital components throughout the episode 202, and there are two potential slots for other content that will be overlaid with the episode 202, the DCDS110 may receive all six requests when the user device 106 requests the podcast episode 202. In some implementations, the DCDS110 receives the request in real-time as the podcast episode 202 is streamed to the user device 106.
The flow continues with step E where the matching and selection system 130 uses the text data 204 from the text-to-speech system 120 to perform matching and selection of digital content to be presented with the media content. The text data 204 may include text data for media content that is streamed to the user device 106 as well as text data for digital content that is available for presentation with the media content. In some implementations, the matching and selection system 130 can access the text data 204 from a searchable database.
Based on the analysis data stored in the content analysis and mapping database 116 of the DCDS110 by the text-to-speech system 120 and other systems accessing the database 116, the DCDS110 finds additional content to be provided with the original content. For example, the matching and selection system 130 can match the original content 202 with other content to be overlaid on the original content 202 and separately match the original content 202 with digital components to be presented with the original content 202.
The matching and selection system 130 accesses databases including the digital component database 112, the content analysis and mapping database 116, and the electronic document server 104 to perform the matching and selection process thereof. In some implementations, the retrieval system may be a scalable system including modules that can query and access different sources including databases and search engines external to the system 100. For example, the matching and selection system 130 may query and access the inventory photograph source and incorporate the inventory photograph into the original content 202 when the photograph is attributed to the source.
The matching and selection system 130 can crawl (scrape)/search related image, video, audio, text data, etc. from multiple sources over the internet and other sources and sort and store the data within a content analysis and mapping database. In some implementations, the matching and selection system 130 stores the content and its relationship to the relevant characteristics. In some implementations, the matching and selection system 130 stores relationships between content and related characteristics and pointers (pointers) to locations of content within external sources.
The matching and selection system 130 uses the stored relationships and content to pick the most relevant other content and/or digital components to include with the original content in response to a particular request. For example, the matching and selection system 130 uses the relationships and analysis stored within the content analysis and mapping database 116 to match specified criteria in the request with characteristics and parameters of the content in the electronic document server 104 and the digital components in the digital component database 112.
For each request, matching and selection system 130 may first retrieve a plurality of candidate content items that match the criteria specified within the request. For example, the matching and selection system 130 may retrieve one or more candidates from the digital component database 112, the electronic document server 104, the content analysis and mapping database 116, or one or more other sources. These candidate content items may be retrieved based on a specified threshold number of criteria within the match request. In some implementations, the candidate content items must match each criterion. In some implementations, if no candidate content item matches all of the criteria specified within the request, the matching and selection system 130 will retrieve content items that match a threshold number of criteria. In other implementations, the matching and selection system 130 does not retrieve any candidates, and will not return a reply in step G.
The matching and selection system 130 may retrieve candidates based on characteristics such as topic or topic, display size, file size, image quality or audio quality, entities mentioned within the content item, entities present within the content item (i.e., a particular historical persona is drawn in the candidate image), etc. The candidates may then be ranked according to characteristics and/or parameters specified by the request or matching and selection system 130. For example, candidates may be ranked based on technical characteristics such as quality, available bandwidth through a connection to a content source, hardware specifications of the user device 106 that will display the content, location of the content, display or file size of the content, light distribution within visual content, sound quality and magnitude of sounds within audio content, and so forth. The matching and selection system 130 may retrieve for each candidate content item, the source of the content item, the content item itself, parameters and characteristics of the content item, and the like. In some implementations, the matching and selection system 130 can determine a particular nature of the content item.
In some implementations, step E may occur before or in parallel with step D, based on whether other content or digital components are being selected. For example, a temporal annotation created by the content creator within the original content 202 may indicate that a particular slot or request for additional content may be performed prior to receiving the request from the user device 106. In some implementations, the text-to-speech system 120 can identify temporal locations within the audio data 302 and/or display locations where additional content can be integrated with the audio data 302.
The matching and selection system 130 performs the matching and selection process according to the technology used with the textual content. For example, the matching and selection system 130 may match characteristics such as topics or entities with digital content items. Certain characteristics of the media content may be given more weight than other characteristics when determining other characteristics. For example, the matching and selection system 130 may give more weight to the title of a podcast episode than just words within the content of the episode. In some implementations, the matching and selection system 130 determines and assigns a topic or some other information to the additional audio content.
The matching and selection system 130 may assign more weight to words spoken at the beginning of the speaking time of different speakers, words spoken at the pitch of a particular voice, phrases containing a particular entity or topic. In some implementations, if there are multiple speakers, the matching and selection system 130 assigns a particular weight to words spoken by the moderator of the content. For example, in a podcast or talk show, the moderator may invite the guest to join them, but the guest may have a different view of the moderator. The matching and selection system 130 may default to matching content only to what the moderator speaks. In some implementations, the matching and selection system 130 can provide options for content creators and/or publishers to choose whether they want to allow content matching and what portions of their content can be used for content matching. The matching and selection system 130 may also provide users (content creators, publishers, end users) with the option of not allowing certain types/categories of digital content to be provided with the media content being streamed.
Additionally, the matching and selection system 130 may assign more weight to the matching characteristics. For example, more weight may be given to the topic-related parameter than to the format or technical characteristics. In one example, if the video file matches the parameters specified within the request but is larger than the display size specified within the request, the video file may still be selected as a candidate. In some implementations, the matching and selection system 130 assigns weights to matching characteristics based on other characteristics, including the quality of the additional audio content to be selected, the quality of the media content being streamed to the user device, whether there is music in the background and/or the type of music being played, among other characteristics.
The text-to-speech system 120 may capture characteristics of the non-text content and provide these characteristics to the matching and selection system 130. For example, the text-to-speech system 120 may determine that the creator of the content has placed emphasis or other special prominence on a particular word or phrase and draw parallels between the non-text emphasis and the corresponding portion of the text. In one example, the text-to-speech system 120 may determine that if a person speaking in the audio content speaks a particular word or phrase aloud, it may be considered that the word or phrase will be emphasized in the same manner as bold text, underlined text, larger fonts, and other visual forms of emphasis. Other forms of audible emphasis include pitch and speed of the spoken word or phrase. For example, if the speaker uses a very low pitch and speaks a phrase quickly, text-to-speech system 120 may determine that the phrase means unimportant or that the speaker disagrees with the phrase spoken.
For example, the podcast 202 is shown as including audio data indicative of an audible indication, which in this case corresponds to a larger magnitude than other words for the words "bulldog Roscoe" and "modern F1 car", and the text-to-speech system 120 may consider that a word will be emphasized as if the word had been bolded, and the matching and selection system 130 may assign weights to the word as if the word was bolded, or otherwise add metadata indicative of increased emphasis, such as bolding or underlining. In addition to indications provided within sources such as metadata, matching and selection system 130 can also use audible indications of words spoken in audio content.
The matching and selection system 130 can continually improve its matching and selection algorithms and processes. For example, the matching and selection system 130 may determine that certain members of the audience of the Podrick Castlerock will stream each of his episodes of the podcast, but not necessarily listen to the entire podcast. The matching and selection system 130 may then assign an increased probability of matching the content over time for the particular content that the user may eventually hear it. As described above with respect to fig. 1, the matching and selection system 130 may use machine learning models and probabilistic models.
In some implementations, when the matching and selection system 130 determines or receives a message that there is insufficient transformed content to perform the matching and selection, the matching and selection system 130 may select a default or generic type of digital content to provide the media content to the user device.
In some implementations, the matching and selection system 130 may access profile information of the user device 106 and/or the content creator or publisher to enhance and/or improve the matching and selection process. For example, even though the user is generally interested in content related to animals, the matching and selection system 130 may access the user's profile information to determine that the user is not interested in seeing content related to cat food, but rather content related to dog food, because the user has indicated that they have dogs.
In some implementations, the matching and selection system 130 may give more weight to information matching the user profile. For example, the matching and selection system 130 may personalize the additional audio content based on the user's profile, the content within the media content being streamed, and the creator and/or publisher of the media content.
In some implementations, step E also includes running the selected digital content in text format through the text-to-speech system 120 to produce audio content to be presented with the media content 202.
The matching and selection system 130 may select the particular portion of the selected content that is most relevant to the criteria specified within the request. For example, the matching and selection system 130 may crop (crop) images or create video clips (clips) with content that is most relevant to the criteria specified within the request. Once the content has been selected as the winning content of the selection process performed by the matching and selection system 130, the matching and selection system 130 may edit the selected content according to the criteria and parameters specified within the request. For example, if the request specifies a display size of 1920 × 1080 pixels and the selected video has a display size of 1280 × 720 pixels, the matching and selection system 130 may scale the selected video to a display size of 1920 × 1080 pixels. The matching and selection system 130 can also crop an image or video to a smaller display size without altering the content quality. In some implementations, the matching and selection system 130 can downscale (downscale) the image or video to a smaller resolution, thereby efficiently improving the quality of the content.
Matching and selection system 130 may edit the content according to quality criteria specified within the request. For example, the matching and selection system 130 may determine whether to crop, zoom out, or zoom in content based on the resolution quality specified by the request.
In some implementations, this editing may occur in step F and be performed by content delivery system 140.
Flow continues with step F, where content delivery system 140 identifies an assigned time or slot during which the selected digital content is to be presented and provides the selected digital content to user device 106.
As previously described, the DCDS110 may detect and/or determine the slot in which the additional audio content should be placed. The content delivery system 140 may detect the appropriate timeliness and display location at which the selected digital content 206 should be placed within the original content 202 based on the temporal annotation within the original content 202. In some implementations, the temporal annotation indicates that the selected content 206 should be overlaid with the original content 202. In some implementations, the temporal annotation indicates that the selected content 206 should be inserted into the original content 202 such that the original content 202 is paused when the selected content 206 begins and the original content 202 is resumed when the selected content 206 ends.
The content delivery system 140 may create the new file 230 by placing the selected content 206 at the temporal and display location indicated by the temporal annotation in the original content 202. For example, the content delivery system 140 may superimpose the image of the selection of the general purpose microphone (to represent the podcast) 206 on the podcast episode audio file 202 to create a new file 230 that is a video file. Once the video file 230 is created, the video file 230 may be streamed to or viewed by a user of the user device 106.
The content delivery system 140 may edit the selected content 206, as described above with respect to step E. For example, the content delivery system 140 may visually edit the selected content 206 to create a cropped portion of the most relevant portion of the selected image 206. The content delivery system 140 may also edit the selected content 206 temporally to clip the selected video 206 to fit the required temporal requirements indicated by the temporal annotations included in the content 202. In some implementations, the content delivery system 140 may edit the selected content to separate the audio and visual data within the video file. For example, the content delivery system 140 may strip audio data from the video 206 with a selection of audio data and visual data to create a silent video and overlay the silent video with the audio data of the podcast episode 202.
The content delivery system 140 may determine where within the display area to display the selected visual content 202 and place the visual content in the appropriate location within the display area to create the new file 230. For example, if the selected content 206 is smaller than the full display area specified for the original content 202 and the temporal annotation included in the original content 202 specifies a particular placement for the selected content 206, the content delivery system 140 may place the selected content 206 in a specified location within the display area, at the specified temporal location, within a specified temporal period, to create the new file 230.
In some implementations, the content delivery system 140 of the DCDS110 may automatically determine and indicate the exclusion zone. The exclusion zone or the portion of the content that should not have additional content inserted or that should be omitted from the analysis during the zone may be defined by a user such as the content creator or publisher. The exclusion zone may be a temporal period or display area where the selected content 206 should not be integrated. For example, the content delivery system 140 may determine when a natural pause occurs and insert a content slot when creating the exclusion zone so that the content is not interrupted in the middle of a sentence or segment.
In some implementations, the content creator can specify exclusion zones where additional content should not be integrated. The content creator may specify a different exclusion zone for each original content 202 and may use, for example, temporal annotations that also specify visual locations within the display area. For example, the content creator may include temporal annotations indicating that particular portions of the video provided as the original content 202 include special messages and should not be overlaid or visually obscured for a period of time. In some implementations, the content creator may use other indicia including inaudible tones within the audio-only content. For example, the content creator may use a first inaudible tone to indicate the beginning of an off-limits zone and a second inaudible tone to indicate the end of the off-limits zone. In some implementations, the first inaudible tone and the second inaudible tone are the same tone. In some implementations, the first inaudible tone and the second inaudible tone have different properties, such as amplitude, frequency, and the like.
In some implementations, if the original content 202 includes visual data, the DCDS110 may replace the visual data or overlay the selected content 206 over part (or all) of the existing visual data. For example, the DCDS110 may automatically detect and improve the quality of the original content 202. In one example, the DCDS110 may determine the quality of the visual or audio data of the original content 202, and if the same visual or audio data is available from another source, the DCDS110 may replace the visual or audio data of the original content 202 to generate the new file 230. For example, if the content creator uses the same still image as a watermark, or a clip as an introductory acoustic digest in its podcast, and the DCDS110 detects that the watermark or acoustic digest has been qualitatively degraded from the watermark or acoustic digest used in previous uploads from the content creator, the DCDS110 may automatically replace the watermark or acoustic digest. In some implementations, the DCDS110 may only overlay better quality images.
In some implementations, the streamed media content may include additional content, such as sponsored content. The content delivery system 140 may create exclusion zones during the sponsorship of content to avoid confusing the audience or viewers. For example, Podrick Castlerock may speak some sentences about Generic CoolCar brand M during "Guest Leuy Hamster talks about new advances in safety with F1 and his dog" because the episode is sponsored by Generic CoolCar brand M. The content delivery system 140 may detect these several sentences and create a forbidden zone so that no additional audio content is inserted into the podlick Castlerock sponsored sales pitch (sponsored) on GenericCoolCar brand M.
The content delivery system 140 can also detect additional content within the media content being streamed based on the topic of the content. For example, a Podrick Castlerock might push example brand phones within his car fan podcast to a cool phone with a super-complimentary camera application. The content delivery system 140 may determine that the podcast of the Podrick Castlerock is about an automobile, and that his promotion of the example brand phone identifies a particular brand and is about a smart phone, so the topic is considered completely unrelated to automobiles. Content delivery system 140 may then determine that his promotion of the example brand phone should be within the exclusion zone. Alternatively or additionally, a exclusion zone may be set during the discussion of particular content so that the original content related to the discussion of the topic is not disturbed or segmented.
The content delivery system 140 may assign exclusion zones based on entities mentioned within the media content. For example, the content delivery system 140 may detect a product name within the media content and omit from the textual content of the media content any words spoken within a specified amount of time of the location of the product name in the second audio. In one example, content delivery system 140 may detect that the butter that Podrick Castlerock mentions brand Y is a very stick of butter that all cool car drivers use to eat with their toast, and may determine any words spoken within 10 seconds of saying "brand Y". The amount of time before and after speaking a particular word need not be the same, and in some implementations, the content delivery system 140 may detect whether the entity name is spoken at the beginning, middle, or end of the promotional segment to adjust the boundaries of the exclusion zone.
The content delivery system 140 also performs delivery of the selected digital content 206 to the user device 106 in response to the request 108. The content delivery system 140 may provide the selected digital content 206 in the reply 114.
In some implementations, the DCDS110 may serve as a medium for delivering streaming media content to the user device 106. For example, the DCDS110 may receive media content, insert additional selected digital content into the media content, and stream the media content to the user device 106 along with the selected digital content such that the user device 106 receives a continuous stream of content.
In some implementations, the system 100 as described with respect to fig. 1-2 can pause the streaming of the media content, stream selected digital content from a different source during a designated content slot, and then continue the streaming of the media content to the user device 106.
The content delivery system 140 may edit the original content 202, the selected content 206, and/or the new file 230 before delivering the reply 114 to the user device 106. For example, if the user device 106 is unable to handle the quality or size of the reply 114, the content delivery system 140 may adjust the transcoding of the new file 230 prior to delivery to the user device 106.
The flow ends with step G, where the DCDS110 provides a reply to the user equipment. For example, the DCDS110 provides the new file 230 within the reply 114 to the user device 106, as described above with respect to fig. 1.
Fig. 3 depicts an example content transformation process 300 for improved content matching and selection in the example environment of fig. 1-2. The operations of process 300 are performed by various components of system 100 and as described with respect to process 200. For example, the operations of the process 300 may be performed by the DCDS110 in communication with the user equipment 106.
In phase 1, the DCDS110 receives the original content 302. In this particular example, the original content 302 is an audio-only file of a podcast recorded by the moderator Podrick Castlerock. The original content 302 may be similar to the original content 202 as described with respect to fig. 2.
In stage 2, the DCDS110 parses the audio data 302 into segments 310 and 320. For example, the text-to-speech system 120 may perform parsing of the audio data 302. The text-to-speech system 120 may automatically detect natural pauses, changes in topics, changes in speakers, etc. to determine where segments begin and end. In some implementations, the original content 302 includes a marker or temporal annotation indicating the segment assigned by the content creator.
In this example, the DCDS110 parses the audio data 302 into segments 310 and 320 based on detecting the topic of each segment. The text-to-speech system 120 performs the analysis, as described above with respect to fig. 2, on the subject of the audio data 302 and the technical nature of the audio data 302. For example, the text-to-speech system 120 may detect temporal boundaries indicated by temporal annotations, entities or topics mentioned in the audio data 302, and so on, and identify temporal locations within the audio data 302 and/or display locations where additional content may be integrated with the audio data 302.
In some implementations, the DCDS110 may group segments of the original content 302 using timestamps of the content. For example, the DCDS110 may determine that particular segments are related to each other even though their topics appear quite different due to the temporal proximity of the segments to each other. In some implementations, the DCDS110 may use groups of segments to reduce the processing resources required to select additional content for a particular segment. For example, the DCDS110 may select a particular image to be displayed along with a particular segment of audio content, and may use the same image or related images for related segments of the original content. In other implementations, the DCDS110 may group the content of various other portions of the original content based on analysis performed by the matching and selection system 130 as described above with respect to fig. 2.
For example, the text-to-speech system 120 detects a mention of bulldog Roscoe in the audio data 302. The text-to-speech system 120 may then determine that the topic is bulldog Roscoe for a particular length of audio data 302, and may create a segment 310. Further, the text-to-speech system 120 detected that a modern F1 car was also mentioned in the audio data 302. The text-to-speech system 120 may then determine that the topic is a modern F1 car for a particular length of audio data 302.
In some implementations, stage 2 is performed when a content creator or content provider uploads content to a system, such as the DCDS 110. For example, when uploading content, the DCDS110 may perform processes to generate and detect annotations and perform content analysis. In some implementations, stage 2 is performed when the user device 106 requests content. For example, the DCDS110 may perform processing in real-time to generate and detect annotations and perform content analysis as the original content 302 is streamed to the user device 106.
In some implementations, stage 2 is performed as a batch process of several original content items uploaded within a particular time period.
In stage 3, the DCDS110 matches the characteristics and parameters of the parsed segments with candidate content (such as image or video data). For example, once the audio data 302 has been parsed into segments 310 and 320, the matching and selection system 130 may match the characteristics and parameters of the segments 310 and 320 to candidate digital components in the digital component database 112, candidate other content in the electronic document server 104, or candidate digital components or other content in the content analysis and mapping database 116 or other sources.
In some implementations, stage 3 is performed in response to a triggering event, such as a request from a user device to access the original content. For example, stage 3 may be performed in response to a request from the user device 106 to access the original content 302. In this particular example, the matching and selection system 130 performs a selection process on the candidate content and selects an image 330 of a bulldog to be displayed with the segment 310. The matching and selection system 130 also performs a selection process on the candidate content and selects a video 340 of a modern F1 car.
In some implementations, the selection process may yield different results based on the time at which the selection process is triggered. For example, a candidate content item for "modern F1 car" in 2012 would be very different from a candidate content item for "modern F1 car" in 2020.
Thus, the system as described in fig. 1-3 allows dynamic personalization of content not only for the user's interest, but also for the current time, so that the relevance of the selected and displayed additional content to the original content can be constantly refreshed and customized for each user each time content is requested.
In stage 4, the DCDS110 combines the original content and the selected additional content to generate new content to provide to the requesting device. For example, as described above with respect to fig. 1-2, content delivery system 140 may combine segments 310 and 320 with additional content 330 and 340, respectively, for delivery to user device 106. In some implementations, the content delivery system 140 creates a new combined file. In some implementations, the content delivery system 140 may provide two separate data streams to the user device 106.
FIG. 4 is a flow diagram of an example process 400 including matching and selected content transformations. In some implementations, process 400 may be performed by one or more systems. The process 400 may be implemented, for example, by the DCDS110 and/or the user equipment 106 of fig. 1-3. In some implementations, process 400 may be implemented as instructions stored on a computer-readable medium, which may be non-transitory, and when executed by one or more servers, the instructions may cause the one or more servers to perform the operations of process 400.
The process 400 continues with transforming the audio data into textual content (404). For example, the text-to-speech system 120 can transform a podcast 202 to be streamed to the user device 106 into textual content 204. In some implementations, transforming the second audio includes detecting a spoken word in the second audio, analyzing one or more audio characteristics of the second audio, adjusting importance of one or more words from among the spoken words based on the analysis of the one or more audio characteristics, generating textual content representing the spoken word, and assigning the adjusted importance of the one or more words to the textual content representing the one or more words. For example, the text-to-speech system 120 may detect a spoken word in the podcast 202, analyze one or more audio characteristics of the podcast 202, such as speech emphasis, generate a text representation of the spoken word, such as text data 204, and assign and/or adjust a weight of one or more words within the text data 204.
In some implementations, analyzing the one or more audio characteristics of the second audio includes detecting an audible indication of emphasis of the one or more words. For example, the emphasized audible indication may include a repetition of one or more words, or a volume or tone of a voice of a speaker of a word. In some implementations, analyzing the one or more audio characteristics includes distinguishing between a first portion of a spoken word spoken by a moderator utterance in the second audio and a second portion of the spoken word spoken by a guest utterance in the second audio. For example, the text-to-speech system 120 may assign portions of the spoken word spoken by the moderator Podrick Castlerock and portions of the spoken word spoken by the guest Leuy Hamster.
In some implementations, adjusting the importance of the one or more words includes increasing the importance of the one or more words based on the emphasized audible indication. For example, the text-to-speech system 120 may increase the weight of words based on an audible indication of emphasis within the model used to match and select the digital content. In some implementations, adjusting the importance of one or more words includes increasing the importance of a first portion of the spoken word relative to the importance of a second portion of the spoken word. For example, the text-to-speech system 120 may increase the weight of words spoken by the moderator Podrick Castlerock relative to the weight of words spoken by the guest Leuy Hamster.
In some implementations, determining the context of the second audio includes determining the topic of the first audio based on the adjusted importance assigned to the one or more words of the textual content representing the one or more words. For example, matching and selection system 130 may determine the topic of the audio clip based on the weights of the words associated with the topic of the audio clip that promoted the dog toy.
In some implementations, the importance or weight of a particular word may be changed based on emphasis, location within a detected paragraph of media content, term frequency, and other characteristics. In some implementations, the weight of the words may be increased due to a match between the words in the text description of the media content and the detected audio. In some implementations, the weight of a word may be adjusted based on whether the host or guest uttered the word.
The process 400 continues with determining, based on the search of the searchable database, that the textual content of the audio data matches the characteristics of the visual data in the searchable database (406). For example, the matching and selection system 130 may determine that the textual content of the podcast episode 202 matches characteristics of one or more content items having visual data in a searchable database, such as the content analysis and mapping database 116, the e-mail server 104, and/or the digital component database 112. The matching and selection system 130 may then select a content item having visual data, such as an image 330.
In some implementations, the media content may be delivered separately from the visual data, and the media content may be paused while the visual data is provided to the user device 106 and restarted at the end of the visual data.
In some implementations, the process 400 includes detecting an annotation located at a particular temporal location within the media content, and wherein integrating the visual data with the media content to create the enhanced content stream includes overlaying the visual data with the media content at the particular temporal location within the media content based on the annotation. For example, the text-to-speech system 120 can detect a temporal annotation at a particular temporal location within the podcast 202. Integrating the visual data (in this example, the image 330) to create the enhanced content stream includes overlaying the image 330 with the podcast 202 to create a new file 230 that is a video file.
In some implementations, the annotation specifies one or more visual data characteristics. For example, the temporal annotation can specify a resolution size of the image 330 to be overlaid with the podcast 202. In some implementations, integrating the visual data with the media content to create the enhanced content stream further includes editing the visual data based on one or more visual data characteristics. For example, the matching and selection system 130 or the content delivery system 140 may scale the image 330 according to the resolution specified by the temporal annotation.
The process 400 continues with integrating visual data with the media content having the matching characteristic to create an enhanced content stream in response to determining that the textual content of the audio data matches the characteristic of the visual data (408). For example, the content delivery system 140 can overlay the image 330 (or video) with the podcast 202 to create an enhanced content stream that includes multiple types of media. Accordingly, the content delivery system 140 creates a new media item that includes at least one different type of data by converting a particular type of media item to the new media item. In this particular example, the content delivery system 140 creates a new media item 230 that includes video data such that the content of the audio-only podcast 202 can be distributed through a content distribution platform (example video hosting platform site) that requires video data.
In some implementations, the process includes determining a first context of the media content based on the textual content of the audio data and determining a second context of the visual data based on the characteristics of the visual data in the searchable database, and wherein determining that the textual content of the audio data matches the characteristics of the visual data in the searchable database includes determining that the first context matches the second context. The context may be a topic or some other information that may be determined from text obtained from the podcast. For example, after determining that the topic match of the podcast 202 "guest Leuy Hamster talks about a new advance on F1 security and his dog" depicts the context of the image 330 of a bullfight dog, the content delivery system 140 may overlay the image 330 with the podcast 202 to create an enhanced content stream.
In some implementations, the process 400 includes identifying a particular temporal location within the media content based on a first context of the media content, and integrating the visual data with the media content to create the enhanced content stream includes superimposing the visual data with the media content at the particular temporal location within the media content. For example, the DCDS110 may parse the podcast 202 into segments and determine the context of each segment. The DCDS110 can then integrate the image 330 with the podcast 202 at the temporal location determined by the DCDS 110.
In some implementations, the process includes identifying a forbidden zone (exclusion zone) of the media content. For example, a temporal annotation may specify that visual data cannot be overlaid with media content at a particular temporal location.
The process 400 continues with the distribution of the enhanced content stream by the content distribution platform requiring that the media content include video content (410). For example, the content delivery system 140 may distribute the new content 230 through a content distribution system example video hosting platform site that requires video data. The process 400 allows content to be distributed across different platforms that require different types of data by transforming the content into new content without diminishing the quality of the original content.
In some implementations, the content delivery system 140 may determine whether to integrate different qualities of visual data with the original media content when distributing the content stream. For example, the content delivery system 140 may determine whether to integrate visual data or integrate visual data at a higher or lower resolution based on available network resources, such as bandwidth, and device capabilities, such as processing capabilities of the user device to which the enhanced content stream is to be delivered. In one example, the content delivery system 140 may determine that there is low bandwidth available and may integrate lower resolution visual data with the media content to create an enhanced content stream.
In some implementations, the content delivery system 140 may replace existing data within the original media content to improve the quality of the content or to adapt the content to the capabilities and/or available network and processing resources of the receiving device. For example, the content delivery system 140 may determine, for the original media content item, that the original media content item includes data that exists in a searchable database, such as the digital component database 112, the content analysis and mapping database 116, and/or the e-document server 104. The content delivery system 140 can replace data in the original media content item that exists in the searchable database at a different resolution and automatically replace content in the original media content item to improve content quality. For example, the content delivery system 140 may determine that image data in the original media content exists in the searchable database at a higher resolution and replace the image content with the higher resolution image data available in the searchable database.
FIG. 5 is a block diagram of an example computer system 500 that may be used to perform the operations described above. The system 500 includes a processor 510, a memory 520, a storage device 530, and an input/output device 540. Each of the components 510, 520, 530, and 540 may be interconnected, for example, using a system bus 550. Processor 510 is capable of processing instructions for execution within system 500. In one implementation, the processor 510 is a single-threaded processor. In another implementation, the processor 510 is a multi-threaded processor. The processor 510 is capable of processing instructions stored in the memory 520 or on the storage device 530.
The input/output device 540 provides input/output operations for the system 500. In one implementation, the input/output device 540 may include one or more of a network interface device such as an Ethernet card, a serial communication device such as an RS-232 port, and/or a wireless interface device such as an 802.11 card. In another implementation, the input/output devices can include driver devices configured to receive input data and send output data to other input/output devices (e.g., keyboard, printer, and display devices 560). However, other implementations may also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and so forth.
Although an example processing system has been described in fig. 5, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, including the structures disclosed in this specification and their structural equivalents, or in computer software, firmware, or hardware, or in combinations of one or more of them.
The media does not necessarily correspond to a file. The media may be stored in a portion of a file that stores (hold) other documents, in a single file dedicated to the document in question, or in multiple coordinated files.
In the context of collecting and/or using information about a user through the techniques discussed throughout this document, a user (such as an end user, a content generator or content provider, among other types of users) may be provided with controls that allow the user to make a selection as to whether to send content or communications from a server to the user, and whether and when the systems, programs, or features described herein may enable the collection of user information (e.g., information about the user's social network, social actions or activities, profession, the user's preferences, or the user's current location). Further, certain data may be processed in one or more ways before being stored or used, such that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information of the user cannot be determined, or the geographic location of the user may be generalized to a location (such as a city, zip code, or state level) where location information is obtained such that a particular location of the user cannot be determined. Thus, the user may have control over what information is collected about the user, how the information is used, and what information is provided to the user.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage medium (or a single medium) for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal, such as a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by the data processing apparatus. The computer storage medium may be or be included in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Further, although the computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be included in one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
The operations described in this specification may be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The term "data processing apparatus" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or a plurality or combination of the foregoing. An apparatus may comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may correspond to a file in a file system, but does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that stores other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with the instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on the user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), an internetwork (e.g., the internet), and a peer-to-peer network (e.g., an ad hoc peer-to-peer network).
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data (e.g., HTML pages) to the client device (e.g., for purposes of displaying data to a user interacting with the client device and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) may be received at the server from the client device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some scenarios, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some implementations, multitasking and parallel processing may be advantageous.
Claims (20)
1. A method, comprising:
receiving media content including audio data for distribution by a content distribution platform that requires the media content to include video content;
converting the audio data into text content;
based on the search of the searchable database, determining characteristics of textual content of the audio data matching visual data in the searchable database;
integrating the visual data with the media content having the matching characteristics to create an enhanced content stream in response to determining that the textual content of the audio data matches the characteristics of the visual data; and
the enhanced content stream is distributed through a content distribution platform that requires media content to include video content.
2. The method of claim 1, further comprising:
detecting an annotation located at a particular temporal location within the media content; and
wherein integrating the visual data with the media content to create the enhanced content stream comprises overlaying the visual data with the media content at the particular temporal location within the media content based on the annotation.
3. The method of claim 2, wherein the annotation specifies one or more visual data characteristics.
4. The method of claim 2 or 3, wherein integrating visual data with media content to create an enhanced content stream further comprises editing the visual data based on the one or more visual data characteristics.
5. The method of any of claims 2 to 4, wherein the annotation specifies that visual data cannot be overlaid with media content at a particular temporal location.
6. The method of any preceding claim, further comprising:
determining a first context of the media content based on the textual content of the audio data;
determining a second context for the visual data based on the characteristic of the visual data in the searchable database; and
wherein determining that the textual content of the audio data matches a characteristic of the visual data in the searchable database comprises determining that a first context matches a second context.
7. The method of claim 6, further comprising:
identifying a particular temporal location within the media content based on a first context of the media content; and
wherein integrating the visual data with the media content to create the enhanced content stream comprises overlaying the visual data with the media content at a particular temporal location within the media content.
8. A system, comprising:
one or more processors; and
one or more memory elements comprising instructions that when executed cause one or more processors to perform operations comprising:
receiving media content including audio data for distribution by a content distribution platform that requires the media content to include video content;
converting the audio data into text content;
based on the search of the searchable database, determining characteristics of textual content of the audio data matching visual data in the searchable database;
integrating the visual data with the media content having the matching characteristic to create an enhanced content stream in response to determining that the textual content of the audio data matches the characteristic of the visual data; and
enhanced content streams are distributed through a content distribution platform that requires media content, including video content.
9. The system of claim 8, the operations further comprising:
detecting an annotation located at a particular temporal location within the media content; and
wherein integrating the visual data with the media content to create the enhanced content stream comprises overlaying the visual data with the media content at a particular temporal location within the media content based on the annotation.
10. The system of claim 9, wherein the annotation specifies one or more visual data characteristics.
11. The system of claim 9 or 10, wherein integrating visual data with media content to create an enhanced content stream further comprises editing the visual data based on the one or more visual data characteristics.
12. The system of any of claims 9 to 11, wherein the annotation specifies that visual data cannot be overlaid with media content at a particular temporal location.
13. The system of any preceding claim, the operations further comprising:
determining a first context of the media content based on the textual content of the audio data;
determining a second context for the visual data based on the characteristic of the visual data in the searchable database; and
wherein determining that the textual content of the audio data matches a characteristic of the visual data in the searchable database comprises determining that a first context matches a second context.
14. The system of claim 13, the operations further comprising:
identifying a particular temporal location within the media content based on a first context of the media content; and
wherein integrating the visual data with the media content to create the enhanced content stream comprises overlaying the visual data with the media content at a particular temporal location within the media content.
15. A computer storage medium encoded with instructions that, when executed by a distributed computing system, cause the distributed computing system to perform operations comprising:
receiving media content including audio data for distribution by a content distribution platform that requires the media content to include video content;
converting the audio data into text content;
based on the search of the searchable database, determining characteristics of textual content of the audio data matching visual data in the searchable database;
integrating the visual data with the media content having the matching characteristic to create an enhanced content stream in response to determining that the textual content of the audio data matches the characteristic of the visual data; and
the enhanced content stream is distributed through a content distribution platform that requires media content to include video content.
16. The computer storage medium of claim 15, the operations further comprising:
detecting an annotation located at a particular temporal location within the media content; and
wherein integrating the visual data with the media content to create the enhanced content stream comprises overlaying the visual data with the media content at a particular temporal location within the media content based on the annotation.
17. The computer storage medium of claim 16, wherein the annotation specifies one or more visual data characteristics.
18. The computer storage medium of claim 16 or 17, wherein integrating visual data with media content to create an enhanced content stream further comprises editing visual data based on the one or more visual data characteristics.
19. The computer storage medium of any of claims 16 to 18, wherein the annotation specifies that visual data cannot be overlaid with media content at a particular temporal location.
20. The computer storage medium of any preceding claim, the operations further comprising:
determining a first context of the media content based on the textual content of the audio data;
determining a second context for the visual data based on the characteristic of the visual data in the searchable database; and
wherein determining that the textual content of the audio data matches a characteristic of the visual data in the searchable database comprises determining that a first context matches a second context.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/065559 WO2022132151A1 (en) | 2020-12-17 | 2020-12-17 | Automatically enhancing streaming media using content transformation |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114945912A true CN114945912A (en) | 2022-08-26 |
Family
ID=74186887
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080046312.XA Pending CN114945912A (en) | 2020-12-17 | 2020-12-17 | Automatic enhancement of streaming media using content transformation |
Country Status (4)
Country | Link |
---|---|
US (1) | US20220398276A1 (en) |
EP (1) | EP4042292A1 (en) |
CN (1) | CN114945912A (en) |
WO (1) | WO2022132151A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11601837B1 (en) * | 2021-10-12 | 2023-03-07 | T-Mobile Innovations Llc | Adaptive distribution of content |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9262539B2 (en) * | 2011-04-29 | 2016-02-16 | Ufaceme, Inc. | Mobile device and system for recording, reviewing, and analyzing human relationship |
US10755744B2 (en) * | 2014-06-06 | 2020-08-25 | Fuji Xerox Co., Ltd. | Systems and methods for direct video retouching for text, strokes and images |
US10847149B1 (en) * | 2017-09-01 | 2020-11-24 | Amazon Technologies, Inc. | Speech-based attention span for voice user interface |
US11314475B2 (en) * | 2018-11-21 | 2022-04-26 | Kyndryl, Inc. | Customizing content delivery through cognitive analysis |
US20200321005A1 (en) * | 2019-04-05 | 2020-10-08 | Adori Labs, Inc. | Context-based enhancement of audio content |
-
2020
- 2020-12-17 EP EP20842427.5A patent/EP4042292A1/en active Pending
- 2020-12-17 WO PCT/US2020/065559 patent/WO2022132151A1/en unknown
- 2020-12-17 CN CN202080046312.XA patent/CN114945912A/en active Pending
- 2020-12-17 US US17/616,866 patent/US20220398276A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
EP4042292A1 (en) | 2022-08-17 |
US20220398276A1 (en) | 2022-12-15 |
WO2022132151A1 (en) | 2022-06-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11315546B2 (en) | Computerized system and method for formatted transcription of multimedia content | |
US8819728B2 (en) | Topic to social media identity correlation | |
US9251532B2 (en) | Method and apparatus for providing search capability and targeted advertising for audio, image, and video content over the internet | |
US20200321005A1 (en) | Context-based enhancement of audio content | |
US11003720B1 (en) | Relevance-ordered message search | |
US20120265819A1 (en) | Methods and apparatus for recognizing and acting upon user intentions expressed in on-line conversations and similar environments | |
JP7171911B2 (en) | Generate interactive audio tracks from visual content | |
US11908462B2 (en) | Adaptive interface in a voice-activated network | |
CN106888154B (en) | Music sharing method and system | |
CN111506794A (en) | Rumor management method and device based on machine learning | |
US20200342856A1 (en) | Multi-modal interface in a voice-activated network | |
CN111782919A (en) | Online document processing method and device, computer equipment and storage medium | |
US20220398276A1 (en) | Automatically enhancing streaming media using content transformation | |
US20230244716A1 (en) | Transforming data from streaming media | |
US11269940B1 (en) | Related content searching | |
US20230410793A1 (en) | Systems and methods for media segmentation | |
EP3854037B1 (en) | Dynamic insertion of supplemental audio content into audio recordings at request time | |
US20240005176A1 (en) | Method and process for checking media content veracity | |
US20240143150A1 (en) | Methods and systems for populating data for content item | |
US11983217B2 (en) | Responding to queries with voice recordings | |
WO2023003537A1 (en) | Bit vector-based content matching for third-party digital assistant actions | |
US20230009983A1 (en) | Responding to queries with voice recordings |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |