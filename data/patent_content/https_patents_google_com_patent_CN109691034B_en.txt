CN109691034B - Robot interaction - Google Patents
Robot interaction Download PDFInfo
- Publication number
- CN109691034B CN109691034B CN201780009158.7A CN201780009158A CN109691034B CN 109691034 B CN109691034 B CN 109691034B CN 201780009158 A CN201780009158 A CN 201780009158A CN 109691034 B CN109691034 B CN 109691034B
- Authority
- CN
- China
- Prior art keywords
- user
- messaging application
- robot
- message
- computer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/02—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail using automatic reactions or user delegation, e.g. automatic replies or chatbot-generated messages
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/04—Real-time or near real-time messaging, e.g. instant messaging [IM]
- H04L51/046—Interoperability with other network applications or services
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/07—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail characterised by the inclusion of specific contents
- H04L51/08—Annexed information, e.g. attachments
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/07—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail characterised by the inclusion of specific contents
- H04L51/10—Multimedia information
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/21—Monitoring or handling of messages
- H04L51/212—Monitoring or handling of messages using filtering or selective blocking
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/52—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail for supporting social networking services
Abstract
A system, method, and computer-readable medium for robotic interaction are described. For example, some embodiments may include a computer-implemented method that includes providing a messaging application on a first computing device associated with a first user to enable communication between the first user and at least one other user. The method also includes detecting a user request on the messaging application and programmatically determining that an action responsive to the user request requires sharing of data associated with the first user. The method may further include causing a permission interface to be rendered in the messaging application on the first computing device, the permission interface enabling the first user to approve or prohibit sharing of data associated with the first user; and upon receiving user input from the first user indicating approval to share data associated with the first user, accessing and sharing data associated with the first user within the messaging application in response to the user request.
Description
RELATED APPLICATIONS
This application claims the benefit of U.S. application No. 62/397,318 entitled "robotic interaction" filed on 9/20/2016, which is hereby incorporated by reference in its entirety.
Background
Users use messaging services for messaging sessions, e.g., chat, instant messaging, etc. Any user device may be used for the messaging session, for example, using a computer, mobile device, wearable device, and the like. Automated assistance (e.g., via a robot or other automated assistance application) with message conversations or tasks may help to improve efficiency as users use the message application to conduct more conversations and to perform more tasks. While automation may help make messaging more efficient for users, it may require management of privacy and permissions related to when and how the messaging bot accesses user information, and what user information is allowed to be accessed by the messaging bot and/or shared to other users.
The background description herein is intended to generally present an environment for the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.
Disclosure of Invention
Some embodiments may include a computer-implemented method. The method may include providing a messaging application on a first computing device associated with a first user to enable communication between the first user and at least one other user. The method may also include detecting a user request on the messaging application and programmatically determining that an operation in response to the user request requires sharing of data associated with the first user. The method may further include causing a permission interface to be rendered in the messaging application on the first computing device, the permission interface enabling the first user to approve or prohibit sharing of data associated with the first user; and upon receiving user input from the first user indicating approval to share data associated with the first user, accessing and sharing data associated with the first user within the messaging application in response to the user request.
In some embodiments, the first user is a human user and at least one of the other users is an auxiliary agent. Detecting the user request includes: analyzing, by a secondary proxy, one or more messages received in the messaging application, wherein the messages are sent from the first user to the secondary proxy. The one or more messages include one or more of a text message, a multimedia message, and a command to the secondary proxy.
The method may also include automatically invoking the auxiliary agent based on one or more actions of the first user. Invoking the auxiliary agent in response to one or more of: a special character command, a user selection of a user interface tagging element associated with the auxiliary agent, a user selection of an avatar corresponding to the auxiliary agent; and user selection of an auto-reply suggestion provided by the secondary agent.
In some implementations, the first user is a human user and at least one of the other users includes a second human user that is different from the first user and associated with a second computing device, and wherein a permission interface is rendered in a messaging application on a first computing device associated with the first user and the permission interface is not displayed on the second computing device associated with the second human user.
The first user is a human user and the at least one other user includes a second human user and an auxiliary agent, the second human user being different from the first user, and wherein the user request is received from the first computing device associated with the first user, the method further comprising, in response to the user request, initiating a separate session in the messaging application, wherein the separate session includes the first user and the auxiliary agent and does not include the second human user.
The method may further comprise providing more than one suggestion in the first messaging application based on input from at least one of said first user and at least one of said other users. The method may further comprise causing the one or more suggestions to be displayed in a messaging application. The one or more suggestions are rendered as suggestion elements that, when selected by the first user, cause details of the corresponding suggestion to be displayed.
Some embodiments may include a computer-implemented method that includes programmatically detecting, in a messaging application, user input corresponding to a selection of a previously displayed confirmation message from a secondary agent, wherein the previously displayed confirmation message is associated with one or more previously displayed suggested auto-reply responses. The method may further include causing the previously displayed suggested auto-reply responses to be displayed and receiving an indication that a user selected a given one of the more than one previously displayed suggested auto-reply responses. The method may further include sending the given one of the one or more previously displayed suggested auto-reply responses to the secondary proxy and a second user participating in a conversation within the messaging application.
The method may further include invoking the assistance agent in response to the user input. In some implementations, the session includes the user and at least one other user, wherein the user is a first human user and at least one other user includes a second human user different from the first human user, the first human user associated with a first computing device and the second human user associated with a second computing device, and wherein the previously displayed message and the one or more suggested auto-reply responses are initially rendered in a messaging application on a first computing device associated with the first human user, and wherein providing the one or more suggested auto-reply responses to one or more of the assistant agents and the session within the messaging application includes: providing the one or more suggested auto-reply responses in the messaging application.
Some embodiments may include a system comprising one or more hardware processors coupled to a non-transitory computer readable medium having stored thereon software instructions that, when executed by one or more processors, cause the one or more processors to perform operations. The operations may include: a user request is received from a messaging application on a first computing device associated with a first user, the messaging application configured to enable communication between the first user and at least one other user, and programmatically determine that an operation in response to the user request requires sharing of data associated with the first user. The operations may further include causing a permission interface to be rendered in the messaging application on the first computing device, the permission interface enabling the first user to approve or prohibit sharing of the data associated with the first user; and upon receiving user input from the first user approving sharing of the data associated with the first user, accessing the data associated with the first user and sharing the data associated with the first user within the messaging application in response to the user request.
In some embodiments, the first user is a human user and at least one of the other users is an auxiliary agent. Detecting the user request may include: analyzing, at a secondary proxy, more than one message received in the messaging application from the first user to the secondary proxy. The first user is a human user and at least one of the other users includes a second human user that is different from the first user and associated with a second computing device, and wherein a permission interface is rendered in the messaging application on the first computing device associated with the first user and the permission interface is not displayed on the second computing device associated with the second human user.
In some implementations, the first user is a human user and at least one of the other users includes a second human user and an assistance agent, the second human user being different from the first user, and wherein a user request is received from a first computing device associated with the first user, the operations further including initiating a separate session in a messaging application in response to the user request, wherein the separate session includes the first user and the assistance agent and does not include the second human user.
The operations may further include providing one or more suggestions to the first messaging application based on input from at least one of the first user and at least one of the other users.
Drawings
FIG. 1 illustrates a block diagram of an exemplary environment in which messages are exchanged between a user and a bot, according to some embodiments.
FIG. 2 is a schematic diagram of an exemplary robot invocation event, according to some embodiments.
FIG. 3 is a flow diagram of an exemplary method of displaying robot invocation, according to some embodiments.
FIG. 4 is a schematic diagram of a user interface displaying an exemplary robot recognition and invocation element, according to some embodiments.
FIG. 5 is a flow diagram of an exemplary method of managing user privacy during robot interaction, according to some embodiments.
FIG. 6 is a schematic diagram of an exemplary user interface for managing user privacy during robotic interaction, according to some embodiments.
Fig. 7 is a flow diagram of an exemplary method for recalling previous robot suggestions, according to some embodiments.
8-12 are schematic diagrams of exemplary user interfaces displaying an exemplary prior robot proposal recall process, according to some embodiments.
FIG. 13 is a flowchart illustrating an exemplary process for managing licenses, according to some embodiments.
FIG. 14 is a schematic diagram showing an exemplary user interface for a given robot type as a theme, according to some embodiments.
FIG. 15 is a schematic diagram showing an exemplary user interface for a given robot type as a theme, according to some embodiments.
FIG. 16 is a schematic diagram of an exemplary computing device configured for robotic interaction, according to some embodiments.
Detailed Description
One or more embodiments described herein relate generally to user interaction with an auxiliary agent (or bot), and in particular to bot recognition, user privacy management for a message application bot, and/or recall of previous bot suggestions.
FIG. 1 illustrates a block diagram of an exemplary environment 100 for providing a messaging service that enables automated assisted brokering, and in some embodiments, automated assisted brokering, e.g., robotics. The exemplary environment 100 includes: a message server 101, one or more client devices 115a, 115n, a server 135, and a network 140. The users 125a-125n may be associated with respective client devices 115a, 115 n. The server 135 may be a third party server, such as a third party server controlled by a party other than the party providing the messaging service. In various embodiments, the server 135 may implement a robotic service, as described in detail below. In some implementations, environment 100 may not include more than one server or device shown in FIG. 1, or may include other servers or devices not shown in FIG. 1. In fig. 1 and the remaining figures, the letters following the reference numbers, such as fig. 115a, indicate references to elements having a particular reference number. Reference numbers in the text without subsequent letters, such as "115", indicate a general reference to an implementation of the element having that reference number.
In the illustrated embodiment, the message server 101, the client device 115, and the server 135 are communicatively coupled via a network 140. In various embodiments, network 140 may be of a conventional type, wired or wireless, and may have a configuration including a star configuration, a token ring configurationOr many different configurations of other configurations. Further, network 140 may include a Local Area Network (LAN), a Wide Area Network (WAN) (e.g., the internet), and/or other interconnected data paths over which multiple devices may communicate. In some implementations, the network 140 may be a peer-to-peer network. The network 140 may also be coupled to or may comprise a portion of a telecommunications network that transmits data in a variety of different communication protocols. In some embodiments, network 140 includes
The message server 101 may include a processor, memory, and network communication capabilities. In some embodiments, the message server 101 is a hardware server. In some embodiments, the message server 101 may be implanted in a virtual environment, for example, the message server 101 may be a virtual machine executing on a hardware server, which may include more than one other virtual machine. Message server 101 is communicatively coupled to network 140 via signal line 102. The signal line 102 may be a wired connection, such as ethernet, coaxial cable, fiber optic cable, etc., or a wireless connection, such as Wi-Fi, bluetooth, or other wireless technology. In some embodiments, the message server 101 sends data to and receives data from one or more client devices 115a-115n, the server 135, and the robot 113 over the network 140. In some implementations, the message server 101 can include a message application 103a that provides client functionality to enable users (e.g., any of the users 125) to exchange messages with other users and/or bots. The messaging application 103 may be a server application, a server module of a client-server application, or a distributed application (e.g., having a corresponding client messaging application 103b on more than one client device 115).
The message server 101 may also include a database 199, which database 199 may store messages exchanged through the message server 101, configuration and/or data of one or more robots, user data relating to one or more users 125, all of which store such data based on explicit permissions from the respective users. In some embodiments, the message server 101 may include more than one secondary agent, e.g., robots 107a and 111. In other embodiments, the secondary proxy may be implemented on the client device 115a-n instead of on the message server 101.
The messaging application 103 may be code and program that is operated by a processor to enable message exchange between a user 125 and one or more of the robots 105, 107a, 107b, 109b, 111, and 113. In some embodiments, messaging application 103 may be implemented using hardware including a Field Programmable Gate Array (FPGA) or an Application Specific Integrated Circuit (ASIC). In some embodiments, messaging application 103 may be implemented using a combination of hardware and software.
In various embodiments, the database 199 may store messages exchanged between more than one client device 115 as each user associated with the client device 115 provides permission to the message store. In some implementations, the database 199 may store messages exchanged between more than one client device 115 and more than one bot implemented on different devices, e.g., another client device, the message server 101 and the server 135, etc., when the respective user associated with the client device 115 provides permission to the message store. In embodiments where more than one user does not provide permission, messages received and sent by those users are not stored.
In some implementations, for example, the message may be encrypted so that only the sender and recipient of the message may view the encrypted message. In some embodiments, the message is stored. In some embodiments, database 199 may further store data and/or configurations of more than one robot, such as robot 107a, robot 111, etc. In some implementations, when a user 125 provides permission for storage of user data (e.g., social network data, contact information, images, etc.), the database 119 may also store user data related to the individual user 125 providing such permission.
In some implementations, the messaging application 103a/103b can provide a user interface that enables the user 125 to create a new robot. In these embodiments, the messaging applications 103a/103b may include functionality that enables user-created bots to be included in conversations between users of the messaging applications 103a/103 b.
The client device 115 may be a computing device including memory and a hardware processor, such as a camera, laptop, tablet, mobile phone, wearable device, mobile email device, portable game console, portable music player, reading device, head mounted display, or other electronic device capable of wirelessly accessing the network 140.
In the illustrated embodiment, client device 115 is coupled to network 140 by signal line 108, and client device 115n is coupled to network 140 by signal line 110. The signal lines 108 and 110 may be wired connections such as ethernet, or wireless connections such as Wi-Fi, bluetooth, or other wireless technologies. Client devices 115a, 115n are accessed by users 125a, 125n, respectively. The client devices 115a, 115n in fig. 1 are used in an exemplary manner. Although fig. 1 shows two client devices 115a and 115n, the present disclosure is applicable to system architectures having more than one client device 115.
In some implementations, the client device 115 can be a wearable device worn by the user 125. For example, the client device 115 may be included as part of a clip (e.g., a wristband), part of jewelry, or part of glasses. In other examples, client device 115 may be a smart watch. In various embodiments, the user 125 may view messages from the messaging applications 103a/103b on a display of the device, may access messages through a speaker or other output device of the device, and so forth. For example, the user 125 may view the message on a display of a smart watch or smart wristband. In another example, the user 125 may access the message through a headset (not shown) coupled with or part of the client device 115, a speaker of the client device 115, a haptic feedback element of the client device 115, or the like.
In some implementations, the messaging application 103b is stored on the client device 115 a. In some implementations, the messaging application 103b (e.g., thin client application, client module, etc.) can be a client application stored on the client device 115a with a corresponding messaging application 103a (e.g., server application, server module, etc.) stored on the message server 101. Application for example, messaging application 103b may transmit a message created by user 125 on client device 115 to messaging application 103 stored on messaging server 101.
In some embodiments, messaging application 103 may be a stand-alone application stored on messaging server 101. The user 125a may access the messaging application 103a via a web page using a browser or other software on the client device 115 a. In some implementations, the messaging application 103b implemented on the client device 115 can include modules that include the same or similar modules as included on the message server 101. In some implementations, the messaging application 103b may be implemented as a standalone client application, e.g., in a peer-to-peer or other configuration in which more than one client device 115 includes functionality to enable messages to be exchanged with other client devices 115. In these embodiments, the message server 101 may include limited functionality or no-message functionality (e.g., client authentication, backup, etc.). In some embodiments, message server 101 may implement more than one robot, e.g., robot 107a and robot 111.
Server 135 may include a processor, memory, and network communication capabilities. In some implementations, the server 135 is a hardware server. Server 135 is communicatively coupled to network 140 via signal line 128. The signal line 128 may be a wired connection, such as ethernet, coaxial cable, fiber optic cable, etc., or the signal line 128 may be a wireless connection, such as Wi-Fi, bluetooth, or other wireless technology. In some implementations, the server 135 sends data to or receives data from more than one message server 101 and client device 115 over the network 140. Although the server 135 is illustrated as one server, different embodiments may include more than one server 135. Server 135 may implement more than one robot, such as robot 109 and robot 113, as a server application or server module.
In various embodiments, the server 135 may be part of the same entity that manages the message server 101, e.g., a provider of message services. In some embodiments, the server 135 may be a third party server, e.g., a third party server controlled by an entity that is different from the entity that provides the messaging application 103a/103 b. In some implementations, the server 135 provides or hosts the robot.
A bot is an automated service implemented on more than one computer, with which a user interacts primarily via text, such as through a messaging application 103a/103 b. The bot may be implemented by a bot provider so that the bot may interact with users of various messaging applications. In some embodiments, the provider of the messaging application 103a/103b may also provide more than one bot. In some embodiments, the bot provided by a provider of a messaging application 103a/103b may be configured such that the bot may be included in other messaging applications, e.g., other messaging applications provided by other providers. The robot may provide several advantages over other modes. For example, the bot may allow a user to attempt a new service (e.g., a taxi booking service, a restaurant booking service, etc.) without installing an application on the client device or accessing a web site. Further, the user may interact with the robot through text, which requires little or no learning as compared to using a website, a software application, a telephone call such as an Interactive Voice Response (IVR) service, or other means of interacting with the service. Incorporating a robot in a messaging service or application may also allow a user to collaborate with other users to accomplish various tasks in the messaging service, e.g., complete travel planning, shopping, scheduling transactions, obtaining information, etc., and eliminate cumbersome operations such as switching between different applications (e.g., a taxi booking application, a restaurant booking application, a calendar application, etc.) or websites to accomplish tasks.
The bot may be implemented as a computer program or application (e.g., a software application) configured to interact with more than one user (e.g., any of users 125 a-n) through messaging application 103a/103b to perform particular actions or provide messages in messaging application 103. As an example, an information retrieval robot may search for information on the internet and present the most relevant search results in a messaging application. As another example, the travel robot may have the ability to make a trip schedule via the message application 103, for example, by purchasing travel and hotel tickets within the message application, making hotel reservations in the message application, making rental car reservations in the message application, and so forth. As another example, a taxi robot may have the capability to call a taxi, e.g., to the user's location (obtained by the taxi robot from the client device 115 when the user 125 allows access to the location information), without having to invoke or call a separate taxi booking application. As another example, the coach/coaching robot can coach the user in instructing the user in certain topics within the messaging application, for example, by asking questions that may be presented in the examination and providing feedback as to whether the user's response is correct. As another example, the gaming robot may play a game on the opposite side or the same side as the user in the messaging application. As another example, a commercial robot may provide services from a particular merchant, for example, by retrieving product information from the merchant's catalog and enabling purchases through a messaging application. As another example, the interface bot may connect to a remote device or vehicle so that a user of the messaging application can chat with, retrieve information from, and/or provide instructions to the remote device or vehicle.
The capabilities of the robot may include understanding the user's intent and executing it. The user's intent can be understood by analyzing and understanding the user's session and its context. The robot may also understand the changing context of the session or the changing mood and/or intent of the user based on the session evolving over time. For example, if user a suggests a meeting to drink coffee, but if user B states that he does not like coffee, the robot may assign a negative sentiment score for user B for coffee and may not suggest a coffee shop for the meeting.
Implementing a bot capable of communicating with a user of a messaging application 103a/103b may provide a number of advantages. Conventionally, a user may utilize a software application or a website to perform activities such as paying bills, ordering food, booking tickets, and the like. A problem with this embodiment is that the user is required to install or use multiple software applications and websites in order to perform multiple activities. For example, a user may have to install different software applications to pay a utility bill (e.g., from a utility company), purchase movie tickets (e.g., a ticketing application from a ticketing services provider), make restaurant reservations (e.g., from a corresponding restaurant), or may need to access a corresponding website for each event. Another problem with such an implementation is that the user may need to learn a complex user interface, such as one implemented using multiple user interface elements, such as windows, buttons, check boxes, dialog boxes, and the like.
Thus, an advantage of one or more of the described embodiments is that a single application enables a user to perform activities involving interaction with any number of parties, without requiring the user to visit a separate website, or to install and run a software application, with the technical effect of reducing the consumption of memory, storage, and processing resources on the client device. An advantage of the described embodiments is that the conversational interface makes it easier and faster for the user to complete such activities, e.g. without learning a complex user interface, with the technical effect of reducing the consumption of computing resources. Another advantage of the described embodiments is that implementing a bot may enable various participating entities to provide user interaction at a lower cost, with the technical effect of reducing computing resources deployed to enable user interaction, e.g., toll-free numbers implemented using one or more communication servers, websites hosted on one or more Web servers, customer-supported emails hosted on email servers, etc. Another technical effect of the described features is to reduce consumption of system processing and transmission resources required to complete user tasks over a communication network.
While some examples described herein are interactions between a bot and more than one user, various types of interactions are possible, such as, for example, a one-to-one interaction between one bot and one user 125, a one-to-many interaction between one bot and more than two users (e.g., in a group message session), a many-to-one interaction between multiple bots and one user, a many-to-many interaction between multiple bots and multiple users. Further, in some embodiments, the robots may also be configured to interact with another robot (e.g., robots 107a/107b, 109a/109b, 111, 113, etc.) through a messaging application 103, direct communication between robots, or a combination of both. For example, a restaurant reservation robot may interact with a robot for a particular restaurant in order to reserve a table.
In some embodiments, the bot may use a conversation interface to converse interact with the user using natural language. In some embodiments, the robot may use a template-based format to create sentences that interact with the user, for example, using a template such as "location of restaurant R is L" in response to a request for a restaurant address. In some cases, the user may be able to select an interaction format for the robot, e.g., whether the robot interacts with the user using natural language, whether the robot uses template-based interactions, etc.
In the case of a robot using natural language for conversational interactions, the content and/or type of interaction of the robot may change dynamically based on one or more of the following: the content of the conversation determined using natural language processing, the identity of the user in the conversation, and one or more contexts of the conversation (e.g., historical information about the user's interactions, connections between users in the conversation based on a social graph), external conditions (e.g., weather, traffic), the user's schedule, relevant contexts associated with the user, and so forth. In these cases, the content and type of interaction of the bot is based only on the user participating in the session having provided permission for such factors to vary.
As one example, if the user of the conversation is determined to be in a formal language (e.g., no or little slang terminology or emoticons), the bot may also interact in the conversation in the formal language, and vice versa. As another example, if the user in the conversation is determined (based on current and/or past conversations) to be a heavyweight user of emoticons, the robot may also interact with the user using more than one emoticon. As another example, if two users in a conversation are determined to be remotely connected in a social graph (e.g., there are more than two intermediate nodes between them indicating that they are friends, such as friends of friends), the bot may use a more formal language in the conversation. In the event that a user participating in a session does not provide permission to the bot for utilization of factors such as the user's social graph, schedule, or other context associated with the user, the interaction content and type of the bot may be of a default type, e.g., a neutral type that does not require utilization of such factors.
Further, in some embodiments, more than one robot may include functionality to engage in a back-and-forth conversation with a user. For example, if a user requests information about a movie, such as by entering "@ moviebot you can recommend a movie? "the robot" moviebot "may be in the form of" do you like comedy? "respond to it. For example, the user may then respond with "no", for which the robot may respond "good, customer movies named space and stars have excellent ratings, should i order you? ". The user may then indicate "good, i can go after 6 pm, please check if Steve can join". When the user grants the bot access to information about their contacts and the friend stev grants receipt of messages from the bot, the bot may send a message to the user's friend stev and perform further actions to book movie tickets at the appropriate time.
In some embodiments, a user participating in a session may be able to invoke a particular robot or invoke a robot that performs a particular task, for example, by typing in a robot name or robot handle (e.g., taxi, @ movie, etc.), by using voice commands (e.g., "call bankbot," etc.), by activating a user interface element (e.g., tag by other element or key of the robot name or handle), etc. Once the bot is invoked, the user 125 can send messages to the bot through the messaging application 103a/103b in a manner similar to sending messages to other users 125. For example, to order a taxi, the user may type "@ taxi to my taxi"; to make a hotel reservation, the user may type "@ hotelbot to reserve 4 people's tables at a chinese restaurant near me. "
In some embodiments, the bot may automatically suggest information or actions in the message conversation without being specifically invoked. That is, the user may not need to specifically invoke the robot. In these embodiments, the robot may rely on analysis and understanding of the conversation on a continuous basis or at discrete points in time. The analysis of the session can be used to understand the specific user needs and to identify when suggestions should be provided by the robot. As one example, if it is determined that a user needs information (e.g., based on the user asking another user for a question, based on multiple users indicating that they do not have some information), the robot may search for some information and suggest an answer. As another example, if it is determined that multiple users have expressed interest in Chinese food, the bot may automatically suggest a set of Chinese restaurants near the user, including optional information such as location, rating, and connection to the restaurant website.
In some embodiments, an automated suggestion may be made to more than one user in a message conversation to invoke more than one bot, as opposed to automatically invoking bots or waiting for a user to explicitly invoke bots. In these embodiments, the sessions may be analyzed on a continuous basis or at discrete points in time, and the analysis of the sessions may be used to understand specific user needs and identify when the robot should provide suggestions in the sessions.
In embodiments where the bot may automatically suggest information or actions in a message conversation without being specifically invoked, such functionality may be disabled, for example, if more than one user participating in the conversation does not provide permission to the bot to perform an analysis of the user conversation. Further, such functionality may also be temporarily disabled based on user input. For example, when the user indicates that the session is private or sensitive, the analysis of the context of the session is interrupted until the user provides input for the robot to be activated. Further, an indication that the analysis functionality is disabled may be provided to a participant of the conversation using, for example, a user interface element.
In various embodiments, the robot may be implemented in a variety of different configurations. For example, as shown in fig. 1, robot 105 is implemented on a client device 115 a. In this example, the bot may be a module in a software application local to the client device 115 a. For example, if the user has installed a taxi call application on the client device 115a, the robot function may be incorporated as a module into the taxi call application. In this example, the user may invoke the taxi robot, for example, by sending the message "@ taxi to my taxi. The "message application 103b may cause a robotic module in a taxi call application to be initiated. In this manner, the bot may be implemented locally on the client device, thereby enabling a user to participate in a conversation with the bot through messaging application 103.
In another example shown in fig. 1, the display robot 107a is implemented on the client device 115a and the display robot 107b is implemented on the message server 101. In this example, for example, the robot may be implemented as a client-server computer program, where each of the robot 107a (server module) and the robot 107b (client module) provides a portion of the robot functionality. For example, if the robot is a scheduling robot with a handle @ calendar, the user 115a may schedule the reminder by typing in "@ calendar to remind me to take a washed garment in the evening", which may be operated by the robot 107b (client module). Continuing with the example, if the user 115a tells the bot "check that Jim is available for a 4 meeting," the bot 107a (server module) may contact the user Jim (or Jim's scheduling bot) to exchange messages and provide a response to the user 115 a.
In another example, robot 109a (server module) is implemented on server 135 and robot 109b (client module) is implemented on client device 115. In this example, the robot functionality is provided by modules implemented on the client device 115 and the server 135, as opposed to the message server 101. In some implementations, the bot is implemented as a distributed application, e.g., having modules distributed across multiple client devices and servers (e.g., client device 115, server 135, message server 101, etc.). In some embodiments, the robot may be implemented as a server application, e.g., robot 111 is implemented on message server 101 and robot 113 is implemented on server 135.
Different embodiments, such as client-only, server-only, client-server, distributed, etc., may provide different advantages. For example, a client-only implementation that allows the functionality of the bot to be provided locally, e.g., without network access, may be advantageous in certain environments, e.g., when the user is outside of a network coverage area, or in any area with low or limited network bandwidth. Embodiments that include more than one server, such as a server-only, client-server, or distributed configuration, may allow for certain functions that may not be available locally at the client device, such as financial transactions, ticket ordering, and the like.
Although fig. 1 shows a different robot than messaging application 103, in some embodiments, more than one robot may be implemented as part of messaging application 103. In embodiments where the bot is implemented as part of the messaging application 103, user permissions are obtained prior to implementing the bot. For example, where a bot is implemented as part of a message application 103a/103b, the message application 103a/103b may provide the bot with the ability to perform certain activities, e.g., a translation bot that translates incoming and outgoing messages, a scheduling bot that schedules events on a user's calendar, etc. In this example, the translation bot is only activated under the user's specific permissions. If the user does not provide permission, the bot within messaging application 103a/103b is not implemented (e.g., disabled, removed, etc.). If the user provides permission, the bot or messaging application 103a/103b may restrict the use of messages exchanged between users through the messaging application 103a/103b, having provided specific functionality, e.g., translation, scheduling, etc. functionality.
In some embodiments, a third party other than the provider of the messaging application 103a/103b and the user 125 may provide a bot for a particular purpose that is capable of communicating with the user 125 through the messaging application 103a/103 b. For example, a taxi service provider may provide a taxi robot, a ticketing service may provide a robot capable of booking event tickets, a banking robot may provide the ability to conduct financial transactions, and so forth.
When the bot is implemented through messaging application 103, the bot is permitted to communicate with the user only upon authorization by the particular user. For example, if the user invokes the robot, the robot may reply, for example, based on the action of the user invoking the robot. In another example, the user may indicate a particular robot or a particular type of robot that can be contacted with the user. For example, the user may permit the travel robot to communicate with her, but not provide authorization for the shopping robot. In this example, the messaging application 103a/103b can permit the travel robot to exchange messages with the user, but filter or reject messages from the shopping robot.
Further, to provide some functionality (e.g., order a taxi, make a flight reservation, contact a friend, etc.), the bot may request that the user grant the bot access to user data such as location, payment information, a contact list, etc. In this case, the user may be presented with an option to grant or deny access to the robot. If the user refuses access, the robot may respond with a message, such as "sorry, i cannot reserve a taxi for you". Further, the user may provide access to information on a limited basis, e.g., the user may grant a taxi robot access to the current location only at a specific invocation of the robot and not otherwise, and in various embodiments, the user may control the type, quantity, and granularity of information that the robot may access and provide the user with the ability to change such permissions at any time (e.g., via a user interface). In some embodiments, the user data may be processed to remove personally identifiable information, limit information to specific data elements, etc., before the robot can access such data. Further, the user may control the use of user data through the messaging application 103a/103 and one or more robots. For example, a user may specify a robot that provides the ability to conduct financial transactions that require user authorization before completing the transaction, e.g., the robot may send the message "movie space and stars ticket $ 12 per sheet. Do i want to continue with the subscription? The best price for the "or" shirt is $ 125, including shipping costs. Should i deduct money from your credit card ending in 1234? "and the like.
In some embodiments, the messaging application 103a/103b may also provide one or more suggestions, e.g., suggestion responses, to the user 125 via a user interface, such as a button or other user interface element. For example, suggesting responses enables rapid interaction by reducing or eliminating the need for a user to type a response. For example, when a client device lacks text input functionality (e.g., a smart watch that does not include a keyboard or microphone), suggesting a response may enable a user to quickly and easily respond to a message. For example, when a user selects a suggested response (e.g., by selecting a corresponding user interface element on a touch screen), the suggested response may also cause the user to quickly respond to the message. The proposed response may be generated using a predictive model, such as a machine learning model, which is trained to generate the response.
For example, messaging applications 103a/103b can implement machine learning, such as a deep learning model, that can enhance user interaction with messaging application 103. The machine-learning model may be trained using synthetic data, e.g., data automatically generated by a computer, without using user information. In some implementations, the machine-learning model can be trained, e.g., based on sample data for which permission to train with user data has been expressly obtained from the user. For example, the sample data may include a received message and a response sent to the received message. Based on the sample data, the machine-learning model may predict a response to the received message, which may then be provided as a suggested response. For example, user interaction is facilitated by relieving the user of the burden of composing a response to a received message by providing response options that are customized based on the received message and the user's context. For example, when the user provides permission, the suggested response may be customized based on previous activities of the user, e.g., previous messages in a conversation, messages in a different conversation, etc. For example, such activity can be used to determine an appropriate suggested response for the user based on the type of interaction of the user, e.g., play response, official response, etc. In another example, when the user specifies more than one preferred language and/or locale, the messaging application 103a/103b can generate a suggested response in the language preferred by the user. In various examples, the suggested response may be a text response, an image, multimedia, and so on.
In some implementations, machine learning can be implemented on the message server 101, the client device 115, or both the message server 101 and the client device 115. In some implementations, a simple machine learning model can be implemented on the client device 115 (e.g., to permit operation of the model within memory, storage, and processing constraints of the client device), and a complex machine learning model can be implemented on the message server 101. If the user does not provide permission to use machine learning techniques, such techniques are not implemented. In some implementations, the user may selectively provide permission for machine learning to be implemented only on the client device 115. In these implementations, machine learning may be implemented on the client device 115 such that updates to the machine learning model or user information used by the machine learning model are stored or used locally and are not shared to other devices, such as the message server 101, the server 135, or other client devices 115.
For users who provide permission to receive suggestions, e.g., based on machine learning techniques, the suggestions may be provided through messaging application 103. For example, the suggestions may include suggestions for content (e.g., movies, books, etc.), schedule (e.g., active time on the user's calendar), event/location (e.g., restaurants, concerts, etc.), and so forth. In some implementations, if a user participating in a conversation provides permission to use conversation data, the suggestion can include a suggested response to the incoming message based on the conversation content. For example, if the first of the two users has granted a suggestion based on the content of the session, a message "what do you want to eat? How about italian dish? ", a response may be suggested to the second user, such as" @ assistant lunch, italy, two person table ". In this example, the suggested response includes the robot (identified by the symbol @ and the robot handle assistant). If the second user selects the response, an assistant bot is added to the session and a message is sent to the bot. The response from the robot is then displayed in the session, and either of the two users can further send a message to the robot. In this example, the assistant bot is not provided access to the conversation content and a suggested response is produced by messaging application 103.
In some implementations, the content of the suggested response can be customized based on whether the bot is already present in the session or can be incorporated into the session. For example, if it is determined that the travel robot can be incorporated into a messaging application, the suggested response to the cost question regarding tickets to france may be "let us ask the travel robot".
In various embodiments, for example, the suggestion of the suggestion response may include one or more of the following: text (e.g., "too baseball"), emoticons (e.g., smiley face, poverty face, etc.), images (e.g., photographs from a user's photo album), text that is inserted into fields of a template based on user data generated by the template (e.g., "her number is < phone number >", where the field "phone number" is filled in based on the user data if the user provides access to the user data), links (e.g., uniform resource locators), and so forth. In some implementations, the format and/or type of the suggested response may be set using, for example, color, font, layout, and the like. For example, a suggested response that includes a movie recommendation may include descriptive text about the movie, images from the movie, and a link to purchase tickets. In different embodiments, the suggestion response may be presented in different types of user interface elements, such as text boxes, information cards, and the like.
In various embodiments, the user is provided with control over whether they receive suggestions, what types of suggestions they receive, how often the suggestions are made, and the like. For example, the user may refuse to receive suggestions altogether, or may select a particular type of suggestion, or only receive suggestions during certain periods of the day. In another example, the user may choose to receive personalized suggestions. In this example, machine learning may be used to provide suggestions based on user preferences related to the use of user data and the use of machine learning techniques.
FIG. 2 is a schematic diagram of an example of a robot invocation event, according to some embodiments. In particular, the user device 202 communicates with the robot 204. The bot 204 may be invoked by more than one event on the user device 202. Events may include special character auto-completion or manual typing 206, a robot annotation click in message 208, a robot avatar click 210, and selection of an auto-reply suggestion 212.
The special character auto-complete or command may include an input from the user followed by a special character (e.g., "@") of the robot name or handle. In this example event, the robot may be identified by a robot handle, e.g., a robot name (e.g., a subscribing robot, assistant, etc.) following the "@" character. An example of a user interface element for special character auto-completion or command is shown in 402 of FIG. 4 and described below. Once the user enters a special character (e.g., "@") into a writing box or other text entry area, the system may monitor subsequent characters and attempt to automatically identify and suggest a robot having a handle (e.g., "@ robot") or name (e.g., "hotel robot") with a portion that matches the character typed or otherwise entered by the user. The user may select the auto-complete suggestion without having to type in the entire robot name or handle name.
Clicking (or other selection by touch, typing, or voice input) on the robot annotation (e.g., 404 in FIG. 4) may invoke the robot associated with the annotation. If annotations are associated with previously suggested responses, those responses may be recalled as shown in FIGS. 8-12 and described below. Click (or other selection by touch, typing, or voice input) on the robot avatar (e.g., 406 in fig. 4). Clicking (or other selection by touch, typing, or voice input) automatically replies to the suggestion (e.g., 408 in fig. 4).
FIG. 3 is a flow diagram showing a robot invoking an example method according to some embodiments. Processing begins at 302, where an indication to invoke an action of a robot is received at 302. The indication may include an indication of a special character robot command, an indication of a special character auto-complete selection, an indication of a click on a robot label, an indication of a click on a robot avatar, and/or an indication of a selection of an auto-reply suggestion. In some embodiments, the bot may be automatically invoked (with or without the actions shown in FIG. 2), and the system may automatically determine which bot to invoke based on different information, such as what the user typed after @ or other special characters, the type of task the user is attempting to perform or needs to perform based on the user's actions or context of use, whether the user is looking for information (what the user is looking for), user voice commands, and so forth. Processing continues to 304.
At 304, a response type of the indication to invoke the action of the robot is determined. For example, if the indication is a selection of an auto-answer suggestion, the action may be to provide an auto-answer to the robot or other participant in the conversation. In another example, if the indication is an indication of a special character command, the action may be determined to be that of the corresponding command. Processing continues to 306.
At 306, based on the response type determined at 304, the robot responds to the received indication invoking the action of the robot. For example, the robot may provide an automatic reply to the conversation suggesting, the robot may act on a received command, and so on.
Fig. 4 is a schematic diagram of a user interface 400 displaying an exemplary robot recognition and invocation element, according to some embodiments. In particular, the user interface 400 includes an exemplary special character auto-completion subsection 402 associated with a message composition box. The user interface also includes an exemplary robot callout 404, which can represent a message or command previously sent to the robot. The user interface 400 includes an exemplary robot avatar 406 and an exemplary auto-reply suggestion 408. There may be more than one robot label, avatar, or auto-reply suggestion in the user interface.
FIG. 5 is a flow diagram of an exemplary method of managing user privacy during robot interaction, according to some embodiments. Processing begins at 502, a request is received from a user and/or a suggestion is generated at a robot, where the request or suggestion may require shared user data. The request may include the requested task that the robot is to perform (e.g., "share my flight data," "share photos of my puppies," etc.). In some implementations, the request can be a command to the robot. For example, the request including a command to the reservation robot may be "@ reservationbot looking for a nearby hotel," the request including a command to the assistant robot may be "@ assistant sending my flight details to jim," and so on. In this example, the robot is identified by a robot handle, e.g., the name of the robot following the "@" symbol (e.g., reservationbot, assigner, etc.). The robot may need to access user data in order to perform tasks and/or provide responses to requests. The user and the robot may be a one-to-one communication arrangement. For example, a user may request a car service pickup, and a car service robot may need to know the user's location in order to determine which car may be used to pick up passengers. In another example, a user may wish to book a hotel at a nearby hotel, and the booking hotel robot may need to know the user's location. In yet another example, the bot may provide a suggested response to the user that includes shared user information (e.g., photos, calendar entries, navigation schedules, etc.), and the bot may need to obtain user permission to access data that may be helpful in suggesting a response, and provide such data as an actual response. The request may be a request from a user or may be an automatically generated request (e.g., from a suggestion response robot, etc.). Processing continues to 504.
At 504, the permission user interface element is caused to be displayed to the user associated with the request or the suggested response. One example of a permission requesting user interface element is shown in FIG. 6 and described below. The permission user interface element may also be presented as an audio prompt or use other user interface and/or output methods. Processing continues to 506.
At 506, an indication is received whether the user granted permission for the bot to share user data in an ongoing session, which may be a one-to-one session or a group session. The indication may be received in the form of a user interface element selection (e.g., by typing, voice input, gesture input, etc., touching, clicking, selecting a user interface button on the screen) that indicates that the user granted permission or not granted permission. For example, the user may select one of the "not now" or "allowed" shown in the permission user interface element of FIG. 6. Processing continues to 508.
At 508, the robotic licensing system determines whether a license is granted. Determining whether permission is granted may be accomplished by evaluating the indication received in step 506. If permission is granted, processing continues to 510. If permission has not been granted, processing continues to 512.
At 510, an indication of the user data is shared by the bot into the session according to permissions granted by the user.
At 512, the user data is not shared by the bot, in accordance with the user not granting permission.
FIG. 6 is a schematic diagram of an exemplary user interface 600 with robotic messages, according to some embodiments. In particular, the user interface 600 includes a message from the user to the robot (602). Message 602 includes a request that may require use of the user's personal information, such as a photo of the dog ("sharing my photo"). In response to a request from a user, the bot may display a permission allow/disallow interface element (604) to the user seeking permission to share user data (606) in order to complete the request. Licensing unit 604 may include a description or sample of what types of user data need to share licenses (606), and input elements 608 and 610 for robotic licensing, respectively, that allow or disallow sharing of user data.
Fig. 7 is a flow diagram of an exemplary method for recalling previous robot suggestions, according to some embodiments. Fig. 8-12 provide exemplary user interface diagrams to aid in the illustration of the method of fig. 7 and are described in conjunction with fig. 7. Processing begins at 702, where a request is received at a robot from a user in a session at 702. The request may come from a text command or other method described herein. For example, the user may issue a command, such as "@ bot 7 this evening for any table of 4 people" shown at 802 in FIG. 8. Processing continues to 704.
At 704, the robot may cause a confirmation interface element (e.g., 804 in fig. 8) to be displayed. The confirmation element may provide an indication to the user that the robot is processing the request or command, or is working on the request or command. The confirmation may include more than one suggested auto-reply (e.g., 806 and 808 in fig. 8). Processing continues to 706.
At 706, an indication of a subsequent message by the session participant (e.g., 902 in fig. 9). Processing continues to 708.
At 708, the suggested response is removed (e.g., 902) based on the determination of the subsequent message. The removal of the suggested response (806 and 808) is shown in FIG. 9 by the absence of a suggestion between 804 and 902. Processing continues to 710.
At 710, an indication is received that a user has clicked (or otherwise selected) (1004 in FIG. 10) a previously displayed confirmation message (e.g., 804) with one or more suggested responses associated therewith. A subsequent message 1002 is also shown in fig. 10. By clicking on or selecting a previously displayed confirmation message, the user may be indicating an interest in recalling the suggested response associated with the confirmation message. Processing continues to 712.
At 712, the session is optionally temporarily hidden or made less prominent by fading or other techniques. Processing continues to 714.
At 714, the previously displayed confirmation message (e.g., 802) and the suggested response associated therewith (806 and 808) are displayed as shown in FIG. 11. Instead of hiding the session, the system may extend the confirmation message and display the suggested response. Processing continues to 716.
At 716, an indication is received suggesting selection of one of the responses. For example, an indication of a selection of a "cancel" suggestion response (808) may be received, as shown at 1102 of FIG. 11. Alternatively, a "back" indication may be received indicating that no suggested automatic response is selected and the user wishes to return to the conversation screen, and the suggested response may be removed from the display again. Processing continues to 718.
At 718, the display returns to the session and the robot reacts to the suggested response. For example, the bot may cause a "cancel him" message to be displayed, as shown at 1202 in fig. 12.
FIG. 13 is a flow diagram of an exemplary method of managing robot permissions within a group message context (e.g., within a "group chat"), according to some embodiments. Processing begins at 1302, a request is received at a robot from a user. The robot may need to access user data in order to perform the requested task and/or provide a response to the request. The user and the robot may be in a group communication arrangement with multiple users and/or robots. For example, in a communication session with multiple users, the user may request, for example, a car service pickup for the multiple users. The car service robot may need to know the location of each user contained in the pickup in order to determine which car may be used to pick up the user. In another example, a user may wish to make a hotel reservation in a nearby hotel for a group of users participating in the session. In this example, the hotel reservation robot may need to know information about the user group, e.g., name, payment information, etc. The request may be a request from a user or an automatically generated request (e.g., from a suggestion response robot, etc.). Processing continues to 1304.
At 1304, a progress indication is optionally displayed by the robot, and the progress indication may be visible to the group or the individual user making the request. For example, the car service robot may display a message such as "i am processing it" in the group session. Processing continues to 1306.
At 1306, a permission user interface element is caused to be displayed to a user associated with the request. An example of a permission requesting user interface element is shown in fig. 6 and described above. When the robot requests permission to access user data rather than share user data, the permission requesting user interface element, if given permission, may describe the data that the robot needs to access, rather than showing an example that the robot will share data. The permission user interface element may also be presented as a verbal prompt or using other user interface and/or output methods. Processing continues to 1308.
At 1308, an indication of whether more than one user has granted the bot access or permission to obtain corresponding user data is received. The indication may be received in the form of a user interface element selection (e.g., by typing, voice input, gesture input, etc., touching, clicking, selecting a user interface key on the screen) that indicates that the user granted permission or not granted permission. For example, the user may select one of the "not now" or "allowed" displayed in the permission user interface element in FIG. 6. Processing continues to 1310.
At 1310, the robotic permission system determines whether permission is granted. Determining whether to grant permission may be accomplished by evaluating the indication received in step 1308. If permission is granted, processing continues to 1312. If permission has not been granted, processing continues to 816.
At 1312, the bot may open a one-to-one chat with the user. The message exchange in one-to-one chats and one-to-one chats is invisible to the group of users in the group message session. Examples of a one-to-one session user interface having different visual themes corresponding to respective robots are shown in fig. 14 and 15. Processing continues with 1314.
At 1314, the bot may perform further processing to complete the task associated with the permission granted in the one-to-one user message session. For example, the car service robot can continue to determine which cars may be in a certain location, providing car services to the user. In another example, the accommodation robot can use the shared user location to determine nearby unoccupied residences as well as rented residences.
At 1316, the robot may cause a "grace" indication of the rejection task to be displayed in the group message session. For example, the robot may provide an indication such as "i am unable to obtain your location-i am unable to arrange a car", etc. The indication may be displayed on a graphical user interface or provided in the form of a voice alert or other output indication. The tangible aspect of the rejection message may include a message that does not explicitly indicate that the user has not granted permission for the bot to use the user data. In different embodiments, the indication may include different textual content, e.g., based on the request or other factors. For example, the indication responsive to the user prohibiting access to a location that is in the context of a subscription car may include textual content such as "no-go, no-get location", "i cannot find cars in your vicinity", "car services are not available", and so on. In some implementations, different indications may be sent to different participants in the group session. In some implementations, the indication may use different formats, such as a text box, a graphical indication, an animated indication, and so forth. In some embodiments, the indication may be of a different type, e.g., bold text, italics text, font, color, etc.
FIG. 14 is a schematic diagram showing an exemplary one-to-one user interface 1400 with a given robot theme, according to some embodiments. Fig. 14 shows an example for an exemplary restaurant robot. The theme interface 1400 may include a particular background color, image, text, etc. (1402). The subject interface 1400 can also include a flag, name, and handle for the robot (1404). The theme interface 1400 may also include information provided by the bot, such as restaurant information 1406.
FIG. 15 is a schematic diagram showing an exemplary one-to-one user interface 1500 for a given robot type theme, according to some embodiments. The example shown in figure 15 is for an exemplary hotel robot. The theme interface 1500 may include a particular background color, image, text, etc. (1502). The subject interface 1500 can also include a flag, name, and handle for the robot (1504). The subject interface may also include information provided by the bot, such as restaurant information 1506-. The one-to-one interface 1500 also includes suggested selection buttons 1510 and 1512, and a compose box 1514 that composes messages (e.g., commands, requests, etc.) for the robot.
FIG. 16 is a block diagram of an exemplary computing device 1600 that may implement one or more features described herein. In one example, computer device 1600 may be used to implement a client (or user) device, such as any of client devices 115a-115n shown in FIG. 1. The computing device 1600 may be any suitable computer system, server, or other electronic or hardware device described above.
One or more methods described herein may operate on: the application can be run in a stand-alone program running on any type of computing device, in a program running on a web browser, in a mobile application ("app") of a mobile computing device, such as a cell phone, smartphone, desktop computer, wearable device (watch, wristband, jewelry, virtual reality goggles or glasses, augmented reality goggles or glasses, etc.). In an example, a client/server architecture may be used, such as a mobile computing device (as a user device) sending user input data to a server device and receiving final output data from the server for output (e.g., for display). In another example, all of the calculations may be performed in a mobile application (and/or other application) on the mobile computing device. In another example, the computation may be split between the mobile computing device and more than one server device.
In some embodiments, computing device 1600 includes a processor 1602, a memory 1604, and input/output (I/O interface 1606. processor 1602 may be one or more processors and/or processing circuits that execute program code and control basic operations of computing device 1600. A "processor" includes any suitable hardware and/or software system, mechanism, or component that processes data, signals, or other information.
Memory 1604 is typically provided in the computing device 1600 for access by the processor 1602, and may be any suitable processor-readable storage medium, such as Random Access Memory (RAM), Read Only Memory (ROM), electrically erasable programmable read-only memory (EEPROM), flash memory, etc., that is suitable for storing commands for execution by the processor and that is separate from the processor 1602 and/or integrated with the processor 1602. The memory 1604 may store software that is executed by the processor 1602 on the computing device 1600, including an operating system 1608 and one or more applications 1610, such as messaging applications, robotic interaction applications, and the like. In some implementations, the application 1610 may include commands that enable the processor 1602 to perform the functions described herein, such as one or more of the methods of fig. 3, 5, 7, and/or 13. For example, the applications 1610 may include messages and/or robot applications, including programs to manage robot permissions, user permissions, recall old robot suggestions, and/or identify robots as described herein. For example, one or more applications may provide a displayed user interface to display selectable options or controls and display data based on the selected options in response to user input. One or more methods disclosed herein may operate in several environments and platforms, for example, as a stand-alone computer program capable of running on any type of computing device, as having a web site application, as a mobile application ("app") running on a mobile computing device.
Any software in the memory 1604 may alternatively be stored on any other suitable storage location or computer readable medium. In addition, memory 1604 (and/or other connected storage devices) can store messages, permission settings, user preferences and associated data structures, parameters, audio data, user preferences, and/or other commands and data used in features described herein in database 1612. The memory 1604 and any other type of storage (magnetic disks, optical disks, tape, or other tangible media) may be considered "storage" or "storage devices.
I/O interface 1606 may provide functionality to interface computing device 1600 with other systems and devices. The interface device may be included within computing device 1600 as part of computing device 1600, or may be separate from computing device 1600 and in communication with computing device 1600. For example, network communication devices, wireless communication devices, storage devices, and input/output devices may communicate via I/O. In some implementations, the I/O interface 1606 can connect to interface devices such as input devices (keyboard, pointing device, touch screen, microphone, camera, scanner, sensor, etc.) and/or output devices (display device, speaker device, printer, motor, etc.).
Some examples of interface devices that may be connected to I/O interface 1606 may include display device 1614, where display device 1614 may be used to display content, such as user interfaces for application images, videos, and/or output applications as described herein. The display device 1614 may be connected to the computing device 1600 via a local connection (e.g., a display bus) and/or via a network connection, and may be any suitable display device. The display device 1614 may include any suitable display device, such as a Liquid Crystal Display (LCD), Light Emitting Diode (LED) or plasma display screen, Cathode Ray Tube (CRT), television, monitor, touch screen, 3D display screen, or other visual display device. For example, the display device 1614 may be a flat display screen provided on a mobile device, multiple display screens provided in a goggle device, or a monitor screen for a computer device.
I/O interface 1606 may connect to other input and output devices. Some examples include more than one camera that can capture image frames. Orientation sensors, such as gyroscopes and/or accelerometers, may provide sensor data (which may correspond to view orientations in some embodiments) indicating device orientation and/or camera orientation. Some implementations may provide a microphone for capturing sound (e.g., voice command commands, etc.), an audio speaker device for outputting sound, or other input and output devices.
For ease of illustration, FIG. 16 shows one block for processor 1602, memory 1604, I/O interface 1606, operating system 1608, and robotic interaction application 1610, respectively. These blocks may represent more than one processor or processing circuit, more than one operating system, more than one memory, more than one I/O interface, more than one application, and/or more than one software module. In other implementations, the computing device 1600 may not have all of the components shown and/or may have other elements including other types of elements instead of or in addition to those shown herein. Although user devices (e.g., 115a-115n) are described as performing blocks and operations as described in some embodiments herein, any suitable component or combination of components of a user device (e.g., 115a-115n) or similar device, or any suitable processor or processors associated with such a system, may perform the described blocks and operations.
The methods described herein may be implemented by computer program instructions or code which may be executed on a computer. For example, the code may be implemented by one or more digital processors (e.g., microprocessors or other processing circuits), and may be stored on a computer program product that includes a non-transitory computer-readable medium (e.g., a storage medium) such as a magnetic, optical, electromagnetic, or semiconductor storage medium, including: semiconductor or solid state memory, magnetic tape, a removable computer diskette, Random Access Memory (RAM), read-only memory (ROM), flash memory, a rigid magnetic disk, an optical disk, a solid state memory drive, or the like. The program commands may also be embodied in electronic signals and provided as electrical signals, for example in the form of software as a service (SaaS) delivered from a server (e.g. a distributed system and/or a cloud computing system). In addition, one or more of the methods may be implemented in hardware (logic gates, etc.) or a combination of hardware and software. Exemplary hardware may be a programmable processor (e.g., a Field Programmable Gate Array (FPGA), Complex Programmable Logic Device (CPLD), etc.), a general purpose processor, a graphics processor, an Application Specific Integrated Circuit (ASIC), etc. One or more of the methods may be performed as part or component of an application running on a system, or as an application or software running in conjunction with other applications and operating systems.
While the specification has been described with respect to specific embodiments thereof, these specific embodiments are merely illustrative and not restrictive. The concepts shown in the examples may be applied to other examples and embodiments.
In the case where certain embodiments discussed herein may collect or use personal information about a user (e.g., the user's phone number or partial phone number, user data, information about the user's social network, the user's location and time, the user's biometric information, the user's activities, and demographic information), the user is provided with more than one opportunity to control: whether to collect personal information, whether to store personal information, whether to use personal information, and how to collect, store, and use information about a user. That is, the systems and methods discussed herein collect, store, and/or use user personal information, particularly upon receiving explicit authorization to do so from an associated user. In addition, certain data may be processed in one or more ways prior to storage or use in order to remove personally identifiable information. As one example, the identity of the user may be processed so that no personally identifiable information may be determined. As another example, the geographic location of the user may be generalized to a larger area so that a particular location of the user cannot be determined.
It is noted that the functional blocks, operations, features, methods, devices, and systems described in this disclosure can be integrated or divided into different combinations of systems, devices, and functional blocks known to those skilled in the art. The routines of particular embodiments may be implemented using any suitable programming language and programming techniques. Different programming techniques, such as procedural or object oriented programming techniques, may be employed. The routines can execute on a single processing device or multiple processors. Although the steps, operations, or computations may be presented in a specific order, these orders may be changed in different specific implementations. In some embodiments, multiple steps or operations shown as sequential in this specification may be performed at the same time. Other exemplary embodiments are disclosed below.
1. A computer-implemented method, comprising:
providing a messaging application on a first computing device associated with a first user to enable communication between the first user and at least one other user;
detecting a user request on a message application;
programmatically determining that an action in response to the user request requires sharing of data associated with the first user;
causing a permission interface to be rendered for application in the messaging application on the first computing device, the permission interface enabling a first user to approve or prohibit sharing of data associated with the first user; and is
Upon receiving user input from the first user indicating approval to share data associated with the first user, accessing and sharing data associated with the first user within the messaging application in response to the user request.
2. The computer-implemented method of embodiment 1, wherein the first user is a human user and the at least one other user is an auxiliary agent.
3. The computer-implemented method of embodiment 1 or 2, wherein the first user is a human user and the at least one other user comprises a second human user, the second human user being different from the first user and associated with a second computing device, and wherein the permission interface is rendered in a messaging application on the first computing device associated with the first user, the permission interface not being displayed on the second computing device associated with the second human user.
4. The computer-implemented method of any of embodiments 1-3, wherein the first user is a human user and the at least one other user comprises a second human user and an assistance agent, the second human user being different from the first user, and wherein the user request is received from a first computing device associated with the first user, the method further comprising initiating a separate session in the messaging application in response to the user request, wherein the separate session comprises the first user and the assistance agent and does not comprise the second human user.
5. The computer-implemented method of any of embodiments 1-4, wherein detecting the user request comprises: more than one message received in the messaging application from the first user to the secondary proxy is analyzed by the secondary proxy.
6. The computer-implemented method of embodiment 5, wherein the one or more messages comprise one or more of a text message, a multimedia message, and a command to an auxiliary agent.
7. The computer-implemented method of any of embodiments 1-6, further comprising providing more than one suggestion in the first messaging application based on input from at least one of the first user and the at least one other user.
8. The computer-implemented method of embodiment 7, further comprising causing one or more suggestions to be rendered in the messaging application.
9. The computer-implemented method of embodiment 8, wherein more than one suggestion is rendered as a suggestion element that, when selected by the first user, causes details about the suggestion to be displayed.
10. The computer-implemented method of one of embodiments 1 to 9, further comprising: the auxiliary agent is automatically invoked based on one or more actions of the first user.
11. The computer-implemented method of any of embodiments 1-10, wherein the secondary proxy is invoked in response to one or more of:
special character commands for the secondary agent;
a user selection of a user interface annotation element associated with the secondary agent;
a user selection of an avatar corresponding to the secondary agent; and
user selection of an auto-reply suggestion provided by the secondary agent.
Claims (20)
1. A computer-implemented method, comprising:
detecting, on a messaging application on a first computing device associated with a first user, a user request to determine that a second user is available within a predetermined time;
programmatically determining that an operation in response to the user request requires sharing of data associated with the first user;
causing a permission interface to be rendered in the messaging application on the first computing device, the permission interface enabling the first user to approve or prohibit sharing of data associated with the first user;
upon receiving user input from the first user indicating approval to share data associated with the first user, accessing data associated with the first user and sharing data associated with the first user with a first auxiliary agent in response to the user request; and is
Performing, by the first auxiliary agent, a task based on data associated with the first user, wherein the task comprises scheduling a transaction for the first user by contacting the second user or a second auxiliary agent of the second user to determine that the second user is available within a predetermined time.
2. The computer-implemented method of claim 1, wherein the messaging application provides the first user with an option to receive suggestions from the first secondary agent during certain periods of the day.
3. The computer-implemented method of claim 1, wherein detecting the user request comprises: analyzing, by the first secondary agent, one or more messages received in the messaging application, wherein the messages are sent from the first user to the first secondary agent.
4. The computer-implemented method of claim 3, wherein the one or more messages comprise one or more of a text message, a multimedia message, or a command to the first auxiliary agent.
5. The computer-implemented method of claim 1, further comprising: determining that the first user invoked the first auxiliary agent by using a name of the first auxiliary agent followed by a special character.
6. The computer-implemented method of claim 1, wherein the first secondary agent is invoked in response to one or more of:
a user selection of an avatar corresponding to the first auxiliary agent; and
a user selection of an auto-reply suggestion provided by the first secondary agent.
7. The computer-implemented method of claim 1, wherein the message application initiates communication between the first user and a second user, the second user being different from the first user and associated with a second computing device, and wherein a permission interface is rendered in the message application on the first computing device associated with the first user and the permission interface is not displayed on the second computing device associated with the second user.
8. The computer-implemented method of claim 1, wherein the messaging application initiates communication between the first user, the second user different from the first user, and the first assistance agent, and wherein the user request is received from the first computing device associated with the first user, the method further comprising, in response to the user request, initiating a separate session in the messaging application, wherein the separate session includes the first user and the first assistance agent and does not include the second user.
9. The computer-implemented method of claim 1, further comprising providing more than one suggestion in the messaging application based on input from at least one of the first user and at least one other user.
10. The computer-implemented method of claim 9, further comprising causing the one or more suggestions to be displayed in the messaging application.
11. The computer-implemented method of claim 10, wherein the one or more suggestions are rendered as suggestion elements that, when selected by the first user, cause details of the corresponding suggestion to be displayed.
12. A computer-implemented method, comprising:
detecting, in a messaging application, a user input corresponding to a selection of a previously displayed confirmation message from a first secondary agent, wherein the previously displayed confirmation message is associated with one or more previously displayed suggested auto-reply responses;
causing an auto-reply response to the previously displayed suggestion to be displayed;
receiving an indication that the first user selected a given one of the one or more previously displayed suggested auto-reply responses;
sending the given one of the one or more previously displayed suggested auto-reply responses to the first auxiliary agent and a second user participating in a conversation within the messaging application;
sharing data associated with the first user with the first auxiliary agent; and is
Performing, by the first auxiliary agent, a task based on data associated with the first user, wherein the task comprises scheduling a transaction for the first user by contacting the second user or a second auxiliary agent of the second user to determine that the second user is available within a predetermined time.
13. The computer-implemented method of claim 12, further comprising invoking the first auxiliary agent in response to the user input.
14. The computer-implemented method of claim 12, wherein the previously displayed message and the one or more suggested auto-reply responses are initially rendered in the message application on a first computing device associated with the first user, and wherein providing the one or more suggested auto-reply responses to the one or more first auxiliary agents and the conversation within the message application comprises: providing the one or more suggested auto-reply responses in the messaging application.
15. A computer-implemented system, comprising:
one or more hardware processors coupled to a non-transitory computer readable medium having stored thereon software instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
receiving a user request from a messaging application on a first computing device associated with a first user, the messaging application configured to enable communication between the first user and at least one other user, wherein the user request is used to determine that a second user is available within a predetermined time;
programmatically determining that an operation in response to the user request requires sharing of data associated with the first user;
causing a permission interface to be rendered in the messaging application on the first computing device, the permission interface enabling the first user to approve or prohibit sharing of the data associated with the first user; and is
Upon receiving user input from the first user approving sharing of data associated with the first user, accessing, within the messaging application and sharing data associated with the first user with a first auxiliary agent in response to the user request; and is
Performing, by the first auxiliary agent, a task based on data associated with the first user, wherein the task comprises scheduling a transaction for the first user by contacting the second user or a second auxiliary agent of the second user to determine that the second user is available within a predetermined time.
16. The system of claim 15, wherein the messaging application provides the first user with an option to receive suggestions from the first secondary agent during certain periods of the day.
17. The system of claim 16, wherein detecting the user request comprises: analyzing, at the first secondary agent, more than one message received in the messaging application from the first user to the first secondary agent.
18. The system of claim 15, wherein the permission interface is rendered in the messaging application on the first computing device associated with the first user and the permission interface is not displayed on a second computing device associated with the second user.
19. The system of claim 15, wherein the user request is received from the first computing device associated with the first user, the operations further comprising, in response to the user request, initiating a separate session in the messaging application, wherein the separate session includes the first user and the first assistance agent and does not include the second user.
20. The system of claim 15, further comprising providing more than one suggestion to the messaging application based on input from at least one of the first user and the at least one other user.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662397318P | 2016-09-20 | 2016-09-20 | |
US62/397,318 | 2016-09-20 | ||
PCT/US2017/052336 WO2018057537A1 (en) | 2016-09-20 | 2017-09-19 | Bot interaction |
Publications (2)
Publication Number | Publication Date |
---|---|
CN109691034A CN109691034A (en) | 2019-04-26 |
CN109691034B true CN109691034B (en) | 2021-07-09 |
Family
ID=60022180
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780009158.7A Active CN109691034B (en) | 2016-09-20 | 2017-09-19 | Robot interaction |
Country Status (6)
Country | Link |
---|---|
US (1) | US10798028B2 (en) |
EP (1) | EP3378204B1 (en) |
JP (1) | JP6605151B2 (en) |
KR (2) | KR102197448B1 (en) |
CN (1) | CN109691034B (en) |
WO (1) | WO2018057537A1 (en) |
Families Citing this family (71)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10218651B2 (en) * | 2015-04-20 | 2019-02-26 | Oracle International Corporation | Virtual assistance for chat agents |
EP3395019B1 (en) | 2015-12-21 | 2022-03-30 | Google LLC | Automatic suggestions and other content for messaging applications |
CN108781175B (en) | 2015-12-21 | 2021-09-21 | 谷歌有限责任公司 | Method, medium, and system for automatic suggestion of message exchange contexts |
US10382370B1 (en) | 2016-08-11 | 2019-08-13 | Amazon Technologies, Inc. | Automated service agents |
CN117634495A (en) | 2016-09-20 | 2024-03-01 | 谷歌有限责任公司 | Suggested response based on message decal |
US10511450B2 (en) | 2016-09-20 | 2019-12-17 | Google Llc | Bot permissions |
US10015124B2 (en) * | 2016-09-20 | 2018-07-03 | Google Llc | Automatic response suggestions based on images received in messaging applications |
US10361975B2 (en) * | 2016-10-10 | 2019-07-23 | Microsoft Technology Licensing, Llc | Messaging bot selection in multi-bot chat sessions |
WO2018075548A1 (en) * | 2016-10-17 | 2018-04-26 | Sd Squared Limited | Systems and method for creating software from library and custom components |
US10484313B1 (en) * | 2016-10-28 | 2019-11-19 | Amazon Technologies, Inc. | Decision tree navigation through text messages |
US10469665B1 (en) | 2016-11-01 | 2019-11-05 | Amazon Technologies, Inc. | Workflow based communications routing |
US10416846B2 (en) | 2016-11-12 | 2019-09-17 | Google Llc | Determining graphical element(s) for inclusion in an electronic communication |
US10713430B2 (en) * | 2016-11-30 | 2020-07-14 | Google Llc | Systems and methods for applying layout to documents |
KR20180084549A (en) * | 2017-01-17 | 2018-07-25 | 삼성전자주식회사 | Method for Producing the Message and the Wearable Electronic Device supporting the same |
US11907272B2 (en) | 2017-02-17 | 2024-02-20 | Microsoft Technology Licensing, Llc | Real-time personalized suggestions for communications between participants |
CN110945839B (en) * | 2017-03-27 | 2022-03-04 | 奥瑞恩实验室 | Shared and per-user robotic group messaging method |
US11695711B2 (en) * | 2017-04-06 | 2023-07-04 | International Business Machines Corporation | Adaptive communications display window |
US10333868B2 (en) * | 2017-04-14 | 2019-06-25 | Facebook, Inc. | Techniques to automate bot creation for web pages |
US10491548B2 (en) * | 2017-04-17 | 2019-11-26 | Facebook, Inc. | Techniques to configure bot flow |
US10860854B2 (en) | 2017-05-16 | 2020-12-08 | Google Llc | Suggested actions for images |
US10404636B2 (en) | 2017-06-15 | 2019-09-03 | Google Llc | Embedded programs and interfaces for chat conversations |
US10348658B2 (en) | 2017-06-15 | 2019-07-09 | Google Llc | Suggested items for use with embedded applications in chat conversations |
US10798029B2 (en) * | 2017-07-25 | 2020-10-06 | Freshworks, Inc. | System and apparatus for generation of multiple automated ephemeral chat messages |
US11777875B2 (en) * | 2017-09-15 | 2023-10-03 | Microsoft Technology Licensing, Llc | Capturing and leveraging signals reflecting BOT-to-BOT delegation |
US10678402B1 (en) * | 2017-11-06 | 2020-06-09 | Amazon Technologies, Inc. | Interactive bot for natural language analytics |
US10891526B2 (en) | 2017-12-22 | 2021-01-12 | Google Llc | Functional image archiving |
US11237779B2 (en) * | 2018-03-20 | 2022-02-01 | Fujifilm Business Innovation Corp. | Message providing apparatus and non-transitory computer readable medium |
US11240180B2 (en) | 2018-03-20 | 2022-02-01 | Fujifilm Business Innovation Corp. | Message providing device and non-transitory computer readable medium |
US10782986B2 (en) | 2018-04-20 | 2020-09-22 | Facebook, Inc. | Assisting users with personalized and contextual communication content |
US11307880B2 (en) | 2018-04-20 | 2022-04-19 | Meta Platforms, Inc. | Assisting users with personalized and contextual communication content |
US11715042B1 (en) | 2018-04-20 | 2023-08-01 | Meta Platforms Technologies, Llc | Interpretability of deep reinforcement learning models in assistant systems |
US11676220B2 (en) | 2018-04-20 | 2023-06-13 | Meta Platforms, Inc. | Processing multimodal user input for assistant systems |
US11886473B2 (en) | 2018-04-20 | 2024-01-30 | Meta Platforms, Inc. | Intent identification for agent matching by assistant systems |
US10419934B1 (en) * | 2018-05-09 | 2019-09-17 | Facebook, Inc. | Systems and methods for authenticating users based on enriched data |
US11249772B2 (en) | 2018-05-29 | 2022-02-15 | Google Llc | Feature exposure for model recommendations and feedback |
CN110751947B (en) * | 2018-11-13 | 2021-05-07 | 北京嘀嘀无限科技发展有限公司 | Method for prompting user, electronic equipment and computer readable storage medium |
US11331581B2 (en) | 2019-03-19 | 2022-05-17 | modl.ai ApS | Experience based game development and methods for use therewith |
US11596867B2 (en) | 2019-03-19 | 2023-03-07 | modl.ai ApS | AI-based content generation for gaming applications |
US10918948B2 (en) * | 2019-03-19 | 2021-02-16 | modl.ai ApS | Game bot generation for gaming applications |
CN110162776A (en) * | 2019-03-26 | 2019-08-23 | 腾讯科技（深圳）有限公司 | Interaction message processing method, device, computer equipment and storage medium |
JP7104277B2 (en) * | 2019-03-29 | 2022-07-21 | 株式会社Aill | Communication support server, communication support system, communication support method, and communication support program |
JP7409781B2 (en) * | 2019-04-01 | 2024-01-09 | Ｌｉｎｅヤフー株式会社 | Output program, output device and output method |
KR20200121064A (en) | 2019-04-15 | 2020-10-23 | 라인플러스 주식회사 | Method, system, and non-transitory computer readable record medium for p managing event messages |
USD907053S1 (en) * | 2019-05-31 | 2021-01-05 | Apple Inc. | Electronic device with animated graphical user interface |
CN110311856A (en) * | 2019-06-28 | 2019-10-08 | 上海连尚网络科技有限公司 | Instant communicating method, equipment and computer readable storage medium |
US11676316B1 (en) | 2019-07-01 | 2023-06-13 | Instasize, Inc. | Shareable settings for modifying images |
US11449664B1 (en) * | 2019-07-01 | 2022-09-20 | Instasize, Inc. | Template for creating content item |
JP7272893B2 (en) * | 2019-07-26 | 2023-05-12 | トヨタ自動車株式会社 | Control device |
US11258731B2 (en) * | 2019-08-22 | 2022-02-22 | Orion Labs, Inc. | Bot proxy for group communication service |
US11379529B2 (en) * | 2019-09-09 | 2022-07-05 | Microsoft Technology Licensing, Llc | Composing rich content messages |
USD939546S1 (en) * | 2019-10-02 | 2021-12-28 | Google Llc | Display screen with transitional graphical user interface |
USD938976S1 (en) | 2019-10-02 | 2021-12-21 | Google Llc | Display screen with transitional graphical user interface |
USD938975S1 (en) | 2019-10-02 | 2021-12-21 | Google Llc | Display screen with transitional graphical user interface |
USD930679S1 (en) * | 2019-10-02 | 2021-09-14 | Google Llc | Display screen with transitional graphical user interface |
USD941327S1 (en) * | 2019-10-02 | 2022-01-18 | Google Llc | Display screen with transitional graphical user interface |
USD930680S1 (en) * | 2019-10-02 | 2021-09-14 | Google Llc | Display screen with transitional graphical user interface |
USD949887S1 (en) * | 2019-10-02 | 2022-04-26 | Google Llc | Display screen with transitional graphical user interface |
JP7423277B2 (en) | 2019-11-28 | 2024-01-29 | キヤノン株式会社 | Server system and program that suppresses printing proposal inquiries |
JP7264799B2 (en) * | 2019-12-12 | 2023-04-25 | トヨタ自動車株式会社 | Server device, information processing system, program for terminal device, and method of operating information processing system |
US10841251B1 (en) * | 2020-02-11 | 2020-11-17 | Moveworks, Inc. | Multi-domain chatbot |
JP7287333B2 (en) * | 2020-04-06 | 2023-06-06 | トヨタ自動車株式会社 | Control device, program, and information processing method |
US11151195B1 (en) * | 2020-05-30 | 2021-10-19 | CareerAmerica, LLC | Method and system for predicative QandA and resource suggestions |
US11888790B2 (en) * | 2020-06-26 | 2024-01-30 | Cisco Technology, Inc. | Dynamic skill handling mechanism for bot participation in secure multi-user collaboration workspaces |
US11410192B2 (en) | 2020-08-27 | 2022-08-09 | Google Llc | Group action fulfillment across multiple user devices |
US11271829B1 (en) | 2020-11-19 | 2022-03-08 | Kyndryl, Inc. | SLA-aware task dispatching with a task resolution control |
US11329933B1 (en) | 2020-12-28 | 2022-05-10 | Drift.com, Inc. | Persisting an AI-supported conversation across multiple channels |
KR20220101856A (en) * | 2021-01-12 | 2022-07-19 | 삼성전자주식회사 | Electronic device and method for providing user interface |
CN113190307A (en) * | 2021-04-14 | 2021-07-30 | 北京达佳互联信息技术有限公司 | Control adding method, device, equipment and storage medium |
CN113204302B (en) * | 2021-04-14 | 2023-05-12 | 北京达佳互联信息技术有限公司 | Virtual robot-based operation method, device, equipment and storage medium |
CN113206781B (en) * | 2021-04-14 | 2023-03-10 | 北京达佳互联信息技术有限公司 | Client control method, device, equipment and storage medium |
FR3127828A1 (en) * | 2021-10-06 | 2023-04-07 | Orange | Conversational agent in interface between a machine and users |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20150046100A (en) * | 2012-08-10 | 2015-04-29 | 뉘앙스 커뮤니케이션즈, 인코포레이티드 | Virtual agent communication for electronic devices |
CN104978383A (en) * | 2015-02-12 | 2015-10-14 | 腾讯科技（深圳）有限公司 | Data interworking method and data interworking equipment |
Family Cites Families (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7630986B1 (en) * | 1999-10-27 | 2009-12-08 | Pinpoint, Incorporated | Secure data interchange |
US7454469B2 (en) * | 2003-12-22 | 2008-11-18 | International Business Machines Corporation | Method and system for instant messaging Bots specification using state transition methodology and XML |
EP2076874A4 (en) * | 2006-05-13 | 2011-03-09 | Sap Ag | Consistent set of interfaces derived from a business object model |
US8769279B2 (en) * | 2006-10-17 | 2014-07-01 | Verifone, Inc. | System and method for variable length encryption |
WO2011011466A1 (en) * | 2009-07-21 | 2011-01-27 | Wms Gaming, Inc. | Integrating social communities and wagering games |
US8224901B2 (en) * | 2009-12-14 | 2012-07-17 | International Business Machines Corporation | Method and apparatus for enhancing compound documents with questions and answers |
US20120042263A1 (en) * | 2010-08-10 | 2012-02-16 | Seymour Rapaport | Social-topical adaptive networking (stan) system allowing for cooperative inter-coupling with external social networking systems and other content sources |
KR101402506B1 (en) * | 2011-12-01 | 2014-06-03 | 라인 가부시키가이샤 | System and method for providing information interactively by instant messaging application |
KR101718176B1 (en) * | 2012-08-07 | 2017-03-20 | 라인 가부시키가이샤 | System and method for providing profit-sharing advertisement system in mobile messenger platform |
CN103595611B (en) * | 2012-08-13 | 2018-10-02 | 腾讯科技（深圳）有限公司 | The realization method and system and equipment of instant messaging application |
US10447711B2 (en) * | 2012-10-18 | 2019-10-15 | White Ops Inc. | System and method for identification of automated browser agents |
US8984080B1 (en) * | 2013-04-09 | 2015-03-17 | Kabam, Inc. | Facilitating user configured assistance requests through a chat in a virtual space |
CN104869047B (en) * | 2014-02-21 | 2018-03-23 | 联想(北京)有限公司 | A kind of information providing method, information processing method, server and electronic equipment |
KR20150035877A (en) * | 2015-02-25 | 2015-04-07 | 네이버 주식회사 | Method, system and recording medium for transaction processing using real time conversation |
US10617948B2 (en) * | 2015-03-27 | 2020-04-14 | Unonimous, Inc. | System and data collection method |
JP6753728B2 (en) * | 2016-08-23 | 2020-09-09 | Ｌｉｎｅ株式会社 | Programs, information processing methods, and terminals |
-
2017
- 2017-09-19 WO PCT/US2017/052336 patent/WO2018057537A1/en unknown
- 2017-09-19 CN CN201780009158.7A patent/CN109691034B/en active Active
- 2017-09-19 EP EP17780571.0A patent/EP3378204B1/en active Active
- 2017-09-19 US US15/709,439 patent/US10798028B2/en active Active
- 2017-09-19 KR KR1020187036133A patent/KR102197448B1/en active IP Right Grant
- 2017-09-19 KR KR1020187017883A patent/KR101961754B1/en active IP Right Grant
- 2017-09-19 JP JP2018539989A patent/JP6605151B2/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20150046100A (en) * | 2012-08-10 | 2015-04-29 | 뉘앙스 커뮤니케이션즈, 인코포레이티드 | Virtual agent communication for electronic devices |
CN104978383A (en) * | 2015-02-12 | 2015-10-14 | 腾讯科技（深圳）有限公司 | Data interworking method and data interworking equipment |
Also Published As
Publication number | Publication date |
---|---|
KR20190045092A (en) | 2019-05-02 |
KR101961754B1 (en) | 2019-03-25 |
EP3378204B1 (en) | 2019-12-25 |
CN109691034A (en) | 2019-04-26 |
EP3378204A1 (en) | 2018-09-26 |
WO2018057537A1 (en) | 2018-03-29 |
US10798028B2 (en) | 2020-10-06 |
JP2019519822A (en) | 2019-07-11 |
JP6605151B2 (en) | 2019-11-13 |
US20180083894A1 (en) | 2018-03-22 |
KR20180096646A (en) | 2018-08-29 |
KR102197448B1 (en) | 2020-12-31 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109691034B (en) | Robot interaction | |
CN109716727B (en) | Method and system for obtaining permission to access data associated with a user | |
JP6625789B2 (en) | Automatic Proposal Response to Received Image in Message Using Language Model | |
US11823087B1 (en) | Network security linkage | |
CA2999230A1 (en) | Interactive user interface based on analysis of chat messages content | |
CN112534837B (en) | System and method for providing a flexible and integrated communication, scheduling and commerce platform | |
US10291575B2 (en) | Dynamic authorization using internet-based social networks | |
US11481837B1 (en) | Authentication circle management | |
US11740853B1 (en) | Smart table system utilizing extended reality | |
US10560402B2 (en) | Communications system with common electronic interface | |
US11853933B1 (en) | Systems and methods for an interactive customer interface utilizing customer device context | |
CN110709869B (en) | Suggested items for use with embedded applications in chat conversations |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |