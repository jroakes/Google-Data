CN109791600B - Method for converting horizontal screen video into vertical screen mobile layout - Google Patents
Method for converting horizontal screen video into vertical screen mobile layout Download PDFInfo
- Publication number
- CN109791600B CN109791600B CN201780051565.4A CN201780051565A CN109791600B CN 109791600 B CN109791600 B CN 109791600B CN 201780051565 A CN201780051565 A CN 201780051565A CN 109791600 B CN109791600 B CN 109791600B
- Authority
- CN
- China
- Prior art keywords
- region
- frame
- score
- media
- feature
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 title claims abstract description 79
- 230000004044 response Effects 0.000 claims abstract description 9
- 230000033001 locomotion Effects 0.000 claims description 54
- 239000013598 vector Substances 0.000 claims description 18
- 230000002123 temporal effect Effects 0.000 claims description 5
- 230000008859 change Effects 0.000 description 34
- 230000003068 static effect Effects 0.000 description 24
- 238000001514 detection method Methods 0.000 description 22
- 238000004458 analytical method Methods 0.000 description 14
- 238000010191 image analysis Methods 0.000 description 14
- 238000012015 optical character recognition Methods 0.000 description 14
- 230000001815 facial effect Effects 0.000 description 13
- 238000013500 data storage Methods 0.000 description 12
- 238000012545 processing Methods 0.000 description 12
- 238000004590 computer program Methods 0.000 description 9
- 230000003287 optical effect Effects 0.000 description 9
- 238000012731 temporal analysis Methods 0.000 description 9
- 238000010586 diagram Methods 0.000 description 8
- 238000006243 chemical reaction Methods 0.000 description 7
- 230000008569 process Effects 0.000 description 7
- 241001465754 Metazoa Species 0.000 description 6
- 239000003086 colorant Substances 0.000 description 6
- 238000007781 pre-processing Methods 0.000 description 6
- 230000000007 visual effect Effects 0.000 description 6
- 238000013135 deep learning Methods 0.000 description 5
- 230000004927 fusion Effects 0.000 description 4
- 230000005540 biological transmission Effects 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000012549 training Methods 0.000 description 3
- 238000013528 artificial neural network Methods 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 238000004422 calculation algorithm Methods 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000007704 transition Effects 0.000 description 2
- 241000282465 Canis Species 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G5/00—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators
- G09G5/36—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators characterised by the display of a graphic pattern, e.g. using an all-points-addressable [APA] memory
- G09G5/37—Details of the operation on graphic patterns
- G09G5/373—Details of the operation on graphic patterns for modifying the size of the graphic pattern
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04845—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range for image manipulation, e.g. dragging, rotation, expansion or change of colour
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
- G06T11/20—Drawing from basic elements, e.g. lines or circles
-
- G06T3/10—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/10—Segmentation; Edge detection
- G06T7/11—Region-based segmentation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/20—Image preprocessing
- G06V10/25—Determination of region of interest [ROI] or a volume of interest [VOI]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20112—Image segmentation details
- G06T2207/20132—Image cropping
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2210/00—Indexing scheme for image generation or computer graphics
- G06T2210/12—Bounding box
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2340/00—Aspects of display data processing
- G09G2340/04—Changes in size, position or resolution of an image
- G09G2340/0442—Handling or displaying different aspect ratios, or changing the aspect ratio
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2340/00—Aspects of display data processing
- G09G2340/04—Changes in size, position or resolution of an image
- G09G2340/045—Zooming at least part of an image, i.e. enlarging it or shrinking it
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N7/00—Television systems
- H04N7/01—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level
- H04N7/0117—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level involving conversion of the spatial resolution of the incoming video signal
- H04N7/0122—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level involving conversion of the spatial resolution of the incoming video signal the input and the output signals having different aspect ratios
Abstract
Systems and methods for tailoring specifically oriented media using a computing device are described. A method may first include receiving, by a video preprocessor of a device, a first media frame of a first orientation. A first region including a first feature within a first frame may be identified by an image analyzer. The cropping calculator of the device may generate a score for the first region based on the characteristic of the first feature and determine that the score for the first region exceeds a threshold. Then, in response to determining that the score of the first region exceeds a threshold, the image processor of the apparatus may crop the first frame of video to include the first region within a predetermined display region, the predetermined display region comprising a subset of the first frame in the second orientation.
Description
Cross Reference to Related Applications
The application claims the benefit and priority of PCT application No. PCT/US2016/065025, filed on month 12 of 2016, 5, entitled "Method for Converting Landscape Video to Portrait Mobile Layout", the entire contents of which are incorporated herein by reference.
Background
In a network environment such as the internet or other network, a first party content provider may provide publicly presented information on a resource (such as a web page, document, application, and/or other resource). The first-party content may contain text, video, and/or audio information provided by the first-party content provider via the resource server for presentation on the client device over the internet. Video or similar media recorded at a wide aspect ratio may be designed to be viewed on a desktop or in a landscape orientation, which cannot fit a full screen directly onto a mobile device held in a portrait or portrait orientation, and typically would be cropped to the center, losing detail at the left and right edges of the video, or surrounded by black bars at the top and bottom, reducing the display size of the video. Vertically oriented media is a popular format for viewing and displaying media in many applications. Since many videos and other media are recorded only in a wide aspect ratio layout, there is a large inventory (inventory) of the layout, however the needs of publishers are increasingly requiring a vertical screen layout.
Disclosure of Invention
One embodiment relates to a method of tailoring specific orientation media using a computing device. The method may include receiving, by a video pre-processor of a device, a first media (e.g., video) frame of a first orientation. A first region within the first frame including the first feature may be identified by the image analyzer. The cropping calculator of the device may generate a score for the first region based on the characteristic of the first feature and determine that the score for the first region exceeds a threshold. Then, in response to determining that the score of the first region exceeds a threshold, the image processor of the apparatus may crop the first frame of video to include the first region within a particular display region, the particular display region including a subset of the first frame in the second orientation.
In some implementations, cropping the media may also include filling one or more edges of the media content to expand the original content. In some implementations, the media is video and the filling includes expanding the original scene content within the video frames. Extending the original scene content may include extending the background color to fill the video frame with the background color.
The first feature may comprise text. Generating the score for the first region based on the characteristics of the first feature may further include generating, by the cropping calculator, a score proportional to the size of the text. Additionally or alternatively, generating a score for the first region based on the characteristics of the first feature may further include generating, by the cropping calculator, a score inversely proportional to a distance of the text from a center of the frame.
The first feature may include a face. Generating the score for the first region based on the characteristics of the first feature may further include generating, by the cropping calculator, the score based on the size of the face relative to the frame.
The method may further comprise: a second region within the first frame that includes a second feature is identified by the image analyzer, a second score for the second region is generated by the cropping calculator, the second score is determined by the cropping calculator to be less than a threshold, and the second region is cropped from the first frame by the image processor in response to the determination.
The method further comprises the steps of: identifying, by the image processor, a second region within the first frame that includes a second feature, generating, by the cropping calculator, a second score for the second region, determining, by the cropping calculator, that the second score exceeds a threshold; and wherein cropping the first frame may further comprise: in response to determining that the score of the first region and the second score of the second region each exceed a threshold, a display region including a second orientation of the first region and the second region is identified by the image analyzer, and a boundary of the first frame to the identified display region is cropped by the image processor.
Identifying a second oriented display region comprising a first region and a second region may comprise: determining, by the cropping calculator, that the score of the first region is higher than the second score of the second region; generating, by the image processor, an intermediate display region centered on the first feature in the second orientation, the intermediate display region having a predetermined size; and adjusting, by the image processor, the position of the intermediate display region within the frame to include the first region and the second region while maintaining the predetermined size and the second orientation.
The method may further comprise: receiving, by the video preprocessor, a second frame of the video of the first orientation; identifying, by the image analyzer, a second location of the first region including the first feature in the second frame; and generating, by the cropping calculator, a second score for the first region based on an amount of movement of the first region between the first frame and the second frame; and wherein determining that the score of the first region exceeds the threshold further comprises adding the score of the first region to the second score of the first region.
The method may further include identifying, by the temporal image processor, a global motion vector from differences between pixels of the first frame and pixels of the second frame; and wherein generating the second score comprises generating, by the cropping calculator, a score proportional to a difference between the global motion vector and a movement of the first region between the first frame and the second frame.
Another embodiment relates to a system for tailoring specifically oriented media using a computing device. The system may include one or more of one or more processors of the device, a network interface electrically connected to the one or more processors, and one or more of computer storage devices electrically connected to the one or more processors storing instructions. The instructions, when executed by one or more processors, may cause the one or more processors to perform operations comprising the methods described above.
Yet another embodiment relates to a computer-readable storage device storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations. The operations may comprise operations comprising the aforementioned methods.
Accordingly, disclosed herein are systems and methods for intelligent or "smart" cropping to transition between landscape and portrait images and videos by identifying regions of interest in the video. Various metrics may be used to identify regions of interest, including facial recognition, text recognition or optical character recognition, object detection, motion analysis, identification of static regions in an image (such as boundaries or banners), and entropy analysis. Aspects and embodiments provide improved generation of video suitable for display on a device.
Drawings
The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the disclosure will become apparent from the description, the drawings, and the claims, in which:
FIG. 1 is a block diagram depicting an embodiment of an environment for automatically converting media from one orientation to another.
Fig. 2 is a diagram depicting cropping of a single media frame in an embodiment of a use case.
FIG. 3 is a block diagram depicting an embodiment of software and/or hardware modules configured for media preprocessing, media analysis, and tailoring received media.
FIG. 4 is a flow chart depicting an embodiment of a method of cropping a media frame.
FIG. 5 is a flow chart depicting an embodiment of a method of cropping a media frame by determining a score for each of a plurality of regions.
FIG. 6 is a flow chart depicting an embodiment of a method of generating or updating a score based on movement of a feature.
FIG. 7 is a flow chart depicting an embodiment of a method for cropping a media frame using received metadata.
FIG. 8 is a flow chart depicting an embodiment of a method of adjusting cropping based on a change in orientation.
FIG. 9 is a block diagram depicting a general architecture for a computing system that may be used to implement the various elements and methods of the systems described and illustrated herein.
It will be appreciated that some or all of the figures are schematic representations for purposes of illustration. The drawings are provided to illustrate one or more embodiments and are not to be clearly understood as limiting the scope or meaning of the claims.
Detailed Description
The following is a more detailed description of various concepts related to, and embodiments of, methods, apparatus, and systems for providing information over a computer network. The various concepts introduced above and discussed in more detail below may be implemented in any of a variety of ways, as the described concepts are not limited to any particular implementation. The various embodiments and applications are provided primarily for illustrative purposes.
The "Dumb" clip to the center of an image or video relies on the assumption that the most important content of the video or image appears in the center of the image. This may be incorrect because important content may be off-center, such as text at the bottom of the screen or a person in a horizontal "one third" position in the frame. The video showing the person entering the room may contain the person entering from one side of the frame while the rest of the frame is stationary, or the video with two people talking may place each person on the side of the frame with empty space in between. In this case the application center cropping may result in an empty room or empty space, which may be confusing and frustrating to the viewer.
Accordingly, disclosed herein are systems and methods for intelligent or "smart" cropping to automatically transition video or images in landscape mode to adapt portrait mode (or vice versa) while maintaining full screen rather than just applying "non-intelligent" cropping or center cropping or adding padding at the top/bottom. The conversion may involve detecting a significant portion (e.g., feature) of the image or video for each frame. Based on the identified important regions, the image or video may be intelligently cropped or populated to preserve important features while non-important regions, static boundaries, etc. are discarded. The detected features may include facial features, object detection and/or recognition, text detection, dominant color detection, motion analysis, scene change detection, and image saliency. The detection and identification may use deep learning based methods and algorithms. Text detection may use Optical Character Recognition (OCR). Feature detection allows for optimal clipping paths. Other aspects of the invention may include filling in the image to match background colors, and removing and/or reformatting any boundaries to adapt to new display modes. Although discussed primarily in terms of video, in many embodiments the system may be applied to individual images or frames.
Fig. 1 is a schematic diagram of a network 106 a block diagram of an embodiment of environment 100 that automatically converts video from one orientation to another. Network 106 may comprise a Local Area Network (LAN), wide Area Network (WAN), telephone network (such as the Public Switched Telephone Network (PSTN)), wireless link, intranet, the internet, or a combination thereof. The environment 100 also includes a mobile device 102. In some implementations, the mobile device 102 includes a processor 122, a data storage 124, a network interface 126, a display 128, an input/output module 130, a sensor module 132, and a media module 134. The sensor module 132 may be configured to include sensors (e.g., accelerometers and/or magnetometers) to detect orientation of the computing device as well as other similar sensors included in many mobile devices. The processor 122 may comprise a microprocessor, an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), or the like, or a combination thereof. The data storage 124 may include, but is not limited to, electronic, optical, magnetic, or any other storage or transmission device capable of providing program instructions to a processor. The memory may include a floppy disk, a compact disk read-only memory (CD-ROM), a Digital Versatile Disk (DVD), a magnetic disk, a memory chip, a read-only memory (ROM), a Random Access Memory (RAM), an electrically erasable programmable read-only memory (EEPROM), an erasable programmable read-only memory (EPROM), a flash memory, an optical media, or any other suitable memory from which the processor 122 can read instructions. The instructions may comprise code from any suitable computer programming language, such as, but not limited to C, C ++, C#, HTML、XML、/>Visual->
The mobile device 102 may include one or more devices, such as computers, laptops, smartphones, tablets, personal digital assistants, configured to communicate with other devices via the network 106. The device canIs any form of portable electronic device that includes a data processor and memory. The data storage 124 may store machine instructions that, when executed by the processor, cause the processor to perform one or more operations described herein. The data store 124 may also store data to enable presentation of one or more resources, content items, etc. on the computing device. The processor may comprise a microprocessor, an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), or the like, or a combination thereof. The data storage 124 may include, but is not limited to, electronic, optical, magnetic, or any other storage or transmission device capable of providing program instructions to a processor. The data storage 124 may comprise a floppy disk, compact disc read only memory (CD-ROM), digital Versatile Disc (DVD), magnetic disk, memory chip, read Only Memory (ROM), random Access Memory (RAM), electrically Erasable Programmable Read Only Memory (EEPROM), erasable Programmable Read Only Memory (EPROM), flash memory, optical media, or any other suitable memory from which a processor can read instructions. The instructions may contain code from any suitable computer programming language, such as, but not limited to C、C++、C#、HTML、/>VisualXML.
Mobile device 102 may execute a software application (e.g., a web browser or other application) to retrieve content from other computing devices over network 106. Such an application may be configured to retrieve first-party content from the media server system 104. In some cases, the application running on the mobile device 102 may itself be first party content (e.g., games, media players, etc.). In one implementation, the mobile device 102 may execute a web browser application that provides a browser window on a display of a client device. A web browser application providing a browser window may operate by receiving input of a Uniform Resource Locator (URL), such as a web address, from an input device, such as a pointing device, keyboard, touch screen, or other form of input device. In response, the one or more processors of the client device executing instructions from the web browser application may request data from another device (e.g., media server system 104) coupled to network 106 referenced by the URL address. Other devices may then provide web page data and/or other data to mobile device 102, which causes visual indicia to be displayed by the display of mobile device 102. Accordingly, the browser window displays the retrieved first-party content (such as web pages from various websites) to facilitate user interaction with the first-party content.
In some implementations, the media module 134 of the mobile device 102 is configured to receive a plurality of media frames and associated metadata. The media may be received through the network interface 126 and stored in the digital storage device 124. In some implementations, the media frames are received as part of streaming media data. Streaming media may be received via network interface 146. In some implementations, the media module 134 is configured to identify regions in the frame based on the received metadata. In some implementations, the media module 134 is configured to clip the media frames based on the region. In some implementations, the cropped regions are also based on one or more media frames preceding and/or following the media frame.
In some implementations, the media module 134 of the mobile device 102 is configured to receive an indication of a change in orientation from the one or more sensor modules 132. In some implementations, the media module 134 is configured to dynamically adjust the cropping of the play media based on the change in orientation.
The media server system 104 may include a processor 142, a data store 144, a network interface 146, a content selection module 148, a media cropping module 150, a metadata module 152, and a media content database 154. In some implementations, the content selection module 148 of the media server system 104 is configured to select media from the media content database 154. In some implementations, the media cropping module 150 is configured to pre-process media, analyze characteristics and/or objects of the media, and crop the media based on the analysis of the characteristics and/or objects. In some implementations, the metadata module 152 is configured to extract data based on preprocessing the media, analyzing features and/or objects of the media, and determining a clipping path for a target aspect ratio or resolution. Although shown on the media server system 104, in many embodiments, the media cropping module 150 may be executed on one or more mobile devices 102.
The media server system is shown as including a media cropping module 150. In some implementations, the media cropping module 150 is configured to pre-process media, analyze characteristics and/or objects of the media, and crop the media based on the analysis of the characteristics and/or objects. In some embodiments, the media cropping module 150 is configured to determine whether cropping is required based on whether one or more values of the target aspect ratio are less than the current value of the aspect ratio of the analyzed media frame. In some implementations, the media cropping module 150 is configured to crop frames of the media only if one or more values of the target resolution are less than a current value of the resolution of the media frames. In some implementations, the media cropping module 150 is configured to crop the media to match the target aspect ratio or to match the target resolution. The media cropping module 150 may be configured to add additional padding to one or more sides of the cropped media frame to match the target aspect ratio or to match the target resolution. In some implementations, the media cropping module 150 may be configured to also base the cropped regions on one or more media frames preceding and/or following the current media frame being cropped. In some implementations, the media cropping module 150 is configured to contain one or more regions that exceed a threshold. In some implementations, the media cropping module 150 is configured to include at least one or more of the plurality of regions having a score exceeding a threshold when the media cropping module 150 determines the region to include when cropping the media frame.
In some implementations, the metadata module 152 is configured to extract data based on preprocessing the media, analyzing features and/or objects of the media, and determining a clipping path for a target aspect ratio or resolution. In some implementations, the metadata module 152 is configured to receive metadata as part of a media file containing a plurality of media frames. In some implementations, the metadata module 152 is configured to receive metadata independently, and an identifier or other data that associates the metadata with the received plurality of media frames. In some implementations, the metadata module 152 is configured to analyze metadata to determine data portions related to one or more associated regions in a media frame associated with media. In some implementations, the metadata module 152 is configured to extract boundary information of one or more regions for each of a plurality of media frames contained in the metadata. In some implementations, the metadata module 152 is configured to extract a location of one or more features within each of the plurality of media frames. Features may include objects such as automobiles, buildings, people, animals, street signs, etc., text, boundaries of media frames, consistent color fills of one or more sides of media frames, etc. In some implementations, the metadata module 152 is configured to identify a plurality of features and/or regions of one or more of a plurality of frames of media. In some implementations, the metadata module 152 is configured to associate the received metadata with a target aspect ratio or target resolution.
Fig. 2 depicts cropping a single media frame in an embodiment of a use case. The input image 202 is shown in a horizontal or landscape orientation. Once the input image 202 is processed to detect important objects and/or features of the input image 202, the regions containing the important objects and/or features are preserved as much as possible when generating the output image 204. In fig. 2, the output image 204 is shown in a vertical or portrait screen orientation on the mobile device 102. In the depiction of the embodiment of this use case, facial features are identified in the input image 202 and the region including the facial features remains in the output image 204 displayed on the mobile device 102.
FIG. 3 is a block diagram of an embodiment of software and/or hardware modules for media preprocessing, media analysis, and cropping of received media. In some implementations, the preprocessing module 310 is configured to preprocess and down-convert the media using a down conversion (down conversion) module 312, a down conversion module 314, a down conversion module 316, a down conversion module 318, and the like. In some implementations, the preprocessing module 310 is configured to send the resulting output to one or more of the temporal analysis module 320 and the image analysis module 330. The temporal analysis module 320 may include a scene change module 322 and a static boundary module 324. The image analysis module 330 may include an OCR module 332, an object detection module 334, a face tracking module 336, a motion analysis module 338, and an entropy module 345. The temporal analysis module 320 and the image analysis module 330 may be configured to send their data results to the signal fusion calculator 350 and the cropping calculator 352. Although shown separately, in many embodiments, the temporal analysis module and the image analysis module may be part of the same analyzer system or module. Similarly, components shown within the temporal analysis module and the image analysis module may be separate from the temporal analysis module or the image analysis module, or may be provided by other modules. In some implementations, the image analysis 330 module is configured to include a deep-learning inference model that can be trained using input data. In some implementations, the input data can be entered based on the marked or selected region.
In some implementations, the temporal analysis 320 module may include an application, applet, service, server, daemon, routine, or other executable logic for performing analysis with respect to a sequence of images (such as images of a video). The temporal analysis 320 module may include a scene change module 322 configured to analyze a plurality of media frames to determine a scene change. The scene change module 322 may include an application, applet, service, server, daemon, routine, or other executable logic for identifying differences between successive images that indicate scene changes or significant breaks in video. In some implementations, the scene change module 322 is configured to determine a scene change by using keypoint detection to analyze when there is a large change in keypoints that indicate a scene break or scene change. In some implementations, the scene change module 322 is configured to compare all pixels in one frame to pixels in a consecutive frame and indicate a scene change if more than a particular threshold of pixels are different when considered as part of optical flow. In some implementations, the scene change module 322 is configured to calculate motion vectors between multiple media frames, and the lack of consecutive motion vectors between consecutive frames indicates a scene change. Features may then be identified within the particular scene and regions containing the particular features tracked among a plurality of media frames within the particular scene. In some implementations, the scene change module 322 is configured to track information of the location of a particular feature within a plurality of media frames, and such information is also used to determine where to crop the media frames based on the region.
In some implementations, the time domain analysis module 320 includes a static boundary module 324 configured to analyze the plurality of media frames to determine whether and where a static boundary exists. The static boundary module 324 may include an application, applet, service, server, daemon, routine, or other executable logic for identifying a static boundary that remains substantially unchanged between successive images that indicates a boundary on at least one edge of a frame. In some implementations, the static boundary module 324 is configured to receive a plurality of media frames and to analyze the plurality of media frames to find a static boundary along edges of the plurality of frames. In some implementations, the static boundary module 324 is configured to locate the boundary by selecting one or more random pixels and compare the line of pixels vertically and/or horizontally to the random pixels to determine whether there is a continuous line of pixels that is close to the randomly selected pixel color. In some embodiments, such lines of pixels may extend over the entire image, or over a portion of the image (e.g., a quarter of the image). In some implementations, the static boundary module 324 is configured to locate boundaries that are static from one frame to the next and contain pixels of relatively consistent color. In some implementations, the static boundary module 324 is configured to locate boundaries that are static from one frame to the next and contain pixels of relatively consistent color, but that also contain some additional static information, such as text embedded in boundaries of different colors. Once the border is located, whether or not it contains embedded text, it can be considered an image during the cropping process.
In some implementations, the image analysis module 330 includes an Optical Character Recognition (OCR) module 332 configured to detect text embedded in the image data. The image data may be one or more media frames such as video. OCR module 332 may include an application, applet, service, server, background program, routine, or other executable logic for identifying text embedded in image data of one or more media frames. In some implementations, the OCR module 332 can compare a predetermined vector or bitmap image corresponding to letters to a portion of the image, such as via a sliding window. In some implementations, the OCR module 332 may select a reference image (e.g., letter) based on a previous letter (e.g., according to a text prediction system), which may improve efficiency.
In some implementations, the image analysis module 330 includes an object detection module 334 configured to use a neural network trained on different objects, such as via reference images of tens, hundreds, or thousands of objects. The object detection module 334 may include an application, applet, service, server, daemon, routine, or other executable logic for identifying visual objects (i.e., data that when displayed creates a visual representation of an object) in one or more media frames. The object detection module 334 may be configured to detect objects such as automobiles, buildings, people, animals, street signs, etc., text, boundaries of media frames, consistent color fills of one or more sides of media frames, etc. The neural network may identify similar elements in an image of the object and create a classification of the elements representing the object, which may then be used to identify the object in the new image. The image analysis module 330 may generate a bounding box around the identified objects such that the bounding box may be tracked from image to image.
In some implementations, the image analysis module 330 includes a face tracking module 336 configured to receive a plurality of media frames and analyze the plurality of media frames to detect facial features, for example, via an eigenface (eigenface) or similar structure. The face tracking module 336 may include an application, applet, service, server, daemon, routine, or other executable logic for identifying similarities between one or more successive frames of media that create a visual representation of one or more faces and relative movement of one or more faces when displayed. Face tracking may then be achieved by matching facial features in each of the plurality of media frames according to the facial features.
In some implementations, the image analysis module 330 includes a motion analysis module 338 configured to analyze the motion of the object detected in the plurality of media frames and calculate motion vectors between the plurality of media frames. The motion analysis module 338 may include an application, applet, service, server, daemon, routine, or other executable logic for identifying similarities between one or more successive frames of media that create a visual representation of one or more objects and relative motion of one or more objects when displayed. In some implementations, the motion analysis module 338 is configured to calculate a global motion vector from a difference from a pixel in a region of the first media frame to a pixel of the second media frame.
In some implementations, the image analysis module 330 includes an entropy module 340 configured to analyze the entropy of each of the plurality of media frames and calculate differences in entropy (i.e., measures of changes or differences that have occurred from one frame to another) to determine key frames. Entropy module 340 may include an application, applet, service, server, daemon, routine, or other executable logic for analyzing entropy of one or more media frames. In some implementations, the entropy module 340 is configured to analyze entropy between the identified regions of the media frame to calculate differences in entropy to determine key regions. In some implementations, the entropy 340 module is configured to extract a value from the plurality of media frames that characterizes randomness of motion vectors associated with regions in the frames, allowing the plurality of media frames to be partitioned into different events (e.g., scene changes in the video).
In some implementations, the signal fusion calculator module 350 is configured to combine the data from the temporal analysis module 320 and the image analysis module 330 and determine important objects and/or features of the entire scene including the plurality of media frames. The cropping calculator module 352 may then use the combined data to crop the plurality of media frames to regenerate the media. In some implementations, the media is regenerated into video of the target aspect ratio. In some implementations, the signal fusion calculator module 350 is configured to assign weights to different outputs of the analyzer. The signal fusion calculator module 350 may normalize the different outputs to values that have been determined by the deep learning method by specifying a range.
Method for automatically cutting media
Fig. 4 is a flow chart of an embodiment of a method 400 of cropping a media frame. In some implementations, the method 400 is implemented by the processor 142 of the media server system 104 executing instructions stored on the data storage 144, and media extracted from the media content database 154 may be used. Briefly, the method 400 includes receiving a media frame at 402 and identifying an area in the frame that includes a feature at 404. If additional regions are identified at 406, the method returns to identifying regions in the frame that include features at 404. If regions are not identified at 406, the method continues at 408 with cropping the media frame based on the identified one or more regions.
Still referring to fig. 4 and in more detail, method 400 begins when a media frame is received at 402. In some implementations, the media is a media file (e.g., a video file) containing a plurality of media frames. The media may be stored in the media content database 154 or retrieved through the network interface 146. In some implementations, the media frames are received as part of streaming media data. Streaming media may be received via network interface 146. In some implementations, the media frames are part of a list of stored media, and each media is processed in turn. In some implementations, a determination is first made as to whether clipping and/or processing of the media is required. The determination may be accomplished by comparing the stored dimensions, aspect ratio, resolution, etc. of the stored media to a target value.
At 404, regions in the frame that include features are identified. In some implementations, features are identified by analyzing frames using facial recognition. In some implementations, features are identified by analyzing the text of the frame using optical character recognition. In some implementations, features are identified by analyzing objects (e.g., cars, buildings, people, animals, street signs, etc.) of a frame using object recognition. In some implementations, features are identified by analyzing boundaries of frames, and/or fills (e.g., boundaries of consistent or near-consistent colors at one or more edges of frames). In some implementations, frames are analyzed to identify a plurality of features. Features may be of different types (e.g., face, text, objects, etc.). If additional regions are identified at 406, the method returns to identifying additional regions including features in the frame at 404.
If additional regions cannot be identified at 406, the media frame is cropped based on the one or more regions at 408. In some implementations, the media frame is cropped only if one or more values of the target aspect ratio are less than the current value of the aspect ratio of the media frame. In some implementations, the media frame is cropped only if one or more values of the target resolution are less than the current value of the resolution of the media frame. In some implementations, the media frames are cropped to match the target aspect ratio or to match the target resolution. Additional padding may be added to one or more sides of the cropped media frame to match the target aspect ratio or to match the target resolution. In some implementations, the cropped regions are also based on one or more of the media frames preceding and/or following the media frame.
In some implementations, some padding may be added to meet the target aspect ratio during cropping. In some implementations, if static boundaries exist on one or more edges of a media frame, they may be moved or reformatted to form and/or be part of the padding.
In some implementations, a plurality of media frames are received and analyzed to determine a scene change. Keypoint detection may be used to analyze when a large change occurs in a keypoint that indicates a scene break or scene change. In some implementations, the comparison of all pixels in one frame is compared to pixels in consecutive frames, and if pixels that are more than some threshold are different when considered as part of optical flow, it indicates a scene change. In some implementations, motion vectors are calculated between multiple media frames, and the lack of consecutive motion vectors between consecutive frames indicates a scene change. Features may then be identified within the particular scene and regions containing the particular features tracked among a plurality of media frames within the particular scene. In an embodiment, the information that tracks the location of a particular feature within a plurality of media frames is also used to determine where to clip the media frames based on the region.
In some implementations, a plurality of media frames are received and analyzed to identify facial features. Face tracking may then be implemented by matching facial features in each of the plurality of media frames by tracking the facial features.
In some implementations, a plurality of media frames are received and analyzed to find static boundaries along edges of the plurality of frames. In some embodiments, to locate the boundary, random pixels are selected and the pixel lines are compared vertically and/or horizontally to the random pixels to determine if there are consecutive pixel lines that are close to the randomly selected pixel color. In some embodiments, a boundary is located that is static from one frame to the next and contains pixels of relatively consistent color. In some embodiments, boundaries may be located that are static from one frame to the next and contain pixels of relatively consistent color, but also contain some additional static information, such as text embedded in boundaries having different colors. Once the border is located, whether or not it contains embedded text, it can be treated as an image during the cropping process.
FIG. 5 is a flow chart of an embodiment of a method 500 of cropping a media frame by determining a score for each of a plurality of regions. In some implementations, the method 500 is implemented by the processor 142 of the media server system 104 executing instructions stored on the data storage 144, and media extracted from the media content database 154 may be used. Briefly, the method 500 includes receiving a media frame at 502 and identifying an area in the frame that includes a feature at 504. If additional regions are identified at 506, the method determines a score for the identified regions based on the corresponding characteristics at 508 and returns to identifying regions of the frame that include features at 504. If regions are not identified at 506, the method continues by determining that a score of one or more of the identified regions exceeds a threshold at 510, and cropping the media frame to contain one or more regions that exceed the threshold at 512.
Still referring to fig. 5 and in more detail, method 500 begins when a media frame is received at 502. In some implementations, the media is a media file (e.g., a video file) containing a plurality of media frames. The media may be stored in the media content database 154 or retrieved through the network interface 146. In some implementations, the media frames are received as part of streaming media data. Streaming media may be received via network interface 146.
At 504, regions in the frame that include features are identified. In some implementations, one or more of the features in the frame are identified by analyzing the frame using facial recognition. In some implementations, one or more of the features in the frame are identified by analyzing the text of the frame using optical character recognition. In some implementations, one or more of the features in the region are identified by analyzing objects (e.g., cars, buildings, people, animals, street signs, etc.) of the frame using object recognition. In some implementations, one or more of the features in the region are identified by analyzing boundaries of the frame, and/or the fill (e.g., boundaries of consistent or near-consistent color at one or more edges of the frame). In some implementations, each region is also analyzed to potentially identify a plurality of features in one or more regions. The features in each of the plurality of regions may be of different types (e.g., face, text, objects, etc.). If additional regions are identified at 506, the method determines a score for the identified regions based on the corresponding characteristics at 508 and returns to identifying additional regions including features in the frame at 504.
A score based on the identified regions of the respective features is determined at 508. In some implementations, the score is based on the type of feature located in the region or at least partially located in the region. In some implementations, the score is weighted based on the type of feature located in the region. The weights may be determined by using training data. In some implementations, the training data can be used as input to a deep learning inference model. In some implementations, the training data is based on data input of a selection of important media regions. Some features on which the score may be based may include the size of the feature in the region, the type of feature in the region, the motion of the feature in the region, the relative motion of the feature in the region, the amount of blur associated with the feature in the region, and so forth. In some implementations, scores are assigned to features rather than regions containing features. In some implementations, determining the score for each of the plurality of regions includes determining a ranking of the plurality of regions, at least a top-ranked region of the plurality of regions. In some implementations, determining the score for each of the plurality of regions includes ordering each of the plurality of regions from highest to lowest, wherein higher ordered regions are more likely to be adapted in any clipping of the media frame.
If additional regions cannot be identified at 506, the method determines that the score of one or more regions exceeds a threshold at 510. In some implementations, the score for each of the plurality of regions includes a value for comparison. In some implementations, the score of the region must exceed a threshold before the region is considered when cropping the media frame. In some implementations, only the regions that contain the highest scores are prioritized when clipping the media frames. In some implementations, when clipping a media frame, multiple regions are preferentially included based on their respective scores. In some implementations, a determination is made as to which region combinations produce the maximization score, where all regions can fit within a region of the cropped media frame.
If additional regions are identified at 506, the media frame is cropped to include one or more regions having an associated score that exceeds a threshold at 512. In some implementations, only regions of the plurality of regions that have scores exceeding a threshold are considered when determining regions to include when cropping the media frame. In some implementations, the media frame is cropped only if one or more values of the target aspect ratio are less than the current value of the aspect ratio of the media frame. In some implementations, the media frame is cropped only if one or more values of the target resolution are less than the current value of the resolution of the media frame. In some implementations, the media frames are cropped to match the target aspect ratio or to match the target resolution. Additional padding may be added to one or more sides of the cropped media frame to match the target aspect ratio or to match the target resolution. In some implementations, the cropped regions are also based on one or more of the media frames preceding and/or following the media frame.
FIG. 6 is a flow chart of an embodiment of a method 600 of generating or updating a score based on movement of a feature. In some implementations, the method 600 is implemented by the processor 142 of the media server system 104 executing instructions stored on the data storage 144, and media extracted from the media content database 154 may be used. Briefly, the method 600 includes receiving a plurality of media frames at 602 and identifying regions of the plurality of media frames that include the same feature at 604. If additional regions are identified at 606, the method continues with determining an amount of movement of the feature from the region at 608, and generating or updating a score for the region based on movement of the feature at 610 before returning to identifying additional regions in each of the plurality of frames including the same feature at 604. If no additional regions are identified at 606, the method stops.
Still fig. 6 and in more detail, method 600 begins when a media frame is received at 602. In some implementations, the media is a media file (e.g., a video file) containing a plurality of media frames. The media may be stored in the media content database 154 or retrieved through the network interface 146. In some implementations, the media frames are received as part of streaming media data. Streaming media may be received via network interface 146.
Regions in each of the plurality of frames that include the same feature are identified at 604. In some implementations, identifying the features as the same feature includes comparing the characteristics of the features. The characteristics of the feature may include object properties, color values, sizes, etc. In some implementations, identifying the features as the same feature is further based on a proximity of regions defining the features between frames of the plurality of frames preceding and following the frame.
If an additional region is identified at 606, a movement of the feature from the region is determined at 608. In some implementations, the amount of movement of the feature from the region is determined by the absolute position of the feature within each of the plurality of frames. In some embodiments, the amount of movement of the feature from the region is determined by the relative position of the feature within each of the plurality of frames when compared to one or more of the preceding or following frames. In some implementations, the amount of movement is determined by an increase or decrease in the size of the feature between one or more of the plurality of frames. The amount of movement between two or more frames of the plurality of frames may be determined using a combination of different ways of determining the amount of movement of the feature.
Based on the movement of the features, a score for the region is generated or updated at 610. In some implementations, the score is adjusted based on an amount of movement of the feature between two or more frames of the received plurality of frames or based on an amount of movement of the feature between two or more frames of the received plurality of frames. In some implementations, the score is adjusted by weighting an existing score of a region of the frame containing one or more features based on a movement amount determined between a plurality of frames of the one or more features. In some implementations, scores are assigned to features, rather than regions containing features. In some implementations, determining the score for each of the plurality of regions includes determining a ranking of the plurality of regions, at least a top-ranked region of the plurality of regions. In some implementations, determining the score for each of the plurality of regions includes ordering each of the plurality of regions from highest to lowest, wherein higher ordered regions are more likely to be included in any cropping of the media frame.
Using metadata
Media conversion may be performed on different computing systems, including detecting portions (e.g., features) of an image or video that are important for each frame, and intelligently cropping or filling in to preserve important features while discarding unimportant regions, still boundaries, etc. In some implementations, detection of portions of an image, video, or other media may be done on a server system and used to create metadata that associates regions or boundaries containing features with frames of the media. Based on the identified important regions, the image or video may be intelligently cropped or populated to preserve important features (such as unimportant regions, static boundaries, etc.), while unimportant regions, static boundaries, etc. are discarded on another device (e.g., mobile device). The detected features may include face tracking, object detection and/or recognition, text detection, dominant color detection, motion analysis, scene change detection, and image saliency. The detection and identification may use deep learning based methods and algorithms. Text detection may use Optical Character Recognition (OCR). Detecting features to be placed in the metadata allows for performing the best clipping path on the mobile device. Other aspects of the invention may include filling in the image to match background colors, and removing and/or reformatting any boundaries to adapt to new display modes. Although the media is discussed primarily in terms of video, in many embodiments the system may be applied to individual images or frames.
Fig. 7 is a flow chart of an embodiment of a method 700 of cropping a media frame using received metadata. In some implementations, the method 700 is performed by the processor 122 of the mobile device 102 executing instructions stored on the data storage device 124. Briefly, the method 700 includes receiving a media frame at 702, receiving metadata associated with the media at 704, identifying a region in the frame based on the received metadata at 706, cropping the media frame based on the region at 708, and receiving a next media frame at 710.
Still referring to fig. 7 and in more detail, method 700 begins when a media frame is received at 702. In some implementations, the media is a media file (e.g., a video file) containing a plurality of media frames. The media may be received through the network interface 126 and stored in the data storage 124. In some implementations, the media frames are received as part of streaming media data. Streaming media may be received via network interface 146.
Metadata associated with the media is received at 704. In some implementations, the metadata is received as part of a media file containing a plurality of media frames. In some implementations, the metadata is received independently, along with an identifier or other data that associates the metadata with the received plurality of media frames. In some implementations, the metadata includes data related to an area associated with one or more of the plurality of media frames. In some implementations, the metadata includes boundary information for one or more regions of each of the plurality of media frames. In some implementations, the metadata contains a location of one or more features within each of a plurality of frames of the media. Features may include objects such as automobiles, buildings, people, animals, street signs, etc., text, boundaries of media frames, consistent color fills of one or more sides of media frames, etc. In some implementations, the metadata can identify a plurality of features and/or regions of one or more of the plurality of media frames. In some implementations, the metadata is associated with a target aspect ratio or a target resolution. In some implementations, the metadata can identify one or more regions of the media frame. Each of the one or more identified regions may be a region determined to have a score exceeding a threshold. The score may be determined by a clipping calculator as described above.
Regions in the frame are identified based on the received metadata at 706. In some implementations, the regions in the frame are retrieved from the metadata and include features identified through the use of facial recognition. In some implementations, the regions in the frame are retrieved from the metadata and include features identified by analyzing the text of the frame using optical character recognition. In some implementations, the regions in the frames are retrieved from the metadata and include features identified by analyzing the frames of objects (e.g., automobiles, buildings, people, animals, street signs, etc.) using object recognition. In some implementations, regions in the frame are retrieved from the metadata and include features identified by analyzing boundaries, and/or fills of the frame (e.g., boundaries of consistent or near-consistent colors at one or more edges of the frame). In some implementations, an area in a frame is retrieved from metadata and includes a plurality of features. Features may be of different types (e.g., face, text, objects, etc.). In some implementations, the plurality of regions are retrieved from metadata of the media frame. In some implementations, a plurality of media frames are received and metadata is associated with the plurality of media frames.
At 708, the media frame is cropped based on the region. In some implementations, the media frame is cropped only if one or more values of the target aspect ratio are less than the current value of the aspect ratio of the media frame. In some implementations, the media frame is cropped only if one or more values of the target resolution are less than the current value of the resolution of the media frame. In some implementations, the media frames are cropped to match the target aspect ratio or to match the target resolution. The target aspect ratio or target resolution may vary depending on the orientation of the mobile device 102 displaying the media frame. Additional padding may be added to one or more sides of the cropped media frame to match the target aspect ratio or to match the target resolution. In some implementations, the cropped regions are also based on one or more of the media frames preceding and/or following the media frame.
The next media frame is received at 710 until there are no more frames available. The next media frame may be received through the network interface 126 and stored in the data storage device 124. In some implementations, the next frame of media is received as part of the streaming media data. Streaming media may be received via network interface 146. As long as there are more frames available, the method may continue again to identify regions in the next frame based on the received metadata.
Fig. 8 is a flow chart of an embodiment of a method 800 of adjusting clipping based on a change in orientation. In some implementations, the method 800 is implemented by the processor 122 of the mobile device 102, the processor 122 of the mobile device 102 executing instructions stored on the data storage device 124 and receiving data from the one or more sensor modules 132. Briefly, the method 800 includes receiving an indication of a change in orientation at 802, identifying a resolution of a new orientation at 804, and dynamically adjusting cropping of a play media based on the new orientation at 806.
Still referring to fig. 8 and in more detail, method 800 begins when an indication of a change in orientation is received at 802. In some implementations, an indication of the change in orientation is received from the sensor module 132 (e.g., an accelerometer and/or magnetometer). In some implementations, the change in orientation or the detection of orientation occurs prior to the display of the media. In some implementations, the change in orientation occurs during display of the media, and the change in display of the media occurs in real-time after the change in orientation is detected.
The resolution and/or aspect ratio of the new orientation is identified at 804. In some implementations, the resolution and/or aspect ratio is predetermined by the application displaying the media. The resolution and/or aspect ratio may have predetermined values of landscape and portrait orientations. In some implementations, an orientation-dependent resolution and/or aspect ratio is determined to minimize the amount of unused display space. In some implementations, an orientation-dependent resolution and/or aspect ratio is determined to minimize the amount of padding required to fit the displayed media into the available display space.
The cropping of the play media is dynamically adjusted based on the new orientation at 806. In some implementations, the change in orientation occurs during display of the media, and the change in display of the media occurs in real-time after the change in orientation is detected. In some implementations, the media frame or frames remain the same, but the cropping is changed to adapt to the new resolution and/or aspect ratio based on the received metadata.
Fig. 9 is a block diagram of a general architecture of a computing system 900 that may be used to implement mobile device 102, media server system 104, etc. The computing system 900 includes a bus 905 or other communication means for communicating information, and a processor 910 coupled to the bus 905 for processing information. Computing system 900 may also contain one or more processors 910 coupled to the bus for processing information. The computing system 900 also includes a main memory 915 (such as RAM or other dynamic storage device), the main memory 915 being coupled to the bus 905 for storing information and instructions to be executed by the processor 910. Main memory 915 also may be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 910. The computing system 900 may also include a ROM 920 or other static storage device coupled to the bus 905 for storing static information and instructions for the processor 910. A storage device 925, such as a solid state device, magnetic disk, or optical disk, is coupled to bus 905 for persistently storing information and instructions. Computing system 900 can include, but is not limited to, a digital computer such as a laptop, desktop, workstation, personal digital assistant, server, blade server, mainframe, cellular telephone, smart phone, mobile computing device (e.g., notepad, electronic reader, etc.), and the like
The computing system 900 may be coupled via bus 905 to a display 935, such as a Liquid Crystal Display (LCD), a thin film transistor LCD (TFT), an Organic Light Emitting Diode (OLED) display, an LED display, an electronic paper display, a Plasma Display Panel (PDP), and/or other display, etc., for displaying information to a user. An input device 930, such as a keyboard including alphanumeric and other keys, may be coupled to bus 905 for communicating information and command selections to processor 910. In another embodiment, the input device 930 may be integrated with the display 935, such as in a touch screen display. The input device 930 may contain a cursor control, such as a mouse, a trackball, or cursor direction keys for communicating orientation information and command selections to the processor 910 and for controlling cursor movement on the display 935.
According to various embodiments, the processes and/or methods described herein may be implemented by the computing system 900 in response to the processor 910 executing an arrangement of instructions contained in the main memory 915. Such instructions may be read into main memory 915 from another computer-readable medium, such as storage device 925. Execution of the arrangement of instructions contained in main memory 915 causes computing system 900 to perform the illustrative processes and/or method steps described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 915. In alternative embodiments, hard-wired circuitry may be used in place of or in combination with software instructions to implement the illustrative embodiments. Thus, embodiments are not limited to any specific combination of hardware circuitry and software.
Although an embodiment of computing system 900 has been described in FIG. 9, embodiments of the subject matter and functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software embodied in tangible media, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal) that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium may be in or contained within a computer readable storage device, a computer readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Furthermore, although the computer storage medium is not a propagated signal, the computer storage medium may be a source or destination of computer program instructions encoded with an artificially generated propagated signal. The computer storage media may also be, or be included in, one or more separate components or media (e.g., multiple CDs, disks, or other storage devices). Accordingly, computer storage media are tangible and non-transitory.
The operations described in this specification may be performed by a data processing apparatus on data stored on one or more computer readable storage devices or received from other sources.
The terms "data processing apparatus," "computing device," or "processing circuit" include all types of apparatus, devices, and machines for processing data, including in some embodiments a programmable processor, a computer, a system-on-a-chip(s), a portion of a programmed processor, or a combination of the foregoing. The device may contain dedicated logic circuits, such as an FPGA or ASIC. In addition to hardware, the device may contain code that creates an execution environment for the associated computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. Devices and execution environments may implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. The computer program may, but need not, correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the relevant program, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located on one site or distributed across multiple sites and interconnected by a communication network.
Processors suitable for the execution of a computer program include, in some embodiments, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, such devices are not required by the computer. Furthermore, a computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media, and memory devices, including in some embodiments semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disk; CD-ROM and DVD discs. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD monitor, for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; in some implementations, the feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
While this specification contains many specifics of embodiments, these should not be construed as limitations on the scope of what may be claimed, but rather as descriptions of features specific to particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated in a single software product or packaged into software products embodied on multiple tangible media.
Reference to "or" may be construed as inclusive such that any term described using "or" may indicate any one of a single, more than one, and all of any term.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Additionally, the processes depicted in the accompanying drawings do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
The claims should not be read as limited to the described order or elements unless stated to that effect. It will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the spirit and scope of the following claims. All embodiments that come within the spirit and scope of the following claims and equivalents thereto are claimed.
Claims (14)
1. A method of converting a landscape video to a portrait mobile layout, comprising:
receiving, by a video pre-processor of a device, a first frame of video of a first orientation;
identifying, by an image analyzer of the apparatus, a first region within the first frame, the first region including a first feature;
generating, by a cropping calculator of the device, a score for the first region based on the characteristics of the first feature;
determining, by the cropping calculator, that the score of the first region exceeds a threshold;
a second region within the first frame including a second feature is identified by the image analyzer,
generating a second score for the second region by the cropping calculator,
determining by the clipping calculator that the second score exceeds the threshold,
a third region within the first frame including a third feature is identified by the image analyzer,
Generating a third score for the third region by the cropping calculator,
determining by the clipping calculator that the third score is less than the threshold,
cropping, by an image processor of the device, a first frame of the video to include the first region within a predetermined display region, the predetermined display region comprising a subset of the first frame in a second orientation, in response to the determination; wherein cropping the first frame further comprises:
in response to determining that the score of the first region and the second score of the second region each exceed the threshold, identifying, by the image analyzer, a display region including a second orientation of the first region and the second region, and
clipping the first frame to the boundary of the identified display area by the image processor,
wherein identifying a display region including a second orientation of the first region and the second region comprises:
determining, by the cropping calculator, that the score of the first region is higher than the second score of the second region;
generating, by the image processor, an intermediate display region centered on the first feature in the second orientation, the intermediate display region having a predetermined size;
Adjusting, by the image processor, a position of an intermediate display region within the frame to include the first region and the second region while maintaining the predetermined size and the second orientation, an
In response to the determination, determining, by the image processor, that the third region is not included within the predetermined display region when the first frame is cropped.
2. The method of claim 1, wherein the first feature comprises text, and wherein generating a score for the first region based on a characteristic of the first feature further comprises generating, by the cropping calculator, a score proportional to a size of the text.
3. The method of claim 1 or 2, wherein the first feature comprises text, and wherein generating a score for the first region based on a characteristic of the first feature further comprises generating, by the cropping calculator, the score in inverse proportion to a distance of the text from a center of the frame.
4. The method of claim 1 or 2, wherein the first feature comprises a face, and wherein generating a score for the first region based on a characteristic of the first feature further comprises generating, by the cropping calculator, the score based on a size of the face relative to the frame.
5. The method of claim 3, wherein the first feature comprises a face, and wherein generating a score for the first region based on a characteristic of the first feature further comprises generating, by the cropping calculator, the score based on a size of the face relative to the frame.
6. The method of claim 1 or 2, further comprising:
receiving, by the video preprocessor, a second frame of the first oriented video;
identifying, by the image analyzer, a second location in the second frame of a first region including the first feature; and is also provided with
Generating, by the cropping calculator, a second score for the first region based on an amount of movement of the first region between the first frame and the second frame; and is also provided with
Wherein determining that the score of the first region exceeds the threshold further comprises adding the score of the first region to a second score of the first region.
7. A method as in claim 3, further comprising:
receiving, by the video preprocessor, a second frame of the first oriented video;
identifying, by the image analyzer, a second location in the second frame of a first region including the first feature; and is also provided with
Generating, by the cropping calculator, a second score for the first region based on an amount of movement of the first region between the first frame and the second frame; and is also provided with
Wherein determining that the score of the first region exceeds the threshold further comprises adding the score of the first region to a second score of the first region.
8. The method of claim 4, further comprising:
receiving, by the video preprocessor, a second frame of the first oriented video;
identifying, by the image analyzer, a second location in the second frame of a first region including the first feature; and is also provided with
Generating, by the cropping calculator, a second score for the first region based on an amount of movement of the first region between the first frame and the second frame; and is also provided with
Wherein determining that the score of the first region exceeds the threshold further comprises adding the score of the first region to a second score of the first region.
9. The method of claim 5, further comprising:
receiving, by the video preprocessor, a second frame of the first oriented video;
identifying, by the image analyzer, a second location in the second frame of a first region including the first feature; and is also provided with
Generating, by the cropping calculator, a second score for the first region based on an amount of movement of the first region between the first frame and the second frame; and is also provided with
Wherein determining that the score of the first region exceeds the threshold further comprises adding the score of the first region to a second score of the first region.
10. The method of claim 6, further comprising: identifying, by a temporal image processor, a global motion vector from differences of pixels of the first frame and pixels of the second frame; and is also provided with
Wherein generating the second score comprises: a score is generated by the cropping calculator, the score being proportional to a difference between the global motion vector and a movement of the first region between the first frame and the second frame.
11. The method of claim 7, further comprising: identifying, by a temporal image processor, a global motion vector from differences of pixels of the first frame and pixels of the second frame; and is also provided with
Wherein generating the second score comprises: a score is generated by the cropping calculator, the score being proportional to a difference between the global motion vector and a movement of the first region between the first frame and the second frame.
12. The method of claim 8, further comprising: identifying, by a temporal image processor, a global motion vector from differences of pixels of the first frame and pixels of the second frame; and is also provided with
Wherein generating the second score comprises: a score is generated by the cropping calculator, the score being proportional to a difference between the global motion vector and a movement of the first region between the first frame and the second frame.
13. The method of claim 9, further comprising: identifying, by a temporal image processor, a global motion vector from differences of pixels of the first frame and pixels of the second frame; and is also provided with
Wherein generating the second score comprises: a score is generated by the cropping calculator, the score being proportional to a difference between the global motion vector and a movement of the first region between the first frame and the second frame.
14. A system for converting a landscape video to a portrait mobile layout, comprising:
one or more processors of the apparatus;
a network interface electrically connected to the one or more processors; and
computer storage electrically connected to the one or more processors and storing instructions that, when executed by the one or more processors, cause the one or more processors to perform operations comprising the method according to any one of claims 1-13.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
USPCT/US2016/065025 | 2016-12-05 | ||
PCT/US2016/065025 WO2018106213A1 (en) | 2016-12-05 | 2016-12-05 | Method for converting landscape video to portrait mobile layout |
PCT/US2017/064542 WO2018106607A1 (en) | 2016-12-05 | 2017-12-04 | Method for converting landscape video to portrait mobile layout |
Publications (2)
Publication Number | Publication Date |
---|---|
CN109791600A CN109791600A (en) | 2019-05-21 |
CN109791600B true CN109791600B (en) | 2023-11-10 |
Family
ID=57750582
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780051565.4A Active CN109791600B (en) | 2016-12-05 | 2017-12-04 | Method for converting horizontal screen video into vertical screen mobile layout |
Country Status (4)
Country | Link |
---|---|
US (1) | US20200020071A1 (en) |
EP (1) | EP3488385B1 (en) |
CN (1) | CN109791600B (en) |
WO (3) | WO2018106213A1 (en) |
Families Citing this family (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109690471B (en) * | 2016-11-17 | 2022-05-31 | 谷歌有限责任公司 | Media rendering using orientation metadata |
WO2019112642A1 (en) * | 2017-12-05 | 2019-06-13 | Google Llc | Method for converting landscape video to portrait mobile layout using a selection interface |
US10839574B1 (en) * | 2018-03-09 | 2020-11-17 | Facebook, Inc. | Systems and methods for generating content |
CN111107192A (en) * | 2018-10-29 | 2020-05-05 | 印象认知（北京）科技有限公司 | Display screen of terminal equipment and terminal equipment |
CN111199171B (en) * | 2018-11-19 | 2022-09-23 | 荣耀终端有限公司 | Wrinkle detection method and terminal equipment |
US10887542B1 (en) | 2018-12-27 | 2021-01-05 | Snap Inc. | Video reformatting system |
US11665312B1 (en) * | 2018-12-27 | 2023-05-30 | Snap Inc. | Video reformatting recommendation |
US10929979B1 (en) * | 2018-12-28 | 2021-02-23 | Facebook, Inc. | Systems and methods for processing content |
US11095848B1 (en) * | 2019-04-19 | 2021-08-17 | Screenshare Technology Ltd. | Method for creating vertically oriented video |
US20200366973A1 (en) * | 2019-05-14 | 2020-11-19 | Pccw Vuclip (Singapore) Pte. Ltd. | Automatic Video Preview Creation System |
US10834465B1 (en) * | 2019-06-28 | 2020-11-10 | Netflix, Inc. | Automated video cropping |
CN111050204A (en) * | 2019-12-27 | 2020-04-21 | 北京达佳互联信息技术有限公司 | Video clipping method and device, electronic equipment and storage medium |
FR3109686B1 (en) * | 2020-04-22 | 2023-05-05 | Wildmoka | Process for transposing an audiovisual stream |
CN112333513B (en) * | 2020-07-09 | 2024-02-06 | 深圳Tcl新技术有限公司 | Method, equipment and storage medium for creating vertical screen video library |
CN112188283B (en) * | 2020-09-30 | 2022-11-15 | 北京字节跳动网络技术有限公司 | Method, device and equipment for cutting video and storage medium |
CN112218160A (en) * | 2020-10-12 | 2021-01-12 | 北京达佳互联信息技术有限公司 | Video conversion method and device, video conversion equipment and storage medium |
CN112181165A (en) * | 2020-10-27 | 2021-01-05 | 维沃移动通信有限公司 | Input method and device and electronic equipment |
CN113077470B (en) * | 2021-03-26 | 2022-01-18 | 天翼爱音乐文化科技有限公司 | Method, system, device and medium for cutting horizontal and vertical screen conversion picture |
US11765428B2 (en) | 2021-04-07 | 2023-09-19 | Idomoo Ltd | System and method to adapting video size |
US11627370B1 (en) * | 2021-12-28 | 2023-04-11 | Kt Corporation | Converting video according to status of user device |
CN114286136A (en) * | 2021-12-28 | 2022-04-05 | 咪咕文化科技有限公司 | Video playing and encoding method, device, equipment and computer readable storage medium |
CN114466145B (en) * | 2022-01-30 | 2024-04-12 | 北京字跳网络技术有限公司 | Video processing method, device, equipment and storage medium |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2002215281A (en) * | 2000-12-27 | 2002-07-31 | Internatl Business Mach Corp <Ibm> | Computer device, display device, output device, display controller, computer program, storage medium and processing method of image |
EP2204733A1 (en) * | 2008-12-30 | 2010-07-07 | Rapid Mobile Media Ltd. | Graphics display |
JP2012156628A (en) * | 2011-01-24 | 2012-08-16 | Nikon Corp | Electronic camera, image processing apparatus, and image processing program |
JP2012216184A (en) * | 2012-01-24 | 2012-11-08 | Nanao Corp | Display device, image processing device, image area detecting method, and computer program |
JP2016019248A (en) * | 2014-07-10 | 2016-02-01 | キヤノン株式会社 | Motion picture display control device, motion picture display control method and program |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB0116113D0 (en) * | 2001-06-30 | 2001-08-22 | Hewlett Packard Co | Tilt correction of electronic images |
US7528846B2 (en) * | 2005-02-23 | 2009-05-05 | Microsoft Corporation | Systems and methods to adjust a source image aspect ratio to match a different target display aspect ratio |
RU2462757C2 (en) * | 2008-09-08 | 2012-09-27 | Сони Корпорейшн | Device and method to process images, device to input images and software |
US9020298B2 (en) * | 2009-04-15 | 2015-04-28 | Microsoft Technology Licensing, Llc | Automated image cropping to include particular subjects |
US10645344B2 (en) * | 2010-09-10 | 2020-05-05 | Avigilion Analytics Corporation | Video system with intelligent visual display |
US9008436B2 (en) * | 2011-10-28 | 2015-04-14 | Intellectual Ventures Fund 83 Llc | Image recomposition from face detection and facial features |
-
2016
- 2016-12-05 WO PCT/US2016/065025 patent/WO2018106213A1/en active Application Filing
-
2017
- 2017-12-04 WO PCT/US2017/064542 patent/WO2018106607A1/en unknown
- 2017-12-04 EP EP17826624.3A patent/EP3488385B1/en active Active
- 2017-12-04 US US16/322,464 patent/US20200020071A1/en not_active Abandoned
- 2017-12-04 CN CN201780051565.4A patent/CN109791600B/en active Active
- 2017-12-05 WO PCT/US2017/064719 patent/WO2018106692A1/en active Application Filing
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2002215281A (en) * | 2000-12-27 | 2002-07-31 | Internatl Business Mach Corp <Ibm> | Computer device, display device, output device, display controller, computer program, storage medium and processing method of image |
EP2204733A1 (en) * | 2008-12-30 | 2010-07-07 | Rapid Mobile Media Ltd. | Graphics display |
JP2012156628A (en) * | 2011-01-24 | 2012-08-16 | Nikon Corp | Electronic camera, image processing apparatus, and image processing program |
JP2012216184A (en) * | 2012-01-24 | 2012-11-08 | Nanao Corp | Display device, image processing device, image area detecting method, and computer program |
JP2016019248A (en) * | 2014-07-10 | 2016-02-01 | キヤノン株式会社 | Motion picture display control device, motion picture display control method and program |
Non-Patent Citations (3)
Title |
---|
Looking into Video Frames on Small Displays;Xin Fan等;《Proceedings of the 11th ACM international conference on Multimedia》;20031108;全文 * |
Video Retargeting: Automating Pan and Scan;Feng Liu等;《Proceedings of the 14th ACM international conference on Multimedia》;20061027;全文 * |
面向PC的实时读出系统二维图象处理及其集成交互软件;高峰，张儒端;光子学报(第02期);全文 * |
Also Published As
Publication number | Publication date |
---|---|
EP3488385B1 (en) | 2023-08-16 |
US20200020071A1 (en) | 2020-01-16 |
CN109791600A (en) | 2019-05-21 |
WO2018106607A1 (en) | 2018-06-14 |
WO2018106692A1 (en) | 2018-06-14 |
WO2018106213A1 (en) | 2018-06-14 |
EP3488385A1 (en) | 2019-05-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109791600B (en) | Method for converting horizontal screen video into vertical screen mobile layout | |
US11605150B2 (en) | Method for converting landscape video to portrait mobile layout using a selection interface | |
US11322117B2 (en) | Media rendering with orientation metadata | |
US8487959B1 (en) | Generating simulated eye movement traces for visual displays | |
US9449230B2 (en) | Fast object tracking framework for sports video recognition | |
US9424479B2 (en) | Systems and methods for resizing an image | |
US20160133297A1 (en) | Dynamic Video Summarization | |
CN112215171B (en) | Target detection method, device, equipment and computer readable storage medium | |
CN103988202A (en) | Image attractiveness based indexing and searching | |
CN111488873B (en) | Character level scene text detection method and device based on weak supervision learning | |
Chen et al. | Improved seam carving combining with 3D saliency for image retargeting | |
CN112789650A (en) | Detecting semi-transparent image watermarks | |
CN112329762A (en) | Image processing method, model training method, device, computer device and medium | |
KR20220058892A (en) | Blank video overlay | |
CN110942056A (en) | Clothing key point positioning method and device, electronic equipment and medium | |
KR20220097945A (en) | Non-Closed Video Overlays | |
US11526652B1 (en) | Automated optimization of displayed electronic content imagery | |
KR20150088653A (en) | Apparatus for inserting advertisement using frame clustering and method thereof |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |