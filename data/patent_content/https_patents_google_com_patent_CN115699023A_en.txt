CN115699023A - Voice robot development technology based on example - Google Patents
Voice robot development technology based on example Download PDFInfo
- Publication number
- CN115699023A CN115699023A CN202180039194.4A CN202180039194A CN115699023A CN 115699023 A CN115699023 A CN 115699023A CN 202180039194 A CN202180039194 A CN 202180039194A CN 115699023 A CN115699023 A CN 115699023A
- Authority
- CN
- China
- Prior art keywords
- training
- voice robot
- conversation
- dialog
- embedding
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000011161 development Methods 0.000 title claims abstract description 97
- 238000005516 engineering process Methods 0.000 title description 6
- 238000012549 training Methods 0.000 claims abstract description 549
- 230000004044 response Effects 0.000 claims abstract description 174
- 238000000034 method Methods 0.000 claims abstract description 105
- 238000010801 machine learning Methods 0.000 claims description 193
- 238000012545 processing Methods 0.000 claims description 55
- 230000006399 behavior Effects 0.000 claims description 24
- 230000015654 memory Effects 0.000 claims description 14
- 230000000977 initiatory effect Effects 0.000 claims description 12
- 230000007246 mechanism Effects 0.000 claims description 7
- 230000000694 effects Effects 0.000 description 34
- 230000008569 process Effects 0.000 description 32
- 230000009471 action Effects 0.000 description 23
- 230000003542 behavioural effect Effects 0.000 description 16
- 230000008859 change Effects 0.000 description 10
- 238000004891 communication Methods 0.000 description 8
- 230000008901 benefit Effects 0.000 description 6
- 230000002452 interceptive effect Effects 0.000 description 6
- 238000009877 rendering Methods 0.000 description 6
- 230000026676 system process Effects 0.000 description 6
- 241000282412 Homo Species 0.000 description 5
- 238000010586 diagram Methods 0.000 description 5
- 230000006870 function Effects 0.000 description 5
- 238000013518 transcription Methods 0.000 description 5
- 230000035897 transcription Effects 0.000 description 5
- 235000013305 food Nutrition 0.000 description 4
- 235000014347 soups Nutrition 0.000 description 3
- 238000010200 validation analysis Methods 0.000 description 3
- 238000003780 insertion Methods 0.000 description 2
- 230000037431 insertion Effects 0.000 description 2
- 238000002372 labelling Methods 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 239000013598 vector Substances 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 235000007688 Lycopersicon esculentum Nutrition 0.000 description 1
- 235000010582 Pisum sativum Nutrition 0.000 description 1
- 240000004713 Pisum sativum Species 0.000 description 1
- 240000003768 Solanum lycopersicum Species 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 231100000871 behavioral problem Toxicity 0.000 description 1
- 230000002457 bidirectional effect Effects 0.000 description 1
- 238000012508 change request Methods 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 230000003483 hypokinetic effect Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000005012 migration Effects 0.000 description 1
- 238000013508 migration Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000011273 social behavior Effects 0.000 description 1
- 230000002194 synthesizing effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3329—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
- G06F40/35—Discourse or dialogue representation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/02—Reservations, e.g. for tickets, services or events
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/10—Office automation; Time management
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/01—Customer relationship services
- G06Q30/015—Providing customer assistance, e.g. assisting a customer within a business location or via helpdesk
- G06Q30/016—After-sales
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/06—Buying, selling or leasing transactions
- G06Q30/0601—Electronic shopping [e-shopping]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q50/00—Systems or methods specially adapted for specific business sectors, e.g. utilities or tourism
- G06Q50/10—Services
- G06Q50/12—Hotels or restaurants
-
- G06Q50/60—
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/027—Concept to speech synthesisers; Generation of natural phrases from machine-based concepts
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/27—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique
- G10L25/30—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique using neural networks
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/02—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail using automatic reactions or user delegation, e.g. automatic replies or chatbot-generated messages
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/487—Arrangements for providing information services, e.g. recorded voice services or time announcements
- H04M3/493—Interactive information services, e.g. directory enquiries ; Arrangements therefor, e.g. interactive voice response [IVR] systems or voice portals
- H04M3/4936—Speech interaction details
Abstract
Embodiments are directed to providing a voice robot development platform that enables third party developers to train voice robots based on training examples. The training instances can each include a training input and a training output. The training input can include a portion of the corresponding conversation and a previous context of the corresponding conversation. The training output can include a corresponding real-valued response to the portion of the corresponding dialog. After training, the voice robot can be deployed for conducting conversations on behalf of third parties. In some implementations, the speech robot is further trained based on the corresponding feature emphasized input that focuses the speech robot on the particular feature of the portion of the corresponding conversation. In some additional or alternative embodiments, the voice robot is further trained to interact with a third party system via Remote Procedure Calls (RPCs).
Description
Background
Humans may participate in human-computer conversations with interactive software applications called "bots," "chat bots," "automated assistants," "interactive personal assistants," "intelligent personal assistants," "conversation agents," and the like, via a variety of computing devices. As one example, these robots can initiate or answer telephone calls and converse with humans to perform actions on behalf of third parties. However, the functionality of these robots may be limited by the predefined intent patterns that the robot uses to perform the actions. In other words, if a person participating in a conversation with the robot provides a spoken utterance that includes an intent that is not defined by the predefined intent pattern, the robot will fail. Further, to update these robots, existing intent patterns may be modified or new intent patterns may be added. However, there are almost limitless patterns of intent that may need to be defined to make the robot robust to various nuances of human speech. Significant utilization of computing resources is required to manually define and/or manually refine such intent patterns. Moreover, even if a large number of intent patterns are defined, a large amount of memory is required to store and/or utilize the large number of intent patterns. Therefore, the intention pattern cannot be actually expanded to the extent of learning the nuances of human speech.
Disclosure of Invention
Embodiments disclosed herein relate to providing a voice robot development platform that enables training of a voice robot associated with a third party based on a plurality of training instances. The voice robot can correspond to one or more processors that utilize multiple Machine Learning (ML) layers of one or more ML models to converse on behalf of a third party for a phone call associated with the third party. The voice robot development platform can obtain, from a third party developer and via a client device associated with the third party developer, a plurality of training instances directed to the voice robot development platform based on the user input. The telephone calls associated with the third party can include incoming telephone calls initiated by humans via the respective client devices and directed to the third party, and/or outgoing telephone calls initiated by the voice robot via the voice robot development platform and directed to humans or additional third parties associated with humans. Further, telephone calls associated with third parties can be performed using various voice communication protocols, such as voice over internet protocol (VoIP), public Switched Telephone Network (PSTN), and/or other telephone communication protocols.
For example, assume that the third party for which the voice robot is being trained is a fictitious restaurant entity named Hypothetical Caf. Assume further that a plurality of training instances for training a voice robot associated with a Hypothetical Caf are obtained via the voice robot development platform. In this example, the voice bot may then answer the incoming telephone call and perform one or more actions related to restaurant reservations, business hour queries, take-out orders, and/or may perform any other actions associated with the incoming telephone call directed to the Hypothetical Caf during the telephone conversation. Further, the voice robot may additionally or alternatively initiate performance of the outgoing telephone call and perform one or more actions related to the inventory order, the information technology request, and/or may perform any other actions associated with the outgoing telephone call on behalf of the Hypothetical Caf during the telephone conversation. Notably, multiple respective instances of the voice bot may be deployed such that the respective instances of the voice bot are capable of participating in multiple respective conversations with respective people at any given time. For example, each instance of the voice robot can include a corresponding processor that utilizes a corresponding instance of the ML layer of the voice robot.
In various embodiments, each training instance of the plurality of training instances can include a training instance input and a training instance output. The training instance input can include a portion of the corresponding conversation and a previous context associated with the corresponding conversation. For example, the portion of the corresponding dialog can include audio data that captures spoken input by a user (e.g., a third party developer or another person), a plurality of speech hypotheses generated based on processing the audio data using one or more Automatic Speech Recognition (ASR) models, and/or text provided by the third party developer. Further, the previous context associated with the corresponding conversation can include prior audio data of the corresponding conversation prior to the portion of the conversation, a plurality of prior speech hypotheses generated based on processing the prior audio data using one or more of the ASR models, text provided by a third party developer, and/or metadata associated with the corresponding conversation. The training instance output can include a corresponding real-valued response to at least the portion of the corresponding dialog. For example, the corresponding true-value response to at least the portion of the corresponding dialog can include audio data that captures a spoken response of the user (e.g., a third-party developer or another person), a plurality of speech hypotheses generated based on processing the audio data using one or more of the ASR models, a true-value embedding associated with the true-value response, and/or text provided by the third-party developer.
In some versions of those embodiments, the training instance input can be processed using multiple ML layers of one or more ML models to generate a prediction embedding associated with a predicted response to at least the portion of the dialog. Further, the predicted embedding can be compared to the actual value embedding associated with the training instance output in an embedding space. One or more losses can be generated based on a distance metric (e.g., a cosine distance, a euclidean distance, and/or other distance metric) between the predicted embedding and the true value embedding, and one or more of the plurality of ML layers can be updated based on the one or more of the losses. In some additional or alternative versions of those implementations, at least the portion of the corresponding dialog can be processed with a first ML layer of the plurality of ML layers to generate a first embedding, a previous context of the corresponding dialog can be processed with a second ML layer of the plurality of ML layers to generate a second embedding, and the first embedding and the second embedding can be concatenated to generate an embedding associated with a current state of the corresponding dialog. In other words, the embedding associated with the current state of the corresponding conversation encodes the corresponding conversation relative to the history of the corresponding conversation. In some additional or alternative further versions of those embodiments, the plurality of speech hypotheses, whether included in at least the portion of the conversation or generated based on audio data included in at least the portion of the conversation, can be aligned and/or annotated before being processed.
In various implementations, one or more of the plurality of training instances can be associated with one or more corresponding feature emphasized inputs. The corresponding feature emphasis input can be obtained based on user input from a third party developer and can be provided as an indication of why a particular feature of the corresponding training instance is important. As described above, the corresponding feature emphasized input can be used as part of a training instance input for training the plurality of ML layers, to bias updates of the plurality of ML layers after processing the training instance input, and/or as an input to a network of pointers that focus the plurality of ML layers on the corresponding feature emphasized input during training. For example, if the portion of the corresponding dialog used as a training instance input for the speech robot associated with the Hypothetical Caf corresponds to "I who like to make a reservation at6 00PM for four people around 6)", the corresponding feature emphasis input may include an indication that the portion of the corresponding dialog includes a time feature and a party people feature for making a restaurant reservation. As a result, the voice robot may be trained to focus on specific features of the input at the time of inference.
By emphasizing inputs using corresponding features described herein, various technical advantages can be achieved. As one non-limiting example, a speech robot can achieve a given level of accuracy and/or robustness based on a given number of training instances by including corresponding feature-emphasized inputs. Without including a corresponding feature emphasized input, a greater number of training instances will be required to achieve a given level of accuracy and/or robustness-or a given level of accuracy and/or robustness will not be achieved. For example, even though the inputs processed by a given voice robot may be nearly infinite, the change in response can be limited to candidate responses for the given voice robot, rather than requiring a predefined intent pattern for each of the nearly infinite inputs. As a result, the voice robot may be trained in a faster and efficient manner, thereby saving computing resources of a client device used to train the voice robot and/or network resources in implementations in which training instances, training losses, and/or other training data are transmitted over one or more networks.
In various embodiments, the voice robot can be trained to make Remote Procedure Calls (RPCs) with one or more third-party systems. The third-party system can include, for example, a reservation system, an inventory system, a status update system, and/or any other third-party system that can receive RPC outbound requests from the voice robot and send responsive RPC inbound requests back to the voice robot. The plurality of training instances obtained by the voice robot development platform can include RPC training instances. Each of the RPC training instances can be either an RPC outbound training instance or an RPC inbound training instance. In implementations where a given RPC training instance is an RPC outbound training instance, the training instance inputs may include portions of the corresponding conversation and previous contexts for the corresponding conversation, as described above. However, the training instance output may include an indication to generate and transmit an RPC outbound request, and optionally a true value response. Continuing with the Hypothetical Caf example above, the RPC outbound request may be a structured request that is transmitted to the reservation system in response to a portion of the corresponding dialog of "I who like to make a reservation at6:00PM for four peoples (I want to reserve 4 positions of 6 PM); party number =4]. In addition, the training instance output may also include a corresponding real-valued response of "Let me check" to tell the human voice robot that it is asking for the availability of restaurant reservations. In an embodiment where the given RPC training instance is an RPC inbound training instance, the training instance inputs may include RPC inbound requests from one or more third-party systems. Further, the training instance output may include a corresponding real-valued response to the RPC inbound request. Continuing with the Hypothetical Caf example above, the RPC inbound request may be a structured request indicating whether the desired time is available (e.g., a four-digit reservation of 6 PM), and optionally one or more alternate times if the desired time is not available. In addition, the training example output may also include "We have 6? (at 6 pm.
By using the RPC training examples described herein, various technical advantages are achieved. As one non-limiting example, the voice robot can learn how and/or when to communicate requests to third-party systems, and how to utilize responses to those requests to address the task of a telephone conversation being conducted by the voice robot. As a result, the task can be solved by the voice robot during the conversation and can be efficiently solved without having to have an additional person converse with the conversation. Furthermore, the utilization of RPC training instances enables a reduction in the number of RPC requests because there are fewer erroneous RPC requests, thereby saving computational resources that would otherwise be consumed in generating RPC requests and/or network resources that would otherwise be consumed in transmitting RPC requests over one or more networks.
In some implementations, one or more of the plurality of training instances can be obtained from a corpus of prior phone calls based on user input from a third party developer and via a voice robot development platform. The third party developer may be requested to label, via further user input, one or more of the training instances from the corpus of prior telephone calls. For example, the voice robot development platform may request that third party developers define corresponding feature emphasis inputs for one or more of the training instances, define RPC requests (if any) for one or more of the training instances, and/or define other labels for one or more of the training instances. The previous phone call can include audio data that captured a plurality of people and/or corresponding conversations between the people and the corresponding voice robots. Previous phone calls can be processed to generate one or more of the training instances. For example, assume that a previous phone call includes audio data that captured a corresponding conversation between a first type of person (e.g., a customer) and a second type of person (e.g., an employee). In this example, audio data corresponding to a portion of a corresponding conversation associated with the customer can be identified, and audio data corresponding to a corresponding response associated with the employee in response to the portion of the corresponding conversation associated with the customer can be identified. Portions of the corresponding dialog associated with the customer can be used as part of the training instance input and corresponding responses associated with the employee can be used as part of the training instance output. Further, at any given point in the corresponding dialog, the previous context of the corresponding dialog can also be used as part of the training instance input. In embodiments where RPC is conducted during a corresponding telephone call, the third party developer may need to inject RPC outbound requests or RPC inbound requests into those training instances. In some versions of those embodiments, the previous phone call may be associated with a third party for whom the voice robot is being trained. In some additional or alternative versions of those embodiments, the previous phone call may be associated with one or more other third parties that are different from the third party for which the voice robot is being trained.
In some additional or alternative implementations, one or more of the plurality of training instances can be obtained from a demonstrative dialog based on user input from a third-party developer and conducted via a voice robot development platform. The demonstrative dialog can include audio data and/or text capturing a corresponding demonstrative dialog between one or more persons (e.g., which may or may not include a third-party developer). Continuing with the Hypothetical Caf example above, a person can provide user input from the perspective of a customer of the Hypothetical Caf to initiate the corresponding conversation, the person or additional persons can provide subsequent user input from the perspective of an employee of the Hypothetical Caf, the person can provide further subsequent user input from the perspective of the employee, the person or additional persons can provide further subsequent user input from the perspective of the customer, and so on until the demonstrative conversation has ended. The demonstrative conversation can be processed in the same or similar manner as described above with respect to generating one or more of the plurality of training instances for previous phone calls in the corpus of previous phone calls.
In some additional or alternative implementations, one or more of the plurality of training instances can be obtained directly via the speech robot development platform based on user input from a third party developer. For example, a third party developer may define at least a portion of a corresponding dialog to be used as training instance input for a given training instance, and may define a true value response to the portion of the corresponding dialog to be used as training instance output for the given training instance. Further, the third party developer may optionally define the previous context of the corresponding conversation to also serve as part of the training instance input for a given training instance, or a conversation digest of the "previous" part of the corresponding conversation. Notably, while the third party developer is defining these portions of the dialog, the third party developer may not need to define the entire dialog as a demonstrative dialog. Thus, the third party developer can define one or more training instances of parameters directed to particular portions of the conversation, such as request tasks (e.g., restaurant reservation tasks, flight change tasks, inventory check tasks, and/or any other tasks that may be performed during a corresponding phone call), perform RPCs, introductions, and/or other aspects of the corresponding conversation.
In various embodiments, a corresponding conversation digest can be generated for each phone call made by the voice robot at the time of deployment. A corresponding dialog summary can be presented to a third party developer via the voice robot development platform to monitor the execution of the voice robot. In some implementations, the corresponding conversation digest can include, for example, a natural language digest of each of the corresponding telephone calls, a duration of the corresponding telephone calls, results or achievements of the corresponding telephone calls, monetary information associated with the corresponding telephone calls, and/or other information associated with the telephone calls. Continuing with the contextual Caf example, the corresponding dialog digest may be, for example, "user called to make a reservation, the time was available, the reservation was made (user called to make a reservation, time available, reservation completed)". In some additional or alternative implementations, the corresponding conversation digest, when selected, may cause a transcription of the corresponding telephone call to be presented to the third party developer via the voice robot development platform. The corresponding conversation digest can be stored in a voice activity database.
By using the techniques described herein, various technical advantages can be achieved. As one non-limiting example, the voice robot development platform enables training of a voice robot based on examples of conversations rather than predefined intent patterns. This allows voice robot behavior to be easily added or modified by adding new training instances or modifying existing training instances. Thus, a voice robot trained using the voice robot development platform described herein is more scalable and reduces memory consumption since a large number of intent patterns need not be defined. Accordingly, the ML model trained and utilized can have a smaller memory footprint, and can be more robust and/or accurate. Further, the voice robot trained using the voice robot development platform achieves a high level of accuracy and recall, enabling telephone calls to be terminated more quickly and efficiently because the voice robot trained using the voice robot development platform is more able to understand the nuances of human speech and respond accordingly.
The above description is provided as an overview of only some of the embodiments disclosed herein. These and other embodiments are described in more detail herein.
Drawings
FIG. 1 depicts a block diagram of an exemplary environment in which various aspects of the present disclosure can be implemented.
FIG. 2A depicts an exemplary process flow for training a voice robot, in accordance with various embodiments.
FIG. 2B depicts an exemplary process flow for using a trained voice robot, in accordance with various embodiments.
3A, 3B, and 3C depict various non-limiting examples of user interfaces associated with a voice robot development platform, according to various embodiments.
FIG. 4 depicts a flowchart illustrating an exemplary method of training a speech robot based at least in part on a feature emphasized input, in accordance with various embodiments.
FIG. 5 depicts a flowchart illustrating an exemplary method of training a voice robot based at least in part on a remote procedure call, in accordance with various embodiments.
Fig. 6 depicts an exemplary architecture of a computing device, according to various embodiments.
Detailed Description
Turning now to fig. 1, a block diagram of an exemplary environment in which aspects of the present disclosure can be implemented is depicted. A client device 110 is shown in fig. 1, and in various embodiments, the client device 110 includes a user input engine 111, a rendering engine 112, and a voice robot development system client 113. The client device 110 can be, for example, a standalone assistant device (e.g., with a microphone, speaker, and/or display), a laptop computer, a desktop computer, a tablet computer, a wearable computing device, a vehicle computing device, and/or any other client device capable of implementing the voice robot development system client 113.
The user input engine 111 is capable of detecting various types of user input at the client device 110. The user input detected at client device 110 can include spoken input detected via a microphone of client device 110, touch input detected via a user interface input device (e.g., a touch screen) of client device 110, and/or typed input detected via a user interface input device of client device 110 (e.g., via a virtual keyboard on a touch screen, a physical keyboard, a mouse, a stylus, and/or any other user interface input device of client device 110).
Rendering engine 112 can cause the output to be visually and/or audibly rendered at client device 110 via the user interface output. The output can include, for example, various types of user interfaces associated with the voice robot development system client 113 that can be visually rendered via a user interface of the client device 110 (e.g., as described with reference to fig. 3A, 3B, and 3C), notifications associated with the voice robot development system client 113 that can be visually rendered via a user interface of the client device 110 and/or audibly rendered via speakers of the client device 110, and/or any other output described herein that can be visually and/or audibly rendered.
In various embodiments, the speech robot development system client 113 can include an Automatic Speech Recognition (ASR) engine 130A, a Natural Language Understanding (NLU) engine 140A1, and a text-to-speech (TTS) engine 150A1. Further, the voice robot development system client 113 can be enabled through one or more networks 199 1 (e.g., any combination of Wi-Fi, bluetooth, near Field Communication (NFC), local Area Network (LAN), wide Area Network (WAN), ethernet, the internet, and/or other networks) with the voice robot development system 120. From the perspective of a user interacting with the client device 110, the voice robot development system client 113 and the voice robot development system 120 form a logical instance of a voice robot development platform. Although the voice robot development system 120 is depicted in fig. 1 as being implemented remotely from the client device 110 (e.g., via one or more servers), it should be understood that this is for purposes of example and is not meant to be limiting. For example, the voice robot development system 120 can alternatively be implemented locally at the client device 110.
The voice bot development platform can be used by a third party developer (e.g., a user of client device 110) to train a voice bot as described herein that is deployed to converse on behalf of a third party associated with the third party developer for a phone call associated with the third party. Notably, the voice robot development platform can be provided by a first party, and a third party developer can utilize the voice robot development platform to train a third party's voice robot associated with the third party developer. As used herein, the term first party refers to an entity that publishes the voice robot development platform, while the term third party refers to an entity that is different from the entity associated with the first party and that does not publish the voice robot development system. Thus, a third party developer refers to a user that interacts with the voice robot development platform to train a voice robot associated with a third party.
Telephone calls described herein can be performed using various voice communication protocols, such as voice over internet protocol (VoIP), public Switched Telephone Network (PSTN), and/or other telephone communication protocols. As described herein, the synthesized speech can be rendered as part of the secondary phone call, which can include injecting the synthesized speech into the call so that it is perceivable by at least one participant of the secondary phone call. The synthesized speech can be generated and/or injected by the client device 110, which is one of the endpoints of a given telephone call, and/or can be generated and/or injected by a server connected to the telephone call (e.g., a server implementing the voice robot development system 120).
In various embodiments, the speech robot development system 120 includes an ASR engine 130A2, an NLU engine 140A2, a TTS engine 150A2, a speech robot training engine 160, a speech robot engine 170, an error recognition engine 180, and a dialog summarization engine 185. The voice robot training engine 160 can be used to train a voice robot to be deployed for conducting a conversation on behalf of a third party for a telephone call associated with the third party, and can include a training instance engine 161 and a training engine 162 in various embodiments. Further, the voice robot engine 170 can then utilize the trained voice robot to conduct a conversation on behalf of the third party for a phone call associated with the third party, and can include, in various embodiments, a response engine 171 and a Remote Procedure Call (RPC) engine 172.
In some implementations, one or more of the plurality of training instances can be obtained from a corpus of prior telephone calls based on user input. The third party developer may need to label one or more of the training instances from the corpus of prior phone calls via user input. The previous phone call can include audio data that captured a plurality of people and/or corresponding conversations between the people and the corresponding voice robots. Training instance engine 161 can process previous phone calls to generate one or more of the training instances. For example, assume that a previous phone call includes audio data that captured a corresponding conversation between a first type of person (e.g., a customer) and a second type of person (e.g., an employee). In this example, training instance engine 161 can identify audio data corresponding to a portion of a corresponding conversation associated with the customer and identify audio data corresponding to a corresponding response associated with the employee in response to the portion of the corresponding conversation associated with the customer. Portions of the corresponding dialog associated with the customer can be used as part of the training instance input and corresponding responses associated with the employee can be used as part of the training instance output. In addition, the previous context of the corresponding dialog can also be used as part of the training instance input. The previous context of the corresponding conversation can include prior audio data of the corresponding conversation (and/or a plurality of speech hypotheses corresponding thereto or recognized text corresponding thereto), metadata associated with the conversation (e.g., a location of the client, a time at which the corresponding telephone call was initiated, whether values of the parameters have been requested, etc.), and/or other contextual information associated with the previous telephone call.
In some versions of those embodiments, the previous phone call may be associated with a third party for whom the voice robot is being trained. For example, assume that the third party is a fictitious retail entity named fictitious market selling various products. The prior telephone call can include audio data that captures a corresponding conversation between one or more of a person of a first type (e.g., a customer) and a person of a second type (e.g., an employee of the hypothetical market), a voice robot associated with the hypothetical market, or an Interactive Voice Response (IVR) system associated with the hypothetical market. In some additional or alternative versions of those embodiments, the previous phone call may be associated with one or more other third parties that are different from the third party for which the voice robot is being trained. In some further versions of those embodiments, previous phone calls associated with one or more other third parties obtained by training instance engine 161 may be limited to other third parties (e.g., retailer entities, airline entities, restaurant entities, school or university entities, vendor entities, shipper entities, government entities, and/or any other type of person, place, or thing) that have the same type of entity as the third party for which the voice robot is being trained. Continuing with the above example, previous phone calls used to generate training instances for the voice robots associated with the hypothetical market may be limited to those associated with other retailers, and optionally to other retailers selling the same or similar products.
In additional or alternative embodiments, one or more of the plurality of training instances can be obtained from a demonstrative dialog conducted based on the user input. The demonstrative dialog can include audio data and/or text capturing the corresponding demonstrative dialog between one or more persons (e.g., which may or may not include third-party developers). For example, assume that the third party is a fictitious retail entity named fictitious market selling various products. In this example, a person can provide user input from the perspective of a customer of the hypothetical market to initiate the corresponding conversation, the person or additional persons can provide subsequent user input from the perspective of an employee of the hypothetical market, the person can provide further subsequent user input from the perspective of the employee, the person or additional persons can provide further subsequent user input from the perspective of the customer, and so on (e.g., as described with reference to fig. 3B). Training instance engine 161 can process the demonstrative dialog in a similar manner as described above with respect to the corpus of training instances to generate one or more of the training instances.
In some additional or alternative embodiments, one or more of the plurality of training instances can be obtained directly based on user input. For example, a third party developer may define at least a portion of a corresponding dialog to be used as training instance input for a given training instance, and may define a true value response to the portion of the corresponding dialog to be used as training instance output for the given training instance. Further, the third party developer may optionally define a previous context for the corresponding dialog to also serve as part of the training instance input for a given training instance, or a dialog summary of the "previous" portion of the corresponding dialog. Notably, while the third party developer is defining these portions of the dialog, the third party developer may not need to define the entire dialog as a demonstrative dialog. Thus, the third party developer can define one or more training instances of parameters, execution of RPCs, introductions, and/or other aspects of the corresponding conversation that point to a particular portion of the conversation, such as a request task (e.g., a restaurant reservation task, a flight change task, an inventory check task, and/or any other task that may be performed during the corresponding telephone call).
In various implementations, one or more corresponding feature emphasis inputs may be associated with one or more of a plurality of training instances. The one or more corresponding feature emphasis inputs can be, for example, natural language inputs (e.g., spoken and/or typed) that indicate why one or more portions of a particular training instance are important for training the voice robot, such as one or more portions of the training instance inputs including a time feature, a date feature, a name feature, an account feature, an email address feature, a phone number feature, a money feature, a quantity feature, a product name feature, a location feature, an RPC request feature, and/or any other feature of the training instance inputs or training instance outputs for a given training instance. One or more corresponding feature emphasized inputs may be included in the training instance inputs for the corresponding training instances, used to bias updates of the plurality of ML layers corresponding to the voice robot being trained after processing the training instance inputs, and/or used as inputs to a network of pointers that cause the voice robot to focus on the one or more corresponding feature emphasized inputs during training for recognition. Thus, when the voice robot is subsequently deployed by a third party for a conversation, the trained voice robot can be made aware of the presence of these features.
In implementations in which one or more corresponding feature emphasized inputs are used as inputs to the network of pointers, the network of pointers can be used during training to process portions of the corresponding conversation (or representations thereof, such as conversation encodings, conversation insertions, conversation vectors, and/or other representations) and/or previous contexts of the corresponding conversation (or representations thereof, such as context encodings, context insertions, context vectors, and/or other representations). One or more tokens of the portion of the corresponding conversation can be labeled with one or more values (e.g., probabilities, log-likelihoods, binary values, and/or other values) that indicate whether the one or more tokens of the portion of the corresponding conversation are predicted to correspond to the one or more corresponding feature emphasis inputs. Further, the one or more values indicating whether the one or more tokens corresponding to the portion of the dialog are predicted to correspond to the one or more corresponding feature emphasized inputs can be compared to values of the one or more real values determined based on the one or more feature emphasized inputs provided by the third party developer.
For example, assume that the speech robot being trained is associated with a hypothetical market, assume that the training instance input includes at least a portion of a corresponding dialog corresponding to "I world like to purchase Product X if available", and assume that the corresponding feature emphasis input or inputs provided by the third party developer indicate a Product feature and a usability feature. In this example, one or more tokens corresponding to "Product X" and "available" may be associated with values indicating that these features are meaningful to properly respond to the portion of the corresponding dialog. However, assume that the pointer network determines that "purchase" is predicted to correspond to the feature emphasis input based on the processing and a predicted value (e.g., a probability of 0.5 indicating that "purchase" should be focused on), and determines that "Product X" is predicted to correspond to the feature emphasis input based on the processing and a predicted value (e.g., a probability of 0.6 indicating that "Product X" should be focused on). In this example, the predicted value 0.5 associated with "purchase" can be compared to a value of a real value (such as a probability of 0.0) because "purchase" is not provided by a third party developer as a corresponding feature emphasis input to generate a first loss, and the predicted value 0.6 associated with "Product X" can be compared to a value of a real value (such as a probability of 1.0) because "Product X" is provided by a third party developer as a corresponding feature emphasis input to generate a second loss. The values of these real values can be determined based on one or more corresponding feature emphasis inputs provided by third party developers. Further, the pointer network can be updated based at least on the first loss and the second loss (e.g., via back propagation). In other words, the pointer network is able to process training instance inputs for a given training instance to learn particular portions of the corresponding dialog included in the training instance inputs that should be focused on and/or previous contexts (or representations thereof) of the corresponding dialog. Thus, during training, the pointer network can be trained based on the same training instances used to train multiple ML layers corresponding to the speech robot.
In some implementations, the plurality of ML layers corresponding to the voice robot can further include an ML layer corresponding to a pointer network. The ML layer corresponding to the pointer network may be similar to the ML layer corresponding to the attention layer, but include differences. For example, the attention tier of a transform ML model is conventionally utilized in sequence-to-sequence processing to focus the transform ML model on an input sequence (e.g., an audio data stream) while generating an output sequence (e.g., a text stream corresponding to the audio data stream). Similarly, the ML layer corresponding to the pointer network can be utilized to focus the transform ML model on the input sequence while generating the output sequence. However, pointer networks focus the transform ML model on a particular portion of the input sequence (e.g., a particular word or phrase included in the input sequence). In some implementations, the ML layer corresponding to the pointer network can be part of a transform ML model described herein. In additional or alternative embodiments, the ML layer corresponding to the pointer network may be different from, but used in conjunction with, the transformer ML model described herein.
Thus, the pointer network can be used to predict portions of a conversation that make sense to multiple ML layers corresponding to a voice robot in responding to a user and/or interpreting what the voice robot is responding to the user in a particular manner. Further, multiple ML layers corresponding to the speech robot can utilize the predicted portions determined by the pointer network to bias selection of candidate responses provided in response. Continuing with the hypothetical market example, assume further that the portion of the corresponding dialog corresponding to "I world like to purchase Product X if available" is used as training instance input, and assume further that "Product X" is actually "available" for sale. Based on this availability, the predicted response may correspond to "It is available", "It is available, who like to purchase Product X? (has goods, do you want to buy product X). In selecting a predicted response in this example, one or more corresponding feature emphasized inputs provided by the third party developer may also be used to bias the predicted response. Thus, the one or more feature emphasized inputs may not only be used to initially train the pointer network to predict one or more particular portions of the corresponding dialog that are meaningful during training, but may also be used by the multiple ML layers corresponding to the voice robot to select a predicted response to that portion of the corresponding dialog.
In other words, the third party developer can interact with the voice robot development system 120 to provide feature emphasized input. The voice robot not only learns the specific features of the corresponding dialog that are important to the corresponding dialog by using the pointer network at the time of inference, but the voice robot is also able to learn how the predicted response may change or be biased based on the output generated using the pointer network at the time of inference. As a result, the trained voice bot can provide an indication to third party developers (e.g., via the dialog summarization engine 185 as described below) as to why it responded in a particular way when inferred.
By emphasizing inputs using corresponding features described herein, various technical advantages can be achieved. As one non-limiting example, a speech robot can achieve a given level of accuracy and/or robustness based on a given number of training instances by including corresponding feature emphasis inputs. Without including a corresponding feature emphasized input, a greater number of training instances will be required to achieve a given level of accuracy and/or robustness-or a given level of accuracy and/or robustness will not be achieved. As a result, the voice robot may be trained in a faster and efficient manner, thereby saving computing resources of a client device used to train the voice robot and/or network resources in implementations in which training instances, training losses, and/or other training data are transmitted over one or more networks.
In various embodiments, one or more of the plurality of training instances may be an RPC training instance. As used herein, an RPC training instance includes a training instance having a corresponding training instance input that includes at least a corresponding RPC inbound request and/or a corresponding training instance output that includes at least a corresponding RPC outbound request. The RPC outbound request included in the corresponding training instance output may indicate that the voice robot should generate an RPC request, and via oneOr multiple networks 199 2 The RPC request is transmitted to one or more third party systems 190 (e.g., a reservation system, an inventory system, a status checking system, and/or any other third party system). The RPC inbound request included in the corresponding training instance input may indicate that the voice robot should go through one or more networks 199 2 A response to the RPC request is received from one or more third party systems 190 and processed to generate an output based on the response. Although in FIG. 1 it is associated with network 199 1 Network 199 is depicted separately 2 It will be understood, however, that this is for clarity and is not meant to be limiting. For example, network 199 2 And network 199 1 May be the same network or a different combination of networks described herein. Because the RPC requests are not directly associated with the corresponding dialog on which the multiple training instances for training the voice robot are generated (e.g., not directly captured in spoken or typed input of the dialog), the third party developer may need to define RPC outbound requests and RPC inbound requests for the training instances, the particular third party system to which the RPC outbound requests in the one or more third party systems 190 should be directed, the format of the RPC requests, the format of the responses to the RPC requests, and/or any other information associated with the RPCs.
In implementations where the user input engine 111 detects the user's spoken input via the microphone of the client device 110 when obtaining the training instance as described above, audio data capturing the spoken input can be processed. In some implementations, the ASR engine 130A1 of the client device 110 can process the audio data that captures the spoken input using the ASR model 130A. In additional or alternative embodiments, the client device 110 can be capable of communicating over the network 199 1 The audio data is communicated to the speech robot development system 120 and the ASR engine 130A2 is able to process the audio data that captures the spoken input using the ASR model 130A. The speech recognition engines 130A1 and/or 130A2 can generate multiple speech hypotheses for the spoken input based on processing of the audio data, and can optionally select a particular speech hypothesis for the spoken input based on corresponding values (e.g., probability values, log-likelihood values, and/or other values) associated with each of the multiple speech hypothesesIs a recognized text for spoken input. In various embodiments, ASR model 130A is an end-to-end speech recognition model such that ASR engines 130A1 and/or 130A2 are able to generate multiple speech hypotheses directly using the model. For example, the ASR model 130A can be an end-to-end model for generating each of a plurality of speech hypotheses on a character-by-character basis (or on other token-by-token basis). One non-limiting example of such an end-to-end model for generating recognition text on a character-by-character basis is the recurrent neural network converter (RNN-T) model. The RNN-T model is a form of sequence-to-sequence model that does not employ a mechanism of attention. In other embodiments, ASR model 130A is not an end-to-end speech recognition model, such that ASR engines 130A1 and/or 130A2 can instead generate predicted phonemes (and/or other representations). For example, the ASR engines 130A1 and/or 130A2 may then utilize the predicted phonemes (and/or other representations) to determine a plurality of speech hypotheses that fit the predicted phonemes. In doing so, the ASR engines 130A1 and/or 130A2 can optionally employ decoding graphs, dictionaries, and/or other resources. In various implementations, corresponding transcriptions can be rendered at the client device 110 (e.g., associated with training instance inputs, training instance outputs, corresponding feature emphasis inputs, demonstrative dialogs, and/or other aspects of a voice robot development platform).
In some versions of those embodiments, NLU engine 140A1 of client device 110 and/or NLU engine 140A2 of speech robot development system 120 can process the recognized text generated by ASR engines 130A1 and/or 130A2 using NLU model 140A to determine the intent included in the spoken input. For example, if the client device 110 detects spoken input from a third party developer of "add training instance input of ' do you have a reservation of at6: (e.g., as part of a separate spoken input defining the training instance input) ' (i.e., adding a training instance input of ' do you have two bits of subscription at6 pm), the client device 110 can process the audio data of the captured spoken input using the ASR models 130A1 and/or 130A2 to generate recognized text corresponding to the training instance input, and can process the recognized text using the NLU model 140A to at least determine an intent to add the training instance input (e.g., which can include audio data for the spoken input and/or corresponding speech hypotheses).
In some versions of those embodiments, TTS engine 150A1 of client device 110 and/or TTS engine 150A2 of speech robot development system 120 is capable of generating synthetic speech audio data that captures the synthetic speech. The synthesized speech can be rendered at the client device 110 using the rendering engine 112 and via speakers of the client device 110. The synthesized speech may capture any output generated by the speech robot development described herein, and may include, for example, an indication that a training instance has been added (or that a particular training instance input is repeated, a training instance output, a feature emphasis input, etc.), a notification that a third party developer is requested to add one or more additional training instances or a set of training instances (and optionally those training instances associated with a particular feature), a notification that a third party developer is requested to modify the ground of one or more existing training instances or training instances (and optionally those training instances associated with a particular feature), an indication that training of the speech robot has been initiated, completed, or a status update regarding the training of the speech robot, and/or any other information related to the speech robot or speech robot development platform that can be audibly communicated to the third party developer.
The training engine 162 can train the speech robot (e.g., its ML layer) using a plurality of training instances obtained by the training instance engine 161 (e.g., stored in the training instance database 161 a). The voice robot can correspond to one or more processors that utilize multiple Machine Learning (ML) layers of one or more ML models (e.g., stored in ML layer database 170 A1) to converse on behalf of a third party for a phone call associated with the third party. The plurality of ML layers may correspond to ML layers of a transform ML model (e.g., an input layer, an encoding layer, a decoding layer, a feed-forward layer, an attention layer, an output layer, and/or other ML layers), unidirectional and/or bidirectional RNN models (e.g., an input layer, a concealment layer, an output layer, and/or other ML layers of other ML models.
For example, and referring to fig. 2A, an exemplary process flow 200A for training a voice robot is depicted. In some embodiments, training instance engine 161 can obtain a given training instance from among multiple training instances associated with the voice robot stored in training instance database 161A. In some implementations, for a given training instance, the training instance input can include at least audio data 201 corresponding to a portion of a corresponding conversation and a conversation context 202 for the corresponding conversation. Further, for a given training instance, the training instance output can include the true value response 203 for that portion of the dialog. Audio data 201 can be processed by ASR engines 130A1 and/or 130A2 using ASR model 130A to generate a plurality of speech hypotheses 204. In other implementations, the training example input may include a plurality of speech hypotheses 204 generated based on the audio data 201, but may not include the audio data 201 itself.
In some implementations, the encoding engine 162A1 can process the plurality of speech hypotheses 204 using a first ML layer of a plurality of ML layers stored in the ML layer database 170A1 to generate the first encoding. The encoding engine 162A1 can process the dialog context 202 using a first ML layer of a second ML layer of the plurality of ML layers stored in the ML layer database 170A1 to generate a second encoding. Further, the concatenation engine 162A2 can concatenate the first encoding and the second encoding to generate a concatenated encoding. The concatenated coding may represent the current state of the corresponding dialog. For example, concatenated coding can encode the history of a conversation and the most recent portion of a conversation to encode the entire conversation as a whole.
By encoding the current state of the corresponding dialog, the dialog's conversation can be tracked, thereby enabling the speech robot to model and/or learn the state of the corresponding dialog. Thus, the resulting trained voice robot may learn the corresponding values of the parameters associated with the task being performed via the corresponding phone call request. For example, assume that the training instance input includes at least a portion of a corresponding dialog corresponding to "Hello, do you have an any of Product X available for sale" and so on. In this example, the voice robot is trained to understand that the person is requesting an inventory check on product X. Further, by encoding the dialog context 202 of the corresponding dialog, the voice robot is also trained to understand that, if product X is available, the person has not provided any corresponding values for a name parameter associated with purchasing or placing product X in a state to be paid for, a money parameter associated with purchasing product X, an address parameter if the person wishes to ship product X to his or her residence, and so on. Thus, the voice robot can be trained to subsequently prompt the person for corresponding values of one or more of these parameters by tracking the state of the session.
Further, the embedding engine 162A3 can process concatenated coding using one or more of the plurality of ML layers to generate predictive embedding associated with the predicted response 205 (e.g., perform RPCs with a third-party system, synthesized speech or text provided in response to training instance input, answer an incoming phone call, initiate an outgoing phone call, and/or other responses predicted to be responsive to training instance input). The predicted response 205 may be selected from a plurality of candidate responses in the candidate response database 171A (e.g., which includes the true-value response 203 and a plurality of additional candidate responses). In generating the prediction embedding, the size of the concatenated coding can be reduced to a fixed dimension. This enables the predicted embedding associated with the predicted response 205 to be easily compared in the embedding space with other embedding described with respect to the loss engine 162 A4.
In some versions of those embodiments, and prior to processing the plurality of speech hypotheses 204, training engine 162 is able to cause the plurality of speech hypotheses to be aligned. For example, assume that multiple speech hypotheses capture verbal input of "for 4PM (4 PM)". In this example, multiple speech hypotheses can be aligned as [ for, # empty,4PM; for,4, PM; four, four, PM ] so that each of a plurality of aligned speech hypotheses can then be processed in combination with one another. In some further versions of those embodiments, the training engine 162 can further cause multiple aligned speech hypotheses to be annotated. Continuing the above example, multiple aligned speech hypotheses can be annotated as [ for, # empty (@ null), 4PM (@ time); for,4 (@ time), PM (@ time); four (@ time), four (@ time), PM (@ time) ].
In embodiments where the training example input also includes audio data 201, in addition to or instead of the encoding generated based on the plurality of speech hypotheses, encoding engine 162A1 may be capable of generating an encoding associated with audio data 201. In these implementations, the concatenation engine 162A2 can process the encoding associated with the audio data 201 and the encoding associated with the dialog context 202 to generate a concatenated encoding. Further, the embedding engine 162A3 can process concatenated coding using one or more of the plurality of ML layers to generate the predictive embedding associated with the predicted response 205.
In various embodiments, and although not depicted in fig. 2A, training engine 162 is capable of further training the point network corresponding to the ML layer stored in ML layer database 170 A1. In these embodiments, and during training, one or more corresponding feature emphasis inputs can be received from third party developers and can be used to train the pointer network. Initially, the training engine 162 can cause the pointer network to process the audio data 201, the dialog context 202, and/or the plurality of speech hypotheses 204 to generate output. The output can include, for example, an indication of one or more features of the conversation (e.g., date features, time features, RPC features, etc.) predicted to be meaningful in determining how to respond to the portion of the conversation captured in the audio data 201, as well as a corresponding value (e.g., binary value, probability, log-likelihood, etc.) that indicates how meaningful a particular portion of the conversation is predicted to be in determining how to respond to the portion of the conversation captured in the audio data 201. One or more features (and corresponding values) can be compared to one or more corresponding feature emphasized inputs (and values of corresponding true values for each of the one or more corresponding feature emphasized inputs) to generate one or more penalties for updating the pointer network.
Moreover, and in addition to training the pointer network based on one or more corresponding feature emphasized inputs for a given training instance, encoding engine 162A1 and/or embedding engine 162A3 may also process one or more of the corresponding feature emphasized inputs associated with the given training instance, provided as side inputs, as well as audio data 201, dialog context 203, and/or a plurality of speech hypotheses 204. When processing one or more corresponding feature emphasized inputs, the resulting predictive embedding can bias towards candidate responses associated with the one or more corresponding feature emphasized inputs. For example, if the one or more corresponding feature emphasis inputs include a time feature for a restaurant reservation, the candidate response can be biased towards candidate responses associated with initiating an outbound RPC request to determine whether the time indicated by the time feature is available for the restaurant reservation.
Further, while the encoding engine 162A1, the cascading engine 162A2, and the embedding engine 162A3 are described herein as performing particular functions in a particular order, it should be understood that the performance of these particular functions may be reordered, and/or one or more of these engines may be omitted. For example, encoding engine 162A1 may be omitted, and embedding engine 162A3 may be capable of processing the plurality of speech hypotheses 204 and the dialog context 202 using respective ML layers of the plurality of ML models to generate a predictive embedding associated with a predicted response to at least the portion of the corresponding dialog associated with the plurality of speech hypotheses.
Further, in various embodiments, the loss engine 162A4 can compare the predicted embedding associated with the predicted response 205 and the actual-value embedding associated with the actual-value response 203 in an embedding space to generate one or more losses 206. Predictive embedding and real-valued embedding can correspond to lower-dimensional representations of the predictive response 205 and the corresponding real-valued response 203, respectively. The embedding space allows comparison of these lower-dimensional embeddings. Furthermore, the predictive embeddings associated with the predictive response 205 should approximate the corresponding real-valued embeddings associated with the corresponding real-valued response 203 in an embedding space. In other words, in processing at least the portion of the corresponding conversation and the previous context of the conversation, the system should predict a response similar to the actual response to at least the portion of the corresponding conversation. For example, a distance metric (e.g., cosine similarity distance, euclidean distance, and/or other distance metric) between a predicted embedding and a corresponding true value embedding in the embedding space can be determined, and one or more penalties 206 can be generated based on the distance metric.
In some implementations, different ML layers of different ML models (not depicted) can be used to generate the real-value embedding associated with the real-value response 203 while training the speech robot based on a given training instance, the different ML layers being different from the plurality of ML layers utilized in generating the predictive embedding associated with the predictive response 205 (e.g., dot-product architecture). The true value may then be embedded and stored in candidate response database 171A to be used as one of a plurality of candidate responses when inferred. Notably, the different ML layers can additionally or alternatively be updated based on the one or more losses 206 such that the different ML layers learn respective portions of the embedding space to assign to the real-value embedding (and the multiple ML layers and/or one or more of the different ML layers can optionally remain fixed). Moreover, the corresponding embedding associated with one or more responses that are incorrect responses to portions of the corresponding conversation may additionally or alternatively be used as a negative example to further distinguish the correct embedding for the portions of the corresponding conversation in the embedding space. After updating the different ML layers, a plurality of additional candidate responses can be processed using the updated different ML layers to generate corresponding candidate response embeddings. These candidate response embeddings and corresponding candidate responses can also be stored in candidate response database 171A, even if they are not used to train the voice robot. Thus, at inference, different ML layers may be omitted, as candidate response embedding and corresponding candidate responses are known. In additional or alternative embodiments, the actual value embedding can be stored in the training instance database 161A in association with the actual value response for a given training instance.
The update engine 162A5 can cause one or more of the plurality of ML layers to be updated based on the one or more losses 206 (and one or more of the plurality of ML layers can optionally remain fixed). For example, the update engine 162A5 can cause the one or more penalties 206 to be propagated back across one or more of the plurality of ML layers to update the respective weights of the one or more of the plurality of ML layers. In some implementations, the update engine 162A5 can bias updating one or more of the plurality of ML layers with one or more of the corresponding feature emphasized inputs for a given training instance. One or more of the plurality of ML layers can be further updated based on additional training instances obtained by training instance engine 161 in the same or similar manner as described above. In some embodiments, the voice robot may be trained in this manner until one or more conditions are met. The one or more conditions can include, for example, validation of one or more of the updated plurality of ML layers or the plurality of additional ML layers, convergence (e.g., with zero loss or within a threshold range of zero loss) of one or more of the updated plurality of ML layers or the plurality of additional ML layers, a determination that one or more of the plurality of ML layers or the plurality of additional ML layers (with respect to accuracy and/or recall) performs better than an instance of the voice robot currently being utilized (if any), an occurrence of training based on at least a threshold number of training instances, and/or a duration of training based on the training instances.
Although the voice robot is described as being trained in a particular manner and using a particular architecture, it should be understood that this is for purposes of example and is not meant to be limiting. For example, a voice robot associated with a fictitious restaurant named Hypothetical Caf can be used as a baseline voice robot when training a voice robot associated with a Hypothetical market. In this example, one or more migration learning techniques (e.g., meta-learning) may be utilized to adapt the voice bot associated with the Hypothetical Caf (or the output generated based on those training instances) for the voice bot associated with the Hypothetical market. For example, the training instance input may include an additional input indicating that the voice robot associated with the Hypothetical market is being trained for a different retail-related purpose, while the original voice robot associated with the Hypothetical Caf is being trained for restaurant purposes.
Refer briefly back to FIG. 1, and are in trainingAfter training the voice robot, the voice robot engine 170 can enable subsequent dialogs with the trained voice robot on behalf of the third party for telephone calls associated with the third party, and can include a response engine 171 and a Remote Procedure Call (RPC) engine 172 in various embodiments. The trained voice robot is able to make phone calls with a person initiating an incoming phone call or answering an outgoing phone call or an additional voice robot via a respective additional client device 195. Over one or more networks 199 using voice communication protocols (e.g., voice over Internet protocol (VoIP), public Switched Telephone Network (PSTN), and/or other telephony communication protocols) 3 A telephone call is made. Notably, the calls can be cloud-based phone calls such that the client device 110 used to train the voice robot is not an endpoint of the corresponding phone call. Instead, the voice robot development system 120 (e.g., one or more servers) may function with one of the additional client devices 195 as an endpoint for a telephone call.
For example, and with specific reference to FIG. 2B, assume that the third party for whom the voice robot is trained is a fictitious retail entity named fictitious marketplace that sells various products. Assume further that a person provides user input at a respective one of the additional client devices 195 to initiate a telephone call with the hypothetical market, the voice robot answers the incoming telephone call initiated by the person, and causes synthesized speech audio data (e.g., generated using the TTS model 150A) that captures synthesized speech corresponding to the particular introduction to the voice robot to be audibly rendered at the respective one of the additional client devices 195 such that the synthesized speech is perceivable by the person via a speaker of the respective one of the additional client devices 195. Assume further that the person provides the spoken utterance "Hello, do you have an and of Product xaavaiable for sale" in response to an audible rendering of the synthesized speech audio data. The spoken utterance may be captured in audio data 207 transmitted over one or more networks to the voice robot development system 120 of fig. 1.
ASR engine 130A2 can process audio data 207 using ASR model 130A to generate a plurality of speech hypotheses 209 corresponding to the spoken utterance. The voice robot engine 170 can optionally cause the plurality of voice hypotheses 209 to be aligned and/or annotated. Further, response engine 171 can process one or more of the dialog context 208 and/or audio data 207 of an incoming telephone call originated by a person using multiple ML layers stored in ML layer database 170A1 to generate a response embedding. In some implementations, in response to determining that the spoken utterance provided by the user is complete, the audio data 207 may be processed only by the ASR engine 130A2 and/or the plurality of speech hypotheses 209 may be processed only by the response engine 171. For example, the speech robot development system 120 can process the audio data using an endpoint model trained to detect when the person is finished providing the spoken utterance to determine that the person is finished providing the spoken utterance after speaking the word "sale".
In some implementations, the response engine 171 can compare the response embedding to a plurality of candidate response embeddings associated with a plurality of candidate responses stored in the candidate response database 171A. Further, the response engine 171 can select a given candidate response of the plurality of candidate responses as the response to the spoken utterance 210 based on a distance metric between the response embedding in the embedding space and one or more of the plurality of candidate response embedding associated with the plurality of candidate responses. For example, a candidate response associated with a corresponding distance metric that satisfies a distance threshold may be selected as response 210. The response 210 can be processed by the TTS engine 150A2 using the TTS model 150A to generate the synthesized speech audio data 212 that captures the response 210. Further, the synthesized speech audio data 212 can be audibly rendered at a respective one of the additional client devices 195.
In various implementations, audio data 207, dialog context 208, and/or multiple speech hypotheses 209 can be processed using a trained pointer network trained in the manner described above with reference to fig. 1 and 2A. The pointer network can identify particular portions of a conversation or conversation context that are predicted to be meaningful in selecting a response provided in response to the audio data 207 and/or in generating an RPC outbound request (e.g., a feature emphasized output) in response to the audio data 207. In these implementations, particular portions of a conversation or conversation context identified using the pointer network can be provided as input to the response engine 171 (e.g., as side input to multiple ML layers corresponding to a trained voice robot). Further, the response engine 171 can bias the selection of the response 210 and/or the generation of the RPC outbound request 211A. Further, as described in more detail with reference to the dialog summary engine 185, the feature emphasis output can be used to generate a dialog summary that provides a summary of the dialog and an explanation of what the trained voice robot is acting in a particular way.
In some implementations, the response engine 171 can determine that RPC requests are required to respond to spoken utterances captured in the audio data 207. In some versions of these embodiments, the RPC engine 172 can generate RPC outbound requests 211A and transmit the RPC outbound requests to one or more third party systems 190. Continuing with the above example, the response engine 171 can determine that an RPC request is needed to determine whether the hypothetical market has any inventory of "Product X" for sale. Thus, the RPC engine 172 can generate a structured request (e.g., inventory = Product X, intent = sell) as the RPC outbound request 211A that is communicated to the inventory third party system 190. The RPC engine 172 can receive RPC inbound requests 211B in response to RPC outbound requests 211B. For example, the RPC inbound request 211B may indicate "Product X" is available for sale or unavailable for sale via the hypothetical market. In embodiments where the response engine 171 determines that an RPC request is required, one or more instances of synthesized speech audio data associated with the RPC outbound request 211A (e.g., "hold on a second white I check (please look up a little, etc.)") and/or one or more instances of synthesized speech audio data associated with the RPC inbound request 211A (e.g., "yes, we have Product X available for sale, we have good to purchase?) can be rendered at a respective one of the additional client devices 195 in the same or similar manner as described above.
The process can be repeated to generate a corresponding response to the spoken utterance provided by the person until the telephone call is completed. The telephone call with the person may be stored in the voice activity database 170 A2. For example, for a given telephone call, the voice activity database 170A2 may include audio data corresponding to spoken utterances of the person, synthesized voice audio data corresponding to synthesized voices of the voice call, results of the given telephone call, duration of the given telephone call, time and/or date associated with the given telephone call, and/or other information derived from the given telephone call. In some implementations, the voice robot may request that the person agree to interact with the voice robot before participating in the conversation. In embodiments where the person agrees to participate in a conversation during a telephone call using voice, the voice robot may participate in a conversation with the user. In embodiments where the person does not intend to participate in the conversation during the telephone call using voice, the voice robot may end the telephone call or request additional people associated with the third party to join the telephone call.
Referring back to fig. 1, the error recognition engine 180 can process the voice robot activity stored in the voice robot activity database 170A2 using multiple ML layers of the ML model stored in the ML layer database 180A to recognize any behavioral errors of the voice robot. The identified behavioral errors can be classified into one or more different error categories based on output generated using the plurality of ML layers. The one or more different categories of errors can include, for example, the voice robot prematurely terminating the call, the voice robot failing to provide a response and timing out, the voice robot failing to request corresponding values for parameters needed to complete the desired action for the person, the voice robot failing to recognize corresponding values for parameters provided by the person to complete the desired action for the person, the voice robot failing to perform an RPC when needed, the voice robot performing an RPC with an incorrect third-party system, and/or any other behavioral errors of the voice robot that may occur during a corresponding telephone call. In other words, if the root causes of the identified behavioral errors are the same, the identified behavioral errors can be classified into one or more of the same or different error categories.
In some implementations, the fault recognition engine 180 can automatically perform one or more actions to correct these recognized behavioral problems. The one or more actions can include, for example, synthesizing a new training instance for retraining the voice robot and/or modifying an existing training instance for retraining the voice robot. For example, the misidentification engine 180 may determine that the voice robot has confused multiple features included in portions of the corresponding dialog, but that there is a sparsity problem in training instances that include these features that prevents the voice robot from being able to consistently distinguish the multiple features. In this example, the error recognition engine 180 may generate a synthetic training instance that includes one or more of the plurality of features to address the sparsity problem and cause the voice robot to be retrained based on a plurality of training instances associated with the voice robot (including the generated synthetic training instance). As another example, the error recognition engine 180 may additionally or alternatively modify re-labeling one or more existing training instances to further distinguish one or more of the plurality of features.
In some additional or alternative implementations, the error recognition engine 180 can cause a notification to be presented to a third party developer. The notification can include an indication of one or more actions that, when performed by the third party developer, should correct the identified behavioral issues. The notification can be rendered along with the dialog summary for the corresponding call, or via a separate interface (e.g., pop-up notification, notification interface, etc.). The one or more actions can include, for example, adding a new training instance for retraining the voice robot and/or modifying an existing training instance for retraining the voice robot. For example, the error recognition engine may present one or more training instances and prompt a third party developer to identify one or more corresponding feature-emphasized inputs for one or more of the plurality of training instances, add more training instances that include features of the one or more training instances, re-label the training instances to include one or more different labels, and/or any other action that may correct the root cause of the identified behavioral error.
The conversation summary engine 185 can generate a corresponding conversation summary for each phone call made by the voice robot based on the voice robot activity stored in the voice robot activity database 170 A2. The corresponding conversation digest can be rendered at a user interface of the client device 110 using the rendering engine 112. In some implementations, the corresponding conversation digest can include, for example, a natural language digest of each of the corresponding telephone calls, a duration of the corresponding telephone call, an outcome or result of the corresponding telephone call, monetary information associated with the corresponding telephone call, and/or other information associated with the telephone call, such as a particular reason for what the voice robot emphasized the output provided a particular output based on one or more features generated during the conversation. Continuing with the hypothetical market example, the corresponding dialog digest may be, for example, "user called to acquire out availability of Product X, I clicked to make Product X was available, the user purchased Product X for
Thus, the voice robot development platform described herein enables third party developers associated with third parties to train voice robots, monitor the execution of the voice robots, and then update the voice robots based on any identified behavioral errors of the voice robots. Notably, the voice robot development platform is example-based in that the voice robot is trained based on portions of the conversation and updated based on adding more examples or modifying existing examples. Thus, third party developers need not have any extensive knowledge of ML or how to define various intent patterns, which may be required to develop rule-based voice robots.
Although the voice bot is described herein as being subsequently deployed for conducting a conversation on behalf of a third party for a telephone call associated with the third party, it should be understood that this is for purposes of example and is not meant to be limiting. For example, the voice robots described herein can be deployed in any scenario in which a person can participate in a human-machine conversation with a given voice robot. For example, a given voice robot can be trained to talk to a person at a car restaurant via a car restaurant (drive thru) system, to talk to the person as an automated assistant via the person's client device, and/or any other area beyond where the person can participate in a phone call for a human-machine conversation with the given voice robot. Thus, it should be understood that the behavior of these voice robots may be based on training examples used to train the corresponding voice robots.
By using the techniques described herein, various technical advantages can be achieved. As one non-limiting example, the voice robot development platform enables training of a voice robot based on examples of conversations rather than predefined intent patterns. This allows voice robot behavior to be easily added or modified by adding new training instances or modifying existing training instances. Thus, a voice robot trained using the voice robot development platform described herein is more scalable and reduces memory consumption since a large number of intent patterns need not be defined. Accordingly, the ML model trained and utilized can have a smaller memory footprint, and can be more robust and/or accurate. Further, the voice robot trained using the voice robot development platform achieves a high level of accuracy and recall, enabling telephone calls to be terminated more quickly and efficiently because the voice robot trained using the voice robot development platform is more able to understand the nuances of human speech and respond accordingly.
Turning now to fig. 3A, 3B, and 3C, various non-limiting examples of a user interface 300 associated with a voice robot developer platform are depicted. A third party developer can interact with the voice robot development platform using a client device (e.g., client device 110 of fig. 1) that includes a voice robot development system client or a voice robot development system. By interacting with the voice robot development platform, third party developers can train voice robots that, when deployed, can converse on behalf of third parties associated with the third party developers for incoming telephone calls directed to the third parties and/or outgoing telephone calls initiated on behalf of the third parties. For purposes of the example throughout fig. 3A, 3B, and 3C, assume that a third party developer is creating a new voice bot to converse for a phone call associated with a Hypothetical Caf.
Referring specifically to fig. 3A, a home screen or login page of the voice robot development platform is depicted as being visually rendered on the user interface 300. In various embodiments, various graphical elements may be presented to the third party developer on a home screen or a login page. For example, the user interface 300 may include a voice robot graphical element 310 that provides a segment of any unique identifiers associated with voice robots developed by third party developers and/or any voice robots associated with third parties (e.g., the contextual Caf). Upon creating a new voice bot, the third party developer can provide a unique identifier in text entry field 318 to associate with the new voice bot being developed. For example, as shown in fig. 3A, the third-party developer may provide a typed input of "Hypothetical Caf" in text input field 318, or capture a spoken input of audio data corresponding to "Hypothetical Caf" (and optionally, in response to a user selection of microphone interface element 350). In some embodiments, the third party developer may select a "view more" graphical element as shown in FIG. 3A to extend a segment of the voice bot (if any other segments exist), to include additional voice bots, or to launch a voice bot interface on user interface 300.
Further, user interface 300 may additionally or alternatively include a training instance graphical element 320 that provides a snippet of multiple training instances for use in creating a new speech robot. Each training instance of the plurality of training instances can include a training instance input and a training instance output. The training instance input can include, for example, at least a portion of the corresponding conversation and a previous context of the corresponding conversation, and the training instance output can include, for example, a corresponding true value response to at least the portion of the corresponding conversation. The plurality of training instances can be obtained, for example, from an existing phone call corpus associated with a hypthetical Caf é (or another restaurant entity), from a demonstrative conversation between one or more persons (e.g., which may or may not include a developer), and/or from one or more other spoken utterances of one or more persons corresponding to segments of the conversation (e.g., which may or may not include a developer). Obtaining a plurality of training instances and training a voice robot associated with a Hypothetical Caf is described below (e.g., with reference to FIG. 3B). In some embodiments, a third party developer may select an "add training instance" graphical element as shown in FIG. 3A to add a training instance for training a voice robot associated with a Hypothetical Caf, or to launch a training instance interface on the user interface 300.
Further, the user interface 300 may additionally or alternatively include a voice robot activity graphical element 330 that provides a segment of voice robot activity associated with the trained voice robot. The voice robot activity can include information related to each corresponding phone call made by the trained voice robot on behalf of the contextual Caf. For example, the voice robot activity can include a time and/or date associated with each corresponding phone call, a duration of each corresponding phone call, a summary associated with each corresponding phone call, a transcription associated with each corresponding phone call, and/or any other information related to each corresponding phone call conducted by the trained voice robot on behalf of the Hypothetical Caf. In some implementations, the voice robot activity can be generated during and/or after each corresponding phone call. Voice robot activity enables third party developers to monitor the execution of voice robots. Voice robot activity is described below (e.g., with reference to fig. 3C). In some implementations, the third party developer can select a "see more" graphical element as shown in FIG. 3A to extend the segments of the voice robot activity (if any segments exist) to include additional voice activity or to launch a voice robot activity interface on the user interface 300.
Still further, the user interface 300 may additionally or alternatively include a voice robot behavior error graphical element 330 that provides a segment of recognized voice robot behavior errors associated with the trained voice robot. The voice robot behavior errors can include errors made by the trained voice robot during a corresponding phone call on behalf of the Hypothetical Caf. These voice robot behavior errors can include, for example, accepting or suggesting unavailable time for restaurant reservations, providing incorrect business hours, accepting unavailable food orders, and/or any other errors corresponding to incorrect behavior of the trained voice robot. The voice robot behavioral errors enable the voice robot development platform to identify corresponding root causes of the voice robot behavioral errors. In some implementations, the voice robot development platform may take one or more actions to automatically correct the corresponding root cause, such as re-labeling one or more of the plurality of training instances for retraining the voice robot, adding one or more feature-emphasizing inputs to one or more of the plurality of training instances for retraining the voice robot, and/or any other action that may be taken by the voice robot training platform to correct the corresponding root cause of the recognized voice robot behavior error. In additional or alternative embodiments, the voice robot development platform may generate one or more notifications to notify third party developers of the root cause of the identified voice robot behavioral errors. The notifications can optionally include an indication of one or more actions that, when performed by the third party developer, can correct the corresponding root cause of the identified voice robot behavior error, such as requesting the third party developer to re-label one or more of the plurality of training instances used to retrain the voice robot, requesting the third party developer to add one or more feature-emphasized inputs to one or more of the plurality of training instances used to retrain the voice robot, requesting the third party developer to add one or more additional training instances used to retrain the voice robot (and optionally having one or more specific labels or specific feature-emphasized inputs), and/or any other action that can be taken by the third party developer to correct the corresponding root cause of the identified voice robot behavior error. In some embodiments, the third party developer may select a "see more" graphical element as shown in FIG. 3A to expand the recognized segments of the voice robot behavior error (if any exist) or to launch the voice robot behavior error interface on the user interface 300.
The third party developer can navigate the home page or login page of the voice robot shown in fig. 3A to create a new voice robot. For example, assume that a third party developer provides a unique identifier of "Hypothetical Caf" for a voice robot in text input field 318 and selects an "add tracing instance(s)" graphical element. The training example interface may be presented to the third party developer via a user interface 300 as shown in FIG. 3B. The third party developer may interact with the training instance interface to define a training instance for training the voice robot.
In some implementations, the training examples can be obtained from a corpus of training examples. The training instance corpus can include, for example, one or more previous conversations between a user (e.g., an employee) associated with a Hypothetical Caf and an additional user (e.g., a customer) during a corresponding previous telephone call, one or more previous conversations between other users not associated with a Hypothetical Caf during a corresponding previous telephone call (e.g., a telephone call associated with another restaurant entity), and/or other conversations on which training instances can be generated. For example, in response to receiving user input directed to training instance corpus interface element 380, the third party developer can access the training instance corpus to select a portion of the corresponding dialog to use as training instance input 321A for the given training instance (and any previous context of the corresponding dialog), and select a corresponding response to the portion of the corresponding dialog to use as training instance output 322A for the given training instance. User input directed to training example corpus interface element 380 can be, for example, touch input detected via a touch screen or via a user interface input device (e.g., a mouse or stylus) and/or spoken input detected via a microphone of a client device (and optionally in response to user input directed to voice interface element 350). In various embodiments, a third party developer can optionally define a feature emphasis input 323A for a given training instance. In these embodiments, the feature emphasized input 323A can be used to train a pointer network and/or multiple ML layers corresponding to a voice robot, as described above with reference to fig. 1 and 2A. The pointer network can be part of the plurality of ML layers corresponding to the voice robot, or a separate ML layer used in conjunction with the plurality of ML layers corresponding to the voice robot.
In some additional or alternative implementations, the training examples can be obtained from user input received at a training example interface presented to the user via user interface 300. The user input received at the training example interface can be, for example, touch or typing input detected via a touch screen or via a user interface input device (e.g., a mouse, stylus, keyboard, etc.) and/or spoken input detected via a microphone of the client device (and optionally in response to user input directed to the voice interface element 350). For example, the user can provide user input including one or more of training instance input 321A and training instance output 322A (and optionally feature emphasis input 323A) in the table of training instances shown in fig. 3B.
In some additional or alternative implementations, the training instance can be obtained from a demonstrative dialog 352B. The demonstrative dialog 352B may then be utilized to generate a plurality of training instances for training the voice robot associated with the hypokinetic Caf. For example, as illustrated by the demonstrative dialog in fig. 3B, the third-party developer (and optionally another person) may be able to act according to different roles to simulate an actual dialog between a person associated with the hypthetical Caf é (e.g., an employee) and another person (e.g., a customer) by providing user input (e.g., typed input or spoken input) for the demonstrative dialog. For example, a third party developer can select the employee graphical element 362A and provide user input 352B1? (what I can do for you, your great, hypothetical Caf), "select custom graphical element 362B and provide user input 354B1" Hi, I would like to book enabled for four foot at 6PM tonight), "select employee graphical element 362A and provide user input 352B2" let me check, "followed by user input 352B3" I "m socket, wenly have7PM and 8PM available for four foot (for nothing, we have only 7pm at night and 8 at night)". Selecting the guest graphical element 362B and providing user input 354b2 ' 7pm will work ' (you can at night) ', selecting the employee graphical element 362A and providing user input 352b4 at white s the name? (what is your name). The voice robot development platform is able to automatically generate a plurality of training instances based on the demonstrative dialog 352B. However, the third party developer may need to specify any feature-emphasized input for the training instance generated based on the demonstrative dialog 352B.
For example, assume that training instance input 321A1 generated based on the demonstrative dialog 352B includes an indication that there is an incoming telephone call, and that training instance output 322A1 includes a corresponding response to the incoming telephone call, such as answering the incoming telephone call and providing an output corresponding to user input 352B 1. In this example, the feature emphasis input 323A1 can correspond to an introductory feature for the incoming telephone call. The referral for the incoming telephone call may be user input 352B1, options presented via an Interactive Voice Response (IVR) system, and/or other referrals that a third party developer may desire voice robot learning. Notably, there is no prior dialog context for training example input 321A1, as there is no prior portion of the demonstrative dialog 352B. Thus, a voice robot trained on this training instance is able to learn how to answer incoming phone calls. In embodiments where the training example input and/or the training example output is based on user input, the user input may correspond to audio data that captured the user input, a plurality of speech hypotheses generated based on processing the audio data, and/or text that corresponds to the user input.
As another example, assume that training instance input 321A2 generated based on the demonstrative dialog 352B includes a portion of the demonstrative dialog 352B corresponding to the user input 354B1 and as such
RPC outbound requests for availability can include, for example, generating a structured request to query restaurant reservation availability for a particular party population at the requested time (e.g., availability: [ party population ] =4; [ time ] =6PM or any other form of structured request), and transmitting the structured request to a third-party system associated with managing restaurant reservations for the Hypothertical Caf. Although RPC outbound requests are not explicitly included in the demonstrative dialog 352B, the third-party developer can add or inject RPC outbound requests for availability into the training instance output 322 A2. Further, although the RPC outbound request is transmitted to a third party system (and not a "client" in the demonstrative dialog 352B), the voice robot can still be trained to generate and transmit an RPC outbound request for availability during the demonstrative dialog 352B based on the training instance input 321A2 requesting availability of restaurant reservations being an RPC outbound request training instance. Further, while the RPC outbound request is described as being associated with restaurant reservation availability, it should be understood that this is for purposes of example and is not meant to be limiting. For example, RPC outbound requests may be associated with food/inventory availability, business hours inquiries, transfer telephone calls, and/or any other function that requires interaction with one or more third party systems during a telephone call. Thus, a voice robot trained on this RPC outbound request training instance can learn when and how to initiate RPC outbound requests.
As yet another example, assume that the training instance input 321A3 generated based on the demonstrative dialog 352B includes a portion of the demonstrative dialog 352B corresponding to the RPC inbound request and a user input, e.g., a user input, corresponding to the RPC inbound request
The RPC inbound request with availability can include, for example, receiving a structured response that includes an indication of whether there are any restaurant reservations that satisfy the parameters of the reservation request (e.g., party size 4, time 6 PM), and optionally one or more alternative times or time ranges that satisfy the alternatives of the parameters of the reservation request. In some implementations, one or more affinity features can be generated based on the current state of the demonstrative dialog 352B. For example, assume that the request time for a restaurant reservation as included in the demonstrative dialog 352B is 6PM, and the request time is available. In this case, one or more affinity features can be generated that indicate that the request time is available. In contrast, assume that the requested time for restaurant reservation is not available. In this case, one or more affinity features indicating the request time are not available and also associate the request time with an alternate time (e.g., one hour after the request time and two hours after the request time if the availability corresponds to 7PM and 8PM instead of request time 6 PM).
Similar to the RPC outbound requests described above, although RPC inbound requests are not explicitly included in the demonstrative dialog 352B, the third-party developer can add or inject RPC inbound requests with availability into training-instance input 321 A3. Furthermore, although RPC inbound requests are received from a third-party system (rather than a "client" in the demonstrative dialog 352B), the voice robot can still be trained to receive RPC inbound requests with availability during the demonstrative dialog 352B based on training-instance input 321A3 (which includes the availability of restaurant reservations as training instances of the RPC inbound requests). Further, while the RPC inbound request is described as being associated with restaurant reservation availability, it should be understood that this is for purposes of example and is not meant to be limiting. For example, the RPC inbound requests may be based on corresponding RPC outbound requests transmitted to one or more third-party systems. Thus, a voice robot trained on this RPC inbound request training instance can learn how to process RPC inbound requests and how to respond based on the context of the conversations and data included in the RPC inbound requests.
The RPC training examples described herein enable various technical advantages to be realized. As one non-limiting example, the voice robot can learn how and/or when to communicate requests to third-party systems, and how to utilize responses to those requests to address the task of a telephone conversation being conducted by the voice robot. As a result, the task can be solved by the voice robot during the conversation and can be efficiently solved without having to have an additional person converse with the conversation. Furthermore, the utilization of RPC training instances enables a reduction in the number of RPC requests because there are fewer erroneous RPC requests, thereby saving computational resources that would otherwise be consumed in generating RPC requests and/or network resources that would otherwise be consumed in transmitting RPC requests over one or more networks.
As yet another example, assume that training example input 321A4 generated based on the demonstrative dialog 352B includes a portion of the demonstrative dialog 352B corresponding to user input 354B3 and as such
In various embodiments, and after defining the training instance, the voice robot associated with the hydraulic Caf can be trained. For example, in response to receiving a user input directed to training the voice robot graphical element 382, the voice robot can be trained based on training instances defined by the user input (e.g., as described with reference to the voice robot training engine 160 of fig. 1). The voice robots associated with the Hypothetical Caf may correspond to multiple layers of ML models (e.g., RNN models, transform models, pointer networks, LSTM models, and/or other ML models). Notably, when training the speech robot, one or more of the plurality of layers of the ML model can focus on the corresponding feature emphasized input associated with one or more of the plurality of training instances by using the pointer network. For example, the voice robot can be enabled to focus at least on party size characteristics, time characteristics, name characteristics, RPC outbound request characteristics, RPC inbound request characteristics, and/or other characteristics defined by the corresponding characteristics of the training examples discussed in conjunction with fig. 3B and the additional training examples not discussed in conjunction with fig. 3B emphasizing the input. The pointer network can be trained to recognize these features that are predicted to be meaningful in determining how to respond based on the training instance input 312A.
After deploying the voice robot, the third party developer can monitor the progress of the voice robot. For example, in response to receiving user input directed to voice robot activity interface element 384, user interface 300 may present a voice robot activity interface as shown in fig. 3C. As another example, in response to receiving a user input directed to the main interface element 386, the user interface 300 may return to a home page or login page as shown in fig. 3A, and the user interface 300 may present a voice robot activity interface as shown in fig. 3C in response to a selection of the voice robot activity graphical element 330 (or the corresponding "see more" graphical element described above with reference to fig. 3A) from a third party developer. The third party developer may interact with the voice robot activity interface to view voice robot activity of the voice robot associated with the Hypothetical Caf. The voice robot activity can be stored in one or more databases (e.g., voice activity database 170A2 of fig. 1) accessible by the client device.
For example, the user can view a dialog summary 331A of a telephone call made by the trained voice bot on behalf of the Hypothetical Caf. In some implementations, the third party developer can view all of the voice robot activity of the voice robot, as shown in fig. 3C and as indicated by 330A. In some additional or alternative embodiments, the third party developer can switch between viewing all voice robot activities and only viewing voice robot activities that include the recognized behavioral errors of the voice robot as shown by 330B. The text associated with 330A and 330B may be selectable and enable third party developers to switch between these different views of voice robot activity. In various embodiments, third party developers can search voice robot activity logs. For example, a third party developer can enter one or more terms into the text entry field 330C to search for voice robot activity. Further, in various embodiments, the dialog summary 331A presented to the user may be sorted using one or more sorting criteria. The one or more ranking criteria can include, for example, recency of corresponding phone calls, recency since a third party developer reviewed a corresponding conversation digest, and/or any other ranking criteria.
In some implementations, the dialog summary provides a natural language interpretation of the corresponding telephone call made by the voice bot on behalf of the hypthetical Caf. For example, the dialog digest 331A1 indicates "User called to make a reservation, requested time wave not available, aborted an alternative time, the User accessed the subscribed alternative time for the reservation, the reservation wave completed (User calls to make a reservation, the requested time is not available, i propose an alternative time, user accepts the proposed alternative time to make a reservation, reservation is completed)". In this example, the conversation digest 331A1 indicates that the telephone call is similar to the demonstrative conversation 352B of fig. 3B. The conversation digest 331A1 can additionally or alternatively include an indication to perform RPC to check availability of the requested time (e.g., based on a feature emphasis output generated using a pointer network) and identify an alternative time and/or other information associated with the corresponding telephone call, such as a time and date the corresponding telephone call was made, a duration of the corresponding telephone call, monetary information associated with the corresponding telephone call (e.g., a cost of a take out order), subscription information associated with the corresponding telephone call, and/or any other information derived from the corresponding telephone call.
In some additional or alternative implementations, the transcription associated with the corresponding phone call made by the voice robot on behalf of the contextual Caf can be accessed from the voice call activity interface (and optionally only when the person talking to the voice robot agrees to monitor the voice robot activity for the corresponding phone call). For example, the conversation digest 331A2 indicates "User called to place cart order, I called to make all the food way available, the cart order way complete" (the User calls to place a take-out order, I checks to make sure all the food is available, the take-out order is complete) ". The dialog summary 331A2 may be generated based on the dialog 352C shown in fig. 3C. For example, assume that a voice robot answers an incoming telephone call directed to a Hypothetical Caf and initiated by a person via a corresponding client device. Further assume that the voice robot renders synthesized voice audio data that captures Hello, hypothetical Caf, how may I help you? (hello, hypoteticAl CAf, what I can do for you)? Let us further assume that the speech robot processes audio data capturing the spoken utterance 354c1 'hi, I who like to place an order for carryou (hi, I want to book take away) "(and the previous context of the conversation 352C) and generates a captured synthetic speech 352c2' okay, what can I get for you? (good, what do i like you what)') as a response 352C2 to the spoken utterance. The dialog 352C can continue as shown in fig. 3C. Notably, the dialog includes an RPC associated third party inventory system to check for availability of a particular item (e.g., peas on Earth Soup and the Here Today, gone Tomato Lasagna) included in the take-out order in spoken utterance 354C 2. The RPC associated third party inventory system can include RPC outbound requests as indicated by the synthesized speech 352C3 in the dialog 352 (e.g., "Let me make sure we have soup") and can include RPC inbound requests as indicated by the synthesized speech 352C4 in the dialog 352 (e.g., "Okay, we have soup)").
In some additional or alternative implementations, any voice robot behavior errors identified for a given phone call can be included in the corresponding conversation digest. For example, the dialog abstract 331A3 indicates "User call to ask while heat we have had a call waiting, the telephone call failed". If the voice robot is unable to generate a response to a spoken utterance provided by a person, or is unable to generate a response to a spoken utterance with sufficient confidence, the dialog summary 331A3 may indicate that the telephone call failed (e.g., as described with reference to the misrecognition engine 180 of FIG. 1). In embodiments where voice robot behavioral errors are identified for a given phone call, dialog summary 331A3 can include one or more actions that are automatically performed by the voice robot development platform to correct the identified behavioral errors of the voice robot. In additional or alternative implementations of recognizing speech robot behavioral errors for a given phone call, the dialog summary 331A3 can include one or more actions that, when executed by a third party developer, can correct the recognized behavioral errors of the speech robot. The one or more actions can include adding one or more additional training instances to retrain the voice robot, or modifying an existing training instance used to train the voice robot associated with the hydraulic Caf. For example, in response to receiving user input directed to training example interface element 388, user interface 300 may return to the training example interface as shown in FIG. 3B. As another example, in response to receiving user input directed to the main interface element 386, the user interface 300 may return to a home page or login page as shown in FIG. 3A, and the user interface 300 may present a training example interface as shown in FIG. 3B in response to selection of the training example graphical element 320 (or the corresponding "see more" graphical element described above with reference to FIG. 3A) from a third party developer. One or more additional training instances may be added to the training instance of the voice robot associated with the Hypothetical Caf and/or an existing training instance may be able to be modified. In response to receiving user input directed to the trained voice robot graphical element 382, the voice robot can be retrained based on the additional training instances and/or the modified training instances (e.g., as described with reference to the voice robot training engine 160 of fig. 1).
While a single example of a speech robot associated with only a Hypothertical Caf is described with reference to FIGS. 3A, 3B, and 3C, it should be understood that this is for purposes of illustration and is not meant to be limiting. For example, multiple instances of a voice robot may be simultaneously implemented by one or more computing devices (e.g., client devices, servers, and/or other computing devices) such that the multiple instances of the voice robot can simultaneously participate in a conversation with a corresponding person (or other voice robot). Each instance of the voice robot can include a corresponding processor that utilizes a corresponding instance of the plurality of ML layers of the voice robot. Furthermore, while the voice bot associated with the contextual Caf is described with reference to fig. 3A, 3B, and 3C as answering an incoming telephone call, it should be understood that this is for purposes of example and is not meant to be limiting. For example, speech can additionally or alternatively be trained to initiate outgoing telephone calls to various entities. For example, for a given training example, the training instance inputs can include actions or commands for initiating a corresponding telephone call with a particular entity to perform a particular task, and for a given training instance, the training instance outputs can include a corresponding real-valued response associated with initiating an outgoing telephone call. In this manner, outgoing telephone calls can be initiated and made with the voice bot associated with the Hypothetical Caf to order more inventory from the supplier, ask software questions from the information technology department, confirm restaurant reservations with the customer, and/or perform any other function for which the voice bot is trained.
Turning now to FIG. 4, a flow diagram is depicted illustrating an exemplary method 400 of training a speech robot based at least in part on a feature emphasized input. For convenience, the operations of method 400 are described with reference to a system performing the operations. The system of method 400 includes at least one processor, at least one memory, and/or other components of a computing device (e.g., client device 110 of fig. 1, voice robot development system 120 of fig. 1, and/or computing device 610 of fig. 6, a server, and/or other computing devices). Further, while the operations of method 400 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 452, the system obtains a plurality of training instances for training the voice robot, each training instance of the plurality of training instances including a training instance input and a training instance output. In some implementations, the training instance input can include at least a portion of the corresponding conversation and a previous context of the corresponding conversation, and the training instance output can include a corresponding true value response to the portion of the conversation. In some additional or alternative embodiments, the training instance input can include an indication of an incoming telephone call, and the training instance output can include a corresponding real-valued response associated with answering the incoming telephone call (and optionally an introduction associated with answering the incoming telephone call). In some additional or alternative embodiments, the training instance input can include an action or command to initiate execution of an outgoing telephone call with a particular entity to perform a particular task, and the training instance output can include a corresponding real-valued response associated with initiating the outgoing telephone call. In various implementations, a plurality of training examples can be obtained from a training example corpus generated based on previous phone calls associated with a third party for which the voice robot is being trained (or another third party (e.g., a restaurant entity, an airline entity, a retailer entity, etc.) that has the same type of entity as the third party), user input from a third party developer associated with the third party, and/or demonstrative conversations provided by one or more people (e.g., that may or may not include the third party developer). Obtaining a plurality of training examples for training a voice robot is described above with reference to fig. 3A, 3B, and 3C.
At block 454, the system obtains corresponding feature emphasis inputs associated with one or more of the plurality of training instances. A corresponding feature emphasis input can be defined by a third party developer for one or more of the plurality of training instances. Further, the feature emphasis input causes the speech robot to focus on particular features of the portion of the corresponding dialog for the given training instance. For example, assume that the third party for which the voice robot is being trained is an imaginary airline entity named Hypothetical Airlines, and that for a given training instance, the training instance input includes a portion of the corresponding dialog of "I woold like to change my flight from SDF to DCA from today at noon to tomorrow at noon" (I would like to change my flight from SDF to DCA from today noon to tomorrow at noon). In this example, the feature emphasis input can include a flight change feature (e.g., "change my flight"), a departure location feature (e.g., "SDF"), a destination location feature (e.g., "DCA"), an original time and date feature (e.g., "today at noon"), and a desired time and date feature (e.g., "tomorrow at noon"). These feature emphasis inputs associated with the training instance inputs can be stored in association with a given training instance in one or more databases (e.g., training instance database 161A of fig. 1) and optionally used to train a pointer network as described herein (e.g., with reference to fig. 1 and 2A).
At block 456, the system processes training instance input (and optionally corresponding feature emphasized input provided by a third party developer) using multiple Machine Learning (ML) layers of the ML model and for a given training instance to generate an embedding associated with a current state of a corresponding dialog associated with the given training instance. In some implementations, the portion of the corresponding dialog included in the training instance input corresponds to a plurality of speech hypotheses for at least the portion of the corresponding dialog. In some versions of those implementations, the plurality of speech hypotheses can be processed using a first ML layer of the plurality of ML layers to generate the first embedding and a previous dialog context can be processed using a second ML layer of the plurality of ML layers to generate the second embedding. The first and second embeddings can be concatenated to generate an embeddings associated with a current state of the corresponding dialog. In some additional or alternative implementations, the portion of the corresponding dialog included in the training instance input corresponds to audio data that captured the portion of the corresponding dialog. In some versions of those embodiments, the audio data can be processed using an Automatic Speech Recognition (ASR) model to generate a plurality of speech hypotheses for at least the portion of the corresponding dialog. In some versions of those implementations, a first ML layer of the plurality of ML layers can be used to process the plurality of speech hypotheses (and optionally the audio data) to generate a first embedding, and a second ML layer of the plurality of ML layers can be used to process a previous dialog context to generate a second embedding. The first and second embeddings can be concatenated to generate an embeddings associated with a current state of the corresponding dialog.
At block 458, the system generates one or more affinity features based on the current state of the corresponding dialog. The one or more affinity features can represent relationships between features included in a previous context of the corresponding conversation and/or features associated with a current state of the conversation. For example, assume that the third party for which the voice robot is being trained is an imaginary airline entity named Hypothetical Airlines, and that for a given training instance, the training instance input includes a portion of the corresponding dialog of "Iwoold like to change my flight from SDF to DCA from today at noon to tomorrow at noon" (i want to change my flight from SDF to DCA from today noon to tomorrow at noon). In this example, the affinity features can include a time affinity feature associated with a 24-hour difference between an original time and date feature (e.g., "today at noon") and a desired time and date feature (e.g., "tomorrow at noon"), an account affinity feature associated with a flight change request of the user associated with a corresponding frequent traveler number (assuming that the corresponding frequent traveler number was previously provided and included in the previous context of the corresponding conversation), and/or other affinity features.
At block 460, the system processes one or more of the embedding and affinity features associated with the current state of the corresponding dialog using the multiple additional ML layers of the ML model or the additional ML model to generate a predicted embedding associated with the predicted response to at least the portion of the corresponding dialog. In some implementations, such as when the third party developer provides one or more corresponding feature emphasized inputs, the prediction embedding associated with the predicted response to at least the portion of the corresponding dialog can be biased based on one or more of the corresponding feature emphasized inputs provided by the third party developer.
At block 462, the system compares the predicted embedding associated with the predicted response and the corresponding real-valued embedding associated with the corresponding real-valued response in an embedding space to generate one or more losses. Predictive embedding and real-value embedding can correspond to lower-dimensional representations of a predictive response and a corresponding real-value response, respectively. The embedding space allows comparison of these lower-dimensional embeddings. Furthermore, the predictive embeddings associated with the predicted responses should be close to the corresponding real-valued embeddings associated with the corresponding real-valued responses in the embedding space. In other words, in processing at least a portion of a corresponding conversation and a previous context of the conversation, the system should predict a response similar to an actual response to at least the portion of the corresponding conversation. For example, a distance metric (e.g., cosine similarity distance, euclidean distance, and/or other distance metric) between a predicted embedding and a corresponding true value embedding in the embedding space can be determined, and one or more penalties can be generated based on the distance metric.
At block 464, the system updates one or more of the plurality of ML layers or the plurality of additional ML layers based on the one or more losses for the given training instance. For example, the system can back-propagate one or more losses across one or more of the plurality of ML layers or the plurality of additional ML layers. Further, by updating the plurality of ML layers or one or more of the plurality of additional ML layers, the plurality of ML layers or one or more of the plurality of additional ML layers can be made aware that the third party developer indicates features included in the given training instance that are important to the given training instance, such as features indicated by one or more of the corresponding feature emphasis inputs. As a result, one or more of the multiple ML layers or the multiple additional ML layers are trained to recognize these particular features, and what these particular meanings are in the context of the entire conversation.
At block 466, the system determines whether there are additional training instances for training the voice robot. If at an iteration of block 466, the system determines that there are additional training instances for the voice robot, the system may return to block 456 and repeat the process of blocks 456 through 464 based on the additional training instances. In some implementations, when additional training instances are available, the system can continue to update the plurality of ML layers or one or more of the plurality of additional ML layers until one or more conditions are satisfied. The one or more conditions can include, for example, validation of one or more of the updated plurality of ML layers or the plurality of additional ML layers, convergence of one or more of the updated plurality of ML layers or the plurality of additional ML layers (e.g., with zero loss or within a threshold range of zero loss), a determination that one or more of the plurality of ML layers or the plurality of additional ML layers performs better (e.g., with respect to precision and/or recall) than an instance of the speech robot currently being utilized (if any), an occurrence of training based on at least a threshold number of training instances, and/or a duration of training based on the training instances. In some additional or alternative implementations, the system may continue to update the plurality of ML layers or one or more of the plurality of additional ML layers until the voice robot has been trained on each of a plurality of training instances obtained for training the voice robot. If at an iteration of block 466, the system determines that there are no additional training instances for the voice robot or until one or more conditions are met, the system may proceed to block 468.
At block 468, the system causes the trained voice bot to be deployed for conducting a conversation on behalf of a third party. For example, a trained voice robot can be deployed to converse on behalf of a third party for a phone call associated with the third party. In some implementations, the trained voice robot can answer an incoming telephone call on behalf of a third party and engage in a corresponding conversation with a person (or additional voice robots associated with the person). In additional or alternative embodiments, the trained voice robot is capable of initiating execution of an outgoing telephone call on behalf of a third party and engaging in a corresponding conversation with a person or entity (or an additional voice robot associated with the person or an Interactive Voice Response (IVR) system associated with the person). As another example, a trained voice robot can be deployed to converse at a car restaurant or in any other service scenario on behalf of a third party. Notably, multiple instances of the trained voice robot can be deployed simultaneously by a third party. By deploying multiple instances of the trained voice robot, any combination of multiple incoming and multiple outgoing telephone calls can be processed simultaneously.
Turning now to FIG. 5, a flow diagram is depicted illustrating an exemplary method 500 of training a voice robot based at least in part on Remote Procedure Calls (RPCs). For convenience, the operations of method 500 are described with reference to a system performing the operations. The system of method 500 includes at least one processor, at least one memory, and/or other components of a computing device (e.g., client device 110 of fig. 1, voice robot development system 120 of fig. 1, and/or computing device 610 of fig. 6, a server, and/or other computing devices). Further, while the operations of method 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 552, the system obtains a plurality of RPC training instances for training the voice robot, each training instance of the plurality of training instances including a training instance input and a training instance output. A plurality of RPC training instances can be selected from a superset of training instances associated with the voice robot (e.g., training instance database 161A of fig. 1). At block 554, the system determines a type of RPC request for the training speech robot for a given RPC training instance of the plurality of RPC training instances.
In some embodiments, the type of RPC request for a given RPC training instance may be an RPC outbound request. The system may determine, based on the training instance output, that the type of RPC request is an RPC outbound request that includes at least a corresponding real-valued RPC outbound request for a given RPC training instance, as indicated by the sub-box 552 A1. If, at the iteration of block 554, the system determines that the type of RPC request for the given training instance is an RPC outbound request, the system can proceed to block 556A.
At block 556A, the system processes the training instance input using multiple Machine Learning (ML) layers of ML models and for a given RPC training instance to generate an embedding associated with the current state of the corresponding dialog. The training instance input can include at least a portion of a corresponding dialog. The training example input can be processed in the same or similar manner as described above with reference to block 456 of FIG. 4.
At block 558A, the system processes the embedding and the one or more affinity features using multiple additional ML layers of the ML model or additional ML models to generate a predictive embedding associated with a predictive response to at least the portion of the corresponding dialog included in the training instance input. The system can generate one or more affinity features in the same or similar manner as described above with reference to block 458 of fig. 4. Further, the system can utilize multiple additional layers of ML to process one or more of the affinity features associated with the current state of the corresponding dialog in the same or similar manner as described above with reference to block 460 of FIG. 4.
At block 560A, the system compares the predicted embedding to the real-valued embedding associated with the corresponding real-valued RPC outbound request in an embedding space to generate one or more penalties. The system can compare the predicted embedding and the actual value embedding to generate one or more penalties in the same or similar manner as described above with reference to block 462 of fig. 4. Notably, the real-value embedding is associated with RPC outbound requests (as opposed to the real-value response to at least a portion of the corresponding dialog described above with reference to fig. 4). For a given RPC training instance, an RPC outbound request may be associated with a particular third-party system (e.g., a third-party booking system, a third-party inventory system, and/or other particular third-party system). Thus, the RPC outbound requests associated with each of these particular third-party systems can be associated with respective portions of the embedding space. For example, assume that the corresponding RPC outbound request is associated with a third party reservation system. In this example, the real-value embedding can be associated with a third party subscription system, as opposed to, for example, embedding associated with a third party inventory system. The system proceeds to block 562. Block 562 is described below.
In other embodiments, the type of RPC request for a given RPC training instance may be an RPC inbound request. The system can determine, based on the training instance input, that the type of RPC request is an RPC inbound request that, for a given RPC training instance, includes at least a corresponding RPC inbound request, as shown in sub-box 552A 2. If, at the iteration of block 554, the system determines that the type of RPC request for the given training instance is an RPC inbound request, the system can proceed to block 556B.
At block 556B, the system processes at least the corresponding RPC inbound requests using the multiple ML layers of the ML model to generate the embedding associated with the current state of the corresponding dialog. The training instance input may additionally include at least a portion of a corresponding dialog. The training example input can be processed in the same or similar manner as described above with reference to block 456 of FIG. 4. Notably, the training example input includes an RPC inbound request (as opposed to only at least a portion of the corresponding dialog described above with reference to fig. 4). For a given RPC training instance, an RPC inbound request may be associated with a particular third-party system (e.g., a third-party booking system, a third-party inventory system, and/or other particular third-party system). As a result, the RPC inbound request can be structured information to be communicated in the conversation, and a corresponding RPC outbound request is received, as described above with reference to block 560A.
At block 558B, the system processes the embedding and the one or more affinity features using multiple additional ML layers of the ML model or additional ML models to generate a predicted embedding associated with a predicted response to at least the RPC inbound request. The system can generate one or more affinity features in the same or similar manner as described above with reference to block 458 of fig. 4. Further, the system can utilize multiple additional ML layers to process one or more of the embedded and affinity features associated with the current state of the corresponding dialog in the same or similar manner as described above with reference to block 460 of FIG. 4.
At block 560B, the system compares the predictive embedding to the true value embedding associated with the training instance output in the embedding space to generate one or more penalties. The training instance output can include a corresponding real-valued response to the RPC inbound request. The system can compare the predicted embedding and the actual value embedding to generate one or more penalties in the same or similar manner as described above with reference to block 462 of fig. 4. The system proceeds to block 562.
At block 562, the system updates one or more of the plurality of ML layers or the plurality of additional ML layers based on the one or more losses generated at block 560A or 560 b. The system can update one or more of the plurality of ML layers or the plurality of additional ML layers in the same or similar manner as described above with reference to block 464 of fig. 4.
At block 564, the system determines whether there are additional RPC training instances to train the voice robot. If at the iteration of block 564, the system determines that there are additional RPC training instances for the voice robot, the system may return to block 554 and repeat the process of blocks 554 through 562 based on the additional RPC training instances. In some implementations, when additional RPC training instances are available, the system can continue to update the multiple ML layers or one or more of the multiple additional ML layers until one or more conditions are satisfied. The one or more conditions can include, for example, a validation of one or more of the plurality of ML layers or the plurality of additional ML layers that are updated, a convergence of one or more of the plurality of ML layers or the plurality of additional ML layers that are updated (e.g., with zero loss or within a threshold range of zero loss), a determination that one or more of the plurality of ML layers or the plurality of additional ML layers (e.g., with respect to accuracy and/or recall) performs better than an instance of the speech robot currently being utilized (if any), an occurrence of training based on at least a threshold number of training instances, and/or a duration of training based on the training instances. In some additional or alternative embodiments, the system may continue to update the plurality of ML layers or one or more of the plurality of additional ML layers until the voice robot has been trained on each of the plurality of RPC training instances obtained for training the voice robot. If at the iteration of block 564, the system determines that there are no additional training instances for the voice robot or until one or more conditions are met, the system may proceed to block 465568.
At block 566, the system causes the trained voice robot to be deployed for conducting conversations on behalf of third parties. For example, a trained voice robot can be deployed to converse on behalf of a third party for a phone call associated with the third party. In some implementations, the trained voice robot can answer an incoming telephone call on behalf of a third party and engage in a corresponding conversation with a person (or additional voice robots associated with the person). In additional or alternative embodiments, the trained voice robot is capable of initiating execution of an outgoing telephone call on behalf of a third party and engaging in a corresponding conversation with a person or entity (or an additional voice robot associated with the person or an Interactive Voice Response (IVR) system associated with the person). As another example, a trained voice robot can be deployed to converse at a car restaurant or in any other service scenario on behalf of a third party. Notably, multiple instances of the trained voice robot may be deployed simultaneously by a third party. By deploying multiple instances of a trained voice robot, any combination of multiple incoming and multiple outgoing telephone calls can be processed simultaneously.
Although FIG. 5 is described herein with reference to only a plurality of RPC training examples, it should be understood that this is for illustrative purposes and is not meant to be limiting. For example, the plurality of RPC training instances can be mixed with a plurality of additional non-RPC training instances for training a voice robot of a third party, such as those described with reference to fig. 4. In addition, one or more of the plurality of RPC training instances may also be associated with a corresponding feature emphasized input (e.g., as described with reference to fig. 1, 2A, 3B, and 4).
FIG. 6 is a block diagram of an exemplary computing device 610 that may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client device, cloud-based automated assistant component, and/or other components can include one or more components of the example computing device 610.
The user interface input devices 622 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or tablet, a scanner, a touch screen incorporated into the display, an audio input device such as a voice recognition system, microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 610 or onto a communication network.
User interface output device 620 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual displays, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 610 to a user or to another machine or computing device.
These software modules are typically executed by processor 614 alone or in combination with other processors. Memory 625 used in storage subsystem 624 can include a number of memories including a main Random Access Memory (RAM) 630 for storing instructions and data during program execution and a Read Only Memory (ROM) 632 in which fixed instructions are stored. File storage subsystem 626 is capable of providing permanent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical disk drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 626 in storage subsystem 624, or in other machines accessible to processor 614.
Where the systems described herein collect or otherwise monitor personal information about a user or personal and/or monitored information may be utilized, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social behavior or activity, profession, the user's preferences, or the user's current geographic location) or whether and/or how to receive content from a content server that may be more relevant to the user. Also, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information of the user cannot be determined, or the geographic location of the user may be generalized (such as to a city, zip code, or state level) where geographic location information is obtained such that a particular geographic location of the user cannot be determined. Thus, the user may control how information about the user is collected and/or used.
In some implementations, a method implemented by one or more processors is provided and includes obtaining a plurality of training instances via a speech robot development platform. Each training instance of the plurality of training instances includes a training instance input and a training instance output. The training instance input includes at least a portion of a corresponding conversation and a previous context of the corresponding conversation, and the training instance output includes a corresponding true value response to at least the portion of the corresponding conversation. The method further comprises the following steps: obtaining, via the voice robot development platform, corresponding feature-emphasized inputs associated with one or more of the plurality of training instances, and training, via the voice robot development platform, a voice robot based on the plurality of training instances and the corresponding feature-emphasized inputs associated with one or more of the plurality of training instances. The corresponding feature emphasis input associated with one or more of the plurality of training instances causes the voice robot to focus on a particular feature of the portion of the corresponding conversation. The method further comprises the following steps: after training the voice robot, causing the trained voice robot to be deployed for conducting a conversation on behalf of a third party.
These and other embodiments of the technology disclosed herein can optionally include one or more of the following features.
In some embodiments, training the voice robot may include: using a plurality of Machine Learning (ML) layers of an ML model, and for a given training instance of the plurality of training instances, processing at least the portion of the corresponding dialog and the previous context of the corresponding dialog to generate an embedding associated with a current state of the corresponding dialog.
In some versions of those embodiments, the portion of the corresponding dialog may include a plurality of speech hypotheses for at least the portion of the corresponding dialog. Processing at least the portion of the corresponding dialog and the previous context of the corresponding dialog to generate the embedding associated with the current state of the corresponding dialog may include: processing the plurality of speech hypotheses using a first ML layer of the plurality of ML layers to generate a first embedding, processing the previous context of the corresponding dialog using a second ML layer of the plurality of ML layers to generate a second embedding, and concatenating the first embedding and the second embedding to generate the embedding associated with the current state of the corresponding dialog.
In some further versions of those embodiments, the method may further include: generating, via the voice robot development platform, a plurality of affinity features based on the embedding associated with the current state of the corresponding conversation. In yet further versions of those embodiments, training the speech robot may further include processing the plurality of affinity features and the embedding associated with the current state of the corresponding dialog using a plurality of additional ML layers of the ML model or an additional ML model to generate a predictive embedding associated with a predictive response to at least the portion of the corresponding dialog. In still further versions of those embodiments, training the speech robot may further comprise: comparing, in an embedding space, the predictive embedding associated with the predictive response to at least the portion of the corresponding dialog with a corresponding real-value embedding associated with the corresponding real-value response to at least the portion of the corresponding dialog, generating one or more losses based on comparing the predictive embedding and the corresponding real-value embedding, and updating the ML model based on one or more of the losses associated with the given training instance and the corresponding feature emphasis input. In still further versions of those embodiments, the ML model may be a transform model including one or more attention mechanisms, and updating the transform model based on one or more of the loss and the corresponding feature-emphasized input associated with the given training instance may include: cause updating weights of one or more of the plurality of ML layers or the plurality of additional ML layers based on one or more of the losses, and cause the one or more of the attention mechanisms of the fransformer model to focus on one or more features of at least the portion of the corresponding dialog based on the corresponding feature emphasis input associated with the given training instance.
In some additional or alternative further versions of those embodiments, the portion of the corresponding dialog may include audio data corresponding to a spoken utterance that captured at least the portion of the corresponding dialog. The plurality of speech hypotheses may be generated based on processing the audio data corresponding to the spoken utterance using an Automatic Speech Recognition (ASR) model to generate the plurality of speech hypotheses for at least the portion of the corresponding dialog. In still further versions of those embodiments, the method may further comprise: the method further includes aligning one or more corresponding text segments associated with each of the plurality of speech hypotheses and annotating each of the one or more corresponding text segments with at least one corresponding tag to generate a plurality of annotated speech hypotheses. Processing the plurality of speech hypotheses using the first of the plurality of ML layers to generate the first embedding may include processing the plurality of annotated speech hypotheses to generate the first embedding.
In some additional or alternative further versions of those implementations, the previous context of the corresponding conversation may include at least one or more previous portions of the corresponding conversation. The one or more previous portions of the corresponding dialog occur in the corresponding dialog before at least the portion of the corresponding dialog.
In some implementations, obtaining the corresponding feature emphasis inputs associated with one or more of the plurality of training instances may include: receiving natural language input from one or more persons associated with the third party, and processing the natural language input to obtain the corresponding feature emphasized input associated with one or more of the plurality of training instances. The natural language input is one or more of: a free form spoken input or a free form typed input.
In some implementations, one or more of the plurality of training instances can be obtained from a corpus of training instances. The corpus of training examples may include a plurality of previous conversations between a plurality of people. In some additional or alternative embodiments, one or more of the plurality of training instances may be obtained from a corresponding demonstrative conversation between one or more persons. The one or more of the people may be associated with the third party. In some additional or alternative implementations, one or more of the plurality of training instances may be obtained from a spoken utterance received via the speech robot development. The spoken utterance may be received from one or more persons associated with the third party.
In some implementations, causing the trained voice robot to be deployed for conducting the conversation on behalf of the third party may include: the dialog causing the trained voice robot to be deployed for conducting a phone call associated with the third party, and the dialog causing the trained voice robot to be deployed for conducting the phone call associated with the third party may include causing the voice robot to answer a corresponding incoming phone call and conduct the dialog with a corresponding person initiating the corresponding incoming phone call via a respective client device. In some versions of those embodiments, the method may further comprise: after ending the incoming phone call, generating a corresponding conversation digest of the conversation conducted during the corresponding incoming phone call, and causing the corresponding conversation digest of the conversation to be rendered via the voice robot development platform.
In some implementations, causing the trained voice robot to be deployed for conducting the conversation on behalf of the third party may include causing the trained voice robot to be deployed for conducting the conversation for a phone call associated with the third party, and causing the trained voice robot to be deployed for conducting the conversation for the phone call associated with the third party may include causing the voice robot to initiate and conduct the conversation with a corresponding person answering the corresponding outgoing phone call via a respective client device. In some versions of those embodiments, the method may further comprise: after ending the outgoing telephone call, generating a corresponding conversation digest of the conversation conducted during the corresponding outgoing telephone call, and causing the corresponding conversation digest of the conversation to be rendered via the voice robot development platform.
In some embodiments, the voice robot development platform is provided by a first party different from the third party deploying the voice robot.
In some implementations, a method implemented by one or more processors is provided and includes obtaining, via a voice robot development platform, a plurality of Remote Procedure Call (RPC) training instances. Each of the plurality of RPC training instances includes a training instance input and a training instance output. The training instance input includes at least a portion of a corresponding conversation and a previous context of the corresponding conversation, and the training instance output includes a corresponding true value response to at least the portion of the corresponding conversation. The method further comprises: training, via the voice robot development platform, a voice robot based at least on the plurality of RPC training instances. Training the voice robot based on the plurality of RPC training instances such that the voice robot interacts with a third party system. The method further comprises: after training the voice robot, causing the trained voice robot to be deployed for conducting a conversation on behalf of a third party.
These and other embodiments of the technology disclosed herein can optionally include one or more of the following features.
In some implementations, the corresponding real-valued response for a given RPC training instance of the plurality of RPC training instances can include at least a corresponding RPC outbound request. Training the voice robot may include: using a plurality of Machine Learning (ML) layers of an ML model, and for the given training instance, processing at least the portion of the corresponding dialog and the previous context of the corresponding dialog to generate an embedding associated with a current state of the corresponding dialog.
In some versions of those embodiments, the portion of the corresponding dialog may include a plurality of speech hypotheses for at least the portion of the corresponding dialog. Processing at least the portion of the corresponding dialog and the previous context of the corresponding dialog to generate the embedding associated with the current state of the corresponding dialog may include: processing the plurality of speech hypotheses using a first ML layer of the plurality of ML layers to generate a first embedding, processing the previous context of the corresponding dialog using a second ML layer of the plurality of ML layers to generate a second embedding, and concatenating the first embedding and the second embedding to generate the embedding associated with the current state of the corresponding dialog.
In some further versions of those embodiments, the method may further comprise: generating, via the voice robot development platform, a plurality of affinity features based on the embedding associated with the current state of the corresponding dialog.
In some further versions of those embodiments, training the speech robot may further include processing the plurality of affinity features and the embedding associated with the current state of the corresponding dialog using the ML model or a plurality of additional ML layers of additional ML models to generate a predicted embedding associated with a predicted response to at least the portion of the corresponding dialog.
In still further versions of those embodiments, training the speech robot may further comprise: comparing, in an embedding space, the predictive embedding associated with the predictive response to at least the portion of the corresponding conversation with a corresponding real-value embedding associated with the corresponding RPC outbound request, generating one or more losses based on comparing the predictive embedding with the corresponding real-value embedding, and updating the ML model based on one or more of the losses.
In some implementations, at least the portion of the corresponding dialog for a given RPC training instance of the plurality of RPC training instances can include at least a corresponding RPC inbound request. Training the voice robot may include: using a plurality of Machine Learning (ML) layers of an ML model, and for the given training instance, processing at least the corresponding RPC inbound request and the previous context of the corresponding dialog to generate an embedding associated with a current state of the corresponding dialog.
In some versions of those implementations, processing at least the portion of the corresponding dialog and the previous context of the corresponding dialog to generate the embedding associated with the current state of the corresponding dialog may include: processing at least the RPC inbound request using a first ML layer of the plurality of ML layers to generate a first embedding, processing the previous context of the corresponding dialog using a second ML layer of the plurality of ML layers to generate a second embedding, and concatenating the first embedding and the second embedding to generate a predicted embedding associated with a predicted response to at least the portion of the corresponding dialog.
In some further versions of those embodiments, training the speech robot may further comprise: comparing, in an embedding space, the predictive embedding associated with the predictive response to at least the portion of the corresponding dialog with a corresponding real-value embedding associated with the corresponding real-value response, generating one or more losses based on comparing the predictive embedding with the corresponding real-value embedding, and updating the ML model based on one or more of the losses.
In some embodiments, the third party deploying the voice robot is different from an additional third party associated with the third party system.
In some implementations, causing the trained voice robot to be deployed for conducting the conversation on behalf of the third party may include: causing the trained voice robot to be deployed for conducting the conversation of the phone call associated with the third party, and causing the trained voice robot to be deployed for conducting the conversation of the phone call associated with the third party may include causing the voice robot to answer a corresponding incoming phone call and conduct the conversation, via a respective client device, with a corresponding person originating the corresponding incoming phone call. The voice robot may interact with the third-party system via a corresponding RPC during the conversation with the correspondent. In some versions of those embodiments, the method may further comprise: after ending the incoming phone call, generating a corresponding conversation digest of the conversation conducted during the corresponding incoming phone call, and causing the corresponding conversation digest of the conversation to be rendered via the voice robot development platform. One or more of the corresponding conversation digests can include an indication that the corresponding RPC occurred based on a corresponding spoken utterance of the corresponding person received during the corresponding incoming telephone call.
In some embodiments, a voice robot development platform is provided and includes: at least one processor; at least one memory; at least one database, the at least one database comprising a plurality of training instances; and at least one user interface for enabling third party developers associated with a third party to interact with the voice robot development platform to: obtaining a plurality of training instances; training a voice robot based on the plurality of training instances to generate a plurality of corresponding behaviors of the voice robot; and after training the voice robot, enabling the third party developer to add additional training instances stored in the at least one database to add new behaviors for the plurality of corresponding behaviors of the voice robot; and enabling the third party developer to modify existing training instances stored in the at least one database to modify existing behaviors of the plurality of corresponding behaviors of the voice robot. Each training instance of the plurality of training instances includes a training instance input and a training instance output. The training instance input includes at least a portion of a corresponding conversation and a previous context of the corresponding conversation, and the training instance output includes a corresponding true value response to at least the portion of the corresponding conversation.
Additionally, some embodiments include one or more processors (e.g., central Processing Units (CPUs), graphics Processing Units (GPUs), and/or Tensor Processing Units (TPUs)) of one or more computing devices, wherein the one or more processors are operable to execute instructions stored in an associated memory, and wherein the instructions are configured to cause performance of any of the above-described methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the above-described methods. Some embodiments also include a computer program product comprising instructions executable by one or more processors to perform any of the foregoing methods.
It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are considered a part of the subject matter disclosed herein. For example, all combinations of the claimed subject matter appearing at the beginning of this disclosure are considered part of the subject matter disclosed herein.
Claims (32)
1. A method implemented by one or more processors, the method comprising:
obtaining, via a speech robot development platform, a plurality of training instances, each of the plurality of training instances comprising:
training instance input comprising at least a portion of a corresponding conversation and a previous context of the corresponding conversation, an
A training instance output comprising a corresponding real-valued response to at least the portion of the corresponding dialog;
obtaining, via the speech robot development platform, corresponding feature-emphasized inputs associated with one or more of the plurality of training instances;
training, via the voice robot development platform, a voice robot based on the plurality of training instances and the corresponding feature-emphasis inputs associated with one or more of the plurality of training instances, wherein the corresponding feature-emphasis inputs associated with one or more of the plurality of training instances cause the voice robot to focus on a particular feature of the portion of the corresponding conversation; and
after training the voice robot:
such that the trained voice robot is deployed for conducting conversations on behalf of third parties.
2. The method of claim 1, wherein training the voice robot comprises:
processing, using a plurality of ML layers of a machine-learned ML model and for a given training instance of the plurality of training instances, at least the portion of the corresponding dialog and the previous context of the corresponding dialog to generate an embedding associated with a current state of the corresponding dialog.
3. The method of claim 2, wherein the portion of the corresponding dialog comprises a plurality of speech hypotheses for at least the portion of the corresponding dialog, and wherein processing at least the portion of the corresponding dialog and the previous context of the corresponding dialog to generate the embedding associated with the current state of the corresponding dialog comprises:
processing the plurality of speech hypotheses using a first ML layer of the plurality of ML layers to generate a first embedding,
processing the previous context of the corresponding dialog using a second ML layer of the plurality of ML layers to generate a second embedding, an
Concatenating the first and second embeddings to generate the embedding associated with the current state of the corresponding dialog.
4. The method of claim 3, further comprising:
generating, via the voice robot development platform, a plurality of affinity features based on the embedding associated with the current state of the corresponding dialog.
5. The method of claim 4, wherein training the voice robot further comprises:
processing the plurality of affinity features and the embedding associated with the current state of the corresponding dialog using the ML model or a plurality of additional ML layers of additional ML models to generate a predictive embedding associated with a predictive response to at least the portion of the corresponding dialog.
6. The method of claim 5, wherein training the voice robot further comprises:
comparing, in an embedding space, the predictive embedding associated with the predictive response to at least the portion of the corresponding dialog with a corresponding real-value embedding associated with the corresponding real-value response to at least the portion of the corresponding dialog;
generating one or more penalties based on comparing the predicted embedding and the corresponding actual value embedding; and
updating the ML model based on one or more of the loss and the corresponding feature emphasis inputs associated with the given training instance.
7. The method of claim 6, wherein the ML model is a fransformer model comprising one or more attention mechanisms, and wherein updating the fransformer model based on one or more of the loss and the corresponding feature emphasis inputs associated with the given training instance comprises:
cause updating weights of one or more of the plurality of ML layers or the plurality of additional ML layers based on one or more of the losses; and
causing the one or more of the attention mechanisms of the transform model to focus on one or more features of at least the portion of the corresponding conversation based on the corresponding feature emphasis input associated with the given training instance.
8. The method of claim 3, wherein the portion of the corresponding dialog further includes audio data corresponding to a spoken utterance that captured at least the portion of the corresponding dialog, and wherein the plurality of speech hypotheses are generated based on processing the audio data corresponding to the spoken utterance using an Automatic Speech Recognition (ASR) model to generate the plurality of speech hypotheses for at least the portion of the corresponding dialog.
9. The method of claim 8, further comprising:
aligning one or more corresponding text segments associated with each of the plurality of speech hypotheses;
annotating each of the one or more corresponding text segments with at least one corresponding tag to generate a plurality of annotated speech hypotheses; and
wherein processing the plurality of speech hypotheses using the first of the plurality of ML layers to generate the first embedding comprises:
processing the plurality of annotated speech hypotheses to generate the first embedding.
10. The method of claim 3, wherein the previous context of the corresponding conversation includes at least one or more previous portions of the corresponding conversation, and wherein the one or more previous portions of the corresponding conversation occur in the corresponding conversation before at least the portion of the corresponding conversation.
11. The method of any preceding claim, wherein obtaining the corresponding feature emphasis input associated with one or more of the plurality of training instances comprises:
receiving natural language input from one or more persons associated with the third party, wherein the natural language input is one or more of: a free-form spoken input or a free-form typed input; and
processing the natural language input to obtain the corresponding feature emphasized input associated with one or more of the plurality of training instances.
12. The method of any preceding claim, wherein one or more of the plurality of training instances are obtained from a corpus of training instances, and wherein the corpus of training instances comprises a plurality of previous conversations between a plurality of people.
13. The method of any preceding claim, wherein one or more of the plurality of training instances are obtained from corresponding demonstrative conversations between one or more people, and wherein the one or more of the people are associated with the third party.
14. The method of any preceding claim, wherein one or more of the plurality of training instances are obtained from a spoken utterance received via the voice robot development, and wherein the spoken utterance is received from one or more persons associated with the third party.
15. The method of any preceding claim, wherein causing the trained voice robot to be deployed for conducting the conversation on behalf of the third party comprises: cause the trained voice robot to be deployed for the conversation of conducting a phone call associated with the third party, and wherein causing the trained voice robot to be deployed for the conversation of conducting the phone call associated with the third party comprises:
causing the voice robot to answer a corresponding incoming telephone call and conduct the conversation with a counterpart initiating the corresponding incoming telephone call via a respective client device.
16. The method of claim 15, further comprising:
after ending the incoming telephone call:
generating a corresponding conversation digest of the conversation conducted during the corresponding incoming telephone call; and
causing the corresponding dialog summary of the dialog to be rendered via the voice robot development platform.
17. The method of any preceding claim, wherein causing the trained voice robot to be deployed for conducting the conversation on behalf of the third party comprises: cause the trained voice robot to be deployed for the conversation of conducting a telephone call associated with the third party, and wherein causing the trained voice robot to be deployed for the conversation of conducting the telephone call associated with the third party comprises:
causing the voice robot to initiate a corresponding outgoing telephone call and conduct the conversation with a correspondent person answering the corresponding outgoing telephone call via a respective client device.
18. The method of claim 17, further comprising:
after ending the outgoing telephone call:
generating a corresponding conversation digest of the conversation conducted during the corresponding outgoing telephone call; and
causing the corresponding dialog summary of the dialog to be rendered via the voice robot development platform.
19. The method of any preceding claim, wherein the voice robot development platform is provided by a first party different from the third party deploying the voice robot.
20. A method implemented by one or more processors, the method comprising:
obtaining, via a voice robot development platform, a plurality of Remote Procedure Call (RPC) training instances, each of the plurality of RPC training instances comprising:
training instance input comprising at least a portion of a corresponding conversation and a previous context of the corresponding conversation, an
A training instance output comprising a corresponding real-valued response to at least the portion of the corresponding dialog;
training, via the voice robot development platform, a voice robot based at least on the plurality of RPC training instances, wherein training the voice robot based on the plurality of RPC training instances causes the voice robot to interact with a third-party system; and
after training the voice robot:
such that the trained voice robot is deployed for conducting conversations on behalf of third parties.
21. The method of claim 20, wherein the corresponding real-valued response for a given RPC training instance of the plurality of RPC training instances includes at least a corresponding RPC outbound request, and wherein training the voice robot includes:
processing, using a plurality of ML layers of a machine-learned ML model and for the given training instance, at least the portion of the corresponding dialog and the previous context of the corresponding dialog to generate an embedding associated with a current state of the corresponding dialog.
22. The method of claim 21, wherein the portion of the corresponding dialog comprises a plurality of speech hypotheses for at least the portion of the corresponding dialog, and wherein processing at least the portion of the corresponding dialog and the previous context of the corresponding dialog to generate the embedding associated with the current state of the corresponding dialog comprises:
processing the plurality of speech hypotheses using a first ML layer of the plurality of ML layers to generate a first embedding,
processing the previous context of the corresponding dialog using a second ML layer of the plurality of ML layers to generate a second embedding, an
Concatenating the first embedding and the second embedding to generate the embedding associated with the current state of the corresponding dialog.
23. The method of claim 22, further comprising:
generating, via the voice robot development platform, a plurality of affinity features based on the embedding associated with the current state of the corresponding dialog.
24. The method of claim 23, wherein training the voice robot further comprises:
processing the plurality of affinity features and the embedding associated with the current state of the corresponding dialog using the ML model or a plurality of additional ML layers of additional ML models to generate a predictive embedding associated with a predictive response to at least the portion of the corresponding dialog.
25. The method of claim 24, wherein training the voice robot further comprises:
comparing, in an embedding space, the predicted embedding associated with the predicted response to at least the portion of the corresponding conversation with a corresponding real-value embedding associated with the corresponding RPC outbound request;
generating one or more penalties based on comparing the predicted embedding and the corresponding true value embedding; and
updating the ML model based on one or more of the losses.
26. The method of any of claims 20-25, wherein at least the portion of the corresponding dialog for a given RPC training instance in the plurality of RPC training instances includes at least a corresponding RPC inbound request, and wherein training the voice robot includes:
processing, using a plurality of ML layers of a machine learning ML model and for the given training instance, at least the corresponding RPC inbound request and the previous context of the corresponding conversation to generate an embedding associated with a current state of the corresponding conversation.
27. The method of any of claims 20-26, wherein processing at least the portion of the corresponding conversation and the previous context of the corresponding conversation to generate the embedding associated with the current state of the corresponding conversation comprises:
processing at least the RPC inbound request using a first ML layer of the plurality of ML layers to generate a first embedding,
processing the previous context of the corresponding dialog using a second ML layer of the plurality of ML layers to generate a second embedding, an
Concatenating the first embedding and the second embedding to generate a predictive embedding associated with a predictive response to at least the portion of the corresponding dialog.
28. The method of any of claims 20 to 27, wherein training the voice robot further comprises:
comparing, in an embedding space, the predicted embedding associated with the predicted response to at least the portion of the corresponding dialog with a corresponding real-valued embedding associated with the corresponding real-valued response;
generating one or more penalties based on comparing the predicted embedding and the corresponding true value embedding; and
updating the ML model based on one or more of the losses.
29. The method of any of claims 20 to 28, wherein the third party deploying the voice robot is different from an additional third party associated with the third party system.
30. The method of any of claims 20 to 29, wherein causing the trained voice robot to be deployed for conducting the conversation on behalf of the third party comprises: cause the trained voice robot to be deployed for the conversation of conducting a telephone call associated with the third party, and wherein causing the trained voice robot to be deployed for the conversation of conducting the telephone call associated with the third party comprises:
causing the voice robot to answer a corresponding incoming telephone call and conduct the conversation with a correspondent initiating the corresponding incoming telephone call via a corresponding client device, wherein the voice robot interacts with the third-party system via a corresponding RPC during the conversation with the correspondent.
31. The method of claim 30, further comprising:
after ending the incoming telephone call:
generating a corresponding conversation digest of the conversation conducted during the corresponding incoming telephone call, wherein one or more of the corresponding conversation digests include an indication that the corresponding RPC occurred based on a corresponding spoken utterance of the corresponding person received during the corresponding incoming telephone call; and
causing the corresponding dialog summary of the dialog to be rendered via the voice robot development platform.
32. A voice robot development platform, the voice robot development platform comprising:
at least one processor;
at least one memory;
at least one database, the at least one database comprising a plurality of training instances;
at least one user interface to enable third party developers associated with a third party to interact with the voice robot development platform to:
obtaining a plurality of training instances, each of the plurality of training instances comprising:
training instance input comprising at least a portion of a corresponding conversation and a previous context of the corresponding conversation, an
A training instance output comprising a corresponding real-valued response to at least the portion of the corresponding dialog;
training a voice robot based on the plurality of training instances to generate a plurality of corresponding behaviors of the voice robot; and
after training the voice robot:
enabling the third party developer to add additional training instances stored in the at least one database to add new behaviors for the plurality of corresponding behaviors of the voice robot; and
enabling the third party developer to modify existing training instances stored in the at least one database to modify an existing behavior of the plurality of corresponding behaviors of the voice robot.
Applications Claiming Priority (7)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/112,418 US11804211B2 (en) | 2020-12-04 | 2020-12-04 | Example-based voice bot development techniques |
US17/112,418 | 2020-12-04 | ||
USPCT/US2020/064722 | 2020-12-12 | ||
PCT/US2020/064722 WO2022119580A1 (en) | 2020-12-04 | 2020-12-12 | Example-based voice bot development techniques |
US17/541,098 | 2021-12-02 | ||
US17/541,098 US20220180858A1 (en) | 2020-12-04 | 2021-12-02 | Example-based voice bot development techniques |
PCT/US2021/061855 WO2022120200A2 (en) | 2020-12-04 | 2021-12-03 | Example-based voice bot development techniques |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115699023A true CN115699023A (en) | 2023-02-03 |
Family
ID=79170938
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180039194.4A Pending CN115699023A (en) | 2020-12-04 | 2021-12-03 | Voice robot development technology based on example |
Country Status (5)
Country | Link |
---|---|
EP (1) | EP4128074A2 (en) |
JP (1) | JP2023534101A (en) |
KR (1) | KR20230006900A (en) |
CN (1) | CN115699023A (en) |
WO (1) | WO2022120200A2 (en) |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6925452B1 (en) * | 2000-05-22 | 2005-08-02 | International Business Machines Corporation | Method and system for recognizing end-user transactions |
US20150278706A1 (en) * | 2014-03-26 | 2015-10-01 | Telefonaktiebolaget L M Ericsson (Publ) | Method, Predictive Analytics System, and Computer Program Product for Performing Online and Offline Learning |
CN108229686B (en) * | 2016-12-14 | 2022-07-05 | 阿里巴巴集团控股有限公司 | Model training and predicting method and device, electronic equipment and machine learning platform |
US11386326B2 (en) * | 2018-06-11 | 2022-07-12 | The Regents Of The University Of California | Training a machine learning model with limited training data |
US10861456B2 (en) * | 2018-09-17 | 2020-12-08 | Adobe Inc. | Generating dialogue responses in end-to-end dialogue systems utilizing a context-dependent additive recurrent neural network |
US10853577B2 (en) * | 2018-09-21 | 2020-12-01 | Salesforce.Com, Inc. | Response recommendation system |
US11206229B2 (en) * | 2019-04-26 | 2021-12-21 | Oracle International Corporation | Directed acyclic graph based framework for training models |
-
2021
- 2021-12-03 JP JP2022571846A patent/JP2023534101A/en active Pending
- 2021-12-03 WO PCT/US2021/061855 patent/WO2022120200A2/en unknown
- 2021-12-03 EP EP21835519.6A patent/EP4128074A2/en active Pending
- 2021-12-03 KR KR1020227042499A patent/KR20230006900A/en unknown
- 2021-12-03 CN CN202180039194.4A patent/CN115699023A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
JP2023534101A (en) | 2023-08-08 |
EP4128074A2 (en) | 2023-02-08 |
KR20230006900A (en) | 2023-01-11 |
WO2022120200A2 (en) | 2022-06-09 |
WO2022120200A3 (en) | 2022-07-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11887595B2 (en) | User-programmable automated assistant | |
US11488601B2 (en) | Dependency graph conversation modeling for use in conducting human-to-computer dialog sessions with a computer-implemented automated assistant | |
US11804211B2 (en) | Example-based voice bot development techniques | |
EP4004719B1 (en) | Ambiguity resolution with dialogue search history | |
US20220229993A1 (en) | Context tag integration with named entity recognition models | |
US20240146668A1 (en) | Updating trained voice bot(s) utilizing example-based voice bot development techniques | |
CN116583837A (en) | Distance-based LOGIT values for natural language processing | |
CN116635862A (en) | Outside domain data augmentation for natural language processing | |
CN116615727A (en) | Keyword data augmentation tool for natural language processing | |
CN116547676A (en) | Enhanced logic for natural language processing | |
US20220180858A1 (en) | Example-based voice bot development techniques | |
US11790906B2 (en) | Resolving unique personal identifiers during corresponding conversations between a voice bot and a human | |
CN115699023A (en) | Voice robot development technology based on example | |
CN116724306A (en) | Multi-feature balancing for natural language processors | |
US11924150B2 (en) | System(s) and method(s) for enabling a representative associated with an entity to modify a trained voice bot associated with the entity |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |