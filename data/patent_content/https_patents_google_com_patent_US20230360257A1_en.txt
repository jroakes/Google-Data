US20230360257A1 - Use Of Image Sensors To Query Real World for Geo-Reference Information - Google Patents
Use Of Image Sensors To Query Real World for Geo-Reference Information Download PDFInfo
- Publication number
- US20230360257A1 US20230360257A1 US18/197,364 US202318197364A US2023360257A1 US 20230360257 A1 US20230360257 A1 US 20230360257A1 US 202318197364 A US202318197364 A US 202318197364A US 2023360257 A1 US2023360257 A1 US 2023360257A1
- Authority
- US
- United States
- Prior art keywords
- interest
- points
- features
- location
- geographical area
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 21
- 238000013507 mapping Methods 0.000 claims abstract description 8
- 230000004044 response Effects 0.000 claims description 5
- 230000000007 visual effect Effects 0.000 claims description 4
- 230000001419 dependent effect Effects 0.000 abstract 1
- 230000015654 memory Effects 0.000 description 14
- 238000010586 diagram Methods 0.000 description 11
- 230000008859 change Effects 0.000 description 9
- 230000009471 action Effects 0.000 description 7
- 238000012545 processing Methods 0.000 description 5
- 235000013305 food Nutrition 0.000 description 3
- 238000010801 machine learning Methods 0.000 description 3
- 230000008569 process Effects 0.000 description 3
- 238000004422 calculation algorithm Methods 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 238000012937 correction Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000002452 interceptive effect Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 230000001052 transient effect Effects 0.000 description 2
- 238000013459 approach Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 230000000295 complement effect Effects 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000005484 gravity Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 229910044991 metal oxide Inorganic materials 0.000 description 1
- 150000004706 metal oxides Chemical class 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000004984 smart glass Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5854—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using shape and object relationship
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9537—Spatial or temporal dependent retrieval, e.g. spatiotemporal queries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
- G06T7/73—Determining position or orientation of objects or cameras using feature-based methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/532—Query formulation, e.g. graphical querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/587—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using geographical or spatial information, e.g. location
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
- G06T7/246—Analysis of motion using feature-based methods, e.g. the tracking of corners or segments
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/414—Specialised client platforms, e.g. receiver in car or embedded in a mobile appliance
- H04N21/41415—Specialised client platforms, e.g. receiver in car or embedded in a mobile appliance involving a public display, viewable by several users in a public space outside their home, e.g. movie theatre, information kiosk
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/45—Management operations performed by the client for facilitating the reception of or the interaction with the content or administrating data related to the end-user or to the client device itself, e.g. learning user preferences for recommending movies, resolving scheduling conflicts
- H04N21/4508—Management of client data or end-user data
- H04N21/4524—Management of client data or end-user data involving the geographical location of the client
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30244—Camera pose
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/20—Scenes; Scene-specific elements in augmented reality scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/30—Scenes; Scene-specific elements in albums, collections or shared content, e.g. social network photos or video
Definitions
- Each device may be connected to a network that allows for querying points of interest.
- a user must provide an input, such as text in a search field or a voice command, to query for points of interest.
- a user may have to provide multiple inputs, such as multiple interactions with the display, text input, etc., to obtain information regarding points of interest.
- Existing image-matching-based search tools may perform a search for a point of interest within an image of a geographical area that has been captured using, for instance, a portable device of the user.
- the image-matching-based searching tools may compare the captured image, or features extracted from the captured image, with one or more previously captured images of the environment, or features extracted therefrom.
- objects, structures etc. within the captured image may be recognized as a point of interest e.g. building, landmark etc., that is known to be present in the previously captured image(s) (e.g. by virtue of a stored association between the previously captured image(s) and the point of interest).
- Information relating to the point of interest that is determined to be present in the captured image may then be output via the user's device.
- image matching may be computationally intensive.
- the image capture aspect of the tool may require use of the device screen (so the user can verify what has been captured) as well as the associated power/processing requirements. It may also require a relatively precise and time-consuming user-device interaction in which the user has to activate the camera and then appropriately frame the image so as to properly capture the point of interest about which they require information.
- the system may include one or more processors configured to detect one or more features captured by the one or more image sensors based on data derived from use of one or more image sensors of a user device, determine pose data including a location and orientation of the device based on at least the one or more detected features, determine a plurality of points of interest within a particular geographical area that is determined based on the pose data, and provide information indicating one or more of the plurality of points of interest via the user device.
- the geographic area may be within a predefined distance of the location of the device included in the pose data.
- the geographic area may be within a pre-defined angular range that is based on the orientation of the device included in the pose data.
- the predefined angular range may be wider than a field of view of the one or more image sensors.
- the one or more image sensors may be located on an edge of the user device.
- the plurality of points of interest may be determined by querying a mapping database for points of interest within the particular geographic area.
- the plurality of points of interest are determined by querying the mapping database for points of interest within a particular range from the location, and the returned points of interest are filtered such that points of interest outside an angular range that is centered on the orientation of the device are excluded.
- the method includes deriving, using one or more processors, data from one or more image sensors.
- the method includes detecting, using the one or more processors and based on derived data, one or more features in a particular geographical area, determining, using the one or more processors and based on at least the one or more detected features, pose data including a location and orientation of the device, determining, using the one or more processors and based on the pose data, a plurality of points of interest within the particular geographical area, and providing, using the one or more processors, information indicating one or more of the plurality of points of interest in response to detecting features.
- Yet another aspect of the disclosure provides for a non-transitory computer-readable medium storing instructions, which when executed by one or more processors, cause the processors to derive data from one or more image sensors of a user device, detect one or more features in a particular geographical area based on the derived data, determine pose data including a location and orientation of the device based on at least the one or more detected features, determine a plurality of points of interest within the particular geographical area based on the pose data, and provide information indicating one or more of the plurality of points of interest in response to detecting features.
- FIG. 1 is a block diagram illustrating an example device according to aspects of the disclosure.
- FIG. 2 is a functional diagram of an example system according to aspects of the disclosure.
- FIG. 3 is a perspective drawing of an example device according to aspects of the disclosure.
- FIG. 4 is a pictorial diagram illustrating use of the example device according to aspects of the disclosure.
- FIG. 5 is a pictorial diagram illustrating a field of view of the device according to aspects of the disclosure.
- FIGS. 6 A- 6 B illustrate features and/or objects captured by images sensors according to aspects of the disclosure.
- FIG. 7 is a pictorial diagram of a map determining the location of the device according to aspects of the disclosure.
- FIG. 8 is a pictorial diagram of a map determining the pose data of the device according to aspects of the disclosure.
- FIG. 9 is a pictorial diagram of a map determining a query area according to aspects of the disclosure.
- FIGS. 10 A- 10 C are pictorial diagrams illustrating information regarding the points of interest according to aspects of the disclosure.
- FIG. 11 is a diagram in accordance with aspects of the disclosure.
- FIG. 12 is a flow diagram according to aspects of the disclosure.
- the present disclosure relates to a system that provides information relating to one or more nearby points of interest in response to sensor data from one or more image sensors. More specifically, the image sensors detect features and/or objects, such as, but not limited to, the outline of structures such as buildings or bridges, in the field of view of the image sensors. Pose data, including a location and orientation of the device is determined based on the one or more detected features and/or objects. The pose data may be determined, for instance, by comparing the detected features and/or objects to features stored in an index or database. Such an index may be referred to as a visual positioning system (VPS) index and may have been generated based on a collection of “street-level” images.
- VPS visual positioning system
- a plurality of points of interest within a geographical area may be determined based on the pose data.
- the determination may, for instance, be made by querying a mapping database for points of interest that are known to be located within a particular distance of the location of the device and, therefore, the user.
- the device may provide information to the user indicating one or more of the plurality of points of interest.
- FIG. 1 provides an example block diagram illustrating components of the device.
- the device 100 includes various components, such as one or more processors 102 , memory 104 , and other components typically present in microprocessors, general purpose computers, or the like.
- Device 100 also includes input 110 , an output 120 , and sensors 112 .
- the sensors may include one or more image sensors 114 , an accelerometer 116 , and a global positioning system (“GPS”) sensor 118 .
- GPS global positioning system
- the one or more processors 102 may be any conventional processors, such as commercially available microprocessors. Alternatively, the one or more processors may be a dedicated device such as an application specific integrated circuit (ASIC) or other hardware-based processor.
- FIG. 1 functionally illustrates the processor, memory, and other elements of device 100 as being within the same block, it will be understood by those of ordinary skill in the art that the processor, computing device, or memory may actually include multiple processors, computing devices, or memories that may or may not be stored within the same physical housing. Similarly, the memory may be a hard drive or other storage media located in a housing different from that of device 100 . Accordingly, references to a processor or computing device will be understood to include references to a collection of processors or computing devices or memories that may or may not operate in parallel.
- Memory 104 may store information that is accessible by the processors 102 , including instructions 106 that may be executed by the processors 102 , and data 108 .
- the memory 104 may be of a type of memory operative to store information accessible by the processors 102 , including a non-transitory computer-readable medium, or other medium that stores data that may be read with the aid of an electronic device, such as a hard-drive, memory card, read-only memory (“ROM”), random access memory (“RAM”), optical disks, as well as other write-capable and read-only memories.
- the subject matter disclosed herein may include different combinations of the foregoing, whereby different portions of the instructions 106 and data 108 are stored on different types of media.
- Data 108 may be retrieved, stored or modified by processors 102 in accordance with the instructions 106 .
- the data 108 may be stored in computer registers, in a relational database as a table having a plurality of different fields and records, XML documents, or flat files.
- the data 108 may also be formatted in a computer-readable format such as, but not limited to, binary values, ASCII or Unicode.
- the data 108 may be stored as bitmaps comprised of pixels that are stored in compressed or uncompressed, or various image formats (e.g., JPEG), vector-based formats (e.g., SVG) or computer instructions for drawing graphics.
- the data 108 may comprise information sufficient to identify the relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories (including other network locations) or information that is used by a function to calculate the relevant data.
- the instructions 106 can be any set of instructions to be executed directly, such as machine code, or indirectly, such as scripts, by the processor 102 .
- the terms “instructions,” “application,” “steps,” and “programs” can be used interchangeably herein.
- the instructions can be stored in object code format for direct processing by the processor, or in any other computing device language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. Functions, methods and routines of the instructions are explained in more detail below.
- the device 100 may further include an input 110 .
- the input 110 may be, for example, a touch sensor, dial, button, or other control for receiving a manual command.
- the input 110 may, in some examples, be a microphone.
- the device 100 may also include an output 120 .
- the output 120 may be, for example, a speaker.
- the Device 100 may include sensors 112 .
- the sensors 112 may be one or more image sensors 114 for detecting features and/or objects around the device 100 .
- the one or more image sensors 114 may convert optical signals into electrical signals to detect, or capture, features and/or objects around the device 100 .
- the one or more image sensors may be, for example, a charge coupled device (“CCD”) sensor or a complementary metal oxide semiconductor (“CMOS”) sensor.
- the one or more processors 102 may process the features and/or objects detected by the one or more image sensors 114 to identify at least one detected feature and/or object as a point of interest.
- the one or more image sensors 114 may be located on at least one edge of the device 100 . In some examples, the one or more image sensors 114 may be located on the back of the device 100 .
- the sensors 112 may also include an accelerometer 116 .
- the accelerometer 116 may determine the pose or orientation of the device 100 .
- the device 100 may identify the orientation based on the device's internal compass and the gravity vector based on the device's internal accelerometer 116 .
- Sensors 112 may further include GPS sensors 118 or other positioning elements for determining the location of device 100 .
- the location of the device may be the latitudinal and longitudinal coordinates of the device 100 .
- the device 100 may include other components which are not shown, such as a battery, charging input for the battery, signals processing components, etc. Such components may also be utilized in execution of the instructions 106 .
- FIG. 2 illustrates an example system 200 in which the features described herein may be implemented. It should not be considered limiting the scope of the disclosure or usefulness of the features described herein.
- system 200 may include a plurality of devices 202 , 212 , 222 , 232 , users 204 , 214 , 224 , 234 , server computing device 270 , storage system 260 , and network 250 .
- the collection of devices 202 , 212 , 222 , 232 or a single device will be referenced as device(s) 202 .
- the group of users 204 , 214 , 224 , 234 and a single user will be referenced as user(s) 204 .
- Each device 202 may be a personal computing device intended for use by a respective user 204 and have all of the components normally used in connection with a personal computing device, as described above with relationship to device 100 , including a one or more processors (e.g., a central processing unit (CPU)), memory (e.g., RAM and internal hard drives) storing data and instructions, a display (e.g., a monitor having a screen, a touch-screen, a projector, a television, or other device such as a smart watch display that is operable to display information), and user input devices (e.g., a mouse, keyboard, touchscreen or microphone).
- processors e.g., a central processing unit (CPU)
- memory e.g., RAM and internal hard drives
- a display e.g., a monitor having a screen, a touch-screen, a projector, a television, or other device such as a smart watch display that is operable to display information
- user input devices e.g
- the devices 202 may also include a camera, speakers, a network interface device, and all of the components used for connecting these elements to one another. As mentioned above, the devices 202 may further include the image sensors. The image sensors may capture features and/or object of a plurality of points of interest 290 . Device 202 may be capable of wirelessly exchanging and/or obtaining data over the network 250 .
- the devices 202 may each comprise mobile computing devices capable of wirelessly exchanging data with a server over a network such as the Internet, they may alternatively comprise a full-sized personal computing device.
- devices may be mobile phones or devices such as a wireless-enabled PDA, a tablet PC, a wearable computing device (e.g., a smartwatch, headset, smartglasses, virtual reality player, other head-mounted display, etc.), or a netbook that is capable of obtaining information via the Internet or other networks.
- the devices 202 may be at various nodes of a network 250 and capable of directly and indirectly communicating with other nodes of network 250 . Although four (4) devices are depicted in FIG. 2 , it should be appreciated that a typical system 200 can include one or more devices, with each computing device being at a different node of network 250 .
- the network 250 and intervening nodes described herein can be interconnected using various protocols and systems, such that the network can be part of the Internet, World Wide Web, specific intranets, wide area networks, or local networks.
- the network 250 can utilize standard communications protocols, such as WiFi, that are proprietary to one or more companies.
- system 200 may include one or more server computing devices having a plurality of computing devices, e.g., a load balanced server farm, that exchange information with different nodes of a network for the purpose of receiving, processing and transmitting the data to and from other computing devices.
- server computing devices 270 may be a web server that is capable of communicating with the one or more devices 202 via the network 250 .
- server computing device 270 may use network 250 to transmit and present information to a user 204 of one of the other devices 202 .
- Server computing device 270 may include one or more processors, memory, instructions, and data. These components operate in the same or similar fashion as those described above with respect to device 100 .
- Storage system 260 may store various types of information.
- the storage system 260 may store information about points of interest, such as publically accessible ratings, map data, etc.
- the storage system 260 may store map data.
- the map data may include, for instance, locations of points of interest. This information may be retrieved or otherwise accessed by a service computing device, such as one or more server computing devices 270 , in order to perform some or all of the features described herein.
- FIG. 3 illustrate an example device 300 . While in this example the device 300 is a mobile phone it should be understood that in other examples the device may be any of a variety of different types.
- the device 300 may include input, a display, sensors, internal electronics, and output.
- the input may include a user input 302 , such as those described with respect to device 100 .
- the input may include a microphone 304 for receiving a verbal command or audio input.
- the display 306 may be any type of display, such as a monitor having a screen, a touch-screen, a projector, or a television. As shown in FIG. 3 , the display may be a touch-screen of a mobile device.
- the display 306 of the device 300 may electronically display information to a user via a graphical user interface (“GUI”) or other types of user interfaces. For example, as will be discussed below, display 306 may electronically display information corresponding to points of interest surrounding a user.
- GUI graphical user interface
- the sensors may include image sensors 308 , including those described with respect to device 100 .
- the image sensors 308 may be provided on the edge of the device.
- the image sensors may be positioned such that when the device is in use, such as when a user is holding the device and viewing its display, a field of view of the image sensors 308 includes objects surrounding the device.
- the field of view may include objects in front of the user.
- the image sensors 308 may be naturally pointing away from the user and in the direction of prominent features and/or objects when the device 300 is being held by the user. While three (3) image sensors 308 are shown, the device 300 may have any number of image sensors 308 .
- image sensors 308 are depicted as being located on an edge of device 300 , the image sensors 308 may be located elsewhere such as on the back of the device 300 , the display 306 side of device 300 , or as part of other image capturing mechanisms. Thus, the number and location of image sensors 308 shown in FIG. 3 is not meant to be limiting.
- the user can enable or disable image sensing by the image sensors 308 , and the device may only detect the features and/or objects if the user has enabled this feature.
- the user may set the image sensors 308 to automatically disable in particular locations, such as familiar locations.
- the user may define parameters that only enable image sensing in particular locations or settings, such as outdoors.
- the image sensors 308 may be any sensors capable of receiving imagery.
- the image sensors 308 may capture features and/or objects that are within the field of view of the image sensors 308 .
- Information regarding the captured features/objects may be used to determine further information, such as pose information, nearby points of interest, etc.
- image sensors 308 are illustrated in FIG. 3 , it should be understood that additional or fewer image sensors may be included. Moreover, a position of the image sensors along an edge or back of the device may be varied. As the number and position of the image sensors is varied, the field of view of the sensors may also vary.
- the captured features and/or objects need not be displayed on the display 306 of device 300 .
- the features and/or objects that are captured by the image sensors 308 may never leave the firmware and, therefore, may not be saved as a picture or image to device 300 .
- the sensors may further include GPS sensors.
- the GPS sensors may provide a rough indication as to the location of the device.
- the features captured by image sensors 308 may be used to refine the location indicated by the GPS sensors or vice versa.
- the GPS data may be used to determine which part of the visual positioning system (“VPS”) index should be considered when determining the pose data.
- the VPS may indicate where different parts of the index correspond to different locations.
- the sensors may additionally include accelerometers.
- the accelerometers may determine the pose or orientation of the device.
- the sensors may include a gyroscope.
- the processors may receive gyroscope data and may process the gyroscope data in combination with the data collected from each of the other sensors to determine the orientation of the device. In some examples, the gyroscope data alone may be enough for the processors to determine the orientation of the device.
- the internal electronics may include, for example, one or more processors or other components adapted to processes the features or objects captured by the image sensors 308 .
- the captured features or objects may be processed to determine pose data.
- the pose data may be based on key or prominent features captured by the image sensors 308 .
- the detected features and/or objects may be used in combination with GPS readings and/or accelerometer or other sensor readings to determine the pose data for the device.
- Pose data may include the location of the device, such as the coordinates, and the orientation of the device, such as which direction the image sensor 308 and, by extension, the device 300 and/or user is facing.
- the pose data may be used to determine a plurality of points of interest within the area surrounding the device and, by extension, the user.
- the internal electronics may provide information regarding the points of interest to the user, for instance, on the display 306 of device 300 .
- the output 310 may include one or more speakers for outputting audio, such as playback of music, speech, or other audio content. According to some embodiments, the output may also be the display 306 .
- FIG. 4 illustrates an example where the user is holding the device and the image sensors are capturing features and/or objects in the field of view of the image sensors.
- the device 402 may be similar to device 300 described herein.
- the user 404 may be a distance “D” away from a point of interest 410 .
- the image sensors while not shown, may be located at a top edge of the device 402 . Thus, as the user 404 holds device 402 , the image sensor may capture features and/or objects that are in the field of view of the user 404 .
- the image sensors may capture, for instance, permanent and/or distinctive structures, such as bus stops, buildings, parks, etc. More distinctive features may be useful for characterizing the device's location/orientation and permanent features may be more likely to be represented in the VPS index that is generated based on previously captured images. Conversely, features that are captured by the image sensors that are less distinctive and/or more transient, such as people, sidewalks, cars, trees, and roads, may be disregarded for the purpose of determining the pose data. Less distinctive features may be less likely to assist in determining the location and orientation of the device and transient features are less likely to be represented in the VPS index. In some implementations, machine learning may be used to determine which features to make use of for determining pose data and which features to disregard.
- the points of interest may be determined based on the orientation of the device as indicated by the pose data. For instance, the points of interest may be selected from points of interest that are within the particular range of the user device location and which are within a particular angular range or range or orientations that is based on the orientation of the device.
- Each of the image sensors may have a vertical field of view.
- FIG. 4 illustrates a vertical field of view 406 for the image sensor located on the top edge of the device.
- the vertical field of view 406 may be defined by a predetermined angle 408 .
- the vertical field of view 406 may be determined by an aperture of the image sensor. As shown in FIG. 4 , the image sensor, based on the vertical field of view 406 , may not capture the entire height of the point of interest 410 .
- the vertical field of view search angle may dynamically change in size depending on whether the device is closer to or farther away from the features and/or objects the image sensors are pointing at.
- the vertical field of view search angle may increase when the distance D is smaller, such as when the image sensors are closer to the features and/or objects, in order to be able to capture more features and/or objects.
- the vertical field of view search angle may decrease when the distance D is larger, such as when the image sensors are farther away from the features and/or objects, in order to limit the amount of features and/or objects that will be captured.
- the vertical field of view search angle may be the same angle as angle 408 . In some examples, the vertical field of view search angle may be more or less than angle 408 .
- FIG. 5 illustrates an example of a horizontal field of view of the device. Similar to the device shown in FIG. 4 , the image sensor may be located on a top edge of device 502 . The image sensor may have a horizontal field of view 506 . The horizontal field of view 506 may be measured by a predetermined angle 508 . The angle 508 may be centered based on the orientation of the device 502 . According to some examples, the predetermined angle 508 may be 30 degrees. In some examples, the angle 508 may be more than 30 degrees or less than 30 degrees. This may serve to provide indication of points of interest that are within the field of view of the user.
- the horizontal point of interest search angle may change in size based on the location of the device as compared to the features and/or objects of interest. For example, the horizontal point of interest search angle may dynamically change depending on whether the image sensors are closer to or farther away from the features and/or objects the image sensors are pointing at.
- the horizontal point of interest search angle may increase when the distance D is smaller, such as when the image sensors are closer to the features and/or objects, in order to be able to capture more features and/or objects.
- the horizontal point of interest search angle may decrease when the distance D is larger, such as when the image sensors are farther away from the features and/or objects, in order to limit the amount of features and/or objects that will be captured.
- the horizontal field of view search angle may be the same angle as angle 508 . In some examples, the horizontal field of view search angle may be more or less than angle 508 .
- the internal electronics of the device 502 may provide pose correction for natural pointing offsets.
- the point of interest may lay outside the field of view 506 due to user error, such as aiming inaccuracies.
- the search angle may be extended, for instance, beyond an angular range corresponding to the field of view of the image sensors. This may serve to compensate for the user's inaccuracy in pointing the image sensor at the point of interest.
- the field of view 506 may increase the search angle by a predetermined amount, as shown by the increased angle 520 .
- the angle 508 may increase by 10 degrees to angle 520 .
- An increase of 10 degrees is merely an example.
- the search angle may increase from field of view angle 508 may increase by 2 degrees, 5 degrees, 7.5 degrees, 20 degrees, etc. Therefore, in some examples, there may be a larger field of view 518 for search purposes than the field of view 516 of the image sensors.
- the points of interest 510 may include a museum, a bar, cafe, or restaurant, a food stand, a store, a medical facility, landmark, or any other location.
- the field of view 506 of the image sensor may only capture buildings 513 - 515 .
- the field of view 508 may increase such that the search area corresponds to search angle 518 , thereby capturing buildings 512 - 515 .
- the processors of device 502 may increase the search angle from field of view 508 based on the distance between the device and the plurality of points of interest within the field of view 508 to provide for pose correction for natural pointing offsets.
- FIGS. 6 A and 6 B illustrate examples of capturing and analyzing objects to determine the points of interest.
- the image sensors may capture features and/or objects in the field of view of the image sensors.
- the captured 600 buildings 602 - 604 may be the prominent features and/or objects in the field of view.
- less distinctive features such as sidewalks, cars, trees, and people, may by disregarded as they may be less likely to assist in determining the location and orientation of the device and, therefore, the points of interest.
- the captured 600 features and/or objects may not be displayed to the user.
- the captured 600 features and/or objects may not be saved to the device.
- FIG. 6 B illustrates an example of how the features and/or objects in the search angle within the field of view are analyzed.
- the captured 600 buildings 602 - 604 may be analyzed to determine the amount of space that each of the buildings 602 - 604 take up in the field of view.
- the device may determine each of the buildings 602 - 604 has a respective width 612 - 614 in the search angle within the field of view.
- the width may be considered when determining the points of interest captured by the image sensors.
- the points of interest, i.e. buildings 602 - 604 may be considered based on their respective widths 612 - 614 instead of just a single point captured by the image sensor.
- width 613 of building 603 is completely within the search angle within the field of view of the image sensor and is the largest width captured by the image sensor.
- Building 604 has a width 614 that is smaller than width 613 but larger than width 612 of building 602 .
- the respective widths 612 - 614 may be used to determine a rank of the likely points of interest. For example, the point of interest that takes up the most width in the search angle within the field of view of the image sensor may indicate that that point of interest was what the image sensor was pointed at.
- FIG. 7 illustrates an example of the location of the device.
- the GPS sensors along with the accelerometers, image sensors, and any other sensors, may determine the location of the device. For example, the sensors may determine that the device is at location X, corresponding to a certain position on map 700 . However, due to uncertainties in the sensors, the location of the device may be identified as location X and a surrounding radius 702 . The radius 702 may compensate for the uncertainty of the device's exact location and provide a greater search area when determining the plurality of points of interest.
- FIG. 8 illustrates an example of determining the pose data of the device.
- the pose data may include the location of the device and the orientation of the device.
- the sensors may determine the direction the image sensors are pointing.
- the image sensors may have a field of view angle 802 .
- the features and/or objects detected by the images sensors in combination with the GPS sensor readings and accelerometer readings, may determine the orientation of the device.
- the accelerometer may determine whether the device has been rotated from a portrait position to a landscape position. The sensors may, therefore, determine that the image sensors are pointing in a certain direction, such as due East.
- FIG. 9 illustrates an example of a query based on the pose data of the device.
- One directional and/or multi-directional querying may be performed to determine the points of interest in the field of view of the image sensors.
- the system may perform either or both the one directional query and multi-directional query.
- One directional query may be used when the device and, therefore, the image sensors are far away from the point of interests 910 , 911 .
- the one directional query may use only the features and/or objects within the search angle within the image sensors' field of view 902 to determine the plurality of points of interest 910 , 911 .
- the device may be at location X, as shown on map 900 .
- the image sensors may be pointed in a direction due East. As shown, location X of the device is across the street from points of interest 910 , 911 .
- the device may be far enough away from points of interest 910 , 911 to only use one directional querying based on the search angle within the field of view 904 of the image sensors.
- Multi-directional queries may be used when the device and, therefore, the image sensors are close to the points of interest. This approach may serve to compensate for the fact that the effect of inaccurate device/image sensor pointing is more pronounced the closer the points of interest are to the user. This may also compensate for the fact that, when close to the points of interest, not as many features and/or objects may be captured by the image sensors.
- the multi-directional query may be performed by searching the search angle within the field of view of the sensors, but also a radius surrounding the location of the device. For example, it may be determined that location X is too close to any particular points of interest to determine what point of interest to identify.
- the device may use multi-directional querying to search the area 904 surrounding location X when determining the points of interest.
- the multi-directional query may identify points of interest 910 , 911 , which are in the search angle within the field of view 902 of the image sensors.
- the multi-directional query may also identify points of interest 912 , 913 which are not in the search angle within the field of view 902 of the image sensors but may be, instead, behind the image sensors.
- multi-directional querying comprises indicating points of interest that are within a first area defined by a first distance from the device location and a first angular range, in addition to points of interest that are within a second area defined by a second distance from the device location, that is greater than the first distance, and a second angular range that is narrower than the first angular range.
- the first angular range may be between 120 and 360 degrees and the first distance may be between 1 and 10 meters.
- the second angular range may be between 20 and 50 degrees and the second distance may be between 10 and 400 meters.
- the first angular range may be between 90 and 300 degrees and the first distance may be between 0.5 and 8 meters; the second angular range may be between 12.5 and 45 degrees and the second distance may be between 8 and 350 meters.
- the angular ranges and distances may be based on the number and position of the image sensors and the specifications of the image sensors.
- a plurality of points of interest may be identified and provided to the user.
- the points of interest may be ranked and provided for display in order of relevance. For instance, after the plurality of points of interest within the particular geographical area has been determined, relevance weights for the points of interest may be calculated.
- the relevance weight may be used to rank the points of interest based on one or more criteria. In this regard, for example, the highest ranking points of interest may be provided as output by the device.
- the relevance weights may be determined based on one or both of a distance from the location indicated by the pose data and a type of the point of interest. In some examples, a rating associated with the point of interest may also be taken into account. For instance, the relevance weights may be calculated with a formula such as:
- Weight (Distance Group Factor*50)+(Type Factor*10)+(Rating Factor*2)
- the distance group factor may be determined based on a predefined distance threshold between the device and the point of interest. For example, a point of interest that is between 0 m and 50 m from the device may have a distance group factor of 3. A point of interest that is between 50 m and 175 m from the device may have a distance group factor of 2. A point of interest that is more than 175 m away from the device may have a distance group factor of 1.
- the distances groups and, therefore, the factors provided are merely examples and thus can be defined by any ranges and any factor number. For example, a point of interest that is between 0 m and 100 m may have a distance group factor of 5 while a point of interest that is between 100 m and 160 m may have a distance group factor of 4.
- the distance thresholds and therefore the distance group factor, may be defined based on the location of the device. For example, in a crowded city, the distance thresholds may decrease as points of interest may be closer together. In a rural area, the distance thresholds may increase as points of interest may be more spread apart.
- the distance factor may be determined using a linear clustering algorithm.
- the type factor may be determined based on the type of location or establishment. For example, the device may consider whether the point of interest is a museum, a bar, cafe, or restaurant, a food stand, a store, a medical facility, landmark, or any other location. The device may learn what types of places are most likely to be queried and, therefore, give those types of places a higher type factor.
- a museum may return a type factor of 4; a car, café, or restaurant may return a type factor of 3.5; a place that serves food but is not a bar, café or restaurant may have a type factor of 3; a store may have a type factor of 2.5; a medical facility, such as a doctor's office or dentist's office, may have a type factor of 2; any other point of interest that does not fall within the predefined types may have a type factor of 1.
- a type factor of 1 may also be used as a default type factor.
- the type factor may be determined based on the percentage of space the point of interest takes up in the field of view.
- a museum or department store may have a type factor of 4 and a café may have a type factor of 2.
- the type factor may be determined based on whether the point of interest is see-through, such as a bus stop, or solid, such as a building.
- the type factors may change as the system learns what types of places users are most likely to query. For example, the system may perform machine learning to continuously update and change the type factors for each of the points of interest.
- Points of interest that are within the same distance group and/or type group may be further ranked.
- the relevance weight may include a further calculation with a formula such as:
- the normalized distance may be calculated using a formula such as:
- the Minimum may be a smallest value between the Distance to User and the Search Range. This may provide for consideration of the distance between objects while only provided a small weight to the distance.
- the Distance to User may be determined by the linear or absolute distance between the device and the identified point of interest.
- the linear distance may be considered the distance “as the crow flies,” such that the distance does not follow the foot path or walking distance between the device and the point of interest.
- the Distance to User may be determined by the distance the user would have to walk to arrive at the point of interest. For example, the user may have to follow traffic patterns and walk on sidewalks, and not through buildings.
- the Search Range may be determined based on a range preset by the device, the operative system, the internal electronics, etc.
- the Search Range may be a 100 meter radius around the determined location of device.
- the search may only include points of interest within a 100 meter radius.
- the 100 meters is merely one example.
- the Search Range may be smaller or larger. For example, in crowded places, like a city, the Search Range may be smaller to limit the number of points of interest returned by the search. In less crowded places, such as a rural location, the Search Range may be larger to increase the number of points of interest returned by the search.
- the Search Range may change without user input. For example, the Search Range may change based on the determined location of the device.
- the rating factor may be determined based on at least one user created rating of a place that is publically available. For example, the system may query internet review websites to determine a publicly available rating for each of the point of interest.
- the relevance weights may be further calculated based on line of sight, the normal vector of the point of interest, and angular size.
- the line of sight may include what the image sensors captures.
- the relevance weight may give more weight to a point of interest in the line of sight of the image sensors, and therefore the user, than a point of interest that is barely visible or not visible at all in the line of sight.
- the normal vector of the point of interest may take into account the angular measurement from the normal of the point of interest.
- the normal is the line taken perpendicular to the façade of the point of interest.
- the relevance weight may consider the angle from normal. The larger the angle from normal, the image sensors may be less likely to be pointing directly at the point of interest.
- the angular size may include the size of the feature and/or object in the field of view, measured as an angle. This may, in some examples, be used to calculate how much of the field of view the feature and/or object occupies.
- the relevance weights may be further calculated based on a previous history of places the user has visited or ranked. A place that has been previously selected may be ranked higher.
- the relevance weights may be calculated based on points of interest having sales or specials. For example, if a department store is having a sale, the department store may be ranked higher than the big box store that is not having a sale. In some examples, points of interest that have an event that day, such as a theater or a concert venue, may be ranked higher based on the date and time of the vent.
- the calculation of the relevance weight may include the size of the façade of the point of interest in the field of view. For example, a façade that is larger may be more prominent in the field of view and, therefore, may be more relevant.
- Device 1000 may be similar to device 300 .
- device 1000 may include an input 1002 , a microphone 1004 , a display 1006 , one or more image sensors 1008 , and an output 1010 . These features may be similar to those discussed herein with respect to device 300 .
- the device may provide information regarding the points of interest to the user, for instance, by overlaying the information on the home screen, or display 1006 , of the device.
- the information may be provided in a variety of different ways, such as a list, a map, annotations to a map, etc.
- the information may be ordered based on the relevance weights, distance to the user, highest rated point of interest, visibility etc. According to other examples, the information may be output as audio data through one or more speakers of the device or accessories paired with the device.
- points of interest may be promoted.
- the point of interest may be promoted if the owner or advertiser of the point of interest pays to have the point of interest promoted.
- the promoted point of interest may be indicated as the most relevant point of interest due to the promotion. For example, if a point of interest is being promoted, the point of interest may be indicated as “promoted” and appear as the most relevant point of interest.
- the promoted point of interest may be indicated as “sponsored” or “advertisement” to indicate that the promoted point of interest may not be the most relevant point of interest based on the calculated relevant weights.
- the device may provide this information to the user with decreased or minimal user input. For example, the user may not have to open an application on the device (or even, in some examples, provide any active input) to run a search query to determine points of interest in the vicinity of the user.
- the device may, according to some examples, automatically (or based on a single user input) query the field of view of the image sensors to determine and provide information relating to the points of interest in the particular geographical area. Further, the device may provide more relevant results to the user based on the field of view of the image sensors and the relevance weight given to each of the plurality of points of interest.
- FIG. 10 A illustrates an example where the information regarding the points of interest is provided as a detailed list.
- Each of the points of interest may be provided as an individual place card 1020 - 1024 , or button, on the display 1006 .
- the more relevant point of interest may be a restaurant, shown as place card 1020 .
- Place card 1020 may include information about the restaurant, including the distance from the determined location of device and the publically available rating.
- the place card 1020 may be interactive, such as serving as an icon for user input to allow the user to find out more information regarding the restaurant.
- the interactive place card 1020 may allow for quick actions relevant to the point of interest.
- the system may be integrated with services for the points of interest to allow for quick actions.
- the quick action may allow a user to make a reservation.
- the second most relevant point of interest may be a coffee shop, shown as place card 1021 .
- the coffee shop may be the second most relevant due to size of the front façade, distance from the determined location of device, publically available rating, etc.
- a quick action for place card 1021 the coffee shop, may be to place a mobile order for coffee.
- the quick action may allow a user to order and pay for their order without ever entering the point of interest.
- the third most relevant point of interest may be a convenience store, shown as place card 1022 .
- the fourth most relevant point of interest may be a big box store, shown as place card 1023 .
- the fifth most relevant point of interest may be a movie theater, shown as place card 1023 .
- a quick action for place card 1023 the movie theater, may be to order movie tickets.
- the quick action may allow a user to order tickets and select the seats for the showing.
- There may be more relevant points of interest than those shown on display 1006 .
- display 1006 may be touch activated such that user can provide a touch input to scroll through the list to see additional points of interest.
- the order of which the places of interest appear may change as the system learns the choices of the users.
- the device may learn over time that users in that particular geographical area consistently choose a particular option and, therefore, the device may promote the particular option.
- the relevance weight formula may rank the points of interest, in order from most relevant to least relevant, as: restaurant, coffee shop, convenience store, big box store, and movie theater.
- the system may learn through machine learning that, when presented with the points of interest in that order, users may select the coffee shop more often than they select the restaurant.
- the system may present the list of points of interest with the coffee shop being the most relevant, even if the relevance weight of the coffee shop is less than the relevance weight of restaurant.
- FIG. 10 B illustrates an example where the information regarding the points of interest is provided as an annotated map.
- each of the points of interest may be provided as a point A-F on map 1030 .
- information regarding points A-F may be provided below map 1030 .
- the name and/or type of point of interest may be shown in relationship to the points A-F on the map.
- Point A may be considered the most relevant point of interest because it is listed first.
- Point A on map 1030 may correspond to a restaurant
- point B may correspond to a coffee shop
- point C may correspond to a convenience store
- point D may correspond to a big box store
- point E may correspond to a movie theater
- point F may correspond to a café.
- the user may be able to interact with map 1030 .
- the user may be able to zoom in and/or out on map 1030 .
- the user may be able to select one point A-F of the points of interest to obtain more information about that point of interest.
- selecting one point A-F may cause information to appear on display 1006 , such as the distance of the point of interest to the determined location of device, the rating of the point of interest, etc.
- the user may select a point A-F from the list below map 1030 , which may cause information pertaining to that point of interest to appear on display 1006 .
- FIG. 10 C illustrates another example where the information regarding the points of interest is provided on the display.
- a street view image 1240 of the points of interest may be provided on display 1006 .
- Each of the points of interest may have a pin A-C identifying the point of interest.
- Below the street view image 1240 there may be a list of the points of interest corresponding to pins A-C.
- the user may be able to select points A-C from the street view image 1240 or from the list provided below the street view image 1240 to display more information regarding the selected point of interest.
- FIG. 11 illustrates a sequence of steps that may occur among device 1102 , server 1104 , VPS 1106 , and database 1108 .
- image data may be sensed by one or more image sensors of the device 1102 .
- features and/or object may be detected in the image data.
- device 1102 may detect, or extract, features and/or objects from the image data.
- the detected features and/or objects from the image data 1112 may be sent from device 1102 to server 1104 or VPS 1106 .
- the detected features and/or objects from the image data 1114 may be sent from server 1104 to VPS 1106 .
- server 1104 or VPS 1106 may detect features and/or objects in the image data 1112 , 1114 .
- the VPS 1106 may send the pose data of the device 1118 to server 1104 .
- the pose data of the device 1118 may be based on the detected features and/or objects from the image data 1112 , 1114 and the VPS index.
- the VPS 1106 may determine a location and orientation of the device based on at least the detected features and/or objects from the image data 1112 , 1114 .
- the pose data of the device 1118 may be used to query a mapping database for a plurality of points of interest.
- the determined plurality of points of interest based on pose data 1116 may be sent from database 1108 to server 1104 .
- the plurality of points of interest may be located within a radius of the location of the device 1102 .
- the plurality of points of interest may be filtered. For example, points of interest that do not fall within a certain angular range that is centered on the device orientation may be filtered out such that only points of interest within the certain angular range are returned.
- the certain angular range may be a point of interest search angle.
- the point of interest search angle may dynamically change based on the distance between the image sensors and the plurality of points of interest. In some examples, the point of interest search angle may be changed by the user.
- the point of interest search angle may be, for example, similar to the field of view for a user.
- the plurality of points of interest may be filtered at the VPS 1106 , the server 1104 , or device 1102 .
- the device 1102 may receive the plurality of points of interest within the radius and filter the plurality of points of interest based on the point of interest search angle.
- the server 1104 may filter the plurality of points of interest based on the point of interest search angle and sent only the filtered plurality of points of interest to the device 1102 .
- the filtered plurality of points of interest 1122 may be provided to device 1102 .
- FIG. 12 illustrates an example method for providing information regarding at least one point of interest in the geographical area of the device.
- the system may derive data from the one or more image sensors.
- the system may detect one or more features in a particular geographical area based on the data derived from the one or more image sensors. For example, the system may detect features and/or objects in the data derived from the one or more image sensors.
- the features and/or objects may include permanent and/or distinctive structures that may be useful in determining the device's location.
- the features and/or objects may be in the VPS index and, therefore, may have already been used to determine a location.
- the system may determine the pose data of the device based on the detected features and/or objects.
- the pose data may include the location and orientation of the device.
- the system may use the features and/or objects captured by the image sensors in combination with a variety of other sensors.
- the other sensors may include a GPS sensor, an accelerometer, a gyroscope, etc.
- the GPS sensor in combination with the features and/or objects detected, may determine the location of the device.
- the accelerometer and/or gyroscope may be used to determine the orientation of the device.
- the orientation of the device may include the direction the image sensors are pointing and, therefore, the direction the user is facing.
- the system may determine points of interest within the particular geographical area based on the pose data.
- the geographical area may be set by a maximum distance away from the pose data, such as the location of the device.
- the geographical area may be a 200 meter radius around the device.
- the geographical area may be a three (3) block radius around the device.
- the determined points of interest may be locations that are visible to the image sensors and, therefore, the user.
- the system may provide information indicating one or more of the points of interest.
- the system may provide this information to a display of the user device.
- the system may provide a list, an annotated map, an annotated street view, etc. indicating the points of interest.
- the system may provide audio feedback indicating the points of interest.
- the points of interest may be ranked based on relevance. For example, a location that is closer to the location of the device, has been chosen more often by others, has a higher publically accessible rating, etc. may be factors in determining the relevance of the point of interest. An algorithm may be used to determine the relevance. The system may rank the points of interest based on their calculated relevance weight.
- the technology described herein may enable a search for points of interest in the vicinity of the user in a manner that is less computationally intensive and requires fewer device resources than existing e.g. image matching-based techniques.
- the technology may also result in indications regarding points of interest in the user's vicinity being provided more quickly, with a less arduous and more intuitive user-device interaction.
Abstract
The present disclosure provides systems and methods that makes use of one or more image sensors of a device to provide users with information relating to nearby points of interest. The image sensors may be used to detect features and/or objects in the field of view of the image sensors. Pose data, including a location and orientation of the device is then determined based on the one or more detected features and/or objects. A plurality of points of interest that are within a geographical area that is dependent on the pose data are then determined. The determination may, for instance, be made by querying a mapping database for points of interest that are known to be located within a particular distance of the location of the user. The device then provides information to the user indicating one or more of the plurality of points of interest.
Description
- This application is a continuation of U.S. patent application Ser. No. 17/527,508, filed Nov. 16, 2021, which is a continuation of U.S. patent application Ser. No. 16/757,638 filed Apr. 20, 2020, which is a national phase entry under 35 U.S.C. § 371 of International Application No. PCT/US2019/060061, filed Nov. 6, 2019, the entire disclosures of which are incorporated herein by reference.
- Many devices, including smartphones and smartwatches, provide for geographic location searching. Each device may be connected to a network that allows for querying points of interest. Typically, a user must provide an input, such as text in a search field or a voice command, to query for points of interest. A user may have to provide multiple inputs, such as multiple interactions with the display, text input, etc., to obtain information regarding points of interest.
- Existing image-matching-based search tools may perform a search for a point of interest within an image of a geographical area that has been captured using, for instance, a portable device of the user. The image-matching-based searching tools may compare the captured image, or features extracted from the captured image, with one or more previously captured images of the environment, or features extracted therefrom. In this way, objects, structures etc. within the captured image may be recognized as a point of interest e.g. building, landmark etc., that is known to be present in the previously captured image(s) (e.g. by virtue of a stored association between the previously captured image(s) and the point of interest). Information relating to the point of interest that is determined to be present in the captured image may then be output via the user's device. Such image matching may be computationally intensive. In addition, the image capture aspect of the tool may require use of the device screen (so the user can verify what has been captured) as well as the associated power/processing requirements. It may also require a relatively precise and time-consuming user-device interaction in which the user has to activate the camera and then appropriately frame the image so as to properly capture the point of interest about which they require information.
- One aspect of the disclosure provides for a system for providing information indicating one or more points of interest. For example, the system may include one or more processors configured to detect one or more features captured by the one or more image sensors based on data derived from use of one or more image sensors of a user device, determine pose data including a location and orientation of the device based on at least the one or more detected features, determine a plurality of points of interest within a particular geographical area that is determined based on the pose data, and provide information indicating one or more of the plurality of points of interest via the user device.
- The geographic area may be within a predefined distance of the location of the device included in the pose data. The geographic area may be within a pre-defined angular range that is based on the orientation of the device included in the pose data. The predefined angular range may be wider than a field of view of the one or more image sensors. The one or more image sensors may be located on an edge of the user device.
- The plurality of points of interest may be determined by querying a mapping database for points of interest within the particular geographic area. The plurality of points of interest are determined by querying the mapping database for points of interest within a particular range from the location, and the returned points of interest are filtered such that points of interest outside an angular range that is centered on the orientation of the device are excluded.
- Another aspect of the disclosure provides for a method for providing information indicating one or more points of interest. The method includes deriving, using one or more processors, data from one or more image sensors. The method includes detecting, using the one or more processors and based on derived data, one or more features in a particular geographical area, determining, using the one or more processors and based on at least the one or more detected features, pose data including a location and orientation of the device, determining, using the one or more processors and based on the pose data, a plurality of points of interest within the particular geographical area, and providing, using the one or more processors, information indicating one or more of the plurality of points of interest in response to detecting features.
- Yet another aspect of the disclosure provides for a non-transitory computer-readable medium storing instructions, which when executed by one or more processors, cause the processors to derive data from one or more image sensors of a user device, detect one or more features in a particular geographical area based on the derived data, determine pose data including a location and orientation of the device based on at least the one or more detected features, determine a plurality of points of interest within the particular geographical area based on the pose data, and provide information indicating one or more of the plurality of points of interest in response to detecting features.
-
FIG. 1 is a block diagram illustrating an example device according to aspects of the disclosure. -
FIG. 2 is a functional diagram of an example system according to aspects of the disclosure. -
FIG. 3 is a perspective drawing of an example device according to aspects of the disclosure. -
FIG. 4 is a pictorial diagram illustrating use of the example device according to aspects of the disclosure. -
FIG. 5 is a pictorial diagram illustrating a field of view of the device according to aspects of the disclosure. -
FIGS. 6A-6B illustrate features and/or objects captured by images sensors according to aspects of the disclosure. -
FIG. 7 is a pictorial diagram of a map determining the location of the device according to aspects of the disclosure. -
FIG. 8 is a pictorial diagram of a map determining the pose data of the device according to aspects of the disclosure. -
FIG. 9 is a pictorial diagram of a map determining a query area according to aspects of the disclosure. -
FIGS. 10A-10C are pictorial diagrams illustrating information regarding the points of interest according to aspects of the disclosure. -
FIG. 11 is a diagram in accordance with aspects of the disclosure. -
FIG. 12 is a flow diagram according to aspects of the disclosure. - The present disclosure relates to a system that provides information relating to one or more nearby points of interest in response to sensor data from one or more image sensors. More specifically, the image sensors detect features and/or objects, such as, but not limited to, the outline of structures such as buildings or bridges, in the field of view of the image sensors. Pose data, including a location and orientation of the device is determined based on the one or more detected features and/or objects. The pose data may be determined, for instance, by comparing the detected features and/or objects to features stored in an index or database. Such an index may be referred to as a visual positioning system (VPS) index and may have been generated based on a collection of “street-level” images.
- A plurality of points of interest within a geographical area may be determined based on the pose data. The determination may, for instance, be made by querying a mapping database for points of interest that are known to be located within a particular distance of the location of the device and, therefore, the user. The device may provide information to the user indicating one or more of the plurality of points of interest.
-
FIG. 1 provides an example block diagram illustrating components of the device. As shown, thedevice 100 includes various components, such as one ormore processors 102,memory 104, and other components typically present in microprocessors, general purpose computers, or the like.Device 100 also includesinput 110, anoutput 120, andsensors 112. The sensors may include one or more image sensors 114, an accelerometer 116, and a global positioning system (“GPS”) sensor 118. - The one or
more processors 102 may be any conventional processors, such as commercially available microprocessors. Alternatively, the one or more processors may be a dedicated device such as an application specific integrated circuit (ASIC) or other hardware-based processor. AlthoughFIG. 1 functionally illustrates the processor, memory, and other elements ofdevice 100 as being within the same block, it will be understood by those of ordinary skill in the art that the processor, computing device, or memory may actually include multiple processors, computing devices, or memories that may or may not be stored within the same physical housing. Similarly, the memory may be a hard drive or other storage media located in a housing different from that ofdevice 100. Accordingly, references to a processor or computing device will be understood to include references to a collection of processors or computing devices or memories that may or may not operate in parallel. -
Memory 104 may store information that is accessible by theprocessors 102, includinginstructions 106 that may be executed by theprocessors 102, anddata 108. Thememory 104 may be of a type of memory operative to store information accessible by theprocessors 102, including a non-transitory computer-readable medium, or other medium that stores data that may be read with the aid of an electronic device, such as a hard-drive, memory card, read-only memory (“ROM”), random access memory (“RAM”), optical disks, as well as other write-capable and read-only memories. The subject matter disclosed herein may include different combinations of the foregoing, whereby different portions of theinstructions 106 anddata 108 are stored on different types of media. -
Data 108 may be retrieved, stored or modified byprocessors 102 in accordance with theinstructions 106. For instance, although the present disclosure is not limited by a particular data structure, thedata 108 may be stored in computer registers, in a relational database as a table having a plurality of different fields and records, XML documents, or flat files. Thedata 108 may also be formatted in a computer-readable format such as, but not limited to, binary values, ASCII or Unicode. By further way of example only, thedata 108 may be stored as bitmaps comprised of pixels that are stored in compressed or uncompressed, or various image formats (e.g., JPEG), vector-based formats (e.g., SVG) or computer instructions for drawing graphics. Moreover, thedata 108 may comprise information sufficient to identify the relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories (including other network locations) or information that is used by a function to calculate the relevant data. - The
instructions 106 can be any set of instructions to be executed directly, such as machine code, or indirectly, such as scripts, by theprocessor 102. In that regard, the terms “instructions,” “application,” “steps,” and “programs” can be used interchangeably herein. The instructions can be stored in object code format for direct processing by the processor, or in any other computing device language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. Functions, methods and routines of the instructions are explained in more detail below. - The
device 100 may further include aninput 110. Theinput 110 may be, for example, a touch sensor, dial, button, or other control for receiving a manual command. Theinput 110 may, in some examples, be a microphone. Thedevice 100 may also include anoutput 120. Theoutput 120 may be, for example, a speaker. -
Device 100 may includesensors 112. Thesensors 112 may be one or more image sensors 114 for detecting features and/or objects around thedevice 100. The one or more image sensors 114 may convert optical signals into electrical signals to detect, or capture, features and/or objects around thedevice 100. The one or more image sensors may be, for example, a charge coupled device (“CCD”) sensor or a complementary metal oxide semiconductor (“CMOS”) sensor. The one ormore processors 102 may process the features and/or objects detected by the one or more image sensors 114 to identify at least one detected feature and/or object as a point of interest. The one or more image sensors 114 may be located on at least one edge of thedevice 100. In some examples, the one or more image sensors 114 may be located on the back of thedevice 100. - The
sensors 112 may also include an accelerometer 116. For example, the accelerometer 116 may determine the pose or orientation of thedevice 100. According to some examples, thedevice 100 may identify the orientation based on the device's internal compass and the gravity vector based on the device's internal accelerometer 116.Sensors 112 may further include GPS sensors 118 or other positioning elements for determining the location ofdevice 100. The location of the device may be the latitudinal and longitudinal coordinates of thedevice 100. - It should be understood that the
device 100 may include other components which are not shown, such as a battery, charging input for the battery, signals processing components, etc. Such components may also be utilized in execution of theinstructions 106. -
FIG. 2 illustrates anexample system 200 in which the features described herein may be implemented. It should not be considered limiting the scope of the disclosure or usefulness of the features described herein. In this example,system 200 may include a plurality ofdevices users server computing device 270,storage system 260, andnetwork 250. For purposes of ease, the collection ofdevices users - Each
device 202 may be a personal computing device intended for use by arespective user 204 and have all of the components normally used in connection with a personal computing device, as described above with relationship todevice 100, including a one or more processors (e.g., a central processing unit (CPU)), memory (e.g., RAM and internal hard drives) storing data and instructions, a display (e.g., a monitor having a screen, a touch-screen, a projector, a television, or other device such as a smart watch display that is operable to display information), and user input devices (e.g., a mouse, keyboard, touchscreen or microphone). Thedevices 202 may also include a camera, speakers, a network interface device, and all of the components used for connecting these elements to one another. As mentioned above, thedevices 202 may further include the image sensors. The image sensors may capture features and/or object of a plurality of points of interest 290.Device 202 may be capable of wirelessly exchanging and/or obtaining data over thenetwork 250. - Although the
devices 202 may each comprise mobile computing devices capable of wirelessly exchanging data with a server over a network such as the Internet, they may alternatively comprise a full-sized personal computing device. By way of example only, devices may be mobile phones or devices such as a wireless-enabled PDA, a tablet PC, a wearable computing device (e.g., a smartwatch, headset, smartglasses, virtual reality player, other head-mounted display, etc.), or a netbook that is capable of obtaining information via the Internet or other networks. - The
devices 202 may be at various nodes of anetwork 250 and capable of directly and indirectly communicating with other nodes ofnetwork 250. Although four (4) devices are depicted inFIG. 2 , it should be appreciated that atypical system 200 can include one or more devices, with each computing device being at a different node ofnetwork 250. Thenetwork 250 and intervening nodes described herein can be interconnected using various protocols and systems, such that the network can be part of the Internet, World Wide Web, specific intranets, wide area networks, or local networks. Thenetwork 250 can utilize standard communications protocols, such as WiFi, that are proprietary to one or more companies. Although certain advantages are obtained when information is transmitted or received as noted above, other aspects of the subject matter described herein are not limited to any particular manner of transmission. - In one example,
system 200 may include one or more server computing devices having a plurality of computing devices, e.g., a load balanced server farm, that exchange information with different nodes of a network for the purpose of receiving, processing and transmitting the data to and from other computing devices. For instance, one or moreserver computing devices 270 may be a web server that is capable of communicating with the one ormore devices 202 via thenetwork 250. In addition,server computing device 270 may usenetwork 250 to transmit and present information to auser 204 of one of theother devices 202.Server computing device 270 may include one or more processors, memory, instructions, and data. These components operate in the same or similar fashion as those described above with respect todevice 100. -
Storage system 260 may store various types of information. For instance, thestorage system 260 may store information about points of interest, such as publically accessible ratings, map data, etc. Thestorage system 260 may store map data. The map data may include, for instance, locations of points of interest. This information may be retrieved or otherwise accessed by a service computing device, such as one or moreserver computing devices 270, in order to perform some or all of the features described herein. -
FIG. 3 illustrate anexample device 300. While in this example thedevice 300 is a mobile phone it should be understood that in other examples the device may be any of a variety of different types. Thedevice 300 may include input, a display, sensors, internal electronics, and output. - The input may include a
user input 302, such as those described with respect todevice 100. The input may include amicrophone 304 for receiving a verbal command or audio input. - The
display 306 may be any type of display, such as a monitor having a screen, a touch-screen, a projector, or a television. As shown inFIG. 3 , the display may be a touch-screen of a mobile device. Thedisplay 306 of thedevice 300 may electronically display information to a user via a graphical user interface (“GUI”) or other types of user interfaces. For example, as will be discussed below,display 306 may electronically display information corresponding to points of interest surrounding a user. - The sensors may include
image sensors 308, including those described with respect todevice 100. In some implementations, theimage sensors 308 may be provided on the edge of the device. The image sensors may be positioned such that when the device is in use, such as when a user is holding the device and viewing its display, a field of view of theimage sensors 308 includes objects surrounding the device. For example, the field of view may include objects in front of the user. With theimage sensors 308 on the edge, for instance, a top edge, of thedevice 300, theimage sensors 308 may be naturally pointing away from the user and in the direction of prominent features and/or objects when thedevice 300 is being held by the user. While three (3)image sensors 308 are shown, thedevice 300 may have any number ofimage sensors 308. Further, while theimage sensors 308 are depicted as being located on an edge ofdevice 300, theimage sensors 308 may be located elsewhere such as on the back of thedevice 300, thedisplay 306 side ofdevice 300, or as part of other image capturing mechanisms. Thus, the number and location ofimage sensors 308 shown inFIG. 3 is not meant to be limiting. - The user can enable or disable image sensing by the
image sensors 308, and the device may only detect the features and/or objects if the user has enabled this feature. According to some examples, the user may set theimage sensors 308 to automatically disable in particular locations, such as familiar locations. As another example, the user may define parameters that only enable image sensing in particular locations or settings, such as outdoors. - The
image sensors 308 may be any sensors capable of receiving imagery. Theimage sensors 308 may capture features and/or objects that are within the field of view of theimage sensors 308. Information regarding the captured features/objects may be used to determine further information, such as pose information, nearby points of interest, etc. - While three
image sensors 308 are illustrated inFIG. 3 , it should be understood that additional or fewer image sensors may be included. Moreover, a position of the image sensors along an edge or back of the device may be varied. As the number and position of the image sensors is varied, the field of view of the sensors may also vary. - The captured features and/or objects need not be displayed on the
display 306 ofdevice 300. According to some examples, the features and/or objects that are captured by theimage sensors 308 may never leave the firmware and, therefore, may not be saved as a picture or image todevice 300. - The sensors may further include GPS sensors. The GPS sensors may provide a rough indication as to the location of the device. The features captured by
image sensors 308 may be used to refine the location indicated by the GPS sensors or vice versa. According to some examples, the GPS data may be used to determine which part of the visual positioning system (“VPS”) index should be considered when determining the pose data. The VPS may indicate where different parts of the index correspond to different locations. - The sensors may additionally include accelerometers. The accelerometers may determine the pose or orientation of the device. Further, the sensors may include a gyroscope. The processors may receive gyroscope data and may process the gyroscope data in combination with the data collected from each of the other sensors to determine the orientation of the device. In some examples, the gyroscope data alone may be enough for the processors to determine the orientation of the device.
- The internal electronics may include, for example, one or more processors or other components adapted to processes the features or objects captured by the
image sensors 308. The captured features or objects may be processed to determine pose data. The pose data may be based on key or prominent features captured by theimage sensors 308. The detected features and/or objects may be used in combination with GPS readings and/or accelerometer or other sensor readings to determine the pose data for the device. Pose data may include the location of the device, such as the coordinates, and the orientation of the device, such as which direction theimage sensor 308 and, by extension, thedevice 300 and/or user is facing. - The pose data may be used to determine a plurality of points of interest within the area surrounding the device and, by extension, the user. The internal electronics may provide information regarding the points of interest to the user, for instance, on the
display 306 ofdevice 300. - The
output 310 may include one or more speakers for outputting audio, such as playback of music, speech, or other audio content. According to some embodiments, the output may also be thedisplay 306. -
FIG. 4 illustrates an example where the user is holding the device and the image sensors are capturing features and/or objects in the field of view of the image sensors. Thedevice 402 may be similar todevice 300 described herein. The user 404 may be a distance “D” away from a point ofinterest 410. The image sensors, while not shown, may be located at a top edge of thedevice 402. Thus, as the user 404 holdsdevice 402, the image sensor may capture features and/or objects that are in the field of view of the user 404. - The image sensors may capture, for instance, permanent and/or distinctive structures, such as bus stops, buildings, parks, etc. More distinctive features may be useful for characterizing the device's location/orientation and permanent features may be more likely to be represented in the VPS index that is generated based on previously captured images. Conversely, features that are captured by the image sensors that are less distinctive and/or more transient, such as people, sidewalks, cars, trees, and roads, may be disregarded for the purpose of determining the pose data. Less distinctive features may be less likely to assist in determining the location and orientation of the device and transient features are less likely to be represented in the VPS index. In some implementations, machine learning may be used to determine which features to make use of for determining pose data and which features to disregard. In some examples, the points of interest may be determined based on the orientation of the device as indicated by the pose data. For instance, the points of interest may be selected from points of interest that are within the particular range of the user device location and which are within a particular angular range or range or orientations that is based on the orientation of the device.
- Each of the image sensors may have a vertical field of view.
FIG. 4 illustrates a vertical field of view 406 for the image sensor located on the top edge of the device. The vertical field of view 406 may be defined by a predetermined angle 408. In some examples, the vertical field of view 406 may be determined by an aperture of the image sensor. As shown inFIG. 4 , the image sensor, based on the vertical field of view 406, may not capture the entire height of the point ofinterest 410. - Within the vertical field of view 406 may be a vertical field of view search angle. According to some examples, the vertical field of view search angle may dynamically change in size depending on whether the device is closer to or farther away from the features and/or objects the image sensors are pointing at. The vertical field of view search angle may increase when the distance D is smaller, such as when the image sensors are closer to the features and/or objects, in order to be able to capture more features and/or objects. The vertical field of view search angle may decrease when the distance D is larger, such as when the image sensors are farther away from the features and/or objects, in order to limit the amount of features and/or objects that will be captured. The vertical field of view search angle may be the same angle as angle 408. In some examples, the vertical field of view search angle may be more or less than angle 408.
-
FIG. 5 illustrates an example of a horizontal field of view of the device. Similar to the device shown inFIG. 4 , the image sensor may be located on a top edge ofdevice 502. The image sensor may have a horizontal field of view 506. The horizontal field of view 506 may be measured by apredetermined angle 508. Theangle 508 may be centered based on the orientation of thedevice 502. According to some examples, thepredetermined angle 508 may be 30 degrees. In some examples, theangle 508 may be more than 30 degrees or less than 30 degrees. This may serve to provide indication of points of interest that are within the field of view of the user. - Within the horizontal field of view 506 may be a horizontal point of interest search angle. The horizontal point of interest search angle may change in size based on the location of the device as compared to the features and/or objects of interest. For example, the horizontal point of interest search angle may dynamically change depending on whether the image sensors are closer to or farther away from the features and/or objects the image sensors are pointing at. The horizontal point of interest search angle may increase when the distance D is smaller, such as when the image sensors are closer to the features and/or objects, in order to be able to capture more features and/or objects. The horizontal point of interest search angle may decrease when the distance D is larger, such as when the image sensors are farther away from the features and/or objects, in order to limit the amount of features and/or objects that will be captured. The horizontal field of view search angle may be the same angle as
angle 508. In some examples, the horizontal field of view search angle may be more or less thanangle 508. - When the image sensors are capturing features and/or objects in the field of view 506, the internal electronics of the
device 502 may provide pose correction for natural pointing offsets. For example, when the image sensors are pointing in a direction of a point of interest, the point of interest may lay outside the field of view 506 due to user error, such as aiming inaccuracies. In some implementations, the search angle may be extended, for instance, beyond an angular range corresponding to the field of view of the image sensors. This may serve to compensate for the user's inaccuracy in pointing the image sensor at the point of interest. For example, the field of view 506 may increase the search angle by a predetermined amount, as shown by the increased angle 520. According to some examples, theangle 508 may increase by 10 degrees to angle 520. An increase of 10 degrees is merely an example. The search angle may increase from field ofview angle 508 may increase by 2 degrees, 5 degrees, 7.5 degrees, 20 degrees, etc. Therefore, in some examples, there may be a larger field ofview 518 for search purposes than the field ofview 516 of the image sensors. - As shown in
FIG. 5 , there may be a plurality of points ofinterest 510 within the field of view. The points ofinterest 510 may include a museum, a bar, cafe, or restaurant, a food stand, a store, a medical facility, landmark, or any other location. As illustrated inFIG. 5 , there may be a row of buildings 511-516, corresponding to a plurality of points of interest, next to each other. The field of view 506 of the image sensor may only capture buildings 513-515. When searching the area for locations of interest based on the features and/or objects captured by the image sensors and the pose data of the device, the field ofview 508 may increase such that the search area corresponds to searchangle 518, thereby capturing buildings 512-515. The processors ofdevice 502 may increase the search angle from field ofview 508 based on the distance between the device and the plurality of points of interest within the field ofview 508 to provide for pose correction for natural pointing offsets. -
FIGS. 6A and 6B illustrate examples of capturing and analyzing objects to determine the points of interest. As shown inFIG. 6A , the image sensors may capture features and/or objects in the field of view of the image sensors. The captured 600 buildings 602-604 may be the prominent features and/or objects in the field of view. As discussed herein, less distinctive features, such as sidewalks, cars, trees, and people, may by disregarded as they may be less likely to assist in determining the location and orientation of the device and, therefore, the points of interest. In some examples, the captured 600 features and/or objects may not be displayed to the user. The captured 600 features and/or objects may not be saved to the device. -
FIG. 6B illustrates an example of how the features and/or objects in the search angle within the field of view are analyzed. For example, the captured 600 buildings 602-604 may be analyzed to determine the amount of space that each of the buildings 602-604 take up in the field of view. In some examples, the device may determine each of the buildings 602-604 has a respective width 612-614 in the search angle within the field of view. The width may be considered when determining the points of interest captured by the image sensors. For example, the points of interest, i.e. buildings 602-604, may be considered based on their respective widths 612-614 instead of just a single point captured by the image sensor. UsingFIG. 6B as an example,width 613 of building 603 is completely within the search angle within the field of view of the image sensor and is the largest width captured by the image sensor. Building 604 has awidth 614 that is smaller thanwidth 613 but larger thanwidth 612 ofbuilding 602. The respective widths 612-614 may be used to determine a rank of the likely points of interest. For example, the point of interest that takes up the most width in the search angle within the field of view of the image sensor may indicate that that point of interest was what the image sensor was pointed at. -
FIG. 7 illustrates an example of the location of the device. The GPS sensors, along with the accelerometers, image sensors, and any other sensors, may determine the location of the device. For example, the sensors may determine that the device is at location X, corresponding to a certain position onmap 700. However, due to uncertainties in the sensors, the location of the device may be identified as location X and asurrounding radius 702. Theradius 702 may compensate for the uncertainty of the device's exact location and provide a greater search area when determining the plurality of points of interest. -
FIG. 8 illustrates an example of determining the pose data of the device. The pose data may include the location of the device and the orientation of the device. The sensors may determine the direction the image sensors are pointing. The image sensors may have a field ofview angle 802. For example, if the sensors determine that the device is at location X, as shown onmap 800, the features and/or objects detected by the images sensors, in combination with the GPS sensor readings and accelerometer readings, may determine the orientation of the device. For example, the accelerometer may determine whether the device has been rotated from a portrait position to a landscape position. The sensors may, therefore, determine that the image sensors are pointing in a certain direction, such as due East. -
FIG. 9 illustrates an example of a query based on the pose data of the device. One directional and/or multi-directional querying may be performed to determine the points of interest in the field of view of the image sensors. The system may perform either or both the one directional query and multi-directional query. - One directional query may be used when the device and, therefore, the image sensors are far away from the point of
interests 910, 911. The one directional query may use only the features and/or objects within the search angle within the image sensors' field ofview 902 to determine the plurality of points ofinterest 910, 911. For example, the device may be at location X, as shown onmap 900. The image sensors may be pointed in a direction due East. As shown, location X of the device is across the street from points ofinterest 910, 911. Thus, according to some examples, the device may be far enough away from points ofinterest 910, 911 to only use one directional querying based on the search angle within the field ofview 904 of the image sensors. - Multi-directional queries may be used when the device and, therefore, the image sensors are close to the points of interest. This approach may serve to compensate for the fact that the effect of inaccurate device/image sensor pointing is more pronounced the closer the points of interest are to the user. This may also compensate for the fact that, when close to the points of interest, not as many features and/or objects may be captured by the image sensors. The multi-directional query may be performed by searching the search angle within the field of view of the sensors, but also a radius surrounding the location of the device. For example, it may be determined that location X is too close to any particular points of interest to determine what point of interest to identify. The device may use multi-directional querying to search the
area 904 surrounding location X when determining the points of interest. The multi-directional query may identify points ofinterest 910, 911, which are in the search angle within the field ofview 902 of the image sensors. In some examples, the multi-directional query may also identify points ofinterest view 902 of the image sensors but may be, instead, behind the image sensors. - According to some examples, multi-directional querying comprises indicating points of interest that are within a first area defined by a first distance from the device location and a first angular range, in addition to points of interest that are within a second area defined by a second distance from the device location, that is greater than the first distance, and a second angular range that is narrower than the first angular range. According to one example, the first angular range may be between 120 and 360 degrees and the first distance may be between 1 and 10 meters. In contrast, the second angular range may be between 20 and 50 degrees and the second distance may be between 10 and 400 meters. In some examples, the first angular range may be between 90 and 300 degrees and the first distance may be between 0.5 and 8 meters; the second angular range may be between 12.5 and 45 degrees and the second distance may be between 8 and 350 meters. These examples are not meant to be limiting as the angular ranges and distances may be based on the number and position of the image sensors and the specifications of the image sensors.
- Based on the features and/or objects captured by the image sensor, the pose data of the device and the location of the device, a plurality of points of interest may be identified and provided to the user. According to some examples, the points of interest may be ranked and provided for display in order of relevance. For instance, after the plurality of points of interest within the particular geographical area has been determined, relevance weights for the points of interest may be calculated. The relevance weight may be used to rank the points of interest based on one or more criteria. In this regard, for example, the highest ranking points of interest may be provided as output by the device. The relevance weights may be determined based on one or both of a distance from the location indicated by the pose data and a type of the point of interest. In some examples, a rating associated with the point of interest may also be taken into account. For instance, the relevance weights may be calculated with a formula such as:
-
Weight=(Distance Group Factor*50)+(Type Factor*10)+(Rating Factor*2) - The distance group factor may be determined based on a predefined distance threshold between the device and the point of interest. For example, a point of interest that is between 0 m and 50 m from the device may have a distance group factor of 3. A point of interest that is between 50 m and 175 m from the device may have a distance group factor of 2. A point of interest that is more than 175 m away from the device may have a distance group factor of 1. The distances groups and, therefore, the factors provided are merely examples and thus can be defined by any ranges and any factor number. For example, a point of interest that is between 0 m and 100 m may have a distance group factor of 5 while a point of interest that is between 100 m and 160 m may have a distance group factor of 4. The distance thresholds, and therefore the distance group factor, may be defined based on the location of the device. For example, in a crowded city, the distance thresholds may decrease as points of interest may be closer together. In a rural area, the distance thresholds may increase as points of interest may be more spread apart. The distance factor may be determined using a linear clustering algorithm.
- The type factor may be determined based on the type of location or establishment. For example, the device may consider whether the point of interest is a museum, a bar, cafe, or restaurant, a food stand, a store, a medical facility, landmark, or any other location. The device may learn what types of places are most likely to be queried and, therefore, give those types of places a higher type factor. For example, a museum may return a type factor of 4; a car, café, or restaurant may return a type factor of 3.5; a place that serves food but is not a bar, café or restaurant may have a type factor of 3; a store may have a type factor of 2.5; a medical facility, such as a doctor's office or dentist's office, may have a type factor of 2; any other point of interest that does not fall within the predefined types may have a type factor of 1. A type factor of 1 may also be used as a default type factor. In some examples, the type factor may be determined based on the percentage of space the point of interest takes up in the field of view. Thus, a museum or department store may have a type factor of 4 and a café may have a type factor of 2. The type factor may be determined based on whether the point of interest is see-through, such as a bus stop, or solid, such as a building. The type factors may change as the system learns what types of places users are most likely to query. For example, the system may perform machine learning to continuously update and change the type factors for each of the points of interest.
- Points of interest that are within the same distance group and/or type group may be further ranked. For example, the relevance weight may include a further calculation with a formula such as:
-
Within Group Relative Distance=(1−Normalized Distance) - The normalized distance may be calculated using a formula such as:
-
Normalized Distance=Minimum(Distance to User, Search Range)/Search Range - In some examples, the Minimum may be a smallest value between the Distance to User and the Search Range. This may provide for consideration of the distance between objects while only provided a small weight to the distance.
- The Distance to User may be determined by the linear or absolute distance between the device and the identified point of interest. For example, the linear distance may be considered the distance “as the crow flies,” such that the distance does not follow the foot path or walking distance between the device and the point of interest. In some examples, the Distance to User may be determined by the distance the user would have to walk to arrive at the point of interest. For example, the user may have to follow traffic patterns and walk on sidewalks, and not through buildings.
- The Search Range may be determined based on a range preset by the device, the operative system, the internal electronics, etc. For example, the Search Range may be a 100 meter radius around the determined location of device. Thus, when performing a one direction or multi-directional query, the search may only include points of interest within a 100 meter radius. The 100 meters is merely one example. The Search Range may be smaller or larger. For example, in crowded places, like a city, the Search Range may be smaller to limit the number of points of interest returned by the search. In less crowded places, such as a rural location, the Search Range may be larger to increase the number of points of interest returned by the search. The Search Range may change without user input. For example, the Search Range may change based on the determined location of the device.
- The rating factor may be determined based on at least one user created rating of a place that is publically available. For example, the system may query internet review websites to determine a publicly available rating for each of the point of interest.
- In some examples, the relevance weights may be further calculated based on line of sight, the normal vector of the point of interest, and angular size. The line of sight may include what the image sensors captures. Thus, the relevance weight may give more weight to a point of interest in the line of sight of the image sensors, and therefore the user, than a point of interest that is barely visible or not visible at all in the line of sight. The normal vector of the point of interest may take into account the angular measurement from the normal of the point of interest. The normal is the line taken perpendicular to the façade of the point of interest. Thus, the relevance weight may consider the angle from normal. The larger the angle from normal, the image sensors may be less likely to be pointing directly at the point of interest. The smaller the angle from normal, the image sensors may be more likely to be pointing directly at the point of interest. The angular size may include the size of the feature and/or object in the field of view, measured as an angle. This may, in some examples, be used to calculate how much of the field of view the feature and/or object occupies.
- In yet other examples, the relevance weights may be further calculated based on a previous history of places the user has visited or ranked. A place that has been previously selected may be ranked higher. The relevance weights may be calculated based on points of interest having sales or specials. For example, if a department store is having a sale, the department store may be ranked higher than the big box store that is not having a sale. In some examples, points of interest that have an event that day, such as a theater or a concert venue, may be ranked higher based on the date and time of the vent. The calculation of the relevance weight may include the size of the façade of the point of interest in the field of view. For example, a façade that is larger may be more prominent in the field of view and, therefore, may be more relevant.
- As shown in
FIG. 10A , the information regarding the points of interest may be shown via the display on the device.Device 1000 may be similar todevice 300. For example,device 1000 may include aninput 1002, amicrophone 1004, adisplay 1006, one ormore image sensors 1008, and anoutput 1010. These features may be similar to those discussed herein with respect todevice 300. - The device may provide information regarding the points of interest to the user, for instance, by overlaying the information on the home screen, or
display 1006, of the device. The information may be provided in a variety of different ways, such as a list, a map, annotations to a map, etc. The information may be ordered based on the relevance weights, distance to the user, highest rated point of interest, visibility etc. According to other examples, the information may be output as audio data through one or more speakers of the device or accessories paired with the device. - According to some examples, points of interest may be promoted. The point of interest may be promoted if the owner or advertiser of the point of interest pays to have the point of interest promoted. The promoted point of interest may be indicated as the most relevant point of interest due to the promotion. For example, if a point of interest is being promoted, the point of interest may be indicated as “promoted” and appear as the most relevant point of interest. The promoted point of interest may be indicated as “sponsored” or “advertisement” to indicate that the promoted point of interest may not be the most relevant point of interest based on the calculated relevant weights.
- In some examples, the device may provide this information to the user with decreased or minimal user input. For example, the user may not have to open an application on the device (or even, in some examples, provide any active input) to run a search query to determine points of interest in the vicinity of the user. The device may, according to some examples, automatically (or based on a single user input) query the field of view of the image sensors to determine and provide information relating to the points of interest in the particular geographical area. Further, the device may provide more relevant results to the user based on the field of view of the image sensors and the relevance weight given to each of the plurality of points of interest.
-
FIG. 10A illustrates an example where the information regarding the points of interest is provided as a detailed list. Each of the points of interest may be provided as an individual place card 1020-1024, or button, on thedisplay 1006. For example, the more relevant point of interest may be a restaurant, shown asplace card 1020.Place card 1020 may include information about the restaurant, including the distance from the determined location of device and the publically available rating. Theplace card 1020 may be interactive, such as serving as an icon for user input to allow the user to find out more information regarding the restaurant. In some examples, theinteractive place card 1020 may allow for quick actions relevant to the point of interest. For example, the system may be integrated with services for the points of interest to allow for quick actions. For a restaurant, the quick action may allow a user to make a reservation. The second most relevant point of interest may be a coffee shop, shown asplace card 1021. Using the formula to calculate the relevance weight, the coffee shop may be the second most relevant due to size of the front façade, distance from the determined location of device, publically available rating, etc. A quick action forplace card 1021, the coffee shop, may be to place a mobile order for coffee. For example, the quick action may allow a user to order and pay for their order without ever entering the point of interest. The third most relevant point of interest may be a convenience store, shown asplace card 1022. The fourth most relevant point of interest may be a big box store, shown asplace card 1023. The fifth most relevant point of interest may be a movie theater, shown asplace card 1023. A quick action forplace card 1023, the movie theater, may be to order movie tickets. For example, the quick action may allow a user to order tickets and select the seats for the showing. There may be more relevant points of interest than those shown ondisplay 1006. For example,display 1006 may be touch activated such that user can provide a touch input to scroll through the list to see additional points of interest. - The order of which the places of interest appear may change as the system learns the choices of the users. The device may learn over time that users in that particular geographical area consistently choose a particular option and, therefore, the device may promote the particular option. For example, the relevance weight formula may rank the points of interest, in order from most relevant to least relevant, as: restaurant, coffee shop, convenience store, big box store, and movie theater. However, over time, the system may learn through machine learning that, when presented with the points of interest in that order, users may select the coffee shop more often than they select the restaurant. Thus, for future queries in that location, the system may present the list of points of interest with the coffee shop being the most relevant, even if the relevance weight of the coffee shop is less than the relevance weight of restaurant.
-
FIG. 10B illustrates an example where the information regarding the points of interest is provided as an annotated map. For example, each of the points of interest may be provided as a point A-F onmap 1030. Belowmap 1030, information regarding points A-F may be provided. For example, the name and/or type of point of interest may be shown in relationship to the points A-F on the map. In some examples, Point A may be considered the most relevant point of interest because it is listed first. Point A onmap 1030 may correspond to a restaurant, point B may correspond to a coffee shop, point C may correspond to a convenience store, point D may correspond to a big box store, point E may correspond to a movie theater, and point F may correspond to a café. - The user may be able to interact with
map 1030. For example, the user may be able to zoom in and/or out onmap 1030. In some examples, the user may be able to select one point A-F of the points of interest to obtain more information about that point of interest. For example, selecting one point A-F may cause information to appear ondisplay 1006, such as the distance of the point of interest to the determined location of device, the rating of the point of interest, etc. In some examples, the user may select a point A-F from the list belowmap 1030, which may cause information pertaining to that point of interest to appear ondisplay 1006. -
FIG. 10C illustrates another example where the information regarding the points of interest is provided on the display. As shown, astreet view image 1240 of the points of interest may be provided ondisplay 1006. Each of the points of interest may have a pin A-C identifying the point of interest. Below thestreet view image 1240 there may be a list of the points of interest corresponding to pins A-C. As in other examples, the user may be able to select points A-C from thestreet view image 1240 or from the list provided below thestreet view image 1240 to display more information regarding the selected point of interest. -
FIG. 11 illustrates a sequence of steps that may occur amongdevice 1102,server 1104,VPS 1106, anddatabase 1108. For example, inblock 1110 image data may be sensed by one or more image sensors of thedevice 1102. - In blocks 1112 and 1114, features and/or object may be detected in the image data. For example,
device 1102 may detect, or extract, features and/or objects from the image data. The detected features and/or objects from theimage data 1112 may be sent fromdevice 1102 toserver 1104 orVPS 1106. In examples where the detected features are sent fromdevice 1102 toserver 1104, the detected features and/or objects from theimage data 1114 may be sent fromserver 1104 toVPS 1106. In some examples,server 1104 orVPS 1106 may detect features and/or objects in theimage data - In
block 1118, theVPS 1106 may send the pose data of thedevice 1118 toserver 1104. The pose data of thedevice 1118 may be based on the detected features and/or objects from theimage data VPS 1106 may determine a location and orientation of the device based on at least the detected features and/or objects from theimage data - In
block 1116, the pose data of thedevice 1118 may be used to query a mapping database for a plurality of points of interest. - In
block 1118, the determined plurality of points of interest based onpose data 1116 may be sent fromdatabase 1108 toserver 1104. The plurality of points of interest may be located within a radius of the location of thedevice 1102. - In
block 1120, the plurality of points of interest may be filtered. For example, points of interest that do not fall within a certain angular range that is centered on the device orientation may be filtered out such that only points of interest within the certain angular range are returned. The certain angular range may be a point of interest search angle. The point of interest search angle may dynamically change based on the distance between the image sensors and the plurality of points of interest. In some examples, the point of interest search angle may be changed by the user. The point of interest search angle may be, for example, similar to the field of view for a user. - In some examples, the plurality of points of interest may be filtered at the
VPS 1106, theserver 1104, ordevice 1102. For example, thedevice 1102 may receive the plurality of points of interest within the radius and filter the plurality of points of interest based on the point of interest search angle. In another example, theserver 1104 may filter the plurality of points of interest based on the point of interest search angle and sent only the filtered plurality of points of interest to thedevice 1102. The filtered plurality of points ofinterest 1122 may be provided todevice 1102. - In
block 1124, information indication the plurality of points of interest is provided for display on thedevice 1102. -
FIG. 12 illustrates an example method for providing information regarding at least one point of interest in the geographical area of the device. For example, inblock 1210, the system may derive data from the one or more image sensors. - In
block 1220, the system may detect one or more features in a particular geographical area based on the data derived from the one or more image sensors. For example, the system may detect features and/or objects in the data derived from the one or more image sensors. The features and/or objects may include permanent and/or distinctive structures that may be useful in determining the device's location. The features and/or objects may be in the VPS index and, therefore, may have already been used to determine a location. - In
block 1230, the system may determine the pose data of the device based on the detected features and/or objects. The pose data may include the location and orientation of the device. To determine the pose data, the system may use the features and/or objects captured by the image sensors in combination with a variety of other sensors. The other sensors may include a GPS sensor, an accelerometer, a gyroscope, etc. The GPS sensor, in combination with the features and/or objects detected, may determine the location of the device. The accelerometer and/or gyroscope may be used to determine the orientation of the device. The orientation of the device may include the direction the image sensors are pointing and, therefore, the direction the user is facing. - In
block 1240, the system may determine points of interest within the particular geographical area based on the pose data. The geographical area may be set by a maximum distance away from the pose data, such as the location of the device. For example, the geographical area may be a 200 meter radius around the device. In some examples, the geographical area may be a three (3) block radius around the device. The determined points of interest may be locations that are visible to the image sensors and, therefore, the user. - In block 1250, the system may provide information indicating one or more of the points of interest. The system may provide this information to a display of the user device. For example, the system may provide a list, an annotated map, an annotated street view, etc. indicating the points of interest. In some examples, the system may provide audio feedback indicating the points of interest.
- The points of interest may be ranked based on relevance. For example, a location that is closer to the location of the device, has been chosen more often by others, has a higher publically accessible rating, etc. may be factors in determining the relevance of the point of interest. An algorithm may be used to determine the relevance. The system may rank the points of interest based on their calculated relevance weight.
- As will be appreciated, the technology described herein may enable a search for points of interest in the vicinity of the user in a manner that is less computationally intensive and requires fewer device resources than existing e.g. image matching-based techniques. The technology may also result in indications regarding points of interest in the user's vicinity being provided more quickly, with a less arduous and more intuitive user-device interaction.
Claims (20)
1. A system comprising:
one or more processors configured to:
detect, based on data derived from use of one or more sensors of a user device, one or more features captured by the one or more sensors;
determine, based on at least the one or more detected features, pose data including a location and orientation of the user device;
determine, based on the pose data, a plurality of points of interest within a particular geographical area that is determined based on the pose data; and
provide, via the user device, information indicating one or more of the plurality of points of interest, wherein the plurality of points of interest are determined by querying a mapping database for points of interest within the particular geographical area and within a particular range from the location.
2. The system of claim 1 , wherein the particular geographical area is within a predefined distance of the location of the user device included in the pose data.
3. The system of claim 1 , wherein the particular geographical area is within a predefined angular range that is based on the orientation of the user device included in the pose data.
4. The system of claim 3 , wherein:
at least one of the one or more sensors is an image sensor, and
the predefined angular range is wider than a field of view of the image sensor.
5. The system of claim 1 , wherein the particular geographical area comprises a first geographical area that is defined by a first distance from the location of the user device and a first angular range and a second geographical area that is defined by a second distance from the location of the user device and a second angular range that is based on the orientation of the user device, wherein the first angular range is wider than the second angular range and the first distance is less than the second distance.
6. The system of claim 1 , wherein the one or more sensors includes at least one of an image sensor, an accelerometer, a visual positioning system, or a location sensor.
7. The system of claim 1 , wherein the points of interest are filtered such that points of interest outside an angular range that is centered on the orientation of the user device are excluded.
8. The system of claim 1 , wherein:
at least one of the one or more sensors is an image sensor, and
the pose data is determined using a database of feature data generated based on a collection of street level images which depicts a geographic area that includes a sub-area in which the features captured by the image sensor of the user device are located.
9. A method comprising:
deriving, using one or more processors, data from one or more sensors of a device;
detecting, using the one or more processors and based on the derived data, one or more features in a particular geographical area;
determining, using the one or more processors and based on at least the one or more detected features, pose data including a location and orientation of the device;
determining, using the one or more processors and based on the pose data, a plurality of points of interest within the particular geographical area; and
providing, using the one or more processors, information indicating one or more of the plurality of points of interest in response to detecting features, wherein the plurality of points of interest are determined by querying a mapping database for points of interest within the particular geographical area and within a particular range from the location.
10. The method of claim 9 , wherein detecting the one or more features in the particular geographical area occurs without a prompt from a user at a time of the detecting.
11. The method of claim 9 , wherein when at least one of the one or more sensors is an image sensor, the method further comprises determining, using the one or more processors, the plurality of points of interest based on a field of view of the image sensor.
12. The method of claim 9 , further comprising computing, using the one or more processors, a relevance weight for each of the plurality of points of interest.
13. The method of claim 12 , further comprising providing, using the one or more processors and based on the computed relevance weights, a ranked list of the plurality of points of interest.
14. The method of claim 12 , wherein computing a relevance weight further includes using a distance group factor, a type factor, and a place rating factor.
15. The method of claim 14 , wherein the distance group factor is a value determined based on a predefined distance threshold between the device and each of the plurality of points of interest.
16. The method of claim 14 , wherein the type factor is a value based on a type of point of interest.
17. The method of claim 14 , wherein the place rating factor is a value based on at least one publicly available rating for each of the plurality of points of interest.
18. The method of claim 9 , wherein determining the one or more sensors includes on at least one of a an image sensor, a location sensor, a visual positioning system, or an accelerometer.
19. A non-transitory computer-readable medium storing instructions, which when executed by one or more processors, cause the one or more processors to:
derive data from one or more sensors of a user device;
detect one or more features in a particular geographical area based on the derived data;
determine pose data including a location and orientation of the user device based on at least the one or more detected features;
determine a plurality of points of interest within the particular geographical area based on the pose data; and
provide information indicating one or more of the plurality of points of interest in response to detecting features, wherein the plurality of points of interest are determined by querying a mapping database for points of interest within the particular geographical area and within a particular range from the location.
20. The non-transitory computer-readable medium of claim 19 , wherein detecting the one or more features in the particular geographical area occurs without a prompt from a user at a time of the detecting.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18/197,364 US20230360257A1 (en) | 2019-11-06 | 2023-05-15 | Use Of Image Sensors To Query Real World for Geo-Reference Information |
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/060061 WO2021091552A1 (en) | 2019-11-06 | 2019-11-06 | Use of image sensors to query real world for geo-reference information |
US202016757638A | 2020-04-20 | 2020-04-20 | |
US17/527,508 US11688096B2 (en) | 2019-11-06 | 2021-11-16 | Use of image sensors to query real world for geo-reference information |
US18/197,364 US20230360257A1 (en) | 2019-11-06 | 2023-05-15 | Use Of Image Sensors To Query Real World for Geo-Reference Information |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/527,508 Continuation US11688096B2 (en) | 2019-11-06 | 2021-11-16 | Use of image sensors to query real world for geo-reference information |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230360257A1 true US20230360257A1 (en) | 2023-11-09 |
Family
ID=69167894
Family Applications (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/757,638 Active 2039-12-26 US11232587B2 (en) | 2019-11-06 | 2019-11-06 | Use of image sensors to query real world for geo-reference information |
US17/527,508 Active US11688096B2 (en) | 2019-11-06 | 2021-11-16 | Use of image sensors to query real world for geo-reference information |
US18/197,364 Pending US20230360257A1 (en) | 2019-11-06 | 2023-05-15 | Use Of Image Sensors To Query Real World for Geo-Reference Information |
Family Applications Before (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/757,638 Active 2039-12-26 US11232587B2 (en) | 2019-11-06 | 2019-11-06 | Use of image sensors to query real world for geo-reference information |
US17/527,508 Active US11688096B2 (en) | 2019-11-06 | 2021-11-16 | Use of image sensors to query real world for geo-reference information |
Country Status (6)
Country | Link |
---|---|
US (3) | US11232587B2 (en) |
EP (2) | EP3844644B1 (en) |
JP (2) | JP7155293B2 (en) |
KR (2) | KR102514000B1 (en) |
CN (1) | CN113079703A (en) |
WO (1) | WO2021091552A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR102514000B1 (en) * | 2019-11-06 | 2023-03-24 | 구글 엘엘씨 | Use of image sensors to query the real world for geo-referenced information |
CN117131249B (en) * | 2023-10-26 | 2024-01-12 | 湖南省不动产登记中心 | Intelligent management method and system for natural resource data |
Family Cites Families (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8174568B2 (en) * | 2006-12-01 | 2012-05-08 | Sri International | Unified framework for precise vision-aided navigation |
US8189964B2 (en) * | 2009-12-07 | 2012-05-29 | Google Inc. | Matching an approximately located query image against a reference image set |
US20120249797A1 (en) * | 2010-02-28 | 2012-10-04 | Osterhout Group, Inc. | Head-worn adaptive display |
US8890896B1 (en) * | 2010-11-02 | 2014-11-18 | Google Inc. | Image recognition in an augmented reality application |
US9699375B2 (en) * | 2013-04-05 | 2017-07-04 | Nokia Technology Oy | Method and apparatus for determining camera location information and/or camera pose information according to a global coordinate system |
EP3742405A3 (en) * | 2013-09-24 | 2021-01-13 | Apple Inc. | Method for representing points of interest in a view of a real environment on a mobile device and mobile device therefor |
US9432421B1 (en) | 2014-03-28 | 2016-08-30 | A9.Com, Inc. | Sharing links in an augmented reality environment |
KR20190093624A (en) * | 2016-12-06 | 2019-08-09 | 돈 엠. 구룰 | System and method for chronological-based search engines |
US10417781B1 (en) * | 2016-12-30 | 2019-09-17 | X Development Llc | Automated data capture |
KR102514000B1 (en) * | 2019-11-06 | 2023-03-24 | 구글 엘엘씨 | Use of image sensors to query the real world for geo-referenced information |
-
2019
- 2019-11-06 KR KR1020207031921A patent/KR102514000B1/en active IP Right Grant
- 2019-11-06 WO PCT/US2019/060061 patent/WO2021091552A1/en unknown
- 2019-11-06 EP EP19836863.1A patent/EP3844644B1/en active Active
- 2019-11-06 EP EP23164549.0A patent/EP4220439A1/en active Pending
- 2019-11-06 CN CN201980029390.6A patent/CN113079703A/en active Pending
- 2019-11-06 KR KR1020237009733A patent/KR20230042423A/en not_active Application Discontinuation
- 2019-11-06 JP JP2020562140A patent/JP7155293B2/en active Active
- 2019-11-06 US US16/757,638 patent/US11232587B2/en active Active
-
2021
- 2021-11-16 US US17/527,508 patent/US11688096B2/en active Active
-
2022
- 2022-10-04 JP JP2022160352A patent/JP2022176297A/en active Pending
-
2023
- 2023-05-15 US US18/197,364 patent/US20230360257A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
JP7155293B2 (en) | 2022-10-18 |
WO2021091552A1 (en) | 2021-05-14 |
CN113079703A (en) | 2021-07-06 |
JP2022176297A (en) | 2022-11-25 |
US11232587B2 (en) | 2022-01-25 |
EP4220439A1 (en) | 2023-08-02 |
US20220076443A1 (en) | 2022-03-10 |
KR20210056946A (en) | 2021-05-20 |
KR102514000B1 (en) | 2023-03-24 |
EP3844644A1 (en) | 2021-07-07 |
US20210134003A1 (en) | 2021-05-06 |
EP3844644B1 (en) | 2023-05-31 |
KR20230042423A (en) | 2023-03-28 |
US11688096B2 (en) | 2023-06-27 |
JP2022510050A (en) | 2022-01-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20230360257A1 (en) | Use Of Image Sensors To Query Real World for Geo-Reference Information | |
US8943049B2 (en) | Augmentation of place ranking using 3D model activity in an area | |
US8938091B1 (en) | System and method of using images to determine correspondence between locations | |
US9874454B2 (en) | Community-based data for mapping systems | |
KR101002030B1 (en) | Method, terminal and computer-readable recording medium for providing augmented reality by using image inputted through camera and information associated with the image | |
US9418482B1 (en) | Discovering visited travel destinations from a set of digital images | |
US9460160B1 (en) | System and method for selecting user generated content related to a point of interest | |
JP5652097B2 (en) | Image processing apparatus, program, and image processing method | |
US9171011B1 (en) | Building search by contents | |
US8532916B1 (en) | Switching between best views of a place | |
KR20120027346A (en) | System and method of searching based on orientation | |
US20150278878A1 (en) | System and method of displaying advertisements | |
US8341156B1 (en) | System and method for identifying erroneous business listings | |
US20130328931A1 (en) | System and Method for Mobile Identification of Real Property by Geospatial Analysis | |
KR101519879B1 (en) | Apparatus for recommanding contents using hierachical context model and method thereof | |
US20160307370A1 (en) | Three dimensional navigation among photos | |
US9888356B2 (en) | Logistic discounting of point of interest relevance based on map viewport | |
JP5951465B2 (en) | Program, information terminal, server and method for determining line-of-sight area based on road information | |
JP6019680B2 (en) | Display device, display method, and display program |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HINCAPIE, JUAN DAVID;LE, ANDRE;REEL/FRAME:064446/0258Effective date: 20191108 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |