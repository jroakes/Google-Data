CN112313643A - Predicting potentially relevant subject matter based on retrieved/created digital media files - Google Patents
Predicting potentially relevant subject matter based on retrieved/created digital media files Download PDFInfo
- Publication number
- CN112313643A CN112313643A CN201980038861.XA CN201980038861A CN112313643A CN 112313643 A CN112313643 A CN 112313643A CN 201980038861 A CN201980038861 A CN 201980038861A CN 112313643 A CN112313643 A CN 112313643A
- Authority
- CN
- China
- Prior art keywords
- user
- digital media
- objects
- media files
- digital
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000010801 machine learning Methods 0.000 claims abstract description 12
- 238000000034 method Methods 0.000 claims description 64
- 230000015654 memory Effects 0.000 claims description 12
- 230000004044 response Effects 0.000 claims description 5
- 239000013598 vector Substances 0.000 claims description 3
- 238000013527 convolutional neural network Methods 0.000 description 8
- 235000015041 whisky Nutrition 0.000 description 8
- 241000271566 Aves Species 0.000 description 7
- 230000000007 visual effect Effects 0.000 description 7
- 239000000463 material Substances 0.000 description 5
- 241000272517 Anseriformes Species 0.000 description 3
- 239000003795 chemical substances by application Substances 0.000 description 3
- 238000004891 communication Methods 0.000 description 3
- 238000010411 cooking Methods 0.000 description 3
- 238000001514 detection method Methods 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 235000013305 food Nutrition 0.000 description 3
- 230000002452 interceptive effect Effects 0.000 description 3
- 238000012549 training Methods 0.000 description 3
- 241000220225 Malus Species 0.000 description 2
- 238000013459 approach Methods 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 238000011960 computer-aided design Methods 0.000 description 2
- 235000013399 edible fruits Nutrition 0.000 description 2
- 235000012054 meals Nutrition 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 238000012015 optical character recognition Methods 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000008569 process Effects 0.000 description 2
- 238000012545 processing Methods 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 241000894007 species Species 0.000 description 2
- 230000001960 triggered effect Effects 0.000 description 2
- 241000282472 Canis lupus familiaris Species 0.000 description 1
- 241000282693 Cercopithecidae Species 0.000 description 1
- 241000282326 Felis catus Species 0.000 description 1
- 241000581835 Monodora junodii Species 0.000 description 1
- 240000008790 Musa x paradisiaca Species 0.000 description 1
- 235000018290 Musa x paradisiaca Nutrition 0.000 description 1
- 230000009471 action Effects 0.000 description 1
- 235000021016 apples Nutrition 0.000 description 1
- 238000013475 authorization Methods 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000009194 climbing Effects 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 239000003814 drug Substances 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000036651 mood Effects 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 235000020092 scotch whiskey Nutrition 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 238000010200 validation analysis Methods 0.000 description 1
- 235000014101 wine Nutrition 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5838—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using colour
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/907—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/908—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/10—File systems; File servers
- G06F16/14—Details of searching files based on file metadata
- G06F16/144—Query formulation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5854—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using shape and object relationship
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7837—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
Abstract
Embodiments are described herein for predicting/determining topics potentially relevant to a user using digital media files retrieved and/or created by the user. In various implementations, the digital media file(s) created and/or retrieved by a user with a client device may be applied as input in a trained machine learning model, in some cases local to the client device, to generate output indicative of objects detected in the digital media file. Data indicative of the indicated object may be provided to the remote computing system without providing the digital media file itself. In some implementations, information associated with the indicated object can be retrieved and proactively output to the user. In some implementations, the frequency with which objects appear in a corpus of digital media files may be considered when determining the likelihood that a detected object is potentially relevant to a user.
Description
Background
Information useful for controlling data sent to a device can be determined from a variety of different sources, such as documents (particularly cloud-based documents), calendars, explicit user preferences, and so forth. However, it may be desirable to control access to the source of the information.
Disclosure of Invention
The present disclosure relates generally to methods, apparatus, and computer-readable media (transitory and non-transitory) for utilizing digital media files retrieved and created by users. In various embodiments, digital media files created/retrieved by a user, such as digital photographs, digital videos, digital audio files, and the like, may be analyzed using various techniques to detect one or more objects represented in the digital media files. An "object" may include any person, place, or thing. Some unique objects, such as particular persons (e.g., celebrities), unique objects (e.g., notorious objects, such as hopefully boring), etc., may alternatively be referred to herein and/or elsewhere as "entities," particularly in the context of knowledge maps. Most objects, such as alternative objects, may have types or classes, and/or may be members of various hierarchies (e.g., Australian green apple → fruit → factory → life style). Based on these detected objects, either alone or in combination with other user interest signals, potentially relevant topics (e.g., user interests) may be determined and used to present relevant information to the user.
Various techniques may be implemented locally on a client device operated by a user to detect objects represented in a digital media file. Data indicative of these detected objects may be provided by the client device to a remote computing server (e.g., the cloud) without providing the underlying digital media file, such that the remote computing system may determine whether these detected objects demonstrate a topic that is likely to be relevant to the user. For example, in some implementations, one or more trained machine learning models may be stored locally on the client device. Data indicative of digital media created/captured at the client device may be applied (locally at the client device) as an input application of the trained machine learning model) to generate an output indicative of one or more objects. For example, such output may be used to actively present content potentially relevant to the user at one or more client devices, as described above.
The techniques described herein yield various technical advantages. As an example, employing a local on-device model to detect objects in digital media files created and/or retrieved by a user enables a remote server to use information associated with the user's digital media files without exposing the user's digital media files (particularly digital media files created by the user that may be considered highly private) to the cloud. Further, using a local on-device model, data is enabled to be sent by the client device for providing information related to potentially related topics, wherein the data consumes less network bandwidth than if the digital media file were sent instead. This may be beneficial, for example, where the client device utilizes a limited bandwidth network connection and may enable the relevant information to be provided to the client device more quickly (e.g., without having to wait for the client device to utilize a higher bandwidth network connection).
As another example, identifying topics that are potentially relevant to a user, and using these topics to obtain/generate/present content for presentation to the user, may reduce the input required by these users to reach their desired content. For example, ranking search results based on topics identified using the techniques described herein may allow users to find what they want to look for faster with less input, thereby saving computing resources such as network bandwidth, memory, battery power, and so forth. To this end, in various embodiments, topics potentially relevant to the user may be persisted in memory in association with the user (e.g., by association with the user's profile), for example, at one or more client devices operated by the user and/or in the cloud for future use. As another example, digital media files created by a user, or at least edited by a user, may be more indicative of the user's interests than digital media files that are only passively retrieved (e.g., viewed, played back, etc.) by the user, files that are shared with other users, or other signals, such as a search history (which may indicate past interests, rather than more core interests). Thus, the techniques described herein may identify topics that are highly likely to be relevant to the user, thereby improving the user experience.
In one aspect, a method implemented by one or more processors may comprise: obtaining one or more digital media files created by a user using a client device; applying the one or more digital media files as input to one or more trained machine learning models local to the client device, wherein the one or more trained machine learning models generate an output indicative of the one or more objects detected in each of the one or more digital media files; providing data indicative of the indicated one or more objects to a remote computing system without providing one or more digital media files; receiving information associated with the indicated one or more objects from the same remote computing system or a different remote computing system; and actively outputting, to the user, an information output associated with the indicated one or more objects at the same client device operated by the user or a different client device.
In various implementations, the providing may cause the remote computing system to formulate a search query based on the identified one or more objects, and the information associated with the indicated one or more objects includes information responsive to the formulated search query.
In various embodiments, the method may further comprise: receiving a search query from a remote computing system; and issuing the search query to receive information responsive to the search query in response to the issuing. In various implementations, the remote computing system may use the formulated search query itself to obtain information responsive to the formulated search query.
In various implementations, the remote computing system may determine one or more topics potentially relevant to the user based on the data indicative of the indicated one or more objects, and the information associated with the indicated one or more objects may include information associated with one or more interests in the database.
In another aspect, there may be provided a computer-implemented method comprising: receiving data indicative of one or more digital media files from one or more client devices operated by a user; identifying one or more objects detected in each of the one or more digital media files based on the data indicative of the one or more digital media files; determining a frequency of each of one or more objects in a corpus of digital media files provided by a community of users; selecting one or more of the objects based on the corresponding frequencies; identifying one or more topics potentially relevant to the user based on the selected one or more objects; formulating a search query based on the identified one or more topics; and proactively causing information responsive to the formulated search query to be output at one or more of the client devices operated by the user.
In various embodiments, the data indicative of the one or more digital media files may include one or more digital images, and identifying the one or more objects may include: image recognition is performed on the one or more digital images to identify one or more objects. In various implementations, the data indicative of the one or more digital media files may include one or more audio files, and identifying the one or more objects may include: audio analysis is performed on the one or more audio files to identify one or more objects.
In various implementations, the data indicative of the one or more digital media files may include one or more feature vectors or dimension reduction embeddings extracted from the one or more digital media files. In various implementations, the selection may be further based on a location of the one or more objects in the one or more digital media files.
In various implementations, the selection may be based further on a measure of the focal distance of one or more objects in the one or more digital media files. In various implementations, the selection may be further based on location coordinates associated with one or more digital media files received from one or more client devices. In various implementations, the selection may be further based on whether the user created or retrieved one or more digital media files received from one or more client devices.
Embodiments may utilize digital media files retrieved and/or created by a user to predict/determine topics that are potentially relevant to the user (e.g., their interests or interests of others proximate to them, user preferences, etc.). In various embodiments, information and/or data related to these topics may then be proactively provided to the user. In one aspect, objects detected in digital media files created/retrieved by a user may be utilized to varying degrees to identify topics that are potentially relevant to the user. For example, background and/or incidental objects (such as furniture, trees, landscapes, etc.) detected in a user's digital photos (and/or digital videos) that are common among a corpus of digital photos taken by a community of users may not be highly indicative of the user's interests. In contrast, less common objects that are less commonly encountered in a corpus of digital photographs may be more highly indicative of the user's interests. This is especially true if these less common objects are detected in a plurality of digital photographs taken by the user. For example, if a wayside whisky bottle is not particularly prevalent in a pre-stock library of digital photographs taken by a community of users, the fact that the wayside whisky is characterized in a digital photograph of a particular user may indicate the user's interest in the wayside whisky or at least the interest in the wayside whisky. Alternatively, this fact may indicate the user's interest in decorative or otherwise unusual bottles. And may indicate general interest in a bottle if there are multiple photographs of different types of bottles (e.g., a bourdon whisky, scotch whisky, wine, medicine, etc.). As another example, if one or more wayside whisky bottles appear in a user's photograph, but the user also views and/or creates a picture of the winery, e.g., while the winery is traveling, and/or views and/or creates a picture of a wayside whisky label, these may collectively be a strong signal of general interest in the wayside whisky.
Once topics that are potentially relevant to a user have been identified using the techniques described herein, these identified topics may be used for a variety of purposes. In some implementations, information associated with identified topics may be obtained and actively presented or "pushed" to a user, e.g., as a notification "card" or "chartlet" presented on a touchscreen of a smartphone, as output from an automated assistant for participating in a man-machine conversation conference, incoming text messages, etc. therewith. In some embodiments, topics potentially relevant to a user may be used to formulate search queries, for example, by a client device operated by the user or by a remote computing server forming a so-called "cloud". Information responsive to the search query may be obtained and pushed to the user. Additionally or alternatively, in some implementations, upon determining that the topic is potentially relevant to the user, one or more user topics may be associated in the database with various predetermined content (e.g., presentation content) that is automatically presented to the user.
Embodiments such as these may enable identification of topics that are potentially relevant to the user, which the user may not know about themselves. As an example, assume that a father has acquired many digital photos and/or digital videos of his family. Assume further that one of his children is always drawn in these data media files that interact with objects of a particular type. For example, in one photo his son is playing a toy motorcycle, in another photo his son is wearing a shirt depicting a motorcycle, in another example his son is putting a figure on a tertiary motorcycle, etc. Even if for some reason the father is not aware of his son's interest in motorcycles, the techniques described herein may be able to identify the topic "motorcycle" as relevant to the father because the topic is of interest to the son (even though the father himself is clearly not interested in motorcycles, his son's interest in motorcycles may at least relate this topic to the father.) thus, information related to motorcycles may be pushed to the father. For example, as the child's birthday approaches, information about the child's potentially motorcycle-related gift may be presented to the father.
Other embodiments may include a non-transitory computer-readable storage medium having stored thereon instructions executable by a processor to perform a method, such as one or more of the methods described above. Yet another embodiment may include a system comprising a memory and one or more processors operable to execute instructions stored in the memory to perform a method, such as one or more of the methods described above.
It should be understood that all combinations of the above concepts and additional concepts described in greater detail herein are considered a part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
FIG. 1 illustrates an example embodiment in which selected aspects of the present disclosure may be implemented.
Fig. 2, 3, and 4 depict example scenarios in which the techniques described herein may be employed, according to various embodiments.
Fig. 5 schematically illustrates an example of how content may be proactively output after performing the techniques described herein.
FIG. 6 depicts a flowchart that illustrates an example method of performing selected aspects of the present disclosure.
FIG. 7 depicts another flowchart illustrating an example method of performing selected aspects of the present disclosure.
FIG. 8 schematically depicts an example architecture of a computer system.
Detailed Description
FIG. 1 illustrates an example environment in which information may be determined by a user creating and/or retrieving (e.g., consuming) a digital media file. The example environment includes a client device 106 and a cloud-based system 102. Cloud-based system 102 may be implemented in one or more computers that communicate, for example, over a network (not shown). Cloud-based system 102 may be an example of a system in which the systems, components, and techniques described herein may be implemented and/or with which the systems, components, and techniques described herein may interface. Although described to a large extent as being implemented on a "cloud-based system" herein, the disclosed techniques may actually be performed on systems used for a variety of purposes, such as email systems, text messaging systems, social networking systems, voicemail systems, productivity systems, enterprise software, search engines, and so forth.
A user may interact with cloud-based system 102 via client device 106. Other computer devices may communicate with cloud-based system 102, including but not limited to additional client devices and/or one or more servers implementing services for websites that have collaborated with the provider of cloud-based system 102. However, for the sake of brevity, examples are described in the context of the client device 106.
The client device 106 may be a computer that communicates with the cloud-based system 102 over a network, such as a Local Area Network (LAN) or a Wide Area Network (WAN), such as the internet (one or more such networks indicated generally at 112). For example, the client device 106 may be a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a standalone interactive speaker operating a voice interactive personal digital assistant (also referred to as an "automated assistant"), or a wearable apparatus of a user that includes a computing device (e.g., a watch of a user with a computing device, glasses of a user with a computing device, a wearable music player). Additional and/or alternative client devices may be provided.
The client device 106 may include various software and/or hardware components. For example, in fig. 1, the client device 106 includes a vision sensor 107 and a microphone 108. The vision sensor 107 may take various forms, such as a digital camera, a passive infrared ("PIR" sensor), an RGBd sensor, a stereo camera, and so forth. The vision sensor 106 may be operated, for example, by a user (not shown) to capture one or more digital images, for example, as individual digital images and/or as a sequence of digital image frames forming a digital video. The microphone 108 may be operated by a user, for example, to capture various sounds, such as utterances of the user. In some scenarios, the visual sensor 107 and microphone 108 may be used together, for example, to capture multimedia audio/visual digital media files, such as video recordings.
The client device 106 may also execute various software. For example, in the embodiment shown in fig. 1, the client device 106 executes a media client 109 and an object recognition client 110. In various implementations, the media client 109 may be operated, for example, by a user to create and/or retrieve (e.g., for consumption) digital media files. As used herein, "digital media files" may include digital images (e.g., photographs), digital videos (with or without sound), computer-aided design ("CAD") files, audio files (e.g., songs, lessons, etc.), and so forth. In some implementations, the client device 106 may execute multiple media clients 109, such as for multiple different types of digital media files. However, for the sake of brevity and simplicity, the embodiments referred to herein will be described as having a single media client 109 installed. In some implementations, digital media files created using the media client 109 can be uploaded (e.g., backed up, transferred) to the cloud-based system 102, for example, for secure and/or reliable storage in one or more cloud storage databases 122 by the online storage engine 120. Additionally or alternatively, online storage engine 120 may form part of a social network such that digital media files uploaded or shared by users on their social network accounts may be analyzed using the techniques described herein.
The object recognition client 110 may be configured to recognize objects represented in digital media files retrieved and/or created using the media client 109. In some implementations, all object recognition may be performed at the cloud-based system 102, in which case the object recognition client 110 cannot be installed on the client device 106. However, in some implementations, the object recognition client 110 may be installed on the client device 106 to protect the privacy of the user. For example, in some embodiments, the object recognition client 110 may have access to an "on-device" object recognition model database 111 (the terms "index" and "database" are used interchangeably herein). The on-device model database 11 may store one or more object identification modules and/or models that may be used, for example, by the object identification client 110 to detect objects represented in digital media files and/or to provide output indicative of one or more detected objects. Thus, data generated by these models, rather than raw data of the digital media file in which the object was detected, may be provided to the cloud-based system 102. An example of how object recognition may be performed will be described below.
In some implementations, the client device 106 and the cloud-based system 102 each include a memory for storing data and software applications, a processor for accessing data and executing applications, and components that facilitate communication over the network 112. Operations performed by client device 106 and/or cloud-based system 102 may be distributed across multiple computer systems. For example, cloud-based system 102 may be implemented as a computer program running on one or more computers in one or more locations coupled to each other by a network.
In various implementations, the cloud-based system 102 can include an online storage engine 120, an object recognition engine 124, an object frequency engine 128, a topic identification engine 132, and/or a knowledge graph engine 136. In some implementations, one or more of the engines 120, 124, 128, 132, and/or 136 may be combined and/or omitted. In some implementations, one or more of the engines 120, 124, 128, 132, and/or 136 may be implemented in a component separate from the cloud-based system 102. In some implementations, one or more of the engines 120, 124, 128, 132, and/or 136, or any operable portion thereof, may be implemented in a component executed by the client device 106.
In this specification, the terms "database" and "index" are used interchangeably and refer broadly to any collection of data. The database and/or indexed data need not be structured in any particular way, and it may be stored on storage in one or more geographic locations. Thus, for example, the indexes 111, 122, 126, 130, 134, 138 may include multiple data sets, each of which may be organized and accessed differently.
Turning now to the cloud-based system 102, in various embodiments, the online storage engine 120 may be configured to store various types of digital content (such as digital media files) of a user in cloud storage 122, which cloud storage 122 is commonly referred to as "cloud storage. For example, a user may have an online profile that synchronizes data among multiple client devices 106 operated by the user. Such synchronization may be facilitated by online storage engine 120, for example, by storing copies of all of the user's digital content to be synchronized in cloud storage 122. A user may intentionally cause online storage engine 120 to store digital content in cloud storage 122, for example, by directly uploading such content and/or by storing such content in a storage folder on one or more client devices 106 designated for cloud storage. Additionally or alternatively, in some implementations, the user's various digital content may be automatically stored (e.g., by the online storage engine 120) in the index 122 as part of an automatic backup process. For example, some cloud computing service providers offer their users free or fee-based automated backup of digital photos, videos, or other digital media files.
Like object recognition client 110, object recognition engine 124 may be configured to identify objects represented in digital media files. However, the object recognition client 110 operates on a resource-constrained client device 106, and thus may have limited capabilities. For example, in some embodiments, a limited number of on-device object recognition models may be stored in on-device model index 111. In contrast, object recognition engine 124 may be able to utilize the nearly unlimited resources of the cloud to perform object recognition, and thus may be far more capable, robust, etc., than object recognition client 110. For example, object recognition engine 124 may have access to an index 126 of object recognition modules and/or models that is much larger than the on-device index 111 available to object recognition client 110. In some implementations, if the object recognition client 110 cannot detect/recognize one or more objects in the digital media file, or at least cannot recognize them with sufficient confidence, the digital media file (or some change in data indicative thereof) may be provided to the object recognition engine 124 to perform object recognition analysis.
Object recognition and/or detection may be performed using a variety of different techniques, for example, by object recognition client 110 and/or object recognition engine 124. In some implementations, the on-device model index 111 and/or the cloud-based model index 126 can include object models (e.g., based on computer-aided design or "CAD") that can be used and/or triggered to provide inferences about object type, e.g., using visual and/or depth data obtained through one or more visual sensors (e.g., 107). Additionally or alternatively, the on-device model index 111 and/or the cloud-based model index 126 may include 2D patterns or profiles of objects that may match portions of 2D image data (e.g., video frames) captured by one or more vision sensors (e.g., 107). In other implementations, on-device model index 111 and/or cloud-based model index 126 may include routines (e.g., state machines) that may be implemented/triggered by object recognition client 110 and/or object recognition engine 124 to provide inferences about object types. In some implementations, the object recognition client 110 and/or the object recognition engine 124 can detect textual content in the digital image, e.g., by using optical character recognition ("OCR") to recognize text, e.g., contained in a background of the image (e.g., on a garment depicted in the image), and so forth.
In some implementations, one or more machine learning models can be trained to generate output indicative of one or more objects detected in a digital media file. For example, one or more convolutional neural networks ("CNNs") may be trained to generate an output based on a digital image file (personal photo or video frame) that is indicative of one or more objects detected in the digital media file, and in some cases, the confidence associated with those detected objects.
As a non-limiting example, the CNN may be trained using a corpus of digital images, each labeled with one or more objects represented in the digital images. Each training example may be used as an input to CNN to generate an output. The output may then be compared to the labels associated with the training examples to determine a difference or error. Based on this error, various techniques may be employed to train the CNN, such as back propagation, gradient descent, and so forth. Once trained (e.g., using hundreds or thousands of training examples), the unlabeled digital image may be used as an input to the CNN to generate an output indicative of one or more objects detected in the digital image, and in some cases, a confidence metric of the output.
In addition to identifying objects in digital images (whether from personal digital photographs or from digital video frames), in some embodiments, object recognition client 110 and/or object recognition engine 124 may be configured to detect objects (or more generally, subjects) in digital audio content (whether separate digital video or accompanying digital video). For example, in some implementations, the object recognition client 110 and/or the object recognition engine 124 may employ techniques such as speech recognition to detect mentions of objects in the audio data. In some implementations, the on-device model index 111 and/or the cloud-based model index 126 can store one or more models, such as one or more machine learning models, that are trained to detect mentions of various objects in the audio data. Additionally or alternatively, in some implementations, the object recognition client 110 and/or the object recognition engine 124 may employ speech-to-text processing to generate speech recognition output (i.e., text) from the audio data, and then analyze the text using techniques such as natural language processing, entity tagging, and the like to identify the objects mentioned, or more generally, to identify topics of potential interest to the user.
The topic identification engine 132 can be configured to identify one or more topics that are potentially relevant to one or more users based on various signals. In some implementations, these topics can be stored in the user interest index 134, e.g., associated with applicable users, so that they can be later used for various purposes, including but not limited to caching information on the client device (e.g., when the client device is connected to a high bandwidth data connection), ranking search results, generating alternative query suggestions, selecting and presenting digital content, recommending media/restaurants, and so forth. These signals may take a variety of different forms and may come from a variety of different sources. In some implementations, the topic identification engine 132 can identify topics that are potentially of interest to or relevant to the user based on analysis performed by the object recognition client 110 and/or the object recognition engine 124 on digital media files created and/or retrieved by the user. For example, assume that the object recognition engine 124 detects multiple representations of a classic car in multiple digital photographs captured by the user. These detections may be used, for example, by topic identification engine 132 to determine that the user may be potentially interested in classic cars, even if the user does not explicitly express an interest in classic cars.
In addition to the fact that the exposure of an object detected in a digital media file is undoubted, the topic identification engine 132 may also consider various other attributes of the digital media file as well as the role or prominence of the detected object in the file. In some implementations, the prominence of the detected object in the digital image may be determined (e.g., at 110/124) and used to weight the potential of the object as a topic related to the user. For example, the location of the detected object in the image, the size of the detected object relative to the entire image, the size of the detected object relative to other objects detected in the image, a measure of the focus of the object relative to other detected objects (e.g., how blurred the object is compared to other detected objects), the number of objects of a particular type in one or more images (e.g., three apples depicted in a picture may be more likely to be relevant than one apple), and other similar data points may be provided, for example, by the object recognition client 110 and/or the object recognition engine 124, and considered in determining the measure of the prominence of the detected object (e.g., by the topic identification engine 132). In some implementations, the measure of prominence of a particular detected object may be compared (e.g., by topic identification engine 132) to one or more thresholds and/or one or more other measures of prominence associated with other detected objects to determine whether (or to what extent) the object (and more generally the topic determined from the object) should be considered potentially relevant to the user.
For example, if a particular type of car is prominently characterized in one or more digital images (such as in the foreground, in the middle of an image, etc.), this may be a high level of proof that the detected car (and more generally the subject matter derived therefrom) may be of interest to the user. In contrast, if an object is detected in the background of the digital image, detected as being off-set to the side, or detected as not even being completely depicted in the image (e.g., cut away at the top, side, etc.), this may not prove as likely that the user is potentially interested in the object, as the representation of the object in the digital media file may be merely accidental and/or incidental. In some implementations, the topic identification engine 132 can consider how many objects are detected in a particular digital media file so that any individual object detected in a digital image is not weighted too heavily. In some such embodiments, the detected objects' saliency within the digital image may be considered in conjunction with the total number of objects in the image, such as by the subject identification engine 132. For example, if a large number of objects are detected in a given digital image, but a particular object is characterized far more prominently than other detected objects, then the particular object may be weighted more heavily by the topic identification engine 132.
Additionally or alternatively, in some implementations, the topic identification engine 132 can identify based on metadata associated with the digital media content. For example, assume that the user gives a particular digital photograph filename that describes the object represented in the digital photograph. Such metadata may be used, for example, by the topic identification engine 132 as a signal that (alone or in combination with other signals) may suggest to the user that one or more topics are of interest. In some implementations, the fact that a user spends time explicitly defining metadata that identifies one or more topics may be highly evidencing of the user's potential interest in those topics. For example, other available metadata may include folders in which digital media files are stored in client device 106 and/or in cloud storage 122. For example, if a large number of images are stored in a folder named "pictures of birds," the topic identification engine 132 may infer that the user is interested in the topic of the bird. As another example, in some implementations, the topic identification engine 132 can consider metadata associated with media consumed (e.g., streamed) by the user. For example, a title, a brief description, a description, an actor lineup, a staff member, or other metadata associated with an online streaming video viewed by a user may be used to identify a topic potentially relevant to the user.
Additionally or alternatively, in some implementations, the topic identification engine 132 can identify topics based on the nature of user interaction with one or more digital media files over time. For example, if a user tends to repeatedly open particular digital images in their library (more than other images in their library), the topic identification engine 132 may determine that the objects represented in those particular images are potentially relevant to the user, particularly if other less viewed images do not tend to depict those same objects. This may include digital images stored in the application cache. For example, assume that the user repeatedly revisits a particular web page listing cars for sale, and that the web page includes a photograph of the car. When this photo is loaded from the cache into the web browser (rather than re-downloading the photo), it may be deemed that the user is viewing the photo again, which in turn results in the objects in the photo being identified as potentially relevant.
In addition to or in lieu of objects detected in digital media files created/retrieved by a user, the topic identification engine 132 can employ various other signals in various embodiments. In some implementations, signals such as a user's search query history, browsing history, explicitly provided personal preferences, and the like can be used by, for example, the topic identification engine 132 to identify one or more topics of potential interest to the user. Additionally or alternatively, in some implementations, signals such as content of the user's email/text message, content of the user's social media profile, content of the user's calendar, and various other application data may be used by, for example, the topic identification engine 132 to identify topics that are potentially relevant to the user. In various embodiments, previously identified topics that are potentially relevant to the user may themselves be used to identify new potentially relevant topics. For example, assume that a user takes a picture of a snowboard, a snow peak, a skate, and views an image of winter equipment (e.g., when shopping online). These may result in users being associated with relatively narrow themes such as "skiing", "mountain climbing", "skating", and "winter gear". In some implementations, these topics may be collectively considered by, for example, the topic identification engine 132 to identify other topics having a higher hierarchical structure of topics (e.g., as determined by the knowledge graph engine 136), such as "winter sports.
It should be understood that in various embodiments, a user may have the option to join or opt out of various data used as signals using the techniques described herein. For example, in some implementations, the default user profile setting may be that little or no signal is available to the topic identification engine 132. Only if the user explicitly provides authorization for each signal (or group of signals) can the topic identification engine 132 gain access to those signals for topic identification purposes.
The topic identification engine 132 can weight various types of signals differently. For example, objects detected in a digital media file created by a user may be more likely to prove a user's potential interest in subject matter related to those objects than objects detected in a digital media file that the user merely retrieves for consumption (e.g., viewing, playback, etc.). Similarly, in some implementations, there may be a hierarchy of weights to be assigned to objects detected in a digital media file that is interacted with by a user. For example, the largest weight may be assigned to objects detected in a digital media file created by a user. An intermediate amount of weight may be assigned to objects detected in, for example, a digital media file shared with the user by another user. Objects detected in digital media files retrieved by a user over the internet (e.g., as part of a web search) may be assigned a minimum amount of weight. Other factors that may be considered part of this "hierarchy" of weights include, but are not limited to, how long a user consumes a particular digital media file, whether the user "finishes" consuming the digital media file (e.g., playing an entire song or an entire video), whether the user edited the digital media file, whether the user shared the digital media file, and so forth.
In some implementations, various signals can be analyzed in conjunction to determine whether/to what extent a particular topic is likely to be relevant to the user. For example, in some implementations, detection of objects in a digital media file created by a user may not be sufficient in itself to ensure that the detected objects are used to identify topics of potential interest to the user. However, if the other signals confirm the original user-created digital media file signal, the detected object may be used to identify one or more topics of potential interest to the user.
In some implementations, signals of potential interest to a topic by a user may be weighted based in part on whether objects associated with the topic are common in digital media content created/retrieved by a community of users. In fig. 1, for example, object frequency engine 128 may be configured to determine a frequency of one or more objects in a corpus of digital media files (e.g., stored in index 130 or even in index 122) retrieved and/or created by a community of users. In some such implementations, the frequency with which objects appear in the corpus may be inversely proportional to the objects being weighted multiply when identifying topics of potential interest. For example, furniture such as desks or tables may be very common in a corpus of images. Thus, if a particular user takes one or more digital photographs that capture a table or desk, this does not necessarily imply that the user is interested in the subject "desk" or "table". Instead, the appearance of those common pieces of furniture may be merely an adjunct to digital photographs.
In some implementations, the topic identification engine 132 and/or the object frequency engine 128 can employ various numerical statistics, such as the term frequency inverse document frequency or "TF-IDF," to determine whether and/or to what extent a detected object should affect topic identification. For example, in some implementations, the TD-IDF may be used, for example, by the object frequency engine 128 to determine how much a particular detected object (which may be used as a "term" for "TF-IDF") should affect whether a particular topic is identified as potentially relevant to the user.
In various implementations, the topic identification engine 132 can utilize one or more of the signals previously mentioned in connection with the information obtained from the knowledge graph engine 136 to identify one or more topics of potential interest to the user. For example, assume that various objects are detected in a plurality of digital photographs captured and/or retrieved by a user. In various embodiments, knowledge graph engine 136 may determine that those species are all included in the hierarchy of a particular genus, and determine that the presence of multiple different species evidences potential interest in the genus.
As another example, assume that a user views multiple digital videos. Each of these digital videos may be represented in the knowledge graph 138 and may be connected to related entities (e.g., actors, producers, entertainers, etc.). In various implementations, the knowledge graph engine 136 may utilize various entity relationships in the knowledge graph 138 to predict one or more topics of potential interest to the user (e.g., movies and/or genres that the user may be interested in, places that the user may be interested in, etc.) based on the plurality of digital videos the user is viewing.
As yet another example, assume that a user watches a football game on one night, then watches a baseball game on another night, and then takes a digital photograph of a basketball game on a third night. In various implementations, the topic identification engine 132 can determine that the user has a broad interest in, for example, professional sports, e.g., based on entities, such as teams associated with each of these events or the events themselves. If all teams are from the same city, in some embodiments, topic identification engine 132 may identify this city as a topic of potential interest to the user.
FIG. 2 depicts an example scenario in which techniques may be employed to base various signals 250 on1-NTo identify one or more topics potentially relevant to or of potential interest to the user. In FIG. 2, a first signal 2501In the form of a digital image captured by a user using the vision sensor 107 of the client device 106. Such digital images may be stored in cloud-based storage 122 by, for example, online storage engine 120. As part of the upload process, the digital image may be analyzed, for example, by object recognition client 110 and/or object recognition engine 124, to detect one or more objects. In the example of fig. 2, a pair of aigrette birds are detected in the first digital image. In some implementations, it may already be sufficient for the topic identification engine 132 to identify topics such as "birds," "waterfowls," "aigres," etc. as being potentially relevant to the user. However, in other embodiments, further validation may be required.
Suppose the user later views a digital video about bald hawk. In various embodiments, this fact and/or aspect of the digital video may be used, for example, by the topic identification engine 132 as another signal 250 for identifying topics potentially relevant to the userN-1. For example, both eagles and aigres are "birds," and thus "birds" may be identified as topics of potential interest to the user. Additionally or alternatively, yet another signal 250 in the form of a user performing a bird related search ("birdhouse supply") may be analyzedNTo further verify the user's interest in the bird.
FIG. 3 depicts another example scenario in which techniques described herein may be employed to identify topics that are potentially relevant to a user. FIG. 3 depicts a digital photograph 350 taken by a user, for example, using a client device 106 in the form of a smartphone. In this example, the digital photograph 350 captures a kitchen that includes a microwave oven 3561Oven 3562Dining table 3563And dining table 3563Unmanned aerial vehicle 356 at the top4. These four objects may be detected, for example, by object recognition client 110 and/or object recognition engine 124And its corresponding location in the digital photograph 350. Such data may be used by the topic identification engine 132 to identify one or more topics that are potentially relevant to the user.
In some implementations, the topic identification engine 132 can consider whether each object detected in the digital media file belongs to a particular topic or genus to determine whether the object is exceptional (and thus should be used to identify potentially relevant topics) or merely incidental to the digital media file. For example, in FIG. 3, the topic identification engine 132, e.g., with the assistance of the knowledge graph engine 136 and/or the object frequency engine 128, may determine that there are three detected objects 3561、3562And 3563Is relatively common in kitchens. Thus, the presence of these objects merely suggests that the digital photograph 350 was captured in the kitchen, and not that the user is potentially interested in these objects. However, unmanned aerial vehicle 3564With other detected objects 3561-3Different (i.e., not related). Further, the drone is relatively prominently characterized in the digital photograph 350. It is in an oven 3562And dining table 3563Both "in front of" (or in the foreground relative to them). It is also relatively close to the central region of the digital photograph 350 and in focus. Thus, in some implementations, the topic identification engine 132 can determine the drone 3564Is proof that the user is interested in a subject such as a drone. This potential interest may be further evidenced by other signals, such as a search query from the user relating to the drone, videos that the user has viewed relating to the drone, and so forth.
Fig. 4 depicts yet another scenario in which the techniques described herein may be used in this example to identify one or more topics that may not be known to the user to be potentially relevant to the user. In this example, the user obtains a first digital photograph 450 of the user's son, for example, using the visual sensor 107 of the client device 1061. It can be seen that the son is wearing a T-shirt painted with the symbol "omega". Such a symbol may be, for example, a symbol used by a musician that the son likes. The symbol and in some cases the identity of the son may be represented bySuch as object recognition client 110 and/or object recognition engine 124. These data may be used, for example, by the topic identification engine 132 with the help of the knowledge graph engine 136 to determine that a symbol corresponds to a particular musician.
Later, the user acquires another digital image 450 depicting the son again2. At this point, the son is wearing a different T-shirt, which does not depict the same symbols. However, the son's computer screen is currently displaying the same symbols as can be detected by the above. The presence of the symbol "Ω" in the multiple images of the user's son may be used, for example, by the topic identification engine 132 to determine that the topic of the musician associated with the symbol "Ω" is potentially relevant to the user. For example, when the son's birthday approaches, the father may be actively informed of the upcoming tour date of the musician.
Once topics have been identified as potentially relevant to a user, they may be used for a variety of different purposes. In some implementations, topics that are potentially relevant to the user can be used to rank search results presented to the user and/or formulate alternative query suggestions for presentation to the user. Additionally or alternatively, in some implementations, topics potentially relevant to the user may be used to disambiguate user input. Assume that a user desires a personal digital assistant executing on a particular client device 106 to call a contact named "John". There may be multiple "John" s in the user's contact list. In various embodiments, rather than requesting disambiguation of input from a user, one or more digital media files (e.g., photos, voice mails, videos, etc.) created and/or retrieved by the user may be analyzed as described herein to identify "John," which the user most likely mentions, as a topic potentially relevant to the user. Additionally or alternatively, topics determined using the techniques described herein may be used to recommend media for consumption by a user.
In some implementations, topics determined using the techniques described herein can be used to proactively present content to a user. For example, in fig. 5, a client device 506 in the form of a smart watch is being used to actively (i.e., without user request) present content to a user. In this example, the techniques described herein have been used to determine that a user is interested in birds from digital media content created and/or retrieved by the user. Thus, the user is presented with a first "card" or "map" recommending to the bird viewer nearby exhibitions and a second "card" or "map" recommending nearby movies related to birds. In various embodiments, these cards/maps may be selectable deep links that, if selected, enable an appropriate user interface that enables the user to obtain more information, make reservations, etc.
In various implementations, user engagement with content presented using the techniques described herein may be used to prove and/or disprove topics identified using the techniques described herein. For example, assume that the user slides away or disregards the card in FIG. 5. This may imply that while the techniques described herein imply that the user is potentially interested in the bird, in fact the user is not. Thus, the topic identification engine 132 can, for example, remove or demote the topic "bird" from the user interest database 134.
Referring now to FIG. 6, an example method 600 implementing selected aspects of the present disclosure is described. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. Such a system may include various components of various computer systems. For example, some operations may be performed at client device 106h, while other operations may be performed by one or more components of cloud-based system 102. Further, while the operations of method 600 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 602, the system, e.g., by way of the online storage engine 120, may receive data indicative of one or more digital media files from one or more client devices (e.g., 106) operated by a user. In some implementations, such data may be the original digital media file itself, e.g., a digital media file that the user is uploading (consciously or automatically) for backup purposes. Additionally or alternatively, in some embodiments, this may include data generated locally on the client device indicative of the detected object, such as dimension reduction embedding or the like. These digital media files have been created and/or retrieved for consumption by a user.
In block 604, the system, e.g., by way of the object recognition client 110 and/or the object recognition engine 124, may identify one or more objects detected in each of the one or more digital media files based on the data indicative of the one or more digital media files. For example, object recognition engine 124 may perform various types of object recognition (described previously) on the digital media file to detect one or more represented objects.
In block 606, the system, e.g., with the aid of the object frequency engine 128, may determine the frequency of each of the one or more objects detected in block 604 in a pre-stock library of digital media files (e.g., 130) provided by the community of users. As described above, various statistics such as TF-IDF may be employed to determine how common detected objects are and thus whether and/or to what extent they should be used to identify topics that are potentially relevant to the user. A highly common object may actually be noise, rather than a useful signal of interest to the user.
In block 608, the system may select one or more of the objects that should be used to identify the subject matter of potential interest based on the corresponding frequencies determined in block 606. For example, common objects such as furniture, trees, etc. may not be used to identify topics of interest to the user unless, for example, those common objects occur frequently in the user's digital media content and/or are prominently characterized in the digital media file without other, less common objects (e.g., the user takes a picture of a large number of trees in the absence of other objects).
In block 610, the system, e.g., via the topic identification engine 132, may identify one or more topics that are potentially relevant to the user based on the one or more objects selected at block 608. In block 612, the system may formulate a search query based on the identified one or more topics. For example, if the topics "bird" and "waterfowl" are identified from the user's digital photograph, a search query such as "bird waterfowl" may be formulated and submitted to a search engine to obtain responsive content. In some implementations, the response content can be ordered based on those topics and/or other topics identified using the techniques described herein.
In block 614, the system may proactively cause information responsive to the formulated search query to be output at one or more of the client devices operated by the user. A non-limiting example of such active content is depicted in fig. 5. Additional examples of active content include, but are not limited to, text messages (e.g., SMS or MMS), social media messages (e.g., on a user's feed), natural language output from a personal digital assistant (e.g., during an existing man-machine conversation between the user and the personal digital assistant, etc., or otherwise), mail messages, pop-up windows (e.g., at an android concert), restaurant recommendations (e.g., provided in a restaurant reservation and/or review application), digital media recommendations (e.g., presented as part of a digital media application), and so forth.
Referring now to fig. 7, another example method 700 is depicted, which may be implemented in whole or in part at a client device 106, in accordance with various embodiments. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. Such a system may include various computer systems, particularly various components of the client device 106, although one or more operations may additionally or alternatively be performed by one or more components of the cloud-based system 102. Further, while the operations of method 700 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 702, the system may obtain one or more digital files created by a user using a client device. For example, in some embodiments, when a user takes a digital photograph, data representing the digital photograph may be stored in volatile and/or non-volatile memory. At block 704, the system, e.g., with the aid of object recognition client 110, may apply the one or more digital media files as input to one or more trained machine learning models local to the client device. For example, object recognition client 110 may apply one or more digital media files in one or more trained machine learning models (e.g., CNNs) stored in on-device model index 111. In various implementations, the one or more trained machine learning models may generate an output indicative of the one or more objects detected in each of the one or more digital media files.
At block 706, the system (e.g., client device 106) may provide data indicative of the indicated one or more objects to a remote computing system, such as cloud-based system 102, without providing the one or more digital media files themselves. Thus, the data provided at block 706 may be different from the digital media file itself, such as in the form of vectors of features extracted from the digital media file, reduced-dimension embedding generated based on the digital media file, and so forth. For example, rather than providing raw digital photograph data to the cloud-based system 102, the client device 106 may provide output generated from one or more CNNs to the cloud-based system 102. For example, such output may include one or more subject predictions and corresponding confidence metrics (e.g., 70% of the probabilistic subjects are cats, 10% of the probabilistic subjects are dogs, 5% of the probabilistic subjects are monkeys, etc.). Notably, providing such data to the cloud-based system 102, rather than exposing the raw data comprising the user's digital media file to the cloud-based system 102, can protect the user's privacy to the extent that the digital media file contains information that the user deems private or sensitive.
At block 708, the system may receive information associated with the indicated one or more objects from the same remote computing system or a different remote computing system. For example, the client device 106 may receive information associated with one or more objects and/or topics inferred from one or more topics from the cloud-based system 102 or another remote computing system (such as a search engine that returns search results responsive to the search query formulated at block 612 of fig. 6). In various embodiments, the objects themselves (e.g., classes or types thereof) may form "topics" that are deemed potentially relevant to the user, for example, by the topic identification engine 132.
In block 710, the system may proactively output information associated with the indicated one or more objects to the user, e.g., at the same client device used to create the digital media file or a different client device operated by the user (such as another client device of the coordinated "ecosystem" of client devices operated by the user). For example, assume that a user captures a photograph using her smartphone. After identifying one or more topics of potential interest to the user from the photos using the techniques described herein, output related to those topics of potential interest may be presented at another client device of the user, for example, as a card or a sticker on a smart watch or smart television. Some examples of active outputs are provided in fig. 5 and described with respect to block 614 of fig. 6.
As another example of active output, assume that a user has previously posted to social media a digital photograph of a number of meals that the user has prepared and/or consumed. The techniques described herein may be employed to identify a user's potential interest in cooking from digital photos posted to social media. Now, assume that the user later participates in a voice-based man-machine conversation with an automated assistant, for example, through a stand-alone interactive speaker. Assume further that there is a third party application or "agent" related to cooking that the user can participate via the automated assistant but that the user is not aware of. Based on the user's perceived interest in cooking, the automated assistant may proactively introduce the user to a third party agent. For example, the automated assistant may provide an audible natural language output, such as "i see a picture that you have posted several meals to your social media account. This is a friend of my favorite recommendations for various recipes. "in some cases, the third party agent may then insert say: "hi, let i know what food you have or how your mood, i will suggest recipes. "
Fig. 8 is a block diagram of an example computer system 810. Computer system 810 typically includes at least one processor 814 that communicates with a number of peripheral devices via a bus subsystem 812. These peripheral devices may include a storage subsystem 824 (e.g., memory subsystem 825 and file storage subsystem 826), user interface output devices 820, user interface input devices 822, and a network interface subsystem 816. The input and output devices allow a user to interact with computer device 810. Network interface subsystem 816 provides an interface to external networks and couples to corresponding interface devices in other computer systems.
The user interface input devices 822 may include a keyboard, a pointing device (such as a mouse, trackball, touchpad, or tablet), a scanner, a touch screen incorporated into the display, an audio input device (such as a voice recognition system, microphone), and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computer system 810 or onto a communication network.
User interface output devices 820 may include a display subsystem, a printer, a facsimile machine, or a non-visual display, such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a viewable image. The display subsystem may also provide a non-visual display, such as through an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computer system 810 to a user or to another machine or computer system.
These software modules are typically executed by processor 814 either alone or in combination with other processors. The memory 825 for the storage subsystem may include a number of memories, including a main Random Access Memory (RAM)830 for storing instructions and data during program execution and a Read Only Memory (ROM)832 in which fixed instructions are stored. File storage subsystem 824 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical disk drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by the file storage subsystem 826 in the storage subsystem 824 or other machines accessible by the processor(s) 814.
Where the system described herein collects personal information about a user or is likely to use personal information, the user may be provided with the following opportunities: control whether programs or features collect user information (e.g., information about the user's social network, social actions or activities, profession, the user's preferences, or the user's current geographic location), or control whether and/or how to receive content from a content server that may be more relevant to the user. Also, certain data may be processed in one or more ways prior to storage or use, such that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information for the user cannot be determined, or the geographic location of the user may be generalized where geographic location information is obtained (such as to a city, ZIP code, or state level) such that a particular geographic location of the user cannot be determined. Thus, the user may control how information is collected about the user and/or used information.
Although various embodiments have been described and illustrated herein, various other means and/or structures for performing the function and/or obtaining the result and/or one or more of the advantages described herein may be utilized and each of such variations and/or modifications is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary, and the actual parameters, dimensions, materials, and/or configurations are dependent on the particular application or applications for which the present teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (14)
1. A method implemented using one or more processors of a client device operated by a user, comprising:
obtaining one or more digital media files created by the user with the client device;
applying the one or more digital media files as input to one or more trained machine learning models local to the client device, wherein the one or more trained machine learning models generate an output indicative of one or more objects detected in each of the one or more digital media files;
providing data indicative of the indicated one or more objects to a remote computing system without providing the one or more digital media files;
receiving information associated with the indicated one or more objects from the same remote computing system or a different remote computing system; and
outputting the information associated with the indicated one or more objects to the user at the same client device operated by the user or a different client device.
2. The method of claim 1, wherein the providing causes the remote computing system to formulate a search query based on the identified one or more objects, and the information associated with the indicated one or more objects comprises information responsive to the formulated search query.
3. The computer-implemented method of claim 2, further comprising:
receiving the search query from the remote computing system; and
issuing the search query such that the information responsive to the search query is received in response to the issuing.
4. The computer-implemented method of claim 2 or 3, wherein the remote computing system uses the formulated search query itself to obtain the information responsive to the formulated search query.
5. The computer-implemented method of any of claims 2 to 4, wherein formulating a search query comprises:
determining a frequency of each of the one or more objects in a corpus of digital media files provided by a community of users;
selecting one or more of the objects based on the corresponding frequencies;
identifying one or more topics potentially relevant to the user based on the selected one or more objects; and
formulating the search query based on the identified one or more topics.
6. The method of any preceding claim, wherein the selecting is further based on a location of the one or more objects in the one or more digital media files.
7. The method of any preceding claim, wherein the selecting is further based on a measure of focus of the one or more objects in the one or more digital media files.
8. The method of any claim, wherein the selecting is further based on whether the user created or retrieved the one or more digital media files received from the one or more client devices.
9. The computer-implemented method of any preceding claim, wherein the remote computing system determines one or more topics potentially relevant to the user based on the data indicative of the indicated one or more objects, and the information associated with the indicated one or more objects includes information associated with one or more interests in a database.
10. The computer-implemented method of any preceding claim, wherein the one or more digital media files comprise one or more digital images.
11. The method of any preceding claim, wherein the data indicative of the one or more digital media files comprises one or more audio files.
12. The method of any preceding claim, wherein the data indicative of the one or more digital media files comprises one or more feature vectors or dimension-reducing embeddings extracted from the one or more digital media files.
13. A system comprising one or more processors and memory operably coupled with the one or more processors, wherein the memory stores instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to perform the method of any preceding claim.
14. At least one non-transitory computer-readable medium comprising instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to perform the method of any one of claims 1-12.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/014,730 | 2018-06-21 | ||
US16/014,730 US10860642B2 (en) | 2018-06-21 | 2018-06-21 | Predicting topics of potential relevance based on retrieved/created digital media files |
PCT/US2019/038152 WO2019246348A1 (en) | 2018-06-21 | 2019-06-20 | Predicting topics of potential relevance based on retrieved/created digital media files |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112313643A true CN112313643A (en) | 2021-02-02 |
Family
ID=67441583
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980038861.XA Pending CN112313643A (en) | 2018-06-21 | 2019-06-20 | Predicting potentially relevant subject matter based on retrieved/created digital media files |
Country Status (5)
Country | Link |
---|---|
US (3) | US10860642B2 (en) |
JP (3) | JP6930041B1 (en) |
KR (3) | KR102574279B1 (en) |
CN (1) | CN112313643A (en) |
WO (1) | WO2019246348A1 (en) |
Families Citing this family (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10884981B1 (en) * | 2017-06-19 | 2021-01-05 | Wells Fargo Bank, N.A. | Tagging tool for managing data |
US10936630B2 (en) * | 2018-09-13 | 2021-03-02 | Microsoft Technology Licensing, Llc | Inferring topics with entity linking and ontological data |
US11341207B2 (en) * | 2018-12-10 | 2022-05-24 | Ebay Inc. | Generating app or web pages via extracting interest from images |
US11308110B2 (en) * | 2019-08-15 | 2022-04-19 | Rovi Guides, Inc. | Systems and methods for pushing content |
US11386144B2 (en) | 2019-09-09 | 2022-07-12 | Adobe Inc. | Identifying digital attributes from multiple attribute groups within target digital images utilizing a deep cognitive attribution neural network |
JP7255032B2 (en) * | 2020-01-30 | 2023-04-10 | グーグル エルエルシー | voice recognition |
WO2021206764A1 (en) * | 2020-04-06 | 2021-10-14 | Google Llc | Dynamic multi-axis graphical user interface |
CN113051379B (en) * | 2021-02-24 | 2023-08-04 | 南京审计大学 | Knowledge point recommendation method and device, electronic equipment and storage medium |
US11874808B1 (en) * | 2021-06-23 | 2024-01-16 | Amazon Technologies, Inc. | Enhanced user profile management across multiple devices |
KR20240025384A (en) * | 2022-08-18 | 2024-02-27 | 삼성전자주식회사 | Electronic apparatus for providing customized metaverse content and thereof method |
WO2024053846A1 (en) * | 2022-09-07 | 2024-03-14 | Samsung Electronics Co., Ltd. | A method and device for personalised image segmentation and processing |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060240862A1 (en) * | 2004-02-20 | 2006-10-26 | Hartmut Neven | Mobile image-based information retrieval system |
CN104781815A (en) * | 2012-12-20 | 2015-07-15 | 英特尔公司 | Method and apparatus for optimization analysis of bonding positions on structure |
CN105809619A (en) * | 2015-01-19 | 2016-07-27 | 株式会社理光 | Image acquisition user interface for linear panoramic image stitching |
US20160350332A1 (en) * | 2015-05-29 | 2016-12-01 | International Business Machines Corporation | Individualized on-demand image information acquisition |
WO2017071969A1 (en) * | 2015-10-30 | 2017-05-04 | Philips Lighting Holding B.V. | Commissioning of a sensor system |
Family Cites Families (46)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5758257A (en) | 1994-11-29 | 1998-05-26 | Herz; Frederick | System and method for scheduling broadcast of and access to video programs and other data using customer profiles |
JPH0926970A (en) | 1994-12-20 | 1997-01-28 | Sun Microsyst Inc | Method and apparatus for execution by computer for retrievalof information |
US20020123928A1 (en) | 2001-01-11 | 2002-09-05 | Eldering Charles A. | Targeting ads to subscribers based on privacy-protected subscriber profiles |
US6701362B1 (en) | 2000-02-23 | 2004-03-02 | Purpleyogi.Com Inc. | Method for creating user profiles |
CA2349914C (en) | 2000-06-09 | 2013-07-30 | Invidi Technologies Corp. | Advertising delivery method |
AU2002223811B8 (en) | 2000-11-20 | 2007-10-18 | British Telecommunications Public Limited Company | Method of managing resources |
US20020174134A1 (en) | 2001-05-21 | 2002-11-21 | Gene Goykhman | Computer-user activity tracking system and method |
US7181488B2 (en) * | 2001-06-29 | 2007-02-20 | Claria Corporation | System, method and computer program product for presenting information to a user utilizing historical information about the user |
US9256685B2 (en) | 2005-03-31 | 2016-02-09 | Google Inc. | Systems and methods for modifying search results based on a user's history |
US20060293957A1 (en) | 2005-06-28 | 2006-12-28 | Claria Corporation | Method for providing advertising content to an internet user based on the user's demonstrated content preferences |
US10949773B2 (en) * | 2005-10-26 | 2021-03-16 | Cortica, Ltd. | System and methods thereof for recommending tags for multimedia content elements based on context |
JP4909033B2 (en) * | 2006-11-30 | 2012-04-04 | 富士通株式会社 | Program search device |
WO2009085336A1 (en) * | 2007-12-27 | 2009-07-09 | Inc. Arbor Labs | System and method for advertisement delivery optimization |
US9123061B2 (en) * | 2010-03-24 | 2015-09-01 | Disney Enterprises, Inc. | System and method for personalized dynamic web content based on photographic data |
US9037600B1 (en) * | 2011-01-28 | 2015-05-19 | Yahoo! Inc. | Any-image labeling engine |
US9058611B2 (en) * | 2011-03-17 | 2015-06-16 | Xerox Corporation | System and method for advertising using image search and classification |
US9672496B2 (en) | 2011-08-18 | 2017-06-06 | Facebook, Inc. | Computer-vision content detection for connecting objects in media to users |
US9135631B2 (en) | 2011-08-18 | 2015-09-15 | Facebook, Inc. | Computer-vision content detection for sponsored stories |
US8943015B2 (en) | 2011-12-22 | 2015-01-27 | Google Technology Holdings LLC | Hierarchical behavioral profile |
US20140019546A1 (en) * | 2012-07-13 | 2014-01-16 | Telibrahma Convergent Communications Pvt. Ltd. | Method and system for creating a user profile to provide personalized results |
US10691743B2 (en) * | 2014-08-05 | 2020-06-23 | Sri International | Multi-dimensional realization of visual content of an image collection |
US20150058079A1 (en) * | 2013-08-26 | 2015-02-26 | Google Inc. | Detecting trends from images uploaded to a social network |
AU2014321165B2 (en) * | 2013-09-11 | 2020-04-09 | See-Out Pty Ltd | Image searching method and apparatus |
US20160292299A1 (en) * | 2014-01-29 | 2016-10-06 | Google Inc. | Determining and inferring user attributes |
US20150365725A1 (en) * | 2014-06-11 | 2015-12-17 | Rawllin International Inc. | Extract partition segments of personalized video channel |
WO2016106383A2 (en) * | 2014-12-22 | 2016-06-30 | Robert Bosch Gmbh | First-person camera based visual context aware system |
KR20160091488A (en) * | 2015-01-23 | 2016-08-03 | 정영규 | Method and System for Automatic Detection of Object using Model Generation |
JP5996748B1 (en) * | 2015-09-04 | 2016-09-21 | 株式会社リクルートホールディングス | Order processing system, order processing method |
JP2017059156A (en) * | 2015-09-18 | 2017-03-23 | 富士フイルム株式会社 | Image processing device, image processing method, program and recording medium |
US9984075B2 (en) * | 2015-10-06 | 2018-05-29 | Google Llc | Media consumption context for personalized instant query suggest |
KR20180069813A (en) * | 2015-10-16 | 2018-06-25 | 알리바바 그룹 홀딩 리미티드 | Title display method and apparatus |
US10282431B1 (en) * | 2015-12-18 | 2019-05-07 | A9.Com, Inc. | Image similarity-based group browsing |
US10706098B1 (en) * | 2016-03-29 | 2020-07-07 | A9.Com, Inc. | Methods to present search keywords for image-based queries |
US10109051B1 (en) * | 2016-06-29 | 2018-10-23 | A9.Com, Inc. | Item recommendation based on feature match |
US10083379B2 (en) * | 2016-09-27 | 2018-09-25 | Facebook, Inc. | Training image-recognition systems based on search queries on online social networks |
US20180101540A1 (en) * | 2016-10-10 | 2018-04-12 | Facebook, Inc. | Diversifying Media Search Results on Online Social Networks |
US20180181667A1 (en) * | 2016-12-23 | 2018-06-28 | 0934781 BC Ltd | System and method to model recognition statistics of data objects in a business database |
US10503775B1 (en) * | 2016-12-28 | 2019-12-10 | Shutterstock, Inc. | Composition aware image querying |
US10043109B1 (en) * | 2017-01-23 | 2018-08-07 | A9.Com, Inc. | Attribute similarity-based search |
US11194856B2 (en) * | 2017-03-07 | 2021-12-07 | Verizon Media Inc. | Computerized system and method for automatically identifying and providing digital content based on physical geographic location data |
CN111295669A (en) * | 2017-06-16 | 2020-06-16 | 马克波尔公司 | Image processing system |
US20190188285A1 (en) * | 2017-12-19 | 2019-06-20 | Facebook, Inc. | Image Search with Embedding-based Models on Online Social Networks |
US10817749B2 (en) * | 2018-01-18 | 2020-10-27 | Accenture Global Solutions Limited | Dynamically identifying object attributes via image analysis |
US10191921B1 (en) * | 2018-04-03 | 2019-01-29 | Sas Institute Inc. | System for expanding image search using attributes and associations |
US11176589B2 (en) * | 2018-04-10 | 2021-11-16 | Ebay Inc. | Dynamically generated machine learning models and visualization thereof |
US20210232577A1 (en) * | 2018-04-27 | 2021-07-29 | Facet Labs, Llc | Devices and systems for human creativity co-computing, and related methods |
-
2018
- 2018-06-21 US US16/014,730 patent/US10860642B2/en active Active
-
2019
- 2019-06-20 KR KR1020227036225A patent/KR102574279B1/en active IP Right Grant
- 2019-06-20 KR KR1020237029582A patent/KR20230132601A/en not_active Application Discontinuation
- 2019-06-20 WO PCT/US2019/038152 patent/WO2019246348A1/en active Application Filing
- 2019-06-20 CN CN201980038861.XA patent/CN112313643A/en active Pending
- 2019-06-20 JP JP2020568971A patent/JP6930041B1/en active Active
- 2019-06-20 KR KR1020207035649A patent/KR102457665B1/en active IP Right Grant
-
2020
- 2020-11-13 US US17/097,110 patent/US11580157B2/en active Active
-
2021
- 2021-08-10 JP JP2021130662A patent/JP2021192241A/en active Pending
-
2023
- 2023-01-06 US US18/094,245 patent/US11971925B2/en active Active
- 2023-09-28 JP JP2023167815A patent/JP2023179583A/en active Pending
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060240862A1 (en) * | 2004-02-20 | 2006-10-26 | Hartmut Neven | Mobile image-based information retrieval system |
CN104781815A (en) * | 2012-12-20 | 2015-07-15 | 英特尔公司 | Method and apparatus for optimization analysis of bonding positions on structure |
CN105809619A (en) * | 2015-01-19 | 2016-07-27 | 株式会社理光 | Image acquisition user interface for linear panoramic image stitching |
US20160350332A1 (en) * | 2015-05-29 | 2016-12-01 | International Business Machines Corporation | Individualized on-demand image information acquisition |
WO2017071969A1 (en) * | 2015-10-30 | 2017-05-04 | Philips Lighting Holding B.V. | Commissioning of a sensor system |
Also Published As
Publication number | Publication date |
---|---|
KR102574279B1 (en) | 2023-09-04 |
JP2021525433A (en) | 2021-09-24 |
US20210064653A1 (en) | 2021-03-04 |
US11971925B2 (en) | 2024-04-30 |
JP6930041B1 (en) | 2021-09-01 |
US10860642B2 (en) | 2020-12-08 |
JP2021192241A (en) | 2021-12-16 |
US20190392055A1 (en) | 2019-12-26 |
KR20210005733A (en) | 2021-01-14 |
WO2019246348A1 (en) | 2019-12-26 |
KR102457665B1 (en) | 2022-10-24 |
KR20230132601A (en) | 2023-09-15 |
KR20220145424A (en) | 2022-10-28 |
US20230153346A1 (en) | 2023-05-18 |
JP2023179583A (en) | 2023-12-19 |
US11580157B2 (en) | 2023-02-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11971925B2 (en) | Predicting topics of potential relevance based on retrieved/created digital media files | |
EP3475848B1 (en) | Generating theme-based videos | |
US9715731B2 (en) | Selecting a high valence representative image | |
US10645142B2 (en) | Video keyframes display on online social networks | |
JP6730335B2 (en) | Streaming media presentation system | |
US9183282B2 (en) | Methods and systems for inferring user attributes in a social networking system | |
US20180101540A1 (en) | Diversifying Media Search Results on Online Social Networks | |
US10397167B2 (en) | Live social modules on online social networks | |
AU2013292585B2 (en) | Context-based object retrieval in a social networking system | |
KR102437640B1 (en) | Image selection suggestions | |
US11126682B1 (en) | Hyperlink based multimedia processing | |
US11558324B2 (en) | Method and system for dynamically generating a card | |
CN110476162B (en) | Controlling displayed activity information using navigation mnemonics | |
US8935299B2 (en) | Identifying relevant data for pages in a social networking system | |
US20170098144A1 (en) | Method and system for generating a card based on intent | |
US11157572B1 (en) | Sharing user activity data with other users | |
US20150055936A1 (en) | Method and apparatus for dynamic presentation of composite media | |
EP3306555A1 (en) | Diversifying media search results on online social networks | |
JP6882534B2 (en) | Identifying videos with inappropriate content by processing search logs | |
US20230370408A1 (en) | Generating and surfacing messaging thread specific and content-based effects |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |