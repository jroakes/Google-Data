CN115836298A - Automatic selection and filter removal optimization of quantization under energy constraints - Google Patents
Automatic selection and filter removal optimization of quantization under energy constraints Download PDFInfo
- Publication number
- CN115836298A CN115836298A CN202180040159.4A CN202180040159A CN115836298A CN 115836298 A CN115836298 A CN 115836298A CN 202180040159 A CN202180040159 A CN 202180040159A CN 115836298 A CN115836298 A CN 115836298A
- Authority
- CN
- China
- Prior art keywords
- neural network
- network model
- computing system
- model
- candidate
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0495—Quantised networks; Sparse networks; Compressed networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/092—Reinforcement learning
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y02—TECHNOLOGIES OR APPLICATIONS FOR MITIGATION OR ADAPTATION AGAINST CLIMATE CHANGE
- Y02D—CLIMATE CHANGE MITIGATION TECHNOLOGIES IN INFORMATION AND COMMUNICATION TECHNOLOGIES [ICT], I.E. INFORMATION AND COMMUNICATION TECHNOLOGIES AIMING AT THE REDUCTION OF THEIR OWN ENERGY USE
- Y02D10/00—Energy efficient computing, e.g. low power processors, power management or thermal management
Abstract
Systems and methods are disclosed for generating neural network architectures, such as devices to be deployed for mobile or other resource-constrained devices, with improved energy consumption and performance tradeoffs. In particular, the present disclosure provides systems and methods for searching a network search space to jointly optimize the size of layers (e.g., the number of filters in a convolutional layer or the number of output cells in a dense layer) and the quantization of values within the layers of a reference neural network model. Examples of the disclosed network architecture search can optimize models of arbitrary complexity by defining a search space to correspond to the architecture of the reference neural network model. The resulting neural network model can be run using relatively less computational resources (e.g., less processing power, less memory usage, less power consumption, etc.) while maintaining performance (e.g., accuracy) competitive with, or even exceeding, that of current state-of-the-art mobile-optimized models.
Description
RELATED APPLICATIONS
This application claims priority and benefit from U.S. provisional patent application No. 63/034,532, filed on 4.6.2020 and incorporated herein by reference in its entirety.
Technical Field
The present disclosure relates generally to neural network architectures. More particularly, the present disclosure relates to systems and methods for generating an architecture optimized for performance and reduced energy consumption.
Background
Neural networks typically rely on computationally expensive computations to achieve the desired accuracy and speed in performing a given task. Increasingly deploying neural network models in battery-powered mobile devices or other resource-constrained environments presents challenges to designing neural networks that operate under more stringent resource constraints.
The efficiency of the currently most advanced neural network architectures (e.g., convolutional neural network architectures for performing object detection) is highly dependent on the optimal choice of hyper-parameters. The hyper-parameters affect the overall structure and operation of the network and are typically outside the training cycle of the network. Since the values are not trained, they are typically selected manually. In view of this difficulty, a widespread approach to improving the efficiency of neural networks follows a basic intuition: making the network smaller to reduce computational costs.
Network architecture searches have achieved this goal by searching for neural network architectures that achieve the desired performance goal under size constraints. Although this approach has been successful with strong performance on many benchmark tests (benchmark), previous architecture search approaches have several limitations. For example, the large size of a typical search space imposes certain practical limitations on the type and arrangement of blocks within a neural network architecture, thereby limiting the creativity of network designers to implement custom neural networks.
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the description which follows, or may be learned by practice of the embodiments.
One example aspect of the present disclosure is directed to a computer-implemented method for quantifying a neural network model while considering performance. The method includes receiving, by a computing system comprising one or more computing devices, a reference neural network model. The method also includes modifying, by the computing system, the reference neural network model to generate a candidate neural network model. The candidate neural network model is generated by selecting one or more values from a first searchable subspace and selecting one or more values from a second searchable subspace, wherein the first searchable subspace corresponds to a quantization scheme used to quantize the one or more values of the reference neural network model and the second parameter corresponds to a size of a layer of the reference neural network model. The method also includes evaluating one or more performance metrics of the candidate neural network model.
In other example aspects, the method further includes outputting a new neural network model based at least in part on the one or more performance metrics.
Other aspects of the disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the relevant principles.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification with reference to the drawings.
Fig. 1-4 depict diagrams of example neural architecture search methods, according to example embodiments of the present disclosure.
Fig. 5 depicts a flowchart of an example method of performing a neural architecture search in accordance with an example embodiment of the present disclosure.
Fig. 6 depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Fig. 7 depicts a block diagram of an example computing device, according to an example embodiment of the disclosure.
Fig. 8 depicts a block diagram of an example computing device, according to an example embodiment of the disclosure.
Fig. 9 depicts a plot of an example scaling factor (scaling factor) curve according to an example embodiment of the present disclosure.
FIG. 10 depicts example test results obtained according to an example embodiment of the present disclosure.
Repeated reference numerals in the various figures are intended to identify similar features in the various embodiments.
Detailed Description
SUMMARY
In general, the present disclosure is directed to systems and methods for performing a neural architecture search to generate a neural network model architecture that provides an improved tradeoff between performance and energy consumption. In some embodiments, the systems and methods of the present disclosure may generate an optimized neural network model by optimizing an existing architecture of the provided reference neural network model.
More specifically, the energy consumption required to execute the neural network model may be estimated as a first order (first order) by summing the amount of energy required to execute each operation in the execution of the neural network model. Given a neural network having, for example, N i A sum of N o An output and corresponding biasDense layer of difference set B, execution of which only would require that N be calculated i ·N o Retrieving N from memory before a Multiply and Accumulate (MAC) operation i ·N o A weight, N o A deviation, and N i And (4) inputting. Convolutional layers also require a large number of MACs, which scale with each additional filter in the layer.
In addition to the amount of basic (bare) computation, the energy consumption or cost associated with executing a neural network model also increases with the accuracy of numerically representing the model values. High precision numbers require more bits to represent within a computing system, and this increased bit width (which may also be referred to as bit depth in some cases) is associated with several energy costs, including increased storage costs, retrieval costs, and computational costs.
Thus, reducing the number of MAC operations and/or limiting the accuracy of at least some values within the neural network may reduce the associated energy costs of executing the neural network model. In some cases, reducing the precision (e.g., bit width) of values within a given neural network may be achieved by quantization, which includes a method of mapping higher precision numbers into bins (bins) corresponding to lower precision numbers. However, a lower accuracy number may not be able to capture as much detail as a higher accuracy number.
In view of the nature of MAC operations, previous network architecture search methods have failed to provide a meaningful way of optimizing the quantification of the model. In particular, there is virtually unlimited variation in network architecture, with different amounts and configurations of layers. Each variation provides a different amount and complexity of MAC operations, affecting the energy consumption of the model. To present an easy-to-handle search problem, web searches have typically investigated limited search spaces, such as using predefined motifs (motifs) or combinations and configurations of building blocks. These search constraints hinder the complexity and adaptability of neural network models.
Advantageously, the system and method of the present disclosure extends the capabilities of network search methods to overcome the challenges described above. In some embodiments, the network search space is structured to correspond to the architecture (e.g., number of arrangements, configurations, and/or layers) of a given neural network model, permitting efficient searching to optimize the neural network structure without limitation to the complexity of the neural network model. For example, in some embodiments, the neural network model may include layers (e.g., convolutional layers) or output units (e.g., in dense layers) with a certain amount of filters. The system and method of the present disclosure can compensate for any loss of accuracy in the quantization process by acknowledging at least two degrees of freedom in the network search space: at least one degree of freedom for varying a quantization scheme used to quantize one or more parameters of a layer within a given neural network model; and at least one degree of freedom for varying (e.g., decreasing or increasing) the size of the layer. In some embodiments, the size of a layer may correspond to the number of output units and/or filters included in the layer. In some embodiments, a neural network search may evaluate candidate neural networks, wherein an increased number of outputs and/or filters contained in the same and/or subsequent layers are used to recover details obscured by aggressive quantization.
The joint search system and method according to aspects of the present disclosure is in sharp contrast to past web searches, which fail to recognize the benefits of jointly searching multiple search spaces to balance the accuracy of tier values with the number of filters in a tier. In particular, past techniques have failed to understand the impact of quantization on the optimal number of filters in a layer, typically quantizing the model only to the last step after establishing layer parameters. For example, each filter of a layer in a deep learning network represents a cut plane that cuts the hyper-space through a nonlinear activation function, reducing the accuracy of the original input and parameter space may increase the number of filters needed to accurately represent the hyper-plane for some models. Furthermore, in some models, quantization may make (render) some filters redundant. For example, two different filters, e.g., with coefficients { -0.63,0.21} and { -0.15,0.79}, may represent the same hyperplane after, e.g., binary quantization. Thus, some filters may be removed after quantizing certain models without degrading the accuracy of the quantized models.
The system and method according to the present disclosure addresses the deficiencies of existing search methods by searching a network search space containing at least two subspaces: at least one subspace corresponding to a quantization scheme used to quantize one or more parameters of a layer within a given neural network model; and at least one subspace corresponding to a plurality of filters and/or a plurality of output units included in the same layer. In this way, the model can be quantified while taking performance into account. Advantageously, adjusting the number of output units and/or filters in conjunction with the quantization of the tier values may improve the trade-off between performance (e.g., accuracy) and energy consumption.
More specifically, while the amount of computation may increase with the number of output units and/or filters included in a layer, an example network architecture search according to the present disclosure may operate to reduce energy consumption of the model in view of both the amount of computation (e.g., MACs) and the computational cost per MAC. For example, the computational complexity required to process the layers of a neural network may vary greatly depending on the bit width of each number involved in the computation. In some embodiments, the energy cost of the collective operation as a function of the number of bits can be estimated by equation (1).
Energy (position) = a (position) 2 + b (bit) + c (1)
The coefficients a, b, and c Can be estimated from empirical data (e.g., data Can be collected by experiment and/or extracted from publications such as M.Horowitz, "1.1 computing" s Energy Problem (and What We Can Do About It), "2014IEEE int.solid-State circuitries Conf. Digest of Tech.papers (ISSCC), san Francisco, CA,2014, pp.10-14, doi. Example coefficients to accommodate the Horowitz data are presented in Table 1 in units of pJ/bit.
TABLE 1
a | b | c | |
Fixed dot adding | 0.0031 | 0 | |
Fixed dot product | 0.0030 | 0.0010 | 0 |
Floating point 16 addition | 0.4 | ||
Floating point 16 by 16 | 1.1 | ||
Floating point 32 addition | 0.9 | ||
Floating point 32 by | 3.7 | ||
SRAM access | 0.02455/64 | -0.2656/64 | 0.8661/64 |
DRAM access | 20.3125 | 0 |
In some examples, energy cost estimation, e.g., according to equation (1), may provide relative energy costs for comparing one or more neural networks or neural network layers. For example, in some examples, energy costs associated with operations common to neural networks and/or layers under comparison may be omitted from and/or ignored by the estimation method to only compare energy cost differences associated with differences between neural networks and/or layers under comparison.
In some examples, systems and methods according to the present disclosure reduce energy consumption of the model in view of both the amount of bits of the values and the cost of the necessary types of operations applied to the values by quantizing one or more values or sets of values (e.g., inputs to layers, weights, filters, and/or biases).
For example, if the two values (e.g., selected from weights, inputs, and/or offsets) are floating point numbers, then both multiplication and addition are performed in floating point. However, if both the input and output are binary, for example, multiplication may be implemented by a single XOR (exclusive or) gate, and addition may be implemented as increment/decrement logic, which is less computationally expensive than a normal MAC. In this way, for example, varying the quantization scheme may advantageously reduce the energy requirements of the operation, since multipliers typically have a quadratic energy consumption in terms of the number of bits, but other representations may have a linear behavior (which is even lower than the addition of a constant factor), such as in the case of XNOR (exclusive nor) AND (AND) operations.
In some examples, selecting a quantization scheme may correspond to selecting a value included in a byte (tuple) for quantizing a value. For example, floating point values are usually made of (-1) Sign bit (2 Exponent bit ) (mantissa bits) and the bits requiring an expression value may be represented as bytes (sign bits, exponent bits, mantissa bits). The quantization scheme may correspond to quantization bit-tuples that characterize the amount of bits (i.e., sign bits, exponent bits, and/or mantissa bits) assigned to each class. For example, the following quantization scheme may be expressed as bytes:
modified binary: in one example, the modified binary quantization scheme corresponds to a byte (0, c, 1), where the sign is assumed to be (-1) 0 . In one example, c =0 (zero bit is used to store the exponent) and the exponent is assumed to be 0 to represent values 0 and 1. In some examples, c =0, and the exponent is assumed to be a constant value to produce the desired scaling of the modified binary value. In a further example, the exponent may also be a constant stored in the c bits. In some examples, the constant index may be defined identically or differently for each of the one or more values quantized according to the modified binary quantization scheme. For example, each of one set of one or more inputs, outputs, weights, and/or filters may be quantized using one constant index (e.g., representable with one c-value), and each of another set of one or more inputs, outputs, weights, and/or filters (e.g., in another layer) may correspond to another constant index (e.g., representable with another c-value). In this way, the constant index may be used to scale one or more sets of quantized values, such as to provide a fixed and/or shared index among the one or more sets of values.
Binary system: in one example, the binary quantization scheme corresponds to a bit-byte (1, c, 0). In one example, c =0, the exponent is assumed to be 0, and the mantissa is assumed to be 1 (or-1) to represent values-1 and 1. As discussed above with respect to some embodiments of the modified binary quantization scheme, in some examples, c =0 and the exponent is assumed to be a constant value to produce the desired scaling of the binary value. In a further example, the exponent may also be a constant stored in the c bit. In some examples, the constant index may be defined identically or differently for each of the one or more values quantized according to a binary quantization scheme. For example, each of one set of one or more inputs, outputs, weights, and/or filters may be quantized using one constant index (e.g., representable with one c-value), and each of another set of one or more inputs, outputs, weights, and/or filters (e.g., in another layer) may correspond to another constant index (e.g., representable with another c-value). In this way, the constant index may be used to scale one or more sets of quantized values, such as to provide a fixed and/or shared index among one or more sets of values.
Ternary system: in one example, the ternary quantization scheme corresponds to a byte (-1, c, 1). In one example, c =0, and an exponent of 0 is assumed to represent values-1, 0, and 1. As discussed above with respect to some embodiments of binary quantization schemes and modified binary quantization schemes, in some examples, c =0 and the exponent is assumed to be a constant value to produce the desired scaling of the ternary value. In a further example, the exponent may also be a constant stored in the c bits. In some examples, the constant index may be defined identically or differently for each of the one or more values quantized according to a ternary quantization scheme. For example, each of one set of one or more inputs, outputs, weights, and/or filters may be quantized using one constant index (e.g., representable with one c-value), and each of another set of one or more inputs, outputs, weights, and/or filters (e.g., in another layer) may correspond to another constant index (e.g., representable with another c-value). In this way, the constant index may be used to scale one or more sets of quantized values, such as to provide a fixed and/or shared index among the one or more sets of values.
e-quant: in one example, a finger quantization ("e-quant") scheme corresponds to a representation having e bits, with bit-tuples (1, e-1, 0), where there are 1 bit for the sign and e-1 bit for the exponent, and the mantissa is assumed to be a fixed value, e.g., 1.
m-quant: in one example, a mantissa quantization ("m-quant") scheme corresponds to a representation with m bits, with 1 bit for sign, and m-1 bits for mantissa magnitude, where the exponent is assumed to be 0. The 1 bit for the symbol may correspond to a signed mantissa, such as a byte (1, 0, m-1), or in some examples, the additional bits used to represent the mantissa are represented in a two's complement, such as a byte (0, m).
When each of the two quantities subject to the (subject to) MAC is quantized with the same or different quantization scheme, the number of bits required for MAC calculation can be determined. For example, one calculation for the required number of bits follows the algorithm provided below, although any suitable algorithm or other method may be used to calculate or estimate the number of bits.
The algorithm above accepts, as an example, the quantized input qi and the quantized weight qw, and returns as size (# - > +), the number of bits required to complete one multiplication and accumulation operation.
The systems and methods of the present disclosure may employ any suitable quantization method or methods, depending on the application. For example, one type of value within a neural network may be quantized according to a first quantization scheme, while another type of value may be quantized according to a second quantization scheme. For example, the weights applied in the neural network layer may be quantized according to a first quantization scheme, the deviations of the layer may be quantized according to the first quantization scheme or, alternatively, according to a second quantization scheme, and the input to the layer may be quantized to the first or second quantization scheme, or, alternatively, to a third quantization scheme. The first, second and third quantization schemes may be the same or different, including all the same or all different. Further, each layer of the multi-layer neural network model may be quantized identically or differently, as desired.
In some examples, the selection of different quantization schemes offers different advantages. For example, m-quantization may supply increased accuracy for a given number of bits compared to e-quantization, but e-quantization may supply increased dynamic range for the same number of bits. In this way, for example, selecting different quantization schemes for one or more different values and/or layers may confer an advantage determined by the different roles of the different layers. Additionally or alternatively, the systems and methods of the present disclosure may use selection of different quantization schemes for one or more different values and/or layers to compensate for any performance characteristics (e.g., accuracy) that may otherwise degrade.
More advantageously, in some embodiments, the systems and methods of the present disclosure construct a network search space corresponding to the architecture of a given neural network model. While systems and methods according to the present disclosure may operate without such limitations as part of an expanded search space for many permutations of various neural network architectures, a carefully limited search space may yield satisfactory results in a shorter time. By constructing a search space corresponding to an existing neural network architecture, the search space can be constrained without limitation on the complexity of the neural network architecture subject to the optimization process.
For example, a neural network model may be selected for optimization. The network search space may be constructed to correspond to the number and configuration of layers of the selected model. For example, the network search space may include searchable subspaces corresponding to the size, type, and/or number of layers within the selected model. For example, for a given layer, the systems and methods of the present disclosure may define a first searchable subspace including values corresponding to a first quantization scheme for one or more values representing the layer (e.g., one or more values or types of values selected from inputs, weights, outputs, biases, activation functions, etc.). In one example, the values contained in the byte may be selected from one or more first searchable subspaces. For example, the selected (0, m) byte may correspond to an m-quant quantization scheme, where the value of m is selected from a searchable subspace. In another example, the selected byte of (1, e-1, 0) may correspond to an e-quant quantization scheme, where the value of e is selected from a searchable subspace. In another example, the searchable subspace may correspond to values corresponding to one or more of a binary, modified binary, ternary, floating point, or other quantitative quantization or representation scheme.
Multiple additional subspaces may also be defined as desired to correspond to the independently searchable quantization schemes used for multiple additional values within a quantization layer. In some embodiments, the network search space of a given layer may further include at least a second searchable subspace including values corresponding to the size of the layer. In some examples, the second searchable subspace may include values corresponding to quantities of filters included in a layer (e.g., in a convolutional layer). In some examples, the second searchable subspace may include values corresponding to quantities of output units contained in a layer (e.g., in a dense layer). In this way, each of the one or more layers may correspond to one or more searchable subspaces. Additionally or alternatively, one or more values contained within one or more layers may be collectively represented by a subspace.
In some embodiments, a given neural network model for optimization may contain multiple layers. Each layer may exhibit the same or different energy consumption or cost during execution and/or training as compared to other layers. In some embodiments, systems and methods according to the present disclosure may more aggressively reduce the energy cost of higher cost layers while maintaining higher accuracy and/or additional filters in lower cost layers. For example, in one embodiment, the multi-layer neural network model may be optimized by searching the network search space for each layer in order of decreasing energy cost of the layer. In this way, more aggressive energy savings can be employed at the beginning of the optimization process, while enjoying the flexibility to tune (tune) model performance by retaining higher accuracy and/or additional filters in cheaper layers.
The web search spaces of the layers of the multi-layer model may be searched in other suitable orders or in any order specified by the user. For example, a given multi-layer network model may have constraints that require that the number of filters in the downstream layer correspond to the number of filters in the upstream layer. In one example, the order may correspond to the order of layers from input to output of the model. In another example, the order may include various sub-orders. For example, the network search space may be ordered as a whole (order) in order of decreasing layer energy cost, unless constraints or dependencies would require alternating ordering among subsets of layers. In this way, the advantages of energy ranking ordering may be fully or partially realized while deliberately addressing the dependencies and complexity of a given neural network model.
When conducting a network architecture search in accordance with the systems and methods of the present invention, it may be desirable to characterize the overall improvement of a given neural network model in terms of one or more performance characteristics or metrics. In one example, the one or more performance characteristics include a score or metric that may be based on or reflect the amount of energy or bits. When there are two objectives in the optimization process, such as optimization parameter 1 and parameter 2, the single score that combines the weighting of each parameter can reflect the user's desired tradeoff between the optimization of the two parameters. For example, in some embodiments, a single score may take into account both the energy cost savings of systems and methods according to the present disclosure, as well as the retained (or improved) performance (e.g., validation accuracy) of the model. In one embodiment, the calculation of the score may include explicit terms corresponding to an acceptable reduction in model performance, which may be exchanged for a specified amount of energy savings. For example, one formula for a suitable score includes a scaling factor calculated according to equation (2).
In equation (2), the permitted level of lost performance p is expressed as a percentage, the target energy reduction r is expressed as a multiplicative factor, stress is a weighting parameter that shifts the function, the reference energy cost corresponds to the energy cost of the reference neural network model (or one or more layers thereof), and the candidate energy cost corresponds to the energy cost of the neural network model (or one or more layers thereof) compared to the reference. In this embodiment, this equation captures some aspect of the explicit trade-off that can be expressed in the problem, "if we reduce the energy of my model by r times (expressed by the ratio of the reference energy cost to the candidate energy cost), e.g., how much percentage p of the accuracy degradation i would tolerate? "
In some embodiments, the score is calculated differently based on the relative sizes of the candidates and the reference. For example, a first score may be calculated based on a first metric when the candidate model is smaller than the reference model, and a second score may be calculated based on a second metric when the candidate model is larger than the reference model. In some examples, the second metric is different from the first metric, such as a different method or calculation. In some examples, the second metric may include a modification to the first metric. For example, the first metric may be calculated with one value emphasized according to equation (2), and the second metric may be calculated with another value emphasized according to equation (2). In the same way, any of p, r or emphasised may vary between the first and second measures.
In calculating a score that accounts for both performance and energy costs (e.g., a model or layers within a model), the energy costs may be measured, predicted, or estimated as needed. For example, in some examples, the reference energy cost, the candidate energy cost, or both are measured when the respective model and/or layer is executed and/or trained on the target device. Thus, when the model and/or layer is deployed on a target device (e.g., a battery-powered device, such as a mobile device, an embedded device, and/or some other resource-constrained environment), the measurements may correspond to real-world energy costs. In other examples, the target device may be simulated or emulated by the host device, enabling estimation of real-world energy costs for executing and/or training the model and/or layers thereof on the target device. For example, the table given above may be used as a look-up table to directly calculate the energy cost of one or more model layers given the description of the one or more layers. Additionally or alternatively, an energy cost model (e.g., an algorithm as described above including estimating the number of bits required for one or more calculations) may be used to estimate or predict the energy cost. In one example, the energy cost model may be a differentiable function. One such example includes an energy cost model that employs a polynomial representation, such as shown in equation (1). In some examples, the energy cost may be estimated by the size of the one or more models being evaluated.
In some embodiments, the network search is part of an iterative search process that generates a new neural network model. For example, the controller model may be employed to generate candidate neural network models by modifying the reference neural network model according to one or more values selected from the network search space, such as from a first searchable subspace selection corresponding to a quantization scheme used to quantize the one or more values within the reference model, and from a second searchable subspace selection corresponding to a plurality of filters included in a layer within the reference model. The candidate models may share the same architecture as the reference model except for modifications made by the controller model based on values selected from the network search space. The candidate model may then be compared to the reference model, such as using a score assigned to the candidate model. The controller model may then repeat the generation of the candidate model until a desired score is reached or some other stopping criterion is met. In this way, for example, a new neural network model may be output based on the desired one or more performance metrics.
In some embodiments, the scores received by one or more candidate models are provided as feedback to the controller model to guide future selected values from the network search space. For example, the score may be used as part of a probabilistic search algorithm to search a network search space. As another example, in some embodiments, the controller model may include an reinforcement learning agent (agent). For each of the plurality of iterations, the computing system may be configured to determine the reward based at least in part on one or more evaluated performance characteristics associated with the candidate neural network model. In some embodiments, the reward is positively correlated with one performance characteristic of interest (e.g., accuracy) and negatively correlated with another performance characteristic of interest (e.g., energy cost). The controller model may then be updated based on the reward, such as by modifying one or more parameters of the controller model. In some embodiments, the controller model may include a neural network (e.g., a recurrent neural network). Accordingly, the controller model may be trained to modify the reference neural network model and/or the candidate neural network model in a manner that maximizes, optimizes, or otherwise adjusts the performance characteristics associated with the resulting candidate neural network model.
As another example, in an evolutionary scheme, the performance of a recently-proposed candidate may be compared to the best previously-observed performance from previous candidates to determine, for example, whether to retain the recently-proposed candidate or to discard the recently-proposed candidate and instead return the best previously-observed candidate. To generate the next iteration candidate, the controller model may perform an evolutionary variation on the candidates selected based on the comparison.
Embodiments of the present invention convey a number of technical advantages and benefits. As one example, the systems and methods of the present disclosure can generate energy-optimized and performance-optimized neural network models faster and use less computing resources (e.g., less processing power, less memory usage, less power consumption, etc.), for example, as compared to naive search techniques that search a network search space that includes many different configurations of neural network architectures. As another result, highly complex neural network architectures can be optimized by the systems and methods of the present disclosure without resorting to a large and cumbersome search space that requires a large amount of computational cost to search. As another result, the systems and methods of the present disclosure can generate (e.g., create and/or modify) new neural architectures that are more suitable for resource-constrained environments while maintaining satisfactory performance characteristics, e.g., as compared to search techniques that do not jointly search for both the amount of filters used for the layers and the degrees of freedom to quantify them. That is, the resulting neural architecture is able to run relatively faster and use relatively less computing resources (e.g., less processing power, less memory usage, less power consumption, etc.), while maintaining performance (e.g., accuracy) competitive with, or even exceeding, the performance of the current state-of-the-art model. Thus, as another example technical effect and benefit, the search techniques described herein may automatically find a much better model than existing approaches and achieve a new, most advanced trade-off between performance and energy cost/size.
Referring now to the drawings, example embodiments of the disclosure will be discussed in more detail.
Example model arrangements
Fig. 1 depicts an example system 100, the example system 100 configured to accept a reference neural network model 102 as input to a controller model 104 (e.g., the reference neural network 102 may be identified, selected from a predefined set of models, uploaded, or specified by a user). The controller model 104 may then search a network search space corresponding to the neural network architecture of the reference neural network model 102. For example, the controller model 104 may search a search space that includes a first searchable subspace corresponding to a quantization scheme used to quantize one or more values within a layer of the reference neural network model 102 and a second searchable subspace corresponding to a size of the layer (e.g., a number of filters and/or a number of output units within the layer). Based on the values selected from the searchable subspace, the controller model 104 may generate one or more candidate models 106 for evaluation by the performance evaluation subsystem 108. The performance evaluation subsystem 108 accepts one or more candidate models 106 to evaluate relative performance changes (e.g., including energy costs, accuracy, etc.) with respect to the reference model 102. In examples that include iterative searching, based on the comparison, the performance evaluation subsystem 108 may optionally provide feedback 110 to the controller model 104. In some examples, the feedback 110 may inform the controller model 104 that a satisfactory candidate model 106 has been generated and stop generating further candidates 106; the feedback 110 may inform the controller models 104 that certain candidate models 106 outperform (outperform) other candidate models 106, permitting the controller models 104 to conduct (engage in) a probabilistic search method to navigate the network search space; the feedback 110 may include rewards rewarding the controller model 104 to produce higher performance candidate models 106 so that the controller model 104 may employ reinforcement learning techniques to improve its search of the network search space.
As another example, in an evolutionary approach, the performance evaluation subsystem 108 may retain the candidate with the best previously observed performance in memory and compare the incoming candidate model 106 thereto. The performance evaluation subsystem 108 may then determine, for example, whether to retain or discard one or more of the recently proposed one or more candidate models 106. Based on feedback 110 received by the controller model 104 from the performance evaluation subsystem 108, the controller model 104 may perform evolutionary variation on candidate models selected based on the comparison.
In some implementations, the performance evaluation subsystem 108 can evaluate the performance of the candidate models 106 using pre-trained model values inherited from the reference neural network model 102 subject to modifications (e.g., quantifications) that the controller model 104 may have applied. In this manner, the performance evaluation subsystem 108 may quickly evaluate the candidate models 106 for comparison to the reference model 102. In other implementations, each candidate model 106 may be trained completely from scratch (e.g., without values inherited from a previous iteration or reference model 102).
In some implementations, the example system 100 may be configured as shown in fig. 2. Performance evaluation subsystem 108 may include a trainer 202, trainer 202 training one or more candidate models 106 to produce one or more trained candidate models 204. The trained model 204 may optionally be trained using the inherited trained values from the reference model 102 as seed values, or directly using the inherited trained values, or both. The trained model 204 may also be trained from scratch.
The trainer 202 may directly evaluate one or more performance characteristics of the trained candidate models 204. For example, the one or more performance characteristics 206 of the trained candidate model 204 may include a validation accuracy and/or an energy cost associated with training and/or execution of the one or more trained candidate models 204. For example, the energy cost may be calculated directly using one or more look-up tables or formulas that translate directly from model characteristics (e.g., number/type of operations and quantization scheme) to an energy cost value. Additionally or alternatively, the one or more trained candidate models may be passed to one or more real-world devices 208 (which may include simulations, and/or functional estimations or approximations thereof) for evaluating one or more performance characteristics 210. For example, the one or more performance characteristics 210 of the trained candidate models 204 may include a validation accuracy and/or an energy cost associated with training and/or execution of the one or more trained candidate models 204 on the real-world device 208.
The one or more performance characteristics 206 and/or the one or more performance characteristics 210 may be passed to a metric-computation model 212 for computing performance metrics, such as scores. In some examples, the performance metrics may include one or more performance characteristics 206, one or more performance characteristics 210, or some combination thereof, such as a combination calculated according to equation (2). In some embodiments, the performance metric is positively correlated with one performance characteristic of interest (e.g., accuracy) and negatively correlated with another performance characteristic of interest (e.g., energy cost). In some embodiments, metric-computation model 212 may pass through one or more performance characteristics 206 and/or one or more performance characteristics 210 unchanged. Feedback 112 may then be output from the metric computation model 212 to the controller model 104, and the controller model 104 may incorporate the feedback in any suitable manner, such as the configurations discussed herein.
In some embodiments, controller model 104 includes reinforcement learning agent 302, as shown in FIG. 3. Reinforcement learning agent 302 may operate in a reinforcement learning scheme to select values from a searchable subspace of a network search space to generate candidate neural network models 106. For example, at each iteration, controller model 104 may apply a policy to select values from the searchable subspace to generate candidate neural network models 106, and reinforcement learning agent 302 may update and/or notify the policy based on feedback 110 received by controller model 104. As one example, reinforcement learning agent 302 may include a recurrent neural network or any suitable machine learning agent. In one embodiment, the feedback 110 may include: other measures of reward or loss, remorse, etc. (e.g., for use in a gradient-based optimization scheme) based on one or more performance characteristics 206 and/or one or more performance characteristics 210 processed by metric-computation model 212, such as a score generated thereby. Example embodiments of the present disclosure may employ a gradient-based reinforcement learning approach to find a solution (e.g., pareto optimal solution) to a search problem (e.g., a multi-objective search problem). Reinforcement learning can be used because it is convenient and the reward is easily customized. However, in other embodiments, other search algorithms, such as an evolutionary algorithm, may be used instead. For example, the new candidate neural network model 106 may be generated by stochastic mutation.
In some embodiments, the one or more performance characteristics 206 and/or the one or more performance characteristics 210 may be evaluated using the actual task for which the reference neural network model 102 is being optimized or designed (e.g., a "real task"). For example, the one or more performance characteristics 206 and/or the one or more performance characteristics 210 may be evaluated using a set of training data to be used to train a result model that includes an optimized neural network model. However, in other embodiments, one or more performance characteristics 206 and/or one or more performance characteristics 210 may be evaluated using proxy tasks that have relatively short training times and are also related to real tasks. For example, evaluating performance characteristics using agent tasks may include: the real task is evaluated using a smaller training and/or validation data set (e.g., a downsampled version of the images and/or other data) than the real task and/or within a fewer time period (epoch) than would typically be used to train the model using the real task.
According to another aspect, in some implementations, the one or more performance characteristics 210 may include a real-world energy cost associated with implementing a new network structure on a real-world mobile device. More specifically, in some implementations, the search system can explicitly incorporate energy cost information (e.g., using a functional representation thereof, such as disclosed herein) into the primary objective, such that the search can identify models that achieve a good tradeoff between accuracy and energy cost. In some implementations, real world energy costs can be measured directly by executing a model on a particular platform (e.g., a mobile device such as a Google Pixel device). In further embodiments, various other performance characteristics may be included in the multi-objective function that guides the search process, including, by way of example, power consumption, user interface response, peak computation requirements, and/or other characteristics of the generated network model.
In some embodiments, the system 100 may evaluate the candidate models 106 in a constraint evaluation module 402, as shown in FIG. 4. In some examples, the constraint evaluation module 402 may be included in the controller model 104 and, additionally or alternatively, may be included in the performance evaluation subsystem 108 in some examples. Constraint evaluation module 402 can evaluate threshold determinations (e.g., dimensions and/or other compatibility concerns, etc.) for candidate models 106 and return constraint feedback 404 to controller model 104 before computationally expensive training in trainer 202 is performed. In this way, a threshold determination regarding performance may be performed before passing the candidate model 106 to the next stage. The controller model 104 may then incorporate constraint feedback 404 to better select values from the searchable subspace (e.g., using probabilistic or reinforcement learning methods) to satisfy the constraints.
In some embodiments, the performance evaluation subsystem 108 may include training data for training the candidate models 106, advantageously avoiding the transmission of training data between the controller models 104 and the trainer 202. For example, when the training data contains sensitive information (such as personal data, medical data, government data, or other such sensitive information), the performance evaluation subsystem 108 may perform tests locally using the sensitive data and transmit only the performance metrics as feedback 110 (which may include one or more performance characteristics 206, one or more performance characteristics 210, or some combination thereof, such as a score), which may advantageously maintain a high level of anonymity and/or other privacy measures around the training data used to evaluate the performance of the candidate model 106. In the same manner, the constraint evaluation module 402 can evaluate the candidate models 106 and return constraint feedback 104 locally prior to training without explicitly requiring that the constraints on the controller model 104 be exposed. Advantageously, preserving the configuration of the architecture of the reference neural network model 102 (subject to modification of the controller model 104) permits the performance evaluation subsystem 108 to readily accept changes thereto and retain much of the relevant expertise of the neural network model for optimally training the architecture when the performance evaluation subsystem 108 is a system that previously operated and/or trained the reference neural network model 102. For example, the performance evaluation subsystem 108 may include a system that is expected to be optimized for energy cost and/or performance of execution, but has been optimized in other aspects, including hyper-parameters of aspects of a policing (govern) network architecture. By preserving the configuration of the architecture of the reference neural network model 102, subject to modification of the controller model 104, systems and methods according to the present disclosure may retain any advantage of previous investments in optimizing the hyperparameters of the regulatory network architecture.
Example method
Fig. 5 depicts a flowchart of an example method to be performed in accordance with an example embodiment of the present disclosure. Although fig. 5 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the specifically illustrated order or arrangement. The various steps of the method 500 may be omitted, rearranged, combined, and/or adjusted in various ways without departing from the scope of the present disclosure.
At 502, a computing system may receive a reference neural network model. The reference neural network model may be received in any suitable manner, such as via transmission to or within a computing system, such as from a local or remote storage device or via a network communication channel.
At 504, the computing system may modify the reference neural network model to generate a candidate neural network model. The candidate neural network model may be generated by modifying the reference neural network model according to one or more values selected from the first searchable subspace and one or more values selected from the second searchable subspace. The first searchable subspace corresponds to a quantization scheme that quantizes one or more values of the candidate neural network model, and the second searchable subspace corresponds to a size of a layer of the candidate neural network model (e.g., a quantity of output units and/or filters included in the layer).
In some implementations, the computing system modifies the reference neural network model using the controller model at 504. In some examples, the controller model includes a reinforcement learning agent and/or a probabilistic search model.
At 506, the computing system may evaluate one or more performance metrics of the candidate neural network model. In some examples, the one or more performance metrics of the candidate neural network model include an estimated energy consumption of the candidate neural network model, and in some examples, the one or more performance metrics include a real-world energy consumption associated with implementation of the candidate neural network model on the real-world device.
In some embodiments, the method 500 includes outputting, from 506, a score based on the one or more performance metrics to a controller model of the computing system for iterative modification of the reference neural network model at 504. In an example iterative approach, the computing system may receive 506 the output and update the controller model based at least in part on the one or more performance metrics before outputting a new neural network model based at least in part on the one or more performance metrics (e.g., using an updated controller model), at 504. In some examples, the update may include a reward based at least in part on the one or more performance metrics.
In some examples, the one or more performance metrics include a scaling factor that is inversely related to a difference in energy expenditure between the candidate neural network model and the reference neural network model. In some examples, a scaling factor is applied to scale the accuracy metric.
Example apparatus and System
Fig. 6 depicts a block diagram of an example computing system 600 for optimizing a neural network model, according to an example embodiment of the present disclosure. It is contemplated that the systems and methods of the present disclosure may be implemented in a variety of suitable arrangements, including fully local applications running within one or more interconnected computing devices, and also including distributed computing systems that perform one or more portions of the methods disclosed herein on each of the one or more interconnected computing devices. Although fig. 6 depicts one example configuration of a computing system for operating the systems and methods of the present disclosure, it will be understood that other alternative configurations of computing devices remain within the scope of the present disclosure.
The example system 600 may include a server computing system 602, a network search computing system 620, and a performance evaluation computing system 640 communicatively coupled by a network 660. In some examples, system 600 may include a user computing device 670.
The server computing system 602 includes one or more processors 604 and memory 606. The one or more processors 604 may be any suitable processing device (e.g., processor cores, microprocessors, ASICs, FPGAs, controllers, microcontrollers, GPUs, neural network accelerators, etc.) and may be one processor or a plurality of processors operatively connected. The memory 606 may include one or more non-transitory computer-readable storage media, such as RAM, SRAM, DRAM, ROM, EEPROM, EPROM, flash devices, disks, and the like, and combinations thereof. The memory 606 may store data 608 and instructions 610 that are executed by the processor 604 to cause the server computing system 602 to perform operations.
In some implementations, the server computing system 602 includes or is otherwise implemented by one or more server computing devices. Where the server computing system 602 includes multiple server computing devices, such server computing devices may operate according to a sequential computing architecture, a parallel computing architecture, or some combination thereof.
The server computing system 602 may store or otherwise include one or more neural network models 612. For example, the one or more neural network models 612 may include a reference neural network model to be optimized in accordance with the present disclosure. The neural network models 612 may be uploaded to the server computing system 602 for storage thereon, and in some embodiments, the server computing system 602 hosts or otherwise operates one or more neural network models 612 in an application. In some implementations, the systems and methods can be provided as a cloud-based service (e.g., provided by the server computing system 602). The user may provide a pre-trained or pre-configured neural network model as the neural network 612.
The network search computing system 620 may receive information describing the neural network 612 from the server computing system 602. The network search computing system 620 can include one or more processors 622 and memory 624. The one or more processors 622 may be any suitable processing device (e.g., processor cores, microprocessors, ASICs, FPGAs, controllers, microcontrollers, GPUs, neural network accelerators, etc.) and may be one processor or a plurality of processors operatively connected. The memory 624 may include one or more non-transitory computer-readable storage media, such as RAM, SRAM, DRAM, ROM, EEPROM, EPROM, flash devices, disks, and the like, as well as combinations thereof. The memory 624 may store data 626 and instructions 628 for execution by the processor 622 to cause the network search computing system 620 to perform operations. In some implementations, the network search computing system 620 includes or is otherwise implemented by one or more server computing devices. The network search computing system 620 may be separate from the server computing system 602 or may be part of the server computing system 602.
The network search computing system 620 may also include a controller model 630 as described above with reference to fig. 1-4. As described above, the controller model 630 may receive information describing the neural network 612 and define the searchable subspace 632. The controller model 630 may be operable to select one or more values from the searchable subspace 632 to generate one or more candidate neural network models, wherein the candidate neural network models are generated by modifying the neural network received from the neural network 612 according to the selected values from the searchable subspace 632, as described above.
As described above with reference to fig. 2-4, the network search computing system 620 may communicate the one or more candidate neural network models to the performance assessment computing subsystem 640. The performance evaluation computing subsystem 640 includes one or more processors 642 and memory 644. The one or more processors 642 may be any suitable processing device (e.g., processor cores, microprocessors, ASICs, FPGAs, controllers, microcontrollers, GPUs, neural network accelerators, etc.) and may be one processor or a plurality of processors operatively connected. The memory 644 may include one or more non-transitory computer-readable storage media, such as RAM, SRAM, DRAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, as well as combinations thereof. Memory 644 may store data 646 and instructions 648 that are executed by processor 642 to cause performance evaluation compute subsystem 640 to perform operations. In some implementations, the performance assessment computing subsystem 640 includes or is otherwise implemented by one or more server computing devices. The performance evaluation computing subsystem 640 may be separate from the web search computing system 620 or may be part of the web search computing system 620.
The performance evaluation computation subsystem 640 may include a model trainer 650, the model trainer 650 training candidate models received from the network search computing system 620, and in some examples, the reference neural network 612 received from the server computing system 602. The model trainer 650 may employ various training or learning techniques, such as back-propagation of errors, for example. In some embodiments, performing back-propagation of the error may include performing truncated back-propagation through time. The model trainer 650 may perform a variety of generalization techniques (e.g., weight decay, drop, etc.) to improve the generalization (generalization) capability of the model being trained. The model trainer 650 may include computer logic for providing desired functionality. The model trainer 650 may be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some embodiments, model trainer 650 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other embodiments, model trainer 650 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium, such as a RAM hard disk or an optical or magnetic medium.
In particular, the model trainer 650 may train or pre-train one or more neural network models (e.g., candidate neural network models) based on the training data 652. The training data 652 may include labeled and/or unlabeled data. In some examples, training data 652 is stored locally on performance evaluation computing system 640. In some examples, training data 652 is accessed from a server computing system, such as server computing system 602, over network 660 (e.g., to inherit the pre-trained model data from neural network model 612).
In some examples, the performance evaluation computing system 640 evaluates one or more performance metrics associated with the trained candidate neural network model. For example, the performance evaluation computing system 640 may store the one or more trained candidate neural network models in the performance evaluation computing system memory 644 and then implement the trained candidate neural network models using the trained candidate neural network models or otherwise using the one or more processors 642. In some implementations, the performance evaluation computing system 640 may implement multiple parallel instances of the trained candidate neural network model. In this manner, the performance evaluation computing system 640 may evaluate one or more performance metrics, such as an accuracy metric and/or an estimated, simulated, and/or calculated energy cost metric associated with the trained candidate neural network model.
In some implementations, the training examples may be provided by the user computing device 670 if the user has provided consent (e.g., based on communications previously provided by the user of the user computing device 670). Thus, in such implementations, the model trainer 650 can be trained using user-specific communication data received from the user computing device 670. In some cases, this process may be referred to as personalizing the trained model.
The user computing device 670 may be any type of computing device, such as a personal computing device (e.g., a laptop or desktop computer), a mobile computing device (e.g., a smartphone or tablet computer), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 670 includes one or more processors 672 and memory 674. The one or more processors 672 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, GPU, neural network accelerator, etc.) and may be one processor or a plurality of processors operatively connected. Memory 674 may include one or more non-transitory computer-readable storage media, such as RAM, SRAM, DRAM, ROM, EEPROM, EPROM, flash devices, disks, and the like, as well as combinations thereof. Memory 674 may store data 676 and instructions 678 that are executed by processor 672 to cause user computing device 670 to perform operations.
The user computing device 670 may also include one or more user input components that receive user input. For example, the user input component may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other device through which a user may input communications.
The user computing device 670 may store or include one or more neural network models 680, and the one or more neural network models 680 may include one or more candidate neural network models generated by the network search computing system 620. In some implementations, the candidate neural network models can be received from the network search computing system 620 and/or the performance assessment computing system 640 over the network 660, stored in the user computing device memory 674, and then used or otherwise implemented by the one or more processors 672. In some implementations, the user computing device 670 can implement multiple, parallel instances of one or more neural networks 680.
In some examples, the neural network model 680 may be trained by the user computing device 670 using a model trainer and data 682. In this manner, real-world energy consumption or costs associated with training of the neural network model 680 may be calculated or measured on the user computing device 670. In some examples, neural network model 680 is trained and/or pre-trained by performance evaluation computing system 640 prior to being loaded onto user computing device 670. The user computing device 670 may then execute and/or apply the neural network 680 to evaluate one or more performance metrics, such as accuracy and/or energy cost metrics. For example, the user computing device may measure real-world energy costs associated with applying the trained neural network model 680 received from the performance assessment computing system 640.
The network search computing system 620 may receive feedback from the performance evaluation computing system 640 and/or the user computing device 670 (e.g., via the network 660). As described above with reference to fig. 1-4, the feedback may be used to update the controller model 630. For example, the controller model 630 may include a controller (e.g., an RNN-based controller) and a reward generator. The controller model 630 may cooperate with the model trainers 650 and/or 682 to train the controller 630. The network search computing system 620 and/or the performance evaluation computing system 640 may also optionally be communicatively coupled with various other devices (not specifically shown) that measure performance parameters of the generated network (e.g., a mobile phone replica that replicates the mobile phone performance of the network).
In some examples, each of the network search computing system 620 and the performance evaluation computing system 640 may be included in the server computing system 602, or otherwise stored and implemented by the server computing system 602, with the server computing system 602 communicating with the user computing device 670 according to a client-server relationship. For example, the functionality included by the network search computing system 620 and the performance evaluation computing system 640 may be provided as part of a network service (e.g., a neural network model optimization service).
Fig. 7 depicts a block diagram of an example computing device 700 that performs operations according to an example embodiment of the present disclosure. The computing device 700 may be, for example, any or all of the server computing system 602, the network search computing system 620, the performance evaluation computing system 640, and the user computing device 670.
As shown in fig. 7, each application may communicate with a plurality of other components of computing device 700, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., a public API). In some embodiments, the API used by each application is specific to that application.
Fig. 8 depicts a block diagram of an example computing device 800 operating in accordance with an example embodiment of the present disclosure. The computing device 800 may be, for example, any or all of the server computing system 602, the network search computing system 620, the performance evaluation computing system 640, and the user computing device 670.
The central smart inlay includes a number of machine learning models. For example, as shown in fig. 8, a respective machine learning model (e.g., model) may be provided for each application and managed by the central intelligence layer. In other embodiments, two or more applications may share a single machine learning model. For example, in some embodiments, the central smart inlay may provide a single model (e.g., a single model) for all applications. In some implementations, the central smart inlay is included within or otherwise implemented by the operating system of the computing device 800.
The central smart inlay may communicate with a central device data plane. The central device data layer may be a central data store of the computing device 800. As shown in fig. 7, the central device data layer may communicate with many other components of the computing device (e.g., such as one or more sensors, a context manager, a device state component, and/or additional components). In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
As one example, the systems and methods of the present disclosure may be included or otherwise employed within the context of an application, browser plug-in, or other context. Thus, in some embodiments, the models of the present disclosure may be included in, or otherwise stored and implemented by, a user computing device, such as a laptop, tablet, or smartphone. As yet another example, the model may be included in or otherwise stored and implemented by a server computing device that communicates with the user computing devices according to a client-server relationship. For example, the model may be implemented by a server computing device as part of a network service (e.g., a network email service).
Test results
The following example embodiments illustrate implementations of various aspects of the present disclosure.
For example, an energy-efficient neural network model may be desired that has a verification accuracy comparable to a reference model while using less energy. For example, a 2% drop in accuracy may be traded for using 3 times less energy. Following equation (2), the scaling factor for calculating the score may be calculated using p =2, r =3, and emphasis =1. The energy cost may be estimated by the size of the model (e.g., the number of parameters and the number of active bits). In one example, the reference energy cost =100000.
In some embodiments, a different scaling factor may be applied when the candidate neural network has a greater energy cost than the reference model than when the candidate neural network has a lower energy cost than the reference model. For example, the scaling factor may be plotted as shown in FIG. 9, where the scaling factor calculated above is applied when the model size is larger than the reference model size. When the model size is smaller than the reference model size, other parameters may be used to calculate the scaling factor, e.g., p =8, r =2, and emphasis =1.
In one example, the reference model may contain the following layers, where the layer name starting with "conv2d" corresponds to the convolutional layer, the layer name starting with "act" corresponds to the active layer, and the layer name "dense" corresponds to the last dense layer:
conv2d _0_ m filters (= 16 filter)
act0_m relu
conv2d_1_m filters＝32
act1_m relu
conv2d_2_m filters＝64
act2_m relu
dense outputs =10
act_output softmax
For clarity, other layers, such as BatchNormalization and Flatten, are not shown here. In this example, the reference model uses 8 bits for weight and activation and 16 bits for the accumulator.
When additional capability is added to search multiple filters while quantizing models, the KerasTuner package may be used as one example way to perform network searches according to the present disclosure. The KerasTuner package may perform a random, hyper-band, or bayesian (gaussian process) search of the hyper-parameter space, but without loss of generalization, the search may be performed by other mechanisms (e.g., using a reinforcement learning scheme). In one example, the main loop of the web search is performed as follows:
in this algorithm, two types of filter _ search (filter search) are allowed without loss of generalization: one is to perform a filter search on the entire block (or model) being searched, and the other is to adjust the number of filters for each layer. The function choose _ quantizer selects one quantizer from the quantizer library template, such as the quantizer provided by QKeras, for example. In some examples, different quantizers may be selected for one or more parameters of a layer containing the one or more parameters. For example, the above examples select quantizers for trainable parameters (e.g., weights, filters, and/or biases) within a layer, and the quantizers may be the same or different for one or more layers and/or one or more parameters within a layer. The functions quantize _ layer and quantize _ activation map layers to quantizing functions, while quantize _ model applies quantizing functions to reference models. The function chord _ range randomly selects a number between min _ range (minimum range) and max _ range (maximum range).
The function shaping factor (energy gain) refers to the scaling factor calculated according to equation (2) above. The fit function creates a hyper-model object and invokes a search process to return the best model found.
The winning searched model had a 74% energy cost reduction (similar to the size of the model), and the results of the trial are presented in fig. 10, where the results are ranked in descending order according to the calculated scores. The quantized and adjusted filter sizes are as follows:
note that the first two convolutional layers reduce the number of filters, but the number of filters of the last layer increases. This indicates that after quantization, the number of filters of the first two layers may become redundant, but the last layer requires more filters (with respect to the score, which corresponds to the accuracy metric and the scaling function, shaping _ factor, which includes an energy cost term, which can be estimated or approximated by a number of bits).
In some examples, the group search may be performed in which the groups are ranked (sort) by decreasing energy cost, and the web search is performed for each layer in the ranked order.
Here, the fit function creates groups of layers from the original model, sorts them in descending energy order, and searches for the best model group by group. However, the groups may be ordered in any desired or specified order, such as from input to output.
Additional disclosure
The techniques discussed herein make reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions among and between components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components operating in combination. The databases and applications may be implemented on a single system or may be distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Alterations, permutations, and equivalents of such embodiments may readily occur to those skilled in the art upon a reading of the foregoing description. Accordingly, the present disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Thus, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (20)
1. A computer-implemented method for quantifying a neural network model while considering performance, the method comprising:
receiving, by a computing system comprising one or more computing devices, a reference neural network model;
modifying, by a computing system, a reference neural network model to generate a candidate neural network model, wherein the candidate neural network model is generated by selecting one or more values from a first searchable subspace and selecting one or more values from a second searchable subspace, wherein the first searchable subspace corresponds to a quantization scheme used to quantize the one or more values of the candidate neural network model and the second searchable subspace corresponds to a size of a layer of the candidate neural network model;
evaluating, by the computing system, one or more performance metrics of a candidate neural network model; and
outputting, by the computing system, a new neural network model based at least in part on the one or more performance metrics.
2. The computer-implemented method of claim 1, wherein modifying, by the computing system, the reference neural network model to generate the candidate neural network model comprises:
selecting, by the computing system, one or more values from the first searchable subspace and one or more values from the second searchable subspace using a controller model.
3. The computer-implemented method of claim 2, wherein outputting, by the computing system, the new neural network model comprises:
updating, by the computing system, a controller model based at least in part on the one or more performance metrics; and
generating, by the computing system, the new neural network model using the updated controller model.
4. The computer-implemented method of claim 2 or 3, wherein the controller model comprises a reinforcement learning agent.
5. The computer-implemented method of any of claims 1-4, wherein the quantization scheme is selected from a binary, modified binary, ternary, exponential, and mantissa quantization scheme.
6. The computer-implemented method of any of claims 1-5, wherein the second searchable subspace corresponds to at least one of a quantity of output units and a quantity of filters.
7. The computer-implemented method of any of claims 1 to 6, wherein the one or more performance metrics comprise an estimated energy consumption of a candidate neural network model directly computed using one or more look-up tables or estimation functions.
8. The computer-implemented method of any of claims 1 to 7, wherein the one or more performance metrics include real-world energy consumption associated with implementation of a candidate neural network model on a real-world device.
9. The computer-implemented method of any of claims 2 to 8, wherein outputting, by the computing system, the new neural network model comprises:
determining, by the computing system, a reward based at least in part on the one or more performance metrics; and
modifying, by the computing system, one or more parameters of a controller model based on the reward.
10. The computer-implemented method of any of claims 2 to 8, wherein the controller model is configured to generate the candidate neural network model by performing evolutionary variation, and wherein modifying, by the computing system, the reference neural network model to generate a new neural network model comprises:
determining, by the computing system, whether to retain or discard the candidate neural network model based at least in part on the one or more performance metrics.
11. The computer-implemented method of any of claims 1 to 10, wherein the one or more performance metrics include a scaling factor that is inversely related to a difference in energy expenditure between the candidate neural network model and the reference neural network model.
12. The computer-implemented method of any of claims 2 to 11, wherein the reference neural network model comprises a plurality of layers, and wherein the method further comprises:
evaluating, by the computing system, an energy cost associated with each of two or more of the plurality of tiers;
modifying, by the computing system, each of the two or more of the plurality of tiers in an order determined by a descending order of energy costs associated with each of the two or more of the plurality of tiers.
13. The computer-implemented method of claim 12, wherein modifying, by the computing system, each of two or more of the plurality of layers comprises:
selecting, by the computing system, a first quantization scheme to quantize values within a first layer and a second quantization scheme to quantize values within a second layer, wherein the first quantization scheme is different from the second quantization scheme, and wherein the first layer is associated with a first energy cost that is higher than a second energy cost associated with the second layer.
14. A computing system, comprising:
one or more processors;
a controller model configured to modify the neural network model to generate a new neural network model; and
one or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more processors, cause the computing system to perform operations comprising:
receiving a reference neural network model as an input to the controller model;
modifying the reference neural network model to generate a candidate neural network model, wherein the candidate neural network model is generated by selecting one or more values from a first searchable subspace and selecting one or more values from a second searchable subspace, wherein the first searchable subspace corresponds to a quantization scheme used to quantize the one or more values of the candidate neural network model and the second searchable subspace corresponds to a size of a layer of the candidate neural network model;
evaluating one or more performance metrics of the candidate neural network model; and
outputting a new neural network model based at least in part on the one or more performance metrics.
15. The computing system of claim 14, wherein outputting the new neural network model comprises:
updating the controller model based at least in part on the one or more performance metrics; and
generating the new neural network model using the updated controller model.
16. The computing system of claim 14 or 15, wherein the one or more performance metrics comprise an estimated energy cost of the candidate neural network model.
17. The computing system of any of claims 14 to 16, wherein the one or more performance characteristics include real-world energy consumption associated with implementation of a candidate neural network model on a real-world device.
18. The computing system of any of claims 14 to 17, wherein updating the controller model based at least in part on the one or more performance characteristics comprises:
determining a reward based at least in part on the one or more performance characteristics; and
modifying one or more parameters of the controller model based on the reward.
19. The computing system of any of claims 14 to 18, wherein:
the quantization scheme is selected from binary, modified binary, ternary, exponential and mantissa quantization schemes; and is
The second searchable subspace corresponds to at least one of an amount of output units and an amount of filters.
20. One or more non-transitory computer-readable media storing instructions that, when executed by a computing system comprising one or more computing devices, cause the computing system to perform operations comprising:
receiving, by the computing system, a reference neural network model;
modifying, by the computing system, the reference neural network model to generate a candidate neural network model, wherein the candidate neural network model is generated by selecting one or more values from the first searchable subspace and selecting one or more values from a second searchable subspace, wherein the first searchable subspace corresponds to a quantization scheme used to quantize the one or more values of the candidate neural network model and the second searchable subspace corresponds to a size of a layer of the candidate neural network model; and
evaluating, by the computing system, one or more performance metrics of the candidate neural network model.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063034532P | 2020-06-04 | 2020-06-04 | |
US63/034,532 | 2020-06-04 | ||
PCT/US2021/035493 WO2021247737A1 (en) | 2020-06-04 | 2021-06-02 | Automatic selection of quantization and filter removal optimization under energy constraints |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115836298A true CN115836298A (en) | 2023-03-21 |
Family
ID=76601839
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180040159.4A Pending CN115836298A (en) | 2020-06-04 | 2021-06-02 | Automatic selection and filter removal optimization of quantization under energy constraints |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230229895A1 (en) |
EP (1) | EP4133416A1 (en) |
CN (1) | CN115836298A (en) |
WO (1) | WO2021247737A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
DE102022205547A1 (en) | 2022-05-31 | 2023-11-30 | Continental Automotive Technologies GmbH | Method for training a convolutional neural network |
-
2021
- 2021-06-02 CN CN202180040159.4A patent/CN115836298A/en active Pending
- 2021-06-02 WO PCT/US2021/035493 patent/WO2021247737A1/en unknown
- 2021-06-02 US US18/007,871 patent/US20230229895A1/en active Pending
- 2021-06-02 EP EP21734729.3A patent/EP4133416A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
EP4133416A1 (en) | 2023-02-15 |
US20230229895A1 (en) | 2023-07-20 |
WO2021247737A1 (en) | 2021-12-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11928574B2 (en) | Neural architecture search with factorized hierarchical search space | |
US9858534B2 (en) | Weight generation in machine learning | |
Couckuyt et al. | Fast calculation of multiobjective probability of improvement and expected improvement criteria for Pareto optimization | |
WO2020072274A1 (en) | Systems and methods for neural network pruning with accuracy preservation | |
JP6212217B2 (en) | Weight generation in machine learning | |
US11734568B2 (en) | Systems and methods for modification of neural networks based on estimated edge utility | |
CN113574327B (en) | Method and system for controlling an environment by selecting a control setting | |
CN105389454A (en) | Predictive model generator | |
CN115357554B (en) | Graph neural network compression method and device, electronic equipment and storage medium | |
US20220366257A1 (en) | Small and Fast Video Processing Networks via Neural Architecture Search | |
CN116684330A (en) | Traffic prediction method, device, equipment and storage medium based on artificial intelligence | |
CN109800138B (en) | CPU testing method, electronic device and storage medium | |
CN115836298A (en) | Automatic selection and filter removal optimization of quantization under energy constraints | |
US20230059708A1 (en) | Generation of Optimized Hyperparameter Values for Application to Machine Learning Tasks | |
US20220207368A1 (en) | Embedding Normalization Method and Electronic Device Using Same | |
CN114251075B (en) | Reservoir exploitation scheme determination method, device and equipment based on multi-target parameters | |
JP2024504179A (en) | Method and system for lightweighting artificial intelligence inference models | |
US20210365787A1 (en) | Pseudo-rounding in artificial neural networks | |
Pollard et al. | Supervised distance matrices | |
KR20230066700A (en) | Apparatus and method for generating adaptive parameters for deep learning accelerators | |
CN115329327A (en) | Method for selecting input horizontal energy analysis convolutional neural network and application | |
CN116579437A (en) | Quantum circuit training method and device, storage medium and electronic device | |
EP4154191A1 (en) | Pseudo-rounding in artificial neural networks | |
CN117520804A (en) | Feature screening method, device and equipment | |
CN116882457A (en) | Neural network quantization method, device, chip, electronic equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |