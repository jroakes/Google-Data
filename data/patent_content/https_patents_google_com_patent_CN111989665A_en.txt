CN111989665A - On-device image recognition - Google Patents
On-device image recognition Download PDFInfo
- Publication number
- CN111989665A CN111989665A CN201980025571.1A CN201980025571A CN111989665A CN 111989665 A CN111989665 A CN 111989665A CN 201980025571 A CN201980025571 A CN 201980025571A CN 111989665 A CN111989665 A CN 111989665A
- Authority
- CN
- China
- Prior art keywords
- entity
- model
- mobile device
- geographic location
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/587—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using geographical or spatial information, e.g. location
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/40—Document-oriented image-based pattern recognition
- G06V30/41—Analysis of document content
- G06V30/413—Classification of content, e.g. text, photographs or tables
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/40—Document-oriented image-based pattern recognition
- G06V30/42—Document-oriented image-based pattern recognition based on the type of document
- G06V30/422—Technical drawings; Geographical maps
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/38—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/387—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using geographical or spatial information, e.g. location
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training a second model to approximate an output of a first model, to classify image data received as input according to a classification scheme, and after training the second model, accessing map data specifying a plurality of geographic locations, and for each geographic location associated with an entity, for each of one or more images depicting the entity located at the geographic location, providing the image to the second model to generate a nest of images, associating each of the one or more nests generated by the second model with the geographic location, and storing the location data specifying the geographic location, the associated one or more nests, and the data specifying the entity in a database as associated entity entries for the entity.
Description
Background
This specification relates to training machine learning models that can be used on mobile devices for image recognition. For example, a user may desire additional information related to the content the user is viewing on a mobile device. Such content may be text or images. A user may desire to highlight an object or place of interest on the user device or some other feature that indicates information that may be useful to the user. For example, a user may encounter a restaurant and desire to know additional information about the restaurant.
The machine learning model may receive input and generate output based on the received input and parameter values of the model. For example, a machine learning model may receive an image and generate a score for each category in a set of categories, the score for a given category representing a probability that the image contains an image of an object belonging to the category.
The machine learning model may consist of, for example, a single level of linear or non-linear operations, or may be a deep network, i.e., a machine learning model consisting of a convolutional neural network. An example of a deep network is a neural network with one or more hidden layers. Neural networks are machine learning models that use layers of nonlinear units to predict outputs for received inputs. Some neural networks are deep neural networks that include a hidden layer in addition to an output layer. The output of each hidden layer serves as the input to the next layer in the network. Each layer of the network generates an output from the received input in accordance with current values of a corresponding set of parameters.
Disclosure of Invention
This specification describes technologies relating to systems and methods for image recognition on mobile devices. The present subject matter provides embodiments of a recognition model that allows for limited processing and storage capabilities of a mobile device, particularly when compared to larger back-end computing devices, and is efficient in network usage.
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of: accessing a first model that has been trained to generate an output that classifies image data received as input according to a classification scheme; training a second model using the first model to approximate an output of the first model to classify the image data received as input according to a classification scheme, wherein the second model comprises a plurality of connected layers including an output layer, an input layer, and a plurality of intermediate layers; and after training the second model: accessing map data specifying a plurality of geographic locations, wherein each geographic location of at least a subset of the geographic locations is associated with an entity located at the geographic location and one or more images depicting the entity at the geographic location; for each geographic location associated with an entity: for each of one or more images depicting an entity located at the geographic location, providing the image to a second model to generate a nesting of the images, wherein a nesting is data generated by one of the intermediate layers; associating each of the one or more nests generated by the second model with the geographic location; and storing the location data specifying the geographic location, the associated one or more nests, and the data specifying the entity in a database as an associated entity entry for the entity. Other embodiments of this aspect include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
These and other embodiments may each optionally include one or more of the following features. In some aspects, the method may further comprise: receiving a request from a mobile device for an entity entry in a database, the request including location data specifying a geographic location of the mobile device; and selecting a proper subset of the entity entries in the database based on the geographic location of the mobile device, wherein each entity entry in the proper subset of entity entries has location data determined to satisfy a proximity threshold indicating that the geographic location of the entity is determined to be proximate to the geographic location of the mobile device.
In some aspects, the proximity threshold is a value that specifies a geographic distance. In some aspects, the proximity threshold is a value that specifies a predefined area that includes the geographic location of the mobile device.
In some aspects, the data specifying the entity includes an entity description describing the entity and an entity name of the entity. In some aspects, the entity is a business entity. In some aspects, the entity is a landmark entity.
In some aspects, the second model is a convolutional neural network.
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods implemented on a mobile device, and the methods include the actions of: storing, on a mobile device, a first model that has been trained to approximate an output of a second model that classifies image data received as input according to a classification scheme, wherein the first model includes a plurality of connected layers including an output layer, an input layer, and a plurality of intermediate layers; sending, to a computer system external to the mobile device, a request for entity entries in a database managed by the computer system, the request including location data specifying a geographic location of the mobile device, wherein each entity entry includes: location data specifying a geographic location; data specifying an entity located at a geographic location; one or more nests generated by the first model at the computer system, each of the one or more nests generated from one or more images received as input to the first model and each of the images being an image depicting an entity at the geographic location, and wherein each nest is data generated by one of the intermediate layers; receiving, from the computer system, a proper subset of the entity entries in the database, the proper subset selected based on the geographic location of the mobile device, and storing the proper subset of the entity entries on the mobile device; capturing an image by the mobile device and providing the image as input to the first model to generate a captured image nest; determining a set of matching nests in the proper subset of entity entries, each matching nest being a nest determined to match the captured image nest according to a matching criterion; selecting entity entries as matching entity entries in the proper subset of entity entries based on the matching nested set; and providing display data describing the entities matching the entity entry on a display device of the mobile device. Other embodiments of this aspect include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
These and other embodiments may each optionally include one or more of the following features. In some aspects, based on the nested set of matches, selecting an entity entry in the proper subset of entity entries as a matching entity entry comprises: for each entity entry having a matching nest, determining a distance value based on location data of the entity entry and location data of the mobile device; and selecting the entity entry having the smallest distance value relative to the distance values of other entity entries having matching nests.
In some aspects, providing display data describing entities matching the entity entry includes: selecting a display position on the captured image; and displaying the display data on the captured image at the display position.
In some aspects, selecting a display location on the captured image comprises: determining from the captured image nest an activation value that is the highest value relative to other active values in the captured image nest; and selecting a display position corresponding to the activation value in the captured image nest.
In some aspects, the captured image is one of a plurality of image frames captured by the mobile device, and for the plurality of image frames: the method includes determining a set of matching nests in a proper subset of entity entries, selecting a display location corresponding to an activation value in a captured image nest, and displaying display data on a captured image at the display location.
In some aspects, the captured image is one of a plurality of image frames captured by the mobile device; and for the plurality of image frames: the method includes determining a matching nested set of a proper subset of entity entries, selecting an entity entry in the proper subset of entity entries as a matching entity entry, and providing display data describing the entity matching the entity entry.
In some aspects, the method may further comprise: receiving, from the computer system, a further proper subset of the entity entries in the database, the further proper subset selected based on the updated geographic location of the mobile device, and storing the further proper subset of the entity entries on the mobile device; capturing, by the mobile device, a further image and providing the further image as an input to the first model to generate a further captured image nest; determining a set of nesting matches in the further proper subset of entity entries, each nesting match being a nest determined to match the further captured image nesting according to a matching criterion; selecting additional entity entries in an additional proper subset of entity entries as additional matching entity entries based on the nested set of matches; and providing display data describing the entities of the additional matching entity entries on a display device of the mobile device.
In some aspects, the method may further comprise: when it is determined that the mobile device is in the updated geographic location, a proper subset of the entity entries are deleted from the mobile device.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The present subject matter provides embodiments of robust image recognition machine learning models that allow for limited processing and storage capabilities of mobile devices, especially when compared to larger back-end computing devices.
Further, the present subject matter provides an implementation that is efficient in terms of network usage, as the trained machine learning model is deployed on the mobile device using a subset of nested databases that are downloadable to the mobile device at one time. This embodiment allows a user to use an image recognition application without having to access a server for each image over a network, as long as the user is located within a location area associated with a subset of the nested databases. Thus, by using a subset of nested databases based on the location of the mobile device, such as a defined geographic portion of a city or area (e.g., a predetermined unit of X Y meters, etc.), the system reduces the number of processing cycles and reduces the number of times additional data must be downloaded from the server. This reduces the required processing resources and reduces the overall system bandwidth requirements (or alternatively, can serve a greater number of users without a corresponding increase in processing resources). Thus, such an improvement in the art of search processing is another significant advantage realized by the systems and methods described below.
In addition, trained student models (student models) are easier to deploy on mobile devices than teacher models (teacher models) because generating output at runtime requires less computation, memory, or both computation and memory than teacher machine learning models. Once trained using the teacher model, the student model may generate output that is not significantly less accurate than the output generated by the teacher machine learning model, although it is easier to deploy or use fewer computing resources than the teacher machine learning model.
In some implementations, the system can receive image data (single image, continuous video, etc.) and user input requesting identification of items of interest in the image data on the mobile device, determine whether objects of interest are present within the image data, and generate display data to display those objects of interest on the user device. The display data may include any type of visual or auditory feedback to inform the user that the object of interest is in the image data. Additionally, the system may provide additional information to the user regarding the identified object.
In some embodiments, the system allows more frames to be processed on the mobile device to reduce the bandwidth and server resources used during the image recognition process. Performing image recognition at the mobile device may also result in higher accuracy, as there are more attempts to recognize objects from different viewpoints.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is an example machine learning model training system.
FIG. 2 is a block diagram of an example environment in which a process of generating nested entity databases from map data using trained machine learning models may occur.
FIG. 3 is a block diagram of an example environment in which an image recognition process may occur on a mobile device using a subset of image nesting databases.
FIG. 4 is a sequence of example screen shots of a mobile device presenting display data describing entities of matching entity entries.
FIG. 5 is a flow diagram of an example process for training and using machine learning models for image recognition.
FIG. 6 is a flow diagram of an example process for using a machine learning model for an image recognition process on a mobile device.
FIG. 7 is a block diagram of an example computing device that may be used to implement the methods, systems, and processes described in this disclosure.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
Systems, methods, and computer program products for on-device image recognition for mobile devices are described. Example mobile devices include personal computers, mobile communication devices, and other devices that can send and receive data over a network. The present subject matter addresses the technical challenges of providing and implementing a robust image recognition model on a mobile device for images captured by the mobile device, wherein the image recognition model takes into account the geographic location of the user device as part of the image recognition process. The recognition model allows for limited processing and storage capabilities of the mobile device, especially when compared to the processing and storage capabilities of larger back-end computing devices, and is efficient in terms of network usage. As described below, the subject matter can relate to providing a proper subset of a large image nesting database to a mobile device based on the location of the mobile device. This may involve providing a subset of nested databases based on comparing the geographic location of the mobile device to geographic locations stored in the databases and providing only entries of the databases that satisfy a proximity threshold. The subject matter may also relate to distillation, or any other machine learning process that generates a compressed model that simulates or approximates the output of a larger model, so that the compressed model may be stored and implemented on the mobile device without undesirable processing effects due to the limited capabilities of the mobile device.
To generate the on-device model, the system accesses a first model that has been trained to generate an output that accurately classifies image data received as input according to a classification scheme. Using the first model, the system trains a second model ("on-device model") to approximate the output of the first model, thereby classifying the image data received as input according to a classification scheme. The second model includes a plurality of connected layers including an output layer, an input layer, and a plurality of intermediate layers. The second model may have fewer layers, fewer convolution filters, or fewer nodes per layer, and occupy less space in computer memory than the first model. As explained further below, the image data received as input to the second model may include data from images captured by a camera of the device on which the second model is stored, such that the device may perform one or more inference steps using the second model to accurately identify aspects of the images without sending the image data over a data network for identification by a back-end computing system.
After training the second model, the system accesses map data that specifies a plurality of geographic locations. Each geographic location in at least a subset of the geographic locations is associated with an entity located at the geographic location and one or more images depicting the entity at the geographic location. The entity may be a store, a building, a landmark, or any other identifiable physical, real-world entity.
For each geographic location associated with an entity, for each image depicting the entity located at that geographic location, the system provides that image to the second model to generate a nesting of the images. Nesting is the data generated by one of the middle layers. The system then associates each of the nests generated by the second model with a geographic location and stores location data specifying the geographic location, the associated one or more nests, and data specifying the entity in a database as associated entity entries for the entity.
The second model is then provided to the mobile device. The mobile device can then identify the entity in a lightweight (lightweight) but robust manner using the second model. More particularly, the system can receive a request for an entity entry in a database from a mobile device. The request may include location data specifying a geographic location of the mobile device. The system selects a proper subset of the entity entries in the database based on the geographic location of the mobile device. Each entity entry in the proper subset of entity entries has location data determined to satisfy a proximity threshold indicating that the geographic location of the entity is determined to be proximate to the geographic location of the mobile device.
The mobile device receives a proper subset of the entity entries in the database and stores it, for example, in a cache. The mobile device captures an image, e.g., a user takes a picture of the storefront, and provides the image as input to the second model to generate a captured image nest. The mobile device then determines a nested set of matches in the proper subset of entity entries. Each matching nest is a nest determined to match the nest of captured images according to the matching criteria. The mobile device selects an entity entry in the proper subset of entity entries as a matching entity entry based on the nested set of matches and then provides display data describing the entity of the matching entity entry on a display device of the mobile device.
When two or more entity entries have matching nests, such as where entity data for two different stores are stored at the mobile device, the mobile device may select the closest entity entry for display. For example, for each entity entry having a matching nest, the mobile device can determine a distance value based on location data of the entity entry and location data of the mobile device. The mobile device then selects the entity entry having the smallest distance value relative to the distance values of other entity entries having matching nests.
These features and additional features are described in more detail below.
FIG. 1 is a block diagram of an example machine learning model training system 100. The machine learning model training system 100 is an example of a system of computer programs implemented on one or more computers in one or more locations in which the systems, components, and techniques described below are implemented.
The machine learning model training system 100 trains a second machine learning model 120 using the trained first machine learning model 110. In general, a machine learning model receives input and generates output based on the received input and parameter values of the model. According to some embodiments, one way to create the trained second model is by distillation (distillation). According to some embodiments, other machine learning processes for generating the second model using a neural network may be used.
In particular, the second machine learning model 120 and the trained first machine learning model 110 are both machine learning models that have been configured to receive input and process the received input to generate a respective score for each category in a predetermined set of categories. Both the first machine learning model 110 and the second machine learning model 120 include a plurality of connected layers. The connection layer includes an output layer, an input layer, and a plurality of intermediate layers. In general, the second machine learning model 120 is a model having a different architecture than the first machine learning model 110, e.g., making the second machine learning model 120 easier to deploy than the first machine learning model 110 because the second machine learning model 120 requires less computation, memory, or both computation and memory to generate output at runtime than the first machine learning model 110. For example, the second machine learning model 120 may have fewer layers, fewer parameters, or both fewer layers and parameters than the first machine learning model 110.
The trained first machine learning model 110 has been trained on a set of training inputs using conventional machine learning training techniques to determine trained values for parameters of the first machine learning model 110. In particular, the trained first machine learning model 110 has been trained such that the score generated by the trained first machine learning model 110 for a given input for a given category represents the probability that the category is an accurate classification of the input.
For example, if the input to the first machine learning model 110 is an image, the score for a given category may represent the probability that the input image contains an image of an object belonging to that category. In a particular example, the first machine learning model 110 can determine whether an image depicts an object classified into one or more of the following categories: buildings (e.g., restaurants, businesses, etc.), text, bar codes, landmarks, media objects (e.g., album covers, movie posters, etc.), or art objects (e.g., paintings, sculptures, etc.). As another example, if the input to the first machine learning model 110 is a text segment, the category may be a topic and the score for a given topic may represent a probability that the input text segment is related to the topic.
According to some embodiments, the first machine learning model 110 is a single machine learning model. In some other cases, the first machine learning model 110 is an integrated (ensemble) machine learning model, which is a compilation (compilation) of multiple individual machine learning models that have been trained individually, wherein the outputs of the individual machine learning models are combined to generate the output of the first machine learning model 110. Further, in some cases, the models in the integrated machine learning model include one or more full models (full models) that generate scores for each of the categories, and one or more specialized models (specialized models) that generate scores for only a respective subset of the categories.
The model training system 100 trains the second machine learning model 120 over a set of training inputs to determine trained values for parameters of the second machine learning model 120 such that a score generated by the second machine learning model 120 for a given input for a given class also represents a probability that the class is an accurate classification of the input.
In particular, to train the second machine learning model 120, the model training system 100 configures both the second machine learning model 120 and the first machine learning model 110 to generate soft outputs from training inputs during training of the second machine learning model 120.
The soft output of the machine learning model for a given input includes a respective soft score generated by a last layer of the machine learning model for each of the categories. The soft scores define a softer score distribution over the input set of classes than the original training data.
During training, the model training system 100 processes the training inputs 102 using the first machine learning model 110 to generate target soft outputs 112 of the training inputs 102. The model training system 100 also processes the training input 102 using the second machine learning model 120 to generate a soft output 122 of the training input 102. The model training system 100 then trains the second machine learning model 120 by adjusting parameter values of the second machine learning model 120 to generate a soft output 122 that matches the target soft output 112 of the training input 102.
The model training system shown in FIG. 1 is only one example model training system 100 for training a distillation machine learning model. Other machine learning processes may be used to train the second model. In some embodiments, image recognition may be performed by using multiple types of models or signals. For example, both nesting and Optical Character Recognition (OCR) are used.
The trained second machine learning model 120 is then used to generate an entity database from the map database. Such generation of this data is described in more detail below with reference to fig. 2, fig. 2 being a block diagram of an example environment 200 in which a process generates a nested database from map data using a second machine learning model in this example environment 200. The example environment 200 includes a map database 210, the trained second machine learning model 120 of FIG. 1, a database generator 220, and an entity database 230.
The map database 210 is a data store (data store) that includes geographic location data, entity data, and image data. The geographic location data may be GPS coordinates, map coordinates (e.g., latitude and longitude), or any other location information that the system may identify. The entity data may include entity names and other metadata (e.g., comment scores, place types, menus, etc.). The image data may be one or more single frame images, continuous video, a stream of images, etc. from a camera of the mobile device. Image data (also referred to as reference images) may be captured by several users and raters (e.g., social media raters at a restaurant). Additionally, image data may be extracted from a street view database that provides a panoramic view from locations along many streets in the world.
According to some aspects, the map data specifies a plurality of geographic locations, wherein each geographic location of at least a subset of the geographic locations is associated with an entity located at the geographic location and one or more images depicting the entity at the geographic location. For example, the entities may be stores, buildings, landmarks, or any other identifiable physical, real-world entity, and the map data specifies the location of the respective identified entity in the image. According to some aspects, several images in the map data may be associated with one respective entity.
The second machine learning model 120 accesses the map database 210, and the second model 120 accesses images in the database 210 to generate an image nest 215 for each accessed image. Image nesting 215 is data generated by one of the middle layers (e.g., the last layer before the output layer) of the second machine learning model 120. Of course, different intermediate layers may also be used for nesting. The database generator 220 may then create an entity database 230 from the generated image nest 215.
The entity database 230 is a data store that may include geographic location data and entity data of the map database 210 and image nests 215 generated by the second model 120. According to some aspects, the location data, entity data, and image nest 215 are associated with respective entities. According to some aspects, several image nests 215 may be associated with a respective entity. For example, if a particular entity, a particular coffee shop at a particular location, has dozens of storefront images of the coffee shop at that location stored in the map database, a respective nest may be created for each storefront image. In some implementations, when there are several image nests 215 associated with one corresponding entity (e.g., identifying cascading restaurants that tend to share similar features), an average nest can be used.
Once the entity database 230 is created, it may be provided to the mobile device along with the second model. However, due to the size of the database 230, only a portion, i.e., a proper subset of the database 230, is provided to the mobile device. The particular portion provided depends on the location of the mobile device. The access and use of a subset of the entity database 230 by a mobile device is described in more detail below with reference to fig. 3.
FIG. 3 is a block diagram of an example environment 300 in which an image recognition process may occur on a mobile device using a subset of an image nesting database. A computer network 302, such as a Local Area Network (LAN), Wide Area Network (WAN), the internet, or a combination thereof, connects the server 320 and the mobile device 310.
The server 320 may represent a combination of application servers, database servers, communication servers, web servers, etc., that include a mobile platform provider's system for collecting data from, controlling, and managing applications and modules used on the various mobile devices 310 described herein. The server 320 may access the entity database 230 to provide the mobile device with a subset of the entity entries in the database based on the geographic location of the mobile device.
The mobile device 310 can use the application to present media. Media is some combination of images, video, audio, text, or the like that a user consumes using an application running on the mobile device 310. A web browser may enable a user to display and interact with text, images, video, music, and other information on web pages, typically located at web sites on the world wide web or local area network.
The mobile device 310 may include a trained on-device machine learning model, such as the second machine learning model 120. The mobile device 310 may also run various applications. The application on the mobile device 310 may include an application environment, e.g., a Graphical User Interface (GUI), in which images may be shown. Examples of such applications are camera-enabled applications that can capture images using an on-device camera, applications that can receive images from publishers through network 302 and display images, and applications that can access and display images stored on mobile device 310. For example, an application may access a repository (repository) of image data stored on the mobile device 310, where the application environment may load images from the image data. Further, the application may facilitate the use of the second machine learning model 120.
According to some embodiments, the mobile device 310 requests a subset of the entity entries from the server 320. The mobile device 310 can receive a subset of the entity entries and store them in the entity database subset 232 (e.g., in on-device memory).
According to some embodiments, the mobile device 310 captures an image, e.g., a user takes a picture of a storefront, and provides the image as input to the second model 120 to generate a captured image nest. The mobile device 310 can determine a nested set of matches in the subset of entity entries. Each matching nest is a nest determined to match the nest of captured images according to the matching criteria. The mobile device selects an entity entry in the subset of entity entries as a matching entity entry based on the nested set of matches and then provides display data describing the entity of the matching entity entry on a display device of the mobile device 310. An example of display data on a display device of the mobile device 310 is described in more detail below with reference to fig. 4.
FIG. 4 depicts a sequence of example screen shots 410 and 420 of a mobile device presenting display data describing entities matching entity entries. The first screen shot 410 depicts an example user interface 412 of a mobile device, such as the mobile device 310. In this example, the user is viewing the entity 414, the coffee shop, through a viewfinder of a camera presented in the user interface 412. The user interface 412 may be generated and presented by an application on the mobile device.
The second screen shot 420 presents an example user interface 422 with display data 430 describing the identified entity 414. In this example, the user interface 422 presents the display data 430 as a graphical overlay over the current view of the user interface 422. In particular, the display data 430 presents a graphical overlay over a portion of the identified entity 414, which identified entity 414 is a COFFEE shop named "ACME COFFEE" in this example. Information about the identified entity 414 is presented to the user in display data 430. The display data 430 may include, for example, but not limited to, the identified name of the entity 414, social ratings from other users (if applicable), average price of items sold at the entity 414, address, business hours, website links, or any other information related to the entity.
The mobile device 310 can identify the entity 414 based on the geographic location of the mobile device 310 by matching the nesting of a subset of entity entries from, for example, the entity database 230, with the nesting of captured images from the entity 414. The recognition process implements the second model 120 on the mobile device 310 using the processes described in more detail above and below with reference to fig. 5 and 6.
FIG. 5 is a flow diagram of an example process 500 for training and using machine learning models for image recognition.
The process 500 trains a second model to approximate the output of the first model to classify the image data received as input according to a classification scheme (504). For example, as shown in fig. 1, the second machine learning model 120 may be trained to approximate the target soft output 112 of the first machine learning model 110 as a soft output 122. According to some embodiments, the second model is a convolutional neural network.
After training the second model, the process 500 accesses map data that specifies a plurality of geographic locations (506). In some implementations, each geographic location in at least a subset of the geographic locations is associated with an entity located at the geographic location and one or more images depicting the entity at the geographic location. For example, as shown in fig. 2, the second machine learning model 120 accesses map data from a map database 210.
For each geographic location associated with the entity, the process 500 provides each image depicting the entity located at the geographic location to the second model to generate a nesting of the images (508). According to some embodiments, the nesting is data generated by one of the middle layers of the second model. For example, as shown in fig. 2, the second machine learning model 120 will generate an image nest 215 from the map data of the map database 210 that depicts each of the one or more images of the entity located at the geographic location.
For each geographic location associated with the entity, process 500 associates each of the nests generated by the second model with a geographic location (510). For example, as shown in fig. 2, the database generator 220 receives the image nest 215 generated from the second machine learning model 120, as well as the location data and the entity data from the map database 210 to generate the entity database 230.
For each geographic location associated with an entity, the process 500 stores location data specifying the geographic location, an association nest, and data specifying the entity as an associated entity entry for the entity (512). For example, as shown in fig. 2, the entity database 230 receives the image nest 215 generated from the second machine learning model 120, as well as the location data and entity data from the map database 210, and stores respective data for each geographic location associated with the entity.
According to some embodiments, the data specifying the entity includes an entity description describing the entity and an entity name of the entity. For example, as shown in FIG. 4, display data 430 describes entity 414, including the name of entity 414. According to some embodiments, the entity is a business entity, such as entity 414. According to some embodiments, the entity is a landmark entity. For example, the captured image of the mobile device may be a statue in a city that the user of the mobile device is unfamiliar with or that the user may want additional information about. Thus, the data specifying the entity may specify the name of the statue as well as additional information about the statue.
After the second model is trained and the second model has generated a nest that depicts each of one or more images of an entity located at a geographic location, the process 500 receives a request for an entity entry (514). According to some embodiments, the request includes location data specifying a geographic location of the mobile device. For example, as shown in fig. 3, the mobile device 310 sends a request to the server 320 for a subset of the entity entries in the entity database 230. The request includes a geographic location of the mobile device.
The process 500 selects a proper subset of the entity entries in the database based on the geographic location of the mobile device (516). According to some embodiments, each entity entry of the subset of entity entries has location data determined to satisfy a proximity threshold indicating that the geographic location of the entity is determined to be proximate to the geographic location of the mobile device. For example, location information from a mobile device (e.g., GPS) is located in a particular city so that the server 320 can provide a subset of the entity database for the particular city to limit the size of data needed by the mobile device.
In some embodiments, predefined cell data (cell data) may be used to further limit the size of the subset of entity entries. For example, some cities may contain thousands of identifiable entities, where the size of the data of the requested subset may be too large or cumbersome for the mobile device to download. Thus, the data is divided into predefined unit areas. In some embodiments, a cell is a mathematical mechanism that helps a computer transform the spherical 3D shape of the earth into a 2D geometry and may assist in dividing a large area, such as a large city, into several organized cells. In some embodiments, other known ways of dividing geographic location areas may be used.
According to some embodiments, the proximity threshold is a value specifying a geographic distance. According to some embodiments, the proximity threshold is a value that specifies a predefined area that includes the geographic location of the mobile device. For example, the proximity threshold may be a distance determined by a radial distance outward from the location of the phone, creating a circle of location data relative to the location of the mobile device. When a user or an application on the mobile device requests a new subset of entity entries, the data subset may be nested data of a circular area surrounding the location of the mobile device.
FIG. 6 is a flow diagram of an example process 600 for using a machine learning model for an image recognition process on a mobile device.
The process 600 stores a first model on a mobile device that has been trained to approximate an output of a second model that classifies image data received as input (602). The image data may be classified according to a classification scheme. For example, as shown in fig. 1, the second machine learning model 120 may be trained to approximate the output of the first machine learning model 110, where the first machine learning model 110 may be trained to score for a given class, and the score may represent the probability that an input image contains an image of an object belonging to that class. Note that the first model of process 600 refers to a student model, e.g., second machine learning model 120 of fig. 1, and the second model of process 600 refers to a teaching model (teaching model), e.g., first machine learning model 110 of fig. 1.
The process 600 sends a request to a computer system external to the mobile device for an entity entry in a database managed by the computer system (604). According to some embodiments, the request includes location data specifying a geographic location of the mobile device. For example, as shown in fig. 3, the mobile device 310 sends a request to the server 320 for a subset of the entity entries in the entity database 230 based on the geographic location of the mobile device.
According to some embodiments, each entity entry includes location data specifying a geographic location, data specifying an entity located at the geographic location, and one or more nests generated by the first model at the computer system. In addition, each of the one or more nests is generated from one or more images received as input to the first model, and each of the images is an image depicting an entity at the geographic location. According to some embodiments, each nest is data generated by one of the middle layers of the first model (i.e., the second machine learning model 120).
The process 600 receives a proper subset of the entity entries in the database from the computer system based on the geographic location of the mobile device and stores the proper subset on the mobile device (606). For example, as shown in fig. 3, the mobile device 310 may store a proper subset of the entity entries in the entity database subset 232 in the mobile device.
The process 600 captures an image by the mobile device and provides the image as input to the first model to generate a captured image nest (608). For example, as shown in FIG. 4, a screen shot 410 depicts an image in a user interface 412 of a mobile device. This image is then provided to a model stored on the mobile device, such as the second machine learning model 120, to generate a captured image nest.
The process 600 determines a nested set of matches in the proper subset of entity entries (610). According to some embodiments, each matching nest is a nest determined to match the nest of captured images according to a matching criterion. For example, as shown in fig. 3, the mobile device 310 may determine whether the nesting of the captured images generated by the model matches the nesting in the entity database subset 232 stored on the mobile device.
The process 600 selects an entity entry as a matching entity entry in a proper subset of entity entries based on the nested set of matches (612). For example, as shown in FIG. 3, if there is a determination of a match between the nesting of captured images generated by the model and the nesting in the entity database subset 232 stored on the mobile device, the mobile device 310 will select an entity entry.
According to some embodiments, selecting an entity entry as a matching entity entry in the proper subset of entity entries includes determining, for each entity entry having a matching nest, a distance value based on location data of the entity entry and location data of the mobile device. In some aspects, selecting an entity entry includes selecting the entity entry having the smallest distance value relative to the distance values of other entity entries having matching nests. For example, when two or more entity entries have matching nests, such as where entity data for two different stores are stored at the mobile device, the mobile device may select the closest entity entry for display. The mobile device then selects the entity entry having the smallest distance value relative to the distance values of other entity entries having matching nests.
The process 600 provides display data describing the entities matching the entity entry (614). According to some embodiments, providing display data describing the entity matching the entity entry includes selecting a display location on the captured image and displaying the display data on the captured image at the display location. For example, as shown in FIG. 4, the second screen shot 420 presents an example user interface 422 having display data 430 describing the identified entity 414. In this example, the user interface 422 presents the display data 430 as a graphical overlay over the current view of the user interface 422. In particular, the display data 430 presents a graphical overlay over a portion of the identified entity 414, which in this example is a coffee shop: "ACME COFFEE".
According to some embodiments, selecting a display position on the captured image includes determining an activation value from the captured image nest that is the highest value relative to other active values in the captured image nest, and selecting the display position corresponding to the activation value in the captured image nest.
According to some embodiments, the captured image is one of a plurality of image frames captured by the mobile device, such that for the plurality of image frames: the method includes determining a set of matching nests in a proper subset of entity entries, selecting a display location corresponding to an activation value in a captured image nest, and displaying display data on a captured image at the display location.
According to some embodiments, process 600 may further include receiving another proper subset of entity entries in the database based on the updated geographic location of the mobile device. For example, the mobile device geographic location moves to a different location (such as a different city), or moves outside of the geographic area covered by the first proper subset. Additionally, the process may include capturing additional images, determining a nested set of matches determined to match the nests of the additional captured images according to the matching criteria, selecting additional entity entries in an additional proper subset of the entity entries as additional matching entity entries, and providing display data describing the entities of the additional matching entity entries.
According to some embodiments, after processing the additional or updated proper subset of entity entries based on the updated geographic location, the process 600 may further include deleting the proper subset of entity entries from the mobile device upon determining that the mobile device is located in the updated geographic location. For example, a mobile device may store only data for the current geographic location of the mobile device. This feature may help reduce storage capacity requirements on the mobile device. Further, by removing the first proper subset of entity entries, if the user returns to the geographic location and needs to download the first proper subset of entity entries again, the updated nesting can be included when the mobile device downloads the proper subset of entity entries.
According to some embodiments, the processes of transmitting or training the machine learning model, accessing a subset of the data (e.g., a subset of the geographic locations of the map data), and using the subset of the data to identify and store the data in the subset database may be processed separately, together as one process described herein (e.g., process 600), or in any combination. According to some embodiments, other signals or data may be used in addition to geographic locations to identify a subset database similar to subset signals (such as geographic locations described herein).
FIG. 7 is a block diagram of an example computing device 700, 750, which example computing device 700, 750 may be used to implement the systems and methods described in this document, either as a client or as a server or servers. Computing device 700 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Computing device 700 is also intended to represent any other generally non-mobile device, such as a television or other electronic device having one or more processors embedded therein or attached thereto. Computing device 750 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart phones, and other computing devices. The components shown herein, their connections and relationships, and their functions, are meant to be examples only, and should not be limiting of the implementations of the disclosure described and/or claimed in this document.
Computing device 700 includes a processor 702, memory 704, a storage device 706, a high-speed controller 708 connected to memory 704 and high-speed expansion ports 710, and a low-speed controller 712 connected to low-speed bus 714 and storage device 706. Each of the components 702, 704, 706, 708, 710, and 712, are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 702 can process instructions for execution within the computing device 700, including instructions stored in the memory 704 or on the storage device 706 to display graphical information for a Graphical User Interface (GUI) on an external input/output device, such as display 716 coupled to high speed controller 708. In other embodiments, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories or types of memory. Additionally, multiple computing devices 700 may be connected, with each device providing portions of the required operations (e.g., as a server array (bank), a group of blade servers, or a multi-processor system).
The memory 704 stores information within the computing device 700. In one implementation, the memory 704 is a computer-readable medium. In one implementation, the memory 704 is a volatile memory unit or units. In another implementation, the memory 704 is a non-volatile memory unit or units.
The storage device 706 is capable of providing mass storage for the computing device 700. In one implementation, the storage device 706 is a computer-readable medium. In various different implementations, the storage device 706 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In one embodiment, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as the methods described above. The information carrier is a computer-readable medium or machine-readable medium, such as the memory 704, the storage device 706, or memory on processor 702.
The high speed controller 708 manages bandwidth-intensive operations for the computing device 700, while the low speed controller 712 manages lower bandwidth-intensive operations. Such allocation of functions is merely an example. In one embodiment, high-speed controller 708 is coupled to memory 704, display 716 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 710, which high-speed expansion ports 710 may accept various expansion cards (not shown). In this embodiment, a low-speed controller 712 is coupled to the storage device 706 and to a low-speed bus 714. May include various communication ports (e.g., USB,
As shown in the figures, computing device 700 may be implemented in many different forms. For example, it may be implemented as a standard server 720 or multiple times in a group of such servers. It may also be implemented as part of a rack server system 724. Further, it may be implemented in a personal computer, such as a laptop computer 722. Alternatively, components from computing device 700 may be combined with other components (not shown) in a mobile device, such as computing device 750. Each such device may contain one or more computing devices 700, 750, and an entire system may be made up of multiple computing devices 700, 750 communicating with each other.
The processor 752 can process instructions for execution within the computing device 750, including instructions stored in the memory 764. The processor may also include a separate analog or digital processor. The processor may provide, for example, for coordination of the other components of the computing device 750, such as control of user interfaces, applications run by the computing device 750, and wireless communication by the computing device 750.
The memory 764 stores information within the computing device 750. In one implementation, the memory 764 is a computer-readable medium. In one implementation, the memory 764 is a volatile memory unit or units. In another implementation, the memory 764 is one or more non-volatile memory units. Expansion memory 774 may also be provided and connected to computing device 750 through expansion interface 772, which expansion interface 772 may include, for example, a Subscriber Identification Module (SIM) card interface. Such expansion memory 774 may provide additional storage space for computing device 750, or may store applications or other information for computing device 750. In particular, expansion memory 774 may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example, expansion memory 774 may be provided as a security module for computing device 750, and may be programmed with instructions that permit secure use of computing device 750. Furthermore, the security application may be provided via the SIM card together with additional information, such as placing identification information on the SIM card in an unbreakable manner.
The memory may include, for example, flash memory and/or MRAM memory, as discussed below. In one embodiment, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as the methods described above. The information carrier is a computer-or machine-readable medium, such as the memory 764, expansion memory 774, or memory on processor 752.
As shown in the figure, computing device 750 may be implemented in many different forms. For example, it may be implemented as a cellular telephone 780. It may also be implemented as part of a smart phone 782, personal digital assistant, or other mobile device.
Where the systems discussed herein collect or may utilize personal information about a user, the user may be provided with an opportunity to control whether applications or features collect user information (e.g., information about the user's social network, social actions or behaviors, profession, the user's preferences, or the user's current location) or whether and/or how to receive content that may be more relevant to the user. Further, certain data may be processed in one or more ways before it is stored or used in order to remove personally identifiable information. For example, the identity of the user may be processed such that personally identifiable information for the user cannot be determined, or the geographic location of the user for which location information is obtained may be generalized (such as to a city, zip code, or state level) such that a particular location of the user cannot be determined. Thus, the user may control how information is collected about the user and used by the content server.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage medium for execution by, or to control the operation of, data processing apparatus.
The computer storage medium may be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or sequential access memory array or device, or a combination of one or more of them. Additionally, although a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be included in one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
The operations described in this specification can be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The term "data processing apparatus" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or a plurality or combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). In addition to hardware, an apparatus can include code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. The computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with the instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Additionally, the computer may be embedded in another device, such as, by way of example only, a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with the user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending documents to and receiving documents from the device used by the user; for example, by sending a web page to a web browser on the user device of the user in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a user computer having a graphical user interface and a web browser through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and point-to-point networks (e.g., ad hoc point-to-point networks).
The computing system may include a user and a server. Generally speaking, a user and a server are remote from each other and typically interact through a communication network. The relationship of user and server arises by virtue of computer programs running on the respective computers and having a user-server relationship to each other. In some embodiments, the server transmits data (e.g., HTML pages) to the user device (e.g., for the purpose of displaying data to and receiving user input from a user interacting with the user device). Data generated at the user device (e.g., a result of the user interaction) may be received at the server from the user device.
Although this specification contains many specific implementation details, these should not be construed as limitations on the scope of any features or possible claims, but rather as descriptions of features specific to particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple independent embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, but rather it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (20)
1. A method implemented in a data processing apparatus, comprising:
Accessing a first model that has been trained to generate an output that classifies image data received as input according to a classification scheme;
training a second model using the first model to approximate the output of the first model to classify image data received as input according to the classification scheme, wherein the second model comprises a plurality of connected layers including an output layer, an input layer, and a plurality of intermediate layers; and
after training the second model:
accessing map data specifying a plurality of geographic locations, wherein each geographic location of at least a subset of the geographic locations is associated with an entity located at the geographic location and one or more images depicting the entity at the geographic location;
for each geographic location associated with an entity:
for each of the one or more images depicting an entity located at the geographic location, providing the image to the second model to generate a nesting of the images, wherein the nesting is data generated by one of the intermediate layers;
associating each of the one or more nests generated by the second model with the geographic location; and
Storing location data specifying the geographic location, the associated one or more nests, and data specifying the entity in a database as an associated entity entry for the entity.
2. The method of claim 1, further comprising:
receiving a request from a mobile device for an entity entry in the database, the request including location data specifying a geographic location of the mobile device; and
selecting a proper subset of entity entries in the database based on the geographic location of the mobile device, wherein each entity entry in the proper subset of entity entries has location data determined to satisfy a proximity threshold indicating that the geographic location of the entity is determined to be proximate to the geographic location of the mobile device.
3. The method of claim 2, wherein the proximity threshold is a value specifying a geographic distance.
4. The method of claim 2, wherein the proximity threshold is a value that specifies a predefined area that includes a geographic location of the mobile device.
5. The method of claim 1, wherein the data specifying the entity comprises an entity description describing the entity and an entity name of the entity.
6. The method of claim 5, wherein the entity is a business entity.
7. The method of claim 5, wherein the entity is a landmark entity.
8. The method of claim 1, wherein the second model is a convolutional neural network.
9. A method implemented in a mobile device, comprising:
storing, on the mobile device, a first model that has been trained to approximate an output of a second model that classifies image data received as input according to a classification scheme, wherein the first model comprises a plurality of connected layers including an output layer, an input layer, and a plurality of intermediate layers;
sending, to a computer system external to the mobile device, a request for entity entries in a database managed by the computer system, the request including location data specifying a geographic location of the mobile device, wherein each entity entry includes:
location data specifying a geographic location;
data specifying an entity located at the geographic location; and
one or more nests generated by the first model at the computer system, each of the one or more nests generated from one or more images received as input to the first model and each of the images being an image depicting the entity at the geographic location, and wherein each nest is data generated by one of the intermediate layers;
Receiving, from the computer system, a proper subset of entity entries in the database, the proper subset selected based on a geographic location of the mobile device, and storing the proper subset of entity entries on the mobile device;
capturing an image by the mobile device and providing the image as input to the first model to generate a captured image nest;
determining a set of nesting matches in the proper subset of entity entries, each nesting match being a nesting determined to match the captured image nesting according to a matching criterion;
selecting an entity entry as a matching entity entry in the proper subset of entity entries based on the nested set of matches; and
providing display data describing the entities of the matching entity entry on a display device of the mobile device.
10. The method of claim 9, wherein selecting an entity entry in the proper subset of entity entries as a matching entity entry based on the nested set of matches comprises:
for each entity entry having a matching nest, determining a distance value based on location data of the entity entry and location data of the mobile device; and
The entity entry having the smallest distance value relative to the distance values of other entity entries having matching nests is selected.
11. The method of claim 9, wherein providing display data describing the entity of the matching entity entry comprises:
selecting a display location on the captured image; and
displaying the display data on the captured image at the display location.
12. The method of claim 11, wherein selecting a display location on the captured image comprises:
determining from the captured image nest an activation value that is the highest value relative to other active values in the captured image nest; and
selecting a display position corresponding to the activation value in the captured image nest.
13. The method of claim 12, wherein the captured image is one of a plurality of image frames captured by the mobile device, and for the plurality of image frames: determining the matching nesting set in the proper subset of entity entries, selecting a display location corresponding to an activation value in the captured image nest, and displaying the display data on the captured image at the display location.
14. The method of any of claims 11, wherein:
the captured image is one of a plurality of image frames captured by the mobile device; and
for the plurality of image frames: the method includes determining a matching nested set of the proper subset of entity entries, selecting an entity entry in the proper subset of entity entries as a matching entity entry, and providing display data describing an entity of the matching entity entry.
15. The method of claim 9, further comprising:
receiving, from the computer system, a further proper subset of entity entries in the database, the further proper subset selected based on the updated geographic location of the mobile device, and storing the further proper subset of entity entries on the mobile device;
capturing, by the mobile device, a further image and providing the further image as an input to the first model to generate a further captured image nest;
determining a set of nesting matches in the additional proper subset of entity entries, each nesting match being a nesting determined to match the additional captured image nesting according to a matching criterion;
Selecting additional entity entries in the additional proper subset of entity entries as additional matching entity entries based on the nested set of matches; and
providing display data describing the entities of the further matching entity entry on a display device of the mobile device.
16. The method of claim 15, further comprising:
deleting the proper subset of entity entries from the mobile device when it is determined that the mobile device is in an updated geographic location.
17. A system, comprising:
a data processing device; and
a non-transitory computer-readable storage medium in data communication with the data processing apparatus and storing instructions executable by the data processing apparatus and that, upon such execution, cause the data processing apparatus to perform operations comprising:
accessing a first model that has been trained to generate an output that classifies image data received as input according to a classification scheme;
training a second model using the first model to approximate an output of the first model to classify image data received as input according to the classification scheme, wherein the second model comprises a plurality of connected layers including an output layer, an input layer, and a plurality of intermediate layers; and
After training the second model:
accessing map data specifying a plurality of geographic locations, wherein each geographic location of at least a subset of the geographic locations is associated with an entity located at the geographic location and one or more images depicting the entity at the geographic location;
for each geographic location associated with an entity:
for each of the one or more images depicting an entity located at the geographic location, providing the image to the second model to generate a nesting of the images, wherein the nesting is data generated by one of the intermediate layers;
associating each of the one or more nests generated by the second model with the geographic location; and
storing location data specifying the geographic location, the associated one or more nests, and data specifying the entity in a database as an associated entity entry for the entity.
18. A non-transitory computer storage medium encoded with a computer program, the computer program comprising instructions that when executed by data processing apparatus cause the data processing apparatus to perform operations comprising:
Accessing a first model that has been trained to generate an output that classifies image data received as input according to a classification scheme;
training a second model using the first model to approximate the output of the first model to classify image data received as input according to the classification scheme, wherein the second model comprises a plurality of connected layers including an output layer, an input layer, and a plurality of intermediate layers; and
after training the second model:
accessing map data specifying a plurality of geographic locations, wherein each geographic location of at least a subset of the geographic locations is associated with an entity located at the geographic location and one or more images depicting the entity at the geographic location;
for each geographic location associated with an entity:
for each of the one or more images depicting an entity located at the geographic location, providing the image to the second model to generate a nesting of the images, wherein the nesting is data generated by one of the intermediate layers;
associating each of the one or more nests generated by the second model with the geographic location; and
Storing location data specifying the geographic location, the associated one or more nests, and data specifying the entity in a database as an associated entity entry for the entity.
19. A system, comprising:
a data processing device; and
a non-transitory computer-readable storage medium in data communication with the data processing apparatus and storing instructions executable by the data processing apparatus and that, upon such execution, cause the data processing apparatus to perform operations comprising:
storing, on a mobile device, a first model that has been trained to approximate an output of a second model that classifies image data received as input according to a classification scheme, wherein the first model comprises a plurality of connected layers including an output layer, an input layer, and a plurality of intermediate layers;
sending, to a computer system external to the mobile device, a request for entity entries in a database managed by the computer system, the request including location data specifying a geographic location of the mobile device, wherein each entity entry includes:
location data specifying a geographic location;
Data specifying an entity located at the geographic location; and
one or more nests generated by the first model at the computer system, each of the one or more nests generated from one or more images received as input to the first model and each of the images being an image depicting the entity at the geographic location, and wherein each nest is data generated by one of the intermediate layers;
receiving, from the computer system, a proper subset of entity entries in the database, the proper subset selected based on a geographic location of the mobile device, and storing the proper subset of entity entries on the mobile device;
capturing an image by the mobile device and providing the image as input to the first model to generate a captured image nest;
determining a set of nesting matches in the proper subset of entity entries, each nesting match being a nesting determined to match the captured image nesting according to a matching criterion;
selecting an entity entry as a matching entity entry in the proper subset of entity entries based on the nested set of matches; and
Providing display data describing the entities of the matching entity entry on a display device of the mobile device.
20. A non-transitory computer storage medium encoded with a computer program, the computer program comprising instructions that when executed by data processing apparatus cause the data processing apparatus to perform operations comprising:
storing, on a mobile device, a first model that has been trained to approximate an output of a second model that classifies image data received as input according to a classification scheme, wherein the first model comprises a plurality of connected layers including an output layer, an input layer, and a plurality of intermediate layers;
sending, to a computer system external to the mobile device, a request for entity entries in a database managed by the computer system, the request including location data specifying a geographic location of the mobile device, wherein each entity entry includes:
location data specifying a geographic location;
data specifying an entity located at the geographic location; and
one or more nests generated by the first model at the computer system, each of the one or more nests generated from one or more images received as input to the first model and each of the images being an image depicting an entity at the geographic location, and wherein each nest is data generated by one of the intermediate layers;
Receiving, from the computer system, a proper subset of entity entries in the database, the proper subset selected based on a geographic location of the mobile device, and storing the proper subset of entity entries on the mobile device;
capturing an image by the mobile device and providing the image as input to the first model to generate a captured image nest;
determining a set of nesting matches in the proper subset of entity entries, each nesting match being a nesting determined to match the captured image nesting according to a matching criterion;
selecting an entity entry as a matching entity entry in the proper subset of entity entries based on the nested set of matches; and
providing display data describing the entities of the matching entity entry on a display device of the mobile device.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/101,594 | 2018-08-13 | ||
US16/101,594 US10769428B2 (en) | 2018-08-13 | 2018-08-13 | On-device image recognition |
PCT/US2019/046169 WO2020036874A1 (en) | 2018-08-13 | 2019-08-12 | On-device image recognition |
Publications (1)
Publication Number | Publication Date |
---|---|
CN111989665A true CN111989665A (en) | 2020-11-24 |
Family
ID=67766389
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980025571.1A Pending CN111989665A (en) | 2018-08-13 | 2019-08-12 | On-device image recognition |
Country Status (3)
Country | Link |
---|---|
US (1) | US10769428B2 (en) |
CN (1) | CN111989665A (en) |
WO (1) | WO2020036874A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10992646B2 (en) | 2016-01-15 | 2021-04-27 | Xingchang Zhou | Data transmission method and data transmission apparatus |
Families Citing this family (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6572269B2 (en) * | 2017-09-06 | 2019-09-04 | 株式会社東芝 | Learning device, learning method, and program |
US11042611B2 (en) * | 2018-12-10 | 2021-06-22 | XNOR.ai, Inc. | Digital watermarking of machine-learning models |
US11151993B2 (en) * | 2018-12-28 | 2021-10-19 | Baidu Usa Llc | Activating voice commands of a smart display device based on a vision-based mechanism |
US11444845B1 (en) * | 2019-03-05 | 2022-09-13 | Amazon Technologies, Inc. | Processing requests using compressed and complete machine learning models |
US11163940B2 (en) * | 2019-05-25 | 2021-11-02 | Microsoft Technology Licensing Llc | Pipeline for identifying supplemental content items that are related to objects in images |
US20220092404A1 (en) * | 2020-09-18 | 2022-03-24 | Arm Cloud Technology, Inc. | Neural network selection |
US20220188598A1 (en) * | 2020-12-11 | 2022-06-16 | Visa International Service Association | System, Method, and Computer Program Product for Evolutionary Learning in Verification Template Matching During Biometric Authentication |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100305844A1 (en) * | 2009-06-01 | 2010-12-02 | Choi Sung-Ha | Mobile vehicle navigation method and apparatus thereof |
US9542948B2 (en) * | 2014-04-09 | 2017-01-10 | Google Inc. | Text-dependent speaker identification |
US20180012107A1 (en) * | 2015-12-11 | 2018-01-11 | Tencent Technology (Shenzhen) Company Limited | Image classification method, electronic device, and storage medium |
CN107832799A (en) * | 2017-11-20 | 2018-03-23 | 北京奇虎科技有限公司 | Object identifying method and device, computing device based on camera scene |
CN108270794A (en) * | 2018-02-06 | 2018-07-10 | 腾讯科技（深圳）有限公司 | Content delivery method, device and readable medium |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH08227410A (en) * | 1994-12-22 | 1996-09-03 | Just Syst Corp | Learning method of neural network, neural network, and speech recognition device utilizing neural network |
US20080082549A1 (en) * | 2006-10-02 | 2008-04-03 | Vic Baker | Multi-Dimensional Web-Enabled Data Viewer |
US20130261969A1 (en) * | 2010-12-24 | 2013-10-03 | Pioneer Corporation | Navigation apparatus, control method, program, and storage medium |
US9524435B2 (en) * | 2015-03-20 | 2016-12-20 | Google Inc. | Detecting the location of a mobile device based on semantic indicators |
US20160334969A1 (en) * | 2015-05-11 | 2016-11-17 | Facebook, Inc. | Methods and Systems for Viewing an Associated Location of an Image |
-
2018
- 2018-08-13 US US16/101,594 patent/US10769428B2/en active Active
-
2019
- 2019-08-12 CN CN201980025571.1A patent/CN111989665A/en active Pending
- 2019-08-12 WO PCT/US2019/046169 patent/WO2020036874A1/en active Application Filing
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100305844A1 (en) * | 2009-06-01 | 2010-12-02 | Choi Sung-Ha | Mobile vehicle navigation method and apparatus thereof |
US9542948B2 (en) * | 2014-04-09 | 2017-01-10 | Google Inc. | Text-dependent speaker identification |
US20180012107A1 (en) * | 2015-12-11 | 2018-01-11 | Tencent Technology (Shenzhen) Company Limited | Image classification method, electronic device, and storage medium |
CN107832799A (en) * | 2017-11-20 | 2018-03-23 | 北京奇虎科技有限公司 | Object identifying method and device, computing device based on camera scene |
CN108270794A (en) * | 2018-02-06 | 2018-07-10 | 腾讯科技（深圳）有限公司 | Content delivery method, device and readable medium |
Non-Patent Citations (2)
Title |
---|
GABRIEL TAKACS 等: "Outdoors augmented reality on mobile phone using loxel-based visual feature organization", 《PROCEEDINGS OF THE WORKSHOP ON MULTIMEDIA INFORMATION RETRIEVAL(MIRO)》 * |
GEORGE SEIF: "Deep Learning for Image Recognition: why it"s challenging, where we"ve been, and what"s next", 《URL:HTTPS://TOWARDSDATASCIENCE.COM/DEEP-LEARNING-FOR-IMAGE-CLASSIFICATION-WHY-ITS-CHALLENGING-WHERE-WE-VE-BEEN-AND-WHAT-S-NEXT-93B56948FCEF》 * |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10992646B2 (en) | 2016-01-15 | 2021-04-27 | Xingchang Zhou | Data transmission method and data transmission apparatus |
Also Published As
Publication number | Publication date |
---|---|
WO2020036874A1 (en) | 2020-02-20 |
US10769428B2 (en) | 2020-09-08 |
US20200050846A1 (en) | 2020-02-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10769428B2 (en) | On-device image recognition | |
US11483268B2 (en) | Content navigation with automated curation | |
US11715473B2 (en) | Intuitive computing methods and systems | |
US11328008B2 (en) | Query matching to media collections in a messaging system | |
JP5843207B2 (en) | Intuitive computing method and system | |
KR101780034B1 (en) | Generating augmented reality exemplars | |
JP6569313B2 (en) | Method for updating facility characteristics, method for profiling a facility, and computer system | |
US11954142B2 (en) | Method and system for producing story video | |
US11966853B2 (en) | Machine learning modeling using social graph signals | |
US20230146563A1 (en) | Automated image processing and insight presentation | |
US10846550B2 (en) | Object classification for image recognition processing | |
KR20230162062A (en) | Neural network accompaniment extraction from songs | |
KR20230162078A (en) | Neural networks for modifying the characteristics of vocals |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |