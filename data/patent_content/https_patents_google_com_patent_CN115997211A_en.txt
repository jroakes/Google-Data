CN115997211A - System and method for local private non-interactive communication - Google Patents
System and method for local private non-interactive communication Download PDFInfo
- Publication number
- CN115997211A CN115997211A CN202180046496.4A CN202180046496A CN115997211A CN 115997211 A CN115997211 A CN 115997211A CN 202180046496 A CN202180046496 A CN 202180046496A CN 115997211 A CN115997211 A CN 115997211A
- Authority
- CN
- China
- Prior art keywords
- input data
- computing system
- representation
- data points
- computer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L63/00—Network architectures or network communication protocols for network security
- H04L63/04—Network architectures or network communication protocols for network security for providing a confidential data exchange among entities communicating through data packet networks
- H04L63/0428—Network architectures or network communication protocols for network security for providing a confidential data exchange among entities communicating through data packet networks wherein the data content is protected, e.g. by encrypting or encapsulating the payload
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/604—Tools and structures for managing or administering access control systems
Abstract
A computer-implemented method for encoding data for communication with improved privacy includes obtaining, by a computing system including one or more computing devices, input data including one or more input data points. The method may include constructing, by the computing system, a mesh tree including potential representations of one or more input data points, the potential representations arranged in a plurality of layers, the mesh tree including a hierarchical data structure including a plurality of hierarchically organized nodes. The method may include determining, by the computing system, a representation of each of the one or more input data points from potential representations of the net tree, the representation including one of a plurality of hierarchically organized nodes. The method may include encoding, by the computing system, a representation of each of the one or more input data points for communication.
Description
RELATED APPLICATIONS
The present application claims priority and benefit from U.S. provisional patent application No. 63/168833, filed 3/31 at 2021. U.S. provisional patent application No. 63/168,533 is incorporated herein by reference in its entirety.
Technical Field
The present disclosure relates generally to systems and methods for local private non-interactive communication. More specifically, the present disclosure relates to differential private k-means clustering in a round of non-interactive local models.
Background
Clustering, such as k-means clustering, involves grouping or clustering sets of dimension input points into clusters based on the distance from the points to the cluster center. In k-means clustering, points are clustered based on Euclidean distances from the points to their respective cluster centers, where the goal is to assign points to candidate centers to minimize the total cost of all points, and possibly subject to other constraints.
Differential privacy has become a popular privacy definition providing strong assurance and mathematical rigor. Differential privacy provides that slight changes in the input set are not trackable at output. Two main differential privacy models occur: a central model in which a trusted central manager encodes data as differentially private; and distributed models, such as local models, where there is no central manager, but rather the output of each client is expected to be differentially private.
Disclosure of Invention
Aspects and advantages of embodiments of the disclosure will be set forth in part in the description which follows, or may be learned from the description, or may be learned by practice of the embodiments.
One example aspect of the present disclosure relates to a computer-implemented method for encoding data for communication with improved privacy. The method may include obtaining, by a computing system including one or more computing devices, input data including one or more input data points. The method may include constructing, by the computing system, a mesh tree including potential representations of one or more input data points, the potential representations arranged in a plurality of layers, the mesh tree including a hierarchical data structure including a plurality of hierarchically organized nodes. The method may include determining, by the computing system, a representation of each of the one or more input data points from potential representations of the net tree, the representation including one of a plurality of hierarchically organized nodes. The method may include encoding, by the computing system, a representation of each of the one or more input data points for communication.
Another example aspect of the present disclosure relates to a computer-implemented method for decoding data encoded by a net tree based encoding algorithm. The method may include obtaining, by a computing system, which may include one or more computing devices, encoded input data including encoded histogram data. The method may include determining, by the computing system, a decoded frequency prediction based at least in part on the encoded histogram data. The method may include constructing, by the computing system, a net tree based at least in part on the decoded frequency prediction, the net tree including a plurality of leaves. The method may include performing, by the computing system, a k-means approximation algorithm on the net tree to divide the plurality of leaves into a plurality of partitions according to respective closest centers.
Another example aspect of the present disclosure relates to a computer-implemented method for clustering input data points with differential privacy guarantees and reduced approximation ratios. The method includes obtaining, by a computing system including one or more computing devices, input data including one or more input data points. The method includes constructing, by the computing system, a mesh tree including potential representations of one or more input data points, the potential representations arranged in a plurality of layers, the mesh tree including a hierarchical data structure including a plurality of hierarchically organized nodes and a plurality of mappings between the plurality of hierarchically organized nodes. The method may include determining, by the computing system, a representation of each of the one or more input data points from potential representations of the net tree, the representation including one of a plurality of hierarchically organized nodes.
Other aspects of the disclosure relate to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of the various embodiments of the present disclosure will become better understood with regard to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the principles of interest.
Drawings
The present specification sets forth a detailed discussion of embodiments directed to those of ordinary skill in the art, with reference to the accompanying drawings, wherein:
FIG. 1 depicts a block diagram of an example computing system performing local differential private communication, according to an example embodiment of the present disclosure.
FIG. 2 depicts an example algorithm for constructing a net tree in accordance with an example embodiment of the present disclosure.
FIG. 3 depicts an example algorithm for calculating an expansion threshold to balance a k-means target in accordance with an example embodiment of the present disclosure.
Fig. 4 depicts an example algorithm for encoding private input data points for non-interactive local differential private communications in accordance with an example embodiment of the present disclosure.
Fig. 5 depicts an example algorithm for decoding private input data points for non-interactive local differential private communications in accordance with an example embodiment of the present disclosure.
Fig. 6A depicts an example histogram encoder in accordance with an example embodiment of the present disclosure.
Fig. 6B depicts an example histogram decoder in accordance with an example embodiment of the present disclosure.
Fig. 7A depicts an example split-bucket vector sum encoder in accordance with an example embodiment of the present disclosure.
Fig. 7B depicts an example split-bucket vector sum decoder according to an example embodiment of the present disclosure.
Fig. 8 depicts a block diagram of an example method for encoding private user data for non-interactive differential private communication, according to an example embodiment of the present disclosure.
Fig. 9 depicts a block diagram of an example method for decoding private data encoded by a net tree based encoding algorithm, according to an example embodiment of the present disclosure.
Fig. 10 depicts a block diagram of an example computing device configured to perform any of the operations described herein, in accordance with an example embodiment of the present disclosure.
Repeated reference characters in the drawings are intended to represent the same features in the various embodiments.
Detailed Description
In general, the present disclosure relates to systems and methods for local private non-interactive communication. Systems and methods according to example aspects of the present disclosure may employ a hierarchical object, referred to as a net tree, to construct a private core set of multiple private input points. The private core set may then be encoded to preserve user privacy with strong differential privacy guarantees. The decoder model (e.g., at the aggregator computing device) may then run an approximation algorithm on the set of encoded cores, which may not necessarily be proprietary. Systems and methods according to example aspects of the present disclosure may operate in a non-interactive local model for differential privacy in that each source may encode all potential representatives of its respective input point without any interaction (e.g., any back-and-forth communication) with the aggregator.
Differential Privacy (DP) may be formally defined such that for epsilon>0 and delta E [0,1 ]]Random algorithm
In particular, systems and methods according to example aspects of the present disclosure may provide differential private and/or non-interactive (e.g., one round) communications between multiple source computing devices (e.g., mobile devices, laptops, etc.) or client devices (e.g., in communication with a central server) such as associated with a user and aggregator computing devices (also referred to as analyzers). In a distributed model of differential privacy, such as a local model and/or a shuffle model of differential privacy, the aggregator appliance may not be trusted. For example, assume that the aggregator itself and/or a device capable of intercepting transmissions between the source computing device and the aggregator computing device is the host of the other party, or that private user data should not be kept secret. Thus, in the distributed model, each transmission from the source computing device to the aggregator computing device is required to be differentially private.
Various methods have been proposed to provide distributed differential privacy. Some of these methods utilize encoder models and k-means clustering of input data. Algorithms for performing k-means clustering are typically NP-hard and/or run at a large approximation rate. Furthermore, some approximation algorithms for k-means clustering are not compatible with certain types of differential privacy models, such as distributed differential privacy. In addition, many existing differential proprietary algorithms are interactive, meaning that they require multiple rounds of communication between the source and aggregator.
However, systems and methods according to example aspects of the present disclosure may provide solutions to these and other challenges related to approximating k-means and/or providing differential private and/or non-interactive communications. For example, systems and methods according to example aspects of the present disclosure may provide a k-means approximation algorithm that provides an approximation ratio that is arbitrarily close to the approximation ratio of a non-private algorithm. Additionally, systems and methods according to example aspects of the present disclosure may provide for non-interactive differential private communications, which may be performed through only a single communication from a source to an aggregator. Additionally, systems and methods according to example aspects of the present disclosure may be applied to various differential private models, including, for example, local models, shuffle models, and/or other distributed models.
According to example aspects of the disclosure, a computing system including one or more computing devices may obtain private data including one or more input data points. For example, the computing system may be or include a source computing device. The source computing device may be a user computing device operated by a user, such as a mobile device, a desktop computer, a wearable computing device, or any other suitable computing device. The private data (e.g., one or more input data points) may be user data. For example, the private data may be or may include vector or other tensor data. For example, the input data point may be a point in d-dimensional space or having a dimension d (e.g., represented by a vector or other tensor).
Example aspects of the present disclosure may provide systems and methods for constructing a net tree (e.g., including input data points) representing private data. An approximate frequency (e.g., frequency prediction) may be associated with each node in the net tree. The approximation frequency may provide an approximation of the number of input data points that a given node represents. For example, consider each node in the tree as a bucket, where each input data point contributes to the bucket, and the approximation frequency may approximate the number of contributions to the bucket. The net tree essentially provides a core set for constructing the input, especially for cases where the dimension of the input is small. As used herein, the weight-point set S' is the (k, γ, t) core set of the weight-point set S if for each set of k centers
For example, in accordance with example aspects of the present disclosure, a computing system may construct a net tree that includes potential representations of one or more input data points. The potential representatives may be arranged in multiple layers (levels). For example, a net tree may be or include a hierarchical data structure including a plurality of hierarchically organized nodes. The tree may additionally include a plurality of mappings between a plurality of hierarchically organized nodes.
The computing system may determine a representation of each of the one or more input data points from the potential representations of the net tree. The representation may be one of a plurality of hierarchically organized nodes. For example, the representation of the input data point of the one or more input data points may be the nearest potential representation to the input data point. The potential representation closest to the input data point may include the potential representation having the smallest (e.g., euclidean) distance from the input data point relative to each of the other potential representations in the net tree.
For example, a net tree may include multiple nets according to example aspects of the present disclosure. The multiple nets may form respective layers of a tree, wherein nodes of the tree at each layer correspond to elements in a respective net of the multiple nets. The net tree may be constructed based at least in part on (e.g., approximate) frequency predictions across multiple nets. For example, at each node of the net tree, the frequency prediction may approximate the number of input data points that the node is representative of (e.g., the number of representative sources). As another example, for each representation, the frequency prediction may approximately provide the number of input data point sources representing the assigned points. A complete net tree is defined as a net tree in which the number of layers in the tree is one greater than the number of nets in the plurality of nets. For example, one additional layer may be the root layer. For example, the net tree may be rooted at zero.
Given net family
In some embodiments, the plurality of networks may be or may include an efficiently decodable network. For example, a net tree may include multiple efficiently decodable networks. Formally, is provided with
In constructing a net tree, the deeper the tree, the closer the representative location of the input point is to the input point itself. In addition, noise is added at a plurality of nodes assigned to each leaf to achieve privacy. Thus, balancing the number of leaves in the tree may be required. Too many leaves will result in larger errors introduced by noise, while too few leaves will result in input points too far from their representation, resulting in increased errors. For example, including too many nodes in the tree may result in too many nodes contributing to the accumulated error associated with differential privacy. In addition, including too few nodes can result in many nodes being at a lower level, resulting in a large representation error introduced by the distance between the representation and the input point, and thus in a large overall error. Example aspects of the present disclosure may provide a balance between these two errors to optimize for the overall k-means target.
To balance these errors, the nodes of the net tree may be expanded in layers of the tree relative to an expansion threshold (referred to herein as τ) of the net tree. The expansion threshold may indicate the number of nodes to be expanded at each level of the net tree. Extending the threshold effectively balances the accumulated error associated with including the node with the additional error associated with including the node and the accuracy lost by including too few nodes. For example, nodes at the first layer may be ranked according to any suitable criteria, such as approximate frequency. The number of highest ranking nodes in the first layer may be expanded to produce a second layer of the tree, where the number of highest ranking nodes is equal to the expansion threshold. An example threshold calculation algorithm is given in algorithm 2 (as shown in fig. 3 and below). For example, constructing, by a computing system, a net tree including potential representations of one or more input data points may include: an expansion threshold is determined by the computing system. Additionally, constructing the net tree may include: the number of one or more highest ranked nodes in the net tree is identified at a first level in the net tree by the computing system. The number of highest ranking nodes identified may be equal to the expansion threshold. For example, the computing system may identify τ highest ranking nodes. The construction tree may then include expanding, by the computing system, one or more highest ranking nodes at a second level in the net tree. For example, the selected node may be extended by having child nodes at the second level (e.g., as compared to being leaf nodes).
In some implementations, the expansion threshold may be based at least in part on an optimal transmission cost, such as an optimal transmission cost between one or more input data points and the potential representation. For example, the optimal transmission cost can be determined by the pairThe best transmission problem solution results, such as the best transmission problem of Monge. The best transmission problem seeks to find a mapping that transmits a first measurement on the metric space to a second measurement on the metric space while minimizing cost. The cost may be any suitable cost and may be defined, for example, as a function of the total mass of movement times the distance of movement. Such a mapping cannot be guaranteed, such as in case of different quality. In some implementations, the quality of the mismatch may be allowed, but compensated for by penalizing based on the L1 difference. For example, let S, S' be
Additionally, the expansion threshold may be expressed as based at least in part on the minimum cost of the center set C and the multiple sets X
According to example aspects of the present disclosure, a net tree provides certain useful characteristics. For example, one characteristic of a net tree is that the potential representation of point x at layer i cannot be undesirably far from x. For example, for any of
Net tree output by tree construction algorithm 1 (shown in fig. 2 and below)algorithm 1, wherein +.>
Additionally, in accordance with example aspects of the present disclosure, the computing system may encode a representation of each of the one or more input data points for non-interactive differential private communication. For example, the representation of each input data point may constitute a core set representing private data. These representations may be encoded by the source computing system and transmitted to the aggregator computing system, which may then decode the encoded representations while providing differential privacy guarantees at the source computing device. An example algorithm for differential proprietary non-interactive coding is given in algorithm 3 (as shown in fig. 4 and below).
For example, in some implementations, one or more input numbers are entered by the computing systemEncoding a representation of each of the points for non-interactive differential private communication may include: the representation is encoded by the computing system through a generalized split-bucket vector sum encoder model. In some implementations, the vector sum encoder model may include vector encoding that shares a dot product of the uniform random component and the potential representation. As an example, the generalized split-bucket vector sum encoder model may include a code configured such that algorithm 7 shown in fig. 7A. The generalized split-bucket vector sum encoder may encode the representation with the input vector x.
Additionally, in some implementations, encoding, by the computing system, a representation of each of the one or more input data points for non-interactive differential private communication may include: the representation is encoded by the computing system through a generalized histogram encoder model. In some implementations, the generalized histogram coder model generates an output based on the shared uniform random component, where the output is probabilistic
Furthermore, an example generalized histogram coder model is given in algorithm 6 shown in fig. 6A.
In some implementations, the computing system can project the one or more input data points to the random subspace prior to determining a representation of each of the one or more input data points from the potential representations of the net tree. The random subspace may be based on shared randomness between the computing device and other computing devices, such as other source computing devices and/or aggregator computing devices. The projection may be performed on d=o (log k) dimensions while maintaining a target for any given partition. For example, for each
further, after projecting one or more input data points into the random subspace, the computing device may scale the projected input data points into the reduced-dimension subspace. For example, a representation in the net tree may be calculated for projected input data points in the dimension-reduction subspace. Random projection and dimension reduction can eliminate the exponential dependence on d from the accumulated error, which can improve the performance of the encoder.
In some implementations, multiple networks may be replaced with location-sensitive hashing. For example, given LSH g 1 ,…,g T The layer i representation of x becomes z i ＝(g 1 (x),…,g T (x) A kind of electronic device. In this sense, the tree has a strong similarity to the LSH forest. Any suitable hash value may be employed, such as a random vector v i Is selected and g i (x) Is that<v i ,x>SimHash of the sign of (c). In some implementations, the input data points may not be randomly projected into the low-dimensional subspace, as LSH is a form of dimension reduction. In addition, in view of this, it is also possible to directly calculate the approximate centers of all nodes in the tree, and then calculate the k-centers on the privately-held dataset using a non-proprietary algorithm (e.g., k-means++).
In addition to providing for encoding private data, systems and methods according to example aspects of the present disclosure may provide a computer-implemented method for decoding private data encoded by a net tree based encoding algorithm. For example, a computing system including one or more computing devices may obtain encoded private input data. The encoded private input data may be received from one or more (e.g., multiple) source computing devices. Additionally and/or alternatively, the computing system may be or include an aggregator computing device. For example, the aggregator computing device may aggregate differential private encoded input data from multiple sources while maintaining the privacy of the individual sources. An example algorithm for decoding private data is given in algorithm 4 (as shown in fig. 5 and below).
The encoded private input data may include encoded histogram data. For example, the encoded histogram data may be encoded by a generalized histogram encoder model (e.g., as described in algorithm 5 shown in fig. 6A and below). The computing system may determine a decoded frequency prediction based at least in part on the encoded histogram data. In some embodiments, determining, by the computing system, that the decoded frequency prediction is based at least in part on
As one example, an example histogram decoder model that may be used to decode the decoded frequency prediction is given in algorithm 6 (as shown in fig. 6B and below). The combination of generalized histogram encoder model and decoder model may be for histograms in the local model
The computing system may then construct a net tree based at least in part on the decoding frequency prediction. For example, the computing system may attempt to recreate a net tree constructed to encode encoded private input data at the source based on the decoding frequency prediction. To decode encoded data, a computing system may first construct a frequency prediction using the encoded histogram, where the computing system may construct a net tree from the frequency prediction using the algorithm described herein (e.g., algorithm 1 of FIG. 2)
The computing system may then run any suitable approximation algorithm for the k-means on the representative set of net trees
Additionally, the encoded private input data may further comprise encoded vector summation data. For example, the encoded vector sum data may be encoded by a generalized bucket sum algorithm (e.g., as described in fig. 7A). The encoded vector sum data may be decoded to determine a vector sum prediction. For example, the computing system may determine a decoded vector sum prediction based at least in part on the encoded vector sum data. In some embodiments, determining the decoded vector sum prediction is performed at least in part by determining the vector sum prediction based on sigma i∈[n] z i ·Z v,i To sum the encoded vector sum data. As one example, an example vector sum decoder is given in algorithm 8 shown in fig. 7B. Vector sum prediction may be used for multiple partitions to determine k centers of original input data in original high-dimensional space. For example, the computing system may upgrade the plurality of partitions to the original higher-dimensional space based at least in part on the decoded vector sum prediction.
For purposes of illustration, the systems and methods are discussed herein in connection with a local model of differential privacy. Example aspects of the present disclosure may be applied to other suitable differential private models, such as a shuffle model. For example, in some implementations, the differential private non-interactive communication in which private data is encoded may be a local model of differential privacy. Additionally and/or alternatively, in some implementations, the differential private non-interactive communication may be a shuffling model of differential privacy.
Systems and methods according to example aspects of the present disclosure may provide a number of technical effects and benefits, including improvements in computing technology. For example, systems and methods according to example aspects of the present disclosure may provide for constructing, by a computing system, a mesh tree including potential representations of one or more input data points, the potential representations being arranged in a plurality of layers, the mesh tree including a hierarchical data structure including a plurality of hierarchically organized nodes and a plurality of mappings between the plurality of hierarchically organized nodes. The net tree, in turn, may provide for encoding one or more input data points for non-interactive differential private communications. As such, systems and methods according to example aspects of the present disclosure may provide and even enable near-rate reduced non-interactive differential private communications (e.g., closer to the performance of actual non-private algorithms), thereby providing more accurate information communication while maintaining privacy guarantees.
Example embodiments of the present disclosure will now be discussed in further detail with reference to the accompanying drawings.
FIG. 1 depicts a block diagram of an example computing system 100 that performs local differential private communication, according to an example embodiment of the disclosure. The computing system 100 may include a plurality of source computing devices 102 in communication with a centralized aggregator computing device 104. For example, the source computing device 102 may transmit the differential private communications 106 to the aggregator computing device 104, respectively. The aggregator computing device 104 can aggregate the communications 106 and output information about the system 100, such as aggregated private data results. For the communication 106 and the aggregator computing device 104, it is desirable to maintain the privacy of the source computing device 102.
Fig. 2-7B depict mathematical marking algorithms for various operations in accordance with example aspects of the present disclosure. It should be understood that the algorithms described herein are provided for purposes of illustrating example aspects of the present disclosure, and that embodiments of these algorithms may include modifications and/or additional operations. For example, data or variables in embodiments of these algorithms may be rounded, discretized, privately owned, or manipulated in other ways beyond the operations explicitly identified in the algorithm. For example, the algorithm may be implemented on a binary computer, such as computing device 1000 of fig. 10.
Fig. 8 depicts a flowchart of an example method of encoding private user data for non-interactive differential private communication, according to an example embodiment of the present disclosure. Although FIG. 8 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 800 may be omitted, rearranged, combined, and/or debugged in various ways without departing from the scope of the present disclosure.
At 802, a computing system (e.g., including one or more computing devices) can obtain private data including one or more input data points. For example, the computing system may be or include a source computing device. The source computing device may be a user computing device operated by a user, such as a mobile device, a desktop computer, a wearable computing device, or any other suitable computing device. The private data (e.g., one or more input data points) may be user data. For example, the private data may be or may include vector or other tensor data. For example, the input data point may be a point in d-dimensional space or having a dimension d (e.g., represented by a vector or other tensor).
At 804, the computing system may construct a net tree including potential representations of one or more input data points. The potential representatives may be arranged in multiple layers. For example, a net tree may be or include a hierarchical data structure including a plurality of hierarchically organized nodes. The tree may additionally include a plurality of mappings between a plurality of hierarchically organized nodes. The representation may be one of a plurality of hierarchically organized nodes. For example, the representation of the input data point of the one or more input data points may be the nearest potential representation to the input data point. The potential representation closest to the input data point may include the potential representation having the smallest (e.g., euclidean) distance from the input data point relative to each of the other potential representations in the net tree.
At 806, the computing system may determine a representation of each of the one or more input data points from the potential representations of the net tree. The representation may be one of a plurality of hierarchically organized nodes. For example, the representation of the input data point of the one or more input data points may be the nearest potential representation to the input data point. The potential representation closest to the input data point may include the potential representation having the smallest (e.g., euclidean) distance from the input data point relative to each of the other potential representations in the net tree.
At 808, the computing system can encode a representation of each of the one or more input data points for non-interactive differential private communication. For example, the representation of each input data point may constitute a core set representing private data. These representations may be encoded by the source computing system and transmitted to the aggregator computing system, which may then decode the encoded representations while providing differential privacy guarantees at the source computing device. An example algorithm for differential proprietary non-interactive coding is given in algorithm 3 shown in fig. 4.
For example, in some implementations, encoding, by the computing system, a representation of each of the one or more input data points for non-interactive differential private communication may include: the representation is encoded by the computing system through a generalized split-bucket vector sum encoder model. As an example, the generalized split-bucket vector sum encoder model may include a code configured such thatalgorithm 7 shown in fig. 7A. The generalized split-bucket vector sum encoder may encode the representation with the input vector x.
Additionally, in some implementations, encoding, by the computing system, a representation of each of the one or more input data points for non-interactive differential private communication may include: the representation is encoded by the computing system through a generalized histogram encoder model. As an example, in some embodiments, the generalized histogram coder model may include a mathematical model configured such that:
Furthermore, an example generalized histogram coder model is given in algorithm 6 shown in fig. 6A.
Fig. 9 depicts a flowchart of an example method of decoding private data encoded by a mesh tree-based encoding algorithm, according to an example embodiment of the present disclosure. Although fig. 9 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 900 may be omitted, rearranged, combined, and/or debugged in various ways without departing from the scope of the present disclosure.
At 902, a computing system including one or more computing devices may obtain encoded private input data. The encoded private input data may be received from one or more (e.g., multiple) source computing devices. Additionally and/or alternatively, the computing system may be or include an aggregator computing device. For example, the aggregator computing device may aggregate differential private encoded input data from multiple sources while maintaining the privacy of the individual sources. An example algorithm for decoding private data is given in algorithm 4 of fig. 5.
The encoded private input data may include encoded histogram data. For example, the encoded histogram data may be encoded by a generalized histogram encoder model (e.g., as described in fig. 6A). At 904, the computing system may determine a decoded frequency prediction based at least in part on the encoded histogram data. In some embodiments, determining, by the computing system, that the decoded frequency prediction is based at least in part onalgorithm 6 of fig. 6B. The combination of generalized histogram encoder model and decoder model may be for the histogram in the local model>
At 906, the computing system may then construct a net tree based at least in part on the decoding frequency prediction. For example, the computing system may attempt to recreate a net tree constructed to encode encoded private input data at the source based on the decoding frequency prediction. To decode encoded data, a computing system may first construct a frequency prediction using the encoded histogram, where the computing system may construct a net tree using an algorithm described herein (e.g., algorithm 1 of fig. 2)
At 908, the computing system may then run any suitable approximation algorithm for the k-means on the representative set of net trees
Additionally, the encoded private input data mayTo further include encoded vector summation data. For example, the encoded vector sum data may be encoded by a generalized bucket sum algorithm (e.g., as described in fig. 7A). The encoded vector sum data may be decoded to determine a vector sum prediction. For example, at 910, the computing system may determine a decoded vector sum prediction based at least in part on the encoded vector sum data. In some embodiments, determining the decoded vector sum prediction is performed at least in part by determining the vector sum prediction based on sigma i∈[n] z i ·Z v,i To sum the encoded vector sum data. As one example, an example vector sum decoder is given in algorithm 8 shown in fig. 7B. Vector sum prediction may be used for multiple partitions to determine k centers of original input data in original high-dimensional space. For example, at 912, the computing system may upgrade the plurality of partitions to the original higher-dimensional space based at least in part on the decoded vector sum prediction.
Fig. 10 depicts a block diagram of an example computing device 1000 configured to perform any of the operations described herein, according to an example embodiment of the disclosure. For example, computing device 1000 may be a source computing device (e.g., source computing device 102 of fig. 1), an aggregator computing device (e.g., aggregator computing device 104 of fig. 1), and/or any other suitable computing device. In particular, computing device 1000 may be any type of computing device, such as, for example, a personal computing device (e.g., a laptop or desktop), a mobile computing device (e.g., a smart phone or tablet), a game console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The operations may be any suitable operations for implementing systems and methods according to example aspects of the present disclosure. As one example, the operations may cause the computing device 1000 to encode private user data for non-interactive differential private communications, such as in accordance with the method 800 of fig. 8. As another example, the operations may cause the computing device 1000 to decode private data encoded by a mesh tree-based encoding algorithm, such as in accordance with the method 900 of fig. 9. The operations may additionally and/or alternatively include other operations, such as intermediate operations, data processing operations, or other suitable operations.
The computing device 1000 may also include one or more user input components 1010 that receive user input. For example, the user input component 1010 may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other ways in which a user may provide user input.
The technology discussed herein refers to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to these systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, tasks, and task divisions between and among components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components working in combination. The database and application may be implemented on a single system or distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail in connection with the various specific example embodiments thereof, each example is provided by way of explanation and not limitation of the present disclosure. Modifications, variations and equivalents of these embodiments will readily occur to those skilled in the art upon review of the foregoing description. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the subject as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (20)
1. A computer-implemented method for encoding data for communication with improved privacy, the method comprising:
obtaining, by a computing system comprising one or more computing devices, input data comprising one or more input data points;
constructing, by the computing system, a mesh tree comprising potential representations of the one or more input data points, the potential representations being arranged in a plurality of layers, the mesh tree comprising a hierarchical data structure comprising a plurality of hierarchically organized nodes;
Determining, by the computing system, a representation of each of the one or more input data points from the potential representations of the net tree, the representation comprising one of the plurality of hierarchically organized nodes;
the representation of each of the one or more input data points is encoded for communication by the computing system.
2. The computer-implemented method of any preceding claim, further comprising:
projecting the one or more input data points into a random subspace prior to determining a representation of each of the one or more input data points from the potential representations of the net tree; and
after projecting the one or more input data points into the random subspace, scaling the projected input data points into a reduced-dimension subspace.
3. The computer-implemented method of any preceding claim, wherein encoding, by the computing system, the representation of each of the one or more input data points for communication comprises: the representation is encoded by the computing system through a generalized split-bucket vector sum encoder model.
4. The computer-implemented method of claim 3, wherein the generalized block vector sum encoder model includes vector encoding sharing dot products of uniform random components and potential representatives.
5. The computer-implemented method of any preceding claim, wherein encoding, by the computing system, the representation of each of the one or more input data points for communication comprises: the representation is encoded by the computing system through a generalized histogram encoder model.
7. The computer-implemented method of any preceding claim, wherein the representation of an input data point of the one or more input data points comprises a potential representation closest to the input data point.
8. The computer-implemented method of claim 7, wherein the potential representation closest to the input data point comprises a potential representation having a minimum euclidean distance from the input data point relative to each of the other potential representations in the net tree.
9. The computer-implemented method of any preceding claim, wherein the communication comprises a local model of differential privacy.
10. The computer-implemented method of any preceding claim, wherein the communication comprises a shuffling model of differential privacy.
11. The computer-implemented method of any preceding claim, wherein constructing, by the computing system, the net tree including potential representations of the one or more input data points comprises:
determining, by the computing system, an expansion threshold;
identifying, by the computing system, a number of one or more highest ranking nodes in the net tree at a first level in the net tree, the number being equal to the expansion threshold; and
expanding, by the computing system, the one or more highest ranking nodes at a second level in the net tree.
12. The computer-implemented method of claim 11, wherein the expansion threshold is based at least in part on an optimal transmission cost between the one or more input data points and the potential representation.
13. The computer-implemented method of claim 11, wherein the expansion threshold is based at least in part on a lower limit of minimum cost between the one or more input data points and the potential representation.
14. The computer-implemented method of any preceding claim, wherein the net tree comprises a plurality of efficiently decodable nets.
15. A computer-implemented method for decoding data encoded by a net tree based encoding algorithm, the computer-implemented method comprising:
obtaining, by a computing system comprising one or more computing devices, encoded input data comprising encoded histogram data;
determining, by the computing system, a decoded frequency prediction based at least in part on the encoded histogram data;
constructing, by the computing system, a net tree based at least in part on the decoded frequency prediction, the net tree comprising a plurality of leaves;
a k-means approximation algorithm is performed on the net tree by the computing system to divide the plurality of leaves into a plurality of partitions according to respective nearest centers.
16. The computer-implemented method of claim 15, wherein the encoded input data further comprises encoded vector summation data; and wherein the computer-implemented method further comprises:
determining, by the computing system, a decoded vector sum prediction based at least in part on the encoded vector sum data; and
Upgrading, by the computing system, the plurality of partitions to an original higher-dimensional space based at least in part on the decoded vector sum prediction.
17. The computer-implemented method of claim 16, wherein determining the decoded vector sum prediction is performed at least in part by summing the encoded vector sum data multiplied by a shared random component.
18. The computer-implemented method of claim 15, 16, or 17, wherein determining, by the computing system, the decoded frequency prediction is performed at least in part by summing the encoded histogram data multiplied by a shared random component.
19. A computer-implemented method for clustering input data points at a differential privacy guarantee and a reduced approximation ratio, the computer-implemented method comprising:
obtaining, by a computing system comprising one or more computing devices, input data comprising one or more input data points;
constructing, by the computing system, a mesh tree comprising potential representations of the one or more input data points, the potential representations being arranged in a plurality of layers, the mesh tree comprising a hierarchical data structure comprising a plurality of hierarchically organized nodes and a plurality of mappings between the plurality of hierarchically organized nodes; and
A representation of each of the one or more input data points is determined by the computing system from the potential representations of the net tree, the representation comprising one of the plurality of hierarchically organized nodes.
20. The computer-implemented method of claim 19, further comprising:
projecting the one or more input data points into a random subspace prior to determining a representation of each of the one or more input data points from the potential representations of the net tree; and
after projecting the one or more input data points into the random subspace, scaling the projected input data points into a reduced-dimension subspace.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163168533P | 2021-03-31 | 2021-03-31 | |
US63/168,533 | 2021-03-31 | ||
PCT/US2021/064371 WO2022211863A1 (en) | 2021-03-31 | 2021-12-20 | Systems and methods for locally private non-interactive communications |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115997211A true CN115997211A (en) | 2023-04-21 |
Family
ID=80123424
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180046496.4A Pending CN115997211A (en) | 2021-03-31 | 2021-12-20 | System and method for local private non-interactive communication |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230308422A1 (en) |
EP (1) | EP4154154A1 (en) |
CN (1) | CN115997211A (en) |
WO (1) | WO2022211863A1 (en) |
-
2021
- 2021-12-20 EP EP21851905.6A patent/EP4154154A1/en active Pending
- 2021-12-20 WO PCT/US2021/064371 patent/WO2022211863A1/en unknown
- 2021-12-20 CN CN202180046496.4A patent/CN115997211A/en active Pending
- 2021-12-20 US US18/011,995 patent/US20230308422A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2022211863A1 (en) | 2022-10-06 |
US20230308422A1 (en) | 2023-09-28 |
EP4154154A1 (en) | 2023-03-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11829880B2 (en) | Generating trained neural networks with increased robustness against adversarial attacks | |
Li et al. | Structural information and dynamical complexity of networks | |
US11562002B2 (en) | Enabling advanced analytics with large data sets | |
US8583649B2 (en) | Method and system for clustering data points | |
Liu et al. | Fast computation of Tukey trimmed regions and median in dimension p> 2 | |
CN111701247B (en) | Method and equipment for determining unified account | |
CN115293919B (en) | Social network distribution outward generalization-oriented graph neural network prediction method and system | |
Zuo | A new approach for the computation of halfspace depth in high dimensions | |
US10445341B2 (en) | Methods and systems for analyzing datasets | |
US20230125308A1 (en) | Data compression based on co-clustering of multiple parameters for ai training | |
Śmieja et al. | Efficient mixture model for clustering of sparse high dimensional binary data | |
Chakraborty et al. | Biconvex clustering | |
Fang et al. | LASSO isotone for high-dimensional additive isotonic regression | |
Badri | A novel Map-Scan-Reduce based density peaks clustering and privacy protection approach for large datasets | |
Li et al. | Development of a global batch clustering with gradient descent and initial parameters in colour image classification | |
JP7316722B2 (en) | Computational Efficiency in Symbolic Sequence Analysis Using Random Sequence Embedding | |
CN115997211A (en) | System and method for local private non-interactive communication | |
Moothedath et al. | Online algorithms for hierarchical inference in deep learning applications at the edge | |
Li et al. | GLDH: Toward more efficient global low-density locality-sensitive hashing for high dimensions | |
US20230032249A1 (en) | Graphics processing unit optimization | |
Ihler et al. | Using sample-based representations under communications constraints | |
US11914678B2 (en) | Input encoding for classifier generalization | |
CN114065617A (en) | Manufacturing service combination recommendation method and device | |
Buhmann | SIMBAD: emergence of pattern similarity | |
He et al. | Rank-based greedy model averaging for high-dimensional survival data |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |