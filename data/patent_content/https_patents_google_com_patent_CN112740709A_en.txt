CN112740709A - Gated model for video analysis - Google Patents
Gated model for video analysis Download PDFInfo
- Publication number
- CN112740709A CN112740709A CN201980060091.9A CN201980060091A CN112740709A CN 112740709 A CN112740709 A CN 112740709A CN 201980060091 A CN201980060091 A CN 201980060091A CN 112740709 A CN112740709 A CN 112740709A
- Authority
- CN
- China
- Prior art keywords
- video
- model
- training
- frames
- gating
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 claims abstract description 138
- 238000010801 machine learning Methods 0.000 claims abstract description 53
- 238000005070 sampling Methods 0.000 claims abstract description 15
- 238000012549 training Methods 0.000 claims description 172
- 238000013527 convolutional neural network Methods 0.000 claims description 57
- 238000013528 artificial neural network Methods 0.000 claims description 29
- 230000015654 memory Effects 0.000 claims description 25
- 230000004927 fusion Effects 0.000 claims description 12
- 230000000306 recurrent effect Effects 0.000 claims description 8
- 238000012545 processing Methods 0.000 description 26
- 238000004891 communication Methods 0.000 description 20
- 230000009183 running Effects 0.000 description 10
- 230000006870 function Effects 0.000 description 9
- 238000010586 diagram Methods 0.000 description 8
- 239000012634 fragment Substances 0.000 description 8
- 230000000694 effects Effects 0.000 description 6
- 230000006855 networking Effects 0.000 description 6
- 230000008901 benefit Effects 0.000 description 4
- 230000008569 process Effects 0.000 description 4
- 238000004364 calculation method Methods 0.000 description 3
- 239000002131 composite material Substances 0.000 description 3
- 238000004590 computer program Methods 0.000 description 3
- 238000001514 detection method Methods 0.000 description 3
- 239000011521 glass Substances 0.000 description 3
- 230000009191 jumping Effects 0.000 description 3
- 239000011159 matrix material Substances 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 239000002699 waste material Substances 0.000 description 3
- 230000004913 activation Effects 0.000 description 2
- 238000013475 authorization Methods 0.000 description 2
- 230000006399 behavior Effects 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 238000003032 molecular docking Methods 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 230000008520 organization Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000009182 swimming Effects 0.000 description 2
- 230000001052 transient effect Effects 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 241000288673 Chiroptera Species 0.000 description 1
- 206010044565 Tremor Diseases 0.000 description 1
- 230000009471 action Effects 0.000 description 1
- 230000004931 aggregating effect Effects 0.000 description 1
- 230000002776 aggregation Effects 0.000 description 1
- 238000004220 aggregation Methods 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000000386 athletic effect Effects 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 230000009977 dual effect Effects 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 238000012886 linear function Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000002028 premature Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 239000011435 rock Substances 0.000 description 1
- 230000035945 sensitivity Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 238000000638 solvent extraction Methods 0.000 description 1
- 238000001228 spectrum Methods 0.000 description 1
- 230000006641 stabilisation Effects 0.000 description 1
- 238000011105 stabilization Methods 0.000 description 1
- 230000000087 stabilizing effect Effects 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 230000000699 topical effect Effects 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
- 210000000707 wrist Anatomy 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/44—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs
- H04N21/44008—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics in the video stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/23418—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/25—Fusion techniques
- G06F18/254—Fusion techniques of classification results, e.g. of results related to same input data
- G06F18/256—Fusion techniques of classification results, e.g. of results related to same input data of results relating to different input data, e.g. multimodal recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/166—Editing, e.g. inserting or deleting
- G06F40/169—Annotation, e.g. comment data or footnotes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/80—Fusion, i.e. combining data from various sources at the sensor level, preprocessing level, feature extraction level or classification level
- G06V10/809—Fusion, i.e. combining data from various sources at the sensor level, preprocessing level, feature extraction level or classification level of classification results, e.g. where the classifiers operate on the same input data
- G06V10/811—Fusion, i.e. combining data from various sources at the sensor level, preprocessing level, feature extraction level or classification level of classification results, e.g. where the classifiers operate on the same input data the classifiers operating on different input data, e.g. multi-modal recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
- G06V20/47—Detecting features for summarising video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/439—Processing of audio elementary streams
- H04N21/4394—Processing of audio elementary streams involving operations for analysing the audio stream, e.g. detecting features or characteristics in audio streams
Abstract
Embodiments described herein relate to methods, apparatuses, and computer-readable media for performing gating for video analysis. In some implementations, a computer-implemented method includes obtaining a video that includes a plurality of frames and corresponding audio. The method further includes performing sampling based on the target frame rate to select a subset of the plurality of frames and extracting a respective audio spectrogram for each frame of the subset of the plurality of frames. The method further includes reducing a resolution of a subset of the plurality of frames. The method further includes applying a machine learning based gating model to a subset of the plurality of frames and the corresponding audio spectrogram, and obtaining as an output of the gating model an indication of whether to analyze the video to add one or more video annotations.
Description
Background
The user uploads the images and videos to an online image management service. Some services perform video annotation. For example, the video annotation includes a tag indicating a face of a person, a subject (e.g., a birthday cake), a movement (e.g., jumping, running, etc.), a sound (e.g., laughter), and so forth in the uploaded video. Video annotations are generated by analyzing videos using programming techniques.
The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.
Disclosure of Invention
Embodiments described herein relate to methods, devices, and computer-readable media for determining whether to analyze a video to add one or more video annotations. In some implementations, a computer-implemented method includes obtaining a video that includes a plurality of frames and corresponding audio. The method further includes performing sampling based on the target frame rate to select a subset of the plurality of frames. In some implementations, the target frame rate is less than or equal to a frame rate of the video. The method further includes extracting a respective audio spectrogram of each frame of the subset of the plurality of frames. The method further includes reducing a resolution of a subset of the plurality of frames, and applying a machine learning based gating model to the subset of the plurality of frames and the corresponding audio spectrogram after reducing the resolution. The method further includes obtaining, as an output of the gating model, an indication of whether to analyze the video to add one or more video annotations.
In some embodiments, the method may further comprise, prior to applying the gating model, dividing the video into a plurality of segments, each segment comprising a plurality of frames, and wherein applying the gating model is performed iteratively in sequence through the plurality of segments, wherein the indication is generated at each iteration. In some embodiments, each segment of the plurality of segments may overlap with another segment of the plurality of segments. In some embodiments, if the indication at a particular iteration is to analyze the video, application of the gating model is terminated such that one or more of the plurality of segments are excluded.
In some embodiments, the gating model is trained to determine whether a particular feature is present in the input video provided to the gating model. In some implementations, the particular feature includes at least one of a face, a type of object, a type of movement, or a type of audio.
In some embodiments, applying the gating model may include: applying a first model that determines a likelihood that a particular feature is present; and applying a second model that receives as input the likelihood that the particular feature is present and generates the indication of whether to analyze the video. In some embodiments, the first model comprises a first convolutional neural network comprising a plurality of layers trained to analyze video; a second convolutional neural network comprising a plurality of layers trained to analyze audio; and a fusion network comprising a plurality of layers, the fusion network receiving as inputs the outputs of the first and second convolutional neural networks and providing to the second model a likelihood that the particular feature is present. In some implementations, the second model is implemented using one or more of heuristic, recurrent neural networks, or markov chain analysis techniques. In some embodiments, the method may further comprise providing additional input to the second model. The additional input may include one or more of an identification of a portion of a particular frame of the subset of the plurality of frames in which the particular feature is detected to be present, a duration in which the particular feature appears in the subset of the plurality of frames, or a heuristic related to early termination. In these embodiments, the second model utilizes the additional input to generate the indication.
In some implementations, when the indication is to analyze the video, the method may further include programmatically analyzing the video to add the one or more video annotations. The video annotation may include one or more tags indicating the presence of one or more of a face, a particular type of object, a particular type of movement, or a particular type of audio in the video.
Some implementations may include a computing device to analyze a video to add one or more video annotations. The apparatus may include a processor and a processor having instructions stored thereon. The instructions, when executed by the processor, cause the processor to perform operations that may include obtaining video that includes a plurality of frames and corresponding audio. The operations may further include performing sampling to select a subset of the plurality of frames based on a target frame rate that is less than or equal to a frame rate of the video. The operations may further include extracting a respective audio spectrogram from the audio of each frame of the subset of the plurality of frames. The operations may further include reducing a resolution of a subset of the plurality of frames. The operations may further include: after reducing the resolution, applying a machine learning based gating model to a subset of the plurality of frames and corresponding audio spectrograms. The operations may further include obtaining, as an output of the gating model, an indication of whether to analyze the video to add one or more video annotations.
In some implementations, the memory may include instructions stored thereon that, when executed by the processor, cause the processor to perform further operations including, prior to applying the gating model, dividing the video into a plurality of segments. Each segment may include a plurality of frames. In these embodiments, applying the gating model is performed iteratively in sequence through the plurality of segments, and the indication is generated at each iteration.
Embodiments described herein further relate to methods, devices, and computer-readable media for training a machine learning based gating model to generate an indication of whether to analyze a video to add annotations corresponding to particular features. The machine learning based gating model may include a first model including a first convolutional neural network that generates a likelihood that the particular feature is present in a video based on video frames of the video; and a second model that receives as input a likelihood that the particular feature is present in the video and generates the indication. In some embodiments, a computer-implemented method includes obtaining a training set including a plurality of training videos. Each training video may include a plurality of frames. Each training video is a low resolution sampled version of the corresponding high resolution video. The training set further includes a plurality of training labels. Each training label indicates the presence of the particular feature in the high resolution video corresponding to one or more of the plurality of training videos.
The method further includes training a gating model, which includes generating a likelihood that the particular feature is present in the training video by applying the first model to the training video. Training the gating model further comprises generating, by applying the second model, an indication of whether to analyze the training video to add annotations corresponding to particular features based on the likelihood that the particular features are present in the training video. Training the gating model further includes generating feedback data based on the training labels associated with the corresponding high-resolution video and the indication, and providing the feedback data as a training input for the first model and a training input for the second model. Training the gating model may be performed for each training video in the training set.
In some implementations, the particular feature includes at least one of a face, a type of movement, or a type of object. In some implementations, the plurality of training videos in the training set includes at least one video in which the particular feature is present and at least one video in which the particular feature is not present. In these embodiments, training the gating model includes one or more of automatically adjusting weights of one or more nodes of the first convolutional neural network of the first model or automatically adjusting connectivity between one or more pairs of nodes of the first convolutional neural network of the first model.
In some embodiments, wherein the second model of the gating model comprises one or more of a heuristic-based model, a recurrent neural network, or a markov chain analysis model. In these embodiments, training the gating model includes automatically adjusting one or more parameters of the heuristic-based model, the recurrent neural network, or the markov chain analysis model.
In some embodiments, training the gating model may further include dividing a plurality of frames of the training video into a plurality of frame stacks. Each stack may include at least one frame. The plurality of stacks may be organized in an ordered sequence. In these embodiments, training the gating model is performed sequentially for each of a plurality of frame stacks. In these embodiments, the second model is configured to store the generated indication for each stack of the training video. Further, in these embodiments, generating the indication for a particular stack is further based on respective stored indications of one or more previous stacks in the ordered sequence.
In some implementations, one or more of the plurality of training videos may include audio spectrograms corresponding to the plurality of frames. In these embodiments, the first model may further include a second convolutional neural network trained to analyze an audio spectrogram; and a fusion network that receives as inputs the outputs of the first and second convolutional neural networks and that generates a likelihood that the particular feature is present in the video.
Drawings
FIG. 1 is a block diagram of an example network environment that may be used for one or more embodiments described herein.
Fig. 2 is a flow diagram illustrating an example method 200 according to some embodiments.
Fig. 3 illustrates the operation of an example gating model 300 according to some embodiments.
Fig. 4 is a flow diagram illustrating an example method 400 for training a machine learning based gating model, in accordance with some embodiments.
Fig. 5 illustrates respective outputs of an example video and gating model.
Fig. 6 is a block diagram of an example device that may be used in one or more embodiments described herein.
Detailed Description
A user captures video using a camera, such as a smartphone or other device. A user may store such videos on a client device or server, e.g., a server that provides a video hosting service. An application may be provided via a user's client device and/or server that enables the user to search for videos, for example, by using keywords or key phrases such as "john's birthday," "mary's graduation," "my baseball game on weekend," and so forth.
To enable a quick search for a user's video, the application may generate and store annotations in association with the user's video. The annotation may be or may include a tag. For example, the annotation may indicate a feature described in the video, such as the presence of a human face (and whether the face is associated with a particular person), the presence of an object type (e.g., cake, candle, baseball bat, etc.), the presence of a movement type, the presence of a behavior or activity (e.g., running, dancing, doing sports, etc.). When a user performs a search, the annotations are analyzed to identify videos that match the search. For example, in response to a search for "last weekend my baseball game," the annotations may be analyzed to determine whether one or more particular annotations, such as "baseball bats," "baseball caps," "stadiums," etc., are associated with the video to determine whether the video matches the search. If user consent is obtained, the annotations may be automatically analyzed to identify videos that match certain criteria to perform certain system tasks, e.g., to find users with whom videos are to be automatically shared, to find relevant videos or portions thereof presented on user devices (e.g., combined into a topical presentation or other image-based creation based on video content such as depicted objects, activities, etc.), and so forth.
Analyzing a video to add one or more annotations to the video may be computationally expensive. If the annotation corresponds to a particular feature, the entire video may need to be analyzed to determine if one or more particular features are present in the video (or in one or more segments of the video), and if a particular feature is present, the corresponding annotation may be added to the video. This operation may be wasteful, for example, analysis of the video may waste computational resources and energy if certain features are not present in the video.
Furthermore, on certain devices (e.g., devices with limited processing power, devices with limited power capacity (e.g., battery-powered devices)), analyzing the video to add annotations may not be feasible or may be particularly expensive. If a user has a video library that includes multiple videos that do not include a particular feature, computationally expensive operations to analyze the video may be run for each of the multiple videos. Furthermore, when only a portion of a video depicts a particular feature, analyzing the entire video may waste computing resources.
Some embodiments include methods, apparatus, and computer-readable media having instructions for performing gated analysis of video. The gating analysis may be performed by applying a trained machine learning-based gating model that generates an indication of whether to analyze the video or one or more segments of the video to add one or more video annotations.
Using a gating model to generate an indication may provide several technical advantages. For example, gating models may be associated with substantially lower computational costs than video analysis techniques used to analyze video to detect the presence of particular features and add corresponding annotations.
For example, the gating model may be applied to a low-resolution, sampled subset of the frames of the video, and therefore, have a lower computational cost than analyzing the original high-resolution video. Furthermore, only such videos for which the gating model generates a positive indication need be analyzed, saving computational cost and power.
The use of gating models may also enable video annotation to be performed on devices with low computational power or limited power. Further, in some implementations, the indication from the gating model may include identifying one or more segments of the video for analysis based on a likelihood that a particular feature is present in the one or more segments. In these embodiments, other segments of the video may be excluded from the analysis to add video annotations, thereby saving computational costs. The gating model may be applied to any number of videos. For example, if a user has a large number of videos, e.g., a thousand videos, a gating model may be applied to identify videos that are to be further analyzed using techniques with high computational cost to add annotations rather than analyzing all videos. In this example, a subset of videos (e.g., three hundred of a thousand videos) identified by the gating model for further analysis, e.g., ten of a hundred videos, are further analyzed without further analysis of other videos. The total computational cost-i.e., the sum of the computational cost of using the gating model for one thousand videos plus the computational cost of further analyzing three hundred videos using high cost techniques-is lower than the computational cost of analyzing one thousand videos using high cost techniques.
In some embodiments, the gating model may be implemented as a two-stage model that includes a first model trained to determine the likelihood that a particular feature is present in the video and a second model trained to utilize likelihood prediction (or, e.g., a sequence of likelihood predictions), e.g., by the output of the first model as input, to generate an indication of whether to analyze the video. In some embodiments, the two-stage model may include a plurality of different first models that determine the likelihood of the presence of a particular feature and a single second stage model.
In some embodiments, when the gated model is implemented as a two-stage model including a first model and a second model, the first model and the second model may be trained independently of each other. For example, the first model may be trained to determine the likelihood that a particular feature (e.g., a human face, a type of object, a type of movement, a type of audio, etc.) is present in the video. The training may be performed independently of the training of the second model, e.g., by providing feedback data obtained based on training labels associated with the training video.
For example, the second model may be trained to utilize likelihood predictions (or likelihood prediction sequences) as input to generate an indication as to whether to analyze the video to add annotations. The training may be performed independently of the training of the first model, e.g. by providing different likelihood values as inputs and training labels as feedback data. With corresponding improvements in performance (e.g., accuracy, computational cost, etc.) of the gated models as a whole, each model can thus be evaluated and trained separately from the other model.
Like reference numerals are used in fig. 1 to identify like elements. A letter following a reference numeral (e.g., "156 a") indicates that the text refers specifically to the element having that particular reference numeral. Reference numerals such as "156" that do not include the following letters in the text refer to any or all of the elements in the figures that carry the reference numeral (e.g., "156" herein refers to reference numerals "156 a" and/or "156 b" in the figures).
Fig. 1 illustrates a block diagram of an example network environment 100 that may be used in some implementations described herein. In some implementations, the network environment 100 includes one or more server systems, such as the server system 102 in the example of fig. 1. For example, server system 102 may communicate with network 130. Server system 102 may include a server device 104 and a database 106 or other storage device. Database 106 may store one or more images and/or videos and metadata associated with the one or more images and/or videos. In some implementations, the server device 104 can provide an image management application 156 b. The image management application 156b may access the images 106b stored in the database 106.
For ease of illustration, fig. 1 shows one block for server system 102, server device 104, and database 106, and shows four blocks for client devices 120, 122, 124, and 126. Server blocks 102, 104, and 106 may represent multiple systems, server devices, and network databases, and these blocks may be provided in different configurations than shown. For example, server system 102 may represent multiple server systems that may communicate with other server systems via network 130. In some implementations, for example, the server system 102 can include a cloud hosting server. In some examples, database 106 and/or other storage devices may be provided in a server system block separate from server device 104 and may be in communication with server device 104 and other server systems via network 130.
There may be any number of client devices. Each client device may be any type of electronic device, such as a desktop computer, laptop computer, portable or mobile device, cell phone, smart phone, tablet computer, television, TV set-top box or entertainment device, wearable device (e.g., display glasses or goggles, wrist watch, headset, armband, jewelry, etc.), Personal Digital Assistant (PDA), media player, gaming device, etc. Some client devices may also include a local database similar to database 106 or other storage. In some implementations, network environment 100 may not have all of the components shown and/or may have other elements including other types of elements instead of or in addition to those described herein.
In various implementations, end users U1, U2, U3, and U4 may use respective client devices 120, 122, 124, and 126 to communicate with server system 102 and/or each other. In some examples, users U1, U2, U3, and U4 may interact with each other via web services implemented on server system 102 (e.g., social networking services, image hosting services, other types of web services) via applications running on respective client devices and/or server system 102. For example, respective client devices 120, 122, 124, and 126 may communicate data to and from one or more server systems (e.g., system 102).
In some implementations, the server system 102 can provide appropriate data to the client devices so that each client device can receive communication content or shared content uploaded to the server system 102 and/or a network service. In some examples, users U1-U4 may interact via audio or video conferencing, audio, video, or text chat, or other communication modes or applications. The network services implemented by the server system 102 may include systems that allow users to perform various communications, form links and associations, upload and publish shared content such as images, text, video, audio, and other types of content, and/or perform other functions. For example, the client device may display received data, such as content posts sent or streamed to the client device and originating from a different client device (or directly from a different client device) or originating from a server system and/or network service via a server and/or network service. In some implementations, the client devices may communicate directly with each other, e.g., using peer-to-peer communication between the client devices as described above. In some embodiments, a "user" may include one or more programs or virtual entities, as well as a person interfacing with a system or network.
In some implementations, any of client devices 120, 122, 124, and/or 126 may provide one or more applications. For example, as shown in fig. 1, the client device 120 may provide a camera application 152 and an image management application 156 a. Client device 122-126 may also provide similar applications. For example, the camera application 152 may provide users of respective client devices (e.g., users U1-U4) with the ability to capture images using cameras of their respective user devices. For example, the camera application 152 may be a software application executing on the client device 120.
In some implementations, the camera application 152 can provide a user interface. For example, the user interface may enable a user of the client device 120 to select an image capture mode, such as a still image (or photo) mode, a burst mode (e.g., capturing a continuous number of images over a short period of time), a moving image mode, a High Dynamic Range (HDR) mode, and so forth. For example, the video mode may correspond to the capture of a video that includes a plurality of frames and may be of any length. Further, the video mode may support different frame rates, e.g., 25 frames per second (fps), 30fps, 50fps, 60fps, etc. One or more parameters of image capture may be changed during the capture of video. For example, a user may zoom in on a scene or zoom out using a client device while capturing a video.
In some implementations, the camera application 152 can implement (e.g., partially or fully) the methods described herein with reference to fig. 2 and 4. In some implementations, the image management application 156a and/or the image management application 156b can implement (e.g., partially or wholly) the methods described herein with reference to fig. 2 and 4.
The camera application 152 and the image management application 156a may be implemented using hardware and/or software of the client device 120. In various embodiments, the image management application 156a may be a stand-alone application, for example, executing on any of the client devices 120 and 124, or may work in conjunction with the image management application 156b provided on the server system 102.
Under user permission, the image management application 156 may perform one or more automatic functions, such as storing (e.g., backing up) images or videos (e.g., to the database 106 of the server system 102), enhancing images or videos, stabilizing images or videos, identifying one or more features in an image, such as a face, a body, a type of object, a type of movement, and so forth. In some examples, image or video stabilization may be performed based on input from an accelerometer, gyroscope, or other sensor of client device 120 and/or based on a comparison of multiple frames of a moving image or video.
The image management application 156 may also provide image management functionality, such as in a user interface (e.g., displaying images and/or videos in a one-up view containing a single image, in a grid view including multiple images, etc.), editing images and/or videos (e.g., adjusting image settings, applying filters, changing image focus, removing one or more frames of a moving image or video), sharing images with other users (e.g., of client device 120) with 126, archiving images (e.g., storing images so that they do not appear in a main user interface), generating image-based authoring (e.g., collage, album, motion-based artifacts (such as animation, story, video rotation, etc.). in some embodiments, to generate image-based authoring, image management application 156 may utilize one or more tags associated with an image or video.
In some implementations, the image management application 156 can programmatically analyze the image or video to detect one or more features in the image by utilizing object recognition techniques. In some implementations, the image management application 156 can store one or more tags associated with the images or videos in the database 106 and/or a local database on the client device (not shown).
The database 106 may store tags (e.g., content annotations) associated with one or more of the images and/or videos. For example, the label may include an indication of whether a particular feature appears in an image or video. For example, the particular feature may be, for example, a human face, a type of subject (e.g., birthday cake, sports equipment, trees, etc.), a type of movement (e.g., jumping, skiing, etc.), a type of audio (e.g., human speech, laughter, music, natural sounds), and so forth. One or more of the tags may also include a particular timestamp, e.g., the timestamp of the tag associated with the type of motion may include a start and end timestamp, which correspond to the start and end of motion in the image or video, respectively. In some implementations, the label may indicate the type of scene depicted in the image or video, e.g., a seaside sunset, a skier, a birthday scene, a wedding, a graduation ceremony, etc.
User interfaces on client devices 120, 122, 124, and/or 126 may enable display of user content and other content, including images, videos, data, and other content as well as communications, privacy settings, notifications, and other data. Such a user interface may be displayed using software on a client device, software on a server device, and/or a combination of client software and server software executing on server device 104 (e.g., application software or client software in communication with server system 102). The user interface may be displayed by a display device (e.g., a touch screen or other display screen, a projector, etc.) of the client device or the server device. In some implementations, an application running on a server system may communicate with a client device to receive user input at the client device and to output data, such as visual data, audio data, and the like, at the client device.
In some implementations, the server system 102 and/or any of the one or more client devices 120 and 126 can provide a communication application. The communication program may allow a system (e.g., a client device or server system) to provide options for communicating with other devices. The communication program may provide one or more associated user interfaces that are displayed on a display device associated with the server system or the client device. The user interface may provide various options to the user to select a communication mode, a user or device with which to communicate, and the like. In some examples, the communication program may provide an option to send or broadcast the content post, e.g., to a broadcast area, and/or may output a notification indicating that the content post has been received by a device and, for example, a device in the defined broadcast area of the post. The communication program may display or otherwise output the transmitted content posts and the received content posts, for example, in any of a variety of formats. The content posts may include, for example, images shared with other users.
Other implementations of the features described herein may use any type of system and/or service. For example, other networking services (e.g., connecting to the internet) may be used instead of or in addition to social networking services. Any type of electronic device may utilize the features described herein. Some embodiments may provide one or more features described herein on one or more client or server devices that are disconnected from or intermittently connected to a computer network. In some examples, a client device that includes or is connected to a display device may display data (e.g., content) stored on a storage device local to the client device, such as images previously received over a communication network.
Fig. 2 is a flow diagram illustrating an example method 200 according to some embodiments. In some implementations, the method 200 may be implemented, for example, on the server system 102 as shown in fig. 1. In some implementations, some or all of method 200 may be implemented on one or more client devices 120, 122, 124, 126, one or more server devices, and/or on both server and client devices, as shown in fig. 1. In the depicted example, the implementation system includes one or more digital processors or processing circuits ("processors"), as well as one or more storage devices (e.g., database 106 or other memory). In some implementations, different components of one or more servers and/or clients may perform different blocks or other portions of method 200. In some examples, a first device is described as performing the blocks of method 200. Some embodiments may have one or more blocks of method 200 that may be performed by one or more other devices (e.g., other client devices or server devices) that may send results or data to the first device.
In some implementations, the method 200 or portions of the method can be initiated automatically by the system. In some embodiments, the implementation system is a first device. For example, the method (or portions thereof) may be performed periodically, or based on one or more particular events or conditions, e.g., video capture is initiated by a user using an application (e.g., camera application 152, image management application 156, etc.), receiving one or more videos that have been newly uploaded to or accessible by the system, a predetermined time period has expired since the last execution of method 200, and/or one or more other conditions that may occur as specified in settings read by the method. In some embodiments, such conditions may be specified in a user's stored custom preferences.
In various implementations, the client device 120 may be a standalone camera, another device including a camera, e.g., a smartphone, a tablet, a computer, a wearable device such as a smartwatch, a headset, etc., or other client device that may receive images or video captured by other devices. In some implementations, the client device 120 may be a capture-only device, e.g., a camera that does not include a screen. In some implementations, the client device 120 may be a view-only device, e.g., a device that includes a screen on which images or video may be displayed but does not have a camera or other capability to capture images or video. In some implementations, the client device 120 may have both capture and viewing capabilities.
In some implementations, the client device 120 may include a single camera to capture images or video. In some implementations, the client device 120 may include multiple cameras (or lenses). For example, a smartphone or other device may include one or more front-facing cameras (on the same side of the device as the screen) and/or one or more rear-facing cameras. In some implementations, one or more front or rear facing cameras may operate together during capture, e.g., a first camera may capture depth information while a second camera may capture image pixels of an image or video. In some implementations, different cameras may be used for different types of image or video capture (e.g., telephoto lens, wide-angle lens, etc.), for example, with different zoom levels. In some implementations, the client device 120 may be configured to capture 360 degree images or video. In some implementations, the camera or lens may capture images using a single image sensor (e.g., a CCD or CMOS sensor) or multiple sensors. In some embodiments, other sensors, e.g., depth sensors, etc., may be used with one or more cameras at the time of image capture.
In some implementations, the client device 120 may combine raw image data captured from one or more cameras (or lenses) at an image sensor with other data obtained from other sensors (e.g., accelerometers, gyroscopes, position sensors, depth sensors, etc.) to form a single image or video. For example, when the client device 120 operates in a mode of capturing a plurality of image frames (e.g., a burst mode or a motion mode of capturing a plurality of frames as a moving image in quick succession, a video mode of capturing a video, a high dynamic range mode of combining a plurality of images with different exposure amounts into a single composite image, etc.), the captured image or video may be stabilized using data obtained from the sensor. For example, accelerometer or gyroscope data may be utilized to compensate for camera motion, e.g., due to hand trembling of a user being captured during capture, by aligning multiple captured frames. In some implementations, the captured image or video may be cropped to produce, for example, a stable version with reduced background motion.
Client device 120 may enable a user to capture images in different modes (e.g., a still image (or photo) mode that captures a single frame, a burst or moving image mode that captures multiple frames, a video mode that captures video including multiple frames, etc.). In some implementations, the method 200 may be performed at capture, after capture is complete, or at a later time (e.g., when the client device 120 is not actively used by a user and has sufficient power) when the camera configures multiple frames, e.g., via a battery or via coupling to an external power source.
Client device 120 may enable a user to view, in different user interfaces, images or videos captured by client device 120 or associated with the user, for example. For example, a leading mode or a slideshow mode may be provided that enables a user to view a single image or video at a time. In another example, a gallery mode may be provided that enables a user to view multiple images simultaneously, e.g., as an image grid.
In some implementations, the client device 120 can perform the method 400. In another example, a client device or a server device may perform method 200. In some implementations, the method 200 may be implemented by a server device. In some implementations, the method 200 may be initiated automatically, for example, when a user of a client device operates a camera to capture a video, download a video to a client device, upload a video to a server, or the like.
An image as referred to herein may include a digital image having pixels with one or more pixel values (e.g., color values, luminance values, etc.). The image may be a static image (e.g., a still photograph, an image having a single frame, etc.), or a moving image (e.g., an image comprising a plurality of frames, such as an animation, an animated GIF, a motion picture film in which a portion of the image includes motion and the other portion is static, etc.). Video as referred to herein includes a plurality of frames with or without audio. In some implementations, one or more camera settings may be modified during the capture of the video, e.g., zoom level, aperture, etc. In some implementations, a client device that captures video may be moved during video capture. As referred to herein, text may include alphanumeric characters, emoticons, symbols, or other characters.
In block 202, it is checked whether user consent (e.g., user permission) to use the user data has been obtained in an implementation of the method 200. For example, the user data may include images or videos captured by the user using the client, images or videos stored or accessed by the user, e.g., using a client device, image/video metadata, user data related to use of the messaging application, user preferences, user biometric information, user characteristics (e.g., identity, name, age, gender, occupation, etc.), information about the user's social networks and contacts, social and other types of behaviors and activities created or submitted by the user, content, ratings and opinions, the user's current position, historical user data, images generated, received and/or accessed by the user, images viewed or shared by the user, and so forth. In certain embodiments, one or more blocks of the methods described herein may use such user data.
If user consent has been obtained in method 200 from the relevant users who may use the user data, then in block 204, it is determined that the blocks of the method herein may be implemented with possible use of the user data as described for those blocks, and the method proceeds to block 212. If user consent is not obtained, it is determined in block 206 that the block is to be implemented without using user data, and the method proceeds to block 212. In some embodiments, if user consent is not obtained, blocks need not be implemented using user data and using synthetic data and/or data that is generic or publicly accessible and publicly available. In some embodiments, method 200 is not performed if user consent is not obtained. For example, if the user denies permission to access one or more videos, method 200 is not performed or method 200 will be stopped after performing block 206.
In block 210, a video is obtained. For example, the video may be a video captured by a user using any of client devices 120 and 126. In another example, videos may be downloaded by a user, for example, from a video sharing website, social network, online video library, or other online resource, and stored on a client device or server device. In yet another example, the video may be downloaded by the user via a messaging application, such as an instant messaging application, a chat application, a Rich Communication Services (RCS) application.
In some implementations, a video can include a plurality of frames and corresponding audio. Each frame of video may be a still image comprising a plurality of pixels. In some implementations, the video may exclude audio. The video may have a frame rate, for example, the frame rate at which the video was captured. For example, the frame rate may be 24 frames per second (fps), 25fps, 30fps, 50fps, 60fps, 72fps, 100fps, and the like. The frame rate of the video may indicate the number of available image frames per second of the video. In some implementations, one or more frames of a plurality of frames of a video may each be associated with a respective timestamp.
In some implementations, the video may be a streaming video or video file of a particular format. In some implementations, the plurality of video frames may be stored separately from the audio of the video. In these embodiments, synchronization information may be provided in the video. The synchronization information may be used to synchronize audio with a plurality of video frames during playback of the video. In some implementations, the audio may be stored in a compressed format. Block 210 may be followed by block 212.
In block 212, sampling is performed to select a subset of the plurality of frames of the video. In some implementations, the sampling may be performed based on a target frame rate (e.g., 5fps, 6fps, 10fps, 20fps, etc.). In some implementations, the sampling may be performed iteratively on the video, e.g., video that may correspond to a certain number of frames per second, e.g., 25 frames of 25fps video, and a respective subset of the frames may be selected among the subsets. In some implementations, the sampling can include random sampling, e.g., 5 frames per second can be randomly selected from the video to obtain a subset of the plurality of frames at the target frame rate. In some implementations, sampling may include selecting every nth frame to obtain a subset of frames, e.g., every 5 th frame of 25fps video may be selected to obtain a target frame rate of 5 fps. In different embodiments, other sampling strategies may be used.
In some implementations, the target frame rate is less than the frame rate of the video. Sampling the video can reduce the processing cost for the method compared to processing the entire video, since only a subset of the frames need to be analyzed in subsequent steps of the method 200. In some implementations, when the video has a low frame rate (e.g., 5fps, 6fps), the target frame rate may be equal to the frame rate of the video. Block 212 may be followed by block 214.
In block 214, a respective audio spectrogram may be extracted for each frame of the subset of the plurality of frames selected in block 212. An audio spectrogram can be extracted from audio of a video. In some implementations, the audio spectrogram is based on audio from a time span that is not just the corresponding frame (e.g., the frame in the subset). For example, the audio spectrogram of a particular frame may include audio corresponding to a video frame corresponding to 0.5 seconds prior to the particular frame. In another example, the audio spectrogram for a particular frame can include audio corresponding to a video frame corresponding to the next 0.5 seconds of video after the particular frame. In some implementations, the audio spectrogram of a particular frame can be based on audio from previous and subsequent video frames for a particular duration (e.g., 0.5 seconds, 1 second, etc.). In different embodiments, the particular duration of audio used for the audio spectrogram may be the same (e.g., 0.5 seconds before and after) or different (e.g., 1 second before and 0.5 second after) for audio corresponding to a previous frame from audio corresponding to a subsequent frame. In some implementations, the audio spectrogram can be a frequency domain representation of audio. In some embodiments, the audio spectrogram may be a mel-frequency spectrogram. Block 214 may be followed by block 216.
In block 216, the resolution of a subset of the plurality of frames may be reduced. For example, if the video is a high definition (e.g., 720p, 1080p, 2K, 4K, 8K, etc.) video, downsampling may be performed on each frame in the subset to reduce the resolution of each frame. In some implementations, downsampling may include selecting a subset of pixels of a frame. In some implementations, the resolution of the reduced resolution video may be 128 × 128 pixels. In some embodiments, the number of pixels in the vertical direction (height of the frame) may be different from the number of pixels in the horizontal direction (width of the frame). For example, based on the available computing power of the device implementing method 200, the number of pixels in the reduced resolution frame may be chosen to optimize performance. In some implementations, the downsampling may include interpolation of one or more frames. In some embodiments, bilinear interpolation is used to reduce resolution. In some implementations, the downsampling may include content-aware downsampling. For example, a blurred region of an image frame may be downsampled more aggressively than a sharp region or a region including edges. In some implementations, the video frame may be cropped. For example, a video frame may be reduced in resolution to 140x140 pixels and then cropped to resize the frame to 128x128 pixels. In some implementations, cropping is performed to select a random patch of the target resolution, e.g., 128x128 pixels at random. Random cropping may result in different portions of the original video frame being included in a subset of the frames. This may improve robustness to local occlusions, fast subject motion in the video, etc. Reducing the resolution of the frames may reduce the computational cost of subsequent steps of the method 200. In some implementations, one or more other operations may be performed after reducing the resolution of the subset of the plurality of frames. For example, image transformations may be performed, for example, altering the color space of an image frame. For example, the color space of an image may be changed from RGB to sRGB. Block 216 may be followed by block 218.
In block 218, a subset of the plurality of frames may be divided into segments (also referred to as stacks). For example, each fragment or stack may include a particular number of frames, e.g., 3 frames, 5 frames, 10 frames, etc. These fragments or stacks may be sequential. For example, a first segment that includes a frame corresponding to timestamp t-1 may be followed in the sequence by a second segment that includes a frame corresponding to timestamp t, which in turn may be followed in the sequence by a third segment that includes a frame corresponding to timestamp t + 1. In some embodiments, the segments may be overlapping segments, e.g., one or more frames of a particular segment may be common to one or more other segments. Any number of frames may overlap, less than the total number of frames in a segment. For example, a slice may include 3 frames, where one frame overlaps a previous slice and one frame overlaps a next slice. A sliding window technique may be utilized to divide a subset of frames into multiple segments, e.g., where the window specifies a first segment at a first location, the window moves many frames in a direction (forward or backward) to a second location, the window specifies a second segment at the second location, etc. In some embodiments, the segments may be non-overlapping segments. Block 218 may be followed by block 220.
In block 220, a machine learning based gating model (also referred to as a gating model) may be applied to the fragments. The machine learning-based gating model may include one or more neural networks, e.g., convolutional neural networks, recursive neural networks, etc., and/or other types of models, e.g., heuristic-based models, markov chain technique-based models, etc. The gating model may be trained to generate an indication of whether to further analyze the video to add one or more video annotations. For example, in some implementations, the gating model may receive frames in a segment as input and generate an indication as output. In another example, in some implementations, the gating model may receive as input frames in a segment and a corresponding audio spectrogram and generate as output an indication.
In some implementations, the gating model may include a plurality of machine learning models. For example, the gating model may include a first model (also referred to as model a) that includes a first convolutional neural network that is trained to determine whether a particular feature is present in the input video provided to the gating model based on a subset of a plurality of video frames obtained from the input video. For example, in some implementations, the particular feature may include a human face, a type of object, or a type of movement. In another example, the first model may further include a second convolutional neural network trained to determine whether a particular feature is present in the input video provided to the gating model based on audio spectrograms corresponding to a subset of a plurality of video frames obtained from the input video. For example, in some implementations, the particular feature may include a type of audio. For example, the type of audio may include human speech, music, and so forth.
For example, the human face may be a known face, such as a face of a person that has been previously depicted in an image and/or video in an image library of the user captured or otherwise obtained as an input video. The face may also correspond to a famous person, such as an actor, a host, a politician, a celebrity, an athlete, and so forth. In another example, the type of object may be any object, such as a cake (e.g., a birthday cake), a swimming pool, a tree, a flower, a racket, or other sports equipment, and so forth. In yet another example, the type of motion may be jumping, running, swimming, dancing, and the like. In some implementations, the human utterance may be based on, for example, an utterance of a person with a known voice signature based on previous videos in the user image library (if user consent has been obtained). In some embodiments, the human utterance may include an utterance of a famous person (such as an actor, a television host, a politician, a celebrity, an athlete, and so forth).
In some embodiments, for example, when the first model includes a first convolutional neural network and a second convolutional neural network, the first model may further include a fusion network that combines the outputs of the first and second convolutional neural networks to determine whether a particular feature is present in the input video provided to the first model. In some implementations, the first convolutional neural network may include multiple layers and may be trained to analyze video, e.g., video frames. In some implementations, the second convolutional neural network may include multiple layers and may be trained to analyze audio, e.g., an audio spectrogram corresponding to a video frame. In some embodiments, the fusion network may include a plurality of layers trained to receive as inputs the outputs of the first and second convolutional neural networks and to provide as outputs the likelihood that a particular feature is present in the input video.
In various embodiments, the first model may include only the first convolutional neural network, only the second convolutional neural network, both the first and second convolutional neural networks, or both the first and second convolutional neural networks and the fusion network. In some embodiments, the first model may be implemented using other types of neural networks or other types of machine learning models.
In some implementations, the gating model may include a second model that receives as input the likelihood that a particular feature is present (e.g., the output of the first model) and generates an indication of whether to analyze the video. In some implementations, the second model can be implemented using one or more of heuristic, recurrent neural networks, or markov chain analysis techniques.
In some implementations, one or more additional inputs can be provided to the gating model. For example, such additional input may include an embedding representing one or more particular image features, e.g., faces of famous characters such as actors, tv presenters, politicians, celebrities, athletes, etc., a character, a commercial video, an animation or composite video, etc. In another example, such additional input may include voice embedding representing one or more particular audio characteristics, e.g., voice signatures with famous characters such as actors, tv hosts, politicians, celebrities, athletes, commercial music, non-human audio, human utterances, and so forth. Such additional input indicates features that are not included in the annotation even when depicted in the video. For example, if the video is from a user's personal image library, the user may not only be interested in videos depicting that the user does not personally know the person (e.g., sharing videos with other users, searching videos by person, etc.), but if the user does not personally know the person, an annotation indicating the presence of a human face is not useful. Thus, videos depicting athletic activities performed by known people (e.g., family members) are important to the tags, whereas videos that tag famous athletes may not be useful. Providing additional input representative of image features not included in the annotation may enable the gating model to detect the presence of these features and generate an indication that the video will not be further analyzed. The additional inputs may enable the gating model to detect a cause, commercial video, animated or synthetic video, or video depicting a famous personality, and generate an indication that further analysis of the video is not required.
In some implementations, data derived from image frames in a segment may be provided as additional input to the gating model. For example, such data may include energy in the image frame, color distribution in the image frame, and the like. In some implementations, data derived from the audio corresponding to the segment can be provided as additional input to the gating model. For example, such data may include whether a human utterance was detected in the audio.
In some implementations, metadata associated with the video can be provided as additional input to the gating model, if allowed by the user. Metadata may include user-allowed factors such as the position of the video and/or the time of capture; whether to share videos via social networks, image sharing applications, messaging applications, and the like; depth information associated with one or more video frames; sensor values of one or more sensors (e.g., accelerometers, gyroscopes, light sensors, or other sensors) of a camera capturing the video; the identity of the user (if user consent has been obtained), and so on. For example, if a video is captured at night in an outdoor location by a camera pointed upward, such metadata may indicate that the camera is pointed skyward when capturing the video, and thus, the video is unlikely to include features such as a human face. In another example, if the particular feature is a face and the gating model detects a face in the video that is 100x100 pixels in size at a depth of 40m, such metadata may be an indication that the face is unlikely to be a real face, but a billboard or screen displaying the face.
In some implementations, additional inputs can be provided to the second model. In these embodiments, the additional inputs may include one or more of the following: an identification of a portion of a particular frame of the subset of the plurality of frames in which the presence of the particular feature was detected, a duration of time the particular feature occurred in the subset of the plurality of frames, or a heuristic related to premature termination. For example, the second model may utilize portions of particular frames of the subset to determine whether particular features are detected at or near the same location in different frames (e.g., segments or sequences of frames in a stack) of the video. For example, if portions of a particular frame are different such that a particular feature appears at different locations in the sequence of frames, such additional input may indicate false detection of the particular feature.
In another example, the second model may utilize the duration of time that a particular feature appears in a subset of the plurality of frames to determine whether the particular feature is transient, and thus determine that the detection may be false. For example, if the duration is short, e.g., a single frame, two frames, or a small number of frames, then the particular feature may be considered transient and, therefore, the detection may be considered false.
In another example, the second model may utilize heuristics related to early termination of analysis of the video by the gating model. Such early termination may result in immediate output of an indication of whether to further analyze the video. Such heuristics may be obtained, for example, based on a large number of previous videos. For example, the heuristic may indicate that when the output of the first model indicates a high likelihood of the presence of a particular feature (e.g., above a threshold, e.g., 80%, 90%), further segments of the video may be excluded from the analysis of the gating model, and the indication may be output as a positive indication that the video is to be further analyzed to add one or more video annotations. In another example, the heuristic may indicate that when the output of a plurality of consecutive segments of the first model indicates that the likelihood of the presence of the particular feature is high (e.g., the likelihood values corresponding to two or more consecutive segments satisfy a threshold, e.g., 50%, 60%, etc.), further segments of the video may be excluded and the indication may be output as a positive indication to further analyze the video to add one or more video annotations.
In block 222, the indication output by the gating model is evaluated to determine whether to analyze the video to add one or more video annotations. If the indication is to analyze the video, block 222 is followed by block 224. In this case, the gating model is not applied to one or more of the remaining fragments (if any) in the sequence. In other words, applying the gating model to a subset of the plurality of frames terminates, thereby excluding one or more fragments in the sequence from analysis by the gating model. Early termination in this manner may reduce the computational cost of the method 200. If the indication is not to analyze the video, the method proceeds to block 230.
In block 224, a video including a plurality of frames and corresponding audio (if available) is programmatically analyzed to add one or more video annotations to the video. For example, a video annotation may include one or more tags. For example, in some implementations, the tags may indicate the presence of a face in the video, the presence of a particular type of object in the video, the presence of a particular type of motion or activity in the video, or the presence of a particular type of audio. Programmatically analyzing the video may include utilizing one or more high-cost video analysis techniques. For example, such techniques may include applying one or more machine learning models trained to detect faces, types of objects, types of movement, types of audio, and so forth with high accuracy. Such techniques have higher computational costs than gated models. Video analysis techniques may also include heuristic based techniques, object recognition techniques, and the like. In some implementations, one or more tags can be stored as part of a video, e.g., as video metadata. In some implementations, one or more tags can be stored in association with the video, for example, in a database of tags that store videos.
Analyzing a video to add one or more annotations may be more computationally expensive than applying a gating model. In some implementations, the total computational cost of the method 200 (including sampling, extraction of audio spectrograms, resolution reduction, application of gating models, and acquisition indications) may be lower than the computational cost of analyzing video using high-cost video analysis techniques. In some embodiments, the computational cost of the method 200 may be much lower than the computational cost of further analyzing the video, e.g., 5 times less, 10 times less, 20 times less, 100 times less, etc. Block 224 may be followed by block 210, where the next video may be obtained in block 210.
In block 230, it may be determined whether one or more further fragments are available in the sequence. If more fragments are available, block 230 may be followed by block 220, wherein the gating model is applied to the next fragment. Block 230 may be followed by block 210 if all segments have been processed. The sequence of blocks 220, 222 and 230 may be repeated one or more times, for example, until the indication from block 222 is that the video is to be further analyzed to add annotations, or until the gating model has been applied to all segments.
While the method 200 has been described with reference to various blocks in fig. 2, it is to be understood that the techniques described in this disclosure may be performed without performing some of the blocks of fig. 2. In various embodiments, the blocks of method 200 may be performed in parallel or in a different order than illustrated in fig. 2. In various embodiments, some blocks of method 200 may be performed multiple times.
For example, in some implementations, when the video has a low frame rate and/or a number of frames below a threshold number, block 212 is not performed and the subset of frames includes all frames of the video. In another example, in some implementations, where the audio spectrogram has been previously extracted, block 214 is not performed, and the previously extracted audio spectrogram can be utilized. In yet another example, in some implementations, where a low resolution version of the video is available (e.g., pre-computed and stored), block 216 is not performed, but the available low resolution version of the video may be utilized. In yet another example, in some implementations, block 218 is not performed, e.g., if the video duration is short, and the gating model may be applied to the entire video. In other examples, block 214 may be performed in parallel with block 216 and/or block 218.
In some embodiments, the gating model may include a plurality of gating models. In these embodiments, each gating model may have a corresponding target frame rate. For example, the first gating model may have a first target frame rate of 5fps, and the second gating model may have a second target frame rate of 20 fps. In these embodiments, block 212 may be performed multiple times to obtain multiple different subsets of the multiple frames based on the target frame rate. In one or more of these embodiments, block 214 and 218 may be performed multiple times corresponding to a particular target frame rate, e.g., once for each subset of the multiple frames. In one implementation, blocks 214 and 216 may be performed for all frames of the video, and block 212 may be performed to select a subset of frames from the reduced resolution frames obtained in block 216, followed by block 218.
In some embodiments, multiple iterations of method 200 may be performed, e.g., a first iteration performed using a first gating model having a first target frame rate and a second iteration performed using a second gating model having a second target frame rate. The first and second gating models may be different gating models, each gating model being trained for a corresponding target frame rate. The computational cost of the method 200 may be lower for lower target frame rates than for higher target frame rates.
For example, it may be advantageous to perform an iteration of method 200 first at a low target frame rate (e.g., 5fps), and if no analysis of the video is indicated, perform another iteration of method 200 at a higher target frame rate (e.g., 20 fps). In this example, the computational cost of performing gating of the plurality of videos of method 200 may be lower, e.g., lower than performing method 200 at a higher target frame rate for the plurality of videos.
In some implementations, if the video is indicated to be analyzed, multiple iterations may be performed in parallel and the in-progress iteration terminated. In this example, the time required to perform gating of multiple videos may be shorter than the time to sequentially perform multiple iterations.
In some implementations, the method 200 can be performed on a client device (e.g., any of the client devices 120, 122, 124, or 126). For example, the method 200 may be performed on a client device having computing power to perform the method 200 (e.g., a processor having sufficient computing power, or including a GPU, ASIC, or neural network processor that may be used to implement the method). These implementations may provide technical advantages by reducing the load on the server device to perform the method 200.
In some implementations, the method 200 can be performed on a server device (e.g., the server device 104). For example, the method 200 may be performed on a server device if the client device that captured the video does not have the computing power to perform the method 200, or if the client device is low on power. These embodiments may provide technical advantages by performing gating techniques with a server device, thereby reducing power consumption on a client device.
Various embodiments of the gating technique described with reference to fig. 2 may reduce the overall computational cost of adding annotations to a video by being able to analyze only a subset of videos associated with a positive indication and not other videos. For example, if the annotation corresponds to a face tag, the gating technique may indicate that the analysis is performed only for the subset of videos associated with a positive indication obtained based on the likelihood of the presence of a face in the subset of videos.
Fig. 3 illustrates the operation of an example gating model 300 according to some embodiments. The gating model 300 includes a first model 320 (model a) and a second model 330 (model B). In some implementations, the gating model 300 may be associated with a particular target frame rate (e.g., 5fps, 10fps, 20fps, etc.).
The first model 320 includes a first Convolutional Neural Network (CNN)322, a second CNN324, and a fusion network 326. The first CNN 322, the second CNN324, and the converged network 326 may each include multiple layers of neural network nodes.
One or more stacks of video frames 302 are provided as input to the first CNN 322. For example, the stack of video frames may be based on a sampled subset of the plurality of video frames, which includes reduced resolution (downsampled) video frames. The subset may be obtained by sampling the video based on a target frame rate of the gating model.
In some implementations, one or more inlays 304 representing particular features may be provided as input to the first CNN 322. For example, the one or more embeddings may be vector representations of low-dimensional learning representing one or more features or feature types. One or more embeddings may be learned using a neural network trained to perform a particular task (e.g., classifying a video as depicting a particular feature or not depicting a particular feature). The one or more embeddings may be parameters (e.g., weights of the neural network). Embedding can be learned by minimizing the loss function for a particular task. For example, one or more of the inlays 304 may represent the face of a famous person such as an actor, a television presenter, a politician, a celebrity, a sports star, or the like; causes (e.g., videos that are widely walked through messaging or social networking applications or viewed via a video hosting website); commercial video (e.g., movies, television, podcasts, or other video content); or animated or composite video (e.g., screenshot video, video taken from a video game, etc.).
The first CNN may receive as input a stack of video frames 302 and embeddings 304 via an input layer of the plurality of layers. The input layer may be connected to a second layer of the plurality of layers. In some implementations, one or more additional layers, each receiving as input the output of a previous layer and providing input to a next layer, may be included in the first CNN 322. The last layer of the first CNN 322 may be an output layer.
The first CNN 322 may generate as output a first probability that a particular feature (e.g., a human face, a known face, a type of movement, etc.) is present in the video. The output of the first CNN may be a probability value, a set of probability values (e.g., each corresponding to a particular stack of video frames), or a vector representation generated by the output layer of the first CNN 322. The output of the first CNN 322 is provided as an input to a convergence network 326.
One or more stacks of audio spectrograms 312 are provided as inputs to the second CNN 324. For example, a stack of audio spectrograms may be extracted from audio based on a sampled subset of a plurality of video frames, e.g., based on frames corresponding to a previous duration and/or a subsequent duration.
In some implementations, one or more embeddings 314 representing particular features can be provided as input to the second CNN 324. For example, one or more of the embeddings can be a low-dimensional learning vector representation representing one or more types of features. One or more embeddings can be learned using a neural network trained to perform a particular task (e.g., classifying audio as depicting a particular feature or not depicting a particular feature). The one or more embeddings may be parameters (e.g., weights of the neural network). Embedding can be learned by minimizing the loss function for a particular task. For example, one or more of the inlays 304 may represent known sound signatures, audio causes (e.g., audio that is widely walked via messaging or social networking applications or listened to via an audio hosting website) corresponding to famous characters such as actors, tv hosts, politicians, celebrities, athletes, and the like; commercial audio (e.g., music, podcasts, or other audio content); or non-human audio (e.g., natural sounds, synthetically generated sounds, etc.).
The second CNN324 may receive the radio frequency spectrum map 312 and the embedded 314 stack as inputs via the input layers of the plurality of layers. The input layer may be connected to a second layer of the plurality of layers. In some implementations, one or more additional layers, each receiving the output of a previous layer as input and providing input to a next layer, may be included in the second CNN 324. The last layer of the second CNN324 may be an output layer.
The second CNN324 may generate as output a first probability of the presence of a particular feature in the audio (e.g., a human utterance, a particular type of audio, etc.). The output of the second CNN may be a probability value, a set of probability values (e.g., each corresponding to a particular stack of audio spectrograms), or a vector representation generated by the output layer of the second CNN 324. The output of the second CNN324 is provided as an input to a convergence network 326.
The converged network 326 may include multiple layers of neural network nodes. The converged network 326 may receive as input the outputs of the first CNN 322 and the second CNN324 via an input layer of the plurality of layers. The input layer may be connected to a second layer of the plurality of layers. In some embodiments, one or more additional layers may be included in the converged network 326, each receiving as input the output of a previous layer and providing the input to a next layer. The last layer of the converged network 326 may be the output layer. The fusion network 326 is trained to generate a likelihood that a particular feature is present in the video (328) based on the outputs of the first CNN 322 and the second CNN 324. In some embodiments, the converged network 326 may include only two layers — an input layer and an output layer (e.g., the second layer is the output layer). In some embodiments, converged network 326 may include three or more layers. The likelihood that a particular feature is present in the video is provided as an input to the second model 330.
The gating model 300 further includes a second model 330 (model B). In some implementations, the second model 330 can include one or more of a heuristic-based model, a recurrent neural network, or a markov chain analysis model. In various embodiments, the second model 330 is implemented using one or more of these techniques. In embodiments where two or more types of models are included in the second model 330, the output of the second model may be based on a weighted combination of the respective outputs of the two or more types of models. In some embodiments, the second model 330 may be implemented using other suitable techniques.
The second model 330 generates an indication of whether to analyze the video to add annotations based on the likelihood that particular features are present in the video. In some implementations, where the first model 320 provides respective likelihoods for multiple stacks of video frames and/or audio spectrograms as a sequence, the second model is configured to store the generated indications for each stack in an ordered sequence. In these embodiments, the generation of the indication is further based on the stored indication for one or more previous stacks in the ordered sequence.
In some implementations, the second model 330 can determine whether the likelihood of the presence of a particular feature (e.g., as determined by the first model) satisfies a threshold probability. In these embodiments, if the likelihood does not meet the threshold, the second model may output a negative indication, e.g., an indication that the video is not being analyzed to add annotations. If the likelihood satisfies a threshold, the second model may output a positive indication, e.g., an indication to analyze the video to add annotations. For example, the threshold may be set to heuristically determined probability values obtained, for example, during training of the second model. The threshold probability may be different for different specific features. The threshold probability may be determined based on an accuracy of the second model, which is determined based on the training data. For example, the accuracy may be determined as a ratio of true positives (video for which the second model provides an accurate positive indication) and false positives (video for which the second model provides an inaccurate positive indication). The proportion of true and false positives can be selected based on a Receiver Operating Characteristic (ROC) curve that is used to evaluate a tradeoff between specificity and sensitivity of the gating model. The threshold value is chosen to achieve a compromise between the execution speed of the gated model and the accuracy of the gated model.
In some embodiments, the second model may also utilize other heuristics. For example, the second model may determine whether a particular feature is detected in at least a threshold number of frames within a particular time window. For example, the second model may determine whether a particular feature is detected in at least a threshold number of frames in the stack of frames (e.g., 2 or more frames in a stack of 3 frames, 3 or more frames in a stack of 5 frames, etc.). For example, decoupling the gated model into the first model and the second model allows the second model to be adapted without expending computational expense to retrain the first model.
In some implementations, the second model can determine whether a particular feature is detected in at least a threshold number of consecutive frames in the sequence of frames. For example, the second model may determine whether a particular feature is detected in at least two consecutive frames in a stack of five frames, at least three consecutive frames in a stack of seven frames, and so on.
In some embodiments, the second model may be based on, for example, a threshold probability, a threshold number of frames, and a particular time window, as well as a threshold number of consecutive frames for which the indication was generated; or a heuristic combination (e.g., a weighted combination) based on two of these factors.
In some implementations, the gating model 300 may be implemented on one or more of the client devices 120, 122, 124, or 126, for example, as part of the image management application 156 a. In some implementations, the gating model 300 may be implemented on the server device 104, for example, as part of the image management application 156 b. In some implementations, the gating model 300 may be implemented on the server device 104 and on one or more of the client devices 120, 122, 124, or 126.
In some embodiments, the gating model 300 may be implemented as software executable on a general purpose processor (e.g., a Central Processing Unit (CPU) of a device). In some embodiments, the gating model 300 may be implemented as software executable on a special-purpose processor, such as a Graphics Processing Unit (GPU), a Field Programmable Gate Array (FPGA), a machine learning processor, or the like. In some embodiments, the gating model 300 may be implemented as dedicated hardware, e.g., as an Application Specific Integrated Circuit (ASIC).
Fig. 4 is a flow diagram illustrating an exemplary method 400 for training a machine learning based gating model to generate an indication of whether to analyze a video to add annotations corresponding to particular features, in accordance with some embodiments. In different embodiments, the specific feature includes a human face, a type of object, a type of movement, or a type of audio. For example, the method 400 may be used to train the gating model 300, described with reference to fig. 3.
The method 400 may begin at block 402. In block 402, a training set is obtained. The training set may include a plurality of training videos. Each training video may include a plurality of frames. Each training video may be a low resolution sampled version of a corresponding high resolution video. For example, each frame of high-resolution video may be 360 pixels wide (corresponding to standard definition), 720 or 1080 pixels wide (corresponding to high definition or HD), 2K/4K/8K pixels wide (corresponding to resolutions of 2K, 4K, and 8K, respectively), or any other resolution. The training video corresponding to the high resolution video may be a down-sampled (reduced resolution) version of the high resolution video such that the total number of pixels of a frame of the training video may be lower than the total number of pixels of the corresponding high resolution video. The training videos in the training set include at least one training video in which the specific feature is present and at least one training video in which the specific feature is not present. Block 402 may be followed by block 404.
The training data may further include a plurality of training labels. Each training label may indicate the presence of one or more particular features (for which a gating model is to be trained) in a high-resolution video corresponding to one or more of a plurality of training videos. For example, the training labels may be generated based on programmatically analyzing the high-resolution video using video analysis techniques that generate the training labels. In another example, the training labels may be generated based on manual user input.
In some implementations, one or more of the plurality of training videos may further include audio spectrograms corresponding to the plurality of frames. In these embodiments, the gating model may include a convolutional neural network trained to analyze the audio spectrogram. In these embodiments, the gating model may further include a fusion network that receives as inputs the outputs of the first and second convolutional neural networks and generates a likelihood that a particular feature is present in the video.
In block 404, a first model of a gating model is applied to each training video in the training set to generate a likelihood that a particular feature is present in the training video. Block 404 may be followed by block 406.
In block 406, a second model of the gating model is applied based on the likelihood that the particular feature is present in the training video to generate an indication of whether to analyze the training video to add annotations corresponding to the particular feature. In some implementations, the gating model may generate an indication with an associated confidence level (e.g., 5%, 10%, 50%, etc.). Block 406 may be followed by block 408.
In block 408, feedback data is generated based on the indication generated in block 406 and the training labels associated with the high resolution video corresponding to the training video. For example, if the indication is negative (no video being analyzed), and the training labels indicate that a particular feature is present, the feedback data may indicate an output error of the gating model (negative feedback). In another example, if the indication is positive (to analyze the video), and the training labels indicate that no particular feature is present, the feedback data may indicate that the output of the gating model is erroneous (negative feedback). In another example, if the indication is positive (to analyze the video) and the training labels indicate that a particular feature is present, the feedback data may indicate that the output of the gating model is correct (positive feedback). In another example, if the indication is negative (no video being analyzed) and the training labels indicate that no particular function is present, the feedback data may indicate that the output of the gating model is correct (positive feedback).
Although the feedback data has been described as positive feedback or negative feedback, the feedback data may be provided in other forms. For example, feedback data generated from multiple training videos may be aggregated before providing the feedback to the gating model. For example, aggregation may include providing an indication that the indication (and associated confidence) generated by the model for a particular feature is of higher precision, but of lower precision for different features. For example, the feedback data may indicate that the gating model has a higher accuracy when generating the indication for the features "face" and "smile", and a lower accuracy when generating the indication for the features "birthday cake", "laugh", or "jump".
Block 408 may be followed by block 410. In block 410, feedback data is provided as a training input to the gating model. In some implementations, block 410 may be followed by block 412.
In block 412, the gating model is automatically updated based on the feedback data. In some embodiments, updating the gating model includes automatically adjusting weights of one or more nodes of the convolutional neural network of the first model. In some embodiments, updating the gating model includes automatically adjusting connectivity between one or more pairs of nodes of the convolutional neural network of the first model.
In some implementations, the multiple frames of each training video may be divided into a stack of multiple frames (or segments). Each stack may include one or more frames. In some embodiments, each stack may include at least two frames. The multiple stacks may be organized in an ordered sequence. Training the gating model may be performed sequentially for each of a plurality of frame stacks.
In some embodiments, the gating model may include a first model that includes one or more Convolutional Neural Networks (CNNs). Before training the gating model, the CNN may include a plurality of nodes organized into multiple layers. The nodes in each layer may be connected to nodes in a previous layer and nodes in a subsequent layer. The nodes in the first layer may be configured to accept a video frame or an audio spectrogram as input. Each node may be any type of neural network node, for example, an LSTM node.
Prior to training, each node may be assigned an initial weight, and connections between nodes of different layers of the neural network may be initialized. Training may include adjusting the weight of one or more nodes and/or the connection between one or a pair of nodes.
In some embodiments, a subset of the training set may be excluded in the initial training phase. This subset may be provided after an initial training phase, and the accuracy of the prediction (indicating whether to analyze the video) may be determined. If the accuracy is below the threshold, further training may be performed using other videos from the training set to adjust model parameters until the model correctly predicts the motion scores of the image subsets. Further training (second phase) may be repeated any number of times, for example, until the model reaches a satisfactory level of accuracy. In some embodiments, the trained model may be further modified, e.g., compressed (to use fewer nodes or layers), transformed (e.g., to be available on different types of hardware), and so forth. In some embodiments, for example, different versions of the model may be provided, and the size of the client version of the model may be optimized and computational complexity reduced, while the accuracy of the server version of the model may be optimized.
Although the method 400 has been described with reference to various blocks in fig. 4, it may be understood that the techniques described in this disclosure may be performed without performing some of the blocks of fig. 4. Block 412 may be performed separately, e.g., the updating of the gating model may be performed in an offline manner. In some implementations, one or more of the blocks illustrated in fig. 4 may be combined, e.g., blocks 410 and 412 may be combined, e.g., for online training.
Furthermore, although training has been described with reference to a training set, the gating model may be trained during operation. For example, if a user requests to analyze a particular video (e.g., by initiating authoring of a video-based creation, such as a video collage, a story with clips obtained from multiple videos, etc.), video analysis of the particular video may be triggered. If the video analysis indicates that a particular feature is present in the video, such an indication may be provided as feedback data to train the gating model. In some implementations, the user can manually provide annotations, e.g., mark a portion of the video as having a particular feature. Some embodiments may utilize such annotations to train a gating model, with user permission.
Fig. 5 illustrates respective outputs of an example video and gating model (e.g., a trained gating model used in an inference phase). Specifically, three frame stacks (502, 512, and 522) of an example video are shown. The three frame stacks are part of a subset of the multiple frames of the video and correspond to different points in time t-1, t and t +1, respectively. As seen in fig. 5, different frames of the video depict a person on the swing (506, 516, 526). During capture of the video, the person rides the swing from the rear to the front and spreads their legs as seen in the stack of video frames 502, 512, and 522. Such motion may be indicated by depth data stored in the depth image, for example, when capturing video using a camera capable of determining depth information. The video also includes a background portion (504).
While in motion, the person speaks the phrase "This is fun! ", which is stored as the audio portion of the video. The first portion (508) of the phrase includes the word "This" and corresponds to the first frame stack (502). The second portion (518) of the phrase includes the word "is (yes)" and corresponds to a second frame stack (512). The third portion (528) of the phrase includes the word "fun" and corresponds to the second frame stack (522). Audio frequency spectrograms corresponding to different stacks of frames are obtained. The video may include other frames, each having a corresponding audio spectrogram, before time t-1 and subsequent to time t + 1.
As illustrated in fig. 5, the frame stack and the corresponding audio spectrogram are provided to a first model 540 (model a). For each frame stack, the first model 540 generates an output prediction of the likelihood that a particular feature is present in the stack. In the illustrated example, the particular feature is a human face. As seen in fig. 5, the first model 540 generates three probability values (0.5, 0.7) corresponding to the stacks 502, 512, and 522, respectively.
The likelihood values generated by the first model 540 are provided as input to a second model 542 (model B). The second model generates, for each frame stack, an indication of whether to analyze the video to add one or more annotations (e.g., labels for "faces") corresponding to particular features. For example, a "no" indication is based on stacks 502 and 512, and a "yes" indication is based on stack 522. For example, the generation of the indication may be based on heuristics. Heuristics may be obtained by training the second model 542. In some examples, the heuristics may be based on a single frame stack and/or multiple frame stacks, e.g., adjacent or contiguous stacks or non-contiguous stacks.
In the example illustrated in fig. 5, three consecutive frame stacks can be seen with a likelihood >0.4 of the presence of a particular feature. In this example, a simple heuristic for the second model 542 may be "generate an indication yes if three or more consecutive stacks are associated with a likelihood >0.4, otherwise generate an indication no". Other heuristics may also be utilized, such as "generate yes if any frame stack is associated with a likelihood > 0.9", "generate no if at least one of the three consecutive stacks has a likelihood < 0.5", etc. In some implementations, the second model 542 can evaluate multiple heuristics as a combination (e.g., a weighted combination), and generate an indication accordingly. In some implementations, the second model 542 can generate different indications corresponding to different particular features. For example, in fig. 5, the indication of "jump" may be "no" and the indication of "rock pendulum" may be "yes".
Fig. 6 is a block diagram of an example device 600 that may be used to implement one or more features described herein. In one example, device 600 may be used to implement a client device, such as any of the client devices (120, 122, 124, 126) shown in fig. 1. Alternatively, device 600 may implement a server device, such as server 104. In some implementations, the device 600 can be a device for implementing a client device, a server device, or both a client and server device. Device 600 may be any suitable computer system, server, or other electronic or hardware device as described above.
One or more of the methods described herein may be run in a stand-alone program executable on any type of computing device, a program running on a web browser, a mobile application ("app") running on a mobile computing device (e.g., a cell phone, a smartphone, a tablet computer, a wearable device (a wristwatch, an armband, jewelry, headwear, virtual reality goggles or glasses, augmented reality goggles or glasses, a head mounted display, etc.), a laptop computer, etc.). In one example, a client/server architecture may be used, for example, a mobile computing device (as a client device) to send user input data to a server device and receive final output data from the server for output (e.g., for display). In another example, all computations may be performed within mobile apps (and/or other apps) on the mobile computing device. In another example, the computing may be split between the mobile computing device and one or more server devices.
In some implementations, the device 600 includes a processor 602, a memory 604 and an input/output (I/O) interface 606, as well as a camera 616. The processor 602 may be one or more processors and/or processing circuits that execute program code and control basic operation of the device 600. A "processor" includes any suitable hardware system, mechanism, or component that processes data, signals, or other information. A processor may include a system having a general purpose Central Processing Unit (CPU) with one or more cores (e.g., in a single core, dual core, or multi-core configuration), multiple processing units (e.g., in a multi-processor configuration), a Graphics Processing Unit (GPU), a Field Programmable Gate Array (FPGA), an Application Specific Integrated Circuit (ASIC), a Complex Programmable Logic Device (CPLD), a special purpose circuit for implementing functionality, a special purpose processor for implementing neural network model-based processing, a neural circuit, a processor optimized for matrix computation (e.g., matrix multiplication), or other system. In some implementations, the processor 602 may include one or more coprocessors that implement neural network processing. In some embodiments, processor 602 may be a processor that processes data to generate probabilistic outputs, e.g., the outputs generated by processor 602 may be inaccurate or may be accurate within a range from expected outputs. Processing need not be limited to a particular geographic location or have temporal limitations. For example, a processor may perform its functions "in real-time," "offline," in a "batch mode," and so forth. Portions of the processing may be performed at different times and at different locations by different (or the same) processing systems. The computer may be any processor in communication with a memory.
Memory 604 is typically provided in device 600 for access by processor 602, and may be any suitable processor-readable storage medium suitable for storing instructions for execution by the processor and located separate from and/or integrated with processor 602, such as Random Access Memory (RAM), Read Only Memory (ROM), electrically erasable read-only memory (EEPROM), flash memory, and the like. The memory 604 may store software operated by the processor 602 on the server device 600, including an operating system 608, a machine learning application 630, other applications 612, and application data 614. Other applications 612 may include, for example, a camera application, an image library application, a data display engine, a web hosting engine, an image display engine, a notification engine, a social networking engine, and so forth. In some implementations, the machine learning application 630 and the other applications 612 may each include instructions that enable the processor 602 to perform the functions described herein (e.g., some or all of the methods of fig. 2 and 4).
Other applications 612 may include, for example, camera applications, image library applications, image management applications, gallery applications, media display applications, communication applications, web hosting engines or applications, mapping applications, media sharing applications, and so forth. One or more methods disclosed herein may operate in several environments and platforms, e.g., as a stand-alone computer program that may run on any type of computing device, as a web application with web pages, as a mobile application ("app") running on a mobile computing device, and so forth.
In various implementations, the machine learning application 630 may utilize a bayesian classifier, a support vector machine, a neural network, or other learning techniques. In some implementations, the machine learning application 630 can include a training model 634, an inference engine 636, and data 632. In some embodiments, the training model 634 may be a gated model and may include one or more models.
In some implementations, the data 632 may include training data, e.g., data used to generate the training model 634. For example, the training data may include any type of data, such as text, images, audio, video, and so forth. For example, the training data may include a training set that includes a plurality of training videos and corresponding labels. The training data may be obtained from any source, such as a data repository specifically labeled for training data or the like that has provided permissions for use as training data for machine learning. In embodiments where one or more users license the use of their respective user data to train a machine learning model (e.g., training model 634), the training data may include such user data. In embodiments where users license their respective user data, data 632 may include licensed data, such as image/video or image/video metadata (e.g., video, data sharing video with other users, tags associated with video, whether video-based authoring (such as video collage, story, etc.) was generated from video, etc.), communications (e.g., email; chat data such as text messages, voice, video, etc.), documents (e.g., spreadsheets, text documents, presentations, etc.).
In some implementations, the training data may include synthetic data generated for training purposes, such as data that is not based on user input or activity in the context being trained, e.g., data generated from analog or computer-generated video or the like. In some implementations, the machine learning application 630 excludes the data 632. For example, in these embodiments, the training models 634 may be generated, e.g., on different devices, and the training models 634 provided as part of the machine learning application 630. In various embodiments, the training model 634 may be provided as a data file that includes a model structure or form (e.g., which defines the number and type of neural network nodes, connectivity between nodes, and node organization into multiple layers) and associated weights. Inference engine 636 may read data files for training model 634 and implement a neural network with node connections, layers, and weights based on the model structure or form specified in training model 634.
The machine learning application 630 also includes a training model 634. In some embodiments, the training model may include one or more model forms or structures. For example, the model form or structure may include any type of neural network, such as a linear network, a deep neural network implementing multiple layers (e.g., a "hidden layer" between an input layer and an output layer, where each layer is a linear network), a convolutional neural network (e.g., splitting or partitioning input data into multiple parts or tiles, processing each tile separately using one or more neural network layers, and aggregating the results from the processing of each tile), a sequence-to-sequence neural network (e.g., a network that receives sequential data (such as words in sentences, frames in videos, etc.) as input and produces a sequence of results as output), and so forth.
The model form or structure may specify the connections between the various nodes and the organization of the nodes into layers. For example, a node of a first tier (e.g., input tier) may receive data as input data 632 or application data 614. For example, when the training model is used for analysis of, for example, a video comprising a plurality of frames, such data may comprise, for example, one or more pixels per node. Subsequent middle layers may receive as input the output of the node of each connected previous layer specified in the model form or structure. These layers may also be referred to as hidden layers. The final layer (e.g., output layer) produces the output of the machine learning application. For example, the output may be an indication of whether to programmatically analyze the video to add one or more annotations (e.g., a set of tags) to the video. In some implementations, the model form or structure also specifies the number and/or type of nodes in each layer.
In various embodiments, the training models 634 may include one or more models. One or more of the models may include a plurality of nodes arranged in layers in a model structure or form. In some embodiments, a node may be a computing node without memory, e.g., a unit configured to process one unit of input to produce an output. The calculations performed by the nodes may include, for example, multiplying each of a plurality of node inputs by a weight, obtaining a weighted sum, and adjusting the weighted sum with an offset or intercept value to produce a node output. In some embodiments, the calculation performed by the node may further include applying a step/activation function to the adjusted weighted sum. In some embodiments, the step/activation function may be a non-linear function. In various embodiments, such calculations may include operations such as matrix multiplication. In some embodiments, computations performed by multiple nodes may be performed in parallel, e.g., using multiple processor cores of a multi-core processor, using individual processing units of a GPU, or dedicated neural circuitry. In some implementations, a node may include memory, e.g., may be able to store and use one or more earlier inputs when processing subsequent inputs. For example, the nodes with memory may include Long Short Term Memory (LSTM) nodes. LSTM nodes may use memory to maintain "states" that permit the nodes to act like Finite State Machines (FSMs). Models with such nodes may be useful when processing sequential data (e.g., words in sentences or paragraphs, frames in video, speech or other audio, etc.). For example, a heuristic-based model used in the gating model may store one or more previously generated indications that correspond to a previous frame stack from a sequence of stacks of video frames.
In some embodiments, training model 634 may include an embedding or weighting for each node. For example, the model may be launched as a plurality of nodes organized into layers as specified by the model form or structure. At initialization, a respective weight may be applied to a connection between each pair of nodes (e.g., nodes in successive layers of a neural network) connected in a model form. For example, the respective weights may be randomly assigned or initialized to default values. The model may then be trained, for example, using data 632 to produce results.
For example, training may include applying supervised learning techniques. In supervised learning, training data may include a plurality of inputs (e.g., a set of videos) and a corresponding expected output for each input (e.g., one or more labels for each video). The values of the weights are automatically adjusted based on a comparison of the output of the model with the expected output, e.g., in a manner that increases the probability that the model will produce the expected output when provided with a similar input.
In some embodiments, training may include applying unsupervised learning techniques. In unsupervised learning, only input data may be provided and a model may be trained to distinguish the data, e.g., to cluster the input data into a plurality of groups, where each group includes input data that is similar in some way. For example, the model may be trained to determine whether to analyze the video to add one or more annotations.
In various embodiments, the training model includes a set of weights or embeddings corresponding to the model structure. In embodiments where the data 632 is omitted, the machine learning application 630 may include a training model 634 based on prior training (e.g., by a developer of the machine learning application 630, by a third party, etc.). In some embodiments, training model 634 may include a fixed set of weights (e.g., downloaded from a server that provides the weights).
The machine learning application 630 also includes an inference engine 636. The inference engine 636 is configured to apply the training model 634 to data, such as application data 614 (e.g., video), to provide inferences. In some implementations, the inference engine 636 can include software code to be executed by the processor 602. In some implementations, the inference engine 636 can specify a circuit configuration (e.g., for a programmable processor, for a Field Programmable Gate Array (FPGA), etc.) that enables the processor 602 to apply the training model. In some implementations, the inference engine 636 can include software instructions, hardware instructions, or a combination. In some implementations, inference engine 636 can provide an Application Programming Interface (API) that can be used by operating system 608 and/or other applications 612 to invoke inference engine 636, e.g., to apply training model 634 to application data 614 to generate inferences.
Analyzing, for example, a video with a high source resolution and having a particular duration may be computationally expensive. If the annotation corresponds to a particular feature, the entire video may need to be analyzed to determine if one or more particular features are present in the video (or one or more segments of the video), and a corresponding annotation may be added to the video. For example, if no particular feature is present, the operation may be wasteful. Furthermore, on certain devices, for example, devices with limited processing power, devices with limited power capabilities (e.g., battery-powered devices), this operation may not be possible or may be particularly expensive. Further, if a user has a video library that includes multiple videos that do not include a particular feature, such expensive operations may be run for each of the multiple videos. Furthermore, when only a portion of a video depicts a particular feature, analyzing the entire video may waste computing resources.
In this case, the machine learning application 630 may provide several technical advantages. For example, when training model 634 is a gating model applied to a video, the model may provide an indication of whether to analyze the video to add one or more video annotations. The gating model may be applied to a low-resolution sampled subset of frames of the video, which may have a lower computational cost than analyzing the original high-resolution video. Then, only such videos for which the gating model generates a positive indication need be analyzed, saving computational cost and power. The use of gating models may also enable video annotation to be performed on devices with low computational power or limited power. Further, in some implementations, the indication from the gating model may include identifying one or more segments of the video for analysis based on a likelihood that a particular feature is present in the one or more segments. In these embodiments, other segments of the video may be excluded from the analysis to add video annotations, thereby saving computational costs.
In some embodiments, when the gating model is implemented as a two-stage model including a first model and a second model, the first model and the second model may be trained independently of each other. For example, the first model may be trained to detect the likelihood of the presence of particular features in the video. The training may be performed independently of the training of the second model, e.g., by providing feedback data obtained based on training labels associated with the training video. For example, the second model may be trained to generate an indication of whether to analyze the video to add annotations. The training may be performed independently of the training of the first model, e.g. by providing different likelihood values as inputs and training labels as feedback data. Thus, each model can be evaluated and trained separately from the other as a whole, with corresponding improvements in performance (e.g., accuracy, computational cost, etc.) of the gated model.
In some implementations, the machine learning application 630 can be implemented in an offline manner. In these embodiments, the training model 634 may be generated in a first stage and provided as part of the machine learning application 630. In some implementations, the machine learning application 630 can be implemented in an online manner. For example, in such embodiments, an application (e.g., operating system 608, one or more of other applications 612, etc.) invoking machine learning application 630 can leverage inferences produced by machine learning application 630, e.g., provide inferences to a user, and can generate a system log (e.g., actions taken by the user based on the inferences if permitted by the user, or results of further processing if used as input for further processing). The system logs may be generated periodically (e.g., hourly, monthly, quarterly, etc.) and may be used to update the training models 634, e.g., update the embedding for the training models 634, if user approval.
In some implementations, the machine learning application 630 can be implemented in a manner that can be adapted to the particular configuration of the device 600 on which the machine learning application 630 is executed. For example, the machine learning application 630 may determine a computational graph that utilizes available computational resources (e.g., the processor 602). For example, if the machine learning application 630 is implemented as a distributed application on multiple devices, the machine learning application 630 may determine the computations to perform on the various devices in a manner that optimizes the computations. In another example, machine learning application 630 may determine that processor 602 includes GPUs having a particular number of GPU cores (e.g., 1000) and implement the inference engine accordingly (e.g., as 1000 individual processes or threads).
In some implementations, the machine learning application 630 can implement a set of training models. For example, the training model 634 may include multiple training models that each apply to the same input data. In these implementations, the machine learning application 630 may choose a particular training model, for example, based on available computing resources, success rates with a priori reasoning, and so on. In some implementations, the machine learning application 630 can execute the inference engine 636 such that a plurality of training models are applied. In these embodiments, the machine learning application 630 may combine the outputs from applying the various models, for example, using a voting technique that scores the various outputs from applying each training model or by choosing one or more particular outputs. Additionally, in these embodiments, the machine learning application may apply a time threshold (e.g., 0.5ms) for applying the respective training models and utilize only those respective outputs that are available within the time threshold. Outputs that are not received within the time threshold may not be utilized (e.g., discarded). Such an approach may be appropriate, for example, when a time limit is specified while a machine learning application is invoked, e.g., by operating system 608 or one or more applications 612.
For example, the gating model may be implemented as a set of training models, where each model has a different target frame rate and associated computational cost. For example, the gating model may implement models trained for frame rates of 5fps, 10fps, and 20fps, where models with higher frame rates are associated with higher computational costs than models with lower frame rates. In another example, the gating model may implement a model trained with different numbers of frames in a segment or stack, e.g., a stack comprising 3 frames, a stack comprising 5 frames, a stack comprising 10 frames, etc. In some implementations, a model trained by a stack with a large number of frames may be associated with a greater computational cost of modeling by a stack with a smaller number of frames. If a model with low computational cost generates an indication with high confidence, other models in the population may not be applied to a particular video, if executed in parallel, or may terminate.
In some implementations, the machine learning application 630 can generate output based on a format specified by the calling application, such as the operating system 608 or one or more applications 612. In some implementations, the calling application can be another machine learning application. For example, such a configuration may be used in generating a countermeasure network, where output from the machine learning application 630 is used to train the invoking machine learning application and vice versa.
Any of the software in memory 604 may alternatively be stored on any other suitable storage location or computer readable medium. Further, memory 604 (and/or other connected storage devices) may store one or more messages, one or more taxonomies, electronic encyclopedias, dictionaries, thesauruses, knowledge bases, message data, grammars, user preferences, and/or other instructions and data used in the features described herein. Memory 604 and any other type of storage (magnetic disk, optical disk, tape, or other tangible medium) may be considered "storage" or "storage.
The I/O interface 606 may provide functionality to enable the server device 600 to interface with other systems and devices. The docking device may be included as part of the device 600 or may be separate and in communication with the device 600. For example, network communication devices, storage devices (e.g., memory and/or database 106), and input/output devices may communicate via the I/O interface 606. In some implementations, the I/O interface can connect to interface devices such as input devices (keyboards, pointing devices, touch screens, microphones, cameras, scanners, sensors, etc.) and/or output devices (display devices, speaker devices, printers, motors, etc.).
Some examples of docking devices that may be connected to the I/O interface 606 may include one or more display devices 620, which display devices 620 may be used to display content, such as images, videos, and/or user interfaces of output applications as described herein. Display device 620 may be connected to device 600 via a local connection (e.g., a display bus) and/or via a networked connection and may be any suitable display device. Display device 620 may include any suitable display device, such as an LCD, LED, or plasma display screen, CRT, television, monitor, touch screen, 3-D display screen, or other visual display device. For example, the display device 620 may be a flat panel display screen provided on a mobile device, multiple display screens provided in a goggle or headphone device, or a monitor screen for a computer device.
The I/O interface 606 may interface with other input and output devices. Some examples include one or more cameras that can capture images. Some implementations may provide a microphone for capturing sound (e.g., as part of a captured image, voice command, etc.), an audio speaker device for outputting sound, or other input and output devices.
Camera 616 may be any type of camera that may capture video including a plurality of frames. A camera as used herein may include any image capture device. In some implementations, the camera 616 may include multiple lenses with different capabilities, e.g., front-to-back, different zoom levels, image resolution of captured images, and so forth. In some implementations, the device 600 may include one or more sensors, such as a depth sensor, an accelerometer, a location sensor (e.g., GPS), a gyroscope, and so forth. In some implementations, one or more sensors may operate with a camera to obtain sensor readings corresponding to different frames of video captured using the camera.
For ease of illustration, fig. 6 shows one block for each of processor 602, memory 604, I/O interface 606, camera 616, and software blocks 608, 612, and 630. These blocks may represent one or more processors or processing circuits, operating systems, memories, I/O interfaces, applications, and/or software modules. In other embodiments, device 600 may not have all of the components shown and/or may have other elements, including other types of elements, in place of or in addition to those elements shown herein. Although some components are described as performing blocks and operations as described in some embodiments herein, any suitable components or combinations of components of the environment 100, the apparatus 600, similar systems, or any suitable one or more processors associated with such systems may perform the described blocks and operations.
The methods described herein may be implemented by computer program instructions or code executable on a computer. For example, the code may be embodied by one or more digital processors (e.g., microprocessors or other processing circuits) and may be stored on a computer program product including a non-transitory computer-readable medium (e.g., a storage medium), such as a magnetic, optical, electromagnetic, or semiconductor storage medium, including a semiconductor or solid state memory, magnetic tape, a removable computer diskette, a Random Access Memory (RAM), a read-only memory (ROM), a flash memory, a rigid magnetic disk, an optical disk, a solid state memory drive, or the like. The program instructions may also be embodied in and provided as electronic signals, for example in the form of software as a service (SaaS) delivered from a server (e.g., a distributed system and/or a cloud computing system). Alternatively, one or more of the methods may be implemented in hardware (logic gates, etc.) or in a combination of hardware and software. Example hardware may be a programmable processor (e.g., a Field Programmable Gate Array (FPGA), a complex programmable logic device), a general purpose processor, a graphics processor, an Application Specific Integrated Circuit (ASIC), etc. One or more methods may be performed as part or component of an application running on the system, or as an application or software running in conjunction with other applications and operating systems.
While the specification has been described with respect to specific embodiments thereof, these specific embodiments are merely illustrative, and not restrictive. The concepts illustrated in the examples may be applied to other examples and embodiments.
Certain embodiments discussed herein may provide a user with one or more opportunities to control whether to collect information, whether to store personal information, whether to use personal information, and how to collect, store, and use information about the user in the event that personal information about the user is collected or used (e.g., user data, information about the user's social network, the user's location and time at the location, biometric information of the user, the user's activities, and demographic information). That is, the systems and methods discussed herein collect, store, and/or use user personal information, particularly upon receiving explicit authorization to do so from an associated user.
For example, the user is provided with control over whether the program or feature collects user information about that particular user or other users related to the program or feature. Each user for whom personal information is to be collected is presented with one or more options to allow control of information collection in relation to that user to provide permission or authorization as to whether information is collected and what portions of the information are to be collected. For example, one or more such control options may be provided to the user over a communications network. In addition, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is removed. As one example, the identity of the user may be processed such that personally identifiable information cannot be determined. As another example, the user's geographic location may be generalized to a larger area such that the user's particular location cannot be determined.
It is noted that the functional blocks, operations, features, methods, devices, and systems described in this disclosure can be integrated or divided into different combinations of systems, devices, and functional blocks, as will be known to those skilled in the art. The routines of a particular embodiment may be implemented using any suitable programming language and programming techniques. Different programming techniques, such as procedural or object oriented, may be employed. The routines can execute on a single processing device or multiple processors. Although the steps, operations, or computations may be presented in a specific order, this order may be changed in different specific embodiments. In some embodiments, multiple steps or operations shown as sequential in this specification may be performed at the same time.
Claims (20)
1. A computer-implemented method, comprising:
obtaining a video comprising a plurality of frames and corresponding audio;
performing sampling to select a subset of the plurality of frames based on a target frame rate that is less than or equal to a frame rate of the video;
extracting a respective audio spectrogram of each frame of the subset of the plurality of frames;
reducing a resolution of the subset of the plurality of frames;
after reducing the resolution, applying a machine learning based gating model to the subset of the plurality of frames and corresponding audio spectrograms; and
obtaining, as an output of the gating model, an indication of whether to analyze the video to add one or more video annotations.
2. The computer-implemented method of claim 1, further comprising: prior to applying the gating model, dividing the video into a plurality of segments, each segment comprising a plurality of frames, and wherein applying the gating model is performed iteratively in sequence over the plurality of segments, wherein the indication is generated at each iteration.
3. The computer-implemented method of claim 2, wherein each segment of the plurality of segments overlaps another segment of the plurality of segments.
4. The computer-implemented method of claim 2, wherein, if the indication at a particular iteration is to analyze the video, terminating application of the gating model such that one or more of the plurality of segments are excluded.
5. The computer-implemented method of claim 1, wherein the gating model is trained to determine whether a particular feature is present in input video provided to the gating model.
6. The computer-implemented method of claim 5, wherein the particular feature comprises at least one of a human face, a type of object, a type of movement, or a type of audio.
7. The computer-implemented method of claim 1, wherein applying the gating model comprises:
applying a first model that determines a likelihood that a particular feature is present; and
applying a second model that receives as input the likelihood that the particular feature is present and generates the indication of whether to analyze the video.
8. The computer-implemented method of claim 7, wherein the first model comprises:
a first convolutional neural network comprising a plurality of layers trained to analyze video;
a second convolutional neural network comprising a plurality of layers trained to analyze audio; and
a fusion network comprising a plurality of layers, the fusion network receiving as inputs the outputs of the first and second convolutional neural networks and providing to the second model a likelihood that the particular feature is present.
9. The computer-implemented method of claim 7, wherein the second model is implemented using one or more of heuristic, recurrent neural networks, or markov chain analysis techniques.
10. The computer-implemented method of claim 7, further comprising providing additional inputs to the second model, wherein the additional inputs include one or more of:
an identification of a portion of a particular frame in the subset of the plurality of frames in which the particular feature is detected to be present;
a duration in which the particular feature occurs in a subset of the plurality of frames; or
As to the heuristic of the early termination,
and wherein the second model utilizes the additional input to generate the indication.
11. The computer-implemented method of claim 1, further comprising: when the indication is to analyze the video, programmatically analyzing the video to add the one or more video annotations, wherein the video annotations include one or more tags indicating a presence in the video of one or more of a face, a particular type of object, a particular type of movement, or a particular type of audio.
12. A computing device, comprising:
a processor; and
a memory having instructions stored thereon that, when executed by the processor, cause the processor to perform operations comprising:
obtaining a video comprising a plurality of frames and corresponding audio;
performing sampling to select a subset of the plurality of frames based on a target frame rate that is less than or equal to a frame rate of the video;
extracting a respective audio spectrogram of each frame of the subset of the plurality of frames;
reducing a resolution of the subset of the plurality of frames;
after reducing the resolution, applying a machine learning based gating model to the subset of the plurality of frames and corresponding audio spectrograms; and
obtaining, as an output of the gating model, an indication of whether to analyze the video to add one or more video annotations.
13. The computing device of claim 12, wherein the memory further has instructions stored thereon that, when executed by the processor, cause the processor to perform further operations comprising: prior to applying the gating model, dividing the video into a plurality of segments, each segment comprising a plurality of frames, and wherein applying the gating model is performed iteratively in sequence over the plurality of segments, wherein the indication is generated at each iteration.
14. A computer-implemented method for training a machine learning based gating model to generate an indication of whether to analyze a video to add annotations corresponding to particular features, wherein the machine learning based gating model comprises:
a first model comprising a first convolutional neural network that generates a likelihood that the particular feature is present in a video based on video frames of the video; and
a second model that receives as input a likelihood that the particular feature is present in the video and generates the indication, the method comprising:
obtaining a training set, the training set comprising:
a plurality of training videos, wherein each training video comprises a plurality of frames, and wherein each training video is a low-resolution sampled version of a corresponding high-resolution video; and
a plurality of training labels, each training label indicating a presence of the particular feature in the high-resolution video corresponding to one or more of the plurality of training videos;
training the gating model, wherein the training comprises, for each training video in the training set,
generating a likelihood that the particular feature is present in the training video by applying the first model to the training video;
generating, by applying the second model, an indication of whether to analyze the training video to add annotations corresponding to particular features based on the likelihood that the particular features are present in the training video;
generating feedback data based on the training labels associated with the corresponding high-resolution video and the indication; and
providing the feedback data as a training input for the first model and a training input for the second model.
15. The computer-implemented method of claim 14, wherein the particular feature comprises at least one of a human face, a type of movement, or a type of object.
16. The computer-implemented method of claim 14, wherein the plurality of training videos in the training set includes at least one video in which the particular feature is present and at least one video in which the particular feature is not present, and wherein training the gating model includes one or more of automatically adjusting weights of one or more nodes of the first convolutional neural network of the first model or automatically adjusting connectivity between one or more pairs of nodes of the first convolutional neural network of the first model.
17. The computer-implemented method of claim 14, wherein the second model comprises one or more of a heuristic-based model, a recurrent neural network, or a markov chain analysis model, and wherein training the gating model comprises automatically adjusting one or more of one or more parameters of the heuristic-based model, the recurrent neural network, or the markov chain analysis model.
18. The computer-implemented method of claim 14, wherein training the gating model further comprises dividing a plurality of frames of the training video into a plurality of frame stacks, each stack comprising at least one frame, wherein the plurality of stacks are organized in an ordered sequence, and wherein training the gating model is performed sequentially for each of the plurality of frame stacks.
19. The computer-implemented method of claim 18, wherein the second model is configured to store the generated indication for each stack of the training video, and wherein generating the indication for a particular stack is further based on respective stored indications of one or more previous stacks in the ordered sequence.
20. The computer-implemented method of claim 14, wherein one or more of the plurality of training videos further comprises audio spectrograms corresponding to the plurality of frames, and wherein the first model further comprises:
a second convolutional neural network trained to analyze an audio spectrogram; and
a fusion network that receives as inputs the outputs of the first and second convolutional neural networks and generates a likelihood that the particular feature is present in the video.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/352,605 | 2019-03-13 | ||
US16/352,605 US10984246B2 (en) | 2019-03-13 | 2019-03-13 | Gating model for video analysis |
PCT/US2019/053501 WO2020185256A1 (en) | 2019-03-13 | 2019-09-27 | Gating model for video analysis |
Publications (2)
Publication Number | Publication Date |
---|---|
CN112740709A true CN112740709A (en) | 2021-04-30 |
CN112740709B CN112740709B (en) | 2023-08-29 |
Family
ID=68296667
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980060091.9A Active CN112740709B (en) | 2019-03-13 | 2019-09-27 | Computer-implemented method, computing device, and computer-readable medium for performing gating for video analytics |
Country Status (6)
Country | Link |
---|---|
US (2) | US10984246B2 (en) |
EP (1) | EP3735777A1 (en) |
JP (1) | JP7228682B2 (en) |
KR (1) | KR102297393B1 (en) |
CN (1) | CN112740709B (en) |
WO (1) | WO2020185256A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210329306A1 (en) * | 2020-04-15 | 2021-10-21 | Nvidia Corporation | Video compression using neural networks |
Families Citing this family (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210201124A1 (en) * | 2018-08-27 | 2021-07-01 | Neuralmagic Inc. | Systems and methods for neural network convolutional layer matrix multiplication using cache memory |
US10984246B2 (en) | 2019-03-13 | 2021-04-20 | Google Llc | Gating model for video analysis |
US11363315B2 (en) * | 2019-06-25 | 2022-06-14 | At&T Intellectual Property I, L.P. | Video object tagging based on machine learning |
CN110543943B (en) * | 2019-09-10 | 2022-03-25 | 北京百度网讯科技有限公司 | Network convergence method and device, electronic equipment and storage medium |
US11455531B2 (en) * | 2019-10-15 | 2022-09-27 | Siemens Aktiengesellschaft | Trustworthy predictions using deep neural networks based on adversarial calibration |
SG10202006357UA (en) * | 2020-07-01 | 2020-09-29 | Alipay Labs Singapore Pte Ltd | A Document Identification Method and System |
US11776273B1 (en) * | 2020-11-30 | 2023-10-03 | Amazon Technologies, Inc. | Ensemble of machine learning models for automatic scene change detection |
CN114581966A (en) * | 2020-11-30 | 2022-06-03 | 伊姆西Ip控股有限责任公司 | Method, electronic device and computer program product for information processing |
CN112528109B (en) * | 2020-12-01 | 2023-10-27 | 科大讯飞(北京)有限公司 | Data classification method, device, equipment and storage medium |
US20220253990A1 (en) * | 2021-02-10 | 2022-08-11 | Adobe Inc. | Media enhancement using discriminative and generative models with feedback |
US11748988B1 (en) | 2021-04-21 | 2023-09-05 | Amazon Technologies, Inc. | Shot contras five self-supervised learning of a plurality of machine learning models for video analysis applications |
EP4089574A1 (en) * | 2021-05-14 | 2022-11-16 | Fyma OÜ | A method and system for gathering information of an object moving in an area of interest |
KR102401955B1 (en) | 2021-05-20 | 2022-05-25 | (주)에어패스 | Virtual sports game contents providing system and method using Finite State Machine |
US11671551B2 (en) * | 2021-05-24 | 2023-06-06 | Sony Group Corporation | Synchronization of multi-device image data using multimodal sensor data |
US20230164389A1 (en) * | 2021-11-19 | 2023-05-25 | Qualcomm Incorporated | Analyzing Content Of A Media Presentation |
US11804245B2 (en) * | 2022-01-21 | 2023-10-31 | Kyndryl, Inc. | Video data size reduction |
WO2023243754A1 (en) * | 2022-06-17 | 2023-12-21 | 주식회사 엔씨소프트 | Electronic device and method for extracting time point at which designated motion is captured |
Citations (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101311746A (en) * | 2007-05-24 | 2008-11-26 | 李世雄 | Vehicular obstacle detection device capable of indicating obstacle distance |
US20150169747A1 (en) * | 2013-12-12 | 2015-06-18 | Google Inc. | Systems and methods for automatically suggesting media accompaniments based on identified media content |
US9176987B1 (en) * | 2014-08-26 | 2015-11-03 | TCL Research America Inc. | Automatic face annotation method and system |
US20160328384A1 (en) * | 2015-05-04 | 2016-11-10 | Sri International | Exploiting multi-modal affect and semantics to assess the persuasiveness of a video |
US20170103752A1 (en) * | 2015-10-09 | 2017-04-13 | Google Inc. | Latency constraints for acoustic modeling |
US20170289617A1 (en) * | 2016-04-01 | 2017-10-05 | Yahoo! Inc. | Computerized system and method for automatically detecting and rendering highlights from streaming videos |
US20180018970A1 (en) * | 2016-07-15 | 2018-01-18 | Google Inc. | Neural network for recognition of signals in multiple sensory domains |
CN107632961A (en) * | 2017-07-12 | 2018-01-26 | 天津大学 | Multifrequency interpolation iteration frequency method of estimation and estimator based on all phase FFT spectrum analysis |
CN107960125A (en) * | 2015-06-24 | 2018-04-24 | 谷歌有限责任公司 | Select the representative video frame of video |
US20180204064A1 (en) * | 2017-01-19 | 2018-07-19 | Adrienne Rebecca Tran | Method and system for annotating video of test subjects for behavior classification and analysis |
US20180373980A1 (en) * | 2017-06-27 | 2018-12-27 | drive.ai Inc. | Method for training and refining an artificial intelligence |
CN109218622A (en) * | 2018-11-01 | 2019-01-15 | 华勤通讯技术有限公司 | The generation method and equipment of photo |
US20190057286A1 (en) * | 2017-08-16 | 2019-02-21 | Microsoft Technology Licensing, Llc | Crime scene analysis using machine learning |
CN109389055A (en) * | 2018-09-21 | 2019-02-26 | 西安电子科技大学 | Video classification methods based on mixing convolution sum attention mechanism |
Family Cites Families (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8706655B1 (en) * | 2011-06-03 | 2014-04-22 | Google Inc. | Machine learned classifiers for rating the content quality in videos using panels of human viewers |
US9118886B2 (en) * | 2012-07-18 | 2015-08-25 | Hulu, LLC | Annotating general objects in video |
TWI490827B (en) * | 2013-05-13 | 2015-07-01 | Univ Nat Cheng Kung | Real-time video annotation learning system and method thereof |
US20150032449A1 (en) * | 2013-07-26 | 2015-01-29 | Nuance Communications, Inc. | Method and Apparatus for Using Convolutional Neural Networks in Speech Recognition |
US9620169B1 (en) * | 2013-07-26 | 2017-04-11 | Dreamtek, Inc. | Systems and methods for creating a processed video output |
EP2833325A1 (en) | 2013-07-30 | 2015-02-04 | Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e.V. | Apparatus and method for resource-adaptive object detection and tracking |
US9330171B1 (en) * | 2013-10-17 | 2016-05-03 | Google Inc. | Video annotation using deep network architectures |
US20160080835A1 (en) * | 2014-02-24 | 2016-03-17 | Lyve Minds, Inc. | Synopsis video creation based on video metadata |
US9646227B2 (en) | 2014-07-29 | 2017-05-09 | Microsoft Technology Licensing, Llc | Computerized machine learning of interesting video sections |
US10299017B2 (en) * | 2015-09-14 | 2019-05-21 | Logitech Europe S.A. | Video searching for filtered and tagged motion |
US20170140260A1 (en) | 2015-11-17 | 2017-05-18 | RCRDCLUB Corporation | Content filtering with convolutional neural networks |
US20170178346A1 (en) | 2015-12-16 | 2017-06-22 | High School Cube, Llc | Neural network architecture for analyzing video data |
US10381022B1 (en) * | 2015-12-23 | 2019-08-13 | Google Llc | Audio classifier |
CN107273782B (en) * | 2016-04-08 | 2022-12-16 | 微软技术许可有限责任公司 | Online motion detection using recurrent neural networks |
US9830516B1 (en) * | 2016-07-07 | 2017-11-28 | Videoken, Inc. | Joint temporal segmentation and classification of user activities in egocentric videos |
EP3767547A1 (en) * | 2016-09-06 | 2021-01-20 | Deepmind Technologies Limited | Processing sequences using convolutional neural networks |
US10152637B2 (en) * | 2016-09-14 | 2018-12-11 | Canon Kabushiki Kaisha | Temporal segmentation of actions using context features |
US10430661B2 (en) | 2016-12-20 | 2019-10-01 | Adobe Inc. | Generating a compact video feature representation in a digital medium environment |
US10445582B2 (en) * | 2016-12-20 | 2019-10-15 | Canon Kabushiki Kaisha | Tree structured CRF with unary potential function using action unit features of other segments as context feature |
US11044520B2 (en) * | 2016-12-29 | 2021-06-22 | Telefonaktiebolaget Lm Ericsson (Publ) | Handling of video segments in a video stream |
EP3625677A4 (en) | 2017-05-14 | 2021-04-21 | Digital Reasoning Systems, Inc. | Systems and methods for rapidly building, managing, and sharing machine learning models |
US10628486B2 (en) * | 2017-11-15 | 2020-04-21 | Google Llc | Partitioning videos |
US10740394B2 (en) * | 2018-01-18 | 2020-08-11 | Oath Inc. | Machine-in-the-loop, image-to-video computer vision bootstrapping |
US10846522B2 (en) * | 2018-10-16 | 2020-11-24 | Google Llc | Speaking classification using audio-visual data |
EP3654249A1 (en) * | 2018-11-15 | 2020-05-20 | Snips | Dilated convolutions and gating for efficient keyword spotting |
US11636681B2 (en) * | 2018-11-21 | 2023-04-25 | Meta Platforms, Inc. | Anticipating future video based on present video |
CN109740670B (en) * | 2019-01-02 | 2022-01-11 | 京东方科技集团股份有限公司 | Video classification method and device |
US10984246B2 (en) * | 2019-03-13 | 2021-04-20 | Google Llc | Gating model for video analysis |
US11151386B1 (en) * | 2020-03-04 | 2021-10-19 | Amazon Technologies, Inc. | Automated identification and tagging of video content |
CN111797771B (en) * | 2020-07-07 | 2022-09-09 | 南京理工大学 | Weak supervision video behavior detection method and system based on iterative learning |
-
2019
- 2019-03-13 US US16/352,605 patent/US10984246B2/en active Active
- 2019-09-27 KR KR1020217006604A patent/KR102297393B1/en active IP Right Grant
- 2019-09-27 JP JP2021514518A patent/JP7228682B2/en active Active
- 2019-09-27 EP EP19790932.8A patent/EP3735777A1/en active Pending
- 2019-09-27 WO PCT/US2019/053501 patent/WO2020185256A1/en unknown
- 2019-09-27 CN CN201980060091.9A patent/CN112740709B/en active Active
-
2021
- 2021-03-30 US US17/216,925 patent/US11587319B2/en active Active
Patent Citations (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101311746A (en) * | 2007-05-24 | 2008-11-26 | 李世雄 | Vehicular obstacle detection device capable of indicating obstacle distance |
US20150169747A1 (en) * | 2013-12-12 | 2015-06-18 | Google Inc. | Systems and methods for automatically suggesting media accompaniments based on identified media content |
US9176987B1 (en) * | 2014-08-26 | 2015-11-03 | TCL Research America Inc. | Automatic face annotation method and system |
US20160328384A1 (en) * | 2015-05-04 | 2016-11-10 | Sri International | Exploiting multi-modal affect and semantics to assess the persuasiveness of a video |
CN107960125A (en) * | 2015-06-24 | 2018-04-24 | 谷歌有限责任公司 | Select the representative video frame of video |
US20170103752A1 (en) * | 2015-10-09 | 2017-04-13 | Google Inc. | Latency constraints for acoustic modeling |
US20170289617A1 (en) * | 2016-04-01 | 2017-10-05 | Yahoo! Inc. | Computerized system and method for automatically detecting and rendering highlights from streaming videos |
US20180018970A1 (en) * | 2016-07-15 | 2018-01-18 | Google Inc. | Neural network for recognition of signals in multiple sensory domains |
US20180204064A1 (en) * | 2017-01-19 | 2018-07-19 | Adrienne Rebecca Tran | Method and system for annotating video of test subjects for behavior classification and analysis |
US20180373980A1 (en) * | 2017-06-27 | 2018-12-27 | drive.ai Inc. | Method for training and refining an artificial intelligence |
CN107632961A (en) * | 2017-07-12 | 2018-01-26 | 天津大学 | Multifrequency interpolation iteration frequency method of estimation and estimator based on all phase FFT spectrum analysis |
US20190057286A1 (en) * | 2017-08-16 | 2019-02-21 | Microsoft Technology Licensing, Llc | Crime scene analysis using machine learning |
CN109389055A (en) * | 2018-09-21 | 2019-02-26 | 西安电子科技大学 | Video classification methods based on mixing convolution sum attention mechanism |
CN109218622A (en) * | 2018-11-01 | 2019-01-15 | 华勤通讯技术有限公司 | The generation method and equipment of photo |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210329306A1 (en) * | 2020-04-15 | 2021-10-21 | Nvidia Corporation | Video compression using neural networks |
Also Published As
Publication number | Publication date |
---|---|
WO2020185256A1 (en) | 2020-09-17 |
US20210216778A1 (en) | 2021-07-15 |
KR20210031756A (en) | 2021-03-22 |
US10984246B2 (en) | 2021-04-20 |
JP2022523606A (en) | 2022-04-26 |
KR102297393B1 (en) | 2021-09-02 |
JP7228682B2 (en) | 2023-02-24 |
US11587319B2 (en) | 2023-02-21 |
EP3735777A1 (en) | 2020-11-11 |
CN112740709B (en) | 2023-08-29 |
US20200293783A1 (en) | 2020-09-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN112740709B (en) | Computer-implemented method, computing device, and computer-readable medium for performing gating for video analytics | |
US11231838B2 (en) | Image display with selective depiction of motion | |
EP3612926B1 (en) | Parsing electronic conversations for presentation in an alternative interface | |
US11641445B2 (en) | Personalized automatic video cropping | |
US11949848B2 (en) | Techniques to capture and edit dynamic depth images | |
US20220053195A1 (en) | Learning-based image compression setting | |
CN110678861A (en) | Image selection suggestions | |
JP7483089B2 (en) | Personalized automatic video cropping |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |