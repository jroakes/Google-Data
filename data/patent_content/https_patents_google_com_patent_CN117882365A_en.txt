CN117882365A - Verbal menu for determining and visually displaying calls - Google Patents
Verbal menu for determining and visually displaying calls Download PDFInfo
- Publication number
- CN117882365A CN117882365A CN202280057265.8A CN202280057265A CN117882365A CN 117882365 A CN117882365 A CN 117882365A CN 202280057265 A CN202280057265 A CN 202280057265A CN 117882365 A CN117882365 A CN 117882365A
- Authority
- CN
- China
- Prior art keywords
- call
- selection
- options
- audio data
- visual
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000001755 vocal effect Effects 0.000 title abstract description 7
- 230000000007 visual effect Effects 0.000 claims abstract description 225
- 238000000034 method Methods 0.000 claims description 159
- 230000015654 memory Effects 0.000 claims description 34
- 230000004044 response Effects 0.000 claims description 28
- 238000004891 communication Methods 0.000 claims description 24
- 230000000977 initiatory effect Effects 0.000 claims description 13
- 238000010801 machine learning Methods 0.000 description 51
- 238000012545 processing Methods 0.000 description 26
- 238000012549 training Methods 0.000 description 24
- 238000010586 diagram Methods 0.000 description 16
- 230000008569 process Effects 0.000 description 15
- 238000012937 correction Methods 0.000 description 12
- 238000013528 artificial neural network Methods 0.000 description 11
- 230000009471 action Effects 0.000 description 9
- 230000006870 function Effects 0.000 description 9
- 238000013518 transcription Methods 0.000 description 6
- 230000035897 transcription Effects 0.000 description 6
- 230000000694 effects Effects 0.000 description 5
- 238000004590 computer program Methods 0.000 description 4
- 230000006855 networking Effects 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 239000011521 glass Substances 0.000 description 3
- 230000002452 interceptive effect Effects 0.000 description 3
- 239000011159 matrix material Substances 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000003190 augmentative effect Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 230000000717 retained effect Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 238000012360 testing method Methods 0.000 description 2
- 238000013459 approach Methods 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000012790 confirmation Methods 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000009977 dual effect Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 238000012886 linear function Methods 0.000 description 1
- 230000007787 long-term memory Effects 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 238000007781 pre-processing Methods 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 238000004088 simulation Methods 0.000 description 1
- 230000000153 supplemental effect Effects 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 239000002699 waste material Substances 0.000 description 1
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72469—User interfaces specially adapted for cordless or mobile telephones for operating the device by selecting functions from two or more displayed items, e.g. menus or icons
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/487—Arrangements for providing information services, e.g. recorded voice services or time announcements
- H04M3/493—Interactive information services, e.g. directory enquiries ; Arrangements therefor, e.g. interactive voice response [IVR] systems or voice portals
- H04M3/4936—Speech interaction details
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/42025—Calling or Called party identification service
- H04M3/42034—Calling party identification service
- H04M3/42059—Making use of the calling party identifier
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/225—Feedback of the input speech
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2201/00—Electronic components, circuits, software, systems or apparatus used in telephone systems
- H04M2201/40—Electronic components, circuits, software, systems or apparatus used in telephone systems using speech recognition
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2201/00—Electronic components, circuits, software, systems or apparatus used in telephone systems
- H04M2201/42—Graphical user interfaces
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2203/00—Aspects of automatic or semi-automatic exchanges
- H04M2203/20—Aspects of automatic or semi-automatic exchanges related to features of supplementary services
- H04M2203/2038—Call context notifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2203/00—Aspects of automatic or semi-automatic exchanges
- H04M2203/25—Aspects of automatic or semi-automatic exchanges related to user interface aspects of the telephonic communication service
- H04M2203/251—Aspects of automatic or semi-automatic exchanges related to user interface aspects of the telephonic communication service where a voice mode or a visual mode can be used interchangeably
- H04M2203/252—Aspects of automatic or semi-automatic exchanges related to user interface aspects of the telephonic communication service where a voice mode or a visual mode can be used interchangeably where a voice mode is enhanced with visual information
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2203/00—Aspects of automatic or semi-automatic exchanges
- H04M2203/55—Aspects of automatic or semi-automatic exchanges related to network data storage and management
- H04M2203/551—Call history
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2250/00—Details of telephonic subscriber devices
- H04M2250/74—Details of telephonic subscriber devices with voice recognition means
Abstract
The determination and visual display of the verbal menu of the call. Audio data output in a call between a calling device and a device associated with a target entity is received. The audio data includes speech indicating one or more selection options for the user of the calling device to navigate in the call via a call menu provided by the target entity. Text is determined by programmatically analyzing the audio data, the text representing speech. The selection option is determined based on programmatically analyzing at least one of the text or audio data. At least a portion of the text is displayed by the calling device as one or more visual options corresponding to the selected option during the call. The visual options are each selectable as an operable item via user input to cause a corresponding navigation through the call menu.
Description
Cross Reference to Related Applications
This application claims priority from U.S. patent application Ser. No.17/540,895 entitled "Determination and Visual Display of Spoken Menus for Calls (oral menu of determine and visual display call)" filed on month 2 of 2021, which claims priority from U.S. provisional patent application Ser. No.63/236,651 entitled "Determination and Visual Display of Spoken Menus for Calls" filed on month 8 of 2021, both of which are incorporated herein by reference in their entirety.
Background
Many businesses and other organizations provide automated phone menus, also known as Interactive Voice Response (IVR), for callers calling the business. Typically, a caller calling the business receives an automated voice describing a menu of several options from which the caller may select in spoken words. Typically a hierarchy of such a collection of options is presented, allowing the caller to navigate to the desired result via the options. For example, a caller may desire to receive specific information, request a product or service of an enterprise, talk to an artificial agent, and so forth. The caller may select an option in the call menu by speaking a number, word or phrase, or pressing a key associated with the option that is detected and identified by the automated system.
The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.
Disclosure of Invention
Embodiments of the present application relate to the determination and visual display of a verbal menu of a call. In some implementations, a computer-implemented method includes receiving audio data output in a call between a calling device and a device associated with a target entity. The audio data includes speech indicating one or more selection options for the user of the calling device to navigate in the call via a call menu provided by the target entity. Text is determined by programmatically analyzing the audio data, wherein the text represents speech in the audio data. The selection option is determined based on programmatically analyzing at least one of the text or audio data. At least a portion of the text is caused to be displayed by the calling device during the call, the text being displayed as one or more visual options corresponding to the selection option. The visual options are each selectable via user input to cause a corresponding navigation via the call menu.
Various embodiments and examples of the method are described. For example, in some embodiments, the method further comprises: in response to receiving a selection of a particular visual option of the one or more visual options, causing an indication of the selection to be sent to a device associated with the target entity, wherein the indication is a signal corresponding to a depression of a key of a keypad associated with the particular visual option or a voice provided by the calling device in the call, the voice including a designator associated with the particular visual option. In some implementations, the one or more visual options are each selectable via touch input on a touch screen of the calling device.
In some implementations, the audio data is first audio data, and in response to receiving a selection of a particular visual option, the method further includes: receiving second audio data in the call, the second audio data including second speech indicative of one or more second selection options, programmatically analyzing the second audio data to determine second text representing the second speech in the second audio data, determining the one or more second selection options based on programmatically analyzing at least one of the second text or the second audio data, and causing at least a portion of the second text to be displayed by the calling device as one or more second visual options corresponding to the second selection options, each selectable via a second user input to cause a corresponding navigation via the call menu. In some implementations, the one or more selection options are a plurality of selection options, and the method further includes programmatically analyzing at least one of the text or audio data to determine a hierarchy of the plurality of selection options in the call menu. In some implementations, the method further includes causing one or more selection options to be stored in a memory of the calling device and/or a memory of a remote device in communication with the calling device over the communication network, and retrieving the one or more selection options for a next call between the calling device and the target entity.
In some embodiments, the method further comprises: selection option data comprising one or more selection options is obtained prior to receiving the audio data, and one or more visual options corresponding to the one or more selection options are caused to be displayed by the calling device prior to the calling device receiving the audio data comprising speech indicative of the one or more selection options. In some examples, the selection option in the selection option data is determined by programmatically analyzing audio data received during the previous call. For example, in some implementations, the obtained selection option data is cached in the calling device prior to initiating the call, the obtained selection option data being associated with entity identifiers that have been previously called by callers in a geographic region of the calling device and with entity identifiers that have been previously called at least a threshold number of times or a higher number of times than other entity identifiers not associated with the obtained selection option data.
In some implementations, a visual indicator is caused to be displayed during the call that highlights a particular portion of text of the visual option displayed during the call that is currently received in voice in the audio data during the call. In some embodiments, the method further comprises: the selection option data is compared to one or more selection options determined from the audio data and a determination is made as to whether there is a mismatch between the selection option data and the one or more selection options determined from the audio data. In various embodiments, the method further includes, in response to determining the mismatch, causing the calling device to output a notification of the mismatch and/or to modify the selection option data to match one or more selection options determined from the audio data. In some implementations, comparing the selection option data to the one or more selection options includes comparing text of the selection option data to text of the one or more selection options and/or comparing audio data of the selection option data to audio data received during the call.
In some implementations, a call device that displays a selection option for a call includes a memory that stores instructions, a display device, and at least one processor coupled to the memory that is configured to access the instructions from the memory to perform operations. The operations include: receiving audio data in a call between a calling device and a device associated with a target entity, the audio data comprising speech indicating one or more selection options for a user of the calling device to navigate through a call menu provided by the target entity in the call; programmatically analyzing the audio data to determine text representing speech in the audio data; determining one or more selection options based on programmatically analyzing at least one of the text or audio data; and causing at least a portion of the text to be displayed by the display device during the call, the portion of the text being displayed as one or more visual options corresponding to the one or more selection options, and the one or more visual options each being selectable via user input to cause a corresponding navigation via the call menu.
In various embodiments of the calling device, the processor performs further operations including: in response to receiving a selection of a particular visual option of the one or more visual options, causing an indication of the selection to be sent to a device associated with the target entity, wherein the indication is a signal corresponding to a press of a key of a keypad associated with the particular visual option or a voice provided by the calling device in the call, the voice including an designator associated with the particular visual option. In some implementations, the processor performs further operations including: obtaining selection option data comprising a hierarchy of one or more selection options and one or more selection options in a call menu prior to receiving the audio data; and causing one or more visual options corresponding to the one or more selection options to be displayed by the display device before the calling device receives the audio data including the voice indicative of the one or more selection options.
In some implementations, the processor performs further operations including causing a visual indicator to be displayed during the call that highlights a particular portion of text in one or more visual options displayed during the call that is currently spoken in speech in the audio data during the call. In some implementations, the processor performs further operations including: the selection option data is compared to one or more selection options determined from the audio data and a determination is made as to whether there is a mismatch between the selection option data and the one or more selection options determined from the audio data. In various implementations, the operations performed by the processor may include one or more features of the methods described above.
In some implementations, a non-transitory computer-readable medium having instructions stored thereon, which when executed by a processor, cause the processor to perform operations. The operations include: receiving audio data in a call between a calling device and a device associated with a target entity, the audio data comprising speech indicating one or more selection options for navigation by a user of the calling device via a call menu provided by the target entity; programmatically analyzing the audio data to determine text representing speech in the audio data; determining one or more selection options based on programmatically analyzing at least one of the text or audio data; and causing at least a portion of the text to be displayed by the calling device during the call, the portion of the text being displayed as one or more visual options corresponding to the one or more selection options, and the one or more visual options each being selectable via user input to cause a corresponding navigation via the call menu. In various embodiments, the operations performed by the processor may include one or more features of the methods or calling devices described above.
Drawings
FIG. 1 is a block diagram of an example system that may be used with one or more embodiments described herein;
FIG. 2 is a flow chart illustrating an example method for determining and visually displaying a verbal menu of a call, in accordance with some embodiments;
FIG. 3 is a flow chart illustrating an example method of obtaining selection options of an entity based on obtained data and/or a call, according to some embodiments;
FIG. 4 is a flowchart illustrating an example method for processing audio data from a call and displaying or updating visual options based on the audio data, in accordance with some embodiments;
FIG. 5 is a schematic diagram of a user interface displayed by a calling device in which a call may be initiated, according to some embodiments;
6-10 are schematic diagrams of user interfaces displayed by a calling device in which select options of a call menu in a call are displayed and selected, according to some embodiments;
11-14 are schematic illustrations of a user interface displayed by a calling device in which visual options of a call menu are displayed before a corresponding selection option is spoken in a call; and
FIG. 15 is a block diagram of an example device that may be used in one or more embodiments described herein.
Detailed Description
One or more embodiments described herein relate to the determination and visual display of a verbal menu for a call. In various embodiments, audio data including speech is obtained from a call between a user's calling device and a target entity (e.g., a person or business). The target entity may use an automated voice system (e.g., using an interactive voice response or IVR or answering machine) or a human agent. The voice includes selection options in a call menu via which the user can navigate to obtain the desired result (receive information, speak to human agent, etc.). Text is identified from the call audio data, the text representing speech describing the selection option. The selection options are detected based on analyzing the text and/or audio data. At least a portion of the text is displayed by the calling device during the call as a visual option corresponding to the selected option. The visual options are each selectable via user input to cause a corresponding navigation via the call menu.
Various additional features are described. For example, in some implementations, user selection of a particular visual option causes the selection to be sent to the target entity, which may be a signal corresponding to pressing an appropriate key of a keypad of the calling device, or may be voice provided by the calling device to select the visual option. The audio data and/or text may be analyzed to determine a hierarchy of selection options in the call menu.
In some implementations, the selection option data is obtained by the calling device prior to the call, e.g., received by the calling device from other remote devices or servers storing the selection option data of the various entities. In some examples, the selection option data may have been determined from audio data of a previous call to the entity by the calling device. The calling device may download and cache selection option data and/or entity identifiers (e.g., an entity's phone number, email address, instant messaging or Over The Top (OTT) service identifier, etc.) for various entities prior to the call. In some examples, the cached selection option data may be for entity identifiers that have been called more frequently by the user (e.g., called the most number of times in a set of entity identifiers), or for entity identifiers that have been called at least a threshold number of times by the user in a geographic area (or threshold distance) of the called device.
Using the cached selection options, corresponding visual options may be displayed during or before the call before the selection options are spoken by the target entity in the call. Some embodiments may compare the selection options spoken during the call with cached selection option data and if a mismatch is detected between these option versions, the user may be notified of the mismatch and/or the selection option data may be modified to match the selection options determined from the voice data of the current call. In some implementations, a visual indicator is displayed during the call that highlights a particular portion of text of the visual option that is currently being spoken during the call.
The described techniques and features have several advantages. The described embodiments may provide visual representations of audio call menus during a call. This can greatly assist the user in navigating the call menu because the audio call menu is typically long and places a significant cognitive load on the user to listen to long audio messages to find their desired options. Presenting a corresponding visual version of the call menu on the calling device can greatly assist the user in determining which options are being presented and which options are of interest to the user. Further, the displayed visual options may be directly manipulated and selected by the user such that the user may select the visual options using a simple selection of options, e.g., via a touch on a touch screen. Thus, the complex audio experience is converted into a simple visual experience by the described features.
Additionally, some embodiments may present the options of the call menu in visual form before the options are spoken by the target entity in the call. This allows the user to look ahead of the call menu and in some call menus, allows the user to select a menu option before the option is spoken, so that the target entity immediately advances the call menu to another level without the user having to wait to hear the remaining options spoken in the call. The visual form of the menu allows the user to scan ahead of the spoken portion of the menu, find the desired option, and select the option faster than when hearing the option in audio form, and then find and select the desired option.
A technical effect of one or more of the described embodiments is that the device expends less computing resources to obtain results. For example, a technical effect of the described techniques is a reduction in consumption of system processing resources and power resources as compared to existing systems that do not provide one or more of the described techniques or features. For example, such existing systems may require a user to spend a significant amount of time during a call hearing the output of the available options before determining which option is best suited to the user's needs. In some cases, in such existing systems, the user may forget which menu options were previously presented due to the length of the spoken option message, and may have to replay the menu or call again to understand the available options, taking more time. Such long call times wastefully consume system resources. The features described herein may reduce such drawbacks by: such as displaying selection options for a call menu, allowing a user to more quickly view available call options and select desired options, reducing call duration and resulting in fewer calls being initiated, thereby reducing overall processing and power requirements of the calling device, the target entity device, and other devices in communication with the calling device to enable the call.
Further, in some embodiments, visual call menu options are displayed before those options spoken in the call. The user may scan the visual options prior to the corresponding spoken option, find the desired option, and select the option faster than if the option were in audio-only form. Such a feature reduces call duration and saves processing resources for the calling device and the physical device by allowing the user to navigate through the call menu at a faster rate, including quickly navigating through the call menu that the user has not heard or encountered.
Furthermore, in some embodiments, the selection option data providing the selection options prior to the call may be downloaded to and cached by the calling device prior to initiating the call, such that consumption of processing and networking resources during the call may be reduced. Further, in some embodiments, the spoken selection options may be detected during the call and compared to the cached selection option data to determine if the displayed options may differ from the spoken options, thereby detecting errors or discrepancies that may otherwise waste processing and network resources of the calling device when the user views and selects incorrect or unexpected options. Further, some implementations of the described technology may provide selection options of a displayed call menu prior to a call and/or prior to speaking those options in the call based on data derived from previous calls to entities by a user and user call device (e.g., client device) without the need to directly receive selection option data from an entity or associated entity that may not be available, for example.
Further to the description herein, controls may be provided to the user that allow the user to make selections as to whether and when the systems, programs, or features described herein may enable collection of user information (e.g., information about the user's call history specifying entities and entity identifiers being called, social networks, social actions or activities, professions, preferences of the user including user preferences for call menus, the user's current location, the user's message, outgoing calls made by the user, audio data of the call, or the user's device), and whether to send content or communications from the server to the user. In addition, certain data may be processed in one or more ways before it is stored or used so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be summarized where location information is obtained (such as to a city, zip code, or state level) such that a particular location of the user cannot be determined. Thus, the user may control what information is collected about the user, how that information is used, and what information is provided to the user.
FIG. 1 illustrates a block diagram of an example network environment 100 that may be used in some embodiments described herein. In some implementations, the network environment 100 includes one or more server devices, such as the server system 102 in the example of fig. 1. For example, server system 102 may communicate over network 130. The server system 102 may include a server device 104 and a database 106 or other storage device. Network environment 100 also includes one or more client devices, such as client devices 120, 122, 124, and 126, which may communicate with server 102, each other, and/or with other devices via network connection 130. The network 130 may be any type of communication network including one or more of the internet, a Local Area Network (LAN), a wireless network, a switch or hub connection, and the like. In some implementations, network 130 may include peer-to-peer communication between devices 120-126, e.g., using a peer-to-peer wireless protocol (e.g.,Wi-Fi direct, etc.) or have one client device act as a server to another client device, etc. One example of peer-to-peer communication between two client devices 120 and 122 is shown by arrow 132.
For ease of illustration, FIG. 1 shows one block for server system 102, server device 104, and database 106, and four blocks for client devices 120, 122, 124, and 126. Server boxes 102, 104, and 106 may represent a plurality of systems, server devices, and network databases, and the boxes may be provided in different configurations than shown. For example, server system 102 may represent a plurality of server systems that may communicate with other server systems, e.g., via network 130. In some implementations, the server system 102 can include a cloud-hosted server or a server that provides call services (e.g., voice over internet protocol, VOIP). In some examples, database 106 and/or other storage devices may be provided in a server system block separate from server device 104 and may communicate with server device 104 and other server systems via network 130. Further, there may be any number of client devices. In some examples, server system 102 communicates wirelessly with a client device over network connection 130, which provides various features that may be enabled or supplemented by signals from a server mobile device.
Server system 102 and client devices 120-126 may be any type of device used in a variety of applications, such as, for example, desktop computers, laptop computers, portable or mobile devices, cellular telephones, smart phones, tablet computers, televisions, TV set-top boxes or entertainment devices, wearable devices (e.g., display glasses or goggles, head Mounted Displays (HMDs), headphones, earpieces, body-building bands, watches, earphones, armbands, jewelry, etc.), virtual Reality (VR) and/or Augmented Reality (AR) enabled devices, personal Digital Assistants (PDAs), media players, gaming devices, and the like. Some client devices may also have a local database or other storage similar to database 106. In other embodiments, network environment 100 may not have all of the components shown and/or may have other elements, including other types of elements, in place of or in addition to those described herein.
In various implementations, the client devices 120-126 may interact with the server system 102 via applications running on the respective client devices and/or the server system 102. For example, the respective client devices 120, 122, 124, and 126 may communicate data to the server system 102 and from the server system 102. In some implementations, the server system 102 can send various data, such as content data (e.g., audio, images, video, messages, emails, etc.), notifications, commands, etc., to all or a particular device of the client device. Each client device may send appropriate data, such as acknowledgements, data requests, notifications, user commands, call requests, etc., to the server system 102. In some examples, the server and client devices may transmit various forms of data, including text data, audio data, video data, image data, or other types of data.
In various implementations, end users U1, U2, U3, and U4 may communicate with server system 102 and/or each other using respective client devices 120, 122, 124, and 126. In some examples, users U1, U2, U3, and U4 may interact with each other via applications running on respective client devices and/or server systems 102 and/or via web services implemented on server systems 102, such as social networking services or other types of web services. In some implementations, the server system 102 can provide appropriate data to the client devices such that each client device can receive communication content or shared content uploaded to the server system 102 and/or network service. In some embodiments, a "user" may include one or more programs or virtual entities, as well as people interfacing with a system or network.
User interfaces on client devices 120, 122, 124, and/or 126 may enable display of user content and other content, including images, video, data, and other content, as well as communications (e.g., for telephone or internet calls, video conferences, synchronous or asynchronous chat, etc.), privacy settings, notifications, and other data. Such a user interface may be displayed using software on a client device, software on a server device, and/or a combination of client software and server software executing on server device 104, such as client software or application software in communication with server system 102. The user interface may be displayed by a display device of the client device or the server device, such as a touch screen or other display screen, projector, or the like. In some implementations, an application running on a server system may communicate with a client device to receive user input at the client device and output data such as visual data, audio data, and the like at the client device.
Various applications and/or operating systems executing on the server and client devices may implement various functions, including communication applications (e.g., connecting and providing audio or voice calls, video conferences, chat or other communications), email applications, display of content data, privacy settings, notifications, browsers, etc. The user interface may be displayed on the client device using an application or other software executing on the client device, software on the server device, and/or a combination of the client software and server software executing on the server 102, such as application software or client software in communication with the server 102. The user interface may be displayed by a display device, such as a display screen, projector, etc., of the client device or the server device. In some implementations, an application running on a server can communicate with a client device to receive user input at the client device and output data such as visual data, audio data, and the like at the client device. In some implementations, one or more devices of the network environment 100, such as one or more servers of the server system 102, can maintain electronic encyclopedias, knowledge maps, one or more databases, corpora of words, phrases, symbols, and other information, social networking applications (e.g., social graphs, social networks of friends, social networks of businesses, etc.), websites of places or locations (e.g., restaurants, car dealers, etc.), mapping applications (e.g., websites looking up map locations), call characteristics, and other call data, etc. In some implementations, the server system 102 can include a classifier for a particular type of content item (e.g., text or image) and can determine whether any particular class is detected in the received content item.
Some implementations may provide one or more features described herein on a client or server device that is disconnected from or intermittently connected to a computer network. In some implementations, the client device can provide features and results for asynchronous communications as described herein, e.g., via chat or other messages.
As described herein, the server system 102 and/or one or more of the client devices 120-126 can employ a machine learning model. In some implementations, the machine learning model may be a neural network having one or more nodes arranged according to a network architecture, e.g., in one or more layers, having various nodes connected via the network architecture, and having associated weights. For example, in a training phase of the model, the training data may be used to train the model, and then in an inference phase, the trained model may determine the output based on the input data. In some implementations, the model may be trained offline, for example, on a test device in a test laboratory or other setting, and the trained model may be provided to a server executing the model. In some implementations, the trained model may be retrained or updated locally on the device, or the untrained model may be trained on the device. In some implementations, one or more trained models can be updated with federal learning, where the user permits, for example, where individual server devices can each perform local model training, and updates to the model can be aggregated to update one or more central versions of the model.
Fig. 2 is a flowchart illustrating an example method 200 for determining and visually displaying a verbal menu of a call, in accordance with some embodiments. In some implementations, the method 200 may be implemented on a server, e.g., the server system 102 as shown in fig. 1. In some implementations, some or all of the blocks of method 200 may be implemented on one or more client devices (e.g., client devices 120, 122, 124, and/or 126 as shown in fig. 1), one or more server devices, and/or on both the server device and the client device. In the depicted example, the system implementing the blocks of method 200 includes one or more processor hardware or processing circuitry ("processors") and may access one or more storage devices, such as database 106 or other accessible storage. In some implementations, different components of one or more server systems may perform different blocks or portions of blocks.
Some embodiments may initiate the method 200 or portions thereof based on user input. For example, the user may have selected the initiation of method 200 or a particular block of method 200 from a displayed user interface. In some embodiments, the method 200 or portions thereof may be performed by a user via guidance entered by the user. In some implementations, the method 200 or portions of the method may be initiated automatically by the device. For example, the method (or portions thereof) may be initiated based on the occurrence of one or more particular events or conditions, or periodically. For example, such events or conditions may include obtaining selection option data indicating one or more selection options provided in a call to an entity (e.g., to cause block 208 to be performed), a predetermined period of time that has expired since a last execution of method 200 or a portion thereof, and/or one or more other events or conditions that may have occurred as specified in a setting of a device implementing method 200. In some examples, the device (server or client) may perform method 200 by accessing selection option data in the call (if user consent is received).
In block 202, it is checked whether user consent (e.g., user permissions) to use the user data has been obtained in an embodiment of the method 200. For example, the user data may include user preferences, responses of user selections (e.g., in a dialer application, communication application, or other application), user call characteristic data (e.g., call duration, time and location of a call, audio data received during a call, etc.), other content in a user interface of the device, or other content data items in a collection of content (e.g., a call associated with a user), messages sent or received by the user, information about the user's social network and/or contacts, content ratings, the user's geographic location, historical user data, and so forth. In some implementations, one or more blocks of the methods described herein can use user data.
If user consent has been obtained from the relevant user that may use the user data in method 200, then in block 204, it is determined that blocks of the method herein may be implemented with usable user data as described for those blocks, and the method continues to block 208. If user consent has not been obtained, then in block 206 it is determined that block is to be implemented without using user data, and the method continues to block 208. In some implementations, if user consent has not been obtained, the remainder of method 200 is not performed and/or particular blocks using user data are not performed. In some implementations, if user consent has not been obtained, the blocks of method 200 will be implemented without the use of user data and with commonly or publicly accessible and publicly available data.
In block 208, selection options provided by various entities in the previous call are determined for the entity identifier of the entity based on the obtained data and/or the call made to the entity identifier. The selection option is an option provided to the user (e.g., the calling device) via audio data by an entity (e.g., a server device configured to automatically answer the call and provide audio data) in a call between the entity and the calling device. In some implementations, the selection option may be an option in a call menu provided by the target entity. In some implementations, the call menu may include multiple levels (e.g., hierarchical levels) of sets of options, e.g., one set of options is presented at one menu level and another or different set of options is presented in a different menu level based on the options selected in the previous menu level.
In some implementations, the selection options can be selectable elements or regions presented in the user interface that are not included in the call menu or other menu, such as selectable buttons, links, or other elements that cause actions of the target entity (e.g., examples described with respect to block 232). Such selection options may be determined from speech provided during the call, for example, before, within or after a call menu option is spoken in the call, or from speech in the call that does not provide any call menu options.
In some implementations, structured (or annotated) information can be determined from speech provided by various entities in the call, and can be presented as selection options and/or visual information. For example, outline, tree, formatted text (e.g., text with paragraph breaks, sentence breaks and/or page breaks, punctuation, etc. added by the system) or other structured information may be determined from the speech. In further examples, the structured information may include a Uniform Resource Locator (URL), hyperlink, email, date, location, confirmation number, account number, etc., which may be an operable selection option, e.g., selectable by user input to cause the system to perform one or more operations, such as retrieving and displaying information or web pages, opening or executing programs, etc. Some structured information may be presented as visual information that is not operable or selectable by user input. The structured information may be determined and displayed by the calling device as a selection option during the call or may be determined and displayed by the calling device in addition to the selection options described in the examples herein.
The target entity active in the call may be an automated system, such as an Interactive Voice Response (IVR) system of entities, an answering machine that can provide a call menu and receive selections from the caller, or in some cases an artificial agent of the entity that speaks options in the call and receives the spoken selections of those options from the calling device. In a call presenting selection options, a user can select one or more of the options to navigate (e.g., advance or return) through one or more hierarchical menu levels of the call menu, and/or obtain desired results, such as receiving specific information, requesting a specific product or service, requesting a live human agent speaking to a question that can be answered, and so forth. The call may be a telephone call, a voice call, or other call (e.g., placed via an instant messaging or Over The Top (OTT) service, etc.) connected to a calling device used by the user, such as initiated or answered by the calling device. In block 208, a selection option is determined for various entity identifiers of entities providing such options in a call with the entity. As referred to herein, a target entity is an entity to be called or active in a call (e.g., after a calling device has called the target entity, or vice versa) and is associated with one or more entity identifiers (e.g., target entity identifiers), such as a telephone number or other address information (e.g., user or entity name, user identifier, etc.) that may be used to connect the target entity to a call that allows voice communication. The entities may include any of a variety of people, organizations, businesses, groups, and the like.
In some implementations, the selection options are determined based on entity data received from the entity (including from an associated entity, e.g., a call center or other entity handling the entity's call). In some implementations, the selection option is determined based on a previous call made by the calling device to the entity.
Block 208 may be performed as a preprocessing block that determines and stores selection option data prior to initiating a call to a target entity and prior to determining and visually displaying selection options for the current call, as described below. Some examples of obtaining selection options provided by an entity are described with reference to fig. 3. The method continues to block 210.
At block 210, an entity identifier of a target entity is obtained by a calling device. A calling device is a device that may be used to place a call to an entity, such as client devices 120-126 of fig. 1, or alternatively a server or other device. The entity identifier may be, for example, a telephone number, other call name, address, or other entity identifier that allows the calling device to initiate a call to the target entity. The entity identifier may be obtained in any of several ways in various embodiments and/or situations. For example, the entity identifier may be obtained via user input from a user of the calling device. Such user input may include a user selecting a physical or virtual keypad or key of a keyboard to enter an identifier. In some examples, the entity identifier may be obtained in response to a user selecting a contact entry in a contact list stored on the calling device, which causes the entity identifier associated with the contact entry to be automatically retrieved from storage and provided for use. For example, the entity identifier may be entered or provided to an application running on the calling device, such as a dialer or calling application that originated the call, or another application that may originate the call. In some other examples, the entity identifier may be obtained from another application running on the calling device or from a remote device over a network.
In some examples, the entity identifier is received independent of the call, e.g., to view displayed selection options provided at the time of calling the entity, to prepare for one or more upcoming calls to the target entity, etc., without initiating the call upon receipt of the identifier and/or upon display of the selection options. In other examples, the entity identifier is received to immediately initiate a call to the target entity at the current time, e.g., the identifier is received in a dialer or call application, and the control to initiate the call is selected by the user, or the call is initiated automatically by the calling device. In some embodiments, the entity identifier is received during a (current) call to the target entity or a different entity. For example, the entity identifier may be received by the calling device to initiate a current call that may already be in progress, or to initiate a second call to a different entity. The method may continue to block 212.
In block 212, it is determined whether selection option data is to be retrieved for the entity identifier of the target entity. For example, it may be determined whether selection option data is available to be retrieved. In some examples, the selection option data for the various entities may have been previously obtained or determined in block 208, for example, by a system accessible to the calling device (e.g., other remote devices or servers connected via a network) and/or by the calling device, as described in the example of fig. 3. In some implementations, a portion of the entire set of selection option data for the target entity identifier may have been obtained or determined in block 208 and available for retrieval. In some cases, the selection option data obtained in block 208 does not include data of the target entity identifier, and the selection option data is not available to be retrieved.
In some implementations, the selection option data may already be stored by the calling device such that the selection option data need not be retrieved in block 212. For example, the selection option data for the target entity identifier may have been previously determined based on one or more previous calls by the calling device to the target entity identifier (e.g., prior to block 210). If only some of the selection options in the call menu of the target entity identifier were previously determined and stored by the calling device, then other remaining selection option data may be retrieved.
In another example, the selection options for the target entity identifier or subset thereof may have been previously retrieved by the calling device as selection option data from one or more remote devices. In some implementations, the selection option or a subset thereof may be retrieved and stored in a local store of the calling device prior to receiving the target entity identifier (or any portion thereof) in block 210. This may enable the calling device to access and display the selection options faster than retrieving the selection options from a remote device (e.g., from a server, client device, or other device) over a network when the entity identifier is obtained.
In some implementations, a subset of the selection option data available on the remote device may be received and stored (e.g., cached) in a local store of the calling device prior to initiation of block 210. For example, selection option data for popular entity identifiers for calls by the user may be obtained and stored locally by the calling device. In some examples, these popular entity identifiers may be most frequently called in a set of entity identifiers (e.g., most frequently called in a set of entity identifiers, or more frequently called than other entity identifiers that are not cached in local storage), most frequently called within a particular time period (as described above), and/or most frequently called by users located in the same geographic location or region of the calling device (or having other characteristics similar to those of the user/calling device) (as described above). In another example, selection option data for an entity identifier that has been previously called at least a threshold number of times by a caller in a geographic area (or threshold distance) of the calling device may be obtained by the calling device and stored locally. In another example, the selection option data may be downloaded for an entity identifier located in the country of the user or the country in which the calling device is currently located.
If it is determined in block 212 that the selection option data for the target entity identifier is not retrieved, the method may continue to block 216, as described below. If the selection option data is to be retrieved, the method continues to block 214. In block 214, the available selection option data is retrieved and cached (or otherwise stored) in a local store of the calling device. The cached selection option data may be used to display selection options during the call, as described below. For example, the calling device may retrieve selection option data associated with the target entity identifier from a remote device such as a server or other device over a network (e.g., from a repository of call menu selection options for various entities), and cache the selection option data in a local store of the calling device. In some implementations, the cached selection option data may include data indicating a structure of a call menu in which the selection options are organized. In some implementations, portions of the original audio data (or signatures thereof) from calls analyzed to determine selection options may be cached in association with the selection options, with user permissions.
In some embodiments, the calling device may request that one or more selection options be prefetched from the remote device before the calling device fully obtains the entity identifier in block 210, e.g., before the user has completed entering the entity identifier into the calling device. For example, the calling device may request and download selection options from the remote device for a plurality of candidate entities (e.g., the most frequently called entities or entity identifiers in the geographic region of the calling device, similar to that described above) that match portions of the entity identifiers that have been entered so far. The calling device may then select and use the set of selection options associated with the entity identifier after the identifier is fully specified. Such prefetching may allow the selection options to be displayed faster by the calling device after the entity identifier is specified, because the download of the selection option data is initiated before the identifier input is completed, and the selection options are displayed from a local store.
In some implementations, the prefetching of the selection option data is performed if a threshold portion of the full entity identifier has been received. In some examples, if the full entity identifier is 10 digits, the prefetch may be performed after the eighth (or alternatively, ninth) digit partial identifier has been received, rather than before. This allows the number of candidates to be reduced to a data amount that can be received at the calling device in a relatively short time sufficient to determine matching selection option data after receipt of the full identifier. In some implementations, a subset of a greater amount of selection option data to be prefetched by the calling device is determined, for example, by the calling device or the remote device. For example, the subset of data may be associated with an entity identifier that is determined to be the most likely identifier entered by the user under the user's permission based on one or more factors, such as historical data indicating which entities the user has previously called (e.g., the most frequent and/or most recent entities previously called) and/or entities referenced in the user data (most recent messages, access under the user's permission, etc.).
In some implementations, cached selection option data stored by the calling device may be updated periodically with newer or corrected data based on particular conditions occurring, such as being corrected based on the call of the calling device in response to the data, being updated at the remote device in response to the data (e.g., based on recent calls made by other users, adding new entities or entity identifiers, etc.), periodically after each particular period of time, etc.
In some implementations, the selection option data (or portions thereof) may be determined (as described below) on the calling device during the call and not downloaded from a different device. The method may continue to block 216.
In block 216, it is detected that a call has been initiated between the calling device and the target entity using the obtained entity identifier. In some implementations, the call connects the calling device with a device associated with the target entity. The call may be any connection to the target entity including audio, such as a telephone call, a call via an OTT application, a call via an application (e.g., browser, banking application, browser, etc.), and so forth. In some implementations, the call may optionally be a video call in which video data is transmitted such that video images of the caller and/or callee are displayed at the calling device and/or the target entity device connected to the call. In some examples, the user of the calling device may have initiated the call, for example, selecting a call control in a user interface of an application, such as a dialer application or a call application, to cause the calling device to dial the entity identifier and initiate a call with the target entity. In some examples, the call may have been initiated automatically by the application of the calling device, e.g., after the entity identifier is obtained in block 210. In these cases, the user and the calling device are callers. In some other examples, the call may have been initiated by the target entity, in which case the target entity device is the caller and the user and the calling device are callee. An automated system (e.g., an IVR system or answering machine) and/or artificial agent that may be active in a call and represent a target entity is referred to herein as a target entity. The method continues to block 218.
In block 218, it is determined whether the cached selection options are available for display in a call with the target entity (e.g., cached selection options related to display in a current menu level or other stage of a current call). As described above with respect to blocks 212 and 214, the selection options for the entity identifier of the target entity may already be cached in the storage of the calling device. In some implementations, there may be selection option data cached in local memory that is determined and stored earlier in the current call (or in a previous call of the same calling device), e.g., from one or more previous iterations of blocks 222-230 below, and this cached data may be related to the display in the current phase (e.g., if the user has returned to a previous menu level in the call menu in the current call). If no cached selection options are available, or if it is determined that the available cached selection options are not associated or relevant to the current stage of the call (e.g., a particular hierarchical level in a call menu to which the user has navigated), the method continues to block 222, as described below.
If the relevant cached selection options are available for display, the method continues to block 220 where one or more visual options (e.g., of a call menu) are displayed based on the one or more corresponding cached selection options. The visual options are items displayed in a user interface of the calling device, which may correspond to selectable options of a call menu typically spoken to the user in a call, for example. For example, the visual options may be displayed within an interface of a dialer application or other application, or in a message or notification displayed on the calling device. In some implementations, the visual options may be displayed in separate windows or display areas and/or in response to a user selecting a control to command the display of the visual options.
Visual options may include text, symbols, images, emoticons, icons, and/or other information, and presents options selectable by the user. In some implementations, the visual options may be selected by a user providing touch input, e.g., touching or otherwise contacting a touch screen in a location corresponding to the display of the visual options. In some implementations, one or more of the visual options are associated with a designator (e.g., number, name, keyword, etc.), which is typically spoken or entered by the user (e.g., via key presses) during the call to select the option associated with the designator. Some examples of displaying visual options are described with reference to fig. 4 (with reference to block 224 of fig. 2) and with reference to fig. 5-13 described below.
In block 220, the visual options may be displayed before the corresponding selectable options are spoken in the call by the target entity, such as by an automated system (e.g., an IVR system or answering machine) or by human agents. Thus, the user may immediately view one or more or all of the selectable options available to the user without having to wait for the options to be heard via the slower voice method. In some embodiments, only the selection options for the current level in the hierarchical call menu may be displayed, or in other embodiments, multiple levels of selection options from the call menu may be displayed, e.g., so that the user may view the selection path through the levels of the call menu. In some implementations, or if the command is set by the user of the calling device, the visual option may be displayed prior to initiating the call on the calling device using the entity identifier based on the cached selection option. In some implementations, the visual options are displayed after the call is initiated. For example, the selection options may be displayed in an interface of a dialer application or other application (or as a notification of an operating system) so that the user may view the visual options prior to initiating the call.
In some implementations, the visual options may also or alternatively include other selectable items. For example, the visual options may provide information related to the target entity, for example, and may or may not be related to the options of the call menu. The visual options may include selectable items or portions, such as buttons or check boxes that may be selected by a user to send specific selections or information to the target entity. In some examples, the visual options may include web links or other types of links to various information sources. For example, if selected by a user, such a link may cause a web page, window, or other display area to be opened on the calling device, such as in a browser application or other application, and cause information to be downloaded for display therein. In some implementations, other visual information (e.g., structured information) that is not selectable as described above may be displayed, for example, in addition to the visual options. The method may continue to block 222.
In block 222, audio data is received from the call, including audio data indicative of or representative of speech made by a target entity (e.g., an automation system or human agent) and by a user in the call. The method may continue to block 224.
In block 224, the audio data is processed and visual options are displayed and/or updated based on the audio data. For example, text is determined from speech represented in the audio data, where the text represents the speech. A selection option (and/or other selection options) of the call menu is determined based on the text, the selection option allowing a user of the call device to navigate through the call menu. In some implementations or situations, the selection option is displayed by the calling device as a visual option. In some implementations or cases, visual options have been displayed based on the cached selection options (e.g., based on block 220), and these visual options and corresponding selection options may be updated based on the processed audio data, if appropriate. In some implementations, the structure of the hierarchical call menu including the selection options may also be determined based on the audio data and text derived therefrom. Some examples of displaying and/or updating visual options are described below with reference to fig. 4.
In some implementations, for example, if the calling device has displayed a cached selection option in block 220, block 224 may be skipped or omitted. In some implementations, block 224 may be skipped if the cached selection option was recently determined and is therefore more likely current.
The audio data received in the call is also output by the calling device, e.g. after the audio system of the calling device processes the audio data, so that the speech in the audio data is played via the device speaker, headphones or other audio device in or connected to the calling device. The method may continue to block 226.
In block 226, it is determined whether the user has selected one or more visual options. Various embodiments may allow one or more methods of selecting visual options. For example, visual options may be selected by a user via a touch screen interface, voice commands, physical input devices (mouse, joystick, touchpad, etc.), or other user input devices. If no visual option is selected in block 226, the method continues to block 218 to receive additional audio data from the call. If one or more of the visual options are selected, the method continues to block 228.
In block 228, the selection option corresponding to the selected visual option is sent to the target entity. In some implementations, an indication of the selection is sent to the target entity, where the indication corresponds to input provided as if the user performed a standard selection of options in the call. In some examples, if the selected option can be selected generally via user speech (e.g., speaking a designator such as a number or word associated with the selected option), the transmitted indication can be an appropriate speech spoken by the calling device in the call, e.g., in a speech recorded or synthesized by the calling device speaking the appropriate designator. In some examples, the user may select a visual option via a non-voice input (e.g., touching a button or area displayed on a touch screen), and the calling device may output speech that selects the corresponding selection option via speech. In another example, if the selected option can be selected, typically via pressing a key of a keypad or keyboard, the calling device may send an indication, which is a signal corresponding to the user pressing a key on the device. For example, such signals may include touch tones (e.g., dual tone multi-frequency or DTMF signals) or encodings thereof, or other in-band signals corresponding to particular keys being pressed. In some implementations, alternatives to touch tone or key input may be used, e.g., out-of-band signals, such as signals provided via Session Initiation Protocol (SIP), real-time transport protocol (RTP), H323, etc.
In block 230, it is determined whether there are more selection options to display. For example, user selection of block 226 may cause navigation to a next level of the call menu (e.g., navigation forward or backward in the call menu). The target entity may begin presenting the next level in the call, for example, by speaking a new set of selection options to the user based on the previously selected options, where the new set of options may be displayed by the calling device. In some cases or implementations, the new set of selection options is in a previous level of the call menu that the user previously navigated to, and those selection options may have been cached in a previous iteration of method 200. The selection options for such caches may be retrieved from the caches in the local memory of the calling device. If there are more selection options to display, the method continues to block 218 to check if the cached selection options are available for the new set of selection options.
If, in response to the user selection, there are no selection options to display in block 230, the method continues to block 232 where a result is obtained based on one or more actions of the target entity. The target entity may perform any type and/or number of actions in response to receiving the selected option. For example, the action may be a presentation of information received from the target entity, e.g., if the user's selection is the last option in a particular path through the call menu. For example, the target entity may present (e.g., speak) in the call the information that the user has requested, which is received by the calling device in block 230. In some embodiments or situations, the call may be ended after such information is received. In some implementations or situations, the target entity may request the user to speak information, such as a user name or other information (address, account number, etc.). In some implementations, the target entity can connect the target entity's human agent to the call to speak to the user, which can be detected by the calling device (e.g., using speech recognition techniques), and provide notification to the user.
In some cases, the action of the target entity may include suspending (on hold) the calling device, e.g., waiting for human agent to become available. In some implementations, the calling device (and/or connected device) may automatically determine, without user input or intervention, whether the calling device is suspended in the current call, e.g., by using voice recognition techniques to determine whether the entity's automation system is being suspended via a particular word (e.g., "an agent can take your call in 10minutes,thanks for waiting (an agent may put on your call in 10minutes, thank you to wait)") via a music play indicating the suspended state. In some implementations, if the call has been placed on hold by the target entity, the calling device may display an indication of the hold status, e.g., a message showing music playing, etc. In some implementations, the calling device may detect whether the human agent has connected to the call while the call is on hold, e.g., via a particular word spoken by the human agent or the user, stop suspending music or automatic voice, etc., such that the call is no longer on hold. In some implementations, the calling device may output a notification indicating that the call is no longer on hold and that the artificial agent is connected to the call.
In some implementations or situations, after the target entity takes the action, the target entity presents an option to return to the call menu, in which case the process may continue to block 218.
In some embodiments, after the call is ended, the selection options determined and displayed by the method 200 may be stored in a cache (or other storage) of the calling device and/or may be sent to a storage of a remote device (via a network connection), such as a server, that stores selection option data for various entities and is accessible by multiple calling devices. If the user calls the same target entity identifier again using the calling device (or other user device or client device), the selection option data stored in the cache of the calling device and/or on the remote device may be used for the new call, e.g. to display selection options before those options are spoken in the call. In some implementations, some selection options may be retrieved from a local store of the calling device (e.g., have been previously stored in the local store based on the call selecting these selection options) and/or some selection options may be retrieved from a remote device, similar to that described above. Similarly, any updates or corrections to the cached selection options may be cached on the calling device and/or transmitted to the storage of the selection option data for various entities at the remote device, such as a server.
In some implementations, where permitted by the user, data indicating the event and/or result of the call may be stored as metadata along with the selection options stored after the call. For example, in the case of user permission, the result data may include an indication of which selection options were selected in the call, an indication of whether the user is able to connect to human agent after dialing a particular selection option, an indication of the selection options and duration selected before the user disconnects from the call, and so on. When accumulated from multiple calls and call devices, such data may be used to determine whether to modify selection options provided by the entity in future calls, for example, to increase the effectiveness and efficiency of the presented caller menu.
In some implementations, if user consent has been obtained, a transcription of the call and/or selection options selected by the user during the call may be stored, and the user may view, for example, from a call log on a calling device or other user interface.
Fig. 3 is a flow chart illustrating an example method 300 of obtaining selection options for an entity based on obtained data and/or a call, according to some embodiments. For example, the method 300 may be implemented as block 208 or a portion of block 208 of fig. 2 to obtain a selection option for an entity before a call to a target entity may use such selection option. In some implementations, the method 300 may be performed by a server or other device in addition to the calling device, for example, to obtain selection options that may be downloaded or accessed by the calling device (e.g., client device or other device) prior to or during a call to the target entity, as described with reference to fig. 2. In some implementations, the method 300 may be performed by a calling device (e.g., a client device), or different portions of the method may be performed by a server device and/or a client device, respectively.
The method begins at block 302. In block 302, entity data is obtained from a set of entities, the entity data including selection option data for entity identifiers associated with entities in the set of entities. In some embodiments, entity data may be made available by the entity for the purpose of providing selection options in a call to the entity. For example, the entity data may include an indication of the selection options spoken in the call menu of the associated entity during a call using the entity identifier, including text and other details of the options, and/or a hierarchy of the call menu in which the selection options appear. In some examples, entity data may be found for a particular set of entities (or entity identifiers) that meet a particular criteria. For example, the collection may include a plurality of entities having called entity identifiers that are most popular in the region or area of the calling device and/or within a particular time period. For example, popular entity identifiers may be similar to the most frequent and/or recent calls described above, and calls may be made during a particular time period and/or by callers within a threshold distance or geographic area of the calling device. In some embodiments, entity data may be periodically obtained from an entity such that more recent updates are included in the obtained entity data. In some implementations, entity data can be obtained from an entity associated with the entity represented by the entity identifier, such as a call center or other entity associated with the entity. The method continues to block 304.
In block 304, it is determined whether the entity data is not available for one or more entity identifiers in the set of entities from which the entity data was found. In various examples, entity data may not be provided by an entity for any of a variety of reasons (e.g., security restrictions, trends in selection options being changed quickly and outdated (obsolete), technical problems, etc.). In some implementations, if the known available entity data expires and/or is incorrect, the entity data for the entity identifier may be deemed unavailable. If entity data is available from the set of entities, the method continues to block 210 of FIG. 2, as described above. If entity data is not available from one or more entities in the set of entities, the method continues to block 306.
In block 306, an entity identifier from which entity data is not available from the entity is selected from the entity identifiers associated with the set of entities. In some implementations, this may include an entity identifier for which the known entity data is incomplete, e.g., the entity data may specify some but not all of the selection options of the call menu. In some implementations, incomplete entity data may be determined from user feedback indicating that one or more of the cached selection options displayed during the call are missing (or incorrect) based on the obtained entity data, and thus the entity data may be incomplete. The method continues to block 308.
In block 308, it is determined whether selection option data is available for the selected entity identifier from one or more previous calls that include the selected entity identifier. For example, one or more users may have made calls using the selected entity identifier on previous occasions, and the selection options received during those calls may have been retained, e.g., detected and/or stored if permitted by the user. In some implementations, other call characteristics (e.g., entity identifier, time of call, location of call, duration of call, etc.) of those calls may also be retained with user permission, where the call characteristics have been disassociated from the user making the call such that only the call characteristics are known. Some or all of such data may be used in method 300. For example, with user consent, the user community may have made a previous call over the communication network using the calling device from which the calling feature was obtained. If no such data is available, the process may continue to block 312, described below. If such selection options (and/or other data) are available, the process continues to block 310.
In block 310, a selection option is determined for the selected entity identifier based on the previous call. If user consent has been obtained, one or more selection options may be automatically determined by the system based on analysis of voice data in the recorded audio data from the previous call using techniques such as voice recognition via a machine learning model or other techniques (as described below with reference to FIG. 4). For example, selection option data determined from a previous call to a selected entity identifier may indicate text of selection options presented in the call, and/or structural data of a call menu including selection options presented in the call (e.g., a hierarchy of selection options in the call menu and dependencies of particular options on previously selected options, indicating which selections of the previous options are required to access those options). In some implementations, a particular call may have navigated to and registered some, but not all, of the selection options in the call menu. For example, in a registered call, the user may have selected a single navigation path of consecutive selection options through the call menu without having to make any other path or option branches down. In some implementations, block 310 may include checking a plurality of previous calls to the selected entity identifier following different branches of selection options in the call menu until all selection options in each branch of the call menu are determined (if possible). In some implementations, where the user permits, a portion of the audio data (or signature thereof) analyzed to determine the selection option may be stored in association with the determined selection option. The method may continue to block 318 described below.
In block 312, after determining that the selection option data is not available from a previous call for the selected entity identifier, one or more calls using the selected entity identifier are initiated. In some implementations, the automation system can be used to call the selected entity identifier one or more times. In some implementations, the call may be made at a particular time, for example, if the entity is an enterprise, the call is made during business hours. In some implementations, multiple calls may be made at various times, e.g., outside of business hours, to determine different selection option data that may be presented at such various times. The method continues to block 314.
In block 314, the selection options spoken in the call are determined-e.g., detected and stored-such that the selection options for the selected entity identifier are determined. In some implementations, one or more speech recognition techniques, such as machine learning models or other techniques, are used to detect the selection options. Some examples of selection options and menu structures from audio voice data detection are described below with reference to fig. 4, and similar techniques may be used in block 314. In some implementations, block 314 includes selecting selection options presented in the call to navigate to further levels of the call menu, and receiving audio data at those levels to detect the further selection options. In some implementations, a different navigation path through the selection options of the call menu may be selected in each call to the selected entity identifier in order to determine each available selection option in the presented call menu. In some embodiments, the same path of the selection option may be navigated among multiple calls, for example, to provide additional data for comparison and to check for errors in detecting the selection option. In some implementations, if some selection options are available before block 314 or before the iteration of block 314, a portion of the menu (e.g., a branch) that is not yet determined may be selected to determine the provided selection options, and the available options or portions may be skipped. In some implementations, where the user permits, a portion of the audio data (or signature thereof) analyzed to determine the selection option may be stored in association with the determined selection option. The method continues to block 316.
In block 316, a menu structure of the call menu of the selected entity identifier may be determined based on the detected selection option of block 314. For example, the detected selection options are stored and a data structure (e.g., graph, table, etc.) is created that provides relationships and dependencies between the selection options at different hierarchical levels of the call menu. Selection option data from a plurality of calls to the selected entity identifier may be checked to form a call menu structure that is as complete as possible from the available data. In some implementations, the structure of the selection options in the call menu may have been previously determined, for example, based on the partially complete entity data from block 302 or a previous iteration of method 300. Selection option data from the calls made in blocks 312 and 314 may be added to such existing data structures. The method may continue to block 318.
In block 318, it is determined whether there are more entity identifiers for which the entity data to be selected is not available, and selection option data may be determined. If so, the process continues to block 306 where another entity identifier is selected to determine selection option data in block 306. If there are no more entity identifiers to select, the process may continue to block 210 in FIG. 2.
Fig. 4 is a flow chart illustrating an example method 400 for processing audio data from a call and displaying or updating visual options based on the audio data, in accordance with some embodiments. For example, following block 222, method 400 may be implemented in block 224 of fig. 2, wherein audio data is received in a call with a target entity that has been initiated using the obtained entity identifier.
The method begins at block 402. In block 402, text representing speech in audio data of a call is determined. In some implementations, text is determined using one or more speech recognition techniques, e.g., using one or more machine learning models and/or other techniques. In some implementations, for example, if the user has given permissions and/or set an associated user setting, the calling device can provide a transcription of the call in which recognition text of all words spoken in the call is displayed, including introductory speech, user response, and the like. The method continues to block 404.
In block 404, one or more current selection options and/or menu structures are determined based on the text determined in block 402 and/or the audio data received in block 222. In some implementations, each selection option generally includes the option described for selection, which may be accompanied by a selection designator for the option that the user is to enter into the call to select the option, e.g., by speaking the designator or pressing a corresponding key or button on the calling device (e.g., to provide a tone provided by a touch tone phone, or other signal described with reference to fig. 2). In some examples, the selection options may be detected based on a particular spoken word (or other spoken designator) that may indicate or demarcate the selection option being presented. For example, selecting options may generally include: the word "to" of the heel verb (e.g., "to speak to a representative (speaking to the representative)" or "for your account balance (for your account balance)"), or the word "for" of the heel noun (e.g., "for span" or "for your account balance (for your account balance)"), the selection option may generally begin or end with a phrase that includes "press" or "say", followed by a designator such as a number or word, e.g., "press or say two". In some embodiments, the speech recognition technique may be directed or trained to recognize such words to detect the selection option.
In some implementations, a menu structure of the call menu for the selected entity identifier may also be determined or added based on the detected selection option, e.g., if the audio data and/or the determined selection option indicates that a different level of the call menu has been accessed in the call. This may occur, for example, after the user selects the presented selection option. In some examples, a data structure (e.g., graph, table, etc.) may be created that provides relationships and dependencies between selection options at different hierarchical levels of the call menu. In some implementations, the determined and user-selected selection options may be examined to form a call menu structure that is added as the call proceeds and further options are selected. In some implementations, the call menu structure may be compared to a cached call menu structure (e.g., which may be similar to the cached selection options as described herein), and/or the call menu structure may be stored or accessed in future calls to the target entity to provide a call menu structure for those calls, as described herein.
In some implementations, one or more models can be used to detect selection options and/or menu structures from audio speech data and/or text determined from the audio speech data. In various embodiments, these models may be different from the models used to determine the text representing speech in the audio data used in block 402, or the functions of these models may be included in the same models used in block 402. In some implementations, a model for detecting selection options may be trained based on call characteristics of previous calls, including providing audio data of spoken selection options, text selection options, call menu structures, and the like. In some examples, the model may be trained with training data that provides examples of words corresponding to the selection options and/or training data that includes non-text data (e.g., audio data segments or signatures corresponding to the selection options). In some implementations, the model is a machine learning model, e.g., a neural network having one or more nodes arranged according to a network architecture, e.g., in one or more layers, wherein the various nodes are connected via the network architecture and have associated weights. For example, during a training phase of the model, training data may be used to train the model, and then during an inference phase, the trained model may provide an output based on the input data. Additional examples of features that may be included in the model are described below with reference to fig. 15. Other types of models or techniques may also or alternatively be used to detect selection options.
In some example implementations, a system including one or more machine learning models processes audio data from a call in a streaming manner, runs the audio data through a speech recognition model to provide text (as in block 402), and then detects selection options and/or call structure by pre-training a private neural network from BERT (bi-directional encoder representation from transformer) or other suitable encoding. In addition, the audio data may be directly processed with the audio-to-intent architecture, and the results may be based on a combination of the outputs. The output provides a call menu structure (e.g., a hierarchy) that selects a set of options and those options detected from the audio data.
Some implementations may use any of several other features. For example, some systems may receive an audio data stream, process text to speech in real-time, and have streaming speech recognition; the machine learning model may correct the identified text by altering the identification of the streaming audio data over time as additional audio data is received; the model may determine a confidence level in the speech recognition; the model may use non-text cues or data portions such as audio and timing (e.g., pauses between words) to help identify speech, etc. The method may continue to block 406.
In block 406, it is determined, for example from block 220 of FIG. 2, whether cached selection options have been displayed in the current call to show the selection options available to the user before the target entity speaks those options. If the cached selection option has not been displayed, the method may continue to block 414, as described below. If the cached selection option has been displayed, the method may continue to block 408.
In block 408, it is determined whether there is a mismatch between the current selection option determined in block 404 and the cached selection options that have been displayed, e.g., whether there is a sufficiently large difference between these options to meet one or more thresholds. The current selection option is compared to the cached selection option and, in some embodiments, the menu structure of the current and cached selection options.
In various embodiments, the current selection option may be compared to the cached selection option using one or more of a variety of techniques. In some examples of the first technique, the cached text of the selection option may be compared to the corresponding determined text of the current selection option. In many cases, the text of these options may not match exactly due to errors in speech recognition, e.g., from poor acoustic properties in calls affecting audio data or for other reasons. In some implementations, the magnitude or severity of the mismatch between the current selection option and the cached selection option may be determined, for example, using text comparison techniques. If the magnitude of the mismatch is below a threshold, the cached selection option may be considered to match the current selection option.
In some examples of another technique for comparing selection options, the audio data of the call received in block 222 of fig. 2 may be compared to a corresponding portion of audio data of the cached selection options (if available), and differences in the audio data may be determined. In some implementations, if user permissions have been obtained, the cached selection options may be based on particular audio data from a call made to the target entity identifier prior to the current call (e.g., in the call initiated in block 312 of fig. 3, as described above); such audio data may also be useful for use in training a machine learning model, for example, for detecting selection options. For example, in the case of user permission, audio data used to determine the cached selection option (e.g., from a previous call) may be stored in association with the cached selection option (or may store an audio signature derived from the audio data), e.g., in a local memory of the calling device or retrieved from a remote device. The cached selection options and the corresponding audio data (or corresponding audio signatures) of the currently selected option may be compared for differences. If a significant difference (e.g., more than a threshold difference exists) in the audio data is found in the current audio data and the buffered audio data, a mismatch may exist. The comparison technique may be selected to be robust to account for possible variations in the audio quality of different calls.
In some examples of techniques for determining the accuracy of the text of the currently selected option, both the audio data and the text of the currently selected option may be used to determine the accuracy of the text by aligning the audio with the text. For example, a machine learning model may be trained based on audio data and/or input of recognition text from a selection option in a previous call to output an indication of the likelihood that text has been accurately recognized from the audio data in the current call. The likelihood that the text is correct is determined based on the audio data of the current call, e.g., based on the audio data of the word corresponding to the text and surrounding words forming the context of the word. Such a model may be used to provide accuracy of the text of the currently selected option determined in block 404. In some implementations, audio data and/or text of the corresponding cached selection options may also be provided as input to the model to provide further reference or comparison to the model to increase the accuracy of the model output (e.g., the model may have been trained based on such supplemental inputs). In some implementations, the technique may be used as a speech recognition technique for the currently selected option in block 402.
The cached selection options may also be compared to the menu structure of the current selection option if enough current selection options (and user selections) have been received to determine at least a portion of the call menu structure from the current call. For example, the selection options navigated to from the previous selection options may be compared between the cached call menu and the current call menu.
In some implementations, the comparison of block 408 may determine whether the cached selection options (and/or menu structure) may be incorrect. For example, the target entity may have changed its call menu and the cached selection options may have been obtained at a previous time before the selection options presented by the target entity were changed. The selection option determined in block 404 may generally be more current as the call option is detected in the current call.
If the current selection option matches the displayed cached selection option (e.g., based on one or more thresholds) (and the call menu structure matches), the method may continue to block 418, as described below. In such a case, the displayed visual option is not changed or updated because there is no significant inconsistency with the currently selected option. If there is a mismatch in the current selection option and the cached selection options, for example, if any of the current selection options differs from the displayed cached selection options (or if the call menu structure is different) based on one or more thresholds, the method continues to block 410.
In block 410, cached selection options that are different from the current selection option are corrected based on the current selection option. For example, the cache or other storage storing the selection options of the different caches may be updated by selectively replacing (deemed incorrect) the selection options of the different caches with the corresponding current selection options that are deemed correct. In some examples, for the corresponding current selection option, cached selection option "to receive information about your order, say 3 or press 3" may be detected as "to receive information about your order, say 4 or press 4" with each word except numbers matching; thus, the previous instance "3" is changed to "4" in the store storing the selection option to correct the option. In some implementations, the entire or a larger portion of the cached selection options may be discarded and replaced with the corresponding current selection options. In some implementations, if a mismatch similarly exists in the call menu structure, the previous structural element of the mismatch may be replaced by the element determined in method 400.
In some implementations, corrections to outdated selection options (and/or call menu structures) may also or alternatively be sent to other devices that may store these selection options to update the selection options stored by the other devices. For example, the server (or other remote device) may store the current selection option obtained in block 208 and may send the correct updated selection option (and/or call menu structure) to the server that has been determined in block 404. In some implementations, the server may also determine whether other calling devices have sent such corrections to the server to determine the accuracy of the corrections. For example, if a threshold number of calling devices have sent a correction to a particular selection option, the server may assume that the correction is accurate and may apply the correction to its corresponding stored selection option.
In some implementations, the cached selection options (and/or call menu structure) may not be corrected based on differences in the current selection options, for example, if one or more particular conditions apply. For example, in some embodiments, if the current selection options (and/or their structure in the call menu) are identified by speech recognition techniques having confidence levels below a particular threshold, the current selection options may not have been correctly identified and the cached selection options are not adjusted. In some implementations, the cached selection options may be adjusted if they have an earlier creation date than a threshold period of time prior to the current time, indicating that they are more likely to be stale or outdated. The method may continue to block 412.
In block 412, the visual options displayed by the calling device are updated based on the current selection options determined from the current call in block 404. For example, visual options corresponding to selection options found to be incorrect or outdated in block 410 may be replaced with visual options corresponding to corresponding (e.g., current) selection options that replace incorrect options. In some examples, the text of the incorrect visual option is changed to the text of the correct visual option. In some embodiments, a notification is also displayed in the user interface of the calling device, indicating that corrections have been performed and/or may specifically indicate which corrections have been performed. In some embodiments, no correction is performed under certain conditions, for example, when the confidence level of speech recognition of the text of the selected option is below a threshold. In some implementations, a notification may be displayed indicating that there may be an inconsistency between the visual options displayed and the voice in the call (e.g., the information indicating the visual options may not be what was spoken by the target entity in the call). In some implementations, no correction is performed, and in response to determining that one or more of the corresponding selection options are incorrect, incorrect (and/or all) visual options may be removed from the screen. The method may continue to block 418, described below.
After determining in block 414 that no cached selection options are available and displayed for the current call in block 406, the current selection options determined in block 404 may be cached in the local memory of the calling device. In some implementations, such cached selection options may be retrieved later for display in the current call-e.g., if the user revisits menu levels in the current call-and/or may be retrieved and displayed in the later call. In some cases, one or more of the currently selected options may have been cached, for example, in a current call or a previous iteration of method 400 in a previous call. The method may continue to block 416.
In block 416, visual options for the current call are determined and displayed based on the current selection options determined in block 404. In some implementations, the selection options are displayed after the speech describing the selection options has completed, and each additional selection option may be displayed after the corresponding speech completes the description (e.g., in a later iteration of method 400 during the current call). In some implementations, if this is the first iteration of block 416 for the current call, the visual option may be the first visual option displayed in the user interface for the current call. In a later iteration, the visual options displayed in block 416 may be added to existing visual options displayed in a previous iteration. The method may continue to block 418.
In block 418, an indicator of the current spoken text is displayed and/or updated in the user interface to point to a portion of the visual options currently being spoken by the target entity in the call. In some implementations, the display indicator visually indicates which word, phrase, or entire selection option is currently being spoken in the call. This feature may be used to indicate to the user which of the visual options previously displayed is currently being presented by voice in the call. In various examples, the indicator may take various forms, such as bold text for the visual option (or portion thereof) currently being spoken; changing the font, color, size, or other visual characteristics of such text relative to other text of the visual option and other visual options being displayed; adding a pointer to the interface that is visually associated with the word currently being spoken in the call; etc. For example, the pointer may be an icon, arrow, or other object that appears over the word that is currently being spoken in the call.
In some implementations, if the confidence of the recognition of text determined from the audio data (as in block 402) is below a threshold, a notification may be output by the calling device when the visual option is displayed in block 412 or 416. In some implementations, text identified from the call audio data (e.g., in block 402) may be determined to be in a different language than the user's standard language used on the calling device, and the text may be automatically translated so that the call menu selection options are displayed in the user's language.
In various implementations, for example, if particular features of those blocks are not provided in particular implementations, one or more blocks may be omitted from method 400. For example, in some embodiments of selection options that do not use a cache, blocks 406-412 may be omitted. In another example, blocks of fig. 3 that are not used to obtain entity data in a particular embodiment may be omitted.
The methods, blocks, and operations described herein may be performed in a different order than shown or described in fig. 2-4, and/or concurrently with other blocks or operations (partially or fully), where appropriate. For example, block 220 of fig. 2 may be performed at least partially concurrently with blocks 222 and/or 224. In another example, blocks 414 and 416 of fig. 4 may be performed in a different order and/or at least partially concurrently. Some blocks or operations may be performed on one portion of data and later, for example, performed again on another portion of data. Not all illustrated blocks and operations may be required to be performed in various embodiments. In some implementations, the blocks and operations may be performed multiple times in different orders and/or at different times in a method.
One or more of the methods disclosed herein may operate in several environments and platforms, e.g., as stand-alone computer programs that may be run on any type of computing device, as a mobile application ("app") running on a mobile computing device, etc.
One or more of the methods (e.g., 200, 300, and/or 400) described herein may be run in a stand-alone program that may be executed on any type of computing device, may be run on a program running on a web browser, may be run on a mobile application ("app") running on a mobile computing device (e.g., a cellular telephone, a smart phone, a tablet computer, a wearable device such as a wristwatch, an arm band, jewelry, headwear, virtual reality goggles or glasses, augmented reality goggles or glasses, a head mounted display, etc., a laptop computer, etc.). In one example, a client/server architecture may be used, for example, a mobile computing device (as a client device) sending user input data to a server device and receiving final output data from the server for output (e.g., for display). In another example, all of the computations of the method may be performed within a mobile app (and/or other apps) on the mobile computing device. In another example, the computation may be split between the mobile computing device and one or more server devices.
In one example, a client/server architecture may be used, for example, a mobile computing device (as a client device) sending user input data to a server device and receiving final output data from the server for output (e.g., for display). In another example, all of the computations may be performed within a mobile app (and/or other apps) on the mobile computing device. In another example, the computation may be split between the mobile computing device and one or more server devices.
The methods described herein may be implemented by computer program instructions or code that may be executed on a computer. For example, the code may be implemented by one or more digital processors (e.g., microprocessors or other processing circuitry), and may be stored on a computer program product comprising a non-transitory computer readable medium (e.g., a storage medium) such as a magnetic, optical, electromagnetic, or semiconductor storage medium, including semiconductor or solid state memory, magnetic tape, removable computer diskette, random Access Memory (RAM), read Only Memory (ROM), flash memory, rigid magnetic disk, optical disk, solid state memory drive, etc. The program instructions may also be included in and provided as electronic signals, for example, in the form of software as a service (SaaS) delivered from a server (e.g., a distributed system and/or cloud computing system). Alternatively, one or more of the methods may be implemented in hardware (logic gates, etc.) or a combination of hardware and software. Example hardware may be a programmable processor (e.g., a Field Programmable Gate Array (FPGA), a complex programmable logic device), a general purpose processor, a graphics processor, an Application Specific Integrated Circuit (ASIC), etc. One or more methods may be performed as part of or a component of an application running on a system or as an application or software running in conjunction with other applications and operating systems.
Fig. 5 is a schematic diagram of an example user interface 500 displayed by a display screen of a calling device in which a call may be initiated, according to some embodiments. For example, interface 500 may be displayed on a touch screen by a display device of a client device, e.g., one of client devices 120-126 as shown in FIG. 1, or a different device such as a server device, e.g., server system 102.
In some implementations, the user interface 500 can be associated with a call application that initiates a call to other devices, answers incoming calls from other devices, and communicates with other devices via a call connection. In this example, the name 502 of the target entity is displayed, where the target entity has been selected, for example, directly by the user from a web page, contact list, or other information display, as a result of a search or navigation of the entity, or as a result of another user application or application process executing on the calling device or other device. Also shown is a Call interface 504 that includes a numeric keypad 508, an identifier-entry field 510, and a Call control 512. The keys of the numeric keypad 508 may be selected by a user (e.g., via a touch screen or other input device) to type an identifier 514 in a typing field 510, e.g., one character at a time, or in other embodiments, multiple characters. Entity identifier 514 is associated with the entity indicated by entity name 502. A call may be initiated to an entity by using entity identifier 514. For example, entity identifier 514 is shown as a telephone number, but other types of identifiers may be entered to enable calls to entities associated with the identifier (e.g., an email address or other address). In some implementations, the identifier 514 can be automatically entered in the enter field 510 by the calling device, for example, in response to a user selecting to call the target entity from a different application (e.g., a map application, web browser, etc.), which causes the calling device to display the interface 500. If the user selects call control 512, call control 512 may cause the calling device to dial identifier 514 of the target entity indicated by name 502 and initiate a call to the target entity.
Fig. 6 is a schematic diagram of a call interface 600 in which select options for a call menu in a call are displayed, according to some embodiments. The call interface 600 may be displayed by a calling device (e.g., a client device such as one of the client devices 120-126, or a display device of a different device such as the server system 102). For example, call interface 600 may be displayed after a call to target entity 502 is initiated via entity identifier 514 shown in fig. 5. In some examples, the call may be initiated in response to a user selecting the call control 512 of the interface 500 of fig. 5 selected by the user. Alternatively, the call may be initiated by the application in various other ways, e.g., in response to a user selection of a target entity or entity identifier, in response to another event, automatically based on a scheduling command from the user, etc.
The name 602 of the target entity may be displayed to indicate the callee of the current call. The duration may also be displayed to indicate the time elapsed since the call was initiated.
In some implementations, if permissions and/or commands from the user have been obtained, a transcription of all speech uttered by the user and the target entity during the call can be transcribed by the calling device and displayed in the user interface 600. For example, a transcription of speech may be displayed in the display area 604 of the user interface 600. Interface 600 may also include various user controls that, when selected by a user, cause control of functions associated with call or interface 600. For example, disconnect control 606 disconnects the calling device from the call, keypad (Keypad) control 608 displays a numeric Keypad (or keyboard) in or on junction 600 (e.g., similar to Keypad 508), speaker (Speaker) control 610 causes the audio output of the calling device to be output as a Speaker phone, and Mute (Mute) control 612 causes user speech and other sounds at the calling device not to be transmitted to the callee in the call.
In the example of fig. 6, a call to a target entity is answered by an automated voice (IVR) system (as the target entity), where the automated voice speaks and provides the user (caller) with a selection option in the call menu. Text 614 is transcribed from speech detected and recognized by the user's calling device during the call. The first portion 616 of the transcript 614 corresponds to the voice of the automated system providing introductory information that is detected as not being a selection option of the call menu nor a part of such selection option. In other implementations, text that is detected as not included in the selection option, such as the first portion 614, is not displayed by the calling device unless the user has set device preferences or is set to do so.
The second portion 618 of text 614 corresponds to speech in the call that is detected as being in a different language than the default language or the language of first portion 616. The second portion 618 corresponds to a selection option ("for Spanish press number (pressing number 9 for spanish)") in the call menu presented by the target entity. In fig. 6, the second portion 618 has not been detected as a selection option.
Fig. 7 is a schematic diagram of a call interface 600 in which select options of a call menu in a call are detected and displayed, according to some embodiments. In the example of fig. 7, a second portion 618 (shown in fig. 6) of the transcribed text 614 has been detected as a selection option in the call menu, for example, using speech recognition techniques. For example, upon detecting a word designating it as a selection option, the second portion 618 of text is converted (e.g., by a calling device or a connected remote device) to a selection option having an associated visual option 702 displayed in interface 600. In addition, text portion 614 is removed from the screen and replaced with visual option 702. In some implementations, when converting text to visual options, some text in the second portion 618 may be removed, as shown for visual options 702, so that visual options may be presented more succinctly.
In some implementations, as shown, a border, outline, or other visual separator may be displayed around the text of visual option 702 to indicate that visual option 702 is a delineated option that may be selected similarly to a button. In this example, the number specified in the spoken selection option ("nueve") is converted to a selection designator 704 displayed in (or associated with) the visual option 702 that indicates the number that can be selected on the keypad (e.g., via keypad control 608) (or in some embodiments, spoken in the call) for selecting the visual option 702. In this example, if the user selects visual option 702, the language in which the automated system speaks in the current call is changed to the indicated language (spanish in this example). Selection of visual option 702 also causes text displayed in interface 600, such as transcribed text and selection options, to be changed to the selected language.
The visual option 702 may be selected by a user, for example, via user input, such as a user touching the visual option 702 on a touch screen, manipulating an input device, voice commands, or the like. For example, if visual option 702 is selected by touch input via a touch screen, the selection of that option is sent by the calling device to the target entity. In some examples, the selection is sent by the calling device outputting a signal (e.g., tone) in the call that provides the equivalent of the signal output when the user presses a designated numeric key, such as the "9" key on the keypad in the example of fig. 7. This causes the target entity to receive a signal indicating that the user has selected the number "9" and the corresponding selection option associated with visual option 702.
In the example of fig. 7, visual option 702 is not selected and the target entity's automated system continues to speak, describing further selection options in its call menu. The speech is detected and transcribed into a text portion 706, in this example, the text portion 706 is displayed in the display area 604 as original text under the visual option 702 before being detected as a selection option.
Fig. 8 is a schematic diagram of a call interface 600 in which additional selection options of a call menu in a call have been detected and displayed, according to some embodiments. In the example of fig. 8, text portion 706 (shown in fig. 7) has been detected as a selection option. Visual option 802 corresponding to the selected option is displayed in display area 604 after visual option 702. Text portion 706 is removed from the screen and replaced with visual option 802.
Visual option 802 may be selected by a user. In this case, the selection option 802 indicates that a designator is spoken as a word ("travel") used to select the option, rather than pressing a key on the keypad as with the visual option 702. If the user speaks the word into the target entity, the callee detects the word and the selection of the associated selection option. In some implementations, the selection options of the voice selection may be visually specified in interface 600 to distinguish them from the selection options selected by pressing the keypad key. In this example, an icon 804 is displayed in the visual option 802 (or otherwise visually associated with the visual option 802) to indicate that a designator of voice selection may select the selection option. In some call menus, the selection option may be selected by voice or by a user's key press. In some implementations, visual options for such selection options may be displayed with a selection designator that shows a key identifier and shows a selectable indication of speech.
In the example of fig. 8, additional selection options have been detected after additional speech from the target entity. These selection options are displayed as visual options that are continuously displayed in the display area 604. Each visual option may be determined and displayed similar to visual options 702 and 802 described above. In some implementations, each selection option is provided as a visual option as text is detected and recognized from the voice in the call, similar to that shown in fig. 6 and 7. In some implementations, the voice in the call can be displayed as original text until all call menu selection options for the current menu level have been spoken in the call, at which point the text is converted to visual options in place of the text display.
In some implementations, if the user selects any visual option before all of the selection options are detected and displayed at the current menu level, the remaining selection options of the current menu level are not displayed as visual options (e.g., upon receiving a selection, the target entity may interrupt speaking further selection options at the current menu level and begin speaking selection options at the next menu level).
In fig. 8, the user has not selected any of the selection options displayed in interface 600 and has detected and displayed additional selection options as associated visual options, similar to visual options 702 and 802. For example, visual options 806 and 808 are detected as selectable by key presses such that selection designators 810 and 812 are displayed with numbers corresponding to the keypad keys used to select these options, respectively. The visual option 814 is detected as selectable by key presses of the "star" key of the keyboard such that the selection designator 816 is displayed in a star symbol.
Fig. 9 is a schematic diagram of a call interface 600 in which a user has selected visual options of a call menu, according to some embodiments. In the example of fig. 9, the user has tapped the touch screen of the calling device at the location of visual option 808 to select the visual option. In this example, in response to the selection, the calling device displays a selected icon 902 in place of a selection designator 812 (shown in fig. 8) to indicate that visual option 808 has been selected, and the other visual options 702, 802, 806, and 814 of the call menu are displayed with lower visibility (e.g., brightness and/or color is changed to more nearly background brightness/color) to emphasize the selected visual option 808. Various embodiments may provide other ways of highlighting selected visual options relative to other visual options of a displayed call menu.
In response to selection of the visual option 808, the calling device sends a signal indicating the selected number ("2") to the target entity in the call. The target entity receives the selected number and responds accordingly, as described below.
Fig. 10 is a schematic diagram of a call interface 600 in which visual options of a call menu have been selected, according to some embodiments. In the example of fig. 10, upon receiving a selection corresponding to the visual option 808 as shown in fig. 9, the target entity changes the call menu to a different level based on the selected option (which may be a second, third, or later level of the call menu, etc.). In this example, the next menu level of this navigation path of the call menu includes a plurality of selection options spoken by the target entity in the call, detected by the calling device, and converted to the displayed visual options 1002, 1004, and 1006. Additional visual options may also be displayed if there are further selection options at the current level of the call menu. One of these visual options may be selected by the user, similar to that described above for the previous visual option. In some implementations, as shown, the display screen may scroll down to display further visual options detected in the call menu.
Fig. 11 is a schematic diagram of a call interface 1100 displayed by a calling device in which visual options of a call menu are displayed before a corresponding selection option is spoken in a call, according to some embodiments. Call interface 1100 may be similar to call interface 600 shown in fig. 6. In some implementations, the call interface 1100 may be displayed after the calling device initiates a call to a target entity, such as the target entity 502 of fig. 5, via the entity identifier. In some implementations, the call interface 1100 (or a similar interface, such as the user interface 500 of fig. 5) may be displayed prior to initiating a call to a target entity. For example, selection options may be displayed for the target entity indicating prior to the call which options will be available to the user in the call after the call is initiated via the entity identifier of the target entity.
In this example, the call is initiated, for example, in response to a user selecting call control 512 of interface 500 of fig. 5 or one of the other ways. The name 1102 of the target entity associated with the target entity of the current call may be displayed and the duration may be displayed to indicate the time elapsed since the call was initiated. In some implementations, if permissions and/or commands from the user have been obtained, a transcription of all speech uttered by the caller and callee during the call can be transcribed by the calling device and displayed in a display area 1104 in the user interface 1100, similar to the call interface 600 of fig. 6. Disconnect control 1106, keypad control 1108, speaker control 1110, and mute control 1112 may be similar to the corresponding controls described above.
In the example of fig. 11, call menu 1120 is displayed immediately after or during call initiation (or may be displayed prior to initiation of the call, as described above). In this example, call menu 1120 includes five visual options 1122, 1124, 1126, 1128, and 1130, which are similar to the selection options described above with respect to FIGS. 6-10. As described herein, the selection options for these visual options are accessible to the calling device prior to the target entity speaking based on the selection option data received prior to the call. Displaying the visual options of the call menu 1120 before the target entity speaks the corresponding selection options allows the user to look ahead at the call menu and, in some call menu embodiments, allows the user to select a selection option that causes the target entity to advance the call menu to another level without having to speak the remaining options of the menu.
In some implementations, as shown in fig. 11, other portions of the spoken content of the target entity in the call to the target entity may be retrieved prior to the call, similar to the selection options of the call menu as described herein, and may be displayed prior to the target entity speaking the text during the call. In the example of fig. 11, text 1132 is displayed before the target entity speaks the text and is displayed over the call menu selection option in display area 1104. For example, text 1132 may include introduction information similar to the example of fig. 6 that is detected based on selection option information obtained prior to the call or is previously known not to be part of the selection options of the call menu.
During a call, the target entity emits voice information that is detected and identified by the calling device and/or other connected devices. In general, the voice information should match the displayed text and visual options (the voice information may not completely match the displayed visual options due to converting some of the voice information into visual option formats, such as selecting designator icons or numbers). As described above with respect to fig. 4, if the spoken information does not match the text of the visual option, the visual option may be corrected and a corrected version displayed instead of the original version. In some embodiments or situations where correction is not performed, a notification may be displayed indicating that an error may be present in the visual options, and/or one or more of the visual options may be removed from the display screen.
In some implementations as shown, an indicator may be displayed to indicate a portion of the displayed text (including text in the selection option) that is currently being spoken by the target entity in the call. In this example, the indicator highlights the current spoken text 1134 in bold. Subsequent text 1134 and selection options 1122-1130 have not been spoken in the call and are displayed in a conventional (e.g., non-bold) font and/or with reduced visibility (e.g., higher or lower brightness, depending on background brightness and/or color). In this example, as the spoken portion of the call menu continues to highlight new text, the text previously spoken in the call remains highlighted in bold so that the new, preceding bold text indicates the current spoken text in the call. In some implementations, as shown in fig. 12, the previous spoken text that is not part of the selection option can be shown with reduced emphasis relative to the text of the selection option. In some implementations, the current spoken text can be highlighted in other ways, for example, in a different color than other displayed text, with a separate pointer, arrow, or other visual indicator, etc. displayed over or near the current spoken text.
The displayed indication of the current callee spoken text allows the user to view the progress of the spoken call menu, which may allow the user to see, for example, whether the target entity is currently waiting for the user to select the provided option. In some implementations of the call menu, the target entity may not respond to selection of the selection option until a certain amount of progress has been made when the call menu is spoken. For example, a selection option may have to be fully spoken or partially spoken a certain amount before it can be selected. In these embodiments, providing an indicator of the current spoken text in the call may allow the user to estimate when visual options are eligible for selection, potentially reducing wasteful attempts by the user to select options that do not respond to the target entity.
Fig. 12 is a schematic diagram of the call interface 1100 of fig. 11, in which an indicator of the current spoken text advances to a visual option of the call menu, according to some embodiments. In this example, the target entity has spoken the remainder of the introduction text 1132 and the selection options represented by visual options 1122. Thus, all text 1132 and visual options 1122 are displayed in highlighted form, e.g., bold text and/or greater visibility. In addition, the initial portion 1202 of the visual option 1124 is currently being spoken by the callee such that the portion 1202 is displayed in a highlighted form as compared to other portions of the visual option 1124 (in some embodiments, one or more portions of the visual option may also be highlighted when at least a portion of the visual option is being spoken, such as the illustrated selection designator and/or boundary of the option). Visual options 1126, 1128, and 1130 have not been spoken by the target entity and are displayed with reduced visibility.
Fig. 13 is a schematic diagram of the call interface 1100 of fig. 11, wherein an indicator of the current spoken text is further advanced in the call menu, in accordance with some embodiments. In this example, the target entity has already spoken introduction text 1132 and selection options represented by visual options 1122, 1124, and 1126. Thus, text 1132 and these visual options are displayed in highlighted form with greater visibility than before being spoken. In addition, the initial portion 1302 of visual option 1128 is currently being spoken by the callee such that portion 1302 is displayed in a highlighted form as compared to the other portions of visual option 1128. Visual options 1130 have not been spoken by the target entity and are displayed with reduced visibility.
Fig. 14 is a schematic diagram of the call interface 1100 of fig. 11 in which an indicator of the current spoken text advances to the next level in the call menu, according to some embodiments. In this example, the target entity has already spoken all of the selection options that introduce text 1132 and an initial level of the call menu. Thus, text 1132 and these visual options are displayed in highlighted form (only visual options 1126, 1128, and 1130 are currently visible in fig. 14 due to scrolling of the display screen). In addition, the user has selected visual option 1128 as indicated by selection designator 1402.
After selecting visual option 1128, the calling device displays the next level of the call menu. As with the previous stage shown in fig. 11, since the calling device has access to data indicating these options as described herein, the next stage of visual options is known in advance and displayed before being spoken in the call. The next level of visual options are displayed as visual options 1404, 1406, 1408, and 1410. In the example of fig. 14, the initial portion 1412 of the visual option 1404 is currently being spoken by the callee such that the portion 1412 is displayed in a highlighted form as compared to other portions of the visual option 1404 (in some embodiments, as shown, one or more portions of the visual option may also be highlighted, such as a selection designator of an associated key number). Visual options 1406-1410 have not been spoken by the target entity and are displayed with reduced visibility.
In the examples of fig. 6-14, the user and/or calling device originated the call and was the caller, and the target entity was the callee (entity callee) in the example call. In other examples, the target entity may call the user and/or the calling device such that the target entity is a caller and the user and/or the calling device is a callee.
Fig. 15 is a block diagram of an example device 1500 that may be used to implement one or more features described herein. In one example, device 1500 can be used to implement a client device, such as any of client devices 120-126 shown in FIG. 1. Alternatively, device 1500 may implement a server device, such as server device 104, and the like. In some implementations, the device 1500 may be used to implement a client device, a server device, or a combination of the above. Device 1500 may be any suitable computer system, server, or other electronic or hardware device as described herein.
In some implementations, the device 1500 includes a processor 1502, a memory 1504, and an I/O interface 1506. The processor 1502 may be one or more processors and/or processing circuits to execute program code and control the basic operations of the device 1500. A "processor" includes any suitable hardware system, mechanism, or component that processes data, signals, or other information. The processor may include a system with a general purpose Central Processing Unit (CPU) having one or more cores (e.g., in a single-core, dual-core, or multi-core configuration), multiple processing units (e.g., in a multi-processor configuration), a Graphics Processing Unit (GPU), a Field Programmable Gate Array (FPGA), an Application Specific Integrated Circuit (ASIC), a Complex Programmable Logic Device (CPLD), dedicated circuitry for implementing functions, a dedicated processor for implementing neural network model-based processing, a neural circuit, a processor optimized for matrix computation (e.g., matrix multiplication), or other system.
In some implementations, the processor 1502 may include one or more coprocessors that implement neural network processing. In some implementations, the processor 1502 may be a processor that processes data to produce a probabilistic output, e.g., the output produced by the processor 1502 may be inaccurate or may be accurate within a range from an expected output. For example, a processor may perform its functions in a "real-time," "offline," "batch mode," or the like manner. Portions of the processing may be performed by different (or the same) processing systems at different times and at different locations. The computer may be any processor in communication with the memory.
The memory 1504 is typically provided in the device 1500 for access by the processor 1502 and may be any suitable processor-readable storage medium suitable for storing instructions for execution by the processor, such as Random Access Memory (RAM), read Only Memory (ROM), electrically erasable read only memory (EEPROM), flash memory, etc., and is located separately from and/or integrated with the processor 1502. The memory 1504 may store software operated by the processor 1502 on the server device 1500, including an operating system 1508, machine learning applications 1530, other applications 1512, and application data 1514. Other applications 1512 may include applications such as data display engines, communication applications (e.g., dialers or calling applications, over-the-top calling applications, other applications with calling capabilities, such as applications associated with particular entities such as banks, restaurants, or other organizations/providers that provide apps), web hosting engines, image display engines, notification engines, social networking engines, and so forth. In some implementations, the machine learning application 1530 and/or the other application 1512 may each include instructions that enable the processor 1502 to perform the functions described herein, e.g., some or all of the methods of fig. 2, 3, and/or 4. The application data 1514 may include call menu data such as selection option data and other entity data, audio data from a call (with user permissions), audio data from a call menu, text transcription of a call menu, timestamps indicating recent call selection options and call menu structures, call characteristics including time of call, duration of call, and other characteristics of previous calls (with user permissions), and/or data structures (e.g., tables, lists, figures) that may be used to determine call selection options as described herein.
The machine learning application 1530 may include one or more Named Entity Recognition (NER) implementations that may use supervised and/or unsupervised learning. The machine learning model may include a model based on multitasking learning, residual task two-way LSTM (long-term memory) with conditional random field, statistical NER, etc. One or more of the methods disclosed herein may operate in several environments and platforms, e.g., as stand-alone computer programs that may be run on any type of computing device, as web applications with web pages, as mobile applications ("apps") running on mobile computing devices, etc.
In various implementations, the machine learning application 1530 may utilize a bayesian classifier, support vector machine, neural network, or other learning technique. In some implementations, the machine learning application 1530 can include a trained model 1534, an inference engine 1536, and data 1532. In some implementations, the data 1532 can include training data, e.g., data for generating a trained model 1534. For example, the training data may include any type of data suitable for training a model for determining selection options for a call, such as voice data indicating voice uttered during a previous call, call menu data indicating selection options provided by an entity in the call, call characteristics of the user's previous call (if user consent has been obtained), and so forth. The training data may be obtained from any source, such as, for example, a data repository specially marked for training, data providing permissions to be used as training data for machine learning, and the like. In embodiments where one or more users allow for training a machine learning model, e.g., trained model 1534, using their respective user data, the training data may include such user data. In embodiments where users allow access to their corresponding user data, data 1532 may include allowed data.
In some implementations, the training data may include synthetic data generated for training purposes, such as data that is not based on user input or activity in the context being trained, e.g., data generated from simulations or models, etc. In some implementations, the machine learning application 1530 excludes data 1532. For example, in these embodiments, the trained model 1534 may be generated, for example, on a different device and provided as part of the machine learning application 1530. In various embodiments, the trained model 1534 may be provided as a data file including model structure or form and associated weights. Inference engine 1536 may read the data file of trained model 1534 and implement a neural network with node connectivity, layers, and weights based on the model structure or form specified in trained model 1534.
The machine learning application 1530 also includes one or more trained models 1534. For example, such models may include trained models for recognizing speech and determining selection options based on speech received as audio data in a call as described herein. In some implementations, the trained model 1534 can include one or more model forms or structures. For example, the model form or structure may include any type of neural network, such as a linear network, a deep neural network that implements multiple layers (e.g., a "hidden layer" between an input layer and an output layer, where each layer is a linear network), a convolutional neural network (e.g., a network that segments or partitions input data into multiple portions or tiles, processes each tile separately using one or more neural network layers, and aggregates results from the processing of each tile), a sequence-to-sequence neural network (e.g., a network that takes as input sequential data such as words in a sentence, frames in a video, etc., and produces as output a sequence of results), and so forth.
The model form or structure may specify connectivity between various nodes and organization of nodes into layers. For example, a node of a first layer (e.g., an input layer) may receive data as input data 1532 or application data 1514. Such data may include, for example, voice data from the call, entity data indicating selection options for the call, call characteristics for previous calls, and/or feedback from the user regarding previous calls and provided selection options. The subsequent middle tier may receive as input the output of the node of the previous tier according to the connectivity specified in the model form or structure. These layers may also be referred to as hidden layers. The final layer (e.g., output layer) produces the output of the machine learning application. For example, the output may be a set of selection options to be provided in the interface. In some implementations, different layers or models may be used to recognize speech, for example, receiving an input of audio data and providing an output of text representing speech in the input audio data. In some implementations, the model form or structure also specifies the number and/or type of nodes in each layer.
In various embodiments, one or more trained models 1534 may include a plurality of nodes arranged in layers in a model structure or form. In some embodiments, the node may be a computing node without memory, e.g., configured to process an input unit to produce an output unit. The computation performed by the node may include, for example, multiplying each of the plurality of node inputs by a weight, obtaining a weighted sum, and adjusting the weighted sum with a deviation or intercept value to produce a node output.
In some implementations, the computation performed by the node may further include applying a step/activate function to the adjusted weighted sum. In some embodiments, the step/activate function may be a non-linear function. In various embodiments, such computation may include operations such as matrix multiplication. In some implementations, the computation of the plurality of nodes may be performed in parallel, for example, using multiple processor cores of a multi-core processor, using individual processing units of a GPU, or dedicated neural circuitry. In some implementations, a node may include memory, e.g., may be capable of storing and using one or more earlier inputs when processing subsequent inputs. For example, the nodes with memory may include Long Short Term Memory (LSTM) nodes. LSTM nodes may use memory to maintain a "state" that allows the node to function like a Finite State Machine (FSM). Models with such nodes may be used to process sequential data, such as words in sentences or paragraphs, frames in video, speech or other audio, and so forth.
In some implementations, the one or more trained models 1534 can include embeddings or weights for individual nodes. For example, a model may be initiated as a plurality of nodes organized in layers specified by a model form or structure. At initialization, a respective weight may be applied to the connections between each pair of nodes connected according to the model form, e.g., nodes in successive layers of the neural network. For example, the corresponding weights may be randomly assigned or initialized to default values. For example, the corresponding weights may be randomly assigned or initialized to default values. The model may then be trained, for example, using data 1532, to produce results.
For example, training may include applying supervised learning techniques. In supervised learning, training data may include a plurality of inputs (e.g., audio data and/or entity data) and a corresponding expected output for each input (e.g., a set of selection options for a call menu; and/or text representing speech in the audio data). The value of the weight is automatically adjusted based on a comparison of the model's output to the expected output, e.g., in a manner that increases the probability that the model will produce the expected output when providing similar inputs.
In some implementations, training may include applying unsupervised learning techniques. In unsupervised learning, only input data may be provided and a model may be trained to distinguish the data, e.g., to cluster the input data into groups, where each group includes input data that are similar in some way. For example, a model may be trained to determine or cluster call characteristics that are similar to each other.
In another example, a model trained using unsupervised learning may cluster features of speech or selection options based on using speech and selection options in a data source. In some implementations, unsupervised learning may be used to generate knowledge representations that may be used, for example, by the machine learning application 1530. In various embodiments, the trained model includes a set of weights or embeddings corresponding to the model structure. In embodiments omitting data 1532, the machine learning application 1530 can include a trained model 1534 based on prior training, for example, by a developer of the machine learning application 1530, a third party, or the like. In some implementations, one or more of the trained models 1534 can each include a set of weights that are fixed, e.g., downloaded from a server that provides the weights.
The machine learning application 1530 also includes an inference engine 1536. The inference engine 1536 is configured to apply the trained models 1534 to data, such as application data 1514, to provide inferences, such as a set of selection options in a call menu and a structure of the call menu. In some implementations, the inference engine 1536 can include software code to be executed by the processor 1502. In some implementations, the inference engine 1536 can specify a circuit configuration (e.g., for a programmable processor, for a Field Programmable Gate Array (FPGA), etc.) that enables the processor 1502 to apply the trained model. In some implementations, the inference engine 1536 may include software instructions, hardware instructions, or a combination. In some implementations, the inference engine 1536 can provide an Application Programming Interface (API) that the operating system 1508 and/or other applications 1512 can use to call the inference engine 1536, e.g., apply the trained model 1534 to the application data 1514 to generate inferences.
The machine learning application 1530 may provide several technical advantages. For example, when generating the trained model 1534 based on unsupervised learning, the inference engine 1536 may apply the trained model 1534 to generate a knowledge representation (e.g., a digital representation) from input data, such as application data 1514. For example, a model trained to determine selection options and/or menu structures may produce a representation thereof. In some implementations, such representations may help reduce processing costs (e.g., computational costs, memory usage, etc.) of generating output (e.g., labels, classifications, estimated characteristics, etc.). In some implementations, such representations can be provided as inputs to different machine learning applications that produce outputs from the outputs of inference engine 1536.
In some implementations, the knowledge representation generated by the machine learning application 1530 may be provided to a different device that performs further processing, for example, over a network. In such embodiments, providing a knowledge representation rather than data may provide technical benefits, e.g., faster data transfer at reduced cost.
In some implementations, the machine learning application 1530 can be implemented in an offline manner. In these embodiments, the trained model 1534 may be generated in a first stage and provided as part of the machine learning application 1530. In some implementations, the machine learning application 1530 may be implemented in an online manner. For example, in such embodiments, an application (e.g., the operating system 1508, one or more other applications 1512) invoking the machine learning application 1530 may utilize the reasoning generated by the machine learning application 1530, e.g., provide the reasoning to the user, and may generate a system log (e.g., if the user allows, actions taken by the user based on the reasoning, or if used as input for further processing, generate results of further processing). The system log may be generated periodically (e.g., hourly, monthly, quarterly, etc.) and may be used to update the trained model 1534 if permitted by the user, e.g., to update the embedding of the trained model 1534.
In some implementations, the machine learning application 1530 may be implemented in a manner that may be adapted to the particular configuration of the device 1500 on which the machine learning application 1530 is executed. For example, the machine learning application 1530 may determine a computational graph, such as the processor 1502, that utilizes available computing resources. For example, if the machine learning application 1530 is implemented as a distributed application across multiple devices, the machine learning application 1530 may determine the computations to be performed on the respective devices in a manner that optimizes the computations. In another example, the machine learning application 1530 may determine that the processor 1502 includes a GPU having a particular number of GPU cores (e.g., 1000), and implement the inference engine accordingly (e.g., as 1000 separate processes or threads).
In some implementations, the machine learning application 1530 can implement an integration of a trained model (ensable). For example, the trained model 1534 may include a plurality of trained models each adapted for the same input data. In these implementations, the machine learning application 1530 can select a particular trained model based on, for example, available computing resources, success rates with previous inferences, and the like. In some implementations, the machine learning application 1530 can execute the inference engine 1536 such that multiple trained models are applied. In these embodiments, the machine learning application 1530 may combine the outputs from applying the individual models, for example, using voting techniques that score the individual outputs from applying each trained model, or by selecting one or more particular outputs. Further, in these embodiments, the machine learning application may apply a time threshold to supply the individual training models (e.g., 0.5 ms) and utilize only those individual outputs that are available within the time threshold. The output that was not received within the time threshold may not be utilized, e.g., discarded. Such an approach may be suitable, for example, when there are time constraints specified when invoking a machine learning application, e.g., by the operating system 1508 or one or more other applications 1512.
In different implementations, the machine learning application 1530 may produce different types of outputs. In some implementations, the machine learning application 1530 can generate output based on a format specified by a calling application, such as the operating system 1508 or one or more other applications 1512. In some implementations, the calling application may be another machine learning application. For example, such a configuration may be used to generate an countermeasure network in which the output from the machine learning application 1530 is used to train the invoking machine learning application and vice versa.
Any software in the memory 1504 may alternatively be stored on any other suitable storage location or computer readable medium. Further, the memory 1504 (and/or other connected storage devices) may store one or more messages, one or more taxonomies, electronic encyclopedias, dictionaries, topics, knowledge bases, message data, grammars, user preferences, and/or other instructions and data used in the features described herein. Memory 1504 and any other type of storage (magnetic disk, optical disk, magnetic tape, or other tangible medium) may be considered a "storage" or "storage device.
The I/O interface 1506 may provide functionality that enables the server device 1500 to interface with other systems and devices. The interface device may be included as part of the device 1500 or may be separate and in communication with the device 1500. For example, network communication devices, storage devices (e.g., memory 1504 and/or database 106), and input/output devices may communicate via I/O interface 1506. In some implementations, the I/O interface may be connected to interface devices such as input devices (keyboard, pointing device, touch screen, microphone, camera, scanner, sensor, etc.) and/or output devices (display device, speaker device, printer, motor, etc.).
Some examples of interface devices that may be connected to the I/O interface 1506 may include one or more display devices 1520 and one or more data stores 1538 (as described above). The display device 1520 may be used to display content, e.g., a user interface of an output application as described herein. Display device 1520 may be connected to device 1500 via a local connection (e.g., a display bus) and/or via a network connection, and may be any suitable display device. Display device 1520 may include any suitable display device such as an LCD, LED or plasma display screen, CRT, television, monitor, touch screen, 3-D display screen, or other visual display device. For example, the display device 1520 may be a flat display screen provided on a mobile device, multiple display screens provided in goggles or a head-mounted device, a projector, or a monitor screen for a computer device.
The I/O interface 1506 may interface to other input and output devices. Some examples include display devices, printer devices, scanner devices, and the like. Some embodiments may provide microphones for capturing sound, voice commands, etc., audio speaker devices for outputting sound, or other input and output devices.
For ease of illustration, FIG. 15 shows one block for each of the processor 1502, memory 1504, I/O interface 1506, and software blocks 1508, 1512, and 1530. These blocks may represent one or more processors or processing circuitry, operating systems, memory, I/O interfaces, applications, and/or software modules. In other embodiments, device 1500 may not have all of the components shown and/or may have other elements, including other types of elements in place of or in addition to those shown herein. Although some components are described as performing the blocks and operations as described in some embodiments herein, the environment 100, the device 1500, a similar system, or any suitable component or combination of components of any suitable one or more processors associated with such a system may perform the described blocks and operations.
Although the present description has been described with respect to particular embodiments, these particular embodiments are merely illustrative and not restrictive. The concepts shown in the examples may be applied to other examples and implementations.
In addition to the above description, a control may be provided to the user that allows the user to make a selection as to whether and when the system, program, or feature described herein may enable collection of user information (e.g., information about the user's social network, social actions or activities, profession, user preferences, or the current location of the user or user device), and whether to send content or communications from a server. In addition, certain data may be processed in one or more ways before it is stored or used so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be summarized where location information is obtained (such as to a city, zip code, or state level) such that a particular location of the user cannot be determined. Thus, the user may control what information is collected about the user, how that information is used, and what information is provided to the user.
Note that the functional blocks, operations, features, methods, devices, and systems described in this disclosure may be integrated or partitioned into different combinations of systems, devices, and functional blocks, as known to those of skill in the art. The routines of the particular embodiments may be implemented using any suitable programming language and programming technique. Different programming techniques may be employed, such as procedural or object oriented. The routines may execute on a single processing device or multiple processors. Although steps, operations, or computations may be presented in a specific order, the order may be changed in different specific implementations. In some embodiments, a plurality of steps or operations sequentially shown in the specification may be performed at the same time.
Claims (20)
1. A computer-implemented method, comprising:
receiving audio data in a call between a calling device and a device associated with a target entity, the audio data comprising speech indicating one or more selection options for a user of the calling device to navigate in the call via a call menu provided by the target entity;
programmatically analyzing the audio data to determine text representing the speech in the audio data;
Determining the one or more selection options based on programmatically analyzing at least one of the text or the audio data; and
causing at least a portion of the text to be displayed by the calling device during the call, wherein the portion of the text is displayed as one or more visual options corresponding to the one or more selection options, wherein the one or more visual options are each selectable via user input to cause a corresponding navigation via the call menu.
2. The computer-implemented method of claim 1, further comprising: in response to receiving a selection of a particular visual option of the one or more visual options, causing an indication of the selection to be sent to the device associated with the target entity, wherein the indication is one of:
a signal corresponding to a depression of a key of a keypad, the key being associated with the particular visual option, or
A voice provided by the calling device in the call, the voice including a designator associated with the particular visual option.
3. The computer-implemented method of claim 1, wherein the one or more visual options are each selectable via touch input on a touch screen of the calling device.
4. The computer-implemented method of claim 1, wherein the audio data is first audio data, and wherein, in response to receiving a selection of a particular visual option of the one or more visual options, the method further comprises:
receiving second audio data in the call, the second audio data comprising second speech indicative of one or more second selection options for the user of the call device;
programmatically analyzing the second audio data to determine second text representing the second speech in the second audio data;
determining the one or more second selection options based on programmatically analyzing at least one of the second text or the second audio data; and
causing at least a portion of the second text to be displayed by the calling device, wherein the at least a portion of the second text is displayed as one or more second visual options corresponding to the one or more second selection options, wherein the one or more second visual options are each selectable via a second user input to cause a corresponding navigation via the call menu.
5. The computer-implemented method of claim 1, wherein the one or more selection options are a plurality of selection options, and further comprising programmatically analyzing at least one of the text or the audio data to determine a hierarchy of the plurality of selection options in the call menu.
6. The computer-implemented method of claim 1, further comprising, prior to receiving the audio data:
obtaining selection option data comprising the one or more selection options; and
such that the one or more visual options corresponding to the one or more selection options are displayed by the calling device before the audio data comprising the speech indicative of the one or more selection options is received by the calling device.
7. The computer-implemented method of claim 6, wherein the one or more of the selection option data is determined by programmatically analyzing audio data received during a previous call.
8. The computer-implemented method of claim 7, wherein the obtained selection option data is cached in the calling device prior to initiating the call, wherein the obtained selection option data is associated with an entity identifier that was previously called by a caller in a geographic area of the calling device, wherein the entity identifier has been previously called at least a threshold number of times or has been previously called a higher number of times than other entity identifiers not associated with the obtained selection option data.
9. The computer-implemented method of claim 6, further comprising:
causing a visual indicator to be displayed during the call, wherein the visual indicator highlights a particular portion of the text in the one or more visual options displayed during the call that is currently received in the speech in the audio data during the call.
10. The computer-implemented method of claim 6, further comprising:
comparing the selection option data with the one or more selection options determined from the audio data; and
a determination is made as to whether there is a mismatch between the selection option data and the one or more selection options determined from the audio data.
11. The computer-implemented method of claim 10, further comprising: in response to determining a mismatch between the selection option data and the one or more selection options determined from the audio data, causing a notification of the mismatch to be output by the calling device.
12. The computer-implemented method of claim 10, further comprising: in response to determining a mismatch between the selection option data and the one or more selection options determined from the audio data, the selection option data is modified to match the one or more selection options determined from the audio data.
13. The computer-implemented method of claim 10, wherein comparing the selection option data with the one or more selection options comprises one of:
comparing text of the selection option data with the text of the one or more selection options; or alternatively
Audio data of the selection option data is compared with the audio data received during the call.
14. The computer-implemented method of claim 1, further comprising:
causing the one or more selection options to be stored in at least one of: storage of the calling device or storage of a remote device in communication with the calling device over a communication network; and
the one or more selection options are retrieved for a next call between the calling device and the target entity.
15. A calling device displaying a selection option for a call, the calling device comprising:
a memory having instructions stored thereon;
a display device; and
at least one processor coupled to the memory, the at least one processor configured to access the instructions from the memory to perform operations comprising:
Receiving audio data in a call between the calling device and a device associated with a target entity, the audio data comprising speech indicating one or more selection options for a user of the calling device to navigate in the call via a call menu provided by the target entity;
programmatically analyzing the audio data to determine text representing the speech in the audio data;
determining the one or more selection options based on programmatically analyzing at least one of the text or the audio data; and
causing at least a portion of the text to be displayed by the display device during the call, wherein the portion of the text is displayed as one or more visual options corresponding to the one or more selection options, wherein the one or more visual options are each selectable via user input to cause a corresponding navigation via the call menu.
16. The calling device of claim 15, wherein said at least one processor performs further operations comprising: in response to receiving a selection of a particular visual option of the one or more visual options, causing an indication of the selection to be sent to the device associated with the target entity, wherein the indication is one of:
A signal corresponding to a depression of a key of a keypad, the key being associated with the particular visual option, or
A voice provided by the calling device in the call, the voice including a designator associated with the particular visual option.
17. The calling device of claim 15, wherein said at least one processor performs further operations comprising, prior to receiving said audio data:
obtaining selection option data comprising a hierarchy of the one or more selection options in the call menu; and
such that the one or more visual options corresponding to the one or more selection options are displayed by the display device before the audio data comprising the speech indicative of the one or more selection options is received by the calling device.
18. The calling device of claim 17, wherein the at least one processor performs further operations comprising:
causing a visual indicator to be displayed during the call, wherein the visual indicator highlights a particular portion of the text in the one or more visual options displayed during the call that is currently spoken in the speech in the audio data during the call.
19. The calling device of claim 17, wherein said at least one processor further performs operations comprising:
comparing the selection option data with the one or more selection options determined from the audio data; and
a determination is made as to whether there is a mismatch between the selection option data and the one or more selection options determined from the audio data.
20. A non-transitory computer-readable medium having instructions stored thereon, which when executed by a processor, cause the processor to perform operations comprising:
receiving audio data in a call between a calling device and a device associated with a target entity, the audio data comprising speech indicating one or more selection options for navigation by a user of the calling device via a call menu provided by the target entity;
programmatically analyzing the audio data to determine text representing the speech in the audio data;
determining the one or more selection options based on programmatically analyzing at least one of the text or the audio data; and
Causing at least a portion of the text to be displayed by the calling device during the call, wherein the portion of the text is displayed as one or more visual options corresponding to the one or more selection options, wherein the one or more visual options are each selectable via user input to cause a corresponding navigation via the call menu.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163236651P | 2021-08-24 | 2021-08-24 | |
US63/236,651 | 2021-08-24 | ||
US17/540,895 US11895269B2 (en) | 2021-08-24 | 2021-12-02 | Determination and visual display of spoken menus for calls |
US17/540,895 | 2021-12-02 | ||
PCT/US2022/037550 WO2023027833A1 (en) | 2021-08-24 | 2022-07-19 | Determination and visual display of spoken menus for calls |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117882365A true CN117882365A (en) | 2024-04-12 |
Family
ID=85285456
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280057265.8A Pending CN117882365A (en) | 2021-08-24 | 2022-07-19 | Verbal menu for determining and visually displaying calls |
Country Status (3)
Country | Link |
---|---|
US (1) | US11895269B2 (en) |
KR (1) | KR20240046508A (en) |
CN (1) | CN117882365A (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117275319B (en) * | 2023-11-20 | 2024-01-26 | 首都医科大学附属北京儿童医院 | Device for training language emphasis ability |
Family Cites Families (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5199062A (en) * | 1988-01-20 | 1993-03-30 | Phone Base Systems Inc. | Telephone communications system including a digital telephone switch, a voice response unit and a stored program sequence for controlling both the switch and the voice response unit |
US4880026A (en) * | 1988-07-28 | 1989-11-14 | Parapluie, Ltd. | Integrated free-standing vehicle detailing service center |
US6167255A (en) * | 1998-07-29 | 2000-12-26 | @Track Communications, Inc. | System and method for providing menu data using a communication network |
US8625756B1 (en) | 2010-02-03 | 2014-01-07 | Tal Lavian | Systems and methods for visual presentation and selection of IVR menu |
US9472185B1 (en) * | 2011-01-05 | 2016-10-18 | Interactions Llc | Automated recognition system for natural language understanding |
US9426289B2 (en) * | 2013-09-12 | 2016-08-23 | Avaya Inc. | Techniques for topical customer service menu reconfiguration based on social media |
US9430186B2 (en) * | 2014-03-17 | 2016-08-30 | Google Inc | Visual indication of a recognized voice-initiated action |
US10659501B2 (en) * | 2014-06-30 | 2020-05-19 | Avaya Inc. | System and method for efficient port and bandwidth utilization in setting up communication sessions |
US9723147B1 (en) * | 2015-03-25 | 2017-08-01 | State Farm Mutual Automobile Insurance Company | Method and system for a scalable computer-telephony integration system |
US10579330B2 (en) | 2015-05-13 | 2020-03-03 | Microsoft Technology Licensing, Llc | Automatic visual display of audibly presented options to increase user efficiency and interaction performance |
US9832308B1 (en) | 2016-05-12 | 2017-11-28 | Google Inc. | Caller preview data and call messages based on caller preview data |
US10277478B2 (en) * | 2016-06-08 | 2019-04-30 | Genesys Telecommunications Laboratories, Inc. | Connected machine initiated service |
CN105933555B (en) * | 2016-06-12 | 2021-02-12 | 腾讯科技（深圳）有限公司 | Communication method, system and related device with call center |
US11128752B2 (en) * | 2018-03-28 | 2021-09-21 | Avaya Inc. | Personalized wait treatment during interaction |
US11418647B1 (en) * | 2020-10-16 | 2022-08-16 | Intrado Corporation | Presenting multiple customer contact channels in a browseable interface |
-
2021
- 2021-12-02 US US17/540,895 patent/US11895269B2/en active Active
-
2022
- 2022-07-19 KR KR1020247005486A patent/KR20240046508A/en unknown
- 2022-07-19 CN CN202280057265.8A patent/CN117882365A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR20240046508A (en) | 2024-04-09 |
US20230066100A1 (en) | 2023-03-02 |
US11895269B2 (en) | 2024-02-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11349991B2 (en) | Systems and methods to present voice message information to a user of a computing device | |
US10521189B1 (en) | Voice assistant with user data context | |
US9940931B2 (en) | Corrective feedback loop for automated speech recognition | |
US9053096B2 (en) | Language translation based on speaker-related information | |
US8170866B2 (en) | System and method for increasing accuracy of searches based on communication network | |
WO2020117504A1 (en) | Training of speech recognition systems | |
WO2020117505A1 (en) | Switching between speech recognition systems | |
WO2020117506A1 (en) | Transcription generation from multiple speech recognition systems | |
WO2020117507A1 (en) | Training speech recognition systems using word sequences | |
US20230179704A1 (en) | Digital assistant integration with telephony | |
US20090249198A1 (en) | Techniques for input recogniton and completion | |
CN114787814B (en) | Reference resolution | |
CN107564526B (en) | Processing method, apparatus and machine-readable medium | |
CN114760387A (en) | Method and device for managing maintenance | |
CN117882365A (en) | Verbal menu for determining and visually displaying calls | |
US20230388422A1 (en) | Determination and display of estimated hold durations for calls | |
JP2015228162A (en) | Information search method, device and program | |
US11765274B2 (en) | Determination and display of estimated hold durations for calls | |
WO2023027833A1 (en) | Determination and visual display of spoken menus for calls | |
US20230153061A1 (en) | Hierarchical Context Specific Actions from Ambient Speech | |
CN117769831A (en) | Determining and displaying estimated hold duration for call | |
CN117014660A (en) | Display method and device and electronic equipment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication |