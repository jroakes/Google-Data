CN103827780A - Methods and systems for a virtual input device - Google Patents
Methods and systems for a virtual input device Download PDFInfo
- Publication number
- CN103827780A CN103827780A CN201280044081.4A CN201280044081A CN103827780A CN 103827780 A CN103827780 A CN 103827780A CN 201280044081 A CN201280044081 A CN 201280044081A CN 103827780 A CN103827780 A CN 103827780A
- Authority
- CN
- China
- Prior art keywords
- projection
- input equipment
- equipment
- image
- input device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 title claims abstract description 100
- 230000033001 locomotion Effects 0.000 claims abstract description 42
- 230000006870 function Effects 0.000 claims description 28
- 230000008859 change Effects 0.000 claims description 20
- 230000001052 transient effect Effects 0.000 claims description 7
- 230000009471 action Effects 0.000 abstract description 14
- 239000011521 glass Substances 0.000 description 28
- 238000004891 communication Methods 0.000 description 24
- 210000003811 finger Anatomy 0.000 description 22
- 230000000007 visual effect Effects 0.000 description 17
- 238000010586 diagram Methods 0.000 description 13
- 210000003128 head Anatomy 0.000 description 9
- 238000004590 computer program Methods 0.000 description 6
- 210000004247 hand Anatomy 0.000 description 6
- 230000008569 process Effects 0.000 description 6
- 230000003287 optical effect Effects 0.000 description 5
- 238000012545 processing Methods 0.000 description 4
- 230000001413 cellular effect Effects 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 3
- 230000008447 perception Effects 0.000 description 3
- 238000012552 review Methods 0.000 description 3
- 238000005452 bending Methods 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000035515 penetration Effects 0.000 description 2
- 230000011514 reflex Effects 0.000 description 2
- 210000003813 thumb Anatomy 0.000 description 2
- 241001062009 Indigofera Species 0.000 description 1
- XUIMIQQOPSSXEZ-UHFFFAOYSA-N Silicon Chemical compound [Si] XUIMIQQOPSSXEZ-UHFFFAOYSA-N 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 239000011449 brick Substances 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 230000010267 cellular communication Effects 0.000 description 1
- 238000013075 data extraction Methods 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000011156 evaluation Methods 0.000 description 1
- 238000007667 floating Methods 0.000 description 1
- 210000000245 forearm Anatomy 0.000 description 1
- 210000001061 forehead Anatomy 0.000 description 1
- 230000036541 health Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 239000013307 optical fiber Substances 0.000 description 1
- 230000001737 promoting effect Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 229920006395 saturated elastomer Polymers 0.000 description 1
- 229910052710 silicon Inorganic materials 0.000 description 1
- 239000010703 silicon Substances 0.000 description 1
- 210000000162 simple eye Anatomy 0.000 description 1
- 238000004513 sizing Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003595 spectral effect Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 230000009897 systematic effect Effects 0.000 description 1
- 210000000707 wrist Anatomy 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/041—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means
- G06F3/042—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means by opto-electronic means
- G06F3/0425—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means by opto-electronic means using a single imaging device like a video camera for tracking the absolute position of a single or a plurality of objects with respect to an imaged reference surface, e.g. video camera imaging a display or a projection screen, a table or a wall surface, on which a computer generated image is displayed or projected
- G06F3/0426—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means by opto-electronic means using a single imaging device like a video camera for tracking the absolute position of a single or a plurality of objects with respect to an imaged reference surface, e.g. video camera imaging a display or a projection screen, a table or a wall surface, on which a computer generated image is displayed or projected tracking fingers with respect to a virtual keyboard projected or printed on the surface
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B27/0172—Head mounted characterised by optical features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/017—Gesture based interaction, e.g. based on a set of recognized hand gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04886—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures by partitioning the display area of the touch-screen or the surface of the digitising tablet into independently controllable areas, e.g. virtual keyboards or menus
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N9/00—Details of colour television systems
- H04N9/12—Picture reproducers
- H04N9/31—Projection devices for colour picture display, e.g. using electronic spatial light modulators [ESLM]
- H04N9/3141—Constructional details thereof
- H04N9/3173—Constructional details thereof wherein the projection device is specially adapted for enhanced portability
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N9/00—Details of colour television systems
- H04N9/12—Picture reproducers
- H04N9/31—Projection devices for colour picture display, e.g. using electronic spatial light modulators [ESLM]
- H04N9/3179—Video signal processing therefor
- H04N9/3185—Geometric adjustment, e.g. keystone or convergence
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N9/00—Details of colour television systems
- H04N9/12—Picture reproducers
- H04N9/31—Projection devices for colour picture display, e.g. using electronic spatial light modulators [ESLM]
- H04N9/3191—Testing thereof
- H04N9/3194—Testing thereof including sensor feedback
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/014—Head-up displays characterised by optical features comprising information/image processing systems
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B2027/0178—Eyeglass type
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0179—Display position adjusting means not related to the information to be displayed
- G02B2027/0187—Display position adjusting means not related to the information to be displayed slaved to motion of at least a part of the body of the user, e.g. head, eye
-
- G—PHYSICS
- G03—PHOTOGRAPHY; CINEMATOGRAPHY; ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ELECTROGRAPHY; HOLOGRAPHY
- G03B—APPARATUS OR ARRANGEMENTS FOR TAKING PHOTOGRAPHS OR FOR PROJECTING OR VIEWING THEM; APPARATUS OR ARRANGEMENTS EMPLOYING ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ACCESSORIES THEREFOR
- G03B17/00—Details of cameras or camera bodies; Accessories therefor
- G03B17/48—Details of cameras or camera bodies; Accessories therefor adapted for combination with other photographic or optical apparatus
- G03B17/54—Details of cameras or camera bodies; Accessories therefor adapted for combination with other photographic or optical apparatus with projector
-
- G—PHYSICS
- G03—PHOTOGRAPHY; CINEMATOGRAPHY; ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ELECTROGRAPHY; HOLOGRAPHY
- G03B—APPARATUS OR ARRANGEMENTS FOR TAKING PHOTOGRAPHS OR FOR PROJECTING OR VIEWING THEM; APPARATUS OR ARRANGEMENTS EMPLOYING ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ACCESSORIES THEREFOR
- G03B21/00—Projectors or projection-type viewers; Accessories therefor
- G03B21/14—Details
- G03B21/20—Lamp housings
- G03B21/2006—Lamp housings characterised by the light source
- G03B21/2033—LED or laser light sources
-
- G—PHYSICS
- G03—PHOTOGRAPHY; CINEMATOGRAPHY; ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ELECTROGRAPHY; HOLOGRAPHY
- G03B—APPARATUS OR ARRANGEMENTS FOR TAKING PHOTOGRAPHS OR FOR PROJECTING OR VIEWING THEM; APPARATUS OR ARRANGEMENTS EMPLOYING ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ACCESSORIES THEREFOR
- G03B29/00—Combinations of cameras, projectors or photographic printing apparatus with non-photographic non-optical apparatus, e.g. clocks or weapons; Cameras having the shape of other objects
Abstract
The present application discloses systems and methods for a virtual input device. In one example, the virtual input device includes a projector and a camera. The projector projects a pattern onto a surface. The camera captures images that can be interpreted by a processor to determine actions. The projector may be mounted on an arm of a pair of eyeglasses and the camera may be mounted on an opposite arm of the eyeglasses. A pattern for a virtual input device can be projected onto a "display hand" of a user, and the camera may be able to detect when the user uses an opposite hand to select items of the virtual input device. In another example, the camera may detect when the display hand is moving and interpret display hand movements as inputs to the virtual input device, and/or realign the projection onto the moving display hand.
Description
The cross reference of related application
It is 13/181 that the application requires on July 12nd, 2011 that submit, sequence number, 238, exercise question is the U.S. Patent application of " Methods and Systems for a Virtual Input Device ",, sequence number that submit with on June 26th, 2012 is 13/533,120, exercise question is the right of priority of the U.S. Patent application of " Methods and Systems for a Virtual Input Device ", both are merged in herein by reference as carried out in this manual elaboration.
Background technology
Projected keyboard is a kind ofly can be projected to surperficial dummy keyboard, and the parts of keyboard detect that finger is mobile is also converted to the keystroke on equipment by described movement.Projected keyboard unit generally includes laser, for example, for visual dummy keyboard (is projected to surface, red diode laser as light source for the full-scale QWERTY layout keyboard of projection, be 295mm x95mm apart from 60mm place, projected keyboard unit projection size), and sensor or video camera are mobile with perception finger.The position of finger or the coordinate detecting can be used to judge action or the characteristic that will produce.
Projected keyboard also can be used second (invisible infrared) light beam projecting on dummy keyboard.In this example, when finger is on dummy keyboard when keystroke, finger cuts off infrared beam, and infrared light reflection returns video camera.The infrared beam of reflection can arrive video camera through Infrared filter, and video camera can be taken the angle of the infrared light of incident.Sensor can judge where infrared beam is cut off, and the coordinate detecting can be used to judge action or the characteristic that will produce.
Projected keyboard can comprise that use microcontroller is to receive the positional information corresponding to the reflected light from sensor or flash of light, and explains the event of the interface by suitable and external device communication.Event can comprise that keystroke, mouse move or touch pad control.
Summary of the invention
Except other things, the disclosure may be openly for the method and system of virtual input device.In one example, provide a kind of system for virtual input device, it comprises projector, video camera and processor.Projector is arranged to virtual input device is provided to surface, and virtual input device comprises the pattern of object.Video camera is arranged to the image that catches virtual input device, and the image of catch surface, and processor is arranged to the image of reception virtual input device and surperficial image, to judge surperficial apparent position.Processor can further be arranged to the apparent position based on surperficial, indicating projector is revised the projection of virtual input device, and in the time that processor judges that the image of virtual input device has the brightness higher than predetermined threshold, judge that virtual input device is used.
In another example, provide a kind of method of the operation for virtual input device.The method comprises virtual input device is provided to surface, and described virtual input device comprises the pattern of object.Described method also comprises the image that receives virtual input device, and judges surperficial apparent position based on this image.Described method further comprises the apparent position based on surperficial, revises the projection of virtual input device, and in the time that processor judges that the image of virtual input device has the brightness higher than predetermined threshold, judgement virtual input device is used.
In another example, a kind of goods that comprise tangible computer-readable medium are provided, on described medium, coding has computer-readable instruction.Computer-readable medium comprises instruction, and for virtual input device is provided to surface, virtual input device comprises the pattern of object.Computer-readable medium also comprises instruction, and for receiving the image of virtual input device, and instruction is for judging surperficial apparent position based on image.Computer-readable medium further comprises instruction, for the apparent position based on surperficial, revise the projection of virtual input device, and judge that virtual input device is used in the time that the image of processor judgement virtual input device has the brightness higher than predetermined threshold.
In another example, a kind of system is provided, it comprises that described virtual input device comprises the pattern of object for virtual input device being provided to lip-deep module.Described system also comprises the module of the image for catching virtual input device, for receiving the module of image, revising the module of virtual input device projection and judge the module that virtual input device is being used when judging that when processor the image of virtual input device has the brightness higher than predetermined threshold for the apparent position based on surperficial.
Aforementioned summary of the invention is only indicative, and is not intended to limit by any way.Except above-described indicative aspect, embodiment and feature, by reference to accompanying drawing and following embodiment, further aspect, embodiment and feature will become obvious.
Accompanying drawing explanation
In the accompanying drawings,
Fig. 1 shows the example system for transmitting and receive data;
Fig. 2 shows the example system for receiving, transmit and show data;
Fig. 3 A-3B shows the exemplary operations of the system of Fig. 2;
Fig. 4 shows the example that receives input at virtual input device;
Fig. 5 shows another example that receives input at virtual input device;
Fig. 6 be according at least some embodiment described herein for judging the block diagram of exemplary method of Object Selection of virtual input device;
Fig. 7 A-7C shows the exemplary in rotary moving of projection surface;
Fig. 8 A-8C shows the additional examples in rotary moving of projection surface;
Fig. 9 A-9C shows the exemplary inclination of projection surface and moves;
The additional exemplary that Figure 10 A-10C shows projection surface tilts mobile;
Figure 11 A-11C shows the exemplary movement of projection surface with the selection of denoted object;
Figure 12 is according to the block diagram of the exemplary method of the operation for virtual input device of at least some embodiment described herein;
Figure 13 is according to the block diagram of another exemplary method of the operation for virtual input device of at least some embodiment described herein;
Figure 14 is according to the block diagram of another exemplary method of the operation for virtual input device of at least some embodiment described herein;
Figure 15 shows the exemplary operations of system;
Figure 16 shows the functional block diagram of the example calculations equipment using in the computing system of arranging according at least some embodiment described herein; With
Figure 17 shows the schematic diagram of notional part viewpoint of exemplary computer program product, and described computer program comprises the computer program for object computer process on computing equipment.
All layouts are according at least some embodiment described herein.
Embodiment
In following embodiment, the accompanying drawing of a part that forms this embodiment is quoted.In the accompanying drawings, unless context is indicated in addition, otherwise similar symbol has identified similar parts conventionally.The illustrative embodiment describing in embodiment, accompanying drawing and claim is not in order to limit.Can use other embodiment, can carry out other and change, and not depart from the scope of the theme of displaying herein.Hold intelligible, as roughly describe herein and aspect of the present disclosure shown in accompanying drawing, can arrange, substitute, merge, separate and design with varied different configurations, all these are considered in this article clearly.
The especially open method and system for virtual input device of the disclosure.Say simply, in one example, described a kind of virtual input device (or keyboard), it comprises laser-projector and video camera.Projector projects a laser onto on surface.Can, with any amount of pattern or graphic user interface (GUI) projection laser, comprise keyboard.Video camera can catch image, and it can be explained that by processor (for example, using computer vision technique) is to judge the button of for example keyboard.
In one example, laser-projector can be loaded on a handel of a pair of glasses, and video camera can be loaded on the relative handel of described glasses.Processor can further be loaded on glasses and be coupled to laser-projector and/or video camera.Laser pattern for virtual input device can be projected to user on hand, and user can use relative picking to select the project of virtual input device.For example, it is upper that laser pattern can be projected to " demonstration hand ", and video camera can detect the phase opponent when user use phase opponent and laser hits doing to select " action finger ".
In another example, processor can detect when show that hand is moving and explaining that demonstration hand moves, as the input to virtual input device.For example, in the time showing that hand tilts, laser pattern changes, and the change of laser pattern can be by identification.When showing that hand is while tilting away from user, little at the image of the antermarginal laser pattern of hand at the image ratio of the laser pattern of the distal edge of hand.Similar, if show that hand-screw turns, the object that the edge of the hand of close projector shows so is larger, and the object showing further from the edge of projector is less.The movement (for example tilt and rotate) that shows hand can be determined and be accepted as user's selection.In one example, the movement of demonstration hand can be interpreted as similar in appearance to the user's input receiving from mouse on standard computer.It is mobile that rotation can be interpreted as level (x axle), and it is mobile to tilt can be interpreted as vertical (y axle).
In another example, when processor can move by detection display hand, and can indicate laser-projector based on the mobile projection of adjusting virtual input device, to follow the trail of mobile demonstration hand.Processor can be further configured to: the brightness of image of judging virtual input device when processor during higher than predetermined threshold value, judges that virtual input device is used.
Refer now to Fig. 1, show the example system 100 for transmitting and receive data.System 100 comprise use communication link 104(for example wired or wireless connections) equipment 102 of communicating by letter with main frame 106.
Equipment 102 can be the equipment that can receive data and show corresponding to or be associated with any type of the information of described data.For example, equipment 102 can be head-up-display system, the near-to-eye unit that comprises display device (for example, the helmet, contact lenses and safety goggles) of for example glasses or any type.Further example embodiment for example can comprise head-mounted display (HMD), comprises the display of liquid crystal display (LCD), light emitting diode (LED), molded polymeric display or free space reflective display or other modes that image produces.Extra example embodiment can be included in display for produce the waveguide of image before user, or comprises the wearable computer with the HMD of eyes display or simple eye display.
Therefore, equipment 102 can comprise display system 108, and it comprises processor 110 and display 112.Display 112 can be that for example, optics penetration indicator, optics are looked about display or video penetration indicator.Display system 108 can be arranged to main frame 106 and communicate by letter (for example, receive and transmit data) wired or wirelessly.Processor 110 can receive the data from main frame 106, and configuration data is for showing at display 112.Processor 110 can be the processor of any type, such as microprocessor, digital signal processor (DSP) etc., and display 112 can be the display of any type, comprises LCD, plasma etc.
In addition, equipment 102 can comprise the one or more sensors that are coupled to processor 110, for example sensor 114.Sensor 114 can be the movement with measuring equipment 102 of gyroscope or accelerometer.Sensor 114 may further include with lower any one: for example, except other, GPS (GPS) receiver, infrared sensor, optical sensor, biology sensor, radio-frequency (RF) identification (RFID) system, wireless senser and/or compass.Sensor 114 also can adopt the form of video camera, and the movement of equipment can for example be judged (for example, being used in the spectral discrimination action from video camera) based on light stream or computer vision.
In addition, equipment 102 can comprise integrated user interface (UI), and its permission user and equipment 102 are mutual.For example, equipment 102 can comprise various buttons and/or touch screen interface, and it allows user that input is provided.As another example, equipment 102 can comprise and is arranged to the microphone receiving from user's voice indication.In addition, equipment 102 can comprise one or more interfaces, and it allows various types of user-interface equipments to be connected to equipment 102.
Equipment 102 may further include on-board data storage, for example, be coupled to the storer 116 of processor 110.Storer 116 can be stored the software that for example can be accessed and be carried out by processor 110.
In Fig. 1, the communication link 104 as wireless connections is shown; But, also can use wired connection.For example, communication link 104 can be via the universal serial bus of for example USB or the wire link of parallel bus.Wired connection can be also proprietary connection.Communication link 104 can be also wireless connections, and for example bluetooth, IEEE802.11(IEEE802.11 can refer to IEEE802.11-2007, IEEE802.11n-2009 or any other IEEE802.11 revision) or other communication links based on wireless.
In another example, system 100 comprises access point 118, and equipment 102 can be communicated by letter with the Internet 120 by described access point 118.In this example, equipment 102 may not require and is connected to main frame 106.Access point 118 can adopt various forms.For example, if equipment 102 use 802.11 or connect via Ethernet, access point 118 can adopt the form of WAP (WAP) or wireless router.As another example, if equipment 102 utilizes cellular air-interface agreement, (for example CDMA or gsm protocol) connects, and access point 118 can be the base station that Internet connection is provided via cellular network in cellular network.
Equally, equipment 102 can comprise wired or radio network interface, and equipment 102 can be connected to access point 118 by described network interface.As example, equipment 102 can be arranged to and use one or more agreements to be connected to access point 118, described agreement for example 802.11,802.16(WiMAX), LTE, GSM, GPRS, CDMA, EV-DO and/or HSPDA.In addition, equipment 102 can be arranged to and use multiple wired and/or wireless protocols to be connected to access point 118, described agreement is for example used " 3G " or " 4G " data of cellular communication protocol (for example, CDMA, GSM or WiMAX are connected with " WiFi " that use 802.11) to connect.Other examples are also possible.
Or main frame 106 also can be included in the connection of the Internet 120, therefore, equipment 102 can pass through main frame 106 access the Internet 120.
The function of one or more descriptions of system 100 can be divided into extra function or physical unit, or integrates with function or physical unit still less.For example, although sensor 114 is shown as the parts of equipment 102, sensor 114 can separate with equipment 102, therefore, can communicate by letter with equipment 102.In other example, extra function and/or physical unit can be added to by the example shown in Fig. 1.
Fig. 2 shows the example system 200 for receiving, transmit and show data.System 200 is shown with the form of wearable computing equipment, for example, is comprised the HMD of a pair of glasses.In one example, system 200 can be arranged to the equipment 104 as shown in Fig. 1 and operate, or comprises any function of the equipment 104 shown in Fig. 1.
Shown video camera 208 is loaded on spectacle-frame.Video camera 208 also can be loaded in other positions.Video camera 208 for example can be arranged to, with various resolution and/or different frame per second (, different frame (fps) per second) and catch image (for example video).Many video cameras with little form factor (for example those cell phone, the first-class middle use of network shooting) are known by those skilled in the art, and can be merged in the example of system 200.But should be appreciated that example described herein is not limited to the video camera of any particular type.Although Fig. 2 shows a video camera 208, can use more video camera, and each can being arranged to catch the identical visual field or catch different visual angles or the visual field, and therefore can be loaded in other regions of a pair of glasses.
In one example, display 202 is positioned and sizing, with the image that makes showing seem to cover or " floating " on the user visual field of physical world, therefore provide so a kind of experience: the information of its Computer generation can merge mutually with the user awareness of physical world.For this reason, airborne computing system 204 can be arranged to analyzes the video segment that caught by video camera 208, should show what figure to judge, and display graphics (for example, the size of the position on display, figure etc.) how.
Shown laser-projector 210 is loaded in the temples relative with video camera 208.But laser-projector 210 can be loaded on the arm identical with video camera 208, or in other regions of a pair of glasses.Laser-projector 210 can be directed so that projected image in the direction of seeing user.Laser-projector 210 can be arranged to projected image (for example virtual input device) to surface.In one example, virtual input device can comprise the laser pattern of the object that has adopted qwerty keyboard form.The laser pattern of object also can comprise many other types or the configuration of the application of dependence system 200.
Laser-projector 210 can provide visual pattern, or alternately, laser-projector 210 can comprise that infrared (IR) projector is to provide the combination of IR or laser pattern and IR.In one embodiment, in the time using IR projector, in Fig. 3 B, in HUD (the following describes), can see image.
Laser-projector 210 can provide the pattern being produced by laser, and for example, laser to create the grid of point or line, causes laser pattern through diffraction pattern generator.
In addition, laser-projector 210 can be arranged in the direction of any requirement projection is provided.For example, laser-projector 210 can comprise mirror, mirror can be arranged at the direction reflective projection requiring, or laser-projector 210 can comprise that movably base for example, to change the direction (, motored or not motored base) of projection.
Further, laser-projector 210 can be or comprise the projector display (for example, laser-projector, LCD are backlight, liquid crystal on silicon (LCOS) etc.) of common any type, and the projector that comprises laser is an example.Other examples are also possible.
The sensor 212 illustrating is loaded on the handel of a pair of glasses, but sensor 212 can be positioned at other regions of system 200.For example, sensor 212 can comprise one or more gyroscopes or accelerometer.Other sensing equipments can be included in sensor 212, or other sensing functions can be carried out by sensor 212.In addition, system 200 can comprise extra sensor, or extra input equipment, such as track pad input equipment, for button of inputting etc.
Fig. 3 A shows the exemplary operations of the system 200 of Fig. 2.In one example, user can put on the system 200 as a pair of glasses, and laser-projector 210 can be arranged to the projection of the laser pattern that object 300 is provided.The direction (for example, the direction that user is seeing) of watching attentively based on user, user can guide the projection of the laser pattern that where shows object 300.In the example of showing at Fig. 3 A, user is seeing hand 302, and therefore the laser pattern of object 300 can be projected in one's hands 302.In this example, hand 302 can be considered to show hand, and the laser projection of object 300 is numeric keypads.But the laser pattern of object 300 can be projected on any surface, and can comprise any amount of object, the content of object can configure as requested.
Fig. 3 B shows another exemplary operations of system of Fig. 2.In this example, can in the HUD of glasses 304, check projection.HUD 304 can be arranged in the time that user checks figure, and alignment patterns is to cover in one's hands 302.In this example, can not have virtual input device to project to surface.
Although the system 200 of showing in Fig. 3 A-3B is presented as a pair of glasses, the parts of system can separate from glasses.For example, video camera 208 and/or projector 210 can separate from glasses, and can be attached to other regions of user, comprise neck, chest, wrist or waistband, and are arranged at projected image with from surface seizure image.As specific example, video camera and projector can be attached to user's waistband, and directed projection laser pattern is to floor or user's pin, rather than project to user on hand or except project to user on hand.Further, comprise that the parts of the system 200 of video camera 208 and projector 210 can remove from glasses, and can be placed in for example user's waistband, or be customized to the direction operation being arranged in any requirement.Further, system can comprise multiple video cameras and/or multiple projector with projection pattern and catch the image in multiple regions.
In one example, in the time that the laser pattern of object 300 is projected to demonstration hand 302, video camera 208 can be taken the image of hand 302 and the laser pattern of object 300.In one example, in the time that the laser pattern of object 300 comprises virtual input device, user can provide input to virtual input device with phase opponent.Fig. 4 shows the example that receives input on virtual input device.For example, in Fig. 4, the laser pattern of object 300 is for example projected 302(in one's hands, in one's hands 302 palm), user can provide input by an object of the laser pattern of alternative 300 with phase opponent.
When user's moveable finger 304(for example, action finger) when laser pattern by object 300, video camera 208 can be taken around the discontinuous laser rays of action finger 304 bendings.Change in laser rays can be used to position or the apparent position of acts of determination finger 304.For known laser pattern, for example, one group is shown as 10 icons of button, and which button the processor of the airborne computer of system 200 can detect in the laser pattern of image 300 and be moved finger 304 distortions, has selected which button to judge.After having judged and selecting which object, the airborne computer of system 200 can guide laser-projector to change selected object, for example, by changing the color of object.
Although Fig. 4 shows the laser pattern of the object 300 of projection in one's hands 302, but the laser pattern of object 300 (for example can be projected to any region or surface, except hand), and user can make a choice by any project that can distortion laser pattern on virtual input device.
Fig. 5 shows another example that receives input on virtual input device.In Fig. 5, the laser pattern of object 308 is projected to user's forearm 306, and user can use phase opponent to provide input by an object of the laser pattern of alternative 308.In this example, the laser pattern of object 308 comprises four buttons that are numbered to four, and the second button changes color after user selects.
Fig. 6 be according at least some embodiment described herein for judging the block diagram of exemplary method of selection of object of virtual input device.The method 600 in Fig. 6 of being illustrated in has represented the embodiment of the method that for example can use together with 200 with system 100 and can for example, be carried out by equipment (helmet or part of appliance).Method 600 can comprise one or more operations, function or the action as shown in one or more frame 602-612.Although frame illustrates with sequential order, that these frames also can walk abreast and/or carry out from those different orders described here.Equally, various frame can be based on required implementation merged enter frame still less, be divided into extra frame and/or be removed.
In addition,, for method 600 described herein and other processes and method, process flow diagram has been shown function and the operation of a possible realization of the present embodiment.In this, each frame can represent a part for module, section or program code, and it comprises and can carry out to realize specific logical function in processing or one or more instructions of step by processor.Program code can be stored on the computer-readable medium of any type, for example, comprises the memory device of disk or hard disk drive.Computer-readable medium can comprise non-transient computer-readable medium, for example, for the computer-readable medium of short-term storage data, as register memory, processor cache and random-access memory (ram).Computer-readable medium also can comprise non-transient medium, for example secondary or continuation longer-term storage, for example, as ROM (read-only memory) (ROM), CD or disk, compact disc read-only memory (CD-ROM).Computer-readable medium can be also any other volatile or Nonvolatile memory system.Computer-readable medium can be considered to for example computer-readable recording medium, or tangible memory device.
In addition,, for method 600 and other processes disclosed herein and method, the each frame in Fig. 6 can represent the circuit with the specific logical function in implementation by line.
At frame 602, method 600 comprise determine whether projection virtual input device and/or catch image.Method 600 can for example be carried out by the system 200 in Fig. 2, and wherein laser-projector and video camera are loaded on glasses.Glasses may further include gyroscope and move for perception head.When user head be stable or roughly stable (for example, fixing or roughly fixing) time, laser-projector can start projection virtual input device (for example, the laser pattern of object) and video camera can be arranged to the image that starts to catch virtual input device.Can judge whether user's head is stable based on gyrostatic output, for example, when output is during within the acceptable range or lower than predetermined threshold value.Once judge that user's head is stable, video camera can start to catch image (or video) to catch the input of user at virtual input device so.By this way, user can indication mechanism: fix or roughly fixing by the head that keeps them, user is ready to start with virtual input device mutual.
As an example, if no longer comprise the image recognizable image of the laser pattern of object (or no longer comprise) of the laser pattern of object from the image of video camera, laser-projector can stop projection virtual input device and video camera can stop catching image.In this example, the projection line that laser-projector may be shifted out in the surface at the laser pattern place of object (for example, user's mobile display hand is outside the projection line of laser pattern), and the processor of processing the pattern that captures can judge that user does not re-use virtual input device.From another viewpoint, user can move his/her head, so that the projection line of laser pattern is no longer stayed surface, described surface can for example provide background, so that the recognizable image of laser pattern (, brightness/resolution is higher than the image of threshold level) can be explained for laser pattern.For example, wearing has the user of the system 200 in Fig. 2 to see in a downward direction, so that the laser pattern of object is projected to user on hand, and after using virtual input device, user then can be upwards or eyes front, thereby the laser pattern of object is projected to direction forward, and less than any specific surface.
At frame 604, method 600 comprises the information joining with the image correlation of background that receives.For example, video camera can catch the image that is projected to lip-deep virtual input device.Image can comprise virtual input device, the laser pattern of for example object.At frame 604, laser-projector can user provide be input to virtual input device before carry out calibration.For example, for initialization virtual input device, it is movable with indicator diagram now that laser-projector can provide the laser pattern (for example, the laser pattern of opening/closing projection) of flicker.When perception or identification flicker laser pattern (based on the image capturing) time, video camera can be carried out calibration by catching the image of laser pattern without user interactions, and it can be used as reference picture (and can be stored in storer) to judge that when user and laser pattern are mutual.For example, can catch with pattern with without the image of pattern, and, can judge that the difference of pattern is to produce image.Image based on pattern self, for example, can regulate the brightness of pattern by revising the power of projector.
As another example, for initialization virtual input device, can send ring or voice and notice.Further, for initialization virtual input device, can provide the notice of any type or form.
At frame 606, method 600 comprises the information being associated with virtual input device that receives.For example, video camera can catch user and the mutual image of virtual input device.Image can comprise object, for example, and the action finger of the line of the laser pattern of interrupt object.In this example, user can pass the line of laser pattern by moveable finger, to indicate an object selecting laser pattern.In the time of the mobile object with indication selection laser pattern of finger, video camera can catch the image of finger.Then processor can receive the information being associated with virtual image equipment.Information can be the image (for example, video) being caught by video camera.
At frame 608, method 600 comprises the feature of extracting image.For example, processor can have from image data extraction the feature of various rank complexities.The example of feature of extracting for example comprises: line, edge, ridge, the partial interest point of for example angle or point, or for example with texture, shape or the relevant complex characteristic of moving.
In one example, image can represent the real-world scene of being checked by video camera, and it comprises static background and transportable various foreground object, and processor can extract the feature of indication foreground object.For example, background object can comprise the laser pattern of projection objects, and foreground object can comprise the finger of the user on an object that is placed in laser pattern.
At frame 610, method 600 comprises the characteristics of image relatively extracting.In one example, the characteristics of image that processor can relatively extract, with the difference between recognisable image.By this way, processor can identification be pointed the movement on the object of laser pattern, so which object identification has selected.
In another example, processor can be accessed the reference background image model in storer, and it can be recorded and store during system calibration.Background image model can representative object the estimation of laser pattern, for example, if corresponding to there is no foreground object in image, what camera review will look like.Foreground object can be supposed different from background image in brightness.Therefore, can create difference image (difference image) with identification foreground object in each time step.Difference image can be the result from background model figure image subtraction camera review.For the each pixel at difference image, if the absolute value of difference is greater than specific threshold value, that pixel can be classified as prospect so; Otherwise that pixel can be classified as background.This difference threshold value may be according to the brightness difference of background.As another example, if being greater than specific threshold value, the absolute value of difference reaches time of length-specific, pixel can be classified as prospect so; Otherwise that pixel can be classified as background.
Therefore, processor can be based on come processing feature or image by pixel, for example, to judge that what part of feature or image is that what part of prospect (, representing the object of the object of the laser pattern of alternative) and feature or image is background.Processor can be distinguished the change in feature and image.
At frame 612, method 600 comprises the selection of the object of judging virtual input device.For example, by reference to being stored in the background image in storer and judging the position of foreground object, processor can judge user has selected which object of the laser pattern of object.Processor can reference stores background image and know the position of the object of the laser pattern of object.For example, if the laser pattern of object is the expression of numeric keypad, processor can be known the position/location of each object of representative digit key.Then processor can judge the position of foreground object, and that position and the known location of object in laser pattern are compared.When location matches (or approximate match or covering), processor can judge that foreground object covers the object in laser pattern.Covering object in laser pattern can be judged as selected object.
In one example, the method 600 in Fig. 6 is examples of a computer vision.Receive the input picture from video camera, and computer vision system can (for example separate interested foreground object in real time from background image, user's finger) so that the position of foreground object and profile can be used as designator, for the input to virtual input device.
In further example, refer back to Fig. 4, the laser projection of object 300 is projected on surface, the for example demonstration hand in Fig. 4, for example, and the parts of system (, processor and video camera) can be arranged to and judge to show when hand 302 moves, and explain the mobile as the input to virtual input device of demonstration hand.
The example that Fig. 7-10 show projection surface moves.For example, when showing that hand 302 is while tilting, the demonstration of the laser projection of object 302 changes, and change in the demonstration of the laser projection of object 302 can be by identification.When showing that hand 302 is while tilting away from user, little at the image of the antermarginal laser pattern of hand at the image ratio of the laser pattern of the distal edge of hand.Similar, if show that hand rotates towards user, the object that the edge of the hand of the most close so projector shows is larger, and the object showing further from the edge of projector is less.The movement (for example tilt and rotate) that shows hand can be determined and be accepted as user's selection.In one example, the movement of demonstration hand can be interpreted as being similar to the user's input receiving from mouse on standard computer.It is mobile that rotation can be interpreted as level (x axle), and it is mobile to tilt can be interpreted as vertical (y axle).
In Fig. 7 A-7C, show the exemplary in rotary moving of projection surface.In Fig. 7 A, show the demonstration hand 302 of the laser projection with object 300 being provided by example system.Also shown direction and the orientation of x-y axle for the movement with reference to about demonstration hand 302, and x-y axle can be the axis of reference from syetematic view.In Fig. 7 A, show that hand 302 is shown as fixing, central object (for example, frame 5) is highlighted displaying.In one example, the parts of system can be arranged to object-based position to give prominence to central object.As Fig. 7 A shows, central frame 5 is in the position (0,0) of x-y axle to be located, and therefore, is in the middle position of systematic point of view.
In Fig. 7 B-7C, show that hand 302 is around y axle rotation, so that the edge 310 that shows hand 302 is towards user, and show that the thumb of hand 302 rotates away from user.By this way, because lacking the mobile laser projection that causes object 300, projector keeps fixing (or roughly fixing), the right row of the laser projection of object 300 (for example, digital 3,6 and 9) be projected to the edge 310 that shows hand 302, and due to the change of projection surface, it seems larger than any remaining object dimensionally.In the time judging demonstration hand 302 for example, by rotation by this way (, using any above-mentioned method in Fig. 6), the parts of system can be arranged to the outstanding frame (for example, frame 6) that is in now middle position.Fig. 7 C shows outstanding central frame.
In Fig. 8 A-8C, show the additional examples in rotary moving of projection surface.In Fig. 8 A, show the demonstration hand 302 with the laser projection of the object 300 being provided by example system.In Fig. 8 B-8C, show that hand 302 is around y axle rotation, so that the inward flange 312 that shows hand 302 is towards user, and show that the edge 310 of hand 302 is rotated away from user.By this way, owing to lacking the movement of projector, the laser projection of object 300 keeps fixing (or roughly fixing), the left column of the laser projection of object 300 (for example, digital 1,4 and 7) be projected to the inward flange 312 that shows hand 302, and due to the change of projection surface, it seems larger than any remaining object dimensionally.In the time judging demonstration hand 302 for example, by rotation by this way (, using any said method in Fig. 6), the parts of system can be arranged to the outstanding frame (for example, frame 4) in middle position now.Fig. 8 C shows outstanding central frame.
It is in rotary moving that Fig. 7-8 show the example of projection surface.For example, in rotary movingly can be interpreted as moving horizontally of selector switch, it is shown as outstanding object.
In Fig. 9 A-9C, the example that shows projection surface tilts mobile.In Fig. 9 B-9C, show that hand 302 tilts around x axle, to show that the heel 314 of hand 302 tilts away from user towards the finger of user and demonstration hand 302.By this way, owing to lacking the movement of projector, the laser projection of object 300 keeps fixing (or roughly fixing), the bottom row of the laser projection of object 300 (for example, digital 3,6 and 9) be projected to the heel 314 that shows hand 302, and due to the change of projection surface, it seems larger than any remaining object dimensionally.In the time judging demonstration hand 302 for example, by rotation by this way (, using any above-mentioned method in Fig. 6), the parts of system can be arranged to the outstanding frame (for example, frame 8) in middle position now.Fig. 9 C shows outstanding central frame.
In Figure 10 A-10C, the extra example that shows projection surface tilts mobile.In Figure 10 B-10C, show that hand 302 tilts around x axle, to show that the finger tip 316 of hand 302 is towards user, and show that the heel 314 of hand 302 is tilted away from user.By this way, owing to lacking the movement of projector, the laser projection of object 300 keeps fixing (or roughly fixing), the top row of the laser projection of object 300 (for example, digital 3,6 and 9) be projected to the finger tip 316 that shows hand 302, and due to the change of projection surface, it seems larger than any remaining object dimensionally.In the time judging demonstration hand 302 for example, by rotation by this way (, using any above-mentioned method in Fig. 6), the parts of system can be arranged to the outstanding frame (for example, frame 2) in middle position now.Figure 10 C shows outstanding central frame.
It is in rotary moving that Fig. 9-10 show the example of projection surface.For example, the vertical movement that can be interpreted as selector switch in rotary moving, it is shown as outstanding object.
Example rotation and the inclination movement in Fig. 7-10, shown can be used in combination.For example, for example, for mobile projector surface makes lower right corner object (, frame 9) in middle position, after having carried out show as Fig. 7 A-7C in rotary moving, the inclination that user can carry out is as shown in Figure 9 A-9C moved.In this example, can carry out in rotary moving and in rotation, can carry out mobile.Can carry out rotation and any other the mobile example combination that tilts, to make any object of laser projection of object 300 in middle position.
As example, laser can be projected to the 3x3 grid of lip-deep 9 buttons of showing as Fig. 7-10.Start most, can give prominence to centre button or indicate (for example,, by using pointer or other graphic icons) by selector switch.By surface of revolution, the processor of system can be configured to judge and have the button larger than outstanding button, and can indicate laser-projector outstanding described in larger button.Similar, show that by inclination hand is away from user, processor is can another button of identification larger, and can indicate laser-projector to give prominence to this larger button.For example, rotation and inclination movement can be interpreted as being similar to the movement of mouse on computing machine.
In one example, user can move by another that make projection surface, selects outstanding object.The example that Figure 11 A-11C shows projection surface moves, with the selection of denoted object.In Figure 11 A, to show and shown the laser projection of hand 302 with object 300, the central object (for example, frame 5) in laser projection is highlighted.In Figure 11 B, select outstanding object in order to indicate, user can be around z axle rotational display hand 302.For example, in the time judging rotation by this way of demonstration hand 302 (, using above-mentioned any method in Fig. 6), the parts of system can be arranged to the selection that receives the outstanding object of described mobile conduct.Show that hand 302 can be interpreted as mouse around the rotation of z axle and click.As another example, as shown in Figure 11 C, user can carry out another hand and move, for example, make fist by closing demonstration hand 302.Fist gesture can be interpreted as mouse and click.The gesture of any type can be interpreted as mouse click, for example make predetermined gesture, lift certain quantity finger, thumb up, handspring come, shake the hand back and forth, pushed or for example any subscriber customized gesture.
Use and move in the example shown in Fig. 7-11, user can be only with showing that hand 302 is provided to the input of virtual input device.In other examples, user can use (independently or side by side) and show that hand and action finger are to provide input.
Figure 12 is according to the block diagram of the exemplary method of the operation for virtual input device of at least some embodiment described herein.The method 1200 of showing at Figure 12 represents the embodiment of this method: for example described method can be used with for example system 100 together with 200, and can be carried out by the equipment of for example helmet or part of appliance.Method 1200 also can together be used with for example method 600.Method 1200 comprises one or more operations, function or the action shown in one or more frame 1202-1208.Although show frame with sequential order, these frames also can be carried out concurrently, and/or with from those different orders described here.Equally, various frames can be incorporated into frame still less, are divided into extra frame, and/or realization based on requiring and being removed.
At frame 1202, method 1200 comprises virtual input device is provided to surface.As an example, virtual input device comprises the laser pattern of object, for example, in Fig. 7-11, show and any laser projection of the object described.The object of any quantity, type, size and configuration can be projected to surface.In addition, surface can be the surface that can show any type of laser projection.
At frame 1204, method 1200 comprises the image that receives virtual input device.As an example, video camera can be arranged to catch and be projected to the image of surperficial virtual input device, and can be arranged to and provide image to processor.
At frame 1206, method 1200 comprises based on image judges surperficial movement.For example, can judge surperficial movement by any method that is included in the method 600 of describing in Fig. 6.The movement of image with identification surface can be compared to each other.
At frame 1208, method 1200 comprises that at least partly movement based on surperficial judges the selection of the object of virtual input device.For example, as shown in Fig. 7-11 and describing, the movement on surface causes the change of the demonstration of the object of virtual input device, and based on this change, can judge the selection of object.Example in object changes and comprises: show due to surface move and compared with other objects larger object.
Figure 13 is according to the block diagram of another exemplary method of the operation for virtual input device of at least some embodiment described herein.The method 1300 of showing in Figure 13 represents the embodiment of such method: for example described method can be used with system 100 together with 200, and can be carried out by the equipment of for example helmet or part of appliance.Method 1300 also can together be used with method 600 or 1200, and for example, method 1300 can comprise one or more operations as shown in one or more frame 1302-1308, function or action.Although frame illustrates with sequential order, these frames also can executed in parallel, and/or with from those different orders described herein.Equally, various frames can be integrated into less frame, be divided into extra frame, and/or realization based on requiring and being removed.
At frame 1302, method 1300 comprises provides virtual input device.As an example, virtual input device comprises the laser pattern of object, for example, in Fig. 7-11, show and any laser projection of the object described.The object of any quantity, kind, size and configuration can be projected to surface.In addition, virtual input device can be provided to surface, and surface can be the surface that shows any type of laser projection in the above.
At frame 1304, method 1300 comprises the image that receives virtual input device.As an example, video camera can be arranged to catch and project to the image of surperficial virtual input device, and can be arranged to and provide image to processor.
At frame 1306, method 1300 comprises the distortion of judging object based on image.For example, the image that can be compared to each other is with the distortion of identification objects, or any change in the laser pattern of object.In one example, the distortion object due to surperficial movement, this can use any method that is included in the method 600 of describing in Fig. 6 to judge.Based on surperficial movement, compare with the object view before apparent motion, now object can seem more greatly, less, distortion, bending, stretch etc.
At frame 1308, method 1300 comprises: at least part of object-based distortion, the Object Selection of judgement virtual input device.For example, as shown in Fig. 7-11 and describing, the movement on surface causes the change in the demonstration of the object of virtual input device, and based on this change, can judge the selection of object.Example change in object comprises and showing due to the surperficial motion object larger than other objects.For example, processor can be arranged to the distortion of Interpretive object, as that Object Selection.
Figure 14 is according to the block diagram of another exemplary method of the operation for virtual input device of at least some embodiment described herein.The method 1400 of showing in Figure 14 represents the embodiment of such method: for example, described method can be used with system 100 together with 200, and can be carried out by the equipment of for example helmet or part of appliance.Method 1400 also can together be used with method 600,1200 or 1300.Method 1400 can comprise one or more operations as shown in one or more frame 1402-1414, function or action.Although frame illustrates with sequential order, these frames also can executed in parallel, and/or with from those different orders described herein.Equally, various frames can be integrated into less frame, be divided into extra frame, and/or realization based on requiring and being removed.
At frame 1402, method 1400 comprises provides virtual input device.As an example, virtual input device comprises the laser pattern of object, for example, in Fig. 7-11, show and any laser projection of the object described.The object of any quantity, kind, size and configuration can be projected to surface.In addition, virtual input device can be provided to surface, and surface can be the surface that can show any type of laser projection in the above.In addition, start most, for example can be in region rather than the specific virtual input device that provides from the teeth outwards.
At frame 1404, method 1400 comprises reception virtual input device and surperficial image.As an example, video camera can be arranged to catch and project to the image of surperficial virtual input device, and can be arranged to and provide image to processor.In addition, video camera can be arranged to catch to be had or there is no virtual input device projection surperficial image thereon, and provides image to processor.
At frame 1406, method 1400 comprises judges surperficial apparent position.For example, the image that processor can receiving surface, and can be based on image relatively judge surperficial apparent position.Especially, processor can use computer vision to follow the trail of surperficial motion and to judge position.As another example of following the trail of surperficial motion, in the time that surface moves through the image of projection, it is brighter than the light from other regions receiving at video camera place that the light from projected image receiving at video camera place becomes.Therefore, processor can judge with projection in the bright area of light source of the pattern match that produces, with positioning projection.Projection can comprise distinguishing pattern, once pattern multilated (for example, by the surface of passing in pattern), the brightness that processor will process decision chart picture.By movement images, processor can be judged surperficial position.For example, projector can provide broad projection to initially begin locating surface, or initial alignment comprises surperficial region.
Position can be to be different from surperficial accurate location or the apparent position except described accurate location.Apparent position can be the surperficial position in threshold value permission, for example about 1 foot to 3 feet, approximately 1cm to 3cm, about 1mm to 3mm, about 1 μ m in 3 μ m, or for surperficial size with from system to other suitable scopes of surperficial distance.
At frame 1408, method 1400 comprises provides virtual input device from the teeth outwards.For example, once determine surperficial position, projector just can be revised the projection of virtual input device from the teeth outwards.Start most, projection can be wide pattern to cover bulk zone, once and determine the surperficial position that provides projection, projection just can narrow to provide more details or higher image resolution ratio.For example, if surface is user's hand,, in the time that hand passes pattern, the upset in pattern is determined to, and can judge the position of hand.As another example, processor can be arranged to judges surperficial region, and virtual input device is provided to surperficial region by indicating projector, for example, by widening or narrowing projection to adapt to Free Region.Therefore, projection can start in the first overlay area most, then can be modified in the second overlay area, and this second overlay area is less than or is greater than the first overlay area (for example, start wide and reduce, or start narrow and increase).
At frame 1410, method 1400 comprises follows the trail of surperficial motion.As an example, if surface is user's hand, if hand is mobile after virtual input device is provided on hand, the part of virtual input device is not recognizable (for example, video camera can not receive enough light from those images) for user or video camera.Therefore, processor can use the method for the position on above-mentioned judgement surface to follow the trail of surperficial motion.As an example, can analyze the image catching by video camera to judge surperficial motion.
At frame 1412, method 1400 comprises that virtual input device is provided to surface by the motion based on surperficial.For example, in the time of surface modification position, processor can indicating projector changes the projecting direction of virtual input device, to roughly virtual input device is provided to surface when surface is in motion.In addition, based drive speed, for example, if be not easy to find surperficial position (, surface integral moves out the visual field of projection), can manner of execution 1400 with the reposition on identification surface.By this way, computer vision is used in when move on surface the projection of virtual input device is rearranged to surface.
At frame 1414, when method 1400 comprises the brightness of image of judging virtual input device when processor higher than predetermined threshold value, judge that virtual input device is used.For example, start most, when projection (and on not specific to surface or to surface at a distance) is provided in region, reflex to video camera and the light that catches will have certain gray scale in image.Once projection is provided to the surface near video camera, be reflected to video camera and the light that catches will have higher gray scale in image.Once image has the gray scale higher than predetermined threshold value, processor just can judge that virtual input device is used.When virtual image equipment is projected to while being in apart from projector lower than the distance of predetermined threshold value surperficial, image can have the gray scale higher than predetermined threshold.Therefore,, in the time that projector is provided near user surperficial by virtual input device, processor can judge that virtual input device is used.In one example, judge not (for example, based on the gray scale receiving) in use time of virtual input device when processor, processor can cut out projector, video camera or other system parts.If virtual input device is not projected to the surface in the distance lower than distance threshold, can receive so gray scale lower than threshold value (for example, user's hand not in domain of the existence using as showing manual manipulation).
The example of above-mentioned predetermined threshold can be specific with respect to other pixels in image on gray scale.For example, for example, in the time having exceeded threshold value (, for judging that gray scale is higher than predetermined threshold), described threshold value can be indicated surperficial the existing (for example, the existence of hand) of projection virtual input device on it.For example, when hand is when (, being produced by laser pattern generator) projection pattern, corresponding to the pixel of laser spots can be saturated (for example, red/green/blue pixel value is 255,255,255, even if laser spots trends towards redness; And residual pixel mean value in image can be approximately 128,128,128).Therefore,, when the pixel value in the region of image is during higher than the mean value of the mean value of pixel value or the pixel value of image, first threshold can be satisfied.
When hand is not when intercepting laser pattern, pattern is for example projected on floor on the contrary, and red/green/blue value of laser spots trends towards on average red/green/indigo plant value of the pixel in camera review.In the situation that surface is hand, relatively better (in the sense, the similar Lambert surface of skin, its incident light is uniformly to all directions reflections) of skin reflex laser pattern.But other surfaces (for example floor) trend towards darker (for example, office's carpet) or minute surface and provide other angles for example, with reflected light (, polished bricks floor).In addition, the surface except user's hand can be farther apart from video camera, and therefore the light in pattern also can not reflect.Therefore, when hand is not when intercepting laser pattern, corresponding to these more at a distance the pixel of the point in the projection pattern on surface can in image, more trend towards average red/green/blue value.
Or, exist other mode to make processor can judge that virtual input device is used.For example, the pattern of projection can be aimed at, thereby is projected to nearly surface rather than in the time that projection pattern is projected at a distance surperficial, the pattern of projection can be checked by video camera when pattern.Especially, with reference to figure 3A, projector 210 can be loaded in the right arm of glasses, and can be positioned as it is arranged to for example, towards (, providing projection) or to user's left hand projection.Video camera 208 can be loaded on the left side arm of glasses, and can be positioned as and it is arranged to towards user's the right hand aim at.For example, because the visual field of video camera 208 is limited (, 60 degree) and projection pattern (for example there is limited vision arc, 10 degree), so video camera 208 may be can't see pattern, unless there is the surface for projection pattern (being Fig. 3 A user's left hand or a piece of paper that left hand is held) near video camera 208 relatively.Image distance the right that surface too far away may cause image to be seen with respect to video camera 208 is too far away.
Use the example of showing in Fig. 3 A, the central ray of video camera 208 and projector 210 can mutually intersect aiming before user.Therefore,, at user's an about arm length place, can there is the overlay area of ray.Use this method, arbitrary hand (or object) can be for interface, and can before user's health, be similar to medially and place.Any object in this overlay area can not seen by video camera 208, or does not have the pattern projecting on object.As the configuration assistant of video camera 208 in this example and projector 210 is avoided Interpretive object or systemic-function mistakenly.
In other examples, projector 210 can be loaded on the left side arm of glasses, and can be positioned such that it is for example arranged to, towards (, projection is provided) or to projection on user's the right hand, and video camera 208 can be loaded in the right arm of glasses, and can be positioned such that it is arranged to the left hand aiming towards user.Further, on the each same side arm that can be loaded in glasses in video camera 208 and projector 210, and the region that can be configured to have before user is aimed at cross one another ray.
In further other examples, projector 210 can be loaded on the left side arm (or right arm) of glasses, and for example can be arranged to, towards (, projection is provided) or to user's right hand projection, and video camera 208 can be loaded in the right side that the projector 210 of glasses is relative (or left) side arm, and can be arranged to the right hand aiming towards user.In this example, video camera 208 can be configured to have region before user and aim to intersect the ray of projection (for example,, according to the position of user's hand).
In another example, can projection for example, than the pattern at the high visual angle of visual field subtend (, the visual field) of video camera.Hand or object near projector can make projector can provide projection pattern image in one's hands.Object too far away can cause for example image to stretch the too many entirety that consequently can not catch image to video camera.
In one example, judging virtual input device when processor is not while being used, processor can indicating projector and/or video camera change projection and picture catching direction so that locating surface.For this reason, can expand the scope of projection or can revise the direction of projection, until receive the image on surface with the projected resolution/brightness of predetermined threshold.The region that can judge projection multilated is surperficial position, and can select any one surface as projection surface, thereon can projection laser pattern.System can present the surface for choice for use, the surface that picks out to user.
Figure 15 shows the exemplary operations of system.In an example, the system in Figure 15 can operate according to the method for Figure 14 1400.For example, start most, system can provide broad projection 1500, so as to be positioned at above can projection virtual input device surface.In this example, because surface 1502 for example, to the distance of (, showing with the form of the wearable glasses of user) system, the gray scale of the image on surface 1502 will be higher than the image in other regions in projection 1500.System can identification or is estimated the position on surface 1502, and indicating projector provides projection 1504 narrow or that focus on to surface 1502.The projection 1504 narrowing can provide than projection 1500 more details or the higher resolution graphics that start most.For example, the projection 1504 narrowing can provide virtual input device 1506 with the form of numeric keypad.
As another example, the system in Figure 15 can be from providing projection 1508 to surface 1510 apart from user a distance.Surface 1510 can be than the more remote wall of user's hand or another surface apart from user.By this way, by larger surf zone, projection 1508 can be provided to apart from user surface more at a distance, and can obtain the resolution of identical or similar rank.In order to make on the virtual input device being provided by projection 1508 that cursor is selected or mobile, user can moving-head, is projected in distortion on surface 1510 to cause.Video camera can catch image distortion and virtual input device non-warping configuration, and can compare, to judge suitable selection or the movement of cursor.In one example, virtual input device is the button groups of 3x3, and head moves right will distortion projection, and the projection of the leftmost row of button groups on surface 1510 is brighter than other two row like this, and therefore, projector can be arranged to cursor movement to the left side.Similar head moves and can make cursor move along any direction to up/down and left/right, and the default location of cursor for example, in any position (, middle position).For alternative in virtual input device, system can comprise that sensor moves to detect other of user, for example nictation (for example, use video camera), nod (use accelerometer), or can point to selection and select (wherein selection can catch by video camera) by uses.
Figure 16 shows the functional block diagram of the example calculations equipment 1600 using in the computing system of arranging according at least some embodiment described herein.Computing equipment can be for example PC, mobile device, cell phone, video game system or GPS, and can be implemented as a part for equipment as described in Fig. 1-2, transmitter or display device or transmitter.In basic configuration 1602, computing equipment 1600 can comprise one or more processors 1610 and system storage 1620.Memory bus 1630 is used in the communication between processor 1610 and system storage 1620.According to desired configuration, processor 1610 can be any type, includes but not limited to microprocessor (μ P), microcontroller (μ C), digital signal processor (DSP) or its any combination.Memory Controller 1615 also can together use with processor 1610, or in certain embodiments, Memory Controller 1615 can be the internal part of processor 1610.
According to desired configuration, system storage 1620 can be any type, includes but not limited to volatile memory (such as RAM), nonvolatile memory (such as ROM, flash memory etc.) or its any combination.System storage 1620 can comprise one or more application 1622, and routine data 1624.According to the disclosure, application 1622 can comprise virtual input device algorithm 1623, and it is arranged to provide and is input to electronic circuit, and can comprise instruction, and it can carry out any method described herein by processor 1610.Routine data 1624 can comprise content information 1625, and it may be directed to the data of any quantity type, for example view data.In some example embodiment, application 1622 can be arranged with in operating system with routine data 1,624 one biconditional operations.
Computing equipment 1600 can have extra feature or function, and extra interface is for promoting the communication between basic configuration 1602 and any equipment and interface.For example, data storage device 1640 be can provide, removable memory device 1642, non-removable memory device 1644 or its combination comprised.The example of removable memory device and non-removable memory device comprises disk unit, for instance, for example floppy disk and hard disk drive (HDD), CD drive are as compact disk (CD) driver or digital versatile disk [Sony] (DVD) driver, solid state drive (SSD) and tape drive.Computer-storage media can comprise for example, volatibility with non-volatile, non-transient, the removable and non-removable medium for storage information (computer-readable instruction, data structure, program module or other data) of realizing with any method or technology.
System storage 1620 and memory device 1640 are examples of computer-readable storage medium.Computer-readable storage medium includes but not limited to, RAM, ROM, EEPROM, flash memory or other memory technologies, CD-ROM, digital versatile disk [Sony] (DVD) or other optical memory, tape cassete, tape, disk storage or other magnetic storage apparatus, or any other can be used for the medium of storing desired information and can being accessed by computing equipment 1600.Any such computer-readable storage medium can be a part for equipment 1600.
Computing equipment 1600 can also comprise output interface 1650, it can comprise Graphics Processing Unit 1652, and this output interface 1650 is arranged to and for example, communicates with various external units (display device 1660 or loudspeaker) via one or more A/V ports or communication interface 1670.Communication interface 1670 can comprise network controller 1672, its can be arranged to promote via one or more communication port 1674 in network service with the communicating by letter of one or more other computing equipments 1680.Communication connection is an example of communication media.Communication media can for example, specifically be presented by computer-readable instruction, data structure, program module or other data in modulated data-signal (carrier wave or other transmission mechanisms), and described communication media comprises any information transmitting medium.Modulated data-signal can be such signal: its have one or more it feature set or change in this mode of the information in coded signal.By example rather than restriction, communication media can comprise that wire medium is as cable network or direct wired connection, and wireless medium is as sound, radio frequency (RF), infrared (IR) and other wireless mediums.
Computing equipment 1600 can be implemented as the part of portable (or mobile) electronic equipment of little form factor, and described electronic equipment is for example cell phone, PDA(Personal Digital Assistant), personal media player device, wireless network evaluation equipment, individual Headphone device, specialized equipment or the mixing apparatus that comprises any above function.Computing equipment 1600 can also be embodied as personal computer, comprises laptop computer and the configuration of non-laptop computer.
In certain embodiments, disclosed method can realize with the form of the computer-readable instruction of encoding on the tangible computer-readable medium in goods or on other non-transient media of goods.Instruction can be for example machine readable format.Figure 17 shows the view of the part viewpoint of the concept of exemplary computer program product 1700, and it comprises the computer program for object computer process on computing equipment of arranging according at least some embodiment that occur herein.In one embodiment, use signal bearing medium 1701 that exemplary computer program product 1700 is provided.Signal bearing medium 1701 can comprise one or more programming instructions 1702, in the time that it is carried out by one or more processors, can provide above-mentioned functions or partial function about Fig. 1-16.Therefore, for example, with reference to the embodiment showing in Fig. 6 and Figure 12-14, one or more features of frame 602-612,1202-1208,1302-1308 and/or frame 1402-1414 can be born by the one or more instructions that are associated with signal bearing medium 1701.
In some instances, signal bearing medium 1701 can comprise computer-readable medium 1703, such as but not limited to hard disk drive, compact disk (CD), digitized video disk (DVD), numerical tape, storer etc.In some implementations, signal bearing medium 1701 can comprise computing machine recordable media 1704, such as but not limited to storer, read/write (R/W) CD, R/W DVD etc.In some implementations, signal bearing medium 1701 can comprise communication media 1705, for example, such as but not limited to numeral and/or analogue communication medium (, optical fiber, waveguide, wire communication link, wireless communication link etc.).Therefore, for example, signal bearing medium 1701 can be passed on by the communication media of wireless 1705 (wireless communication medium of for example, deferring to IEEE802.11 standard or other host-host protocols).
One or more programming instructions 1702 can be, for example, computing machine can be carried out and/or the instruction of logic realization.In some instances, the computing equipment 1600 of for example Figure 16 of computing equipment can be configured in response to programming instruction 1702, by the one or more computing equipments 1600 that are communicated in computer-readable medium 1703, computing machine recordable media 1704 and/or communication media 1705, to provide various operations, function or action.Example programming instruction 1702 is shown in Figure 17 to comprise the Part Methods being illustrated in Fig. 6 and Figure 12-14; But programming instruction 1702 can adopt other forms, and can comprise that instruction is to carry out any method described herein (or part of any method described herein).
In example further, the equipment of any type described herein or the parts of equipment, can be used as or be configured to the means of the function for carrying out any method described herein (or any part of method described herein).
Should be appreciated that layout described herein is only for the object of example.Like this, those those skilled in the art will understand and can alternatively use other to arrange and other elements (for example, the grouping of machine, interface, function, order and function etc.), and result as requested can together be omitted some elements.Further, many elements of description are functional entitys, and it can be implemented as parts discrete or that distribute or is combined with other parts, with any suitable combination and position.
Although disclose each side and embodiment herein, other aspects and embodiment will become obvious to those skilled in the art.Each side disclosed herein and embodiment are not intended to restriction for the object of explaining, the complete scope of the equivalent that true scope is given by following claim and described claim is indicated.Also should be appreciated that term used herein is only used to describe the object of specific embodiment, and be not intended to restriction.
Claims (20)
1. a method, comprising:
Input equipment is provided to surface, and described input equipment comprises the pattern of object;
Receive the image of described input equipment;
Based on described image, judge the apparent position on described surface; With
Based on the described apparent position on described surface, revise the projection of described input equipment; And
In the time that processor judges that the described image of described input equipment has the brightness higher than predetermined threshold, judge that described input equipment is used.
2. method according to claim 1, further comprises the described apparent position based on described surface, adjusts the projecting direction of described input equipment, while moving, roughly described input equipment is provided to described surface with the described surface of box lunch.
3. method according to claim 1, further comprises the movement based on described surface at least partly, judges the Object Selection of the pattern of object.
4. method according to claim 1, further comprises the region of judging described surface, wherein revises the further described region based on described surface of described projection of described input equipment.
5. method according to claim 1, further comprises:
Reception comprises the reference background image of described input equipment;
Receive the Given Graph picture of the Object Selection of the pattern of having described described object of described input equipment; With
Based on the comparison of described Given Graph picture and described reference background image, judge the selection of described object.
6. method according to claim 1, wherein described input equipment being provided to described surface comprises: described input equipment is provided, make described input equipment have given overlay area on described surface, wherein said given overlay area is the described apparent position based on described surface.
7. on it, store a non-transient computer-readable medium for instruction, described instruction carries out to make described computing equipment to carry out following functions by computing equipment, comprising:
The input equipment of projection is provided to surface, and the input equipment of described projection comprises the pattern of object;
Receive the image of the input equipment of described projection;
The gray scale of the input equipment of the described projection based in the described image of the input equipment of described projection, judges the apparent position on described surface; And
Based on the described apparent position on described surface, revise the described gray scale of the input equipment of described projection.
8. non-transient computer-readable medium according to claim 7, the pattern of wherein said object comprises selector switch, and wherein said instruction is further carried out by described computing equipment, so that being carried out, described computing equipment comprises that the rotation of judging described surface is as the function moving horizontally of described selector switch.
9. non-transient computer-readable medium according to claim 7, the pattern of wherein said object comprises selector switch, and wherein said instruction is further carried out by described computing equipment, so that being carried out, described computing equipment comprises that the inclination of judging described surface is as the function of the vertical movement of described selector switch.
10. a system, comprising:
Projector; With
With the computing equipment that described projector is communicated by letter, described computing equipment is arranged to:
Make described projector that the input equipment of projection is provided to surface, the input equipment of described projection comprises the pattern of object;
Reception comprises the reference background image of the input equipment of described projection;
Receive the image of the Object Selection of the pattern of having described described object of the input equipment of described projection;
Based on the comparison of described image and described reference background image, judge the selection of described object; And
One or more based in described image and described reference background image, revise the gray scale of the input equipment of described projection.
11. systems according to claim 10, wherein said computing equipment is further arranged to the projecting direction that makes described projector change the input equipment of described projection, while moving, roughly the input equipment of described projection is provided to described surface with the described surface of box lunch.
12. systems according to claim 10, wherein said projector is arranged to the hand that the input equipment of described projection is provided to user.
13. systems according to claim 12, the pattern of wherein said object comprises selector switch, and wherein said computing equipment is further configured to judge that gesture selects as the user of the object of being indicated by described selector switch.
14. systems according to claim 13, wherein said gesture is to clench fist.
15. systems according to claim 10, further comprise:
Video camera, it is coupled to described computing equipment, and is arranged to one or more images of the input equipment that catches described projection; With
Sensor, it is coupled to described computing equipment, wherein said computing equipment is further configured to receive one or more output from described sensor, and for judge the movement of described system based on described one or more output, and wherein said video camera is arranged in the time that described one or more output indicates described system roughly to fix, start described one or more images of the input equipment that catches described projection.
16. systems according to claim 15, further comprise headset equipment, and wherein said projector and described video camera are coupled to described headset equipment.
17. systems according to claim 10, wherein said computing equipment is further configured to:
On the display device that is coupled to described computing equipment, produce the demonstration of input equipment; And
The request that receives is to project to described surface by shown input equipment, and wherein said computing equipment is arranged to: based on described request, make described projector that the input equipment of described projection is provided to described surface.
18. systems according to claim 10, wherein said computing equipment is further configured to judge the region on described surface, wherein said computing equipment is arranged to: based on the described region on described surface, make described projector that the input equipment of described projection is provided to described surface.
19. systems according to claim 10, wherein said computing equipment is arranged to: the apparent position of judging described surface based on described reference background image, wherein said computing equipment is arranged to: based on the described apparent position on described surface, revise the described gray scale of the input equipment of described projection.
20. systems according to claim 19, wherein said computing equipment is arranged to: make described projector that the input equipment of described projection is provided, thereby the input equipment of described projection has given overlay area on described surface, the described apparent position of wherein said given overlay area based on described surface.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/181,238 | 2011-07-12 | ||
US13/181,238 US8228315B1 (en) | 2011-07-12 | 2011-07-12 | Methods and systems for a virtual input device |
US13/533,120 US9069164B2 (en) | 2011-07-12 | 2012-06-26 | Methods and systems for a virtual input device |
US13/533,120 | 2012-06-26 | ||
PCT/US2012/044574 WO2013009482A2 (en) | 2011-07-12 | 2012-06-28 | Methods and systems for a virtual input device |
Publications (2)
Publication Number | Publication Date |
---|---|
CN103827780A true CN103827780A (en) | 2014-05-28 |
CN103827780B CN103827780B (en) | 2016-11-02 |
Family
ID=47506792
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201280044081.4A Active CN103827780B (en) | 2011-07-12 | 2012-06-28 | Method and system for virtual input device |
Country Status (4)
Country | Link |
---|---|
US (2) | US9069164B2 (en) |
EP (1) | EP2732357B1 (en) |
CN (1) | CN103827780B (en) |
WO (1) | WO2013009482A2 (en) |
Cited By (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104182051A (en) * | 2014-08-29 | 2014-12-03 | 百度在线网络技术（北京）有限公司 | Headset intelligent device and interactive system with same |
CN104182050A (en) * | 2014-08-29 | 2014-12-03 | 百度在线网络技术（北京）有限公司 | Headset intelligent device and projection system with same |
CN104216517A (en) * | 2014-08-25 | 2014-12-17 | 联想(北京)有限公司 | Information processing method and electronic equipment |
CN104615328A (en) * | 2014-12-29 | 2015-05-13 | 联想(北京)有限公司 | Projection output method and electronic equipment |
CN105204613A (en) * | 2014-06-27 | 2015-12-30 | 联想(北京)有限公司 | Information processing method and wearable equipment |
CN105224069A (en) * | 2014-07-03 | 2016-01-06 | 王登高 | The device of a kind of augmented reality dummy keyboard input method and use the method |
CN105652451A (en) * | 2016-04-15 | 2016-06-08 | 深圳市智能体科技有限公司 | Intelligent glasses |
CN105913558A (en) * | 2016-04-19 | 2016-08-31 | 福建联迪商用设备有限公司 | Novel cipher keyboard and realization method thereof |
CN106034212A (en) * | 2015-03-10 | 2016-10-19 | 深圳富泰宏精密工业有限公司 | Projection device, control device and wearable projection system |
CN106415460A (en) * | 2016-07-12 | 2017-02-15 | 香港应用科技研究院有限公司 | Wearable device with intelligent user input interface |
CN106466125A (en) * | 2015-07-24 | 2017-03-01 | 德国福维克控股公司 | Kitchen machine and the operation method of kitchen machine |
CN107209570A (en) * | 2015-01-27 | 2017-09-26 | 微软技术许可有限责任公司 | Dynamic self-adapting virtual list |
CN107533230A (en) * | 2015-03-06 | 2018-01-02 | 索尼互动娱乐股份有限公司 | Head mounted display tracing system |
CN107810465A (en) * | 2015-06-22 | 2018-03-16 | 微软技术许可有限责任公司 | For producing the system and method for drawing surface |
CN108107573A (en) * | 2016-11-24 | 2018-06-01 | 财团法人工业技术研究院 | Interactive display device and system |
CN108431736A (en) * | 2015-10-30 | 2018-08-21 | 奥斯坦多科技公司 | The system and method for gesture interface and Projection Display on body |
CN111176513A (en) * | 2019-12-31 | 2020-05-19 | 维沃移动通信有限公司 | Control method and electronic equipment |
CN112025547A (en) * | 2020-09-15 | 2020-12-04 | 泉芯集成电路制造(济南)有限公司 | Laser projection virtual correction device and method |
CN112530025A (en) * | 2014-12-18 | 2021-03-19 | 脸谱科技有限责任公司 | System, apparatus and method for providing a user interface for a virtual reality environment |
Families Citing this family (145)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP2573039A3 (en) * | 2011-09-23 | 2013-06-26 | Manitowoc Crane Companies, LLC | Outrigger monitoring system and methods |
JP2013110514A (en) * | 2011-11-18 | 2013-06-06 | Konica Minolta Business Technologies Inc | Operation input system |
US20130215132A1 (en) * | 2012-02-22 | 2013-08-22 | Ming Fong | System for reproducing virtual objects |
US9612663B2 (en) * | 2012-03-26 | 2017-04-04 | Tata Consultancy Services Limited | Multimodal system and method facilitating gesture creation through scalar and vector data |
US9857919B2 (en) * | 2012-05-17 | 2018-01-02 | Hong Kong Applied Science And Technology Research | Wearable device with intelligent user-input interface |
US9671566B2 (en) | 2012-06-11 | 2017-06-06 | Magic Leap, Inc. | Planar waveguide apparatus with diffraction element(s) and system employing same |
US9582035B2 (en) | 2014-02-25 | 2017-02-28 | Medibotics Llc | Wearable computing devices and methods for the wrist and/or forearm |
US10314492B2 (en) | 2013-05-23 | 2019-06-11 | Medibotics Llc | Wearable spectroscopic sensor to measure food consumption based on interaction between light and the human body |
US9261759B1 (en) * | 2012-08-31 | 2016-02-16 | Amazon Technologies, Inc. | Aspect corrected laser video projection |
BR112015005692A2 (en) | 2012-09-21 | 2017-07-04 | Sony Corp | control device and storage medium. |
US20140118250A1 (en) * | 2012-10-25 | 2014-05-01 | University Of Seoul Industry Cooperation Foundation | Pointing position determination |
US20140118243A1 (en) * | 2012-10-25 | 2014-05-01 | University Of Seoul Industry Cooperation Foundation | Display section determination |
US11237719B2 (en) | 2012-11-20 | 2022-02-01 | Samsung Electronics Company, Ltd. | Controlling remote electronic device with wearable electronic device |
US10423214B2 (en) | 2012-11-20 | 2019-09-24 | Samsung Electronics Company, Ltd | Delegating processing from wearable electronic device |
US9030446B2 (en) | 2012-11-20 | 2015-05-12 | Samsung Electronics Co., Ltd. | Placement of optical sensor on wearable electronic device |
US10185416B2 (en) | 2012-11-20 | 2019-01-22 | Samsung Electronics Co., Ltd. | User gesture input to wearable electronic device involving movement of device |
US10551928B2 (en) | 2012-11-20 | 2020-02-04 | Samsung Electronics Company, Ltd. | GUI transitions on wearable electronic device |
US11372536B2 (en) | 2012-11-20 | 2022-06-28 | Samsung Electronics Company, Ltd. | Transition and interaction model for wearable electronic device |
US8994827B2 (en) | 2012-11-20 | 2015-03-31 | Samsung Electronics Co., Ltd | Wearable electronic device |
US9477313B2 (en) | 2012-11-20 | 2016-10-25 | Samsung Electronics Co., Ltd. | User gesture input to wearable electronic device involving outward-facing sensor of device |
US11157436B2 (en) | 2012-11-20 | 2021-10-26 | Samsung Electronics Company, Ltd. | Services associated with wearable electronic device |
US9977492B2 (en) * | 2012-12-06 | 2018-05-22 | Microsoft Technology Licensing, Llc | Mixed reality presentation |
US9699433B2 (en) | 2013-01-24 | 2017-07-04 | Yuchen Zhou | Method and apparatus to produce re-focusable vision with detecting re-focusing event from human eye |
US10110647B2 (en) * | 2013-03-28 | 2018-10-23 | Qualcomm Incorporated | Method and apparatus for altering bandwidth consumption |
US9361501B2 (en) | 2013-04-01 | 2016-06-07 | Ncr Corporation | Headheld scanner and POS display with mobile phone |
US10514767B2 (en) * | 2013-04-02 | 2019-12-24 | Sony Corporation | Information processing apparatus and information processing method |
US8922589B2 (en) * | 2013-04-07 | 2014-12-30 | Laor Consulting Llc | Augmented reality apparatus |
US9658453B1 (en) | 2013-04-29 | 2017-05-23 | Google Inc. | Head-mounted display including diffractive combiner to integrate a display and a sensor |
US9128285B2 (en) | 2013-04-30 | 2015-09-08 | Google Inc. | Head-mounted display including integrated projector |
CN105190515A (en) * | 2013-05-08 | 2015-12-23 | 富士通株式会社 | Input device and input program |
EP2804093A1 (en) * | 2013-05-13 | 2014-11-19 | Sony Corporation | A method for stabilization and a system thereto |
US9605950B2 (en) * | 2013-05-22 | 2017-03-28 | Cognex Corporation | System and method for efficient surface measurement using a laser displacement sensor |
US9047514B2 (en) * | 2013-07-10 | 2015-06-02 | Christie Digital Systems Usa, Inc. | Apparatus, system and method for projecting images onto predefined portions of objects |
WO2015006784A2 (en) | 2013-07-12 | 2015-01-15 | Magic Leap, Inc. | Planar waveguide apparatus with diffraction element(s) and system employing same |
US10408613B2 (en) | 2013-07-12 | 2019-09-10 | Magic Leap, Inc. | Method and system for rendering virtual content |
US10019843B2 (en) * | 2013-08-08 | 2018-07-10 | Facebook, Inc. | Controlling a near eye display |
US9286726B2 (en) | 2013-08-20 | 2016-03-15 | Ricoh Company, Ltd. | Mobile information gateway for service provider cooperation |
US10089684B2 (en) | 2013-08-20 | 2018-10-02 | Ricoh Company, Ltd. | Mobile information gateway for customer identification and assignment |
US9665901B2 (en) | 2013-08-20 | 2017-05-30 | Ricoh Company, Ltd. | Mobile information gateway for private customer interaction |
JP2015041052A (en) * | 2013-08-23 | 2015-03-02 | ソニー株式会社 | Wristband type information processing apparatus and storage medium |
US10095833B2 (en) | 2013-09-22 | 2018-10-09 | Ricoh Co., Ltd. | Mobile information gateway for use by medical personnel |
US9763071B2 (en) | 2013-09-22 | 2017-09-12 | Ricoh Company, Ltd. | Mobile information gateway for use in emergency situations or with special equipment |
US20150130688A1 (en) * | 2013-11-12 | 2015-05-14 | Google Inc. | Utilizing External Devices to Offload Text Entry on a Head Mountable Device |
DE102013019574A1 (en) * | 2013-11-22 | 2015-05-28 | Audi Ag | Method for operating electronic data glasses and electronic data glasses |
US9857971B2 (en) * | 2013-12-02 | 2018-01-02 | Industrial Technology Research Institute | System and method for receiving user input and program storage medium thereof |
KR20150067489A (en) * | 2013-12-10 | 2015-06-18 | 삼성전자주식회사 | Method and apparatus for providing input method editor in a electronic devices |
CN103713737B (en) * | 2013-12-12 | 2017-01-11 | 中国科学院深圳先进技术研究院 | Virtual keyboard system used for Google glasses |
JP6127958B2 (en) * | 2013-12-19 | 2017-05-17 | ソニー株式会社 | Information processing apparatus, information processing method, and program |
US9195124B2 (en) * | 2013-12-20 | 2015-11-24 | Plantronics, Inc. | Automatic projector safety protocols |
US9442631B1 (en) * | 2014-01-27 | 2016-09-13 | Google Inc. | Methods and systems for hands-free browsing in a wearable computing device |
US10429888B2 (en) | 2014-02-25 | 2019-10-01 | Medibotics Llc | Wearable computer display devices for the forearm, wrist, and/or hand |
US10691332B2 (en) | 2014-02-28 | 2020-06-23 | Samsung Electronics Company, Ltd. | Text input on an interactive display |
KR102184402B1 (en) * | 2014-03-06 | 2020-11-30 | 엘지전자 주식회사 | glass-type mobile terminal |
KR20150110257A (en) * | 2014-03-21 | 2015-10-02 | 삼성전자주식회사 | Method and wearable device for providing a virtual input interface |
EP2946266B1 (en) * | 2014-03-21 | 2023-06-21 | Samsung Electronics Co., Ltd. | Method and wearable device for providing a virtual input interface |
CN106133644B (en) * | 2014-04-01 | 2019-03-15 | 索尼公司 | Coordinate projected user interface |
JP5850970B2 (en) * | 2014-04-09 | 2016-02-03 | 株式会社東芝 | Information processing apparatus, video projection apparatus, information processing method, and program |
JP6362391B2 (en) * | 2014-04-10 | 2018-07-25 | キヤノン株式会社 | Information processing terminal, information processing method, and computer program |
CN103941867B (en) * | 2014-04-11 | 2017-07-11 | 北京智谷睿拓技术服务有限公司 | Exchange method and system |
CN103984437B (en) * | 2014-04-28 | 2017-09-29 | 京东方科技集团股份有限公司 | A kind of Wearable contactor control device and Wearable touch control method |
DE102014207963A1 (en) * | 2014-04-28 | 2015-10-29 | Robert Bosch Gmbh | Interactive menu |
CN103974049B (en) * | 2014-04-28 | 2015-12-02 | 京东方科技集团股份有限公司 | A kind of Wearable projection arrangement and projecting method |
US10013083B2 (en) * | 2014-04-28 | 2018-07-03 | Qualcomm Incorporated | Utilizing real world objects for user input |
WO2015175964A1 (en) | 2014-05-15 | 2015-11-19 | Burch Reuben | Wearable devices for courier processing and methods of use thereof |
JP6341755B2 (en) | 2014-05-26 | 2018-06-13 | キヤノン株式会社 | Information processing apparatus, method, program, and recording medium |
US9323983B2 (en) | 2014-05-29 | 2016-04-26 | Comcast Cable Communications, Llc | Real-time image and audio replacement for visual acquisition devices |
US9575560B2 (en) | 2014-06-03 | 2017-02-21 | Google Inc. | Radar-based gesture-recognition through a wearable device |
CN106575151A (en) * | 2014-06-17 | 2017-04-19 | 奥斯特豪特集团有限公司 | External user interface for head worn computing |
JP6041016B2 (en) | 2014-07-25 | 2016-12-07 | 裕行 池田 | Eyeglass type terminal |
WO2016017101A1 (en) * | 2014-07-30 | 2016-02-04 | ソニー株式会社 | Information processing device, information processing method and program |
US9811164B2 (en) | 2014-08-07 | 2017-11-07 | Google Inc. | Radar-based gesture sensing and data transmission |
US9921660B2 (en) | 2014-08-07 | 2018-03-20 | Google Llc | Radar-based gesture recognition |
US9946361B2 (en) | 2014-08-14 | 2018-04-17 | Qualcomm Incorporated | Management for wearable display |
US10268321B2 (en) | 2014-08-15 | 2019-04-23 | Google Llc | Interactive textiles within hard objects |
US9588625B2 (en) | 2014-08-15 | 2017-03-07 | Google Inc. | Interactive textiles |
US9829708B1 (en) | 2014-08-19 | 2017-11-28 | Boston Incubator Center, LLC | Method and apparatus of wearable eye pointing system |
US9778749B2 (en) | 2014-08-22 | 2017-10-03 | Google Inc. | Occluded gesture recognition |
US11169988B2 (en) | 2014-08-22 | 2021-11-09 | Google Llc | Radar recognition-aided search |
US9924143B2 (en) * | 2014-09-23 | 2018-03-20 | Intel Corporation | Wearable mediated reality system and method |
CN105528086A (en) * | 2014-09-29 | 2016-04-27 | 百辰光电股份有限公司 | Virtual keyboard input device and input method thereof |
US9600080B2 (en) | 2014-10-02 | 2017-03-21 | Google Inc. | Non-line-of-sight radar-based gesture recognition |
KR102358548B1 (en) * | 2014-10-15 | 2022-02-04 | 삼성전자주식회사 | Method and appratus for processing screen using device |
CN104317398B (en) * | 2014-10-15 | 2017-12-01 | 天津三星电子有限公司 | A kind of gestural control method, Wearable and electronic equipment |
GB2532192A (en) | 2014-10-29 | 2016-05-18 | Ibm | Secure pairing of personal device with host device |
KR20160055577A (en) * | 2014-11-10 | 2016-05-18 | 엘지전자 주식회사 | Wearable device |
JP2016099742A (en) * | 2014-11-19 | 2016-05-30 | 株式会社東芝 | Information processing device, video projection device, information processing method and program |
JP6492588B2 (en) * | 2014-12-01 | 2019-04-03 | セイコーエプソン株式会社 | Projector and projector control method |
US20160178906A1 (en) * | 2014-12-19 | 2016-06-23 | Intel Corporation | Virtual wearables |
US10108832B2 (en) * | 2014-12-30 | 2018-10-23 | Hand Held Products, Inc. | Augmented reality vision barcode scanning system and method |
KR102345911B1 (en) * | 2015-01-16 | 2022-01-03 | 삼성전자주식회사 | Virtual input apparatus and method for receiving user input using thereof |
US10016162B1 (en) | 2015-03-23 | 2018-07-10 | Google Llc | In-ear health monitoring |
US9983747B2 (en) | 2015-03-26 | 2018-05-29 | Google Llc | Two-layer interactive textiles |
CN111522434A (en) | 2015-04-30 | 2020-08-11 | 谷歌有限责任公司 | RF-based micro-motion tracking for gesture tracking and recognition |
CN107466389B (en) | 2015-04-30 | 2021-02-12 | 谷歌有限责任公司 | Method and apparatus for determining type-agnostic RF signal representation |
EP3289434A1 (en) | 2015-04-30 | 2018-03-07 | Google LLC | Wide-field radar-based gesture recognition |
WO2016185845A1 (en) * | 2015-05-21 | 2016-11-24 | 日本電気株式会社 | Interface control system, interface control device, interface control method and program |
US9693592B2 (en) | 2015-05-27 | 2017-07-04 | Google Inc. | Attaching electronic components to interactive textiles |
US10088908B1 (en) | 2015-05-27 | 2018-10-02 | Google Llc | Gesture detection and interactions |
CN105094675B (en) * | 2015-07-28 | 2019-04-02 | 中国联合网络通信集团有限公司 | A kind of man-machine interaction method and touch screen wearable device |
US10565446B2 (en) | 2015-09-24 | 2020-02-18 | Tobii Ab | Eye-tracking enabled wearable devices |
KR20180057693A (en) | 2015-09-24 | 2018-05-30 | 토비 에이비 | Eye wearable wearable devices |
US10416776B2 (en) | 2015-09-24 | 2019-09-17 | International Business Machines Corporation | Input device interaction |
CN108027656B (en) * | 2015-09-28 | 2021-07-06 | 日本电气株式会社 | Input device, input method, and program |
US10817065B1 (en) | 2015-10-06 | 2020-10-27 | Google Llc | Gesture recognition using multiple antenna |
WO2017079484A1 (en) | 2015-11-04 | 2017-05-11 | Google Inc. | Connectors for connecting electronics embedded in garments to external devices |
CN105892636A (en) * | 2015-11-20 | 2016-08-24 | 乐视致新电子科技（天津）有限公司 | Control method applied to head-mounted device and head-mounted device |
US10940790B1 (en) | 2015-12-01 | 2021-03-09 | Apple Inc. | System and method for adjustable lighting based on occupant and object identification in a vehicle |
US10048560B1 (en) | 2015-12-01 | 2018-08-14 | Apple Inc. | Transparent structure with controllable lighting |
US10843535B1 (en) | 2015-12-01 | 2020-11-24 | Apple Inc. | System and method for dynamic privacy and window tinting |
JP2017146927A (en) * | 2016-02-19 | 2017-08-24 | ソニーモバイルコミュニケーションズ株式会社 | Control device, control method, and program |
JP6256497B2 (en) * | 2016-03-04 | 2018-01-10 | 日本電気株式会社 | Information processing system, information processing apparatus, control method, and program |
CN105652651B (en) * | 2016-03-28 | 2018-01-16 | 京东方科技集团股份有限公司 | Wearable items |
CN109416572A (en) | 2016-04-29 | 2019-03-01 | 托比股份公司 | Enable the wearable device of eyes tracking |
WO2017192167A1 (en) | 2016-05-03 | 2017-11-09 | Google Llc | Connecting an electronic component to an interactive textile |
WO2017200570A1 (en) | 2016-05-16 | 2017-11-23 | Google Llc | Interactive object with multiple electronics modules |
US20170351415A1 (en) * | 2016-06-06 | 2017-12-07 | Jonathan K. Cheng | System and interfaces for an interactive system |
US10591988B2 (en) * | 2016-06-28 | 2020-03-17 | Hiscene Information Technology Co., Ltd | Method for displaying user interface of head-mounted display device |
US10579150B2 (en) | 2016-12-05 | 2020-03-03 | Google Llc | Concurrent detection of absolute distance and relative movement for sensing action gestures |
US10690918B2 (en) * | 2016-12-19 | 2020-06-23 | United States Of America As Represented By The Administrator Of Nasa | Optical head-mounted displays for laser safety eyewear |
US10168788B2 (en) * | 2016-12-20 | 2019-01-01 | Getgo, Inc. | Augmented reality user interface |
US10313148B1 (en) * | 2017-06-29 | 2019-06-04 | State Farm Mutual Automobile Insurance Company | Configuring a device for movement-based control |
US10355875B1 (en) * | 2017-06-29 | 2019-07-16 | State Farm Mutual Automobile Insurance Company | Utilizing movement-based control data |
US10452263B2 (en) * | 2017-09-13 | 2019-10-22 | Biosense Webster (Israel) Ltd. | Patient face as touchpad user interface |
US10564420B2 (en) | 2017-10-02 | 2020-02-18 | International Business Machines Corporation | Midair interaction with electronic pen projection computing system |
US10983144B2 (en) | 2017-10-11 | 2021-04-20 | Rohde & Schwarz Gmbh & Co. Kg | Measurement apparatus with projected user interface |
US10345895B2 (en) | 2017-11-07 | 2019-07-09 | International Business Machines Corporation | Hand and finger line grid for hand based interactions |
TWI656362B (en) * | 2018-03-26 | 2019-04-11 | 仁寶電腦工業股份有限公司 | Electronic device and object rework method thereof |
US11353952B2 (en) | 2018-11-26 | 2022-06-07 | Tobii Ab | Controlling illuminators for optimal glints |
KR20200098034A (en) * | 2019-02-11 | 2020-08-20 | 삼성전자주식회사 | Electronic device for providing augmented reality user interface and operating method thereof |
CN114175103A (en) * | 2019-05-30 | 2022-03-11 | 辉达公司 | Virtual reality simulation using surface tracking |
JP2021002288A (en) * | 2019-06-24 | 2021-01-07 | 株式会社ソニー・インタラクティブエンタテインメント | Image processor, content processing system, and image processing method |
FR3099957B1 (en) * | 2019-08-12 | 2022-07-08 | Yanis Rouvrais | Portable device for projecting an image onto at least one part of a user's body |
US11132058B1 (en) | 2019-09-12 | 2021-09-28 | Facebook Technologies, Llc | Spatially offset haptic feedback |
DE102020100073A1 (en) * | 2020-01-03 | 2021-07-08 | Bayerische Motoren Werke Aktiengesellschaft | Method and system for selecting an entry from a list |
US11669988B1 (en) | 2020-02-10 | 2023-06-06 | 4DMobile, LLC | System and method for three-dimensional box segmentation and measurement |
DE102020205954A1 (en) | 2020-05-12 | 2021-11-18 | Robert Bosch Gesellschaft mit beschränkter Haftung | Method for recognizing at least one object and / or a movement |
TWI746169B (en) * | 2020-09-17 | 2021-11-11 | 宏碁股份有限公司 | Augmented reality eyeglasses having structured light detecting function |
EP3985486B1 (en) | 2020-10-13 | 2024-04-17 | Hiroyuki Ikeda | Glasses-type terminal |
US11797162B2 (en) | 2020-12-22 | 2023-10-24 | Snap Inc. | 3D painting on an eyewear device |
US20220197393A1 (en) * | 2020-12-22 | 2022-06-23 | Snap Inc. | Gesture control on an eyewear device |
US11782577B2 (en) | 2020-12-22 | 2023-10-10 | Snap Inc. | Media content player on an eyewear device |
JP7318669B2 (en) * | 2021-01-27 | 2023-08-01 | セイコーエプソン株式会社 | Display method and display system |
US11442582B1 (en) * | 2021-03-05 | 2022-09-13 | Zebra Technologies Corporation | Virtual keypads for hands-free operation of computing devices |
JP2022186140A (en) * | 2021-06-04 | 2022-12-15 | 京セラドキュメントソリューションズ株式会社 | Information input device |
US20240077936A1 (en) * | 2022-09-07 | 2024-03-07 | Snap Inc. | Selecting ar buttons on a hand |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6512838B1 (en) * | 1999-09-22 | 2003-01-28 | Canesta, Inc. | Methods for enhancing performance and data acquired from three-dimensional image systems |
CN1604632A (en) * | 2003-09-29 | 2005-04-06 | 精工爱普生株式会社 | Projector and drive control of light source lamp for projector |
CN1664755A (en) * | 2005-03-11 | 2005-09-07 | 西北工业大学 | Video recognition input system |
CN101685342A (en) * | 2008-09-26 | 2010-03-31 | 联想(北京)有限公司 | Method and device for realizing dynamic virtual keyboard |
US20100199232A1 (en) * | 2009-02-03 | 2010-08-05 | Massachusetts Institute Of Technology | Wearable Gestural Interface |
Family Cites Families (41)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3157304B2 (en) | 1992-09-22 | 2001-04-16 | 富士通株式会社 | Virtual keyboard |
US20090273574A1 (en) | 1995-06-29 | 2009-11-05 | Pryor Timothy R | Programmable tactile touch screen displays and man-machine interfaces for improved vehicle instrumentation and telematics |
US6266048B1 (en) | 1998-08-27 | 2001-07-24 | Hewlett-Packard Company | Method and apparatus for a virtual display/keyboard for a PDA |
US20030174125A1 (en) | 1999-11-04 | 2003-09-18 | Ilhami Torunoglu | Multiple input modes in overlapping physical space |
US20030021032A1 (en) | 2001-06-22 | 2003-01-30 | Cyrus Bamji | Method and system to display a virtual input device |
US6771294B1 (en) * | 1999-12-29 | 2004-08-03 | Petri Pulli | User interface |
GB2370395A (en) | 2000-12-19 | 2002-06-26 | Ubinetics Ltd | Display apparatus |
GB2374266A (en) | 2001-04-04 | 2002-10-09 | Matsushita Comm Ind Uk Ltd | Virtual user interface device |
US7259747B2 (en) | 2001-06-05 | 2007-08-21 | Reactrix Systems, Inc. | Interactive video display system |
US6614995B2 (en) * | 2001-11-28 | 2003-09-02 | Sunplus Technology Co., Ltd. | Apparatus and method for compensating auto-focus of image capture device by utilizing red-eye eliminating function |
US7170492B2 (en) | 2002-05-28 | 2007-01-30 | Reactrix Systems, Inc. | Interactive video display system |
US7348963B2 (en) | 2002-05-28 | 2008-03-25 | Reactrix Systems, Inc. | Interactive video display system |
JP2004004284A (en) * | 2002-05-31 | 2004-01-08 | Canon Inc | Projection display apparatus |
EP1408443B1 (en) * | 2002-10-07 | 2006-10-18 | Sony France S.A. | Method and apparatus for analysing gestures produced by a human, e.g. for commanding apparatus by gesture recognition |
US7774075B2 (en) | 2002-11-06 | 2010-08-10 | Lin Julius J Y | Audio-visual three-dimensional input/output |
KR100528537B1 (en) * | 2003-04-04 | 2005-11-15 | 엘지전자 주식회사 | Device and the Method for automatic controlling the lighting of liquid crystal display |
CA2524213A1 (en) * | 2003-04-30 | 2004-11-18 | D4D Technologies, L.P. | Intra-oral imaging system |
US7173605B2 (en) | 2003-07-18 | 2007-02-06 | International Business Machines Corporation | Method and apparatus for providing projected user interface for computing device |
US7536032B2 (en) | 2003-10-24 | 2009-05-19 | Reactrix Systems, Inc. | Method and system for processing captured image information in an interactive video display system |
WO2005064439A2 (en) | 2003-12-31 | 2005-07-14 | France Telecom | Dynamically modifiable virtual keyboard or virtual mouse interface |
US7369584B2 (en) | 2003-12-31 | 2008-05-06 | Symbol Technologies, Inc. | Laser projection display |
US20060007056A1 (en) | 2004-07-09 | 2006-01-12 | Shu-Fong Ou | Head mounted display system having virtual keyboard and capable of adjusting focus of display screen and device installed the same |
US20060050062A1 (en) | 2004-08-19 | 2006-03-09 | Masanori Ozawa | Input device |
US20060232558A1 (en) | 2005-04-15 | 2006-10-19 | Huan-Wen Chien | Virtual keyboard |
JP4422074B2 (en) * | 2005-06-16 | 2010-02-24 | Ｎｅｃディスプレイソリューションズ株式会社 | Projector and focus adjustment method |
WO2007053116A1 (en) | 2005-10-31 | 2007-05-10 | National University Of Singapore | Virtual interface system |
US20100214267A1 (en) | 2006-06-15 | 2010-08-26 | Nokia Corporation | Mobile device with virtual keypad |
US20080018591A1 (en) | 2006-07-20 | 2008-01-24 | Arkady Pittel | User Interfacing |
CN101339450B (en) * | 2007-07-04 | 2011-08-17 | 群康科技(深圳)有限公司 | Touch control display and its drive method |
TW200907764A (en) | 2007-08-01 | 2009-02-16 | Unique Instr Co Ltd | Three-dimensional virtual input and simulation apparatus |
US8007110B2 (en) | 2007-12-28 | 2011-08-30 | Motorola Mobility, Inc. | Projector system employing depth perception to detect speaker position and gestures |
US7869204B2 (en) | 2008-09-15 | 2011-01-11 | International Business Machines Corporation | Compact size portable computer having a fully integrated virtual keyboard projector and a display projector |
US8610726B2 (en) | 2008-09-26 | 2013-12-17 | Apple Inc. | Computer systems and methods with projected display |
US20100177035A1 (en) | 2008-10-10 | 2010-07-15 | Schowengerdt Brian T | Mobile Computing Device With A Virtual Keyboard |
US8860693B2 (en) | 2009-07-08 | 2014-10-14 | Apple Inc. | Image processing for camera based motion tracking |
KR20110049162A (en) | 2009-11-04 | 2011-05-12 | 삼성테크윈 주식회사 | Apparatus and method for virtual input/output in portable image processing device |
US8421634B2 (en) * | 2009-12-04 | 2013-04-16 | Microsoft Corporation | Sensing mechanical energy to appropriate the body for data input |
AU2011220382A1 (en) * | 2010-02-28 | 2012-10-18 | Microsoft Corporation | Local advertising content on an interactive head-mounted eyepiece |
JP5439347B2 (en) * | 2010-12-06 | 2014-03-12 | 日立コンシューマエレクトロニクス株式会社 | Operation control device |
US10061387B2 (en) * | 2011-03-31 | 2018-08-28 | Nokia Technologies Oy | Method and apparatus for providing user interfaces |
JP6039248B2 (en) * | 2012-06-04 | 2016-12-07 | キヤノン株式会社 | Information processing apparatus and control method thereof |
-
2012
- 2012-06-26 US US13/533,120 patent/US9069164B2/en active Active
- 2012-06-28 EP EP12810718.2A patent/EP2732357B1/en active Active
- 2012-06-28 WO PCT/US2012/044574 patent/WO2013009482A2/en active Application Filing
- 2012-06-28 CN CN201280044081.4A patent/CN103827780B/en active Active
-
2015
- 2015-06-10 US US14/735,398 patent/US9727174B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6512838B1 (en) * | 1999-09-22 | 2003-01-28 | Canesta, Inc. | Methods for enhancing performance and data acquired from three-dimensional image systems |
CN1604632A (en) * | 2003-09-29 | 2005-04-06 | 精工爱普生株式会社 | Projector and drive control of light source lamp for projector |
CN1664755A (en) * | 2005-03-11 | 2005-09-07 | 西北工业大学 | Video recognition input system |
CN101685342A (en) * | 2008-09-26 | 2010-03-31 | 联想(北京)有限公司 | Method and device for realizing dynamic virtual keyboard |
US20100199232A1 (en) * | 2009-02-03 | 2010-08-05 | Massachusetts Institute Of Technology | Wearable Gestural Interface |
Cited By (28)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105204613A (en) * | 2014-06-27 | 2015-12-30 | 联想(北京)有限公司 | Information processing method and wearable equipment |
CN105224069A (en) * | 2014-07-03 | 2016-01-06 | 王登高 | The device of a kind of augmented reality dummy keyboard input method and use the method |
CN104216517A (en) * | 2014-08-25 | 2014-12-17 | 联想(北京)有限公司 | Information processing method and electronic equipment |
CN104182050A (en) * | 2014-08-29 | 2014-12-03 | 百度在线网络技术（北京）有限公司 | Headset intelligent device and projection system with same |
CN104182051A (en) * | 2014-08-29 | 2014-12-03 | 百度在线网络技术（北京）有限公司 | Headset intelligent device and interactive system with same |
CN112530025A (en) * | 2014-12-18 | 2021-03-19 | 脸谱科技有限责任公司 | System, apparatus and method for providing a user interface for a virtual reality environment |
CN104615328A (en) * | 2014-12-29 | 2015-05-13 | 联想(北京)有限公司 | Projection output method and electronic equipment |
CN107209570A (en) * | 2015-01-27 | 2017-09-26 | 微软技术许可有限责任公司 | Dynamic self-adapting virtual list |
CN107209570B (en) * | 2015-01-27 | 2020-08-25 | 微软技术许可有限责任公司 | Dynamic adaptive virtual list |
CN107533230B (en) * | 2015-03-06 | 2021-10-26 | 索尼互动娱乐股份有限公司 | Tracking system for head-mounted display |
CN107533230A (en) * | 2015-03-06 | 2018-01-02 | 索尼互动娱乐股份有限公司 | Head mounted display tracing system |
CN106034212A (en) * | 2015-03-10 | 2016-10-19 | 深圳富泰宏精密工业有限公司 | Projection device, control device and wearable projection system |
CN107810465A (en) * | 2015-06-22 | 2018-03-16 | 微软技术许可有限责任公司 | For producing the system and method for drawing surface |
CN107810465B (en) * | 2015-06-22 | 2021-03-19 | 微软技术许可有限责任公司 | System and method for generating a drawing surface |
CN106466125A (en) * | 2015-07-24 | 2017-03-01 | 德国福维克控股公司 | Kitchen machine and the operation method of kitchen machine |
TWI756191B (en) * | 2015-10-30 | 2022-03-01 | 美商傲思丹度科技公司 | System and methods for on-body gestural interfaces and projection displays |
US11106273B2 (en) | 2015-10-30 | 2021-08-31 | Ostendo Technologies, Inc. | System and methods for on-body gestural interfaces and projection displays |
CN108431736A (en) * | 2015-10-30 | 2018-08-21 | 奥斯坦多科技公司 | The system and method for gesture interface and Projection Display on body |
CN105652451A (en) * | 2016-04-15 | 2016-06-08 | 深圳市智能体科技有限公司 | Intelligent glasses |
CN105913558A (en) * | 2016-04-19 | 2016-08-31 | 福建联迪商用设备有限公司 | Novel cipher keyboard and realization method thereof |
WO2017181563A1 (en) * | 2016-04-19 | 2017-10-26 | 福建联迪商用设备有限公司 | Novel password keyboard and implementation method thereof |
CN106415460B (en) * | 2016-07-12 | 2019-04-09 | 香港应用科技研究院有限公司 | Wearable device with intelligent subscriber input interface |
CN106415460A (en) * | 2016-07-12 | 2017-02-15 | 香港应用科技研究院有限公司 | Wearable device with intelligent user input interface |
CN108107573B (en) * | 2016-11-24 | 2020-09-29 | 财团法人工业技术研究院 | Interactive display device and system |
CN108107573A (en) * | 2016-11-24 | 2018-06-01 | 财团法人工业技术研究院 | Interactive display device and system |
CN111176513A (en) * | 2019-12-31 | 2020-05-19 | 维沃移动通信有限公司 | Control method and electronic equipment |
CN111176513B (en) * | 2019-12-31 | 2021-07-13 | 维沃移动通信有限公司 | Control method and electronic equipment |
CN112025547A (en) * | 2020-09-15 | 2020-12-04 | 泉芯集成电路制造(济南)有限公司 | Laser projection virtual correction device and method |
Also Published As
Publication number | Publication date |
---|---|
US9727174B2 (en) | 2017-08-08 |
WO2013009482A3 (en) | 2013-05-30 |
US20150268799A1 (en) | 2015-09-24 |
US9069164B2 (en) | 2015-06-30 |
EP2732357B1 (en) | 2018-09-19 |
EP2732357A4 (en) | 2015-03-04 |
CN103827780B (en) | 2016-11-02 |
EP2732357A2 (en) | 2014-05-21 |
US20130016070A1 (en) | 2013-01-17 |
WO2013009482A2 (en) | 2013-01-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN103827780A (en) | Methods and systems for a virtual input device | |
CN110647237B (en) | Gesture-based content sharing in an artificial reality environment | |
US11816296B2 (en) | External user interface for head worn computing | |
US11886638B2 (en) | External user interface for head worn computing | |
US8228315B1 (en) | Methods and systems for a virtual input device | |
US10643390B2 (en) | Head mounted display, method for controlling head mounted display, and computer program | |
EP3752897B1 (en) | Systems and methods for eye tracking in virtual reality and augmented reality applications | |
US20170336872A1 (en) | External user interface for head worn computing | |
CN110646938B (en) | Near-eye display system | |
US20160025979A1 (en) | External user interface for head worn computing | |
US20170017323A1 (en) | External user interface for head worn computing | |
US10474226B2 (en) | Head-mounted display device, computer program, and control method for head-mounted display device | |
US20170100664A1 (en) | External user interface for head worn computing | |
US20150205351A1 (en) | External user interface for head worn computing | |
US20160025977A1 (en) | External user interface for head worn computing | |
US20160027211A1 (en) | External user interface for head worn computing | |
CN107646098A (en) | System for tracking portable equipment in virtual reality | |
WO2015195444A1 (en) | External user interface for head worn computing | |
WO2015179877A2 (en) | External user interface for head worn computing | |
CN106919262A (en) | Augmented reality equipment | |
JP6996115B2 (en) | Head-mounted display device, program, and control method of head-mounted display device | |
KR102539045B1 (en) | Dashboard control apparatus and method for wearable augmented reality device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
C14 | Grant of patent or utility model | ||
GR01 | Patent grant | ||
CP01 | Change in the name or title of a patent holder |
Address after: American CaliforniaPatentee after: Google limited liability companyAddress before: American CaliforniaPatentee before: Google Inc. |
|
CP01 | Change in the name or title of a patent holder |