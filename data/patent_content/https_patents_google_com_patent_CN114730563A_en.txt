CN114730563A - Re-scoring automatic speech recognition hypotheses using audio-visual matching - Google Patents
Re-scoring automatic speech recognition hypotheses using audio-visual matching Download PDFInfo
- Publication number
- CN114730563A CN114730563A CN201980102308.8A CN201980102308A CN114730563A CN 114730563 A CN114730563 A CN 114730563A CN 201980102308 A CN201980102308 A CN 201980102308A CN 114730563 A CN114730563 A CN 114730563A
- Authority
- CN
- China
- Prior art keywords
- utterance
- candidate
- user
- candidate transcriptions
- transcriptions
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/083—Recognition networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/774—Generating sets of training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/168—Feature extraction; Face representation
- G06V40/171—Local features and components; Facial parts ; Occluding parts, e.g. glasses; Geometrical relationships
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/01—Assessment or evaluation of speech recognition systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/24—Speech recognition using non-acoustical features
- G10L15/25—Speech recognition using non-acoustical features using position of the lips, movement of the lips or face analysis
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
- G10L25/57—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination for processing of video signals
Abstract
A method (400) comprising: receiving audio data (112) corresponding to an utterance (101) spoken by a user (10); receiving video data (114) representing movement of a user's lips as the user speaks an utterance; and obtaining a plurality of candidate transcriptions (135) of the utterance based on the audio data. For each candidate transcription of the plurality of candidate transcriptions, the method further comprises: generating a synthetic speech representation (145) corresponding to the candidate transcription; and a consistency score is determined that indicates a likelihood that the synthesized speech representation matches the movement of the user's lips when the user speaks the utterance (155). The method further comprises the following steps: one of a plurality of candidate transcriptions of the utterance is selected as a speech recognition output based on the consensus scores determined for the plurality of candidate transcriptions of the utterance (175).
Description
Technical Field
The present disclosure relates to re-scoring Automatic Speech Recognition (ASR) hypotheses using audiovisual matching.
Background
Automatic Speech Recognition (ASR) is a technology commonly used in mobile and other devices. Typically, automatic speech recognition attempts to provide an accurate transcription of what a person says. Obtaining accurate ASR results can be a difficult task in noisy environments, or when the audio quality of the recorded utterance is poor. When video data for a speaker is available, the video data can be utilized to help improve ASR results. For example, the speaker's video data may provide the movement of the lips while the speaker is speaking the utterance, which in turn may be combined with the audio data of the utterance to help process the ASR results.
Disclosure of Invention
One aspect of the present disclosure provides a method of re-scoring Automatic Speech Recognition (ASR) hypotheses using audiovisual matching. The method includes receiving, at data processing hardware, audio data corresponding to an utterance spoken by a user and video data representing movement of a user's lips while the user speaks the utterance. The method also includes obtaining, by the data processing hardware, a plurality of candidate transcriptions of the utterance based on the audio data. For each candidate transcription of a plurality of candidate transcriptions of an utterance, the method comprises: generating, by data processing hardware, a synthesized speech representation corresponding to the candidate transcription; and determining, by the data processing hardware, a consistency score that indicates a likelihood that the synthesized speech representation of the corresponding candidate transcription matches a movement of the user's lips when the user speaks the utterance. The method also includes selecting, by the data processing hardware, one of the plurality of candidate transcriptions of the utterance as a speech recognition output based on the coherence score determined for the plurality of candidate transcriptions of the utterance.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, determining the consistency score includes: providing, to a consistency score model, a synthesized speech representation of the corresponding candidate transcription and video data representing movement of the user's lips as feature inputs; and determining a consistency score from the consistency score model as a feature output based on a degree to which the synthesized speech representation corresponding to the candidate transcription matches the movement of the user's lips. In these implementations, the consistency score model is trained on a plurality of training examples including positive training examples and negative training examples. The positive training examples include audio data representing a voice utterance and video data representing movement of a speaker's lips that match the voice utterance, while the negative training examples include audio data representing a voice utterance and video data representing movement of a speaker's lips that do not match the voice utterance.
In some examples, selecting one of the plurality of candidate transcriptions of the utterance as the speech recognition output includes selecting a candidate transcription associated with a highest consensus score from the plurality of candidate transcriptions of the utterance as the speech recognition output for the utterance.
In some implementations, obtaining a plurality of candidate transcriptions of an utterance includes: an initial set of candidate transcriptions of the utterance is generated based on the audio data using a speech recognizer module, each candidate transcription in the initial set of candidate transcriptions being associated with a corresponding likelihood score that indicates a likelihood that the candidate transcription is correct. The implementation further includes ranking the candidate transcriptions in the initial set of candidate transcriptions based on the likelihood scores, and determining the plurality of candidate transcriptions for the utterance as N candidate transcriptions in the initial set of candidate transcriptions associated with the highest likelihood scores, ranking the identified plurality of candidates according to the associated likelihood scores. In these implementations, the method may further include, prior to selecting one of the multiple transcriptions of the utterance, re-ranking, by the data processing hardware, the multiple candidate transcriptions of the utterance based on the consensus scores determined for the multiple candidate transcriptions of the utterance.
In some examples, obtaining a plurality of candidate transcriptions of an utterance includes: an initial set of candidate transcriptions for the utterance is generated based on the audio data using a speech recognizer module, each candidate transcription in the initial set of candidate transcriptions being associated with a corresponding likelihood score indicating a likelihood that the candidate transcription is correct. In these examples, the method further comprises: identifying, in the initial set of candidate transcriptions, two or more candidate transcriptions associated with likelihood scores that satisfy a likelihood threshold; and determining a plurality of candidate transcriptions for the utterance as the two or more identified candidate transcriptions in the initial set of candidate transcriptions associated with likelihood scores that satisfy the likelihood threshold.
In some implementations, multiple candidate transcriptions of an utterance are associated with the same language. In other examples, at least one of the plurality of candidate transcriptions of the utterance is associated with a different language than the other plurality of candidate transcriptions.
In some examples, receiving audio data corresponding to an utterance spoken by a user includes receiving audio data from a client device associated with the user, the client device in communication with one or more audio capture devices configured to capture audio data corresponding to the utterance. In these examples, the data processing hardware resides on the client device. In other examples, the client device is remote from and in communication with the data processing hardware over a network.
In some implementations, receiving video data representing movement of a user's lips while the user is speaking the utterance includes: video data is received from a client device associated with a user. In these implementations, the client device includes one or more video capture devices configured to record video data representing movement of the user's lips while the user is speaking the utterance.
Another aspect of the present disclosure provides a system for re-scoring (ASR) hypotheses using audiovisual matching. The system includes data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that, when executed by the data processing hardware, cause the data processing hardware to perform operations comprising: audio data corresponding to an utterance spoken by a user and video data representing movement of a user's lips while the user is speaking the utterance are received. The operations further include obtaining a plurality of candidate transcriptions of the utterance based on the audio data. For each candidate transcription of a plurality of candidate transcriptions of an utterance, the operations include: generating a synthetic speech representation corresponding to the candidate transcription; and determining a consistency score that indicates a likelihood that the synthesized speech representation of the corresponding candidate transcription matches movement of the user's lips when the user speaks the utterance. The operations further include: one of the multiple candidate transcriptions of the utterance is selected as a speech recognition output based on the consensus scores determined for the multiple candidate transcriptions of the utterance.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, determining the consistency score includes: providing, to a consistency score model, a synthesized speech representation of the corresponding candidate transcription and video data representing movement of the user's lips as feature inputs; and determining a consistency score from the consistency score model as a feature output based on a degree to which the synthesized speech representation corresponding to the candidate transcription matches the movement of the user's lips. In these examples, the consistency score model is trained on a plurality of training examples including positive training examples and negative training examples. The positive training examples include audio data representing a speech utterance and video data representing movement of a lip of a speaker matched to the speech utterance; and, the negative training examples include audio data representing the voice utterance and video data representing movement of lips of a speaker that does not match the voice utterance.
In some examples, selecting one of a plurality of candidate transcriptions of the utterance as the speech recognition output includes: the candidate transcription associated with the highest consensus score is selected from the plurality of candidate transcriptions of the utterance as a speech recognition output for the utterance.
In some implementations, obtaining the plurality of candidate transcriptions of the utterance includes generating, using the speech recognizer module, an initial set of candidate transcriptions of the utterance based on the audio data, each candidate transcription in the initial set of candidate transcriptions being associated with a corresponding likelihood score that indicates a likelihood that the candidate transcription is correct. In these implementations, the operations further comprise: ranking the candidate transcriptions in the initial set of candidate transcriptions based on the likelihood scores; and determining a plurality of candidate transcriptions for the utterance as N candidate transcriptions in the initial set of candidate transcriptions associated with the highest likelihood scores, the identified plurality of candidates being ranked according to the associated likelihood scores. The operations may also include, prior to selecting one of the multiple transcriptions of the utterance, re-ranking, by the data processing hardware, the multiple candidate transcriptions of the utterance based on the consensus scores determined for the multiple candidate transcriptions of the utterance.
In some examples, obtaining the plurality of candidate transcriptions of the utterance includes generating, using the speech recognizer module, an initial set of candidate transcriptions for the utterance based on the audio data, each candidate transcription in the initial set of candidate transcriptions) associated with a corresponding likelihood score that indicates a likelihood that the candidate transcription is correct. In these examples, the operations further comprise: identifying, in the initial set of candidate transcriptions, two or more candidate transcriptions associated with likelihood scores that satisfy a likelihood threshold; and determining a plurality of candidate transcriptions for the utterance as the two or more identified candidate transcriptions in the initial set of candidate transcriptions associated with likelihood scores that satisfy the likelihood threshold.
In some implementations, multiple candidate transcriptions of an utterance are associated with the same language. In other examples, at least one of the plurality of candidate transcriptions of the utterance is associated with a different language than the other plurality of candidate transcriptions.
In some examples, receiving audio data corresponding to an utterance spoken by a user includes receiving audio data from a client device associated with the user, the client device in communication with one or more audio capture devices configured to capture audio data corresponding to the utterance. In these examples, the data processing hardware resides on the client device. In other examples, the client device is remote from and in communication with the data processing hardware over a network.
In some implementations, receiving video data representing movement of a user's lips while the user is speaking the utterance includes: video data is received from a client device associated with a user. In these implementations, the client device includes one or more video capture devices configured to record video data representing movement of the user's lips while the user is speaking the utterance.
The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
Drawings
FIG. 1 is a schematic diagram of an example system for automatic speech recognition using audio data and visual data.
Fig. 2A and 2B are schematic diagrams illustrating diagrams of examples of word lattices.
FIG. 3 is a schematic diagram of an example model trainer.
FIG. 4 is a flow diagram of an example arrangement of operations of a method for improving automatic speech recognition.
FIG. 5 is a schematic diagram of an example computing device that may be used to implement the systems and methods described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
The present disclosure provides a computer-implemented method that improves Automatic Speech Recognition (ASR) related to utterances spoken by a user. For example, the utterance may involve the user speaking into a digital assistant on a user device such as a smart phone, smart speaker, or smart display. The audio data of the utterance is used to generate a plurality of candidate transcriptions (e.g., also referred to as "transcription hypotheses" or "ASR result hypotheses") of the utterance, thereby enabling generation of a synthesized speech representation of the plurality of candidate transcriptions (e.g., using a text-to-speech system). Each of the synthesized speech representations may then be scored or ranked based on how well each synthesized speech representation matches the video data (i.e., based on how well each synthesized speech representation matches the motion/movement of the user's face and/or lips in the video data) using the video data of the user's face and/or lips when speaking the utterance. In this manner, a speech recognition output may be selected based on the candidate transcription corresponding to the synthesized speech representation that best matches the video data.
One technical effect of this approach (as compared to approaches that rely solely on audio data) is to improve the choice of speech recognition output. In other words, the present method makes it more likely that the correct speech recognition output (i.e., an accurate transcription of the user's utterance) will be selected. In effect, the video data is used as an additional data source to authenticate/verify/enhance the output of the audio-based automatic speech recognition system. Thus, when video data is available for a user to speak an utterance, the video data can be used to determine which of a plurality of candidate transcriptions is most likely to be correct, thereby improving the accuracy of the speech recognition system. The method solves the technical problem of how to improve the automatic voice recognition system based on the audio frequency. This is achieved here by using the video data to score or rank the options generated using only the audio data.
Another technical effect of the present method is improved recognition of the language of the dialogs. In particular, if the language of the utterance is unknown, multiple candidate transcriptions may be generated in multiple languages. In this case, the language of the utterance may be recognized based on the selected speech recognition output. Since the video data has been used to determine the best matching synthesized speech representation, the associated candidate transcription is more likely to be in the correct language.
In some cases, it is contemplated to perform audio data analysis in the cloud (i.e., remote from the user device), with subsequent video data matching done on the user device itself. One technical effect of this arrangement is reduced bandwidth requirements, as the video data may be retained on the user device without being transmitted to a remote cloud server. If the video data is to be transmitted to the cloud, it may first need to be compressed for transmission. Thus, another technical effect of video matching on the user device itself is that video data matching can be performed using uncompressed (highest quality) video data. The use of uncompressed video data makes it easier to identify matches/mismatches between the synthesized speech representation and the video data. Thus, it is expected that the score/ranking will improve, making it more likely that the correct speech recognition output will be selected.
In some examples, it is contemplated to use a system (e.g., a deep neural network) trained on a large collection of audiovisual samples to measure the degree of match between the synthesized speech representation and the video data. In one example, training examples/samples include: training examples/samples that include audio data representing a speech utterance and video data representing movement of a speaker's lips that match the speech utterance; and, negative training examples/samples that include audio data representing the voice utterance and video data representing movement of a speaker's lips that does not match the voice utterance. Such training data ensures that the system is trained to recognize both matches and mismatches between the synthesized speech representation and the video data, thereby improving the accuracy of the system.
Fig. 1 is a block diagram illustrating an example of a system 100 for automatic speech recognition of an utterance 101 spoken by a user 10 using audio data 112 corresponding to the utterance 101 and video data 114 representing movement of lips of the user 10 while the user 10 is speaking the utterance 101. System 100 includes client device 110, computing system 120, and network 118. In an example, the computing system 120 receives the audio data 112 and the video data 114 from the client device 110, and the computing system 120 obtains a plurality of candidate transcriptions 135, 135a-n for the utterance 101 based on the audio data 112. As used herein, the terms "candidate transcription" and "transcription hypothesis" may be used interchangeably. As described in more detail below, for each candidate transcription 135, the computing system 120 is configured to generate a synthesized speech representation 145 of the corresponding candidate transcription 135 and determine a consistency score 155 using the video data 114, the consistency score 155 indicating a likelihood that the synthesized speech representation 145 of the corresponding candidate transcription 135 matches the movement of the lips of the user 10 when speaking the utterance 101. Thereafter, the computing system 120 can select one of the plurality of candidate transcriptions 135 of the utterance as the speech recognition output 175 based on the consensus scores 155 determined for the plurality of candidate transcriptions 135 of the utterance 101. Fig. 1 shows stages (a) to (H) illustrating the data flow.
The client device 110 may be, for example, a desktop computer, a laptop computer, a smart phone, a smart speaker, a smart display, a tablet computer, a music player, an e-book reader, or a navigation system. The client device 110 includes: one or more audio capture devices (e.g., microphones) 103 configured to record the utterance 101 spoken by the user 10; and one or more video image/video capture devices (e.g., cameras) 105 configured to capture image/video data 114 representing movement of the lips of the user 10 as the user 10 speaks the utterance 101. In some examples, the microphone 103 or camera 105 is separate from the client device 110 and communicates with the client device 110 to provide the recorded utterance 101 or the captured image/video data 114 to the client device 110. The functions performed by computing system 120 may be performed by a single computer system or may be distributed across multiple computer systems. The network 118 may be wired or wireless or a combination of both and may include a private network and/or a public network, such as the internet.
As will become apparent, the video data 114 representing the movement of the lips of the user 10 while the user 10 is speaking may be used to improve speech recognition accuracy by re-scoring and re-ranking the plurality of candidate transcriptions 135 obtained for the utterance 101 based on the audio data 112 alone. For example, after obtaining a set of candidate transcriptions 135 for the utterance 101, a set of n best candidate transcriptions 135, 135a-n may be further processed, where n is an integer (e.g., 3, 5, or 10 most likely transcriptions). Thus, rather than accepting the candidate transcription 135 indicated by the speech recognizer module 130 as most likely based solely on the audio data, a set of n-best candidate transcriptions 135 may be re-scored and re-ranked using the video data 114.
For example, the speech recognizer module 130 may employ a language model that is wide enough to model naturally spoken utterances, but may not be able to resolve ambiguities between acoustically confusable sentences such as "I say" and "Ice Age (the glacier century)". However, by comparing video data representing movement of the lips of the user 10 with the synthesized speech representation 145 of the candidate transcription 135 to determine the consistency score 155 of the candidate transcription 135, the sentence "glacier century" including a consistency score 155 higher than the consistency score 155 of "i say" may indicate that the user 10 is more likely to say "glacier century" than "i say".
Each of the n best candidate transcriptions 135 may be provided as a synthesized speech representation 145 of the corresponding candidate transcription 135. The consistency score 155 for each of the n-best candidate transcriptions 135 is analyzed to, for example, determine how well each synthesized speech representation 145 of the corresponding candidate transcription 135 matches the video data 114, the video data 114 representing the movement of the lips of the user 10 while speaking the utterance 101 that obtained the candidate transcription 135. The consistency score 155 for each corresponding candidate transcription 135 may indicate a degree of likelihood that each candidate transcription is correct, for example, based on a likelihood that the synthesized speech representation 145 of the corresponding candidate transcription 135 matches the movement of the lips of the user 10 while the user 10 is speaking the utterance 101. If candidate transcription 135 has a low consensus score 155 (e.g., consensus score 155 is less than the consensus score threshold), candidate transcription 135 is unlikely to be the correct transcription for utterance 101. On the other hand, if candidate transcript 135 has a high consistency score 155 (e.g., consistency score 155 is greater than or equal to a consistency score threshold), then candidate transcript 135 is more likely to be correct. Thus, the consistency scores 155 of the synthesized speech representation 145 based on the video data 114 and the candidate transcriptions 135 may be used to re-rank the n best candidate transcriptions 135 obtained by the speech recognizer module 130 based on the audio data 112 alone.
In the example of fig. 1, during stage (a), the user 10 speaks the utterance 101 and the microphone 103 of the client device 110 records the utterance 101. For example, the utterance 101 may include the user 10 speaking the term "kite". At the same time, the camera 105 captures video data 114 representing the movement of the lips of the user 10 as the user 10 speaks the utterance 101. Thereafter, the client device 110 transmits, via the network 118, audio data 112 corresponding to the utterance 101 recorded by the microphone 103 and video data 114 captured by the camera 105 to the computing system 120.
During stage (B), computing system 120 receives audio data 112 and obtains a plurality of candidate transcriptions 135 for utterance 101 based on audio data 112. For example, the computing system 120 can include a speech recognizer module 130 (e.g., an Automatic Speech Recognition (ASR) module) for generating a word lattice 200 that indicates a plurality of candidate transcriptions 135 that are possible for the utterance 101 based on the audio data 112.
FIG. 2A is an example of a word lattice 200, 200a that may be provided by the speech recognizer module 130 of FIG. 1. The word lattice 200a represents a number of possible word combinations that may form different candidate transcriptions 135 of the utterance 101.
The word lattice 200a includes one or more nodes 202a-g corresponding to possible boundaries between words. Word lattice 200a includes a plurality of edges 204a-l for possible words in a transcription hypothesis (e.g., candidate transcription 135) generated from word lattice 200 a. Further, each of the edges 204a-1 may have one or more weights or probabilities that the edge is the correct edge from the corresponding node. The weights are determined by the speech recognizer module 130 and may be based on, for example, the confidence of the match between the speech data and the word of the edge and how well the word fits grammatically and/or lexically to other words in the word lattice 200 a.
For example, initially, the most likely path through the word lattice 200a (e.g., the most likely candidate transcription 135) may include edges 204c, 204e, 204i, 204k, which have the text "we're meeting about 11:30 (we come about 11: 30)". The second best path (e.g., the second best candidate transcription) may include edges 204d, 204h, 204j, 304l having the text "deer hunting scouts7:30 (hunting scouts7: 30)".
Each pair of nodes may have one or more paths corresponding to the surrogate words in the various candidate transcriptions 135. For example, the initial most likely path between a pair of nodes starting at node 202a and ending at node 202c is edge 204c "we're (we is)". The path has an alternating path that includes edges 204a-b "we are" and edge 204d "dee".
Fig. 2B is an example of a hierarchical word lattice 200, 200B that may be provided by the speech recognizer module 130 of fig. 1. Word lattice 200b includes node 252a-1, node 252a-1 representing words that make up various candidate transcriptions 135 of utterance 101. The edges between nodes 252a-1 indicate possible candidate transcriptions including: (1) nodes 252c, 252e, 252i, 252k "we're coming about 11:30 (we come at about 11: 30)"; (2) nodes 252a, 252b, 252e, 252i, 252k "we are coming about 11: 30"; (3) nodes 252a, 252b, 252f, 252g, 252i, 252k "we are come at about 11: 30"; (4) nodes 252d, 252f, 252g, 252i, 252k "der come at about 11: 30"; (5) nodes 252d, 252h, 252j, 252k "der hunting clouds 11: 30"; and (6) nodes 252d, 252h, 252j, 252l "der numbering clusters 7: 30".
Likewise, the edges between nodes 252a-l may have associated weights or probabilities based on the confidence of the speech recognition and the grammatical/lexical analysis of the resulting text. In this example, "we're corning about 11: 30" may be the best hypothesis at present, and "der hunting crops 7: 30" may be the next best hypothesis. One or more segments 254a-d may be made in the word lattice 200b that combines words and their alternatives. For example, segment 254a includes the word "we're" and the alternate words "we are" and "deer". Segment 254b includes the words "coming" and the alternatives "come at" and "hunting". Segment 254c includes the word "about" and the alternate word "scouts", and segment 254d includes the word "11: 30" and the alternate word "7: 30".
Referring back to fig. 1, the speech recognizer module 130 may use the acoustic model and the language model to generate a word lattice 200 or otherwise recognize a plurality of candidate transcriptions 135 of the utterance 101 based on the audio data 112. The speech recognizer module 130 may also indicate which candidate transcription 135 the speech recognizer module 130 considers to be most likely correct, for example, by providing a likelihood score and/or ranking of the candidate transcriptions 135.
During stage (C), computing system 120 identifies a set of highest ranked candidate transcriptions 135 from within a set of candidate transcriptions received in word lattice 200. For example, using likelihood scores or ranking information from the speech recognizer module 130, the computing system 120 may select the n candidate transcriptions 135 with the highest likelihood, where n is an integer. In the illustrated example, the first five candidate transcriptions (e.g., the five indicated as most likely correct) are selected as a set of highest ranked candidate transcriptions 135, 135 a-n. In the example shown, the set of highest ranked candidate transcriptions 135 includes the words "write", "bite", "sight", "night", and "kite" ranked in order from highest to lowest. It is worth noting that the candidate transcription 135 of "kite" is ranked last, even though this is the word actually spoken by the user 10 in the recorded utterance 101. In other words, if the highest ranked candidate transcription 135 output from the speech recognizer module 130 were selected as the speech recognition result, the word "write" would be erroneously selected instead of selecting the word "kill".
In some examples, the speech recognizer module 130 generates an initial set of candidate transcriptions 135 for the utterance 101 based on the audio data 112, whereby each candidate transcription 135 in the initial set is associated with a corresponding likelihood score that indicates a likelihood that the candidate transcription 135 is correct. Thereafter, the speech recognizer module 130 ranks the candidate transcriptions 135 in the initial set based on likelihood scores (e.g., from most likely to least likely), and stage (C) determines a plurality of candidate transcriptions 135 for the utterance as the N candidate transcriptions 135 in the initial set of candidate transcriptions associated with the highest likelihood scores. Here, the identified plurality of candidate transcriptions 135a-n are ranked according to the associated likelihood scores.
In further examples, after speech recognizer module 130 generates an initial set of candidate transcriptions 135, speech recognizer module 130 identifies two or more candidate transcriptions 135 of the initial set that are associated with likelihood scores that satisfy a likelihood threshold. Here, stage (C) determines a plurality of candidate transcriptions 135a-n for the utterance as the two or more identified candidate transcriptions 135 in the initial set that are associated with likelihood scores that satisfy a likelihood threshold. In these examples, candidate transcriptions 135 associated with low likelihood scores are excluded from consideration.
During stage (D), computing system 120 provides each candidate transcription 135 to a text-to-speech (TTS) module 140 (e.g., a speech synthesizer or speech synthesis module). For each candidate transcription 135a-n identified at stage (C), TTS module 140 is configured to generate a synthesized speech representation 145, 145a-n of the corresponding candidate transcription 135 a-n. For example, TTS module 140 may convert text from each candidate transcription 135 into a corresponding synthesized speech representation 145.
At stage (E), computing system 120 provides video data 114 representing the movement of the lips of user 10 and synthesized speech representation 145 output from TTS module 140 for each candidate transcription 135 as feature inputs to consistency score determiner 150. In turn, the consensus score determiner 150 is configured to determine consensus scores 155, 155a-n for the candidate transcriptions 135a-n as feature outputs. The consistency score determiner 150 may determine consistency scores 155 in parallel, determine each consistency score 155 individually, or a combination thereof.
Prior to determining the consistency score 155, the consistency score determiner 150 may initially process each synthesized speech representation 145 and the video data 114 to time align each synthesized speech representation 145 with the video data 114. That is, the consistency score determiner 150 may apply any technique to identify and tag frames of the video data 114 that contain the movement of the lips of the user 10 when speaking the utterance 101, and time align each synthesized speech representation 145 with the video data 114 using the identified and tagged frames.
In the illustrated example, the consistency score determiner 150 includes a consistency score model 152 (FIG. 3), the consistency score model 152 being trained to predict a consistency score 155 for a corresponding candidate transcription 135 based on the degree to which the synthetic speech representation 145 for the corresponding candidate transcription 135 matches the movement of the lips of the user 10 speaking the utterance 101. In essence, the consistency score model 152 is trained to discriminate between a synthesized speech representation 145 that matches the video data 114 representing movement of the lips and a synthesized speech representation 145 that does not match the video data 114 representing movement of the lips.
In some examples, the consistency score 155 output from the consistency score determiner 150 (i.e., using the consistency score model 152) includes a binary value, where a "1" represents a synthesized speech representation 145 that matches the movement of the lips of the user 10 represented by the video data 114 and a "0" represents a synthesized speech representation that does not match the movement of the lips of the user 10 represented by the video data 114. In further examples, the consistency score 155 is a numerical value, e.g., from zero to one, indicating the degree to which the synthesized speech representation 145 of the corresponding candidate transcription 135 matches the movement of the lips. For example, a consistency score 155 having a value closer to 1 is more indicative of a synthesized speech representation 145 matching the movement of the lips than a consistency score 155 having a value closer to zero. In some scenarios, the consistency score 155 having a value that meets (e.g., exceeds) a consistency score threshold indicates a synthesized speech representation 145 that matches the movement of the lips of the user 10 speaking the utterance 101. In these scenarios, a binary value representing a consistency score 155 may be output from consistency score determiner 150 based on whether the initial numerical value satisfies a consistency score threshold.
Each consistency score 155 represents a re-scoring of the plurality of candidate transcriptions 135 identified at stage (C) by indicating a degree to which the synthesized speech representation 145 for a given candidate transcription 135 matches the movement of the lips of the user 10 in the video data 114. The degree to which the video data 114 representing the movement of the lips of the user 10 matches the synthesized speech representation 145 of the candidate transcription 135 may be based on, for example, whether the sequence of speech features in the synthesized speech representation 145 completely or partially matches the sequence of lip positions and shapes of the user 10 in the video data 114. For example, at a given time instance, when the lip position/shape of the user 10 indicates the user's mouth that was open when the synthesized speech representation 145 pronounces a vowel, the model 152 will recognize a match. Similarly, if at another instance in time, the conformity score model 152 would not recognize a match when the lip position of the user 10 indicates the user's mouth as being open while the synthesized phonetic representation 145 is pronouncing a "B" consonant.
FIG. 3 illustrates an example model trainer 300 for generating the consistency score model 152. In the illustrated example, the model trainer 300 is trained on a plurality of training examples 302, including a positive training example 302a and a negative training example 302 b. Each training example 302a contains training audio data 112T representing a speech utterance and training video data 114T representing movement of a speaker's lips that match (e.g., are synchronized with) the speech utterance. That is, the model trainer 300 feeds the consistency score model 152 with the training examples 302a to teach examples to the consistency score determiner 150, where the consistency score determiner 150 should output a consistency score 155, the consistency score 155 indicating a match/synchronization between the synthesized speech representation and lip movements/movements in the video data 114.
In contrast, each negative training example 302b contains training audio data 112T representing a speech utterance and training video data 114T representing movement of a speaker's lips that does not match (e.g., is not synchronized with) the speech utterance. That is, the model trainer 300 feeds the negative training examples 302b to the consistency score model 152 to teach examples to the consistency score determiner 150, where the consistency score determiner 150 should output a consistency score 155, the consistency score 155 indicating a mismatch and dyssynchrony between the synthesized speech representation and lip movements/movements in the video data 114.
By training the model trainer 300 on the positive training example 302a and the negative training example 302b to generate the consistency score model 152, the consistency score determiner 150 is taught to discriminate between a synthesized speech representation that matches/synchronizes with the movement of the lips represented by the video data 114 and the synthesized speech representation 145 that does not match or synchronize with the motion/movement of the lips represented by the video data 114. Accordingly, the consistency score determiner 150 may use the trained consistency score model 152 to generate a consistency score 155 that indicates a degree to which the synthesized speech representation 145 of the corresponding candidate transcription 135 matches the movement of the lips represented by the video data 114.
In some examples, the training audio data 112T includes the artificially generated utterance 101 of speech. In other examples, the training audio data 112T includes the synthesized utterance 145 (e.g., generated by the TTS module 140). In other examples, the training audio data 112T includes both the synthesized utterance 145 and the artificially generated utterance 101.
In some configurations, model trainer 300 is configured to separate training examples 302 into training and evaluation sets (e.g., 90% training and 10% evaluation). With these sets, model trainer 300 trains consistency score model 152 using training examples 302 until the performance of consistency score model 152 on the evaluation set stops degrading. Once performance on the evaluation set stops degrading, the consistency score model 152 is ready for modeling, where the consistency score model 152 allows the consistency score determiner 150 to output consistency scores 155, each consistency score 155 indicating a likelihood that the synthesized speech representation 145 of the corresponding candidate transcription 135 matches the movement of the lips of the user 10 when the user speaks the utterance 101.
Referring back to FIG. 1, the consensus scores 155, 155a-n for the plurality of candidate transcripts 135 include 0.4 for "Write," 0.7 for "Bite," 0.3 for "Sight," 0.4 for "Night," and 0.9 for "Kite. Here, the candidate transcription 135 of "Kite" includes the highest consensus score 155 and is actually the word that the user 10 actually uttered in the utterance 101. At stage (F), re-ranker 160 receives the consensus scores 155 for the plurality of candidate transcriptions 135 from consensus score determiner 150 and outputs re-ranking results 165 for the plurality of candidate transcriptions 135 based on the consensus scores 155. In the illustrated example, the plurality of candidate transcriptions 135 are re-ranked from highest consensus score 155 to lowest consensus score 155. Thus, the computing system 120 generates (e.g., by the re-ranker 160) re-ranking results 165 associated with a new ranking that is different from the initial/original ranking indicated by the speech recognizer module 130 based only on the audio data 112.
In some examples, candidate transcriptions 135 associated with a higher rank identified at stage (C) may be ranked higher in re-ranking results 165 by re-ranker 160 in the event of a tie between two or more candidate transcriptions 135 having the same consensus score 155. That is, the re-ranker 160 may consider speech recognition features associated with the plurality of candidate transcriptions 135 generated by the speech recognizer module 130. The speech recognition features may include information generated by the language model at the speech recognizer 130 for a given candidate transcription, such as language model probabilities, locations in a ranking, number of tokens, or confidence scores from the speech recognizer module 130. In the case of a tie, the re-ranker 160 may additionally consider semantic features and/or speech recognition features. The semantic features may indicate information about pattern matching analysis, e.g., matching candidate transcriptions to grammars. For example, if a candidate transcription matches a popular speech action pattern, it has a better chance of being a correct recognition. Many voice queries are commands such as "show me movies by Jim Carey" or "open Web site".
At stage (G), the computing system 120 receives the re-ranked results 165 and selects a candidate transcription 135 associated with the highest consensus score 145 from the plurality of candidate transcriptions 135 for the utterance 101 as a speech recognition output 175 for the utterance 101. In the illustrated example, the candidate transcription 135 of the word "Kite" includes the highest consistency score equal to "0.9", and is therefore selected as the speech recognition output 175 for the utterance 101.
During stage (H), computing system 120 provides speech recognition output 175 to client device 110 over network 118. Client device 110 may then display speech recognition output 175 on a screen of client device 110 and/or perform actions/commands using speech recognition output 175. For example, client device 110 may submit speech recognition output 175 as a search query or otherwise use output 175. In further examples, the computing system 120 provides the speech recognition output 175 directly to another system to perform actions/commands related to the speech recognition output 175. For example, computing system 120 may provide speech recognition output 175 to a search engine to perform a search query using speech recognition output 175.
Although the candidate transcription 135 shown in fig. 1 is described as a single word for simplicity, it should be understood that the plurality of candidate transcriptions 135 and the synthesized speech representation 145 produced therefrom may include a plurality of words contained in one or more phrases, one or more sentences of the utterance 101, or even longer forms of the utterance recorded from a meeting or lecture. For example, the audio data 112 and the video data 114 may represent the entire query spoken by the user, and each candidate transcription 135 may be a respective candidate transcription of the audio data 112 as a whole.
The computing system 120 may include data processing hardware (e.g., a processor) 510 (fig. 5) and memory hardware 520 (fig. 5) in communication with the data processing hardware 510 and storing instructions that, when executed on the data processing hardware, cause the data processing hardware 510 to perform operations. For example, the data processing hardware 510 may execute the speech recognizer module 130, the TTS module 140, the consistency score determiner 150, and the re-ranker 160. In some implementations, all of the functionality of computing system 120 resides on client device 110. Advantageously, latency may be improved because the client device 110 does not have to transmit the audio and video data 112, 114 over the network 118 and wait to receive the resulting speech recognition output 175.
In some implementations, the functionality of computing system 120 described in fig. 1 is divided among client device 110 and computing system 120, whereby some operations are performed on client device 110 and other operations are performed remotely on computing system 120. For example, audio data analysis may be performed on computing system 120 (e.g., a cloud computing environment) such that client device 110 provides audio data 112 to speech recognizer module 130 to obtain a plurality of candidate transcription candidates 135 for utterance 101, and TTS module 140 may generate a corresponding synthesized speech representation 145 for each candidate transcription 135 of the plurality of candidate transcription candidates 135 for utterance 101. Instead of providing video data 114 representing the movement of the lips of the user 10 as the user 10 speaks the utterance 101, the client device 110 may execute a consistency score determiner 150 on the device. Thus, computing system 120 may transmit synthesized speech representations 145 for the plurality of candidate transcriptions 135 to client device 110 over network 118, whereby client device 110 is configured to obtain video data 114 and determine a consistency score 155 for each candidate transcription 135 of the plurality of candidate transcriptions 135 received from the computing system. As described in detail above, each consistency score 155 indicates a likelihood that the synthesized speech representation 145 of the corresponding candidate transcription 135 matches the movement of the lips of the user 10 as the user 10 speaks the utterance 101. Client device 110 may then select one of the plurality of candidate transcriptions 135 (e.g., the one associated with the highest consistency score 155) as speech recognition output 175 based on the consistency score determined on the device.
With this configuration, bandwidth requirements are reduced because the video data 114 is retained on the client device 110 without the need to transmit the video data 114 over the network 118 to the remote computing system 120. Furthermore, if the video data 114 is transmitted over the network 118, the video data 114 may need to be compressed by the client device 110 prior to transmission, thereby reducing the quality of the video data 114. Thus, another advantage of preserving the video data 114 and performing video data matching on the device is that the video data matching may use uncompressed (highest quality) video data 114. That is, the use of uncompressed video data makes it easier to identify matches/mismatches between the synthesized speech representation 145 and the video data 114.
Each of the plurality of candidate transcriptions 135 output by the speech recognizer module 130 for the utterance 101 may be associated with the same language. In some examples, at least one of the plurality of candidate transcriptions 135 of the utterance 101 is associated with a different language than the other candidate transcriptions 135. For example, the computing system 120 may not know the language of the utterance 101 a priori, and may rely on the speech recognizer module 130 to output a plurality of candidate transcriptions 135 divided between two or more different languages using different language models. In this case, the candidate transcription 135 associated with the correct language for the utterance 101 is identified/selected by comparing the corresponding synthesized speech representation 145 to the video data 114 representing the movement of the lips when the user 10 speaks the utterance 101. That is, the language of the candidate transcription 135 associated with the highest correspondence score 155 may be selected as the speech recognition output 175 to identify the correct language. Because the video data 114 has been used to determine the best matching synthesized speech representation 145, the associated candidate transcriptions 135 are more likely to be in the correct language.
Where certain implementations discussed herein may collect or use personal information about a user (e.g., user data extracted from other electronic communications, information about the user's social network, the user's location, the user's time, the user's biometric information, as well as the user's activities and demographic information, relationships between users, etc.), the user is provided with one or more opportunities to control whether information is collected, whether personal information is stored, whether personal information is used, and how information about the user is collected, stored, and used. That is, the systems and methods discussed herein collect, store, and/or use user personal information only upon receiving explicit authorization to do so from an associated user.
For example, the user is provided with control over whether the program or feature collects user information about that particular user or other users related to the program or feature. Each user whose personal information is to be collected is provided with one or more options to allow control over the collection of information associated with that user, whether information is collected and which portions of information are collected to provide permission or authorization. For example, one or more such control options may be provided to the user over a communications network. In addition, certain data may be processed in one or more ways before it is stored or used in order to delete personally identifiable information. As one example, the identity of the user may be processed such that no personal identity information can be determined. As another example, the geographic location of the user may be generalized to a larger area such that a particular location of the user cannot be determined.
FIG. 4 is a flow diagram of an example operational arrangement for a method 400 of re-scoring candidate transcriptions using audiovisual matching. At operation 402, the method 400 includes: audio data 112 corresponding to the utterance 101 spoken by the user 10 and video data 114 representing movement of lips of the user 10 as the user 10 speaks the utterance 101 are received at the data processing hardware 510. At operation 404, the method 400 includes obtaining, by the data processing hardware 510, a plurality of candidate transcriptions 135 for the utterance 101 based on the audio data 112. At operation 406, for each candidate transcription 135, the method 400 further includes: the synthesized speech representation 145 corresponding to the candidate transcription 135 is generated by the data processing hardware 510. At operation 408, for each candidate transcription 135, the method 400 further includes: a consistency score 155 is determined by the data processing hardware indicating the likelihood of: the synthesized speech representation 145 corresponding to the candidate transcription 135 matches the movement of the lips of the user 10 as the user 10 speaks the utterance 101. At operation 410, the method 400 includes: one of the plurality of candidate transcriptions 135 of the utterance 101 is selected as the speech recognition output 175 by the data processing hardware 510 based on the consensus scores 155 determined for the plurality of candidate transcriptions 135 of the utterance 101.
A software application (i.e., software resource) may refer to computer software that causes a computing device to perform tasks. In some examples, a software application may be referred to as an "application," app, "or" program. Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
The non-transitory memory may be a physical device used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device. The non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, Random Access Memory (RAM), Dynamic Random Access Memory (DRAM), Static Random Access Memory (SRAM), Phase Change Memory (PCM), and magnetic disks or tape.
FIG. 5 is a schematic diagram of an example computing device 500 that may be used to implement the systems and methods described in this document. Computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
The memory 520 stores information within the computing device 500 non-temporarily. The memory 520 may be a computer-readable medium, a volatile memory unit or a nonvolatile memory unit. Non-transitory memory 520 may be a physical device for storing programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by computing device 500. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, Random Access Memory (RAM), Dynamic Random Access Memory (DRAM), Static Random Access Memory (SRAM), Phase Change Memory (PCM), and magnetic disks or tape.
The storage device 530 is capable of providing mass storage for the computing device 500. In some implementations, the storage device 530 is a computer-readable medium. In various different implementations, the storage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices (including devices in a storage area network or other configurations). In a further implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 520, the storage device 530, or memory on processor 510.
The high speed controller 540 manages bandwidth-intensive operations for the computing device 500, while the low speed controller 560 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 540 is coupled to memory 520, display 580 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 550, which may accept various expansion cards (not shown). In some implementations, low-speed controller 560 is coupled to storage device 530 and low-speed expansion port 590. The low-speed expansion port 590, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device (e.g., a switch or router), for example, through a network adapter.
As shown, computing device 500 may be implemented in a number of different forms. For example, it may be implemented as a standard server 500a or multiple times in a group of such servers 500a, as a laptop computer 500b, or as part of a rack server system 500 c.
Various implementations of the systems and techniques described here can be realized in digital electronic and/or optical circuits, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from and to transmit data and instructions to: a storage system, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, non-transitory computer-readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
The processes and logic flows described in this specification can be performed by one or more programmable processors (also known as data processing hardware) executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and performed by, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Typically, a computer will also include or be operatively coupled to receive data from or transfer data to: one or more mass storage devices for storing data (e.g., magnetic, magneto-optical disks, or optical disks). However, a computer need not have such a device. Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example: semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CDROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, one or more aspects of the disclosure may be implemented on a computer having: a display device for displaying information to a user, for example, a CRT (cathode ray tube), LCD (liquid crystal display) monitor or touch screen; and, optionally, a keyboard and a pointing device, such as a mouse or a trackball, by which a user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and may receive input from the user in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on the user's client device in response to a request received from the web browser.
Many implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (26)
1. A method (400) comprising:
receiving, at data processing hardware (510), audio data (112) corresponding to an utterance (101) spoken by a user (10);
receiving, at the data processing hardware (510), video data (114) representing movement of a user's lips while the user (10) is speaking the utterance (101);
obtaining, by the data processing hardware (510), a plurality of candidate transcriptions (135) of the utterance (101) based on the audio data (112);
for each candidate transcription (135) of a plurality of candidate transcriptions (135) of the utterance (101):
generating, by the data processing hardware (510), a synthesized speech representation (145) of the corresponding candidate transcription (135); and
determining, by the data processing hardware (510), a consistency score (155), the consistency score (155) indicating a likelihood that a synthesized speech representation (145) of the corresponding candidate transcription (135) matches movement of the user's lips as the user (10) speaks the utterance (101); and
selecting, by the data processing hardware (510), one of a plurality of candidate transcriptions (135) of the utterance (101) as a speech recognition output (175) based on the coherence scores (155) determined for the plurality of candidate transcriptions (135) of the utterance (101).
2. The method (400) of claim 1, wherein determining the consistency score (155) comprises:
providing a synthesized speech representation (145) of the corresponding candidate transcription (135) and video data (114) representing movement of the user's lips as feature inputs to a consistency score model (152); and
determining the consistency score (155) from the consistency score model (152) as a feature output based on a degree to which a synthesized speech representation (145) of the corresponding candidate transcription (135) matches movement of lips of the user (10).
3. The method (400) of claim 2, wherein the consistency score model (152) is trained on a plurality of training examples (302), the plurality of training examples (302) comprising:
a training example (302a) comprising audio data (112T) representing an utterance of speech and video data (114T) representing movement of a speaker's lips that match the utterance of speech; and
a negative training example (302b) includes audio data (112T) representing a speech utterance and video data (114T) representing movement of a speaker's lips that does not match the utterance (101) of speech.
4. The method (400) according to any one of claims 1-3, wherein selecting one of a plurality of candidate transcriptions (135) of the utterance (101) as the speech recognition output (175) includes: selecting, from a plurality of candidate transcriptions (135) of the utterance (101), a candidate transcription (135) associated with a highest consensus score (155) as a speech recognition output (175) of the utterance (101).
5. The method (400) according to any one of claims 1-4, wherein obtaining a plurality of candidate transcriptions (135) of the utterance (101) includes:
generating, using a speech recognizer module (130), an initial set of candidate transcriptions (135) of the utterance (101) based on the audio data (112), each candidate transcription (135) in the initial set of candidate transcriptions (135) being associated with a corresponding likelihood score indicating a likelihood that the candidate transcription (135) is correct;
ranking candidate transcriptions (135) in the initial set of candidate transcriptions (135) based on the likelihood scores; and
determining a plurality of candidate transcriptions (135) of the utterance (101) as N candidate transcriptions (135) in an initial set of candidate transcriptions (135) associated with a highest likelihood score, the identified plurality of candidates being ranked according to the associated likelihood scores.
6. The method (400) of claim 5, further comprising: prior to selecting one of the multiple transcriptions of the utterance (101), re-ranking, by the data processing hardware (510), the multiple candidate transcriptions (135) of the utterance (101) based on a consensus score (155) determined for the multiple candidate transcriptions (135) of the utterance (101).
7. The method (400) according to any one of claims 1-6, wherein obtaining a plurality of candidate transcriptions (135) of the utterance (101) includes:
generating, using a speech recognizer module (130), an initial set of candidate transcriptions (135) of the utterance (101) based on the audio data (112), each candidate transcription (135) in the initial set of candidate transcriptions (135) being associated with a corresponding likelihood score indicating a likelihood that the candidate transcription (135) is correct;
identifying, in the initial set of candidate transcriptions (135), two or more candidate transcriptions (135) associated with likelihood scores that satisfy a likelihood threshold; and
determining a plurality of candidate transcriptions (135) of the utterance (101) as the identified two or more candidate transcriptions (135) in the initial set of candidate transcriptions (135) associated with the likelihood score satisfying the likelihood threshold.
8. The method (400) of any of claims 1-7, wherein each of the plurality of candidate transcriptions (135) of the utterance (101) is associated with a same language.
9. The method (400) of any of claims 1-8, wherein at least one of the plurality of candidate transcriptions (135) of the utterance (101) is associated with a language different from the other plurality of candidate transcriptions (135).
10. The method (400) according to any one of claims 1-9, wherein receiving audio data (112) corresponding to an utterance (101) spoken by the user (10) includes receiving the audio data (112) from a client device (110) associated with the user (10), the client device (110) being in communication with one or more audio capture devices (103), the audio capture devices (103) being configured to capture the audio data (112) corresponding to the utterance (101).
11. The method (400) of claim 10, wherein the data processing hardware (510) resides on the client device (110).
12. The method (400) of claim 10 or 11, wherein the client device (110) is remote from the data processing hardware (510) and in communication with the data processing hardware (510) via a network (118).
13. The method (400) according to any one of claims 10-12, wherein receiving video data (114) representing movement of the user's lips while the user (10) is speaking the utterance (101) includes: receiving the video data (114) from a client device (110) associated with the user (10), the client device (110) comprising one or more video capture devices (105), the video capture devices (105) configured to record video data (114) representing movement of the user's lips while the user (10) is speaking the utterance (101).
14. A system (100) comprising:
data processing hardware (510); and
memory hardware (520) in communication with the data processing hardware (510), the memory hardware (520) storing instructions that, when executed on the data processing hardware (510), cause the data processing hardware (510) to perform operations comprising:
receiving audio data (112) corresponding to an utterance (101) spoken by a user (10);
receiving video data (114) representing movement of the user's lips while the user (10) is speaking the utterance (101);
obtaining a plurality of candidate transcriptions (135) of the utterance (101) based on the audio data (112);
for each candidate transcription (135) of a plurality of candidate transcriptions (135) of the utterance (101):
generating a synthetic speech representation (145) of the corresponding candidate transcription (135); and
determining a consistency score (155), the consistency score (155) indicating a likelihood that a synthesized speech representation (145) of the corresponding candidate transcription (135) matches movement of the user's lips while the user (10) speaks the utterance (101); and
based on the consensus scores (155) determined for the plurality of candidate transcriptions (135) of the utterance (101), one of the plurality of candidate transcriptions (135) of the utterance (101) is selected as a speech recognition output (175).
15. The system (100) of claim 14, wherein determining the consistency score (155) comprises:
providing a synthesized speech representation (145) of the corresponding candidate transcription (135) and video data (114) representing movement of the user's lips as feature inputs to a consistency score model (152); and
determining the consistency score (155) from the consistency score model (152) as a feature output based on a degree to which a synthesized speech representation (145) of the corresponding candidate transcription (135) matches movement of the user's lips.
16. The system (100) according to claim 15, wherein the consistency score model (152) is trained on a plurality of training examples (302), the plurality of training examples (302) including:
a training example (302a) comprising audio data (112T) representing an utterance of speech and video data (114T) representing movement of a speaker's lips that match the utterance of speech; and
a negative training example (302b) includes audio data (112T) representing a speech utterance and video data (114T) representing movement of a lip of a speaker that does not match the utterance of speech.
17. The system (100) according to any one of claims 14-16, wherein selecting one of a plurality of candidate transcriptions (135) of the utterance (101) as the speech recognition output (175) includes: selecting, from the plurality of candidate transcriptions (135) of the utterance (101), a candidate transcription (135) that is associated with a highest agreement score (155) as the speech recognition output (175) of the utterance (101).
18. The system (100) according to any one of claims 14-17, wherein obtaining a plurality of candidate transcriptions (135) of the utterance (101) includes:
generating, using a speech recognizer module (130), an initial set of candidate transcriptions (135) of the utterance (101) based on the audio data (112), each candidate transcription (135) in the initial set of candidate transcriptions (135) being associated with a corresponding likelihood score indicating a likelihood that the candidate transcription (135) is correct;
ranking candidate transcriptions (135) in the initial set of candidate transcriptions (135) based on the likelihood scores; and
determining a plurality of candidate transcriptions (135) of the utterance (101) as N candidate transcriptions (135) in an initial set of candidate transcriptions (135) associated with a highest likelihood score, the identified plurality of candidates being ranked according to the associated likelihood scores.
19. The system (100) of claim 18, wherein the operations further comprise, prior to selecting one of the plurality of transcriptions of the utterance (101), re-ranking the plurality of candidate transcriptions (135) of the utterance (101) based on the consensus scores (155) determined for the plurality of candidate transcriptions (135) of the utterance (101).
20. The system (100) according to any one of claims 14-19, wherein obtaining a plurality of candidate transcriptions (135) of the utterance (101) includes:
generating, using a speech recognizer module (130), an initial set of candidate transcriptions (135) of the utterance (101) based on the audio data (112), each candidate transcription (135) in the initial set of candidate transcriptions (135) being associated with a corresponding likelihood score indicating a likelihood that the candidate transcription (135) is correct;
identifying, in the initial set of candidate transcriptions (135), two or more candidate transcriptions (135) associated with a likelihood score satisfying a likelihood threshold; and
determining a plurality of candidate transcriptions (135) of the utterance (101) as the identified two or more candidate transcriptions (135) in the initial set of candidate transcriptions (135) associated with the likelihood score satisfying the likelihood threshold.
21. The system (100) according to any one of claims 14-20, wherein each of the plurality of candidate transcriptions (135) of the utterance (101) is associated with a same language.
22. The system (100) according to any one of claims 14-21, wherein at least one of the plurality of candidate transcriptions (135) of the utterance (101) is associated with a language different from the other plurality of candidate transcriptions (135).
23. The system (100) according to any one of claims 14-22, wherein receiving audio data (112) corresponding to an utterance (101) spoken by the user (10) includes: receiving the audio data (112) from a client device (110) associated with the user (10), the client device (110) being in communication with one or more audio capture devices (103), the audio capture devices (103) being configured to capture audio data (112) corresponding to the utterance (101).
24. The system (100) of claim 23, wherein the data processing hardware (510) resides on the client device (110).
25. The system (100) according to claim 23 or 24, wherein the client device (110) is remote from the data processing hardware (510) and communicates with the data processing hardware (510) via a network (118).
26. The system (100) according to any one of claims 23-25, wherein receiving video data (114) representing movement of the user's lips while the user (10) is speaking the utterance (101) includes: receiving the video data (114) from a client device (110) associated with the user (10), the client device (110) comprising one or more video capture devices (103), the video capture devices (103) being configured to record video data (114) representing movement of the user's lips while the user (10) is speaking the utterance (101).
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/061967 WO2021101500A1 (en) | 2019-11-18 | 2019-11-18 | Rescoring automatic speech recognition hypotheses using audio-visual matching |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114730563A true CN114730563A (en) | 2022-07-08 |
Family
ID=68848446
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980102308.8A Pending CN114730563A (en) | 2019-11-18 | 2019-11-18 | Re-scoring automatic speech recognition hypotheses using audio-visual matching |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220392439A1 (en) |
EP (1) | EP4052254A1 (en) |
JP (1) | JP7265094B2 (en) |
KR (1) | KR20220090586A (en) |
CN (1) | CN114730563A (en) |
WO (1) | WO2021101500A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111225237B (en) * | 2020-04-23 | 2020-08-21 | 腾讯科技（深圳）有限公司 | Sound and picture matching method of video, related device and storage medium |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7761296B1 (en) * | 1999-04-02 | 2010-07-20 | International Business Machines Corporation | System and method for rescoring N-best hypotheses of an automatic speech recognition system |
US6219640B1 (en) * | 1999-08-06 | 2001-04-17 | International Business Machines Corporation | Methods and apparatus for audio-visual speaker recognition and utterance verification |
US7203648B1 (en) * | 2000-11-03 | 2007-04-10 | At&T Corp. | Method for sending multi-media messages with customized audio |
US6975991B2 (en) * | 2001-01-31 | 2005-12-13 | International Business Machines Corporation | Wearable display system with indicators of speakers |
US8009966B2 (en) * | 2002-11-01 | 2011-08-30 | Synchro Arts Limited | Methods and apparatus for use in sound replacement with automatic synchronization to images |
US8635066B2 (en) | 2010-04-14 | 2014-01-21 | T-Mobile Usa, Inc. | Camera-assisted noise cancellation and speech recognition |
US10332515B2 (en) * | 2017-03-14 | 2019-06-25 | Google Llc | Query endpointing based on lip detection |
US11200902B2 (en) | 2018-02-15 | 2021-12-14 | DMAI, Inc. | System and method for disambiguating a source of sound based on detected lip movement |
US10573312B1 (en) * | 2018-12-04 | 2020-02-25 | Sorenson Ip Holdings, Llc | Transcription generation from multiple speech recognition systems |
-
2019
- 2019-11-18 CN CN201980102308.8A patent/CN114730563A/en active Pending
- 2019-11-18 KR KR1020227020415A patent/KR20220090586A/en unknown
- 2019-11-18 EP EP19818393.1A patent/EP4052254A1/en active Pending
- 2019-11-18 US US17/755,972 patent/US20220392439A1/en active Pending
- 2019-11-18 WO PCT/US2019/061967 patent/WO2021101500A1/en unknown
- 2019-11-18 JP JP2022528637A patent/JP7265094B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
JP7265094B2 (en) | 2023-04-25 |
US20220392439A1 (en) | 2022-12-08 |
WO2021101500A1 (en) | 2021-05-27 |
JP2022546640A (en) | 2022-11-04 |
KR20220090586A (en) | 2022-06-29 |
EP4052254A1 (en) | 2022-09-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20210350786A1 (en) | Speech Recognition Using Unspoken Text and Speech Synthesis | |
US10235994B2 (en) | Modular deep learning model | |
KR101780760B1 (en) | Speech recognition using variable-length context | |
CN105654940B (en) | Speech synthesis method and device | |
KR20220054704A (en) | Contextual biasing for speech recognition | |
US10152298B1 (en) | Confidence estimation based on frequency | |
EP4078572B1 (en) | Proper noun recognition in end-to-end speech recognition | |
WO2023055410A1 (en) | Contrastive siamese network for semi-supervised speech recognition | |
WO2023048746A1 (en) | Speaker-turn-based online speaker diarization with constrained spectral clustering | |
CN111126084B (en) | Data processing method, device, electronic equipment and storage medium | |
Moyal et al. | Phonetic search methods for large speech databases | |
Basak et al. | Challenges and Limitations in Speech Recognition Technology: A Critical Review of Speech Signal Processing Algorithms, Tools and Systems. | |
CN117099157A (en) | Multitasking learning for end-to-end automatic speech recognition confidence and erasure estimation | |
US20140142925A1 (en) | Self-organizing unit recognition for speech and other data series | |
JP7265094B2 (en) | Rescoring Automatic Speech Recognition Hypotheses Using Audio-Visual Matching | |
WO2023200946A1 (en) | Personalizable probabilistic models | |
Rasipuram et al. | Grapheme and multilingual posterior features for under-resourced speech recognition: a study on scottish gaelic | |
US20230237987A1 (en) | Data sorting for generating rnn-t models | |
Chung et al. | Unsupervised discovery of structured acoustic tokens with applications to spoken term detection | |
JP2008233782A (en) | Pattern matching device, program, and method | |
US20230186898A1 (en) | Lattice Speech Corrections | |
US11580956B2 (en) | Emitting word timings with end-to-end models | |
JP7485858B2 (en) | Speech individuation and association training using real-world noise | |
US20220310061A1 (en) | Regularizing Word Segmentation | |
Soundarya et al. | An Investigational Analysis of Automatic Speech Recognition on Deep Neural Networks and Gated Recurrent Unit Model |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |