EP3593323B1 - High speed, high-fidelity face tracking - Google Patents
High speed, high-fidelity face tracking Download PDFInfo
- Publication number
- EP3593323B1 EP3593323B1 EP18746325.2A EP18746325A EP3593323B1 EP 3593323 B1 EP3593323 B1 EP 3593323B1 EP 18746325 A EP18746325 A EP 18746325A EP 3593323 B1 EP3593323 B1 EP 3593323B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- face
- depth
- model mesh
- face model
- vertices
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 239000013598 vector Substances 0.000 claims description 28
- 230000014509 gene expression Effects 0.000 claims description 15
- 238000000034 method Methods 0.000 claims description 14
- 230000001815 facial effect Effects 0.000 claims description 7
- 230000006870 function Effects 0.000 description 15
- 230000008901 benefit Effects 0.000 description 8
- 238000010586 diagram Methods 0.000 description 6
- 230000000694 effects Effects 0.000 description 4
- 238000005457 optimization Methods 0.000 description 4
- 238000009877 rendering Methods 0.000 description 4
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 230000003190 augmentative effect Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 230000008921 facial expression Effects 0.000 description 2
- 238000012545 processing Methods 0.000 description 2
- 230000002123 temporal effect Effects 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 238000007476 Maximum Likelihood Methods 0.000 description 1
- 230000004075 alteration Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 238000013016 damping Methods 0.000 description 1
- 210000005069 ears Anatomy 0.000 description 1
- 230000004886 head movement Effects 0.000 description 1
- 238000003384 imaging method Methods 0.000 description 1
- 238000002372 labelling Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 230000008569 process Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000012549 training Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
- G06T7/246—Analysis of motion using feature-based methods, e.g. the tracking of corners or segments
- G06T7/251—Analysis of motion using feature-based methods, e.g. the tracking of corners or segments involving models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/012—Head tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T17/00—Three dimensional [3D] modelling, e.g. data description of 3D objects
- G06T17/20—Finite element generation, e.g. wire-frame surface description, tesselation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/60—Type of objects
- G06V20/64—Three-dimensional objects
- G06V20/647—Three-dimensional objects by matching two-dimensional images to three-dimensional objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10028—Range image; Depth image; 3D point clouds
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20076—Probabilistic image processing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30196—Human being; Person
- G06T2207/30201—Face
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30204—Marker
Definitions
- the present disclosure relates generally to imagery capture and processing and more particularly to face tracking using captured imagery.
- Face tracking allows facial expressions and head movements to be used as an input mechanism for virtual reality and augmented reality systems, thereby supporting a more immersive user experience.
- a conventional face tracking system captures images and depth data of the user's face and fits a generative model to the captured image or depth data. To fit the model to the captured data, the face tracking system defines and optimizes an energy function to find a minimum that corresponds to the correct face pose.
- conventional face tracking systems typically have accuracy and latency issues that can result in an unsatisfying user experience.
- the problem of pose estimation is cast as that of label assignment with these constraints.
- Major parts of the human upper body are labeled by this process.
- the second step estimates joint positions optimally based on kinematic constraints using dense correspondences between depth profile and human model parts.
- the proposed framework is shown to overcome some issues of existing approaches for human pose tracking using similar types of data streams. Performance comparison with motion capture data is presented to demonstrate the accuracy of our approach.
- FIGs. 1-4 illustrate techniques for estimating a pose of a face by fitting a generative face model mesh to a depth map of the face, based on vertices of the face model mesh that are estimated to be visible from the point of view of a depth camera.
- a face tracking module receives a depth image of a face from a depth camera and generates a depth map of the face based on the depth image.
- the face tracking module identifies a pose of the face by fitting a face model mesh to the pixels of a depth map that correspond to the vertices of the face model mesh that are estimated to be visible from the point of view of the depth camera.
- the face tracking module estimates which vertices of the face model mesh are visible from the point of view of the depth camera by assuming that the face model mesh is largely convex (i.e., the face model mesh is more convex than not). In some embodiments, the face tracking module estimates which vertices of the face model mesh are visible from the point of view of the depth camera by estimating the degree to which a vector that is normal to each vertex of the face model mesh is facing toward or away from the depth camera.
- the face tracking module does not associate those vertices of the face model mesh for which associations with the depth image should not exist (i.e., for vertices that are not visible from the point of view of the depth camera). Excluding such vertices improves accuracy of the facial pose estimation, because if those vertices were included, an energy function used to estimate the pose would become inaccurate, as the energy function would assign a high energy to a correct pose.
- the face model mesh is parameterized by a set of identity and expression coefficients that indicate how to non-rigidly deform the vertices of the face model mesh to fit the depth map.
- the face tracking module bicubically interpolates the depth map to smooth intersections at pixel boundaries. The face tracking module adjusts the identity and expression coefficients to better match the depth map. The face tracking module then minimizes an energy function based on the distance of each visible vertex of the face model mesh to the depth map to identify the face model mesh that most closely approximates the pose of the face.
- FIG.1 illustrates a face tracking system 100 configured to support face tracking functionality for AR/VR applications, using depth sensor data in accordance with at least one embodiment of the present disclosure.
- the face tracking system 100 is an electronic device that can include a user-portable mobile device, such as a tablet computer, computing-enabled cellular phone (e.g., a "smartphone"), a head-mounted display (HMD), a notebook computer, a personal digital assistant (PDA), a gaming system remote, a television remote, camera attachments with or without a screen, and the like.
- the face tracking system 100 can include another type of mobile device, such as an automobile, robot, remote-controlled drone or other airborne device, and the like.
- the face tracking system 100 is generally described herein in the example context of a mobile device, such as a tablet computer or a smartphone; however, the face tracking system 100 is not limited to these example implementations.
- the face tracking system 100 includes a face tracking module 110 for estimating a current pose 140 of a face 120 based on a depth image 115 captured by a depth camera 105 in accordance with at least one embodiment of the present disclosure.
- the depth camera 105 uses a modulated light projector (not shown) to project modulated light patterns into the local environment, and uses one or more imaging sensors 106 to capture reflections of the modulated light patterns as they reflect back from objects in the local environment 112.
- modulated light patterns can be either spatially-modulated light patterns or temporally-modulated light patterns.
- the captured reflections of the modulated light patterns are referred to herein as "depth images" 115 and are made up of a three-dimensional (3D) point cloud having a plurality of points.
- the depth camera 105 calculates the depths of the objects, that is, the distances of the objects from the depth camera 105, based on the analysis of the depth images 115.
- the face tracking module 110 receives a depth image 115 from the depth camera 105 and generates a depth map based on the depth image 115.
- the face tracking module 110 identifies a pose of the face 120 by fitting a face model mesh to the pixels of the depth map that correspond to the face 120.
- the face tracking module 110 leverages the parameters ⁇ inferred in the previous frame received from the depth camera.
- the model is parameterized by a set of identity coefficients ⁇ ⁇ ⁇ R H , a set of expression weights, or coefficients ⁇ ⁇ ⁇ R K , a three-dimensional (3D) position of the head t ⁇ ⁇ R 3 , and a quaternion indicating the 3D rotation of the head q ⁇ ⁇ R 4 .
- the identity and expression coefficients indicate how to non-rigidly deform the 3D positions (vertices) of the face model mesh to fit corresponding pixels of the depth map.
- the face model mesh is a triangular mesh model.
- the face tracking module 110 models the deformation of the 3D positions of the face model mesh using a bi-linear (PCA) basis of the N 3D vertex positions
- V ⁇ ⁇ R 3 ⁇ N represents the mean face
- the face tracking module 110 estimates the parameters ⁇ of the face model mesh given the depth image based on a probabilistic inference problem in which it solves
- the face tracking module 110 includes only the vertices of the face model mesh that are assumed to be visible from the point of view of the depth camera 105, and bicubically interpolates the depth map associated with , allowing the face tracking module 110 to jointly optimize the pose and blendshape estimation of the face 120 using a smooth and differentiable energy. Based on the energy, the face tracking module estimates a current pose 140 of the face 120.
- the face tracking module 110 uses the current pose estimate 140 to update graphical data 135 on a display 130.
- the display 130 is a physical surface, such as a tablet, mobile phone, smart device, display monitor, array(s) of display monitors, laptop, signage and the like or a projection onto a physical surface.
- the display 130 is planar.
- the display 130 is curved.
- the display 130 is a virtual surface, such as a three-dimensional or holographic projection of objects in space including virtual reality and augmented reality.
- the virtual surface is displayed within an HMD of a user. The location of the virtual surface may be relative to stationary objects (such as walls or furniture) within the local environment 112 of the user.
- FIG. 2 is a diagram illustrating the face tracking module 110 of the face tracking system 100 of FIG. 1 in accordance with at least one embodiment of the present disclosure.
- the face tracking module 110 includes a memory 205, a visibility estimator 210, an energy minimizer 215, a landmarks module 220, and a regularizer 225. Each of these modules represents hardware, software, or a combination thereof, configured to execute the operations as described herein.
- the face tracking module 110 is configured to receive a depth image 115 from the depth camera (not shown) and to generate a current pose estimate 140 based on the depth image 115.
- the memory 205 is a memory device generally configured to store data, and therefore may be a random access memory (RAM) memory module, non-volatile memory device (e.g., flash memory), and the like.
- RAM random access memory
- non-volatile memory device e.g., flash memory
- the memory 205 may form part of a memory hierarchy of the face tracking system 100 and may include other memory modules, such as additional caches not illustrated at FIG. 1 .
- the memory 205 is configured to receive and store the depth image 115 from the depth camera (not shown).
- the visibility estimator 210 is a module configured to estimate whether a vertex of the face model mesh is visible from the point of view of the depth camera by determining to what degree the associated normal is facing toward or away from the depth camera.
- S n ⁇ 1 1 + e ⁇ N n ⁇ T 001 T ⁇ + v where N n ( ⁇ ) is the normal vector of vertex n.
- the parameters ⁇ and v respectively control the curvature and where the value 0.5 is reached.
- the energy minimizer 215 is a module configured to formulate and minimize an energy function describing the difference between the face model mesh and the depth map of the face.
- D ⁇ V n ⁇ is a piecewise constant mapping and is usually held fixed during optimization.
- obtaining the set O ( ⁇ ) requires an explicit rendering and endows the function with discontinuities.
- (6) is only smooth and differentiable once each vertex is associated with a specific depth value. In such a case, rendering and explicit correspondences must be re-established in closed form every time the pose ⁇ is updated.
- the energy minimizer allows ( ⁇ ) to bicubically interpolate the depth map associated with , such that the energy is fully differentiable and well defined.
- the landmarks module 220 is a module configured to detect and localize distinctive features (e.g., nose tip, eye corners), referred to as landmarks, of human faces. Landmarks provide strong constraints, both for the general alignment of the face and for estimating the identity and expression of the face. However, detected landmarks can be slightly incorrect, or may be estimated if they are not directly visible by the depth camera 105, resulting in residuals in the image domain.
- the regularizer 225 is a module configured to adjust the identity and expression weights to avoid over-fitting the depth map.
- the regularizer 225 normalizes the eigen-vectors (blendshapes) to provide a standard normal distribution. While expression coefficients generally do not follow a Gaussian distribution, identity parameters roughly do.
- the regularizer 225 performs statistical regularization by minimizing the L 2 norm of the identity parameters.
- the weights adjustor 225 therefore effectively encourages solutions to be close to the maximum likelihood estimation (MLE) of the multivariate Normal distribution, which is the mean face.
- MLE maximum likelihood estimation
- the regularizer 225 incorporates temporal regularization overall the entries of ⁇ during the joint optimization by adding in the following temporal regularization term to the energy:
- E temp ⁇ ⁇ id ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ 2 2 + ⁇ exp ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ 2 2 ⁇ trans ⁇ t ⁇ ⁇ ⁇ t ⁇ ⁇ 2 2 + ⁇ rot ⁇ q ⁇ ⁇ ⁇ q ⁇ ⁇ 2 2
- q ⁇ ⁇ R 4 is the sub-vector of rotational parameters in quaternion form
- ⁇ is the solution from the previous frame.
- the face tracking module 110 computes the Jacobian J ⁇ ⁇ R D ⁇ D and performs Levenberg updates (which are variants of Gauss-Newton updates) as ⁇ ⁇ ⁇ + J ⁇ T J ⁇ + ⁇ I D ⁇ D ⁇ 1 J T ⁇ r ⁇ where ⁇ is a damping term that can be increasingly raised when steps fail in order to achieve very small gradient descent like updates.
- the face tracking module 110 initializes ⁇ ⁇ ⁇ using the parameters from the previous frame. In some embodiments, the face tracking module 110 performs this Levenberg optimization on a GPU. The face tracking module 110 generates a current pose estimate 140 of the face based on the optimized energy.
- FIG. 3 illustrates the face tracking module 110 of FIGs. 1 and 2 estimating the visibility of vertices of a face model mesh 305 in poses 310 and 320 in accordance with at least one embodiment of the present disclosure.
- the face model mesh 305 includes a plurality of vertices, each of which the face tracking module 110 associates with a normal vector.
- the visibility estimator 210 estimates the degree to which normal vectors 312, 314, 316, and 318 are pointing toward the depth camera (not shown). This is based on the assumption that the face is largely convex, and thus normal vectors pointing toward the camera can typically be assumed to be visible. This assumption is violated only occasionally around the non-convex regions beside the nose and the ears.
- the visibility estimator 210 assigns a value to each vertex of the face model mesh 305 based on whether the normal vector associated with the vertex is estimated to face toward or away from the depth camera. For example, if 90° is considered directly facing (orthogonal to) the depth camera, and -90° is considered facing directly away from the depth camera, the visibility estimator 210 assigns a first value (e.g., 1) indicating that a normal vector associated with a vertex is visible from the point of view of the depth camera, and assigns a second value (e.g., 0) indicating that a normal vector associated with a vertex is not visible from the point of view of the depth camera.
- a first value e.g., 1
- a second value e.g., 0
- the visibility estimator 210 estimates that the vector 312 is at a 45° angle to the depth camera, that vector 314 is at a 2° angle to the depth camera, that vector 316 is at a 2° angle to the depth camera, and that vector 318 is at a 10° angle to the depth camera.
- the visibility estimator 210 assigns a value of 1 to the vertices associated with each of vectors 312, 314, 316, and 318, because each vector is estimated to be pointing toward the depth camera.
- the face model mesh 305 is rotated to the left, such that the visibility estimator 210 estimates that vector 312 is at a 10° angle to the depth camera, that vector 314 is at a 20° angle to the depth camera, that vector 316 is at a -20° angle to the depth camera, and that vector 318 is at a -45° angle to the depth camera.
- the visibility estimator 210 therefore assigns a value of 1 to the vertices associated with each of vectors 312 and 314, because vectors 312 and 314 are estimated to be pointing toward the depth camera.
- the visibility estimator assigns a value of 0 to the vertices associated with each of vectors 316 and 318 when the face model mesh 305 is in pose 320, because vectors 316 and 318 are estimated to be pointing away from the depth camera.
- the face tracking module 110 can smoothly turn on and off data terms of the energy function without rendering the face model mesh 305.
- FIG. 4 is a flow diagram illustrating a method 400 of estimating a pose of a face in accordance with at least one embodiment of the present disclosure.
- the face tracking module 110 of FIGs. 1 and 2 receives captured depth data from the depth camera 105.
- the face tracking module 110 generates a depth map of the face 120 based on the captured depth data.
- the visibility estimator 210 estimates the visibility of each vertex of a face model mesh 305 based on an estimate of whether a normal vector associated with each vertex of the face model mesh 305 points toward or away from the depth camera 105.
- the regularizer 225 adjusts identity and expression weights of the face model mesh 305 to avoid over-fitting to the depth map.
- the energy minimizer 215 bicubically interpolates the pixels of the depth map to smooth the energy function as features move from one pixel to another.
- the energy minimizer 215 defines and minimizes an energy function that fits the face model mesh 305 to the depth map.
- the energy minimizer 215 leverages a pose of a frame of the depth camera immediately preceding a current frame to optimize the energy function.
- certain aspects of the techniques described above may implemented by one or more processors of a processing system executing software.
- the software comprises one or more sets of executable instructions stored or otherwise tangibly embodied on a non-transitory computer readable storage medium.
- the software can include the instructions and certain data that, when executed by the one or more processors, manipulate the one or more processors to perform one or more aspects of the techniques described above.
- the non-transitory computer readable storage medium can include, for example, a magnetic or optical disk storage device, solid state storage devices such as Flash memory, a cache, random access memory (RAM) or other non-volatile memory device or devices, and the like.
- the executable instructions stored on the non-transitory computer readable storage medium may be in source code, assembly language code, object code, or other instruction format that is interpreted or otherwise executable by one or more processors.
- a computer readable storage medium may include any storage medium, or combination of storage media, accessible by a computer system during use to provide instructions and/or data to the computer system.
- Such storage media can include, but is not limited to, optical media (e.g., compact disc (CD), digital versatile disc (DVD), Blu-Ray disc), magnetic media (e.g., floppy disc, magnetic tape, or magnetic hard drive), volatile memory (e.g., random access memory (RAM) or cache), non-volatile memory (e.g., read-only memory (ROM) or Flash memory), or microelectromechanical systems (MEMS)-based storage media.
- optical media e.g., compact disc (CD), digital versatile disc (DVD), Blu-Ray disc
- magnetic media e.g., floppy disc, magnetic tape, or magnetic hard drive
- volatile memory e.g., random access memory (RAM) or cache
- non-volatile memory e.g., read-only memory (ROM) or Flash memory
- MEMS microelectro
- the computer readable storage medium may be embedded in the computing system (e.g., system RAM or ROM), fixedly attached to the computing system (e.g., a magnetic hard drive), removably attached to the computing system (e.g., an optical disc or Universal Serial Bus (USB)-based Flash memory), or coupled to the computer system via a wired or wireless network (e.g., network accessible storage (NAS)).
- system RAM or ROM system RAM or ROM
- USB Universal Serial Bus
- NAS network accessible storage
Description
- The present disclosure relates generally to imagery capture and processing and more particularly to face tracking using captured imagery.
- Face tracking allows facial expressions and head movements to be used as an input mechanism for virtual reality and augmented reality systems, thereby supporting a more immersive user experience. A conventional face tracking system captures images and depth data of the user's face and fits a generative model to the captured image or depth data. To fit the model to the captured data, the face tracking system defines and optimizes an energy function to find a minimum that corresponds to the correct face pose. However, conventional face tracking systems typically have accuracy and latency issues that can result in an unsatisfying user experience. Document THIBAUT WEISE HAO ET AL, "Face/Off: Live Facial Puppetry", PROCEEDINGS OF THE 2009 ACM SIGGRAPH/EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION, 1 August 2009 (20090801), pages 7 - 16, XP055098528, DOI:10.1145/1599470.1599472 discloses a complete integrated system for live facial puppetry that enables high-resolution real-time facial expression tracking with transfer to another person's face. The system utilizes a real-time structured light scanner that provides dense 3D data and texture. A generic template mesh, fitted to a rigid reconstruction of the actor's face, is tracked offline in a training stage through a set of expression sequences. These sequences are used to build a person-specific linear face model that is subsequently used for online face tracking and expression transfer. Even with just a single rigid pose of the target face, convincing real-time facial animations are achievable. The actor becomes a puppeteer with complete and accurate control over a digital face. Document YOUDING ZHU ET AL: "Constrained Optimization for Human Pose Estimation from Depth Sequences", 18 November 2007 (20071118), COMPUTER VISION - ACCV 2007; [LECTURE NOTES IN COMPUTER SCIENCE], SPRINGER BERLIN HEIDELBERG, BERLIN, HEIDELBERG, PAGE(S) 408 - 418, XP019082392, ISBN 978-3-540-76385-7 discloses a new 2-step method for human upper-body pose estimation from depth sequences, in which coarse human part labeling takes place first, followed by more precise joint position estimation as the second phase. In the first step, a number of constraints are extracted from notable image features such as the head and torso. The problem of pose estimation is cast as that of label assignment with these constraints. Major parts of the human upper body are labeled by this process. The second step estimates joint positions optimally based on kinematic constraints using dense correspondences between depth profile and human model parts. The proposed framework is shown to overcome some issues of existing approaches for human pose tracking using similar types of data streams. Performance comparison with motion capture data is presented to demonstrate the accuracy of our approach.
- The solution is specified by a first independent claim referring to a method (claim 1) and a second dependent claim referring to an electronic device (claim 6). Dependent claims specify embodiments thereof.
- The present disclosure may be better understood, and its numerous features and advantages made apparent to those skilled in the art by referencing the accompanying drawings. The use of the same reference symbols in different drawings indicates similar or identical items.
-
FIG. 1 is a diagram illustrating a face tracking system estimating a current pose of a face based on a depth image in accordance with at least one embodiment of the present disclosure. -
FIG. 2 is a diagram illustrating a face tracking module of the face tracking system ofFIG. 1 configured to estimate a current pose of a face based on a depth image in accordance with at least one embodiment of the present disclosure. -
FIG. 3 is a diagram illustrating estimating visibility of vertices of a face model mesh in accordance with at least one embodiment of the present disclosure. -
FIG. 4 is a flow diagram illustrating a method of estimating a pose of a face in accordance with at least one embodiment of the present disclosure. - The following description is intended to convey a thorough understanding of the present disclosure by providing a number of specific embodiments and details involving estimating a pose of a face by fitting a generative face model mesh to a depth map based on vertices of the face model mesh that are estimated to be visible from the point of view of a depth camera. It is understood, however, that the present disclosure is not limited to these specific embodiments and details, which are examples only, and the scope of the disclosure is accordingly intended to be limited only by the appended claims. It is further understood that one possessing ordinary skill in the art, in light of known systems and methods, would appreciate the use of the disclosure for its intended purposes and benefits in any number of alternative embodiments, depending upon specific design and other needs, as long as these alternative embodiments are within the scope of the appended claims.
-
FIGs. 1-4 illustrate techniques for estimating a pose of a face by fitting a generative face model mesh to a depth map of the face, based on vertices of the face model mesh that are estimated to be visible from the point of view of a depth camera. A face tracking module receives a depth image of a face from a depth camera and generates a depth map of the face based on the depth image. The face tracking module identifies a pose of the face by fitting a face model mesh to the pixels of a depth map that correspond to the vertices of the face model mesh that are estimated to be visible from the point of view of the depth camera. The face tracking module estimates which vertices of the face model mesh are visible from the point of view of the depth camera by assuming that the face model mesh is largely convex (i.e., the face model mesh is more convex than not). In some embodiments, the face tracking module estimates which vertices of the face model mesh are visible from the point of view of the depth camera by estimating the degree to which a vector that is normal to each vertex of the face model mesh is facing toward or away from the depth camera. By including only the vertices of the face model mesh that are estimated to be visible, the face tracking module does not associate those vertices of the face model mesh for which associations with the depth image should not exist (i.e., for vertices that are not visible from the point of view of the depth camera). Excluding such vertices improves accuracy of the facial pose estimation, because if those vertices were included, an energy function used to estimate the pose would become inaccurate, as the energy function would assign a high energy to a correct pose. - In some embodiments, the face model mesh is parameterized by a set of identity and expression coefficients that indicate how to non-rigidly deform the vertices of the face model mesh to fit the depth map. In some embodiments, the face tracking module bicubically interpolates the depth map to smooth intersections at pixel boundaries. The face tracking module adjusts the identity and expression coefficients to better match the depth map. The face tracking module then minimizes an energy function based on the distance of each visible vertex of the face model mesh to the depth map to identify the face model mesh that most closely approximates the pose of the face.
-
FIG.1 illustrates aface tracking system 100 configured to support face tracking functionality for AR/VR applications, using depth sensor data in accordance with at least one embodiment of the present disclosure. Theface tracking system 100 is an electronic device that can include a user-portable mobile device, such as a tablet computer, computing-enabled cellular phone (e.g., a "smartphone"), a head-mounted display (HMD), a notebook computer, a personal digital assistant (PDA), a gaming system remote, a television remote, camera attachments with or without a screen, and the like. In other embodiments, theface tracking system 100 can include another type of mobile device, such as an automobile, robot, remote-controlled drone or other airborne device, and the like. For ease of illustration, theface tracking system 100 is generally described herein in the example context of a mobile device, such as a tablet computer or a smartphone; however, theface tracking system 100 is not limited to these example implementations. Theface tracking system 100 includes aface tracking module 110 for estimating acurrent pose 140 of aface 120 based on adepth image 115 captured by adepth camera 105 in accordance with at least one embodiment of the present disclosure. - The
depth camera 105, in one embodiment, uses a modulated light projector (not shown) to project modulated light patterns into the local environment, and uses one ormore imaging sensors 106 to capture reflections of the modulated light patterns as they reflect back from objects in thelocal environment 112. These modulated light patterns can be either spatially-modulated light patterns or temporally-modulated light patterns. The captured reflections of the modulated light patterns are referred to herein as "depth images" 115 and are made up of a three-dimensional (3D) point cloud having a plurality of points. In some embodiments, thedepth camera 105 calculates the depths of the objects, that is, the distances of the objects from thedepth camera 105, based on the analysis of thedepth images 115. - The
face tracking module 110 receives adepth image 115 from thedepth camera 105 and generates a depth map based on thedepth image 115. Theface tracking module 110 identifies a pose of theface 120 by fitting a face model mesh to the pixels of the depth map that correspond to theface 120. In some embodiments, theface tracking module 110 estimates parametersface tracking module 110 leverages the parameters θ̂ inferred in the previous frame received from the depth camera. In some embodiments, the model is parameterized by a set of identity coefficientsface tracking module 110 models the deformation of the 3D positions of the face model mesh using a bi-linear (PCA) basis of the N 3D vertex positions whereface tracking module 110 calculates the deformed and repositioned vertices of the face model mesh as -
-
- To facilitate increased efficiency of minimizing the energy, the
face tracking module 110 includes only the vertices of the face model mesh that are assumed to be visible from the point of view of thedepth camera 105, and bicubically interpolates the depth map associated withface tracking module 110 to jointly optimize the pose and blendshape estimation of theface 120 using a smooth and differentiable energy. Based on the energy, the face tracking module estimates acurrent pose 140 of theface 120. - In some embodiments, the
face tracking module 110 uses thecurrent pose estimate 140 to updategraphical data 135 on adisplay 130. In some embodiments, thedisplay 130 is a physical surface, such as a tablet, mobile phone, smart device, display monitor, array(s) of display monitors, laptop, signage and the like or a projection onto a physical surface. In some embodiments, thedisplay 130 is planar. In some embodiments, thedisplay 130 is curved. In some embodiments, thedisplay 130 is a virtual surface, such as a three-dimensional or holographic projection of objects in space including virtual reality and augmented reality. In some embodiments in which thedisplay 130 is a virtual surface, the virtual surface is displayed within an HMD of a user. The location of the virtual surface may be relative to stationary objects (such as walls or furniture) within thelocal environment 112 of the user. -
FIG. 2 is a diagram illustrating theface tracking module 110 of theface tracking system 100 ofFIG. 1 in accordance with at least one embodiment of the present disclosure. Theface tracking module 110 includes amemory 205, avisibility estimator 210, anenergy minimizer 215, alandmarks module 220, and aregularizer 225. Each of these modules represents hardware, software, or a combination thereof, configured to execute the operations as described herein. Theface tracking module 110 is configured to receive adepth image 115 from the depth camera (not shown) and to generate acurrent pose estimate 140 based on thedepth image 115. - The
memory 205 is a memory device generally configured to store data, and therefore may be a random access memory (RAM) memory module, non-volatile memory device (e.g., flash memory), and the like. Thememory 205 may form part of a memory hierarchy of theface tracking system 100 and may include other memory modules, such as additional caches not illustrated atFIG. 1 . Thememory 205 is configured to receive and store thedepth image 115 from the depth camera (not shown). - The
visibility estimator 210 is a module configured to estimate whether a vertex of the face model mesh is visible from the point of view of the depth camera by determining to what degree the associated normal is facing toward or away from the depth camera. - The
energy minimizer 215 is a module configured to formulate and minimize an energy function describing the difference between the face model mesh and the depth map of the face. The energy function may be defined as - To facilitate more efficient pose estimation without necessitating rendering, based on the visibility estimates of the
visibility estimator 210, theenergy minimizer 215 replaces the sum (6) over an explicit set of vertices O(θ) with a sum over all vertices {1, ..., N} using the visibility term to turn on and off the individual terms as -
- The
landmarks module 220 is a module configured to detect and localize distinctive features (e.g., nose tip, eye corners), referred to as landmarks, of human faces. Landmarks provide strong constraints, both for the general alignment of the face and for estimating the identity and expression of the face. However, detected landmarks can be slightly incorrect, or may be estimated if they are not directly visible by thedepth camera 105, resulting in residuals in the image domain. Thelandmarks module 220 thus defines L facial landmarkslandmarks module 220 minimizes an energy function that reduces the variation in landmarks due to the distance of the face from the depth camera 105:depth camera 105. - The
regularizer 225 is a module configured to adjust the identity and expression weights to avoid over-fitting the depth map. Theregularizer 225 normalizes the eigen-vectors (blendshapes) to provide a standard normal distribution. While expression coefficients generally do not follow a Gaussian distribution, identity parameters roughly do. Theregularizer 225 performs statistical regularization by minimizing the L 2 norm of the identity parameters. The weights adjustor 225 therefore effectively encourages solutions to be close to the maximum likelihood estimation (MLE) of the multivariate Normal distribution, which is the mean face. Theregularizer 225 performs the statistical regularization using - In some embodiments, the
regularizer 225 incorporates temporal regularization overall the entries of θ during the joint optimization by adding in the following temporal regularization term to the energy: - In some embodiments, the
face tracking module 110 optimizes the energy function of (11) by recasting the energy function of (11) as a sum of M squared residuals □face tracking module 110 computes the Jacobianface tracking module 110 initializes θ ← θ̂ using the parameters from the previous frame. In some embodiments, theface tracking module 110 performs this Levenberg optimization on a GPU. Theface tracking module 110 generates acurrent pose estimate 140 of the face based on the optimized energy. -
FIG. 3 illustrates theface tracking module 110 ofFIGs. 1 and2 estimating the visibility of vertices of aface model mesh 305 inposes face model mesh 305 includes a plurality of vertices, each of which theface tracking module 110 associates with a normal vector. In the example ofFIG. 3 , where theface model mesh 305 is in a frontal facing pose 310, thevisibility estimator 210 estimates the degree to whichnormal vectors - Assuming that the depth camera is facing
FIG. 3 , thevisibility estimator 210 assigns a value to each vertex of theface model mesh 305 based on whether the normal vector associated with the vertex is estimated to face toward or away from the depth camera. For example, if 90° is considered directly facing (orthogonal to) the depth camera, and -90° is considered facing directly away from the depth camera, thevisibility estimator 210 assigns a first value (e.g., 1) indicating that a normal vector associated with a vertex is visible from the point of view of the depth camera, and assigns a second value (e.g., 0) indicating that a normal vector associated with a vertex is not visible from the point of view of the depth camera. - Accordingly, for the
pose 310, thevisibility estimator 210 estimates that thevector 312 is at a 45° angle to the depth camera, thatvector 314 is at a 2° angle to the depth camera, thatvector 316 is at a 2° angle to the depth camera, and thatvector 318 is at a 10° angle to the depth camera. Thevisibility estimator 210 assigns a value of 1 to the vertices associated with each ofvectors - For the
pose 320, however, theface model mesh 305 is rotated to the left, such that thevisibility estimator 210 estimates thatvector 312 is at a 10° angle to the depth camera, thatvector 314 is at a 20° angle to the depth camera, thatvector 316 is at a -20° angle to the depth camera, and thatvector 318 is at a -45° angle to the depth camera. Thevisibility estimator 210 therefore assigns a value of 1 to the vertices associated with each ofvectors vectors vectors face model mesh 305 is inpose 320, becausevectors face model mesh 305 based on whether the normal vector associated with each vertex is estimated to face toward or away from the depth camera, theface tracking module 110 can smoothly turn on and off data terms of the energy function without rendering theface model mesh 305. -
FIG. 4 is a flow diagram illustrating amethod 400 of estimating a pose of a face in accordance with at least one embodiment of the present disclosure. Atblock 402, theface tracking module 110 ofFIGs. 1 and2 receives captured depth data from thedepth camera 105. Atblock 404, theface tracking module 110 generates a depth map of theface 120 based on the captured depth data. Atblock 406, thevisibility estimator 210 estimates the visibility of each vertex of aface model mesh 305 based on an estimate of whether a normal vector associated with each vertex of theface model mesh 305 points toward or away from thedepth camera 105. Atblock 408, theregularizer 225 adjusts identity and expression weights of theface model mesh 305 to avoid over-fitting to the depth map. Atblock 410, theenergy minimizer 215 bicubically interpolates the pixels of the depth map to smooth the energy function as features move from one pixel to another. Atblock 412, theenergy minimizer 215 defines and minimizes an energy function that fits theface model mesh 305 to the depth map. In some embodiments, theenergy minimizer 215 leverages a pose of a frame of the depth camera immediately preceding a current frame to optimize the energy function. - In some embodiments, certain aspects of the techniques described above may implemented by one or more processors of a processing system executing software. The software comprises one or more sets of executable instructions stored or otherwise tangibly embodied on a non-transitory computer readable storage medium. The software can include the instructions and certain data that, when executed by the one or more processors, manipulate the one or more processors to perform one or more aspects of the techniques described above. The non-transitory computer readable storage medium can include, for example, a magnetic or optical disk storage device, solid state storage devices such as Flash memory, a cache, random access memory (RAM) or other non-volatile memory device or devices, and the like. The executable instructions stored on the non-transitory computer readable storage medium may be in source code, assembly language code, object code, or other instruction format that is interpreted or otherwise executable by one or more processors.
- A computer readable storage medium may include any storage medium, or combination of storage media, accessible by a computer system during use to provide instructions and/or data to the computer system. Such storage media can include, but is not limited to, optical media (e.g., compact disc (CD), digital versatile disc (DVD), Blu-Ray disc), magnetic media (e.g., floppy disc, magnetic tape, or magnetic hard drive), volatile memory (e.g., random access memory (RAM) or cache), non-volatile memory (e.g., read-only memory (ROM) or Flash memory), or microelectromechanical systems (MEMS)-based storage media. The computer readable storage medium may be embedded in the computing system (e.g., system RAM or ROM), fixedly attached to the computing system (e.g., a magnetic hard drive), removably attached to the computing system (e.g., an optical disc or Universal Serial Bus (USB)-based Flash memory), or coupled to the computer system via a wired or wireless network (e.g., network accessible storage (NAS)).
- Note that not all of the activities or elements described above in the general description are required, that a portion of a specific activity or device may not be required, and that one or more further activities may be performed, or elements included, in addition to those described. Still further, the order in which activities are listed are not necessarily the order in which they are performed. Also, the concepts have been described with reference to specific embodiments. However, one of ordinary skill in the art appreciates that various modifications and changes can be made without departing from the scope of the present disclosure as set forth in the claims below. Accordingly, the specification and figures are to be regarded in an illustrative rather than a restrictive sense, and all such modifications are intended to be included within the scope of the present disclosure as defined by the claims below.
- Benefits, other advantages, and solutions to problems have been described above with regard to specific embodiments. However, the benefits, advantages, solutions to problems, and any feature(s) that may cause any benefit, advantage, or solution to occur or become more pronounced are not to be construed as a critical, required, or essential feature of any or all the claims. Moreover, the particular embodiments disclosed above are illustrative only, as the disclosed subject matter may be modified and practiced in different but equivalent manners within the scope of the appended claims. No limitations are intended to the details of construction or design herein shown, other than as described in the claims below. It is therefore evident that the particular embodiments disclosed above may be altered or modified as long as the results of these alterations or modifications are within the scope of the claims below. Accordingly, the protection sought herein is as set forth in the claims below.
Claims (10)
- A method comprising:capturing, at a depth camera, a depth image of a face, the depth image comprising a three-dimensional (3D) point cloud comprising a plurality of points;generating, at a processor, a depth map of the face based on the 3D point cloud, the depth map comprising a plurality of pixels; andestimating, at the processor, a pose of the face by fitting a face model mesh comprising a plurality of vertices to the depth map, the fitting comprising:estimating which vertices of the face model mesh are visible from a point of view of the depth camera, wherein the estimating comprises assuming that the face model mesh is largely convex and further comprises estimating a degree to which a vector that is normal to each vertex of the face model mesh is facing toward or away from the depth camera; andminimizing an energy function based on a distance from each pixel of the depth map to a corresponding vertex of a subset of the vertices of the face model mesh, the subset comprising only the vertices that are estimated to be visible from the point of view of the depth camera.
- The method of claim 1, wherein the face model mesh is parameterized by expression weights that indicate how to non-rigidly deform the vertices of the face model mesh to fit the face model mesh to the depth map, and wherein identifying the pose of the face comprises adjusting the expression weights.
- The method of claim 1 or claim 2, wherein the face model mesh is parameterized by identity coefficients that indicate how to non-rigidly deform the vertices of the face model mesh to fit the face model mesh to the depth map.
- The method of any preceding claim, further comprising bicubically interpolating the pixels of the depth map associated with the depth image.
- The method of any preceding claim, further comprising regularizing a difference between the pose estimation and an estimate of facial landmarks based on an average depth of the depth map and a focal length of the depth camera.
- An electronic device, comprising:a user-facing depth camera to capture depth images of a face of a user, each depth image comprising a three-dimensional (3D) point cloud comprising a plurality of points; anda processor configured to:generate a depth map of the face based on the 3D point cloud, the depth map comprising a plurality of pixels; andestimate a pose of the face by fitting a face model mesh comprising a plurality of vertices to the depth map, the fitting comprising:estimating which vertices of the face model mesh are visible from a point of view of the depth camera, wherein the estimating comprises assuming that the face model mesh is largely convex and further comprises estimating a degree to which a vector that is normal to each vertex of the face model mesh is facing toward or away from the depth camera; andminimizing an energy function based on a distance from each pixel of the depth map to a corresponding vertex of a subset of the vertices of the face model mesh, the subset comprising only the vertices that are estimated to be visible from the point of view of the depth camera.
- The electronic device of claim 6, wherein the face model mesh is parameterized by expression weights that indicate how to non-rigidly deform the vertices of the face model mesh to fit the face model mesh to the depth map, and wherein identifying the pose of the face comprises adjusting the expression weights.
- The electronic device of claim 6 or claim 7, wherein the face model mesh is parameterized by identity coefficients that indicate how to non-rigidly deform the vertices of the face model mesh to fit the face model mesh to the depth map.
- The electronic device of any of claims 6-8, further comprising bicubically interpolating the pixels of the depth map associated with the depth image.
- The electronic device of any of claims 6-9, wherein the processor is further configured to regularize a difference between the pose estimation and an estimate of facial landmarks based on an average depth of the depth map and a focal length of the depth camera.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762516646P | 2017-06-07 | 2017-06-07 | |
PCT/US2018/036528 WO2018227001A1 (en) | 2017-06-07 | 2018-06-07 | High speed, high-fidelity face tracking |
Publications (2)
Publication Number | Publication Date |
---|---|
EP3593323A1 EP3593323A1 (en) | 2020-01-15 |
EP3593323B1 true EP3593323B1 (en) | 2020-08-05 |
Family
ID=64563496
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP18746325.2A Active EP3593323B1 (en) | 2017-06-07 | 2018-06-07 | High speed, high-fidelity face tracking |
Country Status (5)
Country | Link |
---|---|
US (1) | US10824226B2 (en) |
EP (1) | EP3593323B1 (en) |
KR (1) | KR102376948B1 (en) |
CN (1) | CN110476186B (en) |
WO (1) | WO2018227001A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112686978B (en) * | 2021-01-07 | 2021-09-03 | 网易（杭州）网络有限公司 | Expression resource loading method and device and electronic equipment |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9400921B2 (en) * | 2001-05-09 | 2016-07-26 | Intel Corporation | Method and system using a data-driven model for monocular face tracking |
GB2415344B (en) * | 2004-06-14 | 2010-10-06 | Canon Europa Nv | Texture data compression and rendering in 3D computer graphics |
WO2009124139A1 (en) * | 2008-04-01 | 2009-10-08 | The Government Of The United States Of America, As Represented By The Secretaty Of The Navy | Methods and systems of comparing face models for recognition |
JP2013156680A (en) * | 2012-01-26 | 2013-08-15 | Kumamoto Univ | Face tracking method and face tracker and vehicle |
US9159135B2 (en) * | 2012-03-28 | 2015-10-13 | Intel Corporation | Systems, methods, and computer program products for low-latency warping of a depth map |
US9846960B2 (en) * | 2012-05-31 | 2017-12-19 | Microsoft Technology Licensing, Llc | Automated camera array calibration |
CN103198523B (en) | 2013-04-26 | 2016-09-21 | 清华大学 | A kind of three-dimensional non-rigid body reconstruction method based on many depth maps and system |
US9524582B2 (en) * | 2014-01-28 | 2016-12-20 | Siemens Healthcare Gmbh | Method and system for constructing personalized avatars using a parameterized deformable mesh |
CN105023280A (en) | 2015-08-21 | 2015-11-04 | 武汉汇思聚创信息技术有限公司 | Detection method based on 3D skeleton modeling |
WO2018183751A1 (en) * | 2017-03-30 | 2018-10-04 | Body Surface Translations Inc. | Determining anthropometric measurements of a nonstationary subject |
US20180330496A1 (en) * | 2017-05-11 | 2018-11-15 | Siemens Healthcare Gmbh | Generation Of Personalized Surface Data |
-
2018
- 2018-06-07 WO PCT/US2018/036528 patent/WO2018227001A1/en active Search and Examination
- 2018-06-07 CN CN201880022747.3A patent/CN110476186B/en active Active
- 2018-06-07 KR KR1020197029061A patent/KR102376948B1/en active IP Right Grant
- 2018-06-07 US US16/002,595 patent/US10824226B2/en active Active
- 2018-06-07 EP EP18746325.2A patent/EP3593323B1/en active Active
Non-Patent Citations (1)
Title |
---|
None * |
Also Published As
Publication number | Publication date |
---|---|
KR102376948B1 (en) | 2022-03-21 |
US20180356883A1 (en) | 2018-12-13 |
WO2018227001A1 (en) | 2018-12-13 |
KR20200015459A (en) | 2020-02-12 |
EP3593323A1 (en) | 2020-01-15 |
CN110476186A (en) | 2019-11-19 |
US10824226B2 (en) | 2020-11-03 |
CN110476186B (en) | 2020-12-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11468645B2 (en) | Optimizing head mounted displays for augmented reality | |
US11030773B2 (en) | Hand tracking based on articulated distance field | |
US10937182B2 (en) | Non-rigid alignment for volumetric performance capture | |
KR20230173217A (en) | Systems and methods for photorealistic real-time portrait animation | |
US20190080462A1 (en) | Method and apparatus for calculating depth map based on reliability | |
US11921291B2 (en) | Systems and methods for performing self-improving visual odometry | |
US20220058808A1 (en) | Real Time Perspective Correction on Faces | |
US11948309B2 (en) | Systems and methods for jointly training a machine-learning-based monocular optical flow, depth, and scene flow estimator | |
EP3593323B1 (en) | High speed, high-fidelity face tracking | |
US20190096073A1 (en) | Histogram and entropy-based texture detection | |
US20230093827A1 (en) | Image processing framework for performing object depth estimation | |
US20230147722A1 (en) | Reconstructing three-dimensional models of objects from real images based on depth information | |
US11657506B2 (en) | Systems and methods for autonomous robot navigation | |
Chen et al. | Depth recovery with face priors | |
Karvounas et al. | Multi-view image-based hand geometry refinement using differentiable monte carlo ray tracing | |
CN110800024B (en) | Method and electronic device for estimating current posture of hand | |
Ravikumar | Lightweight Markerless Monocular Face Capture with 3D Spatial Priors | |
CN110785790A (en) | Non-rigid alignment of volumetric performance capture | |
CN117813626A (en) | Reconstructing depth information from multi-view stereo (MVS) images |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: UNKNOWN |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE INTERNATIONAL PUBLICATION HAS BEEN MADE |
|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: REQUEST FOR EXAMINATION WAS MADE |
|
17P | Request for examination filed |
Effective date: 20191007 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTG | Intention to grant announced |
Effective date: 20200225 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
DAV | Request for validation of the european patent (deleted) | ||
DAX | Request for extension of the european patent (deleted) | ||
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 1299770Country of ref document: ATKind code of ref document: TEffective date: 20200815 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602018006724Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG4D |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20200805 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1299770Country of ref document: ATKind code of ref document: TEffective date: 20200805 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201207Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201105Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201106Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201105 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201205 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602018006724Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805 |
|
26N | No opposition filed |
Effective date: 20210507 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20210630 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210607 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210630Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210607Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210630 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: FRFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210630 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210630 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20220628Year of fee payment: 5 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: DEPayment date: 20220629Year of fee payment: 5 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230506 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20180607 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R119Ref document number: 602018006724Country of ref document: DE |
|
GBPC | Gb: european patent ceased through non-payment of renewal fee |
Effective date: 20230607 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200805Ref country code: DEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20240103Ref country code: GBFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20230607 |