CN112005213A - Large lookup tables for image processors - Google Patents
Large lookup tables for image processors Download PDFInfo
- Publication number
- CN112005213A CN112005213A CN201980022634.8A CN201980022634A CN112005213A CN 112005213 A CN112005213 A CN 112005213A CN 201980022634 A CN201980022634 A CN 201980022634A CN 112005213 A CN112005213 A CN 112005213A
- Authority
- CN
- China
- Prior art keywords
- lookup table
- execution
- local
- value
- remote
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 62
- 238000004590 computer program Methods 0.000 claims abstract description 14
- 238000005192 partition Methods 0.000 claims description 143
- 239000013598 vector Substances 0.000 claims description 33
- 238000006073 displacement reaction Methods 0.000 claims description 28
- 239000000872 buffer Substances 0.000 claims description 27
- 230000004044 response Effects 0.000 claims description 7
- 230000008569 process Effects 0.000 description 29
- 238000012545 processing Methods 0.000 description 21
- 238000010586 diagram Methods 0.000 description 8
- 238000000638 solvent extraction Methods 0.000 description 8
- 230000009471 action Effects 0.000 description 5
- 238000004422 calculation algorithm Methods 0.000 description 5
- 230000006870 function Effects 0.000 description 5
- 238000004364 calculation method Methods 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 241000270295 Serpentes Species 0.000 description 2
- 230000015556 catabolic process Effects 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 238000006731 degradation reaction Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000012804 iterative process Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 230000005574 cross-species transmission Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 150000004820 halides Chemical class 0.000 description 1
- 125000001475 halogen functional group Chemical group 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000007620 mathematical function Methods 0.000 description 1
- 238000012805 post-processing Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T1/00—General purpose image data processing
- G06T1/20—Processor architectures; Processor configuration, e.g. pipelining
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F8/00—Arrangements for software engineering
- G06F8/40—Transformation of program code
- G06F8/41—Compilation
- G06F8/44—Encoding
- G06F8/441—Register allocation; Assignment of physical memory space to logical memory space
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F8/00—Arrangements for software engineering
- G06F8/40—Transformation of program code
- G06F8/41—Compilation
- G06F8/44—Encoding
- G06F8/443—Optimisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F8/00—Arrangements for software engineering
- G06F8/40—Transformation of program code
- G06F8/41—Compilation
- G06F8/44—Encoding
- G06F8/445—Exploiting fine grain parallelism, i.e. parallelism at instruction level
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F8/00—Arrangements for software engineering
- G06F8/40—Transformation of program code
- G06F8/41—Compilation
- G06F8/45—Exploiting coarse grain parallelism in compilation, i.e. parallelism between groups of instructions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/30003—Arrangements for executing specific machine instructions
- G06F9/30007—Arrangements for executing specific machine instructions to perform operations on data operands
- G06F9/30036—Instructions to perform operations on packed data, e.g. vector, tile or matrix operations
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/30003—Arrangements for executing specific machine instructions
- G06F9/3004—Arrangements for executing specific machine instructions to perform operations on memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/38—Concurrent instruction execution, e.g. pipeline, look ahead
- G06F9/3885—Concurrent instruction execution, e.g. pipeline, look ahead using a plurality of independent parallel functional units
- G06F9/3887—Concurrent instruction execution, e.g. pipeline, look ahead using a plurality of independent parallel functional units controlled by a single instruction for multiple data lanes [SIMD]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/38—Concurrent instruction execution, e.g. pipeline, look ahead
- G06F9/3885—Concurrent instruction execution, e.g. pipeline, look ahead using a plurality of independent parallel functional units
- G06F9/3889—Concurrent instruction execution, e.g. pipeline, look ahead using a plurality of independent parallel functional units controlled by multiple instructions, e.g. MIMD, decoupled access or execute
- G06F9/3891—Concurrent instruction execution, e.g. pipeline, look ahead using a plurality of independent parallel functional units controlled by multiple instructions, e.g. MIMD, decoupled access or execute organised in groups of units sharing resources, e.g. clusters
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T1/00—General purpose image data processing
- G06T1/60—Memory management
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for supporting large lookup tables on an image processor. One of the methods includes: an input kernel program is received for an image processor having a two-dimensional array of execution lanes, a shift register array, and a plurality of memory banks. If the kernel program has instructions to read lookup table values of a lookup table partitioned across the plurality of memory banks, replacing the instructions in the kernel program with a sequence of instructions that, when executed by an execution lane, cause the execution lane to read a first value from a local memory bank and to read a second value from the local memory bank on behalf of another execution lane belonging to a different execution lane group.
Description
Background
The present specification relates to an image processor.
The image processor is a programmable, domain-specific parallel processing device designed to exploit two-dimensional spatial locality in image data. An image processor is designed to efficiently process existing image data, which is distinguished from a Graphics Processing Unit (GPU) that is designed to first generate an image from an internal representation.
The image processor is designed to exploit two-dimensional spatial locality to efficiently, low-power, parallel execute workloads. When the output data for a location in the input data is dependent on data adjacent to or near the location in the input data, the computational task has two-dimensional spatial locality. For example, a 3x3 blur filter may use data in a 9 pixel square region of input image data to calculate an output value for a pixel at the center of the square region. In other words, the blurring filter has spatial locality, since the output values only use data from neighboring pixels. The image processor may also be used to execute workloads in parallel with high performance in other areas, including computer vision, object recognition, and neural networks.
Programming an image processor typically requires writing and compiling a kernel program that is then executed simultaneously by each of a plurality of execution channels. Each execution channel is itself a component that can execute instructions and store data in one or more registers.
Some image processors exploit spatial locality by coupling an array of execution channels with an array of shift registers. This arrangement allows each execution lane to access the data required by its kernel program by shifting the input data within the shift register array rather than performing a memory access. Conceptually, this can be thought of as shifting the image data array below the execution lane array. For example, the execution channel may access the data needed to compute the blur filter by repeatedly reading data shifted in the following snake scan order: two pixels to the left, one pixel down, two pixels to the right, one pixel down, two pixels to the left.
This strategy works well when the kernel does not rely on data. For example, the blur filter does not rely on data because the kernel performs the same data access pattern regardless of the value of the input pixel. When the kernel program does not depend on data, the compiler may pre-schedule all data shifts by generating instructions to cause the compiler to follow a pattern that can be predicted in advance, for example, in the snake scan order described above. However, some kernel programs rely on data. In other words, the data accessed by the execution channel may change according to the input data. Therefore, the compiler cannot schedule access patterns in advance, which must be computed at runtime.
One class of kernel programs that rely on data includes kernel programs that utilize look-up tables. A lookup table is a data structure that maps input values to pre-computed output values. Lookup tables are typically used to reduce runtime computations. For example, the lookup table may map color image values to corresponding grayscale correspondence entries, which avoids runtime conversion between formats. When the kernel program utilizes a lookup table, the compiler generates a sequence of instructions for performing random memory accesses to obtain the lookup table values in memory. The sequence of instructions generally includes: 1) compute an index from the input data, 2) compute a memory address from the index, and 3) read from the computed memory address.
The image processor may use a look-up table as long as it fits within a memory accessible by each execution channel. For large lookup tables, this can be a problem. To maximize speed, the execution channels of the image processor typically do not have access to cache or main memory. In contrast, an execution channel may only have access to one or more local memory banks (SRAM banks), for example, designed for speed rather than capacity. The problem is exacerbated if the local memory is divided into a plurality of smaller memory banks that are each accessible only to a subset of the execution lanes. In that case, the look-up table needs to be replicated in each local memory bank. If the look-up table for a particular kernel program does not fit within each local memory bank, the program may not be compiled.
Disclosure of Invention
This specification describes how a compiler for an image processor can generate program code to support large look-up tables. In this context, a large lookup table is the following lookup table: for one or more local memory banks that are accessible by the execution channel of the image processor, the table is too large to fit those banks. The compiler may support large lookup tables by efficiently distributing portions of the lookup table across multiple local memory banks. This means that: each execution channel can only access a portion of the lookup table. The compiler may make the remaining look-up table data accessible by generating the following code: the code causes the other execution channels to read the required indices and values and shift them by using the shift register array.
In this specification, a lookup table is a data structure that maps indices to values at runtime. Look-up tables are common in many image processing algorithms and applications. For example, a look-up table may be used for complex function approximation. As another example, the fast accurate super resolution of images (RAISR) algorithm relies on a large lookup table to obtain specially learned filters in order to access the image data.
In this specification, a partition lookup table means: no single bank of memory contains all the values of the table. Partitioned merely implies distribution and not necessarily data exclusivity. Instead, some values may be replicated in multiple memory banks.
In this description, a local memory bank is a memory bank that is accessible by a particular execution channel. Thus, whether the memory bank is considered local or remote depends on the designation of a particular execution channel.
In this specification, an execution channel group includes all execution channels that may access the same local memory bank.
In this specification, a shift register array is a hardware device that logically arranges shift registers in a two-dimensional layout so that adjacent shift registers can shift data from each other, typically during a single clock cycle. The shift register array is typically implemented as a ring or ring shape so that data from one edge of the shift register array can be shifted directly to the other edge of the shift register array.
In this specification, a shift access sequence is a sequence of kernel program instructions that execute channels that read a first value from the local memory bank and that read a second value from the local memory bank on behalf of different execution channels belonging to different execution channel groups.
In this specification, a vector access sequence is a sequence of kernel program instructions that allow an execution channel to obtain multiple values of a structured data object having a fixed address pattern partitioned across multiple banks of memory. Thus, the vector access sequence further instructs the execution lane to read at least a first value from the local memory bank and to read a second value from the local memory bank on behalf of a different execution lane belonging to a different execution lane group. The vector access sequence also includes instructions for restoring the original order of the structured data objects.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. Providing support for large lookup tables provides additional computational power to the image processor. In particular, the image processor may execute many other computational algorithms that rely on large look-up tables, for example, the RAISR algorithm or an algorithm for complex function approximation. Generating a shifted access sequence also increases the storage size of the look-up table that may be used by the image processor. Generating the vector access sequence provides this increase in storage size without a significant performance degradation due to the discarding of unwanted data.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a flow diagram of an example process for reading values of a lookup table partitioned across multiple banks of memory.
FIG. 2 is a flow diagram of an example process for performing a shift access sequence for a partition lookup table.
FIG. 3A illustrates an example of reading and shifting with partition indexes.
FIG. 3B illustrates reading and shifting remote lookup table values.
FIG. 4 is a flow diagram of an example process for generating a vector access sequence.
FIG. 5 is a flow diagram of an example process for performing a sequence of vector accesses to a partition lookup table.
Fig. 6 illustrates different group phases of the execution channel groups.
FIG. 7 illustrates components of an example stencil processor.
FIG. 8 illustrates an example data computation component. Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
FIG. 1 is a flow diagram of an example process for reading values of a lookup table partitioned across multiple banks of memory. The example process may be performed by any suitably programmed computer system, such as a compiler, assembler or program optimizer for an image processor, to name a few. For convenience, the example process is described as being performed by a suitably programmed system of one or more computers.
The system receives input program code that references a lookup table value (110). The input program code may be any suitable programming language that may define a look-up table to be stored in the local memory bank of the image processor. For example, the program code may be a high level programming language, such as C or Halide; a middle level language representation; or a lower level programming language such as assembly language code, object code.
For example, the input program code may declare the array or buffer as a lookup table, either explicitly or implicitly. For example, a compiler may treat an array named "lut" as a lookup table. Thus, the compiler may support statements such as "a ═ lut [ x ]", where lut is an array or buffer that is considered a lookup table, and x is a lookup table index.
The system determines whether the program has a lookup table that needs to be partitioned (120). Typically, the look-up table is partitioned when the entire look-up table does not fit within at least one local memory bank of the image processor. Thus, the system can calculate the total size of the look-up table declared in the input program and determine whether the total size will fit within each local memory bank in the image processing. If not, the system may determine that the lookup table needs to be partitioned.
The system may also consider the total size of all look-up tables in the program. If the total size of all the look-up tables is greater than the size of the at least one local memory bank, the system may select one or more look-up tables for partitioning. In some embodiments, the system computes an iterative process by repeatedly selecting the next lookup table to partition and then re-determining whether all partition tables and non-partition tables will fit into all local memory banks of the image processor. For example, the system may perform the iterative process in an order defined by the respective sizes of the look-up tables, starting with the largest look-up table or the smallest look-up table.
The system may also consider program language commands. Some programming languages may support annotations that designate a particular lookup table as a candidate for partitioning or that must require partitioning. If a lookup table is annotated as a candidate for partitioning, the system may, for example, partition the table according to table size only if necessary, and then may iteratively traverse all lookup tables annotated as candidates. If a lookup table is annotated as needing to be partitioned, the system may first partition the lookup table before considering only those tables annotated as candidates or the unannotated lookup table.
If no partitioning of the lookup table is required, the process ends (branch to end) without the system modifying any program code.
If the lookup table needs to be partitioned, the system optionally generates code for partitioning the lookup table across multiple IPU memory banks (branch to 130). In general, the system may partition the lookup table by interleaving data values in N steps across multiple IPU memory banks for N partitions. As mentioned above, partitions in this context need not be unique, but can be replicated across multiple memory banks.
For example, if the lookup table is partitioned into two partitions, e.g., N-2, the system may generate code that stores all even indices in some banks and all odd indices in other banks. If there are four partitions, the system may generate the following code: the code places the 1 st index, the 5 th index, the 9 th index, etc. in some banks of storage, every 2 nd index, the 6 th index, the 10 th index, etc. in another bank of storage, and so on. In general, the system may interleave values so that there is a relatively simple and therefore fast way to compute partition indices from the original lookup table indices. Computing the partition index is described in more detail below with reference to FIG. 2.
Step 130 is described as optional because other systems or tools may be used to partition the lookup tables across the bank of memory or generate partitioned code that is separate and apart from optimizing each lookup table access of the kernel program. For example, a completely separate kernel running at system boot time may partition the lookup table across the bank of memory.
The system replaces each original look-up table access in the program with a shifted access sequence (140). As described above, a shift access sequence is a series of instructions that an execution channel executes to obtain a desired value from a partition lookup table. Typically, each execution channel will read a first value from its local memory bank for itself, and will receive a remote index and use that remote index to read a second value from its local memory bank on behalf of another execution channel. The second value is then shifted through the shift register array to the execution lane that provided the remote index. Each execution channel may then select either a first value read from the local memory bank or another value read from the remote memory bank by another execution channel. As part of this process, the execution lane effectively shifts the lookup table index through the shift register array in one direction and shifts the lookup table value back through the shift register array in the other direction. This process allows each execution channel to obtain any value in the partition lookup table, even when the required value is not stored in the local memory bank for the execution channel. An example shift access sequence is described in more detail below with reference to fig. 2 and 3A-3B.
After replacing the original lookup table access with the shifted access sequence, the process ends. The code modified with the shifted access sequences may then be further compiled or assembled into object code to be executed on an image processor.
FIG. 2 is a flow diagram of an example process for performing a shift access sequence for a partition lookup table. Typically, multiple execution lanes of the image processor execute the shift access sequence simultaneously, e.g., one execution lane per local memory bank. For convenience, the process will be described from the perspective of a single execution lane.
The execution channel receives a lookup table index (210). Receiving a lookup table index typically occurs as a result of generating the index into a lookup table using kernel code of the input data (e.g., pixel values). For example, the kernel program may map a series of possible input values to a particular look-up table index.
The execution channel computes a local partition index (220). The local partition index is an index into the local memory bank of the execution channel. If the local memory bank executing the channel has a value indicated by the original lookup table index, the partition index will indicate the location of the value in the local memory bank.
If not, the partition index will indicate the location of the value in a different remote memory bank local to a different execution channel.
The calculation of the partition index depends on how the lookup table is partitioned, including how many partitions. For example, if the values of the lookup table are interleaved in two partitions, the partition index may be calculated by:
partitioned_index＝lut_index/2
where lut _ index represents the original look-up table index and the "/" operator indicates integer division with no remainder.
Thus, in general, with a lookup table that interleaves its values in N partitions, the partition index may be computed in the following way:
partitioned_index＝lut_index/N。
the execution channel reads the local lookup table value using the local partition index (230). In other words, the execution channel uses the local partition index to read from its local memory bank. This typically involves: the local partition index is used as an offset from the base address of the lookup table partition in the local memory bank.
Note that some of the steps of FIG. 2 are performed in parallel by all execution lanes, while other steps may be ordered. For example, all execution lanes may receive the LUT index (210) and compute the local partition index (220) simultaneously. In some embodiments, however, only one address can be read at a time from any one local memory bank. Thus, step (230) is an example of an operation that may have to be ordered between execution channels local to the memory bank.
The execution channel shifts the local partition index and receives the remote partition index (240). In other words, the local partition indices that the execution lane computes are shuffled through the shift register array in the same direction. Typically, an execution lane shifts data to a corresponding execution lane of a different bank. Thus, the 0 th execution channel for local store 0 shifts the index to the 0 th execution channel for local store 1, and so on. Therefore, the shift amount varies depending on how many execution channels are allocated to the same local memory bank.
FIG. 3A conceptually illustrates an example of utilizing partition indexes for reading and shifting. Fig. 3A illustrates a lookup table that has been partitioned into two partitions, such that even lookup table indices 302 are stored in some banks of memory, e.g., banks 310a and 310c, and odd lookup table indices 304 are stored in other banks of memory, e.g., banks 310b and 310 d.
In FIG. 3A, the execution lanes may use a shift register array to shift data from each other. In this example, the shift register array is arranged in a ring such that from left to right and top to bottom, one edge of the shift register array is shifted around to the opposite edge of the shift register array.
In FIG. 3A, for clarity, all execution channels for all memory banks are illustrated as residing along the same row. However, in some image processor embodiments, the execution lanes sharing the memory banks are arranged along the same row or column of the two-dimensional execution lane array. Thus, for example, the shift illustrated as horizontally "east" from execution lanes 321 a-324 a to execution lanes 321 b-324 b, respectively, may actually be a shift to "south," e.g., where execution lanes 321 b-324 b are on a different row than execution lanes 321 a-324 a. Thus, while FIG. 3A shows four positions shifted horizontally, the same operation may actually only require one position shift down.
FIG. 3A also illustrates sixteen execution channels, with a group of four execution channels assigned to each of the four memory banks. As shown, channels 321 a-324 a are assigned to bank 310a, channels 321 b-324 b are assigned to bank 310b, channels 321 c-324 c are assigned to bank 310c, and channels 321 d-324 d are assigned to bank 310 d.
FIG. 3A also illustrates the hardware limitations of a system that has shift sequences to support the necessary partition lookup tables. For example, there is no hardware path between execution path 321a and bank 1310 b. Execution path 321a may still indirectly access bank 1310 b through a series of shifts through the shift register array.
FIG. 3A also shows an example ordering of reads into a local store using partition indexes. Since channels 321 b-324 b share memory bank 310b, only one address can be read at a time. If the addresses are the same, the execution channels may share data, but this is unlikely for a look-up table.
Thus, in step 1, channel 321b reads the local lookup table value using its local partition index; in step 2, channel 322b reads the local lookup table value using its local partition index; in step 3, channel 323b reads the local lookup table value using its local partition index; and in step 4, the channel 324b reads the local lookup table value using its local partition index. Although not illustrated in FIG. 3A, the system may order each other execution lane group in the same manner. Thus, channels 321a through 324a also read the local lookup table values in steps 1 through 4, respectively. The total number of reads in order may be determined by the total number of execution channels assigned to the same memory bank, which in this example is four.
In step 5, all execution lanes use the shift register array to shift all partition indices to corresponding execution lanes in the next group. Thus, channel 321b has its partition index shifted to channel 321 c; channel 322b has its partition index shifted to channel 322 c; channel 323b has its partition index shifted to channel 323 c; and channel 324b has its partition index shifted to channel 324 c. Although not illustrated in FIG. 3A, each execution channel may have its partition index shifted in the same manner at the same time. The hardware of the image processor typically forms a logical loop or ring such that an execution lane on an edge may move data to an execution lane on another edge in a single cycle.
All read and shift operations of the example image processor may be coordinated by broadcasting instructions of the kernel program to each execution channel. In some embodiments, the broadcast operation is performed by a scalar processor or controller that decodes and broadcasts instructions of the execution lane. As will be discussed in more detail below, with respect to some steps, some executions do not perform operations because a received mask load instruction directs an execution lane to read a value only if the execution lane is in a particular location. For example, these mask load instructions may drive the read sequence in steps 1 through 4 shown in fig. 3A. In another aspect, step 5 involves: in response to shift instructions received by all execution lanes, all execution lanes simultaneously shift data through the shift register array.
Each execution channel may also receive a remote partition index from another execution channel in another group. In this context, remote means: the partition index is computed by a different execution channel in another group. Thus, channel 321b receives the remote partition index from channel 321 a; channel 322b receives the remote partition index from channel 322 a; channel 323b receives the remote partition index from channel 323 a; and channel 324b receives the remote partition index from channel 324 a. Although not illustrated in FIG. 3A, each execution channel may likewise receive remote partition indices from another execution channel in the same manner at the same time.
As shown in fig. 2, the execution channel reads the remote lookup table values using the remote partition index (250). In this context, a remote lookup table value means: this value may be associated with an execution channel in another group. But the remote lookup table value will typically not be the value required by the executing channel that is reading. Instead, the read value will be shifted back to the execution channel that computed the remote partition index. Similar to local lookup table values, the system can typically order the reads of remote lookup table values because each bank can only serve one different address at a time.
The execution channel shifts the remote lookup table value and receives the remote LUT value (260). In other words, the execution channel shifts the remote lookup table value back to the execution channel that computed the remote partition index. The execution channel also receives a remote lookup table value read by another execution channel.
FIG. 3B illustrates reading and shifting remote lookup table values. Similar to FIG. 3A, reading remote lookup table values using remote partition indices is ordered among execution channels in a group. Thus, in step 1, channel 321b reads the remote lookup table values using the remote partition index; in step 2, channel 322b reads the remote lookup table value using the remote partition index; in step 3, channel 323b reads the remote lookup table values using the remote partition index; and in step 4, the channel 324b reads the remote lookup table value using the remote partition index. And the system may order each other execution lane group in the same manner. Thus, channels 321 c-324 c also read the remote lookup table values in steps 1-4, respectively.
In step 5, all execution lanes shift all remote lookup table values back to the corresponding execution lanes in the previous group. Thus, channel 321b shifts its remote lookup table value back to channel 321 a; channel 322b shifts its remote lookup table value back to channel 322 c; channel 323b has its partition index shifted to channel 323 c; and channel 324b has its partition index shifted to channel 324 c. Although not illustrated in FIG. 3A, each execution channel may have its partition index shifted in the same manner at the same time.
Each execution channel also receives a remote lookup table value from another execution channel in another group. In this context, remote means: the lookup table values are read by different execution channels in another group. Thus, channel 321b receives the remote lookup table values from channel 321 c; channel 322b receives the remote lookup table value from channel 322 c; channel 323b receives the remote lookup table values from channel 323 c; and channel 324b receives the remote lookup table value from channel 324 c.
Fig. 3A and 3B illustrate examples of only two partitions. However, if there are more than two, the execution channel may perform additional reads and shifts.
Thus, the system determines whether there are more partitions to process (270). If so, the execution channel shifts the received remote partition index and receives another remote partition index (branch to 240). The execution channel may then use the received remote partition index to read another remote lookup table value and shift the remote lookup table value back to the execution channel in another group.
If there are no more partitions to process, the execution lane selects between the local lookup table value and one of the one or more remote lookup table values. As described above, typically, for N partitions, each execution channel will have read or received N lookup table values. The execution channel may then decide which execution channel is needed by the kernel.
The execution channel selects between a local lookup table value and one or more remote lookup table values (280). In general, the execution channel may determine which lookup table value to select for the kernel program using the original lookup table index denoted in this specification as "% N" modulo N. For example, if the table is partitioned into two partitions, e.g., N-2, the execution lane may select a local lookup table value if index% 2 equals the partition number of the execution lane, and otherwise select a remote lookup table value.
If the lookup table is partitioned into N partitions, the execution lane may select the local lookup table value if index% N equals the partition number of the execution lane. Conversely, the execution lane may select the (index% N-1) th remote lookup table value otherwise.
Note that for N partitions, each execution channel may always read N values, even when only one value is needed. However, there are the following cases: this aspect can be used as an advantage in a way that reduces or even completely eliminates the performance degradation caused by reading multiple values.
One such case is when the look-up table stores structured data rather than just individual elements of scalar data. Structured data types include wide data types and vector data types that occupy space in multiple registers. For these data types, the kernel program will typically read multiple sequential values from a lookup table. For example, instead of the kernel program only accessing one value at lut [ index ], the kernel program may specify that all values at lut [ index ], lut [ index +1], lut [ index +2] … are accessed up to a particular data width.
FIG. 4 is a flow diagram of an example process for generating a vector access sequence. The example process may be performed by any suitably programmed computer system, such as a compiler, assembler or program optimizer for an image processor, to name a few. For convenience, the example process is described as being performed by a suitably programmed system of one or more computers.
The system identifies a structured data access pattern in a kernel (410). To this end, the system may identify several reads in succession in the look-up table in a portion of the kernel. The system may identify any suitable number of look-up table reads. In some implementations, the system identifies the structured data access pattern as a number of reads equal to a structured data width of the lookup table. Thus, if a vector has a width of the size of four registers, the system can identify code segments with four reads of consecutive portions of the lookup table. In some embodiments, the system also imposes a tightness constraint by requiring each of the consecutive read instructions to be within a threshold number of the other consecutive read instructions.
In some embodiments, the compiler examines each lut instruction and transforms each index into a form of x + N, where x is an arbitrarily computed value and N is a constant. The compiler may maintain a table of previous lut instructions on an x basis and for each new lut instruction on an x basis, the compiler may determine whether another previous lut instruction with a constant N' is part of the same vector. If so, the system may determine to generate a vector access sequence.
The following code segments include structured data access patterns on lines 8, 10, 12, and 14:
in this example, lines 0 through 7 populate the lookup table with only test data for vectors having a width of four registers.
Line 8, line 10, line 12, and line 14 include instructions that utilize the instruction "lut. b 16" to read successive sets of four values from the lookup table.
In this example, the number of lookup table access instructions is equal to the vector width of the vector data in the lookup table. In addition, all instructions are relatively close together. That is, each instruction is located within two instructions in the most recent other look-up table access. Thus, the system may determine to replace the set of lookup table accesses with a sequence of vector accesses.
The system optionally generates code for partitioning the lookup table by the width of the structured data (420). The system may select the partition count of the lookup table to match the structured data width. For example, if the lookup table stores a double-wide data type, the compiler may select the partition count to be 2. If the lookup table stores vector data types, the compiler may select a partition count that matches the vector width.
Step 420 is described as optional because other systems or tools may be used to select the number of partitions based on the structured data width and partition the lookup table across the bank of memory. For example, a completely separate kernel program running at system boot time may analyze the instruction access pattern or structured data width to select the number of partitions and partition the lookup table across the bank of memory.
The system replaces the structured data access pattern with a vector access sequence (430). The vector access sequence causes all execution channels to automatically read all required values of the structured data by the operation of the shift access sequence process. The vector access sequence may be used to read any suitable structured data, not just vector data.
The vector access sequence is similar in operation to the shift access sequence process described above, except that each execution channel will use all the data values read, rather than selecting only one of them.
In addition, post-processing is typically required to take care of reading the data and shifting it out of order. The structured data values will be read and shifted to each execution channel in a different order. The order depends on the location of the execution channels in other native execution channels that share the same memory bank. Thus, each execution lane may perform a process for restoring the original order of the plurality of values. Thus, the system can generate code with a remapping step that matches all local and remote lookup table values with the original structured data element.
In some embodiments, the system generates a mid-level instruction that represents accessing multiple values of the structured data. For example, the system may replace lines 8 through 14 in the above example with the following vector access instructions:
8,lut.b16 t9,t10,t11,t12<-__lookup,t7；
the vector access instruction simplifies the determination of where the N outputs belong because the instruction has multiple outputs. Alternatively, the compiler may skip vectoring, but instead determine for each lut instruction whether the value is already in the previous displacement buffer, which effectively inlines vectoring.
FIG. 5 is a flow diagram of an example process for performing a sequence of vector accesses to a partition lookup table. In this example, the lookup table has been partitioned by the width of the structured data. As will be appreciated, some steps are similar to the shifted access sequence described above with reference to fig. 2. However, additional steps are used to restore the original order in which the data passed through the displaced buffers. Multiple execution channels of the image processor may execute the sequence of vector accesses simultaneously. For convenience, the process will be described from the perspective of a single execution lane.
The execution channel receives a structured data base index (505). In other words, the execution lane receives an index value in the lookup table for the first value of the structured data, e.g., an index of the first vector element of the vector.
The execution channel computes a local partition index from the base index (510). As described above, the calculation of the local partition index depends on how the data is interleaved and how many partitions are. When partitioning a table according to the width of the structured data, the system may calculate the local partition index by dividing by the width of the structured data.
To continue the example from the above, the system may execute the following shift right instruction to shift the base index by two positions. This operation results in the base index being divided by 4, 4 being the number of partitions.
8,shr.b16 t1012<-t7,2；
The execution channel may then load the address of the structured data element (line 9) by adding the partition index to the base address of the table.
9,add.b16 t1013<-t1012,0；
The execution channel reads the local LUT values using the local partition indices (515). This step may be performed serially by all execution lanes sharing the same local memory bank, as described above with reference to fig. 2.
Thus, to continue the example, the execution channel may perform the following serialized reads. The "lddm" instruction is a load instruction with a mask. Thus, the instruction will actually load data only when executed by an execution channel having an identifier that matches the mask value.
The execution lane calculates a position for the displacement (520). As described above, the execution channel will read the structured data values in a different order.
Fig. 6 illustrates different group phases of the execution channel groups. In fig. 6, the lookup table has been partitioned four ways so that volume 0610 a has an index with a modulus value of zero. In other words, the partition arrangement means: whatever the index is in the volume 0610 a, index% 4 will be zero. Likewise, the modulus value of the index in volume 1610 b is 1, the modulus value of the index in volume 1610 c is 2, and the modulus value of the index in volume 1610 d is 3.
These modulus values determine the group phase of the execution lane local to those index values. The group phase specifies the order in which different locations of the structured data are to be read. As shown, execution lanes 621 a-624 a have group phases of 0, 1, 2, 3. This means that: regardless of which value is read from the lookup table, the execution lane with the set of phases will always read first from bank 0610 a, then from bank 1610 b, and so on. On the other hand, execution channels with group phases of 1, 2, 3, 0 will always first read the second value of structured data from bank 1610 b.
In the following code segment, the execution channel reads its internal configuration to calculate its current group phase value. The code causes the execution lane to read the "y" position of the current lane inside the array (line 19); divided by the height of the channel group, which in this example is 2 (row 20); add the element index and vector base address within the vector to compute the address of the particular element and handle any possible misalignment (lines 21-22), compute the modulus N to determine the group index (line 23), and retrieve the data from the remote access (line 24).
As shown in FIG. 5, the execution lane stores the local LUT values in a displacement buffer in association with the calculated location for the displacement (525). The displacement buffer may be another assigned memory block in the local memory bank of the execution lane. Alternatively, the displacement buffer may be a register set of the execution lane.
In any case, the execution lane writes the local LUT value in a location in the displacement buffer corresponding to the phase of the group of execution lanes. This means that: the execution lane stores the read LUT values in a displacement buffer in association with their correct locations in the structured data.
The following code segment obtains the remote data in t1025, and stores the remote data at the computed index in t1027 in a displacement buffer, represented in this example as four memory mapped registers (spill1012 to spill 1015).
// store into a Shift buffer
25,vstpm.b16 spill1012,spill1013,spill1014,spill1015<-memimm(255),t1027,t1025,internal1,null,null,null,null；
The execution channel shifts the partition index and receives a remote partition index (530). As described above with reference to FIG. 2, by shifting the partition index, each execution channel may perform remote reads on a different execution channel. In a running example, the following example code causes a partition index to be moved to an execution channel in another group.
// sending the index to the neighbor set
26,shift.b16 t1028<-t1013,0,-2；
The execution channel reads the remote LUT values using the remote partition index (535). Since all execution channels in a group may read from different addresses, these remote reads may also be serialized with load and mask instructions.
The execution lane calculates the location for the displacement for the remote LUT values (540). Typically, this can be done by modulo the number of partitions by adding a previous position. In the operational example, this may be accomplished by an "add" instruction and an "and" instruction.
Calculating the channel position for the Displacement
35,add.b16 t1038<-t1015,1；
36,and.b16 t1039<-t1038,3；
The execution channel shifts the remote LUT value and receives the remote LUT value (545). In other words, the execution shifts back the LUT values it reads and receives LUT values for which another execution channel reads.
// sending data back to neighboring groups
37,shift.b16 t1037<-t1036,0,2；
The execution lane stores the received remote LUT values in a displacement buffer in association with the location for displacement (550). In other words, the execution lane takes into account its set phase to store the newly read LUT values in a displacement buffer.
The execution channel then repeats steps 530 through 550 for each of the other remaining partitions. Thus, if more partitions remain (555), the execution channel again shifts the partition index and receives the remote partition index (branch to 530).
In the example of execution, the subsequent instruction is similar to the second read sequence, with the expected shift distance changed.
39,shift.b16 t1040<-t1013,0,-4；
40,lddm.b16 t1041<-0,1,t1040；
41,lddm.b16 t1042<-t1041,2,t1040；
42,lddm.b16 t1043<-t1042,4,t1040；
43,lddm.b16 t1044<-t1043,8,t1040；
44,lddm.b16 t1045<-t1044,16,t1040；
45,lddm.b16 t1046<-t1045,32,t1040；
46,lddm.b16 t1047<-t1046,64,t1040；
47,lddm.b16 t1048<-t1047,128,t1040；
48,add.b16 t1050<-t1015,2；
49,and.b16 t1051<-t1050,3；
50,shift.b16 t1049<-t1048,0,4；
51,vstpm.b16 spill1020,spill1021,spill1022,spill1023<-memimm(255),t1051,t1049,internal1,spill1016,spill1017,spill1018,spill1019；
52,shift.b16 t1052<-t1013,0,-6；
53,lddm.b16 t1053<-0,1,t1052；
54,lddm.b16 t1054<-t1053,2,t1052；
55,lddm.b16 t1055<-t1054,4,t1052；
56,lddm.b16 t1056<-t1055,8,t1052；
57,lddm.b16 t1057<-t1056,16,t1052；
58,lddm.b16 t1058<-t1057,32,t1052；
59,lddm.b16 t1059<-t1058,64,t1052；
60,lddm.b16 t1060<-t1059,128,t1052；
61,add.b16 t1062<-t1015,3；
62,and.b16 t1063<-t1062,3；
63,shift.b16 t1061<-t1060,0,6；
64,vstpm.b16 spill1024,spill1025,spill1026,spill1027<-memimm(255),t1063,t1061,internal1,spill1020,spill1021,spill1022,spill1023；
If no more partitions remain (555), then the execution channel has successfully read all the values of the structured data element.
Thus, the execution lane may use the data by reading from the displacement buffer (branch to 560).
In the running example, reading is simple, since the displacement buffer already includes all data in the correct order.
FIG. 7 illustrates components of an example stencil processor. An stencil processor is an example of a processing component that may be used by an image processor to implement the techniques described above. The image processor may have one or more stencil processors that may be programmed to coordinate to accomplish different phases of the processing task. In commonly owned U.S. patent application serial No. 14/694,828; other suitable architectures that may utilize an stencil processor are described in greater detail in serial No. 14/694,815, serial No. 14/694,806, serial No. 14/960,334, and serial No. 15/352,260, which are incorporated herein by reference.
As shown in fig. 7, stencil processor 700 includes a data computation unit 701, a scalar processor 702, an associated scalar memory bank 703, and an I/O unit 704. The data computation unit 701 comprises an execution channel array 705, a two-dimensional shift register array 706 and separate random access memory banks 707_1 to 707_ R, which are each associated with a respective row or column of the two-dimensional execution channel array 706.
The I/O unit 704 is responsible for loading input data sheets received from a sheet generator of the image processor into the data calculation unit 701 and storing output data sheets from the image processor into the sheet generator. Loading the input data sheet into the data calculation unit 701 may include: the received tiles are parsed into rows or columns of image data and the rows or columns of image data are loaded into a two-dimensional shift register array 706 or into a corresponding bank of memory 707 of the rows/columns of the execution channel array. If the input tile is initially loaded into memory bank 307, then individual execution channels within execution channel array 705 may then load the tile data from random access memory bank 307 into corresponding portions of two-dimensional shift register array 706 as appropriate (e.g., due to a load instruction that precedes only the operation of the data on the tile). After completion of either loading a data slice directly from the slice generator or from memory 307 into shift register array 706, the execution channels of execution channel array 705 may operate on the data and eventually write the completed data as a slice directly back to the slice generator or into random access memory bank 707. If the latter, I/O unit 704 may retrieve data from random access memory bank 707 to form an output tile, which may then be forwarded to a tile generator.
The scalar processor 702 includes a program controller 709, and the program controller 709 may read instructions of program code of the stencil processor from the scalar memory 703 and issue the instructions to the execution channels in the execution channel array 705. In some embodiments, a single identical instruction is broadcast to all execution lanes within execution lane array 705 to enable the behavior of a single instruction, multiple similar data from data computation unit 701. In some embodiments, the instruction format of instructions read from scalar memory 703 and issued to the execution channels of execution channel array 705 comprises a Very Long Instruction Word (VLIW) type format that includes more than one opcode per instruction. In another embodiment, the VLIW format includes ALU opcodes that direct mathematical functions to be performed by the ALUs of each execution lane and memory opcodes that direct memory operations for a particular execution lane or set of execution lanes.
Each execution channel is a component having one or more execution units (e.g., logic circuitry that can execute instructions) that are capable of executing instructions. In addition to execution units only, execution channels may also include other processor-like functionality. For example, an execution lane may include logic circuitry to decode a received instruction in addition to one or more execution units, or in the case of a more MIMD-like design, to fetch and decode an instruction. With respect to MIMD-like methods, distributed methods may be implemented in various alternative embodiments, for example, with a program controller within each execution lane of execution lane array 705.
The combination of the execution lane array 705, the program controller 709, and the two-dimensional shift register array 706 provides a widely adaptable and configurable hardware platform for a wide range of programmable functions. For example, an application software developer may generate kernel programs having a variety of different functional capabilities and sizes (e.g., mask sizes) because a single execution channel is capable of performing multiple functions and easily accessing input image data near any output location in the two-dimensional shift register array 706.
In addition to serving as data storage for image data used to perform operations for the channel array 705, the random access memory bank 707 may also hold one or more look-up tables. In various embodiments, one or more scalar look-up tables may also be instantiated within scalar memory 703. Scalar lookups involve: the same data value from the same lookup table from the same index is passed to each execution lane within execution lane array 705. The VLIW instruction format may include a scalar operation code that directs a lookup operation performed by a scalar processor into a scalar lookup table. The index designated for use with the opcode may be an immediate operand or the index may be retrieved from some other data storage location. Looking up from a scalar lookup table within scalar memory 703 may involve: the same data value is broadcast to all execution channels within execution channel array 705 during the same clock cycle.
FIG. 8 illustrates an example data computation component 801. As shown in FIG. 8, the data computation element 801 includes an array 805 of execution lanes logically "above" a two-dimensional shift register array 806. As discussed above, in various embodiments, the image data tiles provided by the tile generator are loaded into the two-dimensional shift register array 806. The execution lane may then operate on the tile data from the two-dimensional shift register array 806.
Some significant architectural features of the data computation unit 801 include a shift register array 406 having a wider dimension than the execution lane array 805. That is, there is an "outer ring (halo)" of registers 809 outside of execution lane array 805. Although the outer rings 809 are shown as being present on both sides of the execution channel array 805, depending on the implementation, the outer rings may be present on fewer (one) or more (three or four) sides of the execution channel array 805. Outer loop 809 is used to provide "overflow" space for data that overflows outside the boundaries of execution lane array 805 as it shifts "below" execution lane 805. As a simple case, a 5 x 5 mask centered at the right edge of execution lane array 805 may require four outer ring register locations further to the right as the leftmost pixels of the mask are processed. For ease of drawing, in some embodiments, fig. 8 illustrates registers on the right side of the outer ring as having only horizontal shift connections and registers on the bottom side of the outer ring as having only vertical shift connections, when registers on either side (right, bottom) will have horizontal and vertical connections.
Additional overflow space is provided by a random access memory bank 807 coupled to each row and/or each column in the array, or a portion thereof, e.g., a random access memory bank may be allocated to a "region" of 4 execution lanes across a row range and 2 execution lanes across a column range of execution lane array 805. For simplicity, some portions of this specification relate primarily to row and/or column based assignment schemes. Here, if the kernel program operation of the execution channel requires pixel values that it may need to process some image processing routines outside the two-dimensional shift register array 806, the plane of image data may overflow further, e.g., from the outer ring area 809 to the random access memory bank 807. For example, consider a 6x6 mask in which the hardware includes an outer ring region of only four storage elements to the right of an execution channel on the right edge of the execution channel array. In this case, it would be necessary to further shift the data to the right beyond the right edge of the outer ring 809 to fully process the mask. The data shifted outside the outer ring area 809 will then spill over to the random access memory 807.
Embodiments of the subject matter described in this specification and the functional operations may be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware (including the structures disclosed in this specification and their structural equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices and machines for processing data, including: such as a programmable processor, a computer, or multiple processors or computers. The apparatus can also be or further include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, app, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
For a system of one or more computers to be configured to perform a particular operation or action means that software, firmware, hardware, or a combination thereof, that in operation causes the system to perform the operation or action, has been installed on the system. By one or more computer programs to be configured to perform particular operations or actions is meant that the one or more programs include instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in combination with, special purpose logic circuitry, e.g., an FPGA or an ASIC, or a combination of special purpose logic circuitry and one or more programmed computers.
A computer suitable for executing a computer program may be based on a general-purpose or special-purpose microprocessor or both or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of the computer are: a central processing unit for carrying out or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such a device. Moreover, the computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game player, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including: for example, semiconductor memory devices (e.g., EPROM, EEPROM, and flash memory devices); magnetic disks (e.g., internal hard disks or removable disks); magneto-optical disks; as well as CD ROM discs and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having: a display device for displaying information to a user, for example, a CRT (cathode ray tube) or LCD (liquid crystal display) monitor; and a keyboard and pointing device such as a mouse, trackball, or presence-sensitive display or other surface on which a user may provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by sending documents to and receiving documents from a device used by the user (e.g., by sending web pages to a web browser on the user's device in response to requests received from the web browser). Moreover, a computer may interact with a user by sending a text message or other form of message to a personal device (e.g., a smartphone), running a messaging application, and then receiving a response message from the user.
In addition to the embodiments described above, the following embodiments are also innovative:
receiving an input kernel program for an image processor having a two-dimensional array of execution lanes, an array of shift registers, and a plurality of banks of memory, wherein a plurality of execution lanes in each of a plurality of execution lane groups are configured to share a same respective bank of the plurality of banks of memory of the image processor;
determining that the kernel program has instructions to read a lookup table value of a lookup table partitioned across the plurality of memory banks; and
in response, the instructions in the kernel program are replaced with a sequence of instructions that, when executed by an execution lane, cause the execution lane to read a first value from a local memory bank and to read a second value from the local memory bank on behalf of another execution lane belonging to a different execution lane group.
calculating a local partition index from the original lookup table index;
reading a local lookup table value from a local memory bank using the local partition index;
shifting the local partition index through the shift register array and receiving a remote partition index through the shift register array from a different execution lane;
reading a remote lookup table value from the local memory bank using the remote partition index; and
a local partition index is shifted through the shift register array, and a remote partition index is received from a different execution lane through the shift register array.
remote lookup table values read from a remote memory bank by different execution channels are received.
Embodiment 4 is the method of embodiment 3, wherein the operations further comprise:
selecting between the local lookup table value or the remote lookup table value.
Embodiment 5 is the method of embodiment 4, wherein the operations further comprise:
selecting the local lookup table value if the original lookup table index modulo N is equal to a partition number of the execution channel, where N is a number of partitions of the lookup table.
Embodiment 6 is the method of any of embodiments 1-5, wherein the sequence of instructions causes each execution channel to read multiple lookup table values for each single lookup table access in the input kernel program.
Embodiment 7 is the method of embodiment 6, wherein the lookup table is partitioned such that all even indices are stored in the first bank and all odd indices are stored in the second bank.
Embodiment 8 is the method of any of embodiments 1-7, wherein the lookup table is larger than each of the banks of memory.
Embodiment 9 is the method of any of embodiments 1-8, wherein each execution channel is capable of reading from only a respective one of the plurality of memory banks.
Embodiment 10 is the method of any of embodiments 1-9, wherein the lookup table values are structured values having a width greater than a size of a register of the image processor, and wherein the sequence of instructions cause an execution lane to perform operations comprising:
reading a local lookup table value using a local partition index;
calculating a location of the local lookup table value in a displacement buffer, wherein the location depends on a group phase of the execution lane; and
storing the local lookup table values in the displacement buffer in association with the calculated position.
Embodiment 11 is the method of embodiment 10, wherein the operations further comprise:
receiving remote lookup table values read from different memory banks by different execution channels;
calculating a second location of the remote lookup table value in the displacement buffer based on the set of phases for the execution lane; and
storing the remote lookup table value in the displacement buffer in association with the second location.
Embodiment 12 is the method of embodiment 10, wherein the structured value is a vector having multiple elements or double wide data types.
Embodiment 13 is a system, comprising: one or more computers and one or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform the method of any of embodiments 1-12.
Embodiment 14 is a computer storage medium encoded with a computer program, the program comprising instructions operable, when executed by data processing apparatus, to cause the data processing apparatus to perform the method according to any of embodiments 1 to 12.
Embodiment 15 is a processor, comprising:
a two-dimensional execution channel array;
a two-dimensional shift register array; and
a plurality of memory banks, wherein a plurality of execution lanes in each of a plurality of execution lane groups are configured to share a same respective memory bank of the plurality of memory banks of the processor,
wherein each execution lane is configured to execute a sequence of instructions to obtain a lookup table value, wherein the sequence of instructions causes each execution lane to perform operations comprising:
calculating a local partition index from the original lookup table index;
reading a local lookup table value from a local memory bank using the local partition index;
shifting the local partition index through the shift register array and receiving a remote partition index through the shift register array from a different execution lane;
reading a remote lookup table value from the local memory bank using the remote partition index; and
shifting the remote lookup table value back through the shift register array to the different execution channel.
Embodiment 16 is the processor of embodiment 15, wherein the operations further comprise:
remote lookup table values read from a remote memory bank by different execution channels are received.
Embodiment 17 is the processor of embodiment 16, wherein the operations further comprise:
selecting between the local lookup table value or the remote lookup table value.
Embodiment 18 is the processor of embodiment 17, wherein the operations further comprise:
selecting the local lookup table value if the original lookup table index modulo N is equal to a partition number of the execution channel, where N is a number of partitions of the lookup table.
Embodiment 19 is the processor of any one of embodiments 15 to 18, wherein the sequence of instructions causes each execution channel to read multiple lookup table values for each single lookup table access in the input kernel program.
Embodiment 20 is the processor of embodiment 19, wherein the lookup table is partitioned such that all even indices are stored in the first bank and all odd indices are stored in the second bank.
Embodiment 21 is the processor of any of embodiments 15-20, wherein the lookup table is larger than each of the banks of memory.
Embodiment 22 is the processor of any one of embodiments 15 to 21, wherein each execution channel is capable of reading from only a respective one of a plurality of memory banks.
Embodiment 23 is the processor of any of embodiments 15 to 22, wherein the lookup table values are structured values having a width greater than a size of a register of the image processor, and wherein the sequence of instructions cause an execution lane to perform operations comprising:
reading a local lookup table value using a local partition index;
calculating a location of the local lookup table value in a displacement buffer, wherein the location depends on a group phase of the execution lane; and
storing the local lookup table values in the displacement buffer in association with the calculated position.
Embodiment 24 is the processor of embodiment 23, wherein the operations further comprise:
receiving remote lookup table values read from different memory banks by different execution channels;
calculating a second location of the remote lookup table value in the displacement buffer based on the set of phases for the execution lane; and
storing the remote lookup table value in the displacement buffer in association with the second location.
Embodiment 25 is the processor of embodiment 23, wherein the structured value is a vector having multiple elements or a double wide data type.
Embodiment 26 is a method, comprising: performing the operations of any of embodiments 15-25.
Embodiment 27 is a computer storage medium encoded with a computer program, the program comprising instructions that, when executed by a processor, are operable to cause the processor to perform operations according to any of embodiments 15 to 25.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a second embodiment. And claimed combinations may be directed to subcombinations or variations of subcombinations.
Also, while operations are shown in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order described or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the present subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions set forth in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain cases, multitasking and parallel processing may be advantageous.
Claims (21)
1. A computer-implemented method, the computer-implemented method comprising:
receiving an input kernel program for an image processor having a two-dimensional array of execution lanes, an array of shift registers, and a plurality of banks of memory, wherein a plurality of execution lanes in each of a plurality of execution lane groups are configured to share a same respective bank of the plurality of banks of memory of the image processor;
determining that the kernel program has instructions to read a lookup table value of a lookup table partitioned across the plurality of memory banks; and
in response, the instructions in the kernel program are replaced with a sequence of instructions that, when executed by an execution lane, cause the execution lane to read a first value from a local memory bank and to read a second value from the local memory bank on behalf of another execution lane belonging to a different execution lane group.
2. The method of claim 1, wherein the sequence of instructions cause an execution lane to perform operations comprising:
calculating a local partition index from the original lookup table index;
reading a local lookup table value from a local memory bank using the local partition index;
shifting the local partition index through the shift register array and receiving a remote partition index through the shift register array from a different execution lane;
reading a remote lookup table value from the local memory bank using the remote partition index; and
shifting the remote lookup table value back through the shift register array to the different execution channel.
3. The method of claim 2, wherein the operations further comprise:
remote lookup table values read from a remote memory bank by different execution channels are received.
4. The method of claim 3, wherein the operations further comprise:
selecting between the local lookup table value or the remote lookup table value.
5. The method of claim 4, wherein the operations further comprise:
selecting the local lookup table value if the original lookup table index modulo N is equal to a partition number of the execution channel, where N is a number of partitions of the lookup table.
6. The method of claim 1, wherein the sequence of instructions causes each execution channel to read multiple lookup table values for each single lookup table access in the input kernel program.
7. The method of claim 6, wherein the lookup table is partitioned such that all even indices are stored in a first bank of memory and all odd indices are stored in a second bank of memory.
8. The method of claim 1, wherein the lookup table is larger than each of the banks of memory.
9. The method of claim 1, wherein each execution channel is capable of reading from only a corresponding one of the plurality of memory banks.
10. The method of claim 1, wherein the lookup table value is a structured value having a width greater than a size of a register of the image processor, and wherein the sequence of instructions causes an execution lane to perform operations comprising:
reading a local lookup table value using a local partition index;
calculating a location of the local lookup table value in a displacement buffer, wherein the location depends on a group phase of the execution lane; and
storing the local lookup table values in the displacement buffer in association with the calculated position.
11. The method of claim 10, wherein the operations further comprise:
receiving remote lookup table values read from different memory banks by different execution channels;
calculating a second location of the remote lookup table value in the displacement buffer based on the set of phases for the execution lane; and
storing the remote lookup table value in the displacement buffer in association with the second location.
12. The method of claim 10, wherein the structured value is a vector having multiple elements or a double-wide data type.
13. A system, the system comprising:
one or more computers and one or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:
receiving an input kernel program for an image processor having a two-dimensional array of execution lanes, an array of shift registers, and a plurality of banks of memory, wherein a plurality of execution lanes in each of a plurality of execution lane groups are configured to share a same respective bank of the plurality of banks of memory of the image processor;
determining that the kernel program has instructions to read a lookup table value of a lookup table partitioned across the plurality of memory banks; and
in response, the instructions in the kernel program are replaced with a sequence of instructions that, when executed by an execution lane, cause the execution lane to read a first value from a local memory bank and to read a second value from the local memory bank on behalf of another execution lane belonging to a different execution lane group.
14. The system of claim 13, wherein the sequence of instructions cause an execution lane to perform operations comprising:
calculating a local partition index from the original lookup table index;
reading a local lookup table value from a local memory bank using the local partition index;
shifting the local partition index through the shift register array and receiving a remote partition index through the shift register array from a different execution lane;
reading a remote lookup table value from the local memory bank using the remote partition index; and
shifting the remote lookup table value back through the shift register array to the different execution channel.
15. The system of claim 14, wherein the operations further comprise:
remote lookup table values read from a remote memory bank by different execution channels are received.
16. The system of claim 15, wherein the operations further comprise:
selecting between the local lookup table value or the remote lookup table value.
17. The system of claim 16, wherein the operations further comprise:
selecting the local lookup table value if the original lookup table index modulo N is equal to a partition number of the execution channel, where N is a number of partitions of the lookup table.
18. The system of claim 13, wherein the sequence of instructions causes each execution channel to read multiple lookup table values for each single lookup table access in the input kernel program.
19. The system of claim 18, wherein the lookup table is partitioned such that all even indices are stored in a first bank of memory and all odd indices are stored in a second bank of memory.
20. The system of claim 13, wherein the lookup table is larger than each of the banks of memory.
21. A computer program product encoded on one or more non-transitory computer storage media, the computer program product comprising instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:
receiving an input kernel program for an image processor having a two-dimensional array of execution lanes, an array of shift registers, and a plurality of banks of memory, wherein a plurality of execution lanes in each of a plurality of execution lane groups are configured to share a same respective bank of the plurality of banks of memory of the image processor;
determining that the kernel program has instructions to read a lookup table value of a lookup table partitioned across the plurality of memory banks; and
in response, the instructions in the kernel program are replaced with a sequence of instructions that, when executed by an execution lane, cause the execution lane to read a first value from a local memory bank and to read a second value from the local memory bank on behalf of another execution lane belonging to a different execution lane group.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862636055P | 2018-02-27 | 2018-02-27 | |
US62/636,055 | 2018-02-27 | ||
PCT/US2019/019020 WO2019168739A1 (en) | 2018-02-27 | 2019-02-21 | Large lookup tables for an image processor |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112005213A true CN112005213A (en) | 2020-11-27 |
Family
ID=65724525
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980022634.8A Pending CN112005213A (en) | 2018-02-27 | 2019-02-21 | Large lookup tables for image processors |
Country Status (5)
Country | Link |
---|---|
US (1) | US11321802B2 (en) |
EP (1) | EP3743806B1 (en) |
CN (1) | CN112005213A (en) |
TW (1) | TWI794423B (en) |
WO (1) | WO2019168739A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2022016257A1 (en) * | 2020-07-21 | 2022-01-27 | The Governing Council Of The University Of Toronto | System and method for using sparsity to accelerate deep learning networks |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105874437A (en) * | 2013-12-31 | 2016-08-17 | 三星电子株式会社 | Memory management method and apparatus |
WO2016171846A1 (en) * | 2015-04-23 | 2016-10-27 | Google Inc. | Compiler for translating between a virtual image processor instruction set architecture (isa) and target hardware having a two-dimensional shift array structure |
CN107133908A (en) * | 2016-02-26 | 2017-09-05 | 谷歌公司 | Compiler for image processor manages memory |
US20170344369A1 (en) * | 2014-12-15 | 2017-11-30 | Samsung Electronics Co., Ltd. | Method and apparatus for memory access |
CN107563953A (en) * | 2016-07-01 | 2018-01-09 | 谷歌公司 | The blocks operation of channel array and the image processor of two-dimensional shift register is performed with two dimension |
CN107610035A (en) * | 2017-09-11 | 2018-01-19 | 郑州云海信息技术有限公司 | A kind of method and system for handling image |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5038300A (en) * | 1988-06-29 | 1991-08-06 | Digital Equipment Corporation | Extendable-size color look-up table for computer graphics systems |
US6397324B1 (en) * | 1999-06-18 | 2002-05-28 | Bops, Inc. | Accessing tables in memory banks using load and store address generators sharing store read port of compute register file separated from address register file |
US20130212353A1 (en) * | 2002-02-04 | 2013-08-15 | Tibet MIMAR | System for implementing vector look-up table operations in a SIMD processor |
JP4013887B2 (en) | 2003-10-30 | 2007-11-28 | セイコーエプソン株式会社 | Image processing circuit, image display device, and image processing method |
US8868677B2 (en) | 2012-04-16 | 2014-10-21 | HGST Netherlands B.V. | Automated data migration across a plurality of devices |
US20180005346A1 (en) | 2016-07-01 | 2018-01-04 | Google Inc. | Core Processes For Block Operations On An Image Processor Having A Two-Dimensional Execution Lane Array and A Two-Dimensional Shift Register |
US10552939B1 (en) * | 2019-02-12 | 2020-02-04 | Google Llc | Image processor complex transfer functions |
-
2019
- 2019-02-21 CN CN201980022634.8A patent/CN112005213A/en active Pending
- 2019-02-21 US US16/976,316 patent/US11321802B2/en active Active
- 2019-02-21 WO PCT/US2019/019020 patent/WO2019168739A1/en unknown
- 2019-02-21 EP EP19710205.6A patent/EP3743806B1/en active Active
- 2019-02-26 TW TW108106484A patent/TWI794423B/en active
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105874437A (en) * | 2013-12-31 | 2016-08-17 | 三星电子株式会社 | Memory management method and apparatus |
US20170344369A1 (en) * | 2014-12-15 | 2017-11-30 | Samsung Electronics Co., Ltd. | Method and apparatus for memory access |
WO2016171846A1 (en) * | 2015-04-23 | 2016-10-27 | Google Inc. | Compiler for translating between a virtual image processor instruction set architecture (isa) and target hardware having a two-dimensional shift array structure |
CN107133908A (en) * | 2016-02-26 | 2017-09-05 | 谷歌公司 | Compiler for image processor manages memory |
CN107563953A (en) * | 2016-07-01 | 2018-01-09 | 谷歌公司 | The blocks operation of channel array and the image processor of two-dimensional shift register is performed with two dimension |
CN107610035A (en) * | 2017-09-11 | 2018-01-19 | 郑州云海信息技术有限公司 | A kind of method and system for handling image |
Non-Patent Citations (1)
Title |
---|
窦勇;邬贵明;徐进辉;周兴铭;: "支持循环自动流水线的粗粒度可重构阵列体系结构", 中国科学(E辑:信息科学), no. 04 * |
Also Published As
Publication number | Publication date |
---|---|
TW201937919A (en) | 2019-09-16 |
EP3743806B1 (en) | 2023-08-30 |
WO2019168739A1 (en) | 2019-09-06 |
US20210042875A1 (en) | 2021-02-11 |
EP3743806A1 (en) | 2020-12-02 |
US11321802B2 (en) | 2022-05-03 |
TWI794423B (en) | 2023-03-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10531030B2 (en) | Block operations for an image processor having a two-dimensional execution lane array and a two-dimensional shift register | |
US11204976B2 (en) | Expanded kernel generation | |
KR102278658B1 (en) | Architecture for high performance, power efficient, programmable image processing | |
US20230306249A1 (en) | Transposed convolution using systolic array | |
US9978116B2 (en) | Core processes for block operations on an image processor having a two-dimensional execution lane array and a two-dimensional shift register | |
KR20190022627A (en) | Convolutional neural network on programmable two-dimensional image processor | |
CN107223237B (en) | Method and apparatus for memory access | |
EP3093757B1 (en) | Multi-dimensional sliding window operation for a vector processor | |
JP2018124867A (en) | Arithmetic processing device and control method therefor | |
CN115552371A (en) | Variable position shifting for matrix processing | |
CN112005213A (en) | Large lookup tables for image processors | |
TWI759373B (en) | Replicate elements instruction | |
TWI722684B (en) | Computer-implemented methods and non-transitory computer storage media related to image processor complex transfer functions and computing devices employing an image processor | |
US11915338B2 (en) | Loading apparatus and method for convolution with stride or dilation of 2 | |
US20230214236A1 (en) | Masking row or column positions for matrix processing | |
WO2020059156A1 (en) | Data processing system, method, and program | |
US20210272232A1 (en) | Filter Independent L1 Mapping Of Convolution Data Into General Purpose Register |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |