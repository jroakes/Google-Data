US20150169070A1 - Visual Display of Interactive, Gesture-Controlled, Three-Dimensional (3D) Models for Head-Mountable Displays (HMDs) - Google Patents
Visual Display of Interactive, Gesture-Controlled, Three-Dimensional (3D) Models for Head-Mountable Displays (HMDs) Download PDFInfo
- Publication number
- US20150169070A1 US20150169070A1 US14/108,365 US201314108365A US2015169070A1 US 20150169070 A1 US20150169070 A1 US 20150169070A1 US 201314108365 A US201314108365 A US 201314108365A US 2015169070 A1 US2015169070 A1 US 2015169070A1
- Authority
- US
- United States
- Prior art keywords
- model
- gesture
- hmd
- axis
- display
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
- 230000002452 interceptive effect Effects 0.000 title 1
- 230000000007 visual effect Effects 0.000 title 1
- 238000000034 method Methods 0.000 claims abstract description 37
- 230000004044 response Effects 0.000 claims description 32
- 230000006870 function Effects 0.000 claims description 26
- 230000033001 locomotion Effects 0.000 description 85
- 210000003811 finger Anatomy 0.000 description 21
- QONODMQQXMILPX-UHFFFAOYSA-N N-(3-chloro-5-fluorophenyl)-2-nitro-4-(trifluoromethylsulfonyl)aniline Chemical compound [O-][N+](=O)c1cc(ccc1Nc1cc(F)cc(Cl)c1)S(=O)(=O)C(F)(F)F QONODMQQXMILPX-UHFFFAOYSA-N 0.000 description 20
- 238000004891 communication Methods 0.000 description 20
- 238000003491 array Methods 0.000 description 17
- 210000003813 thumb Anatomy 0.000 description 17
- 238000013500 data storage Methods 0.000 description 14
- 210000004247 hand Anatomy 0.000 description 13
- 230000008859 change Effects 0.000 description 9
- 238000010586 diagram Methods 0.000 description 7
- 210000003128 head Anatomy 0.000 description 7
- 230000003287 optical effect Effects 0.000 description 7
- 230000009471 action Effects 0.000 description 6
- 230000005540 biological transmission Effects 0.000 description 6
- 210000000988 bone and bone Anatomy 0.000 description 6
- 238000012545 processing Methods 0.000 description 6
- 208000013057 hereditary mucoepithelial dysplasia Diseases 0.000 description 5
- 230000003993 interaction Effects 0.000 description 5
- 239000000463 material Substances 0.000 description 5
- 230000004886 head movement Effects 0.000 description 4
- 238000013499 data model Methods 0.000 description 3
- 238000013461 design Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 230000004424 eye movement Effects 0.000 description 3
- 238000013507 mapping Methods 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 3
- 238000009877 rendering Methods 0.000 description 3
- 230000003190 augmentative effect Effects 0.000 description 2
- 238000004422 calculation algorithm Methods 0.000 description 2
- 239000011248 coating agent Substances 0.000 description 2
- 238000000576 coating method Methods 0.000 description 2
- 210000005069 ears Anatomy 0.000 description 2
- 238000001914 filtration Methods 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 230000008569 process Effects 0.000 description 2
- 230000005236 sound signal Effects 0.000 description 2
- 230000008901 benefit Effects 0.000 description 1
- 210000000959 ear middle Anatomy 0.000 description 1
- 238000005538 encapsulation Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 230000008447 perception Effects 0.000 description 1
- 230000005043 peripheral vision Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 210000001525 retina Anatomy 0.000 description 1
- 238000013515 script Methods 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000012163 sequencing technique Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000007480 spreading Effects 0.000 description 1
- 238000010897 surface acoustic wave method Methods 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/017—Gesture based interaction, e.g. based on a set of recognized hand gestures
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/20—Editing of 3D images, e.g. changing shapes or colours, aligning objects or positioning parts
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/0132—Head-up displays characterised by optical features comprising binocular systems
- G02B2027/0134—Head-up displays characterised by optical features comprising binocular systems of stereoscopic type
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/014—Head-up displays characterised by optical features comprising information/image processing systems
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B2027/0178—Eyeglass type
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0179—Display position adjusting means not related to the information to be displayed
- G02B2027/0187—Display position adjusting means not related to the information to be displayed slaved to motion of at least a part of the body of the user, e.g. head, eye
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2219/00—Indexing scheme for manipulating 3D models or images for computer graphics
- G06T2219/20—Indexing scheme for editing of 3D models
- G06T2219/2004—Aligning objects, relative positioning of parts
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2219/00—Indexing scheme for manipulating 3D models or images for computer graphics
- G06T2219/20—Indexing scheme for editing of 3D models
- G06T2219/2016—Rotation, translation, scaling
Definitions
- three-dimensional (3D) modeling involves generation of a representation of a 3D surface of an object.
- the representation may be referred to as a 3D object data model, or simply a 3D model, and can be rendered or displayed as a two-dimensional image via 3D rendering or displayed as a three-dimensional image.
- a 3D object data model may represent a 3D object using a collection of points in 3D space, connected by various geometric entities such as triangles, lines, curved surfaces, etc.
- a method in one aspect, includes sending a 3D-model query from a head-mountable display (HMD).
- the 3D-model query includes information about an object.
- the HMD receives a 3D model for the object.
- the 3D model includes three-dimensional shape and texture information about the object.
- the HMD displays a view of the 3D model.
- the HMD receives an input gesture. It is determined whether the input gesture is related to the 3D model. After determining that the input gesture is related to the 3D model, the HMD updates the view of the 3D model based on the input gesture and displays the updated view of the 3D model.
- an HMD in another aspect, includes a display, a processor, and a non-transitory computer readable medium.
- the non-transitory computer readable medium is configured to store at least instructions.
- the instructions are executable by the processor to cause the HMD to perform functions including: receiving a 3D model for an object, where the 3D model includes three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, where each of the first axis, the second axis, and the third axis differs; displaying a view of the 3D model using the display; receiving an input gesture; determining whether the input gesture includes a 3D-model gesture of a plurality of 3D-model gestures, where the plurality of 3D-model gestures include a first 3D-model gesture to modify display of the 3D model with respect to the first axis, a second 3D-model gesture to modify display
- a non-transitory computer readable medium has stored therein instructions executable by a processor of a head-mountable display (HMD) to cause the HMD to perform functions.
- the functions include: receiving a 3D model for an object, where the 3D model includes three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, where each of the first axis, the second axis, and the third axis differs; displaying a view of the 3D model; receiving an input gesture; determining whether the input gesture includes a 3D-model gesture of a plurality of 3D-model gestures, where the plurality of 3D-model gestures include a first 3D-model gesture to modify display of the 3D model with respect to the first axis, a second 3D-model gesture to modify display of the 3D model with respect to the
- a HMD includes: means for receiving a 3D model for an object, where the 3D model includes three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, where each of the first axis, the second axis, and the third axis differs; means for displaying a view of the 3D model; means for receiving an input gesture; means for determining whether the input gesture includes a 3D-model gesture of a plurality of 3D-model gestures, where the plurality of 3D-model gestures include a first 3D-model gesture to modify display of the 3D model with respect to the first axis, a second 3D-model gesture to modify display of the 3D model with respect to the second axis, and a third 3D-model gesture to modify display of the 3D model with respect to the third axis; and means for, after determining that the
- FIG. 1 is a flowchart of a method, in accordance with an example embodiment.
- FIG. 2A illustrates a HMD according to an example embodiment.
- FIG. 2B illustrates an alternate view of the HMD illustrated in FIG. 2A .
- FIG. 2C illustrates another HMD according to an example embodiment.
- FIG. 2D illustrates another HMD according to an example embodiment.
- FIGS. 2E to 2G are simplified illustrations of the HMD shown in FIG. 2D , being worn by a wearer.
- FIG. 3 is a block diagram of an HMD, in accordance with an example embodiment.
- FIG. 4A depicts a scenario of displaying a 3D model obtained in response to a 3D-model query, in accordance with an example embodiment.
- FIG. 4B depicts a variation of the scenario of FIG. 4A for displaying a 3D model in an environment, in accordance with an example embodiment.
- FIG. 4C depicts another variation of the scenario of FIG. 4A for displaying a 3D model in an environment, in accordance with an example embodiment.
- FIGS. 5A and 5B are charts of images captured over time with corresponding motions and gestures, in accordance with an example embodiment.
- FIG. 6 shows input gestures and corresponding changes to a 3D model, in accordance with an example embodiment.
- FIG. 7 depicts a distributed computing architecture, in accordance with an example embodiment.
- FIG. 8A is a block diagram of a computing device, in accordance with an example embodiment.
- FIG. 8B depicts a cloud-based server system, in accordance with an example embodiment.
- Example embodiments disclosed herein relate to requesting delivery of a 3D model using a query that includes image information and to controlling display of a 3D model using gesture inputs.
- Some devices such as some implementations of HMDs, have limited input mechanisms. For example, some HMDs may not include a keyboard or other touch-based interface. Then, textual input can be difficult using these HMDs. Therefore, other inputs, such as images, video, and/or spoken inputs can be used to generate search queries, such as queries for 3D models.
- a wearer of an HMD can capture one or more image(s) and/or video of an object.
- the HMD can formulate a query for a 3D model including the image(s) and/or video and provide the query to a 3D-model server.
- the 3D-model server can use the image(s) and/or video to identify the object, locate one or more 3D models of the object, and deliver some or all of the 3D models to the HMD.
- the HMD can generate a display of the 3D model(s).
- the HMD can capture images of the wearer's hands (or other objects) making gestures, perhaps using the same camera used to capture image(s) of the object.
- the images can be processed to determine hand motions and corresponding gestures.
- the gestures can be interpreted by the HMD as commands for interacting with the 3D model(s).
- the wearer can interact with the 3D model(s) by making gestures. For example, a wearer can move their hand in a vertical line downward to command the HMD to move the 3D model downward. As another example, the wearer can move their hand in a clockwise circular motion to command the HMD to rotate the 3D model clockwise.
- the herein-disclosed techniques can use a combination of cloud-based model servers and HMDs to request, render, and control 3D models using images as inputs to query for 3D models and to control rendering of the 3D models using hand gestures.
- cloud-based model servers and HMDs can be used to request, render, and control 3D models using images as inputs to query for 3D models and to control rendering of the 3D models using hand gestures.
- FIG. 1 is a flowchart of method 100 , in accordance with an example embodiment.
- Method 100 can be carried out by a HMD, such as HMD 272 discussed below.
- method 100 can be carried out by a computing device acting as an HMD, such as computing device 800 discussed below.
- Method 100 can begin at block 110 , where a HMD can receive a 3D model for an object.
- the 3D model can include three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, where each of the first axis, the second axis, and the third axis differs, such as discussed below in the context of at least FIG. 4A .
- the information about the object can include an image related to the object.
- the image related to the object can be at least one image selected from the group consisting of an image that depicts at least part of the object, an image of a Quick Response (QR) code related to the object, and an image of a bar code related to the object.
- QR Quick Response
- the HMD can display a view of the 3D model, such as discussed below in the context of at least FIG. 4A .
- the HMD can receive an input gesture, such as discussed below in the context of at least FIGS. 4B , 4 C, 5 A, 5 B, and 6 .
- the input gesture can include a gesture to rotate the 3D model.
- the gesture to rotate the 3D model can include a first gesture to rotate the 3D model on a first axis and a second gesture to rotate the 3D model on a second axis, and wherein the first axis differs from the second axis.
- the HMD can determine whether the input gesture is related to the 3D model.
- the HMD can determine whether the input gesture is related to the 3D model without communicating with another computing device; i.e., the HMD can determine whether the input gesture is related to the 3D model as a standalone device. In other particular embodiments, the HMD can determine whether the input gesture is related to the 3D model via communication with another computing device; e.g., the HMD can provide data related to the input gesture, such as video or image data, to an gesture server or other computing device, and in response, the gesture server can inform the HMD whether the input gesture is related to the 3D model so that the HMD can determine whether the input gesture is related to the 3D model.
- the HMD can update the view of the 3D model based on the input gesture and display the updated view of the 3D model.
- updating the view of the 3D model can include: rotating the 3D model as indicated by the gesture to rotate the 3D model and generating a display of the rotated 3D model.
- method 100 can further include: receiving a second input gesture at the HMD, determining whether the second input gesture corresponds to a close gesture, and in response to determining that the second input gesture corresponds to the close gesture, terminating the view of the 3D model.
- an example system may be implemented in or may take the form of a wearable computer (also referred to as a wearable computing device).
- a wearable computer takes the form of or includes a head-mountable display (HMD).
- HMD head-mountable display
- An example system may also be implemented in or take the form of other devices, such as a mobile phone, among other possibilities. Further, an example system may take the form of a non-transitory computer readable medium, which has program instructions stored thereon that are executable by at a processor to provide the functionality described herein. An example system may also take the form of a device such as a wearable computer or mobile phone, or a subsystem of such a device, which includes such a non-transitory computer readable medium having such program instructions stored thereon.
- An HMD may generally be any display device that is capable of being worn on the head and places a display in front of one or both eyes of the wearer.
- An HMD may take various forms such as a helmet or eyeglasses.
- references to “eyeglasses” or a “glasses-style” HMD should be understood to refer to an HMD that has a glasses-like frame so that it can be worn on the head.
- example embodiments may be implemented by or in association with an HMD with a single display or with two displays, which may be referred to as a “monocular” HMD or a “binocular” HMD, respectively.
- FIG. 2A illustrates a wearable computing system according to an example embodiment.
- the wearable computing system takes the form of HMD 202 . It should be understood, however, that example systems and devices may take the form of or be implemented within or in association with other types of devices, without departing from the scope of the invention.
- the HMD 202 includes frame elements including lens-frames 204 , 206 and a center frame support 208 , lens elements 210 , 212 , and extending side-arms 214 , 216 .
- the center frame support 208 and the extending side-arms 214 , 216 are configured to secure HMD 202 to a user's face via a user's nose and ears, respectively.
- Each of the frame elements 204 , 206 , and 208 and the extending side-arms 214 , 216 may be formed of a solid structure of plastic and/or metal, or may be formed of a hollow structure of similar material so as to allow wiring and component interconnects to be internally routed through HMD 202 . Other materials may be possible as well.
- each of the lens elements 210 , 212 may be formed of any material that can suitably display a projected image or graphic.
- Each of the lens elements 210 , 212 may also be sufficiently transparent to allow a user to see through the lens element. Combining these two features of the lens elements may facilitate an augmented reality or heads-up display where the projected image or graphic is superimposed over a real-world view as perceived by the user through the lens elements.
- the extending side-arms 214 , 216 may each be projections that extend away from the lens-frames 204 , 206 , respectively, and may be positioned behind a user's ears to secure the HMD 202 to the user.
- the extending side-arms 214 , 216 may further secure the HMD 202 to the user by extending around a rear portion of the user's head. Additionally or alternatively, for example, the HMD 202 may connect to or be affixed within a head-mounted helmet structure. Other configurations for an HMD are also possible.
- the HMD 202 may also include an on-board computing system 218 , an image capture device 220 , a sensor 222 , and a finger-operable touch pad 224 .
- the on-board computing system 218 is shown to be positioned on the extending side-arm 214 of the HMD 202 ; however, the on-board computing system 218 may be provided on other parts of the HMD 202 or may be positioned remote from the HMD 202 (e.g., the on-board computing system 218 could be wire- or wirelessly-connected to the HMD 202 ).
- the on-board computing system 218 may include a processor and memory, for example.
- the on-board computing system 218 may be configured to receive and analyze data from the image capture device 220 and the finger-operable touch pad 224 (and possibly from other sensory devices, user interfaces, or both) and generate images for output by the lens elements 210 and 212 .
- the image capture device 220 may be, for example, a camera that is configured to capture still images and/or video. In the illustrated configuration, image capture device 220 is positioned on the extending side-arm 214 of the HMD 202 ; however, the image capture device 220 may be provided on other parts of the HMD 202 .
- the image capture device 220 may be configured to capture images at various resolutions or at different frame rates. Many image capture devices with a small form-factor, such as the cameras used in mobile phones or webcams, for example, may be incorporated into an example of the HMD 202 .
- FIG. 2A illustrates one image capture device 220
- more than one image capture device may be used, and each may be configured to capture the same view, or to capture different views.
- the image capture device 220 may be forward facing to capture at least a portion of the real-world view perceived by the user. This forward facing image captured by the image capture device 220 may then be used to generate an augmented reality where computer generated images appear to interact with or overlay the real-world view perceived by the user.
- the sensor 222 is shown on the extending side-arm 216 of the HMD 202 ; however, the sensor 222 may be positioned on other parts of the HMD 202 .
- the HMD 202 may include multiple sensors.
- an HMD 202 may include sensors 222 such as one or more gyroscopes, one or more accelerometers, one or more magnetometers, one or more light sensors, one or more infrared sensors, and/or one or more microphones.
- Other sensing devices may be included in addition or in the alternative to the sensors that are specifically identified herein.
- the finger-operable touch pad 224 is shown on the extending side-arm 214 of the HMD 202 . However, the finger-operable touch pad 224 may be positioned on other parts of the HMD 202 . Also, more than one finger-operable touch pad may be present on the HMD 202 .
- the finger-operable touch pad 224 may be used by a user to input commands.
- the finger-operable touch pad 224 may sense at least one of a pressure, position and/or a movement of one or more fingers via capacitive sensing, resistance sensing, or a surface acoustic wave process, among other possibilities.
- the finger-operable touch pad 224 may be capable of sensing movement of one or more fingers simultaneously, in addition to sensing movement in a direction parallel or planar to the pad surface, in a direction normal to the pad surface, or both, and may also be capable of sensing a level of pressure applied to the touch pad surface.
- the finger-operable touch pad 224 may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger-operable touch pad 224 may be formed to have a raised, indented, or roughened surface, so as to provide tactile feedback to a user when the user's finger reaches the edge, or other area, of the finger-operable touch pad 224 . If more than one finger-operable touch pad is present, each finger-operable touch pad may be operated independently, and may provide a different function.
- HMD 202 may be configured to receive user input in various ways, in addition or in the alternative to user input received via finger-operable touch pad 224 .
- on-board computing system 218 may implement a speech-to-text process and utilize a syntax that maps certain spoken commands to certain actions.
- HMD 202 may include one or more microphones via which a wearer's speech may be captured. Configured as such, HMD 202 may be operable to detect spoken commands and carry out various computing functions that correspond to the spoken commands.
- HMD 202 may interpret certain head-movements as user input. For example, when HMD 202 is worn, HMD 202 may use one or more gyroscopes and/or one or more accelerometers to detect head movement. The HMD 202 may then interpret certain head movements as being user input, such as nodding, or looking up, down, left, or right. HMD 202 could also pan or scroll through graphics in a display according to movement. Other types of actions may also be mapped to head movement.
- HMD 202 may interpret certain gestures (e.g., by a wearer's hand or hands) as user input such as describe herein.
- HMD 202 may capture hand movements by analyzing image data from image capture device 220 , and initiate actions that are defined as corresponding to certain hand movements.
- HMD 202 may interpret eye movement as user input.
- HMD 202 may include one or more inward-facing image capture devices and/or one or more other inward-facing sensors (not shown) sense a user's eye movements and/or positioning.
- certain eye movements may be mapped to certain actions.
- certain actions may be defined as corresponding to movement of the eye in a certain direction, a blink, and/or a wink, among other possibilities.
- HMD 202 can include a speaker 225 for generating audio output.
- the speaker could be in the form of a bone conduction speaker, also referred to as a bone conduction transducer (BCT).
- Speaker 225 may be, for example, a vibration transducer or an electroacoustic transducer that produces sound in response to an electrical audio signal input.
- the frame of HMD 202 may be designed such that when a wearer wears HMD 202 , speaker 225 contacts the wearer.
- speaker 225 may be embedded within the frame of HMD 202 and positioned such that, when the HMD 202 is worn, speaker 225 vibrates a portion of the frame that contacts the wearer.
- HMD 202 may be configured to send an audio signal to speaker 225 , so that vibration of the speaker may be directly or indirectly transferred to the bone structure of the wearer.
- the wearer can interpret the vibrations provided by BCT 225 as sounds.
- bone-conduction transducers may be implemented, depending upon the particular implementation.
- any component that is arranged to vibrate the HMD 202 may be incorporated as a vibration transducer.
- an HMD 202 may include a single speaker 225 or multiple speakers.
- the location(s) of speaker(s) on the HMD may vary, depending upon the implementation. For example, a speaker may be located proximate to a wearer's temple (as shown), behind the wearer's ear, proximate to the wearer's nose, and/or at any other location where the speaker 225 can vibrate the wearer's bone structure.
- FIG. 2B illustrates an alternate view of the wearable computing device illustrated in FIG. 2A .
- the lens elements 210 , 212 may act as display elements.
- the HMD 202 may include a first projector 228 coupled to an inside surface of the extending side-arm 216 and configured to project a display 230 onto an inside surface of the lens element 212 .
- a second projector 232 may be coupled to an inside surface of the extending side-arm 214 and configured to project a display 234 onto an inside surface of the lens element 210 .
- the lens elements 210 , 212 may act as a combiner in a light projection system and may include a coating that reflects the light projected onto them from the projectors 228 , 232 . In some embodiments, a reflective coating may not be used (e.g., when the projectors 228 , 232 are scanning laser devices). In alternative embodiments, other types of display elements may also be used.
- the lens elements 210 , 212 themselves may include: a transparent or semi-transparent matrix display, such as an electroluminescent display or a liquid crystal display, one or more waveguides for delivering an image to the user's eyes, or other optical elements capable of delivering an in focus near-to-eye image to the user.
- a corresponding display driver may be disposed within the frame elements 204 , 206 for driving such a matrix display.
- a laser or LED source and scanning system could be used to draw a raster display directly onto the retina of one or more of the user's eyes. Other possibilities exist as well.
- FIG. 2C illustrates another wearable computing system according to an example embodiment, which takes the form of an HMD 252 .
- the HMD 252 may include some or all of the elements of HMD 202 described with respect to FIGS. 2A and 2B .
- the HMD 252 may additionally include an on-board computing system 254 and an image capture device 256 , as detailed above with respect to FIGS. 2A and 2B .
- the image capture device 256 is shown mounted on a frame of the HMD 252 . However, the image capture device 256 may be mounted at other positions as well.
- the HMD 252 may include a single display 258 which may be coupled to the device.
- the display 258 may be formed on one of the lens elements of the HMD 252 , such as a lens element described with respect to FIGS. 2A and 2B , and may be configured to overlay computer-generated graphics in the user's view of the physical world.
- the display 258 is shown to be provided in a center of a lens of the HMD 252 , however, the display 258 may be provided in other positions, such as for example towards either the upper or lower portions of the wearer's field of view.
- the display 258 is controllable via the computing system 254 that is coupled to the display 258 via an optical waveguide 260 .
- FIG. 2D illustrates another wearable computing system according to an example embodiment, which takes the form of a monocular HMD 272 .
- HMD 272 can include some or all of the elements of HMD 202 and/or HMD 252 , described above in the context of FIGS. 2A , 2 B, and 2 C.
- the HMD 272 may include side-arms 273 , a center frame support 274 , and a bridge portion with nosepiece 275 .
- the center frame support 274 connects the side-arms 273 .
- the HMD 272 does not include lens-frames containing lens elements.
- the HMD 272 may additionally include a component housing 276 , which may include an on-board computing system (not shown), an image capture device 278 , and a button 279 for operating the image capture device 278 (and/or usable for other purposes).
- Component housing 276 may also include other electrical components and/or may be electrically connected to electrical components at other locations within or on the HMD.
- HMD 272 can include a speaker, such as BCT 286 .
- HMD 272 may include a single display 280 , which may be coupled to one of the side-arms 273 via the component housing 276 .
- the display 280 may be a see-through display, which is made of glass and/or another transparent or translucent material, such that the wearer can see their environment through the display 280 .
- the component housing 276 may include the light sources (not shown) for the display 280 and/or optical elements (not shown) to direct light from the light sources to the display 280 .
- display 280 may include optical features that direct light that is generated by such light sources towards the wearer's eye, when HMD 272 is being worn.
- HMD 272 may include a sliding feature 284 , which may be used to adjust the length of the side-arms 273 .
- sliding feature 284 may be used to adjust the fit of HMD 272 .
- an HMD may include other features that allow a wearer to adjust the fit of the HMD, without departing from the scope of the invention.
- FIGS. 2E to 2G are simplified illustrations of the HMD 272 shown in FIG. 2D , being worn by a wearer 290 .
- BCT 286 is arranged such that when HMD 272 is worn, BCT 286 is located behind the wearer's ear. As such, BCT 286 is not visible from the perspective shown in FIG. 2E .
- the display 280 may be arranged such that when HMD 272 is worn, display 280 is positioned in front of or proximate to a user's eye when the HMD 272 is worn by a user.
- display 280 may be positioned below the center frame support and above the center of the wearer's eye, as shown in FIG. 2E .
- display 280 may be offset from the center of the wearer's eye (e.g., so that the center of display 280 is positioned to the right and above of the center of the wearer's eye, from the wearer's perspective).
- display 280 may be located in the periphery of the field of view of the wearer 290 , when HMD 272 is worn.
- FIG. 2F when the wearer 290 looks forward, the wearer 290 may see the display 280 with their peripheral vision.
- display 280 may be outside the central portion of the wearer's field of view when their eye is facing forward, as it commonly is for many day-to-day activities. Such positioning can facilitate unobstructed eye-to-eye conversations with others, as well as generally providing unobstructed viewing and perception of the world within the central portion of the wearer's field of view.
- the wearer 290 may view the display 280 by, e.g., looking up with their eyes only (possibly without moving their head). This is illustrated as shown in FIG. 2G , where the wearer has moved their eyes to look up and align their line of sight with display 280 . A wearer might also use the display by tilting their head down and aligning their eye with the display 280 .
- FIG. 3 is a block diagram of HMD 272 , in accordance with an example embodiment.
- HMD 272 can include data storage 304 , where data storage 304 is configured to store at least instructions 310 .
- HMD 272 can be a computing device such as computing device 800 discussed below at least in the context of FIG. 8A
- instructions 310 can be stored in data storage 304 , such as data storage 804 discussed below at least in the context of FIG. 8A .
- Instructions 310 can be instructions such as computer-readable program instructions 806 discussed below at least in the context of FIG. 8A .
- instructions 310 can be utilized in other computing devices than HMDs, such as, but not limited to, a telephone/smart phone, a laptop, a desktop, a server, and/or another type of wearable computing device.
- Instructions 310 when executed by one or processors of HMD 272 can perform one or more functions.
- the one or more processors of HMD 272 can be processors 803 discussed below in the context of computing device 800 .
- the functions can include obtaining a 3D-model query, perhaps via a user interface of HMD 272 .
- the 3D-model query can include information related to an object.
- the object can be any object that may have a 3D model, even objects that may be primarily in less than three dimensions; e.g., a piece of paper, a human hair, an object not visible to the human eye.
- the information can include text, one or more still images, video information, audio information, binary information, other information, and combinations thereof.
- HMD 272 can be configured with input devices, such as text-input devices, image capture devices (e.g., one or more still and/or video cameras), audio capture devices (e.g., one or more microphones).
- HMD 272 can have a camera configured to capture images related to the object, such as images of part or all of the object and/or images otherwise related to the object, such as, but not limited to, images of Quick Response (QR) codes, bar codes, advertisements, location information, catalog information, ordering information, and information about an environment of the object.
- HMD 272 can provide the images, and perhaps other information such as text related to the object or images, in the 3D-model query for the object.
- HMD 272 can obtain the 3D-model query by generating the 3D-model query, formatting the 3D-model query, and/or otherwise gathering the 3D-model query.
- QR Quick Response
- HMD 272 can send the 3D-model query to a model server, such as 3D-model server 410 discussed below in the context of at least FIG. 4A .
- the model server can use the information in the 3D-model query to locate a 3D model of the object and provide the 3D model to HMD 272 .
- the model server can provide HMD 272 with a reference to the 3D model; in these embodiments, the model server can render the 3D model and perhaps update the 3D model rendering based on commands provided by HMD 272 related to the 3D model, such as the commands based on gestures discloses herein.
- HMD 272 can generate a display of a view of the 3D model and display the view of the 3D model.
- a wearer of the HMD 272 can make gestures and/or provide other inputs related to the model.
- the other inputs can include, but are not limited to, voice inputs, text inputs, other image-based inputs, and inputs provided by a user interface, such as a Graphical User Interface (GUI) of HMD 272 and/or other computing device(s).
- GUI Graphical User Interface
- Example gestures are discussed below in more detail at least in the context of FIGS. 5A , 5 B, and 6 .
- the wearer of the HMD 272 can make a gesture to rotate the 3D model.
- HMD 272 can capture one or more images of the wearer making the rotation gesture.
- the images of the gesture can be processed; e.g., by HMD 272 , model server 410 , and/or other computing devices, to determine a corresponding command to change a model view of the 3D model; in this example, to rotate the 3D model.
- HMD 272 can update the display of the 3D model in response; in this example, HMD 272 can rotate the 3D model, update the view of the 3D model to show the rotated 3D model, and display the updated view of the 3D model.
- One gesture can be a close gesture (or corresponding other input) intended to terminate the view of the 3D model.
- an end-display command to end the view of the 3D model can be provided to HMD 272 .
- HMD 272 can indicate that the view of the 3D model is terminated; e.g., using appropriate data, such as the done variable shown in FIG. 3 , and consequently terminate the view of the 3D model.
- FIG. 4A depicts scenario 400 of displaying a 3D model obtained in response to a 3D-model query, in accordance with an example embodiment.
- Scenario 400 begins with HMD 272 capturing image 412 of an object—in this scenario, the object is a house.
- HMD 272 generates 3D-model query 420 with image 412 to request delivery of a 3D model of the object depicted in image 412 .
- FIG. 4A shows that scenario 400 continues with HMD 272 sending 3D-model query 420 to 3D-model server 410 .
- 3D-model server 410 Upon reception of 3D-model query 420 , 3D-model server 410 analyzes image 412 in 3D-model query 420 to determine that the object shown in image 412 is a house. 3D-model server 410 can then query a database of 3D models, or an equivalent data structure, to locate a 3D model of a house.
- the database of 3D models can store a number of 3D models, include multiple 3D models of a same type of an object; e.g., multiple 3D models of a house.
- the database of 3D models in response to query from 3D-model server 410 , provides multiple models of a house to 3D-model server 410 .
- the database of 3D models can provide an indication of which provided 3D model or models are closest to the object based on the information provide in a 3D-model query; e.g., an indication of which 3D house model is closest to the house depicted in image 412 .
- 3D-model server 410 finds N models, N>1, in the database of 3D models, then 3D-model server 410 can provide M models, 1 ⁇ M ⁇ N, as part of a response to 3D-model query. If M>1, then 3D-model server 410 may provide, in a query response, an indication of which model(s) are closest to the object. In some scenarios, search query 420 can specify a maximum value for M.
- 3D-model server can determine a 3D-model that is closest to the object as identified in information provided in 3D-model query 420 , and provide a single closest 3D model 430 as part of query response 422 .
- HMD 272 can obtain and display 3D model 430 .
- Scenario 400 can end when HMD 272 terminates display of 3D model 430 .
- FIG. 4B depicts a variation of scenario 400 with 3D model 430 displayed in an environment shown as fields of view (FOVs) 440 a and 440 b , in accordance with an example embodiment.
- 3D model 430 is initially displayed within display 442 generated by HMD 272 while a wearer of HMD 272 observes an environment depicted by FOV 440 a .
- a left edge of display 442 is shown having a horizontal distance D1 from a left edge of FOV 440 a and a top edge of display 442 is shown having a vertical distance D2 from a top edge of FOV 440 a .
- display 442 of 3D model 430 is shown in FIG. 4B roughly centered above pond 452 and somewhat to the right of tree 450 .
- FIG. 4B shows that 3D model 430 is shown in display 442 within FOV 440 b .
- FOV 440 a of FIG. 4B the left edge of display 442 has horizontal distance D1 from a left edge of FOV 440 b and the top edge of display 442 has vertical distance D2 from a top edge of FOV 440 b . That is, FIG.
- FIG. 4B shows that 3D model 430 and display 442 remain in a constant position relative to the FOV after the FOV changes from FOV 440 a to FOV 440 b .
- FIG. 4B also shows that 3D model 430 and display 442 change position relative to the environment after the FOV changes; e.g., display 442 is no longer centered over pond 450 and has moved close to a top of tree 450 .
- FIG. 4C depicts another variation of scenario 400 with 3D model 430 displayed in an environment shown as FOVs 440 a and 440 b , in accordance with an example embodiment.
- 3D model 430 is initially displayed by HMD 272 while a wearer of HMD 272 observes an environment depicted by FOV 440 a .
- a left edge of 3D model 430 is shown having a horizontal distance D3 from tree 450 and a top edge of 3D model 430 is shown having vertical distance D4 from a rightmost edge of pond 452 .
- 3D model 430 is shown in FIG. 4C roughly centered and near the bottom of the field of view.
- scenario 400 depicted in FIG. 4C continues with a wearer of HMD 272 moving their focus leftward; e.g., turning the head and/or eyes of the wearer leftward to look upon the environment as depicted in FIG. 4C as FOV 440 b .
- FOV 440 a of FIG. 4C the left edge of 3D model 430 is shown having a horizontal distance D3 from tree 450 and a top edge of 3D model 430 is shown having vertical distance D4 from a rightmost edge of pond 452 . That is, FIG. 4C shows that 3D model 430 remains in a constant position relative to the environment after the FOV changes.
- FIG. 4C shows that 3D model 430 remains in a constant position relative to the environment after the FOV changes.
- 3D model 430 changes position relative to the FOV after the FOV changes; e.g., 3D model 430 is no longer roughly centered within the FOV, but has moved to near to a right edge of the FOV depicted as FOV 440 b.
- FIGS. 5A and 5B are charts of images captured over time with corresponding motions and gestures, in accordance with an example embodiment. Near the top of FIG. 5A , an example set of three-dimensional axes is shown that indicates positive and negative directions in each axis.
- FIG. 5A shows, arranged in columns, seven example sets of input images, corresponding motions, and related gestures.
- FIG. 5B shows, arranged in columns similar to FIG. 5A , seven additional example sets of input images, corresponding motions, and related gestures.
- the images shown in FIGS. 5A and 5B can be captured by HMD 272 , or some other device.
- the input images can be single images captured in succession during a video stream; e.g., using HMD 272 equipped with a video camera.
- each image can be a still image captured successively over time; e.g., using HMD 272 equipped with a still camera.
- Other techniques and/or devices for capturing sequences of images over time, such as the images shown in FIGS. 5A and 5B are possible as well.
- FIGS. 5A and 5B depict movements of hands of a person related to HMD 272 , such as the wearer of HMD 272 .
- the leftmost column of FIG. 5A shows example image 510 a capturing a hand of a wearer to the left and roughly vertically centered within the image.
- Image 510 a is followed in time by images 510 b and 510 c , as indicated by the Time arrow on the left side of FIG. 5A .
- Image 510 b shows the hand shown in image 510 a after the hand moves to the right
- image 510 c shows the hand after moving to the right of the hand position shown in image 510 b.
- One or more hand positions can be determined for each image of a hand; e.g., at the center of the hand, at a position of one or more fingers of the hand, at a corner of a bounding box around the hand.
- the hand positions of hands in images 510 a , 510 b , and 510 c can be placed on a grid, such as a 5 ⁇ 5 grid shown in FIG. 5A overlaying images 510 a , 510 b , and 510 c , another sized grid, or as pixel positions in a grid of pixels that make up an image.
- the hand positions are at a grid cell of row 3, column 2 or (3, 2) for image 510 a , at a grid cell of (3, 3) for image 510 b , and at a grid cell of (3, 4) for image 510 b .
- the net movement of the hand then is to move two grid cells to the right, which can be shown as motion 512 in the +X direction.
- a hand movement in the +X direction can be classified as a gesture to the right; e.g., right gesture 514 .
- an overlaid grid can be used to determine general movements of hands for gestures. By translating the motions of hands to integer-valued grid cell coordinates, and then determining motions based on coordinates, slight variations in movement (e.g., a slight downward or upward movement for right gesture 514 ) can be filtered out.
- the size of the grid can be adapted as needed based on an amount of filtering of gestures.
- the 5 ⁇ 5 grid shown in FIG. 5A filters hand positions into 25 grid cells, which is more filtering than may be provided by a finer grid, such as a 10 ⁇ 10 grid or a grid indicated by a two-dimensional array of pixels that forms an image.
- the additional grid cells of a finer grid can be used in circumstances where a more accurate determination of hand position is indicated; e.g., using a 10 ⁇ 10 grid locates hands with twice the accuracy in both horizontal and vertical dimensions than a 5 ⁇ 5 grid.
- the sizes of motions can be determined relative to grid cells as well; e.g., a motion of two net grid cells can be considered to be smaller than a movement of three net grid cells and larger than a movement of one net grid cell. Then, the size of a motion can indicate an amount for a command based on the gesture. For example, as discussed above, right motion 514 involves a net movement of two grid cells. Then, a command based on right gesture 514 ; e.g., move a 3D model right, can indicate an amount of movement based on the net movement of grid cells; e.g., an amount of 2 in this example, or a relative movement of grid cells; e.g., an amount of 40% of the grid in this example.
- a hand position can be considered to in a stationary hand position.
- Stationary hand positions can mark beginnings and ends of gestures. For example, if a hand starts with hand position (x1, y1), stays in at hand position (x1, y1) for two or more images, moves to hand position (x2, y2), and stays in hand position (x2, y2) for two or more images, where (x1, y1) and (x2, y2) each indicate a hand location as a grid cell, pixel, or by some other indicator. Then, the gesture can be determined based on a motion from hand position (x1, y1) to hand position (x2, y2).
- Another technique to determine a beginning or end of a gesture is to determine a first or last image related to the gesture that includes depiction of a hand involved in making the gesture. For example, suppose five images, I1, I2, I3, I4, and I5, are captured over time with I1 captured first, I2 captured second, and so on until I5 is captured.
- Image I1 does not depict a hand
- I2 includes a hand at hand position (x3, y3)
- I3 shows the hand at hand position (x4, y4)
- I4 and I5 each show the hand at hand position (x5, y5).
- the gesture can be determined based on a motion from hand position (x3, y3) to hand position (x5, y5), as the hand originally appeared in image I2 at (x3, y3) and stayed at (x5, y5) for at least two images (I4 and I5).
- Image I1 shows a hand for the first time at hand position (x6, y6)
- I2 shows a hand at hand position (x7, y7)
- I3 shows the hand at hand position (x8, y8)
- I4 and I5 each do not depict a hand.
- the gesture can be determined based on a motion from hand position (x6, y6) to hand position (x8, y8), as the hand originally appeared in I1 at (x6, y7) and was last captured in I3 at (x8, y8).
- Other grids, hand position indications, movement sizes, and techniques for determining gestures from input images are possible as well.
- the second-leftmost column of FIG. 5A shows images 516 a , 516 b , and 516 c related to motion 518 in the ⁇ X dimension and to left gesture 520 .
- This relationship can be determined using the above-mentioned techniques regarding images 510 a - 510 c , motion 512 , and right gesture 514 ; e.g., a grid can be overlaid on images 516 a - 516 c , a net motion can be determined with respect to grid cells that corresponds to motion 518 , and motion 518 in the ⁇ X dimension can be related to left gesture 520 .
- FIGS. 5A show respective images, gestures, and motions for up gesture 526 in the +Y dimension shown by motion 524 , down gesture 532 in the ⁇ Y dimension shown by motion 530 , out gesture 538 in the +Z dimension (away from a camera or other image capture device) indicated by motion 536 , and in gesture 544 in the ⁇ Z dimension (toward the camera) indicated by motion 542 .
- the rightmost column of FIG. 5A shows a hand position at roughly a 9 o'clock position relative to a clock in image 546 a , at roughly 12 o'clock in image 546 b , at roughly 3 o'clock in image 546 c , and at roughly 6 o'clock in image 546 d .
- the hand positions in images 546 a - 546 d trace a clockwise (CW) motion, shown as clockwise motion 548 .
- Clockwise motion 548 can then be related to clockwise gesture 550 .
- the leftmost column of FIG. 5B shows a hand position at roughly a 9 o'clock position relative to a clock in image 552 a , at roughly 6 o'clock in image 552 b , at roughly 3 o'clock in image 552 c , and at roughly 12 o'clock in image 552 d .
- the hand positions in images 552 a - 552 d trace a clockwise (CCW) motion, shown as counterclockwise motion 554 .
- Counterclockwise motion 554 can then be related to clockwise gesture 556 .
- the second and third leftmost columns of FIG. 5B show example gestures related to multiple hand positions per hand.
- image 558 a an index finger and a thumb of the imaged hand are shown relatively far apart.
- image 558 b the index finger and thumb are closer together than shown in image 558 a
- image 558 c the index finger and thumb are shown in contact. If the positions of the thumb and the index finger are considered two hand positions, then motions of the thumb and index finger as indicated in images 558 a , 558 b , 558 c are coming together as shown as thumb/finger motion 560 .
- Thumb/finger motion 560 can be classified as pinch gesture 562 , as the index finger and thumb motion indicates a pinching movement.
- the reverse of the pinch gesture 562 e.g., from index finger and thumb initially being in contact to index finger and thumb being relatively far apart, is shown in images 564 a , 564 b , and 564 c . If the positions of the thumb and the index finger are considered two hand positions, then motions of the thumb and index finger as indicated in images 564 a , 564 b , and 564 are moving apart as shown as thumb/finger motion 566 . Thumb/finger motion 566 can be classified as spread gesture 568 , as the index finger and thumb motion are spreading away from each other.
- the fourth through seventh-leftmost columns of FIG. 5B show gestures made using two hands.
- Images 570 a , 570 b , and 570 c show two hands coming together as indicated in left hand/right hand motion 572 , in a fashion similar to the motions made by the index finger and thumb captured in images 558 a - 558 c .
- left hand/right hand motion 572 can be determined to be pinch gesture 562 .
- Images 576 a , 576 b , and 576 c show two hands moving apart as indicated in left hand/right hand motion 578 , in a fashion similar to the motions made by the index finger and thumb captured in images 564 a - 564 c .
- left hand/right hand motion 578 can be determined to be spread gesture 578 .
- Images 582 a , 582 b , and 582 c show two hands moving along a horizontal line and eventually touching.
- This motion shown in FIG. 5B as left hand/right hand motion 584 , can indicate closing; e.g., like curtains at a theater being closed at the end of a show or an act.
- left hand/right hand motion 584 can be interpreted as close gesture 586 .
- images 588 a , 582 b , and 582 c show two hands starting in contact and moving apart along a horizontal line.
- This motion, shown in FIG. 5B as left hand/right hand motion 590 can indicate opening; e.g., like curtains at a theater being opened at the beginning of a show or act.
- left hand/right hand motion 590 can be interpreted as open gesture 592 .
- FIG. 6 shows input gestures and corresponding changes to a 3D model, in accordance with an example embodiment.
- 3D-model display 610 of HMD 272 is initially displaying a view of 3D model 430 .
- the portion of 3D model 430 visible in 3D-model display 610 is shown in FIG. 6 using a rectangle.
- 3D-model display 610 is shown generally centered over 3D model 430 .
- FIG. 6 shows three pairs of columns below the Initial Conditions portion. Each pair of columns includes a “Gesture” column indicating a gesture input to HMD 272 and a “Resulting Display” column showing an effect of the gesture input on 3D-model display 630 .
- right gesture 514 is indicated as input to HMD 272 , and in response, resulting display 610 a shows that the view of 3D model 430 has been shifted left which corresponds to a rightward movement of 3D model 430 as commanded by right gesture 514 . That is, right gesture 514 can correspond to; i.e., lead to generation of, a command to HMD 272 to shift the view shown in 3D-model display 610 left and/or move 3D model 430 right.
- the other input gestures discussed at least in the context of FIG. 6 when input to HMD 272 can correspond to one or more commands to HMD 272 to change the view shown in 3D-model display 610 and/or 3D model 430 as indicated as discussed in the context of each respective input gesture.
- the second-from-top row in the leftmost pair of columns shows left gesture 520 indicated as input to HMD 272 , and in response, resulting display 610 b shows that the view of 3D model 430 has been shifted right which corresponds to a leftward movement of 3D model 430 as commanded by left gesture 514 .
- the third-from-top row in the leftmost pair of columns shows up gesture 526 indicated as input to HMD 272 .
- HMD 272 generates resulting display 610 c showing that the view of 3D model 430 has been moved down corresponding to commanded upward movement of the 3D model.
- the fourth-from-top rows in the leftmost pair of columns shows down gesture 532 indicated as input to HMD 272 .
- HMD 272 generates resulting display 610 d with the view of 3D model 430 having been moved upward, corresponding to a commanded downward movement of the 3D model.
- HMD 272 can interpret that pair of gestures as one left-then-right gesture 620 .
- HMD 272 can interpret left-then-right gesture 620 as an edge-on view of a gesture representing a circle rotating about the Y axis in a clockwise direction; in response, HMD 272 can generate resulting display 610 e showing a view of 3D model 430 rotated clockwise about its Y axis.
- HMD 272 can interpret a right gesture (immediately) followed by a left gesture as right-then-left gesture 622 ; in response, HMD 272 can generate resulting display 610 f showing a view of 3D model 430 rotated counterclockwise about its Y axis.
- HMD 272 can interpret that pair of gestures as one up-then-down gesture 630 .
- HMD 272 can interpret up-then-down gesture 630 as an edge-on view of a gesture representing a circle rotating about the X axis in a counterclockwise direction; in response, HMD 272 can generate resulting display 610 g showing 3D model 430 rotated clockwise about its X axis.
- HMD 272 can interpret a down gesture (immediately) followed by an up gesture as one down-then-up gesture 632 and can generate resulting display 610 h showing a view of 3D model 430 rotated counterclockwise about its X axis.
- HMD 272 can interpret clockwise gesture 550 as a command to rotate 3D model 430 clockwise about its Z axis and generate resulting display 610 i showing a view of 3D model 430 rotated in a clockwise direction about its Z axis. Additionally, HMD 272 can interpret counterclockwise gesture 550 as a command to rotate 3D model 430 counterclockwise about its Z axis and generate resulting display 610 j showing 3D model 430 rotated in a counterclockwise direction about its Z axis.
- two or more different gestures can have identical effects on a resulting display.
- performing either in gesture 544 or spread gesture 568 can be interpreted as a command for zooming in, enlarging, moving 3D model 430 closer to the display, and/or moving 3D model 430 in the ⁇ Z direction.
- HMD 272 can generate an enlarged view of 3D model 430 as indicated using resulting display 610 k , shown in the top row of the rightmost pair of columns in FIG. 6 .
- performing either out gesture 538 or pinch gesture 562 can be interpreted as a command for zooming out, shrinking, moving 3D model 430 away from the display, and/or moving 3D model 430 in the +Z direction.
- HMD 272 can generate a smaller view of 3D model 430 as indicated using resulting display 610 m , shown in the second-from-top row of the rightmost pair of columns in FIG. 6 .
- the middle row of the rightmost pair of columns in FIG. 6 shows that close gesture 586 can correspond to commanding HMD 272 to terminate display a view of 3D model 430 .
- the second-from-bottom (fourth-from-top) middle row of the rightmost pair of columns in FIG. 6 shows that open gesture 586 can correspond to commanding HMD 272 to launch a query for a new model using query display 640 .
- one or more gestures, such as close gesture 586 received as input while HMD 272 is displaying query display 640 can be interpreted as closing query display 640 and perhaps reverting to the previous display; e.g., display 610 shown as part of the initial conditions.
- a first gesture such as an up-then-down gesture or a down-then-up gesture
- a second gesture such as a left-then-right gesture or right-then-left gesture
- other gestures 650 can be input to HMD 272 .
- a diagonal line gesture can be input to HMD 272 corresponding to a combination of either a left or a right gesture and either an up or a down gesture.
- a gesture with an initial hand position at or near an upper-left-hand corner of an image and ending with a hand position at or near a lower-right-hand corner of a subsequent image can be interpreted as a combination of a right gesture and a down gesture, and so translate a view of 3D model 430 rightwards and downwards in 3D model display 610 .
- Other gestures 650 can include gestures that interact with the object modeled by 3D model 430 .
- a gesture to touch the object modeled by 3D model 430 can lead to HMD 272 providing additional information about part of or the entire object, such as name, model number, identification number, size, weight, material(s) used to make the object, component information, cost/price/delivery information, other information, and combinations thereof.
- other gestures 650 can include gestures to touch virtual buttons or other control elements that control 3D-model display 630 .
- a sequence of gestures to touch a virtual “color” button, to touch a blue color on a color-palette control, and to touch 3D model 430 can change the color of 3D model 430 to a blue color.
- a later sequence of gestures to touch a beige color on the color-palette control and to touch 3D model 430 can change the color of 3D model 430 from blue to beige.
- gestures shown in FIG. 6 can be interpreted as a gesture to split the object into components.
- close gesture 586 begun with each hand on a separate component of the object (or two separate objects) can be interpreted as a gesture to combine the components of the object (or the two separate objects) into one object.
- Other gestures 650 can include gestures to perform other operations on 3D model 430 than already discussed in the context of FIGS. 5A , 5 B, and 6 . These other operations can include, but are not limited to, skew operations, move within or outside of 3D model 430 , store 3D model 430 , undo or redo a change to 3D model 430 , add an additional model to display 610 (e.g., add landscaping-related model(s) to the house modeled as 3D model 430 ), remove model(s) from display 610 , operations that involve sub-operations discussed in the context of previously-discussed gestures, and combinations thereof.
- gestures not identified for object/3D model interaction can be ignored or otherwise processed. For example, a gesture to touch or point at something other than the object/3D model can be interpreted as a gesture unrelated to the object. Gestures determined to be unrelated to the object/3D model can be ignored, prompted (e.g., “Did you mean to interact with the 3D model? If so, please repeat your gesture.”), or otherwise processed.
- HMD 272 can access a define/redefine gesture function to create and/or change mappings between gestures and operations.
- HMD 272 can provide a prompt to the wearer to inform the wearer about the new gesture being an unrecognized gesture.
- HMD 272 can request the wearer then either indicate which operation(s) be performed in response to the unrecognized gesture or if the unrecognized gesture should be ignored.
- HMD 272 can use the define/redefine gesture function to associate two-handed leftward movements with two (or more) left model view changes.
- the define/redefine gesture function can be used to redefine operations that occur in response to gestures shown in FIG. 6 ; e.g., a wearer that would like to reverse the left and right gestures can access the define/redefine gesture function. Then, HMD 272 can prompt the wearer to make a gesture, such as the left gesture, perhaps identify the gesture, and then prompt the wearer to select one or more operations to perform in response to the gestures; e.g., a rightward model movement. After receiving the inputs about the left gesture to be redefined and the operation(s) to be performed for the redefined left gesture, HMD 272 can use the define/redefine gesture function to change the mapping of the left gesture so to make a rightward model movement. A similar procedure can then be used by the wearer to change the mapping of the right gesture to a leftward model movement. Many other examples of input gestures and corresponding commands to HMD 272 are possible as well.
- FIG. 7 shows server devices 708 , 710 configured to communicate, via network 706 , with programmable devices 704 a , 704 b , and 704 c .
- Network 706 may correspond to a LAN, a wide area network (WAN), a corporate intranet, the public Internet, or any other type of network configured to provide a communications path between networked computing devices.
- the network 706 may also correspond to a combination of one or more LANs, WANs, corporate intranets, and/or the public Internet.
- One or more of server devices 708 , 710 , network 706 , and programmable devices 704 a , 704 b , and 704 c can be configured to perform part or all of method 100 and/or some or all of the herein-described functionality of HMD 272 and/or 3D-model server 410 .
- FIG. 7 only shows three programmable devices, distributed application architectures may serve tens, hundreds, or thousands of programmable devices.
- programmable devices 704 a , 704 b , and 704 c may be any sort of computing device, such as an ordinary laptop computer, desktop computer, network terminal, wireless communication device (e.g., a cell phone or smart phone), and so on.
- programmable devices 704 a , 704 b , and 704 c may be dedicated to the design and use of software applications.
- programmable devices 704 a , 704 b , and 704 c may be general purpose computers that are configured to perform a number of tasks and need not be dedicated to software development tools. In still other embodiments, programmable devices 704 a , 704 b , and/or 704 c can be configured to perform some or all of the herein-described functionality of a computing device.
- Server devices 708 , 710 can be configured to perform one or more services, as requested by programmable devices 704 a , 704 b , and/or 704 c .
- server device 708 and/or 710 can provide content to programmable devices 704 a - 704 c .
- the content can include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio, and/or video.
- the content can include compressed and/or uncompressed content.
- the content can be encrypted and/or unencrypted. Other types of content are possible as well.
- server device 708 and/or 710 can provide programmable devices 704 a - 704 c with access to software for database, search, computation, graphical, audio, video, World Wide Web/Internet utilization, and/or other functions.
- server devices are possible as well.
- FIG. 8A is a block diagram of a computing device (e.g., system) in accordance with an example embodiment.
- computing device 800 shown in FIG. 8A can be configured to perform part or all of method 100 and/or some or all of the herein-described functionality of HMD 272 , 3D-model server 410 , one or more functions of server devices 708 , 710 , network 706 , and/or one or more of programmable devices 704 a , 704 b , and 704 c .
- Computing device 800 may include a user interface module 801 , a network-communication interface module 802 , one or more processors 803 , and data storage 804 , all of which may be linked together via a system bus, network, or other connection mechanism 805 .
- User interface module 801 can be operable to send data to and/or receive data from external user input/output devices.
- user interface module 801 can be configured to send and/or receive data to and/or from user input devices such as a keyboard, a keypad, a touch screen, a computer mouse, a track ball, a joystick, a camera, a voice recognition module, and/or other similar devices.
- User interface module 801 can also be configured to provide output to user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays (LCDs), light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, either now known or later developed.
- User interface module 801 can also be configured to generate audible output(s), such as a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices.
- Network-communications interface module 802 can include one or more wireless interfaces 807 and/or one or more wireline interfaces 808 that are configurable to communicate via a network, such as network 706 shown in FIG. 7 .
- Wireless interfaces 807 can include one or more wireless transmitters, receivers, and/or transceivers, such as a Bluetooth transceiver, a Zigbee transceiver, a Wi-Fi transceiver, a WiMAX transceiver, and/or other similar type of wireless transceiver configurable to communicate via a wireless network.
- Wireline interfaces 808 can include one or more wireline transmitters, receivers, and/or transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network.
- wireline transmitters such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network.
- USB Universal Serial Bus
- network communications interface module 802 can be configured to provide reliable, secured, and/or authenticated communications. For each communication described herein, information for ensuring reliable communications (i.e., guaranteed message delivery) can be provided, perhaps as part of a message header and/or footer (e.g., packet/message sequencing information, encapsulation header(s) and/or footer(s), size/time information, and transmission verification information such as CRC and/or parity check values). Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, DES, AES, RSA, Diffie-Hellman, and DSA. Other cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications.
- cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications.
- Processors 803 can include one or more general purpose processors and/or one or more special purpose processors (e.g., digital signal processors, application specific integrated circuits, etc.). Processors 803 can be configured to execute computer-readable program instructions 806 that are contained in the data storage 804 and/or other instructions as described herein. In some embodiments, data storage 804 can additionally include storage required to perform at least part of the herein-described methods and techniques and/or at least part of the functionality of the herein-described devices and networks.
- Data storage 804 can include one or more computer-readable storage media that can be read and/or accessed by at least one of processors 803 .
- data storage 804 can provide memory for the herein-described instructions; e.g., instructions 310 , and/or lighting data storage; e.g., lighting data storage 320 .
- the one or more computer-readable storage media can include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with at least one of processors 803 .
- data storage 804 can be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other embodiments, data storage 804 can be implemented using two or more physical devices.
- FIG. 8B depicts a network 606 of computing clusters 809 a , 809 b , 809 c arranged as a cloud-based server system in accordance with an example embodiment.
- Server devices 608 and/or 610 can be cloud-based devices that store program logic and/or data of cloud-based applications and/or services.
- server devices 608 and/or 610 can be a single computing device residing in a single computing center.
- server device 608 and/or 610 can include multiple computing devices in a single computing center, or even multiple computing devices located in multiple computing centers located in diverse geographic locations.
- FIG. 6 depicts each of server devices 608 and 610 residing in different physical locations.
- data and services at server devices 608 and/or 610 can be encoded as computer readable information stored in non-transitory, tangible computer readable media (or computer readable storage media) and accessible by programmable devices 604 a , 604 b , and 604 c , and/or other computing devices.
- data at server device 608 and/or 610 can be stored on a single disk drive or other tangible storage media, or can be implemented on multiple disk drives or other tangible storage media located at one or more diverse geographic locations.
- FIG. 8B depicts a cloud-based server system in accordance with an example embodiment.
- the functions of server device 608 and/or 610 can be distributed among three computing clusters 809 a , 809 b , and 809 c .
- Computing cluster 809 a can include one or more computing devices 800 a , cluster storage arrays 810 a , and cluster routers 811 a connected by a local cluster network 812 a .
- computing cluster 809 b can include one or more computing devices 800 b , cluster storage arrays 810 b , and cluster routers 811 b connected by a local cluster network 812 b .
- computing cluster 809 c can include one or more computing devices 800 c , cluster storage arrays 810 c , and cluster routers 811 c connected by a local cluster network 812 c.
- each of the computing clusters 809 a , 809 b , and 809 c can have an equal number of computing devices, an equal number of cluster storage arrays, and an equal number of cluster routers. In other embodiments, however, each computing cluster can have different numbers of computing devices, different numbers of cluster storage arrays, and different numbers of cluster routers. The number of computing devices, cluster storage arrays, and cluster routers in each computing cluster can depend on the computing task or tasks assigned to each computing cluster.
- computing devices 800 a can be configured to perform various computing tasks of electronic communications server 812 .
- the various functionalities of electronic communications server 812 can be distributed among one or more of computing devices 800 a , 800 b , and 800 c .
- Computing devices 800 b and 800 c in computing clusters 809 b and 809 c can be configured similarly to computing devices 800 a in computing cluster 809 a .
- computing devices 800 a , 800 b , and 800 c can be configured to perform different functions.
- computing tasks and stored data associated with server devices 608 and/or 610 can be distributed across computing devices 800 a , 800 b , and 800 c based at least in part on the processing requirements of server devices 608 and/or 610 , the processing capabilities of computing devices 800 a , 800 b , and 800 c , the latency of the network links between the computing devices in each computing cluster and between the computing clusters themselves, and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency, and/or other design goals of the overall system architecture.
- the cluster storage arrays 810 a , 810 b , and 810 c of the computing clusters 809 a , 809 b , and 809 c can be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives.
- the disk array controllers alone or in conjunction with their respective computing devices, can also be configured to manage backup or redundant copies of the data stored in the cluster storage arrays to protect against disk drive or other cluster storage array failures and/or network failures that prevent one or more computing devices from accessing one or more cluster storage arrays.
- the cluster routers 811 a , 811 b , and 811 c in computing clusters 809 a , 809 b , and 809 c can include networking equipment configured to provide internal and external communications for the computing clusters.
- the cluster routers 811 a in computing cluster 809 a can include one or more internet switching and routing devices configured to provide (i) local area network communications between the computing devices 800 a and the cluster storage arrays 801 a via the local cluster network 812 a , and (ii) wide area network communications between the computing cluster 809 a and the computing clusters 809 b and 809 c via the wide area network connection 813 a to network 606 .
- Cluster routers 811 b and 811 c can include network equipment similar to the cluster routers 811 a , and cluster routers 811 b and 811 c can perform similar networking functions for computing clusters 809 b and 809 b that cluster routers 811 a perform for computing cluster 809 a.
- Example methods and systems are described herein. It should be understood that the words “example” and “exemplary” are used herein to mean “serving as an example, instance, or illustration.” Any embodiment or feature described herein as being an “example” or “exemplary” is not necessarily to be construed as preferred or advantageous over other embodiments or features.
- each block and/or communication may represent a processing of information and/or a transmission of information in accordance with example embodiments.
- Alternative embodiments are included within the scope of these example embodiments.
- functions described as blocks, transmissions, communications, requests, responses, and/or messages may be executed out of order from that shown or discussed, including substantially concurrent or in reverse order, depending on the functionality involved.
- more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow charts discussed herein, and these ladder diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.
- a block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
- a block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique.
- a block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data).
- the program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique.
- the program code and/or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage medium.
- the computer readable medium may also include non-transitory computer readable media such as computer-readable media that stores data for short periods of time like register memory, processor cache, and random access memory (RAM).
- the computer readable media may also include non-transitory computer readable media that stores program code and/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- a computer readable medium may be considered a computer readable storage medium, for example, and/or a tangible storage device.
Abstract
Methods and systems are provided for controlling a three-dimensional (3D) model for a head-mountable display (HMD). The HMD can receive a 3D model for the object, where the 3D model includes three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, where each of the first axis, the second axis, and the third axis differs. The HMD can display a view of the 3D model. The HMD can receive an input gesture. The HMD can determine whether the input gesture includes a 3D model gesture. After determining that the input gesture does includes a 3D model gesture, the HMD can update the view of the 3D model based on the input gesture and can display the updated view of the 3D model.
Description
- In computer graphics, three-dimensional (3D) modeling involves generation of a representation of a 3D surface of an object. The representation may be referred to as a 3D object data model, or simply a 3D model, and can be rendered or displayed as a two-dimensional image via 3D rendering or displayed as a three-dimensional image. A 3D object data model may represent a 3D object using a collection of points in 3D space, connected by various geometric entities such as triangles, lines, curved surfaces, etc. Various techniques exist for generating 3D object data models utilizing point clouds and geometric shapes.
- In one aspect, a method is provided. The method includes sending a 3D-model query from a head-mountable display (HMD). The 3D-model query includes information about an object. The HMD receives a 3D model for the object. The 3D model includes three-dimensional shape and texture information about the object. The HMD displays a view of the 3D model. The HMD receives an input gesture. It is determined whether the input gesture is related to the 3D model. After determining that the input gesture is related to the 3D model, the HMD updates the view of the 3D model based on the input gesture and displays the updated view of the 3D model.
- In another aspect, an HMD is provided. The HMD includes a display, a processor, and a non-transitory computer readable medium. The non-transitory computer readable medium is configured to store at least instructions. The instructions are executable by the processor to cause the HMD to perform functions including: receiving a 3D model for an object, where the 3D model includes three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, where each of the first axis, the second axis, and the third axis differs; displaying a view of the 3D model using the display; receiving an input gesture; determining whether the input gesture includes a 3D-model gesture of a plurality of 3D-model gestures, where the plurality of 3D-model gestures include a first 3D-model gesture to modify display of the 3D model with respect to the first axis, a second 3D-model gesture to modify display of the 3D model with respect to the second axis, and a third 3D-model gesture to modify display of the 3D model with respect to the third axis; and after determining that the input gesture is related to the 3D model: updating the view of the 3D model based on the input gesture, and displaying the updated view of the 3D model using the display.
- In yet another aspect, a non-transitory computer readable medium is provided. The non-transitory computer readable medium has stored therein instructions executable by a processor of a head-mountable display (HMD) to cause the HMD to perform functions. The functions include: receiving a 3D model for an object, where the 3D model includes three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, where each of the first axis, the second axis, and the third axis differs; displaying a view of the 3D model; receiving an input gesture; determining whether the input gesture includes a 3D-model gesture of a plurality of 3D-model gestures, where the plurality of 3D-model gestures include a first 3D-model gesture to modify display of the 3D model with respect to the first axis, a second 3D-model gesture to modify display of the 3D model with respect to the second axis, and a third 3D-model gesture to modify display of the 3D model with respect to the third axis; and after determining that the input gesture is related to the 3D model: updating the view of the 3D model based on the input gesture, and displaying the updated view of the 3D model.
- In even another aspect, a HMD is provided. The HMD includes: means for receiving a 3D model for an object, where the 3D model includes three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, where each of the first axis, the second axis, and the third axis differs; means for displaying a view of the 3D model; means for receiving an input gesture; means for determining whether the input gesture includes a 3D-model gesture of a plurality of 3D-model gestures, where the plurality of 3D-model gestures include a first 3D-model gesture to modify display of the 3D model with respect to the first axis, a second 3D-model gesture to modify display of the 3D model with respect to the second axis, and a third 3D-model gesture to modify display of the 3D model with respect to the third axis; and means for, after determining that the input gesture is related to the 3D model: updating the view of the 3D model based on the input gesture, and displaying the updated view of the 3D model.
-
FIG. 1 is a flowchart of a method, in accordance with an example embodiment. -
FIG. 2A illustrates a HMD according to an example embodiment. -
FIG. 2B illustrates an alternate view of the HMD illustrated inFIG. 2A . -
FIG. 2C illustrates another HMD according to an example embodiment. -
FIG. 2D illustrates another HMD according to an example embodiment. -
FIGS. 2E to 2G are simplified illustrations of the HMD shown inFIG. 2D , being worn by a wearer. -
FIG. 3 is a block diagram of an HMD, in accordance with an example embodiment. -
FIG. 4A depicts a scenario of displaying a 3D model obtained in response to a 3D-model query, in accordance with an example embodiment. -
FIG. 4B depicts a variation of the scenario ofFIG. 4A for displaying a 3D model in an environment, in accordance with an example embodiment. -
FIG. 4C depicts another variation of the scenario ofFIG. 4A for displaying a 3D model in an environment, in accordance with an example embodiment. -
FIGS. 5A and 5B are charts of images captured over time with corresponding motions and gestures, in accordance with an example embodiment. -
FIG. 6 shows input gestures and corresponding changes to a 3D model, in accordance with an example embodiment. -
FIG. 7 depicts a distributed computing architecture, in accordance with an example embodiment. -
FIG. 8A is a block diagram of a computing device, in accordance with an example embodiment. -
FIG. 8B depicts a cloud-based server system, in accordance with an example embodiment. - Overview
- Example embodiments disclosed herein relate to requesting delivery of a 3D model using a query that includes image information and to controlling display of a 3D model using gesture inputs. Some devices, such as some implementations of HMDs, have limited input mechanisms. For example, some HMDs may not include a keyboard or other touch-based interface. Then, textual input can be difficult using these HMDs. Therefore, other inputs, such as images, video, and/or spoken inputs can be used to generate search queries, such as queries for 3D models. For example, a wearer of an HMD can capture one or more image(s) and/or video of an object. Then, the HMD can formulate a query for a 3D model including the image(s) and/or video and provide the query to a 3D-model server. In response to the query, the 3D-model server can use the image(s) and/or video to identify the object, locate one or more 3D models of the object, and deliver some or all of the 3D models to the HMD.
- Once the HMD receives the 3D model(s), the HMD can generate a display of the 3D model(s). To interact with the 3D model, the HMD can capture images of the wearer's hands (or other objects) making gestures, perhaps using the same camera used to capture image(s) of the object. The images can be processed to determine hand motions and corresponding gestures. The gestures can be interpreted by the HMD as commands for interacting with the 3D model(s). As such, the wearer can interact with the 3D model(s) by making gestures. For example, a wearer can move their hand in a vertical line downward to command the HMD to move the 3D model downward. As another example, the wearer can move their hand in a clockwise circular motion to command the HMD to rotate the 3D model clockwise.
- The herein-disclosed techniques can use a combination of cloud-based model servers and HMDs to request, render, and control 3D models using images as inputs to query for 3D models and to control rendering of the 3D models using hand gestures. By taking advantage of these rich interaction modalities, it is possible to avoid interactions through bandwidth limited input mechanisms. For example, using an HMD to find a 3D model of an object based on an input image can be accomplished much easier and faster than trying to input the appropriate text using the HMD. Additionally, manipulating the rendered model with hand movements should be more natural and intuitive than using other input devices due to an implied physical presence of the virtual object. Thus, the herein-described techniques for obtaining, displaying, and interacting with information about objects, such as 3D models of the objects, can speed and simply wearer interactions with an HMD.
- Example Operations
-
FIG. 1 is a flowchart ofmethod 100, in accordance with an example embodiment.Method 100 can be carried out by a HMD, such asHMD 272 discussed below. In some embodiments,method 100 can be carried out by a computing device acting as an HMD, such ascomputing device 800 discussed below. -
Method 100 can begin atblock 110, where a HMD can receive a 3D model for an object. The 3D model can include three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, where each of the first axis, the second axis, and the third axis differs, such as discussed below in the context of at leastFIG. 4A . In some embodiments, the information about the object can include an image related to the object. In particular embodiments, the image related to the object can be at least one image selected from the group consisting of an image that depicts at least part of the object, an image of a Quick Response (QR) code related to the object, and an image of a bar code related to the object. - At
block 120, the HMD can display a view of the 3D model, such as discussed below in the context of at leastFIG. 4A . - At
block 130, the HMD can receive an input gesture, such as discussed below in the context of at leastFIGS. 4B , 4C, 5A, 5B, and 6. In some embodiments, the input gesture can include a gesture to rotate the 3D model. In particular embodiments, the gesture to rotate the 3D model can include a first gesture to rotate the 3D model on a first axis and a second gesture to rotate the 3D model on a second axis, and wherein the first axis differs from the second axis. - At
block 140, it can be determined whether the input gesture includes a 3D-model gesture of a plurality of 3D-model gestures, where the plurality of 3D-model gestures includes a first 3D-model gesture to modify display of the 3D model with respect to the first axis, a second 3D-model gesture to modify display of the 3D model with respect to the second axis, and a third 3D-model gesture to modify display of the 3D model with respect to the third axis, such as discussed below in the context of at leastFIGS. 4B , 4C, 5A, 5B, and 6. In some embodiments, the HMD can determine whether the input gesture is related to the 3D model. In particular embodiments, the HMD can determine whether the input gesture is related to the 3D model without communicating with another computing device; i.e., the HMD can determine whether the input gesture is related to the 3D model as a standalone device. In other particular embodiments, the HMD can determine whether the input gesture is related to the 3D model via communication with another computing device; e.g., the HMD can provide data related to the input gesture, such as video or image data, to an gesture server or other computing device, and in response, the gesture server can inform the HMD whether the input gesture is related to the 3D model so that the HMD can determine whether the input gesture is related to the 3D model. - At
block 150, after determining that the input gesture is related to the 3D model, the HMD can update the view of the 3D model based on the input gesture and display the updated view of the 3D model. In some embodiments where the input gesture includes a gesture to rotate the 3D model, updating the view of the 3D model can include: rotating the 3D model as indicated by the gesture to rotate the 3D model and generating a display of the rotated 3D model. - In some embodiments,
method 100 can further include: receiving a second input gesture at the HMD, determining whether the second input gesture corresponds to a close gesture, and in response to determining that the second input gesture corresponds to the close gesture, terminating the view of the 3D model. - Example Head-Mountable Displays
- Systems and devices in which example embodiments may be implemented will now be described in greater detail. In general, an example system may be implemented in or may take the form of a wearable computer (also referred to as a wearable computing device). In an example embodiment, a wearable computer takes the form of or includes a head-mountable display (HMD).
- An example system may also be implemented in or take the form of other devices, such as a mobile phone, among other possibilities. Further, an example system may take the form of a non-transitory computer readable medium, which has program instructions stored thereon that are executable by at a processor to provide the functionality described herein. An example system may also take the form of a device such as a wearable computer or mobile phone, or a subsystem of such a device, which includes such a non-transitory computer readable medium having such program instructions stored thereon.
- An HMD may generally be any display device that is capable of being worn on the head and places a display in front of one or both eyes of the wearer. An HMD may take various forms such as a helmet or eyeglasses. As such, references to “eyeglasses” or a “glasses-style” HMD should be understood to refer to an HMD that has a glasses-like frame so that it can be worn on the head. Further, example embodiments may be implemented by or in association with an HMD with a single display or with two displays, which may be referred to as a “monocular” HMD or a “binocular” HMD, respectively.
-
FIG. 2A illustrates a wearable computing system according to an example embodiment. InFIG. 2A , the wearable computing system takes the form ofHMD 202. It should be understood, however, that example systems and devices may take the form of or be implemented within or in association with other types of devices, without departing from the scope of the invention. - As illustrated in
FIG. 2A , theHMD 202 includes frame elements including lens-frames center frame support 208,lens elements arms center frame support 208 and the extending side-arms HMD 202 to a user's face via a user's nose and ears, respectively. - Each of the
frame elements arms HMD 202. Other materials may be possible as well. - One or more of each of the
lens elements lens elements - The extending side-
arms frames HMD 202 to the user. The extending side-arms HMD 202 to the user by extending around a rear portion of the user's head. Additionally or alternatively, for example, theHMD 202 may connect to or be affixed within a head-mounted helmet structure. Other configurations for an HMD are also possible. - The
HMD 202 may also include an on-board computing system 218, animage capture device 220, asensor 222, and a finger-operable touch pad 224. The on-board computing system 218 is shown to be positioned on the extending side-arm 214 of theHMD 202; however, the on-board computing system 218 may be provided on other parts of theHMD 202 or may be positioned remote from the HMD 202 (e.g., the on-board computing system 218 could be wire- or wirelessly-connected to the HMD 202). The on-board computing system 218 may include a processor and memory, for example. The on-board computing system 218 may be configured to receive and analyze data from theimage capture device 220 and the finger-operable touch pad 224 (and possibly from other sensory devices, user interfaces, or both) and generate images for output by thelens elements - The
image capture device 220 may be, for example, a camera that is configured to capture still images and/or video. In the illustrated configuration,image capture device 220 is positioned on the extending side-arm 214 of theHMD 202; however, theimage capture device 220 may be provided on other parts of theHMD 202. Theimage capture device 220 may be configured to capture images at various resolutions or at different frame rates. Many image capture devices with a small form-factor, such as the cameras used in mobile phones or webcams, for example, may be incorporated into an example of theHMD 202. - Further, although
FIG. 2A illustrates oneimage capture device 220, more than one image capture device may be used, and each may be configured to capture the same view, or to capture different views. For example, theimage capture device 220 may be forward facing to capture at least a portion of the real-world view perceived by the user. This forward facing image captured by theimage capture device 220 may then be used to generate an augmented reality where computer generated images appear to interact with or overlay the real-world view perceived by the user. - The
sensor 222 is shown on the extending side-arm 216 of theHMD 202; however, thesensor 222 may be positioned on other parts of theHMD 202. For illustrative purposes, only onesensor 222 is shown. However, in an example embodiment, theHMD 202 may include multiple sensors. For example, anHMD 202 may includesensors 222 such as one or more gyroscopes, one or more accelerometers, one or more magnetometers, one or more light sensors, one or more infrared sensors, and/or one or more microphones. Other sensing devices may be included in addition or in the alternative to the sensors that are specifically identified herein. - The finger-
operable touch pad 224 is shown on the extending side-arm 214 of theHMD 202. However, the finger-operable touch pad 224 may be positioned on other parts of theHMD 202. Also, more than one finger-operable touch pad may be present on theHMD 202. The finger-operable touch pad 224 may be used by a user to input commands. The finger-operable touch pad 224 may sense at least one of a pressure, position and/or a movement of one or more fingers via capacitive sensing, resistance sensing, or a surface acoustic wave process, among other possibilities. The finger-operable touch pad 224 may be capable of sensing movement of one or more fingers simultaneously, in addition to sensing movement in a direction parallel or planar to the pad surface, in a direction normal to the pad surface, or both, and may also be capable of sensing a level of pressure applied to the touch pad surface. In some embodiments, the finger-operable touch pad 224 may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger-operable touch pad 224 may be formed to have a raised, indented, or roughened surface, so as to provide tactile feedback to a user when the user's finger reaches the edge, or other area, of the finger-operable touch pad 224. If more than one finger-operable touch pad is present, each finger-operable touch pad may be operated independently, and may provide a different function. - In a further aspect,
HMD 202 may be configured to receive user input in various ways, in addition or in the alternative to user input received via finger-operable touch pad 224. For example, on-board computing system 218 may implement a speech-to-text process and utilize a syntax that maps certain spoken commands to certain actions. In addition,HMD 202 may include one or more microphones via which a wearer's speech may be captured. Configured as such,HMD 202 may be operable to detect spoken commands and carry out various computing functions that correspond to the spoken commands. - As another example,
HMD 202 may interpret certain head-movements as user input. For example, whenHMD 202 is worn,HMD 202 may use one or more gyroscopes and/or one or more accelerometers to detect head movement. TheHMD 202 may then interpret certain head movements as being user input, such as nodding, or looking up, down, left, or right.HMD 202 could also pan or scroll through graphics in a display according to movement. Other types of actions may also be mapped to head movement. - As yet another example,
HMD 202 may interpret certain gestures (e.g., by a wearer's hand or hands) as user input such as describe herein. For example,HMD 202 may capture hand movements by analyzing image data fromimage capture device 220, and initiate actions that are defined as corresponding to certain hand movements. - As a further example,
HMD 202 may interpret eye movement as user input. In particular,HMD 202 may include one or more inward-facing image capture devices and/or one or more other inward-facing sensors (not shown) sense a user's eye movements and/or positioning. As such, certain eye movements may be mapped to certain actions. For example, certain actions may be defined as corresponding to movement of the eye in a certain direction, a blink, and/or a wink, among other possibilities. -
HMD 202 can include aspeaker 225 for generating audio output. In one example, the speaker could be in the form of a bone conduction speaker, also referred to as a bone conduction transducer (BCT).Speaker 225 may be, for example, a vibration transducer or an electroacoustic transducer that produces sound in response to an electrical audio signal input. The frame ofHMD 202 may be designed such that when a wearer wearsHMD 202,speaker 225 contacts the wearer. Alternatively,speaker 225 may be embedded within the frame ofHMD 202 and positioned such that, when theHMD 202 is worn,speaker 225 vibrates a portion of the frame that contacts the wearer. In either case,HMD 202 may be configured to send an audio signal tospeaker 225, so that vibration of the speaker may be directly or indirectly transferred to the bone structure of the wearer. When the vibrations travel through the bone structure to the bones in the middle ear of the wearer, the wearer can interpret the vibrations provided byBCT 225 as sounds. - Various types of bone-conduction transducers (BCTs) may be implemented, depending upon the particular implementation. Generally, any component that is arranged to vibrate the
HMD 202 may be incorporated as a vibration transducer. Yet further it should be understood that anHMD 202 may include asingle speaker 225 or multiple speakers. In addition, the location(s) of speaker(s) on the HMD may vary, depending upon the implementation. For example, a speaker may be located proximate to a wearer's temple (as shown), behind the wearer's ear, proximate to the wearer's nose, and/or at any other location where thespeaker 225 can vibrate the wearer's bone structure. -
FIG. 2B illustrates an alternate view of the wearable computing device illustrated inFIG. 2A . As shown inFIG. 2B , thelens elements HMD 202 may include afirst projector 228 coupled to an inside surface of the extending side-arm 216 and configured to project adisplay 230 onto an inside surface of thelens element 212. Additionally or alternatively, asecond projector 232 may be coupled to an inside surface of the extending side-arm 214 and configured to project adisplay 234 onto an inside surface of thelens element 210. - The
lens elements projectors projectors lens elements frame elements -
FIG. 2C illustrates another wearable computing system according to an example embodiment, which takes the form of anHMD 252. TheHMD 252 may include some or all of the elements ofHMD 202 described with respect toFIGS. 2A and 2B . TheHMD 252 may additionally include an on-board computing system 254 and animage capture device 256, as detailed above with respect toFIGS. 2A and 2B . Theimage capture device 256 is shown mounted on a frame of theHMD 252. However, theimage capture device 256 may be mounted at other positions as well. - As shown in
FIG. 2C , theHMD 252 may include asingle display 258 which may be coupled to the device. Thedisplay 258 may be formed on one of the lens elements of theHMD 252, such as a lens element described with respect toFIGS. 2A and 2B , and may be configured to overlay computer-generated graphics in the user's view of the physical world. Thedisplay 258 is shown to be provided in a center of a lens of theHMD 252, however, thedisplay 258 may be provided in other positions, such as for example towards either the upper or lower portions of the wearer's field of view. Thedisplay 258 is controllable via thecomputing system 254 that is coupled to thedisplay 258 via anoptical waveguide 260. -
FIG. 2D illustrates another wearable computing system according to an example embodiment, which takes the form of amonocular HMD 272.HMD 272 can include some or all of the elements ofHMD 202 and/orHMD 252, described above in the context ofFIGS. 2A , 2B, and 2C. TheHMD 272 may include side-arms 273, acenter frame support 274, and a bridge portion withnosepiece 275. In the example shown inFIG. 2D , thecenter frame support 274 connects the side-arms 273. TheHMD 272 does not include lens-frames containing lens elements. TheHMD 272 may additionally include acomponent housing 276, which may include an on-board computing system (not shown), animage capture device 278, and abutton 279 for operating the image capture device 278 (and/or usable for other purposes).Component housing 276 may also include other electrical components and/or may be electrically connected to electrical components at other locations within or on the HMD.HMD 272 can include a speaker, such asBCT 286. -
HMD 272 may include asingle display 280, which may be coupled to one of the side-arms 273 via thecomponent housing 276. In an example embodiment, thedisplay 280 may be a see-through display, which is made of glass and/or another transparent or translucent material, such that the wearer can see their environment through thedisplay 280. Further, thecomponent housing 276 may include the light sources (not shown) for thedisplay 280 and/or optical elements (not shown) to direct light from the light sources to thedisplay 280. As such,display 280 may include optical features that direct light that is generated by such light sources towards the wearer's eye, whenHMD 272 is being worn. In a further aspect,HMD 272 may include a slidingfeature 284, which may be used to adjust the length of the side-arms 273. Thus, slidingfeature 284 may be used to adjust the fit ofHMD 272. Further, an HMD may include other features that allow a wearer to adjust the fit of the HMD, without departing from the scope of the invention. -
FIGS. 2E to 2G are simplified illustrations of theHMD 272 shown inFIG. 2D , being worn by awearer 290. As shown inFIG. 2F , whenHMD 272 is worn,BCT 286 is arranged such that whenHMD 272 is worn,BCT 286 is located behind the wearer's ear. As such,BCT 286 is not visible from the perspective shown inFIG. 2E . - In the illustrated example, the
display 280 may be arranged such that whenHMD 272 is worn,display 280 is positioned in front of or proximate to a user's eye when theHMD 272 is worn by a user. For example,display 280 may be positioned below the center frame support and above the center of the wearer's eye, as shown inFIG. 2E . Further, in the illustrated configuration,display 280 may be offset from the center of the wearer's eye (e.g., so that the center ofdisplay 280 is positioned to the right and above of the center of the wearer's eye, from the wearer's perspective). - Configured as shown in
FIGS. 2E to 2G ,display 280 may be located in the periphery of the field of view of thewearer 290, whenHMD 272 is worn. Thus, as shown byFIG. 2F , when thewearer 290 looks forward, thewearer 290 may see thedisplay 280 with their peripheral vision. As a result,display 280 may be outside the central portion of the wearer's field of view when their eye is facing forward, as it commonly is for many day-to-day activities. Such positioning can facilitate unobstructed eye-to-eye conversations with others, as well as generally providing unobstructed viewing and perception of the world within the central portion of the wearer's field of view. Further, when thedisplay 280 is located as shown, thewearer 290 may view thedisplay 280 by, e.g., looking up with their eyes only (possibly without moving their head). This is illustrated as shown inFIG. 2G , where the wearer has moved their eyes to look up and align their line of sight withdisplay 280. A wearer might also use the display by tilting their head down and aligning their eye with thedisplay 280. -
FIG. 3 is a block diagram ofHMD 272, in accordance with an example embodiment.HMD 272 can includedata storage 304, wheredata storage 304 is configured to store atleast instructions 310. For example,HMD 272 can be a computing device such ascomputing device 800 discussed below at least in the context ofFIG. 8A , andinstructions 310 can be stored indata storage 304, such asdata storage 804 discussed below at least in the context ofFIG. 8A .Instructions 310 can be instructions such as computer-readable program instructions 806 discussed below at least in the context ofFIG. 8A . In other embodiments,instructions 310 can be utilized in other computing devices than HMDs, such as, but not limited to, a telephone/smart phone, a laptop, a desktop, a server, and/or another type of wearable computing device. -
Instructions 310 when executed by one or processors ofHMD 272 can perform one or more functions. For example, the one or more processors ofHMD 272 can beprocessors 803 discussed below in the context ofcomputing device 800. - The functions can include obtaining a 3D-model query, perhaps via a user interface of
HMD 272. The 3D-model query can include information related to an object. The object can be any object that may have a 3D model, even objects that may be primarily in less than three dimensions; e.g., a piece of paper, a human hair, an object not visible to the human eye. The information can include text, one or more still images, video information, audio information, binary information, other information, and combinations thereof. To provide this information,HMD 272 can be configured with input devices, such as text-input devices, image capture devices (e.g., one or more still and/or video cameras), audio capture devices (e.g., one or more microphones). - For example,
HMD 272 can have a camera configured to capture images related to the object, such as images of part or all of the object and/or images otherwise related to the object, such as, but not limited to, images of Quick Response (QR) codes, bar codes, advertisements, location information, catalog information, ordering information, and information about an environment of the object.HMD 272 can provide the images, and perhaps other information such as text related to the object or images, in the 3D-model query for the object.HMD 272 can obtain the 3D-model query by generating the 3D-model query, formatting the 3D-model query, and/or otherwise gathering the 3D-model query. - Once obtained,
HMD 272 can send the 3D-model query to a model server, such as 3D-model server 410 discussed below in the context of at leastFIG. 4A . The model server can use the information in the 3D-model query to locate a 3D model of the object and provide the 3D model toHMD 272. In other embodiments, the model server can provideHMD 272 with a reference to the 3D model; in these embodiments, the model server can render the 3D model and perhaps update the 3D model rendering based on commands provided byHMD 272 related to the 3D model, such as the commands based on gestures discloses herein. - After receiving the 3D model,
HMD 272 can generate a display of a view of the 3D model and display the view of the 3D model. After the view of the 3D model has been displayed, a wearer of theHMD 272 can make gestures and/or provide other inputs related to the model. The other inputs can include, but are not limited to, voice inputs, text inputs, other image-based inputs, and inputs provided by a user interface, such as a Graphical User Interface (GUI) ofHMD 272 and/or other computing device(s). Example gestures are discussed below in more detail at least in the context ofFIGS. 5A , 5B, and 6. - For example, the wearer of the
HMD 272 can make a gesture to rotate the 3D model.HMD 272 can capture one or more images of the wearer making the rotation gesture. The images of the gesture can be processed; e.g., byHMD 272,model server 410, and/or other computing devices, to determine a corresponding command to change a model view of the 3D model; in this example, to rotate the 3D model. Once a rotation command is available,HMD 272 can update the display of the 3D model in response; in this example,HMD 272 can rotate the 3D model, update the view of the 3D model to show the rotated 3D model, and display the updated view of the 3D model. - One gesture (or other input) can be a close gesture (or corresponding other input) intended to terminate the view of the 3D model. Upon determining that the wearer made the close gesture (or corresponding other input), an end-display command to end the view of the 3D model can be provided to
HMD 272. Upon receiving the end-display command,HMD 272 can indicate that the view of the 3D model is terminated; e.g., using appropriate data, such as the done variable shown inFIG. 3 , and consequently terminate the view of the 3D model. - Example Queries and Gesture-Based Interactions with 3D Models
-
FIG. 4A depictsscenario 400 of displaying a 3D model obtained in response to a 3D-model query, in accordance with an example embodiment.Scenario 400 begins withHMD 272 capturingimage 412 of an object—in this scenario, the object is a house.HMD 272 generates 3D-model query 420 withimage 412 to request delivery of a 3D model of the object depicted inimage 412. -
FIG. 4A shows thatscenario 400 continues withHMD 272 sending 3D-model query 420 to 3D-model server 410. Upon reception of 3D-model query model server 410 analyzesimage 412 in 3D-model query 420 to determine that the object shown inimage 412 is a house. 3D-model server 410 can then query a database of 3D models, or an equivalent data structure, to locate a 3D model of a house. - The database of 3D models can store a number of 3D models, include multiple 3D models of a same type of an object; e.g., multiple 3D models of a house. In
scenario 400, in response to query from 3D-model server 410, the database of 3D models provides multiple models of a house to 3D-model server 410. In some embodiments, the database of 3D models can provide an indication of which provided 3D model or models are closest to the object based on the information provide in a 3D-model query; e.g., an indication of which 3D house model is closest to the house depicted inimage 412. Generally, if 3D-model server 410 finds N models, N>1, in the database of 3D models, then 3D-model server 410 can provide M models, 1≦M≦N, as part of a response to 3D-model query. If M>1, then 3D-model server 410 may provide, in a query response, an indication of which model(s) are closest to the object. In some scenarios,search query 420 can specify a maximum value for M. - In
scenario model query 420, and provide a singleclosest 3D model 430 as part ofquery response 422. Upon reception ofquery response 422,HMD 272 can obtain anddisplay 3D modelScenario 400 can end whenHMD 272 terminates display of3D model 430. -
FIG. 4B depicts a variation ofscenario 400 with3D model 430 displayed in an environment shown as fields of view (FOVs) 440 a and 440 b, in accordance with an example embodiment. In the variation ofscenario 400 shown inFIG. 4B ,3D model 430 is initially displayed withindisplay 442 generated byHMD 272 while a wearer ofHMD 272 observes an environment depicted byFOV 440 a. A left edge ofdisplay 442 is shown having a horizontal distance D1 from a left edge ofFOV 440 a and a top edge ofdisplay 442 is shown having a vertical distance D2 from a top edge ofFOV 440 a. With respect to an environment viewed byFOV 440 a,display 442 of3D model 430 is shown inFIG. 4B roughly centered abovepond 452 and somewhat to the right oftree 450. - The variation of
scenario 400 depicted inFIG. 4B continues with a wearer ofHMD 272 moving their focus leftward; e.g., turning the head and/or eyes of the wearer leftward to look upon the environment as depicted inFIG. 4B asFOV 440 b.FIG. 4B shows that3D model 430 is shown indisplay 442 withinFOV 440 b. As inFOV 440 a ofFIG. 4B , the left edge ofdisplay 442 has horizontal distance D1 from a left edge ofFOV 440 b and the top edge ofdisplay 442 has vertical distance D2 from a top edge ofFOV 440 b. That is,FIG. 4B shows that3D model 430 anddisplay 442 remain in a constant position relative to the FOV after the FOV changes fromFOV 440 a toFOV 440 b.FIG. 4B also shows that3D model 430 and display 442 change position relative to the environment after the FOV changes; e.g.,display 442 is no longer centered overpond 450 and has moved close to a top oftree 450. -
FIG. 4C depicts another variation ofscenario 400 with3D model 430 displayed in an environment shown as FOVs 440 a and 440 b, in accordance with an example embodiment. - In the variation of
scenario 400 shown inFIG. 4C ,3D model 430 is initially displayed byHMD 272 while a wearer ofHMD 272 observes an environment depicted byFOV 440 a. A left edge of3D model 430 is shown having a horizontal distance D3 fromtree 450 and a top edge of3D model 430 is shown having vertical distance D4 from a rightmost edge ofpond 452. With respect toFOV 3D model 430 is shown inFIG. 4C roughly centered and near the bottom of the field of view. - The variation of
scenario 400 depicted inFIG. 4C continues with a wearer ofHMD 272 moving their focus leftward; e.g., turning the head and/or eyes of the wearer leftward to look upon the environment as depicted inFIG. 4C asFOV 440 b. As inFOV 440 a ofFIG. 4C , the left edge of3D model 430 is shown having a horizontal distance D3 fromtree 450 and a top edge of3D model 430 is shown having vertical distance D4 from a rightmost edge ofpond 452. That is,FIG. 4C shows that3D model 430 remains in a constant position relative to the environment after the FOV changes.FIG. 4C also shows that3D model 430 changes position relative to the FOV after the FOV changes; e.g.,3D model 430 is no longer roughly centered within the FOV, but has moved to near to a right edge of the FOV depicted asFOV 440 b. -
FIGS. 5A and 5B are charts of images captured over time with corresponding motions and gestures, in accordance with an example embodiment. Near the top ofFIG. 5A , an example set of three-dimensional axes is shown that indicates positive and negative directions in each axis. - Below the example set of axes,
FIG. 5A shows, arranged in columns, seven example sets of input images, corresponding motions, and related gestures.FIG. 5B shows, arranged in columns similar toFIG. 5A , seven additional example sets of input images, corresponding motions, and related gestures. - The images shown in
FIGS. 5A and 5B can be captured byHMD 272, or some other device. For example, the input images can be single images captured in succession during a video stream; e.g., usingHMD 272 equipped with a video camera. As another example, each image can be a still image captured successively over time; e.g., usingHMD 272 equipped with a still camera. Other techniques and/or devices for capturing sequences of images over time, such as the images shown inFIGS. 5A and 5B , are possible as well. - Each image shown in
FIGS. 5A and 5B depicts movements of hands of a person related toHMD 272, such as the wearer ofHMD 272. For example, the leftmost column ofFIG. 5A showsexample image 510 a capturing a hand of a wearer to the left and roughly vertically centered within the image. Image 510 a is followed in time byimages FIG. 5A .Image 510 b shows the hand shown inimage 510 a after the hand moves to the right, andimage 510 c shows the hand after moving to the right of the hand position shown inimage 510 b. - One or more hand positions can be determined for each image of a hand; e.g., at the center of the hand, at a position of one or more fingers of the hand, at a corner of a bounding box around the hand. The hand positions of hands in
images FIG. 5A overlaying images column 2 or (3, 2) forimage 510 a, at a grid cell of (3, 3) forimage 510 b, and at a grid cell of (3, 4) forimage 510 b. The net movement of the hand then is to move two grid cells to the right, which can be shown asmotion 512 in the +X direction. A hand movement in the +X direction can be classified as a gesture to the right; e.g.,right gesture 514. - The use of an overlaid grid can be used to determine general movements of hands for gestures. By translating the motions of hands to integer-valued grid cell coordinates, and then determining motions based on coordinates, slight variations in movement (e.g., a slight downward or upward movement for right gesture 514) can be filtered out. The size of the grid can be adapted as needed based on an amount of filtering of gestures. For example, the 5×5 grid shown in
FIG. 5A filters hand positions into 25 grid cells, which is more filtering than may be provided by a finer grid, such as a 10×10 grid or a grid indicated by a two-dimensional array of pixels that forms an image. The additional grid cells of a finer grid can be used in circumstances where a more accurate determination of hand position is indicated; e.g., using a 10×10 grid locates hands with twice the accuracy in both horizontal and vertical dimensions than a 5×5 grid. - The sizes of motions can be determined relative to grid cells as well; e.g., a motion of two net grid cells can be considered to be smaller than a movement of three net grid cells and larger than a movement of one net grid cell. Then, the size of a motion can indicate an amount for a command based on the gesture. For example, as discussed above,
right motion 514 involves a net movement of two grid cells. Then, a command based onright gesture 514; e.g., move a 3D model right, can indicate an amount of movement based on the net movement of grid cells; e.g., an amount of 2 in this example, or a relative movement of grid cells; e.g., an amount of 40% of the grid in this example. - If a hand position remains in a same hand position for multiple images, then the hand position can be considered to in a stationary hand position. Stationary hand positions can mark beginnings and ends of gestures. For example, if a hand starts with hand position (x1, y1), stays in at hand position (x1, y1) for two or more images, moves to hand position (x2, y2), and stays in hand position (x2, y2) for two or more images, where (x1, y1) and (x2, y2) each indicate a hand location as a grid cell, pixel, or by some other indicator. Then, the gesture can be determined based on a motion from hand position (x1, y1) to hand position (x2, y2).
- Another technique to determine a beginning or end of a gesture is to determine a first or last image related to the gesture that includes depiction of a hand involved in making the gesture. For example, suppose five images, I1, I2, I3, I4, and I5, are captured over time with I1 captured first, I2 captured second, and so on until I5 is captured. In one example, Image I1 does not depict a hand, I2 includes a hand at hand position (x3, y3), I3 shows the hand at hand position (x4, y4), and I4 and I5 each show the hand at hand position (x5, y5). In this example, the gesture can be determined based on a motion from hand position (x3, y3) to hand position (x5, y5), as the hand originally appeared in image I2 at (x3, y3) and stayed at (x5, y5) for at least two images (I4 and I5).
- In another example, Image I1 shows a hand for the first time at hand position (x6, y6), I2 shows a hand at hand position (x7, y7), I3 shows the hand at hand position (x8, y8), and I4 and I5 each do not depict a hand. In this example, the gesture can be determined based on a motion from hand position (x6, y6) to hand position (x8, y8), as the hand originally appeared in I1 at (x6, y7) and was last captured in I3 at (x8, y8). Other grids, hand position indications, movement sizes, and techniques for determining gestures from input images are possible as well.
- The second-leftmost column of
FIG. 5A showsimages motion 518 in the −X dimension and to leftgesture 520. This relationship can be determined using the above-mentioned techniques regarding images 510 a-510 c,motion 512, andright gesture 514; e.g., a grid can be overlaid on images 516 a-516 c, a net motion can be determined with respect to grid cells that corresponds tomotion 518, andmotion 518 in the −X dimension can be related toleft gesture 520. The third through sixth columns ofFIG. 5A show respective images, gestures, and motions for upgesture 526 in the +Y dimension shown bymotion 524, downgesture 532 in the −Y dimension shown bymotion 530, outgesture 538 in the +Z dimension (away from a camera or other image capture device) indicated bymotion 536, and ingesture 544 in the −Z dimension (toward the camera) indicated bymotion 542. - The rightmost column of
FIG. 5A shows a hand position at roughly a 9 o'clock position relative to a clock inimage 546 a, at roughly 12 o'clock inimage 546 b, at roughly 3 o'clock inimage 546 c, and at roughly 6 o'clock inimage 546 d. Thus, the hand positions in images 546 a-546 d trace a clockwise (CW) motion, shown asclockwise motion 548.Clockwise motion 548 can then be related toclockwise gesture 550. - The leftmost column of
FIG. 5B shows a hand position at roughly a 9 o'clock position relative to a clock inimage 552 a, at roughly 6 o'clock inimage 552 b, at roughly 3 o'clock inimage 552 c, and at roughly 12 o'clock inimage 552 d. Thus, the hand positions inimages 552 a-552 d trace a clockwise (CCW) motion, shown ascounterclockwise motion 554.Counterclockwise motion 554 can then be related toclockwise gesture 556. - The second and third leftmost columns of
FIG. 5B show example gestures related to multiple hand positions per hand. For example, inimage 558 a, an index finger and a thumb of the imaged hand are shown relatively far apart. Inimage 558 b, the index finger and thumb are closer together than shown inimage 558 a, and inimage 558 c, the index finger and thumb are shown in contact. If the positions of the thumb and the index finger are considered two hand positions, then motions of the thumb and index finger as indicated inimages finger motion 560. Thumb/finger motion 560 can be classified aspinch gesture 562, as the index finger and thumb motion indicates a pinching movement. - The reverse of the
pinch gesture 562, e.g., from index finger and thumb initially being in contact to index finger and thumb being relatively far apart, is shown inimages images finger motion 566. Thumb/finger motion 566 can be classified asspread gesture 568, as the index finger and thumb motion are spreading away from each other. - The fourth through seventh-leftmost columns of
FIG. 5B ; i.e., the four rightmost columns, show gestures made using two hands.Images right hand motion 572, in a fashion similar to the motions made by the index finger and thumb captured in images 558 a-558 c. As such, left hand/right hand motion 572 can be determined to bepinch gesture 562.Images right hand motion 578, in a fashion similar to the motions made by the index finger and thumb captured in images 564 a-564 c. As such, left hand/right hand motion 578 can be determined to be spreadgesture 578. -
Images FIG. 5B as left hand/right hand motion 584, can indicate closing; e.g., like curtains at a theater being closed at the end of a show or an act. As such, left hand/right hand motion 584 can be interpreted asclose gesture 586. Conversely,images FIG. 5B as left hand/right hand motion 590, can indicate opening; e.g., like curtains at a theater being opened at the beginning of a show or act. Thus, left hand/right hand motion 590 can be interpreted asopen gesture 592. -
FIG. 6 shows input gestures and corresponding changes to a 3D model, in accordance with an example embodiment. As indicated at the top ofFIG. 6 as “Initial Conditions”, 3D-model display 610 ofHMD 272 is initially displaying a view of3D model 430. The portion of3D model 430 visible in 3D-model display 610 is shown inFIG. 6 using a rectangle. For example, in the “Initial Conditions” portion ofFIG. 6 , 3D-model display 610 is shown generally centered over3D model 430. -
FIG. 6 shows three pairs of columns below the Initial Conditions portion. Each pair of columns includes a “Gesture” column indicating a gesture input toHMD 272 and a “Resulting Display” column showing an effect of the gesture input on 3D-model display 630. - At the top of the leftmost pair of columns in
FIG. 6 ,right gesture 514 is indicated as input toHMD 272, and in response, resultingdisplay 610 a shows that the view of3D model 430 has been shifted left which corresponds to a rightward movement of3D model 430 as commanded byright gesture 514. That is,right gesture 514 can correspond to; i.e., lead to generation of, a command toHMD 272 to shift the view shown in 3D-model display 610 left and/or move3D model 430 right. Similarly, the other input gestures discussed at least in the context ofFIG. 6 , when input toHMD 272 can correspond to one or more commands toHMD 272 to change the view shown in 3D-model display 610 and/or3D model 430 as indicated as discussed in the context of each respective input gesture. - The second-from-top row in the leftmost pair of columns shows
left gesture 520 indicated as input toHMD 272, and in response, resultingdisplay 610 b shows that the view of3D model 430 has been shifted right which corresponds to a leftward movement of3D model 430 as commanded byleft gesture 514. - The third-from-top row in the leftmost pair of columns shows up
gesture 526 indicated as input toHMD 272. In response,HMD 272 generates resultingdisplay 610 c showing that the view of3D model 430 has been moved down corresponding to commanded upward movement of the 3D model. The fourth-from-top rows in the leftmost pair of columns shows downgesture 532 indicated as input toHMD 272. In response,HMD 272 generates resultingdisplay 610 d with the view of3D model 430 having been moved upward, corresponding to a commanded downward movement of the 3D model. - The bottom row in the leftmost pair of columns and the five rows in the center pair of columns of
FIG. 6 show example input gestures related to commanded rotations of3D model 430. When a left gesture is (immediately) followed by a right gesture,HMD 272 can interpret that pair of gestures as one left-then-right gesture 620.HMD 272 can interpret left-then-right gesture 620 as an edge-on view of a gesture representing a circle rotating about the Y axis in a clockwise direction; in response,HMD 272 can generate resultingdisplay 610 e showing a view of3D model 430 rotated clockwise about its Y axis. Similarly,HMD 272 can interpret a right gesture (immediately) followed by a left gesture as right-then-leftgesture 622; in response,HMD 272 can generate resultingdisplay 610 f showing a view of3D model 430 rotated counterclockwise about its Y axis. - When an up gesture is (immediately) followed by a down gesture,
HMD 272 can interpret that pair of gestures as one up-then-downgesture 630.HMD 272 can interpret up-then-downgesture 630 as an edge-on view of a gesture representing a circle rotating about the X axis in a counterclockwise direction; in response,HMD 272 can generate resultingdisplay 610 g showing3D model 430 rotated clockwise about its X axis. SimilarlyHMD 272 can interpret a down gesture (immediately) followed by an up gesture as one down-then-upgesture 632 and can generate resultingdisplay 610 h showing a view of3D model 430 rotated counterclockwise about its X axis. -
HMD 272 can interpretclockwise gesture 550 as a command to rotate3D model 430 clockwise about its Z axis and generate resultingdisplay 610 i showing a view of3D model 430 rotated in a clockwise direction about its Z axis. Additionally,HMD 272 can interpretcounterclockwise gesture 550 as a command to rotate3D model 430 counterclockwise about its Z axis and generate resultingdisplay 610 j showing3D model 430 rotated in a counterclockwise direction about its Z axis. - In some cases, two or more different gestures can have identical effects on a resulting display. For example, performing either in
gesture 544 or spreadgesture 568 can be interpreted as a command for zooming in, enlarging, moving3D model 430 closer to the display, and/or moving3D model 430 in the −Z direction. In response to either ingesture 544 or spreadgesture 568,HMD 272 can generate an enlarged view of3D model 430 as indicated using resultingdisplay 610 k, shown in the top row of the rightmost pair of columns inFIG. 6 . As another example, performing either outgesture 538 orpinch gesture 562 can be interpreted as a command for zooming out, shrinking, moving3D model 430 away from the display, and/or moving3D model 430 in the +Z direction. In response to either outgesture 538 orpinch gesture 562,HMD 272 can generate a smaller view of3D model 430 as indicated using resultingdisplay 610 m, shown in the second-from-top row of the rightmost pair of columns inFIG. 6 . - The middle row of the rightmost pair of columns in
FIG. 6 shows thatclose gesture 586 can correspond tocommanding HMD 272 to terminate display a view of3D model 430. The second-from-bottom (fourth-from-top) middle row of the rightmost pair of columns inFIG. 6 shows thatopen gesture 586 can correspond tocommanding HMD 272 to launch a query for a new model usingquery display 640. In some embodiments, one or more gestures, such asclose gesture 586, received as input whileHMD 272 is displayingquery display 640 can be interpreted as closingquery display 640 and perhaps reverting to the previous display; e.g.,display 610 shown as part of the initial conditions. - The gestures discussed in the context of
FIGS. 5A , 5B, and 6 can be combined. For example, to rotate a 3D model on its X axis and then on its Y axis, a first gesture, such as an up-then-down gesture or a down-then-up gesture, can be received to rotate the 3D model on the X axis and then a second gesture, such as a left-then-right gesture or right-then-left gesture, can be received to rotate the 3D model on the Y axis. - In some embodiments,
other gestures 650 can be input toHMD 272. For example, a diagonal line gesture can be input toHMD 272 corresponding to a combination of either a left or a right gesture and either an up or a down gesture. For example, a gesture with an initial hand position at or near an upper-left-hand corner of an image and ending with a hand position at or near a lower-right-hand corner of a subsequent image can be interpreted as a combination of a right gesture and a down gesture, and so translate a view of3D model 430 rightwards and downwards in3D model display 610. -
Other gestures 650 can include gestures that interact with the object modeled by3D model 430. For example, a gesture to touch the object modeled by3D model 430 can lead toHMD 272 providing additional information about part of or the entire object, such as name, model number, identification number, size, weight, material(s) used to make the object, component information, cost/price/delivery information, other information, and combinations thereof. - As another example,
other gestures 650 can include gestures to touch virtual buttons or other control elements that control 3D-model display 630. For example, a sequence of gestures to touch a virtual “color” button, to touch a blue color on a color-palette control, and to touch3D model 430 can change the color of3D model 430 to a blue color. A later sequence of gestures to touch a beige color on the color-palette control and to touch3D model 430 can change the color of3D model 430 from blue to beige. - Other interpretations can be used as well for gestures shown in
FIG. 6 , perhaps in the context of touching part or all of the object. For example,open gesture 592 made while both hands are touching the object (but perhaps not in contact with each other) can be interpreted as a gesture to split the object into components. As another example,close gesture 586 begun with each hand on a separate component of the object (or two separate objects) can be interpreted as a gesture to combine the components of the object (or the two separate objects) into one object. -
Other gestures 650 can include gestures to perform other operations on3D model 430 than already discussed in the context ofFIGS. 5A , 5B, and 6. These other operations can include, but are not limited to, skew operations, move within or outside of3D model 430,store 3D model3D model 430, add an additional model to display 610 (e.g., add landscaping-related model(s) to the house modeled as 3D model 430), remove model(s) fromdisplay 610, operations that involve sub-operations discussed in the context of previously-discussed gestures, and combinations thereof. - Once all gestures are identified for interacting with the object/3D model, then gestures not identified for object/3D model interaction can be ignored or otherwise processed. For example, a gesture to touch or point at something other than the object/3D model can be interpreted as a gesture unrelated to the object. Gestures determined to be unrelated to the object/3D model can be ignored, prompted (e.g., “Did you mean to interact with the 3D model? If so, please repeat your gesture.”), or otherwise processed.
- As an example of prompting for gestures, suppose a new gesture, such as a two-handed leftward movement, is performed by a wearer of
HMD 272. Then, after determining the new gesture is unrecognized,HMD 272 can access a define/redefine gesture function to create and/or change mappings between gestures and operations.HMD 272 can provide a prompt to the wearer to inform the wearer about the new gesture being an unrecognized gesture.HMD 272 can request the wearer then either indicate which operation(s) be performed in response to the unrecognized gesture or if the unrecognized gesture should be ignored. In this example, in response to being prompted about the unrecognized two-handed leftward movement, the wearer can indicate that, that two (or more) “left” model view changes are to be made in succession; i.e., two-handed leftward gestures can now act as a “fast forward left” motion. After receiving the indication from the wearer,HMD 272 can use the define/redefine gesture function to associate two-handed leftward movements with two (or more) left model view changes. - The define/redefine gesture function can be used to redefine operations that occur in response to gestures shown in
FIG. 6 ; e.g., a wearer that would like to reverse the left and right gestures can access the define/redefine gesture function. Then,HMD 272 can prompt the wearer to make a gesture, such as the left gesture, perhaps identify the gesture, and then prompt the wearer to select one or more operations to perform in response to the gestures; e.g., a rightward model movement. After receiving the inputs about the left gesture to be redefined and the operation(s) to be performed for the redefined left gesture,HMD 272 can use the define/redefine gesture function to change the mapping of the left gesture so to make a rightward model movement. A similar procedure can then be used by the wearer to change the mapping of the right gesture to a leftward model movement. Many other examples of input gestures and corresponding commands toHMD 272 are possible as well. - Example Data Network
-
FIG. 7 showsserver devices network 706, withprogrammable devices Network 706 may correspond to a LAN, a wide area network (WAN), a corporate intranet, the public Internet, or any other type of network configured to provide a communications path between networked computing devices. Thenetwork 706 may also correspond to a combination of one or more LANs, WANs, corporate intranets, and/or the public Internet. One or more ofserver devices network 706, andprogrammable devices method 100 and/or some or all of the herein-described functionality ofHMD 272 and/or 3D-model server 410. - Although
FIG. 7 only shows three programmable devices, distributed application architectures may serve tens, hundreds, or thousands of programmable devices. Moreover,programmable devices programmable devices programmable devices programmable devices -
Server devices programmable devices server device 708 and/or 710 can provide content to programmable devices 704 a-704 c. The content can include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio, and/or video. The content can include compressed and/or uncompressed content. The content can be encrypted and/or unencrypted. Other types of content are possible as well. - As another example,
server device 708 and/or 710 can provide programmable devices 704 a-704 c with access to software for database, search, computation, graphical, audio, video, World Wide Web/Internet utilization, and/or other functions. Many other examples of server devices are possible as well. - Computing Device Architecture
-
FIG. 8A is a block diagram of a computing device (e.g., system) in accordance with an example embodiment. In particular,computing device 800 shown inFIG. 8A can be configured to perform part or all ofmethod 100 and/or some or all of the herein-described functionality ofHMD model server 410, one or more functions ofserver devices network 706, and/or one or more ofprogrammable devices Computing device 800 may include a user interface module 801, a network-communication interface module 802, one ormore processors 803, anddata storage 804, all of which may be linked together via a system bus, network, orother connection mechanism 805. - User interface module 801 can be operable to send data to and/or receive data from external user input/output devices. For example, user interface module 801 can be configured to send and/or receive data to and/or from user input devices such as a keyboard, a keypad, a touch screen, a computer mouse, a track ball, a joystick, a camera, a voice recognition module, and/or other similar devices. User interface module 801 can also be configured to provide output to user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays (LCDs), light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, either now known or later developed. User interface module 801 can also be configured to generate audible output(s), such as a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices.
- Network-
communications interface module 802 can include one or morewireless interfaces 807 and/or one ormore wireline interfaces 808 that are configurable to communicate via a network, such asnetwork 706 shown inFIG. 7 . Wireless interfaces 807 can include one or more wireless transmitters, receivers, and/or transceivers, such as a Bluetooth transceiver, a Zigbee transceiver, a Wi-Fi transceiver, a WiMAX transceiver, and/or other similar type of wireless transceiver configurable to communicate via a wireless network. Wireline interfaces 808 can include one or more wireline transmitters, receivers, and/or transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network. - In some embodiments, network
communications interface module 802 can be configured to provide reliable, secured, and/or authenticated communications. For each communication described herein, information for ensuring reliable communications (i.e., guaranteed message delivery) can be provided, perhaps as part of a message header and/or footer (e.g., packet/message sequencing information, encapsulation header(s) and/or footer(s), size/time information, and transmission verification information such as CRC and/or parity check values). Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, DES, AES, RSA, Diffie-Hellman, and DSA. Other cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications. -
Processors 803 can include one or more general purpose processors and/or one or more special purpose processors (e.g., digital signal processors, application specific integrated circuits, etc.).Processors 803 can be configured to execute computer-readable program instructions 806 that are contained in thedata storage 804 and/or other instructions as described herein. In some embodiments,data storage 804 can additionally include storage required to perform at least part of the herein-described methods and techniques and/or at least part of the functionality of the herein-described devices and networks. -
Data storage 804 can include one or more computer-readable storage media that can be read and/or accessed by at least one ofprocessors 803. For example,data storage 804 can provide memory for the herein-described instructions; e.g.,instructions 310, and/or lighting data storage; e.g., lighting data storage 320. The one or more computer-readable storage media can include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with at least one ofprocessors 803. In some embodiments,data storage 804 can be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other embodiments,data storage 804 can be implemented using two or more physical devices. - Cloud-Based Servers
-
FIG. 8B depicts a network 606 of computing clusters 809 a, 809 b, 809 c arranged as a cloud-based server system in accordance with an example embodiment. Server devices 608 and/or 610 can be cloud-based devices that store program logic and/or data of cloud-based applications and/or services. In some embodiments, server devices 608 and/or 610 can be a single computing device residing in a single computing center. In other embodiments, server device 608 and/or 610 can include multiple computing devices in a single computing center, or even multiple computing devices located in multiple computing centers located in diverse geographic locations. For example,FIG. 6 depicts each ofserver devices 608 and 610 residing in different physical locations. - In some embodiments, data and services at server devices 608 and/or 610 can be encoded as computer readable information stored in non-transitory, tangible computer readable media (or computer readable storage media) and accessible by programmable devices 604 a, 604 b, and 604 c, and/or other computing devices. In some embodiments, data at server device 608 and/or 610 can be stored on a single disk drive or other tangible storage media, or can be implemented on multiple disk drives or other tangible storage media located at one or more diverse geographic locations.
-
FIG. 8B depicts a cloud-based server system in accordance with an example embodiment. InFIG. 8B , the functions of server device 608 and/or 610 can be distributed among three computing clusters 809 a, 809 b, and 809 c. Computing cluster 809 a can include one ormore computing devices 800 a, cluster storage arrays 810 a, and cluster routers 811 a connected by alocal cluster network 812 a. Similarly, computing cluster 809 b can include one ormore computing devices 800 b, cluster storage arrays 810 b, and cluster routers 811 b connected by alocal cluster network 812 b. Likewise, computing cluster 809 c can include one ormore computing devices 800 c, cluster storage arrays 810 c, andcluster routers 811 c connected by alocal cluster network 812 c. - In some embodiments, each of the computing clusters 809 a, 809 b, and 809 c can have an equal number of computing devices, an equal number of cluster storage arrays, and an equal number of cluster routers. In other embodiments, however, each computing cluster can have different numbers of computing devices, different numbers of cluster storage arrays, and different numbers of cluster routers. The number of computing devices, cluster storage arrays, and cluster routers in each computing cluster can depend on the computing task or tasks assigned to each computing cluster.
- In computing cluster 809 a, for example,
computing devices 800 a can be configured to perform various computing tasks of electronic communications server 812. In one embodiment, the various functionalities of electronic communications server 812 can be distributed among one or more ofcomputing devices Computing devices devices 800 a in computing cluster 809 a. On the other hand, in some embodiments,computing devices - In some embodiments, computing tasks and stored data associated with server devices 608 and/or 610 can be distributed across
computing devices computing devices - The cluster storage arrays 810 a, 810 b, and 810 c of the computing clusters 809 a, 809 b, and 809 c can be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives. The disk array controllers, alone or in conjunction with their respective computing devices, can also be configured to manage backup or redundant copies of the data stored in the cluster storage arrays to protect against disk drive or other cluster storage array failures and/or network failures that prevent one or more computing devices from accessing one or more cluster storage arrays.
- Similar to the manner in which the functions of server devices 608 and/or 610 can be distributed across
computing devices server device 610. Additionally, some cluster storage arrays can be configured to store backup versions of data stored in other cluster storage arrays. - The
cluster routers 811 a, 811 b, and 811 c in computing clusters 809 a, 809 b, and 809 c can include networking equipment configured to provide internal and external communications for the computing clusters. For example, the cluster routers 811 a in computing cluster 809 a can include one or more internet switching and routing devices configured to provide (i) local area network communications between thecomputing devices 800 a and the cluster storage arrays 801 a via thelocal cluster network 812 a, and (ii) wide area network communications between the computing cluster 809 a and the computing clusters 809 b and 809 c via the wide area network connection 813 a to network 606.Cluster routers 811 b and 811 c can include network equipment similar to the cluster routers 811 a, andcluster routers 811 b and 811 c can perform similar networking functions for computing clusters 809 b and 809 b that cluster routers 811 a perform for computing cluster 809 a. - In some embodiments, the configuration of the
cluster routers 811 a, 811 b, and 811 c can be based at least in part on the data communication requirements of the computing devices and cluster storage arrays, the data communications capabilities of the network equipment in thecluster routers 811 a, 811 b, and 811 c, the latency and throughput oflocal networks - Example methods and systems are described herein. It should be understood that the words “example” and “exemplary” are used herein to mean “serving as an example, instance, or illustration.” Any embodiment or feature described herein as being an “example” or “exemplary” is not necessarily to be construed as preferred or advantageous over other embodiments or features.
- The present disclosure is not to be limited in terms of the particular embodiments described in this application, which are intended as illustrations of various aspects. Many modifications and variations can be made without departing from its scope, as will be apparent to those skilled in the art. Functionally equivalent methods and apparatuses within the scope of the disclosure, in addition to those enumerated herein, will be apparent to those skilled in the art from the foregoing descriptions. Such modifications and variations are intended to fall within the scope of the appended claims.
- The above detailed description describes various features and functions of the disclosed systems, devices, and methods with reference to the accompanying figures. In the figures, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the detailed description, figures, and claims are not meant to be limiting. Other embodiments can be utilized, and other changes can be made, without departing from the scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein.
- It should be understood that other embodiments can include more or less of each element shown in a given figure. Further, some of the illustrated elements can be combined or omitted. Yet further, an example embodiment can include elements that are not illustrated in the figures.
- With respect to any or all of the ladder diagrams, scenarios, and flow charts in the figures and as discussed herein, each block and/or communication may represent a processing of information and/or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments, for example, functions described as blocks, transmissions, communications, requests, responses, and/or messages may be executed out of order from that shown or discussed, including substantially concurrent or in reverse order, depending on the functionality involved. Further, more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow charts discussed herein, and these ladder diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.
- Moreover, a block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
- A block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique. Alternatively or additionally, a block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data). The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and/or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage medium.
- The computer readable medium may also include non-transitory computer readable media such as computer-readable media that stores data for short periods of time like register memory, processor cache, and random access memory (RAM). The computer readable media may also include non-transitory computer readable media that stores program code and/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example. The computer readable media may also be any other volatile or non-volatile storage systems. A computer readable medium may be considered a computer readable storage medium, for example, and/or a tangible storage device.
- While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting, with the true scope being indicated by the following claims.
Claims (20)
1. A method, comprising:
receiving a 3D model for an object at a head-mountable display (HMD), wherein the 3D model comprises three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, wherein each of the first axis, the second axis, and the third axis differs;
displaying a view of the 3D model using the HMD;
receiving an input gesture at the HMD;
determining whether the input gesture comprises a 3D-model gesture of a plurality of 3D-model gestures, wherein the plurality of 3D-model gestures comprise a first 3D-model gesture to modify display of the 3D model with respect to the first axis, a second 3D-model gesture to modify display of the 3D model with respect to the second axis, and a third 3D-model gesture to modify display of the 3D model with respect to the third axis; and
after determining that the input gesture is related to the 3D model, the HMD:
updating the view of the 3D model based on the input gesture, and
displaying the updated view of the 3D model.
2. The method of claim 1 , wherein the input gesture comprises a gesture to rotate the 3D model.
3. The method of claim 2 , wherein updating the view of the 3D model comprises:
rotating the 3D model as indicated by the gesture to rotate the 3D model; and
generating a display of the rotated 3D model.
4. The method of claim 2 , wherein the gesture to rotate the 3D model comprises a first gesture to rotate the 3D model on a first axis and a second gesture to rotate the 3D model on a second axis, and wherein the first axis differs from the second axis.
5. The method of claim 1 , wherein the information about the object comprises an image related to the object.
6. The method of claim 5 , wherein the image related to the object is at least one image selected from the group consisting of an image that depicts at least part of the object, an image of a Quick Response (QR) code related to the object, and an image of a bar code related to the object.
7. The method of claim 1 , further comprising:
receiving a second input gesture at the HMD;
determining whether the second input gesture corresponds to a close gesture; and
in response to determining that the second input gesture corresponds to the close gesture, terminating the view of the 3D model.
8. A head-mountable display (HMD), comprising:
a display;
a processor; and
a non-transitory computer readable medium, configured to store at least instructions, wherein the instructions are executable by the processor to cause the HMD to perform functions comprising:
receiving a 3D model for an object, wherein the 3D model comprises three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, wherein each of the first axis, the second axis, and the third axis differs;
displaying a view of the 3D model using the display;
receiving an input gesture;
determining whether the input gesture comprises a 3D-model gesture of a plurality of 3D-model gestures, wherein the plurality of 3D-model gestures comprise a first 3D-model gesture to modify display of the 3D model with respect to the first axis, a second 3D-model gesture to modify display of the 3D model with respect to the second axis, and a third 3D-model gesture to modify display of the 3D model with respect to the third axis; and
after determining that the input gesture is related to the 3D model:
updating the view of the 3D model based on the input gesture, and
displaying the updated view of the 3D model using the display.
9. The HMD of claim 8 , wherein the input gesture comprises a gesture to rotate the 3D model.
10. The HMD of claim 9 , wherein updating the view of the 3D model comprises:
rotating the 3D model as indicated by the gesture to rotate the 3D model; and
generating a display of the rotated 3D model.
11. The HMD of claim 9 , wherein the gesture to rotate the 3D model comprises a first gesture to rotate the 3D model on a first axis and a second gesture to rotate the 3D model on a second axis, and wherein the first axis differs from the second axis.
12. The HMD of claim 8 , further comprising a camera, and wherein receiving the input gesture comprises:
capturing a plurality of images using the camera; and
determining the input gesture from the plurality of images.
13. The HMD of claim 8 , wherein the information about the object comprises an image related to the object.
14. The HMD of claim 8 , further comprising:
receiving a second input gesture;
determining whether the second input gesture corresponds to a close gesture; and
in response to determining that the second input gesture corresponds to the close gesture, terminating the view of the 3D model.
15. A non-transitory computer readable medium having stored therein instructions executable by a processor of a head-mountable display (HMD) to cause the HMD to perform functions, the functions comprising:
receiving a 3D model for an object, wherein the 3D model comprises three-dimensional shape and texture information about the object, the three-dimensional shape and texture information about the object specified with respect to at least a first axis, a second axis, and a third axis, and wherein each of the first axis, the second axis, and the third axis differs;
displaying a view of the 3D model;
receiving an input gesture;
determining whether the input gesture comprises a 3D-model gesture of a plurality of 3D-model gestures, wherein the plurality of 3D-model gestures comprise a first 3D-model gesture to modify display of the 3D model with respect to the first axis, a second 3D-model gesture to modify display of the 3D model with respect to the second axis, and a third 3D-model gesture to modify display of the 3D model with respect to the third axis; and
after determining that the input gesture is related to the 3D model:
updating the view of the 3D model based on the input gesture, and
displaying the updated view of the 3D model.
16. The non-transitory computer readable medium of claim 15 , wherein the input gesture comprises a gesture to rotate the 3D model.
17. The non-transitory computer readable medium of claim 16 , wherein updating the view of the 3D model comprises:
rotating the 3D model as indicated by the gesture to rotate the 3D model; and
generating a display of the rotated 3D model.
18. The non-transitory computer readable medium of claim 16 , wherein the gesture to rotate the 3D model comprises a first gesture to rotate the 3D model on a first axis and a second gesture to rotate the 3D model on a second axis, and wherein the first axis differs from the second axis.
19. The non-transitory computer readable medium of claim 15 , wherein the information about the object comprises an image related to the object.
20. The non-transitory computer readable medium of claim 15 , further comprising:
receiving a second input gesture;
determining whether the second input gesture corresponds to a close gesture; and
in response to determining that the second input gesture corresponds to the close gesture, terminating the view of the 3D model.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/108,365 US20150169070A1 (en) | 2013-12-17 | 2013-12-17 | Visual Display of Interactive, Gesture-Controlled, Three-Dimensional (3D) Models for Head-Mountable Displays (HMDs) |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/108,365 US20150169070A1 (en) | 2013-12-17 | 2013-12-17 | Visual Display of Interactive, Gesture-Controlled, Three-Dimensional (3D) Models for Head-Mountable Displays (HMDs) |
Publications (1)
Publication Number | Publication Date |
---|---|
US20150169070A1 true US20150169070A1 (en) | 2015-06-18 |
Family
ID=53368391
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/108,365 Abandoned US20150169070A1 (en) | 2013-12-17 | 2013-12-17 | Visual Display of Interactive, Gesture-Controlled, Three-Dimensional (3D) Models for Head-Mountable Displays (HMDs) |
Country Status (1)
Country | Link |
---|---|
US (1) | US20150169070A1 (en) |
Cited By (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150192776A1 (en) * | 2014-01-06 | 2015-07-09 | Samsung Electronics Co., Ltd. | Electronic glasses and method for correcting color blindness |
US20150227198A1 (en) * | 2012-10-23 | 2015-08-13 | Tencent Technology (Shenzhen) Company Limited | Human-computer interaction method, terminal and system |
US20150378158A1 (en) * | 2013-02-19 | 2015-12-31 | Brilliantservice Co., Ltd. | Gesture registration device, gesture registration program, and gesture registration method |
WO2016208881A1 (en) * | 2015-06-22 | 2016-12-29 | Samsung Electronics Co., Ltd. | Three-dimensional user interface for head-mountable display |
WO2017126709A1 (en) * | 2016-01-18 | 2017-07-27 | 엘지전자 주식회사 | Mobile terminal and control method therefor |
US20170228929A1 (en) * | 2015-09-01 | 2017-08-10 | Patrick Dengler | System and Method by which combining computer hardware device sensor readings and a camera, provides the best, unencumbered Augmented Reality experience that enables real world objects to be transferred into any digital space, with context, and with contextual relationships. |
CN108780360A (en) * | 2016-03-22 | 2018-11-09 | 微软技术许可有限责任公司 | Virtual reality is navigated |
US10416835B2 (en) | 2015-06-22 | 2019-09-17 | Samsung Electronics Co., Ltd. | Three-dimensional user interface for head-mountable display |
US20190302484A1 (en) * | 2018-04-03 | 2019-10-03 | Boe Technology Group Co., Ltd. | Smart glasses and wearing instruction method for smart glasses |
US10466487B2 (en) | 2017-06-01 | 2019-11-05 | PogoTec, Inc. | Releasably attachable augmented reality system for eyewear |
JP2019207574A (en) * | 2018-05-29 | 2019-12-05 | 富士ゼロックス株式会社 | Information processing device, information processing system, and program |
US10567730B2 (en) * | 2017-02-20 | 2020-02-18 | Seiko Epson Corporation | Display device and control method therefor |
US10634912B2 (en) | 2017-06-01 | 2020-04-28 | NewSight Reality, Inc. | See-through near eye optical module |
US10634921B2 (en) | 2017-06-01 | 2020-04-28 | NewSight Reality, Inc. | See-through near eye optical display |
US10884246B2 (en) | 2017-06-01 | 2021-01-05 | NewSight Reality, Inc. | Releasably attachable augmented reality system for eyewear |
US11119353B2 (en) | 2017-06-01 | 2021-09-14 | E-Vision Smart Optics, Inc. | Switchable micro-lens array for augmented reality and mixed reality |
US11175803B2 (en) | 2019-02-07 | 2021-11-16 | International Business Machines Corporation | Remote guidance for object observation |
US11269421B2 (en) * | 2015-05-15 | 2022-03-08 | Atheer, Inc. | Method and apparatus for applying free space input for surface constrained control |
CN114365214A (en) * | 2020-08-14 | 2022-04-15 | 海思智财控股有限公司 | System and method for superimposing virtual image on real-time image |
CN114365027A (en) * | 2019-11-06 | 2022-04-15 | 海思智财控股有限公司 | System and method for displaying object with depth of field |
US20220180603A1 (en) * | 2019-03-15 | 2022-06-09 | Witapp S.R.L. | System and computer implemented method for 3d processing of a tomography test |
US11953689B2 (en) | 2020-09-30 | 2024-04-09 | Hes Ip Holdings, Llc | Virtual image display system for virtual reality and augmented reality devices |
Citations (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090040225A1 (en) * | 2007-07-20 | 2009-02-12 | Fujitsu Limited | Apparatus and method for three-dimensional model retrieval |
US20090319181A1 (en) * | 2008-06-20 | 2009-12-24 | Microsoft Corporation | Data services based on gesture and location information of device |
US20100103104A1 (en) * | 2008-10-29 | 2010-04-29 | Electronics And Telecommunications Research Institute | Apparatus for user interface based on wearable computing environment and method thereof |
US20120087580A1 (en) * | 2010-01-26 | 2012-04-12 | Gwangju Institute Of Science And Technology | Vision image information storage system and method thereof, and recording medium having recorded program for implementing method |
US20120146998A1 (en) * | 2010-12-14 | 2012-06-14 | Samsung Electronics Co., Ltd. | System and method for multi-layered augmented reality |
US20130024819A1 (en) * | 2011-07-18 | 2013-01-24 | Fuji Xerox Co., Ltd. | Systems and methods for gesture-based creation of interactive hotspots in a real world environment |
US8471849B1 (en) * | 2012-03-19 | 2013-06-25 | Google Inc. | Prioritization of display of portions of three-dimensional object models |
US20130249944A1 (en) * | 2012-03-21 | 2013-09-26 | Sony Computer Entertainment Europe Limited | Apparatus and method of augmented reality interaction |
US20140169621A1 (en) * | 2012-12-13 | 2014-06-19 | Intel Corporation | Gesture pre-processing of video stream to reduce platform power |
US20140210947A1 (en) * | 2013-01-30 | 2014-07-31 | F3 & Associates, Inc. | Coordinate Geometry Augmented Reality Process |
US20140247278A1 (en) * | 2013-03-01 | 2014-09-04 | Layar B.V. | Barcode visualization in augmented reality |
US20140282275A1 (en) * | 2013-03-15 | 2014-09-18 | Qualcomm Incorporated | Detection of a zooming gesture |
US20140361981A1 (en) * | 2013-06-07 | 2014-12-11 | Canon Kabushiki Kaisha | Information processing apparatus and method thereof |
US20150055821A1 (en) * | 2013-08-22 | 2015-02-26 | Amazon Technologies, Inc. | Multi-tracker object tracking |
US20150181200A1 (en) * | 2012-09-14 | 2015-06-25 | Nokia Corporation | Remote control system |
US20150227795A1 (en) * | 2012-01-06 | 2015-08-13 | Google Inc. | Object Outlining to Initiate a Visual Search |
-
2013
- 2013-12-17 US US14/108,365 patent/US20150169070A1/en not_active Abandoned
Patent Citations (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090040225A1 (en) * | 2007-07-20 | 2009-02-12 | Fujitsu Limited | Apparatus and method for three-dimensional model retrieval |
US20090319181A1 (en) * | 2008-06-20 | 2009-12-24 | Microsoft Corporation | Data services based on gesture and location information of device |
US20100103104A1 (en) * | 2008-10-29 | 2010-04-29 | Electronics And Telecommunications Research Institute | Apparatus for user interface based on wearable computing environment and method thereof |
US20120087580A1 (en) * | 2010-01-26 | 2012-04-12 | Gwangju Institute Of Science And Technology | Vision image information storage system and method thereof, and recording medium having recorded program for implementing method |
US20120146998A1 (en) * | 2010-12-14 | 2012-06-14 | Samsung Electronics Co., Ltd. | System and method for multi-layered augmented reality |
US20130024819A1 (en) * | 2011-07-18 | 2013-01-24 | Fuji Xerox Co., Ltd. | Systems and methods for gesture-based creation of interactive hotspots in a real world environment |
US20150227795A1 (en) * | 2012-01-06 | 2015-08-13 | Google Inc. | Object Outlining to Initiate a Visual Search |
US8471849B1 (en) * | 2012-03-19 | 2013-06-25 | Google Inc. | Prioritization of display of portions of three-dimensional object models |
US20130249944A1 (en) * | 2012-03-21 | 2013-09-26 | Sony Computer Entertainment Europe Limited | Apparatus and method of augmented reality interaction |
US20150181200A1 (en) * | 2012-09-14 | 2015-06-25 | Nokia Corporation | Remote control system |
US20140169621A1 (en) * | 2012-12-13 | 2014-06-19 | Intel Corporation | Gesture pre-processing of video stream to reduce platform power |
US20140210947A1 (en) * | 2013-01-30 | 2014-07-31 | F3 & Associates, Inc. | Coordinate Geometry Augmented Reality Process |
US20140247278A1 (en) * | 2013-03-01 | 2014-09-04 | Layar B.V. | Barcode visualization in augmented reality |
US20140282275A1 (en) * | 2013-03-15 | 2014-09-18 | Qualcomm Incorporated | Detection of a zooming gesture |
US20140361981A1 (en) * | 2013-06-07 | 2014-12-11 | Canon Kabushiki Kaisha | Information processing apparatus and method thereof |
US20150055821A1 (en) * | 2013-08-22 | 2015-02-26 | Amazon Technologies, Inc. | Multi-tracker object tracking |
Cited By (32)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150227198A1 (en) * | 2012-10-23 | 2015-08-13 | Tencent Technology (Shenzhen) Company Limited | Human-computer interaction method, terminal and system |
US9857589B2 (en) * | 2013-02-19 | 2018-01-02 | Mirama Service Inc. | Gesture registration device, gesture registration program, and gesture registration method |
US20150378158A1 (en) * | 2013-02-19 | 2015-12-31 | Brilliantservice Co., Ltd. | Gesture registration device, gesture registration program, and gesture registration method |
US20150192776A1 (en) * | 2014-01-06 | 2015-07-09 | Samsung Electronics Co., Ltd. | Electronic glasses and method for correcting color blindness |
US10025098B2 (en) * | 2014-01-06 | 2018-07-17 | Samsung Electronics Co., Ltd. | Electronic glasses and method for correcting color blindness |
US20220261086A1 (en) * | 2015-05-15 | 2022-08-18 | West Texas Technology Partners, Llc | Method and apparatus for applying free space input for surface constrained control |
US11269421B2 (en) * | 2015-05-15 | 2022-03-08 | Atheer, Inc. | Method and apparatus for applying free space input for surface constrained control |
US11579706B2 (en) * | 2015-05-15 | 2023-02-14 | West Texas Technology Partners, Llc | Method and apparatus for applying free space input for surface constrained control |
US11836295B2 (en) * | 2015-05-15 | 2023-12-05 | West Texas Technology Partners, Llc | Method and apparatus for applying free space input for surface constrained control |
US20230297173A1 (en) * | 2015-05-15 | 2023-09-21 | West Texas Technology Partners, Llc | Method and apparatus for applying free space input for surface constrained control |
WO2016208881A1 (en) * | 2015-06-22 | 2016-12-29 | Samsung Electronics Co., Ltd. | Three-dimensional user interface for head-mountable display |
US10416835B2 (en) | 2015-06-22 | 2019-09-17 | Samsung Electronics Co., Ltd. | Three-dimensional user interface for head-mountable display |
US20170228929A1 (en) * | 2015-09-01 | 2017-08-10 | Patrick Dengler | System and Method by which combining computer hardware device sensor readings and a camera, provides the best, unencumbered Augmented Reality experience that enables real world objects to be transferred into any digital space, with context, and with contextual relationships. |
WO2017126709A1 (en) * | 2016-01-18 | 2017-07-27 | 엘지전자 주식회사 | Mobile terminal and control method therefor |
US11308704B2 (en) * | 2016-01-18 | 2022-04-19 | Lg Electronics Inc. | Mobile terminal for controlling VR image and control method therefor |
US10133345B2 (en) * | 2016-03-22 | 2018-11-20 | Microsoft Technology Licensing, Llc | Virtual-reality navigation |
CN108780360A (en) * | 2016-03-22 | 2018-11-09 | 微软技术许可有限责任公司 | Virtual reality is navigated |
US10567730B2 (en) * | 2017-02-20 | 2020-02-18 | Seiko Epson Corporation | Display device and control method therefor |
US10634921B2 (en) | 2017-06-01 | 2020-04-28 | NewSight Reality, Inc. | See-through near eye optical display |
US10634912B2 (en) | 2017-06-01 | 2020-04-28 | NewSight Reality, Inc. | See-through near eye optical module |
US11119353B2 (en) | 2017-06-01 | 2021-09-14 | E-Vision Smart Optics, Inc. | Switchable micro-lens array for augmented reality and mixed reality |
US11852914B2 (en) | 2017-06-01 | 2023-12-26 | E-Vision Smart Optics, Inc. | Switchable micro-lens array for augmented reality and mixed reality |
US10466487B2 (en) | 2017-06-01 | 2019-11-05 | PogoTec, Inc. | Releasably attachable augmented reality system for eyewear |
US10884246B2 (en) | 2017-06-01 | 2021-01-05 | NewSight Reality, Inc. | Releasably attachable augmented reality system for eyewear |
US20190302484A1 (en) * | 2018-04-03 | 2019-10-03 | Boe Technology Group Co., Ltd. | Smart glasses and wearing instruction method for smart glasses |
JP2019207574A (en) * | 2018-05-29 | 2019-12-05 | 富士ゼロックス株式会社 | Information processing device, information processing system, and program |
US11175803B2 (en) | 2019-02-07 | 2021-11-16 | International Business Machines Corporation | Remote guidance for object observation |
US20220180603A1 (en) * | 2019-03-15 | 2022-06-09 | Witapp S.R.L. | System and computer implemented method for 3d processing of a tomography test |
CN114365027A (en) * | 2019-11-06 | 2022-04-15 | 海思智财控股有限公司 | System and method for displaying object with depth of field |
EP4055437A4 (en) * | 2019-11-06 | 2023-12-20 | HES IP Holdings, LLC | System and method for displaying an object with depths |
CN114365214A (en) * | 2020-08-14 | 2022-04-15 | 海思智财控股有限公司 | System and method for superimposing virtual image on real-time image |
US11953689B2 (en) | 2020-09-30 | 2024-04-09 | Hes Ip Holdings, Llc | Virtual image display system for virtual reality and augmented reality devices |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20150169070A1 (en) | Visual Display of Interactive, Gesture-Controlled, Three-Dimensional (3D) Models for Head-Mountable Displays (HMDs) | |
CN110199267B (en) | Miss-free cache structure for real-time image conversion with data compression | |
US9377869B2 (en) | Unlocking a head mountable device | |
US10114466B2 (en) | Methods and systems for hands-free browsing in a wearable computing device | |
US10591731B2 (en) | Ocular video stabilization | |
US9804682B2 (en) | Systems and methods for performing multi-touch operations on a head-mountable device | |
US9448687B1 (en) | Zoomable/translatable browser interface for a head mounted device | |
US9454288B2 (en) | One-dimensional to two-dimensional list navigation | |
US20150262424A1 (en) | Depth and Focus Discrimination for a Head-mountable device using a Light-Field Display System | |
US9686466B1 (en) | Systems and methods for environment content sharing | |
US20130021374A1 (en) | Manipulating And Displaying An Image On A Wearable Computing System | |
WO2016203792A1 (en) | Information processing device, information processing method, and program | |
US20170139567A1 (en) | Entering Unlock Sequences Using Head Movements | |
CN111488056B (en) | Manipulating virtual objects using tracked physical objects | |
CN112041788A (en) | Selecting text entry fields using eye gaze | |
KR20230079155A (en) | Eyewear comprising a virtual scene with 3D frames | |
US9265415B1 (en) | Input detection | |
CN110968248B (en) | Generating a 3D model of a fingertip for visual touch detection | |
US11961195B2 (en) | Method and device for sketch-based placement of virtual objects | |
US20230377249A1 (en) | Method and Device for Multi-Camera Hole Filling | |
WO2022005658A1 (en) | Visual interface for a computer system | |
US20230343027A1 (en) | Selecting Multiple Virtual Objects | |
WO2022103741A1 (en) | Method and device for processing user input for multiple devices | |
KR20240025593A (en) | Method and device for dynamically selecting an action modality for an object | |
CN115802143A (en) | Adjusting display of images based on device location |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HARP, ANDREW LOGAN;JAIN, SUMIT;KUFFNER, JAMES JOSEPH;REEL/FRAME:031794/0992Effective date: 20131216 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |