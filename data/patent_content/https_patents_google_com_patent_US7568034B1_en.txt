US7568034B1 - System and method for data distribution - Google Patents
System and method for data distribution Download PDFInfo
- Publication number
- US7568034B1 US7568034B1 US10/613,626 US61362603A US7568034B1 US 7568034 B1 US7568034 B1 US 7568034B1 US 61362603 A US61362603 A US 61362603A US 7568034 B1 US7568034 B1 US 7568034B1
- Authority
- US
- United States
- Prior art keywords
- slave
- data
- master
- slaves
- file
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/50—Allocation of resources, e.g. of the central processing unit [CPU]
- G06F9/5005—Allocation of resources, e.g. of the central processing unit [CPU] to service a request
- G06F9/5027—Allocation of resources, e.g. of the central processing unit [CPU] to service a request the resource being a machine, e.g. CPUs, Servers, Terminals
- G06F9/5033—Allocation of resources, e.g. of the central processing unit [CPU] to service a request the resource being a machine, e.g. CPUs, Servers, Terminals considering data affinity
Definitions
- the invention relates generally to data distribution. More particularly, the invention is directed to a system and method for distributing large amounts of data over a widely dispersed network.
- Data distribution otherwise known as data deployment, data logistics, or data replication, includes the placement and maintenance of replicated data at multiple data sites across a network.
- data distribution has been either point-to-point, i.e., communication from one location to another, or multipoint, i.e., communication from one location to many.
- point-to-point i.e., communication from one location to another
- multipoint i.e., communication from one location to many.
- data distribution has many drawbacks. For example, if multiple clients simultaneously request the same file from the server, the server may become overloaded and no longer be able to respond efficiently to normal requests. This is commonly known as denial of service.
- Clients and servers are often widely distributed from one another. Therefore, communication between the clients and server may consume valuable system resources, where system resources are the components that provide the network's inherent capabilities and contribute to its overall performance.
- System resources include routers, switches, dedicated digital circuits, bandwidth, memory, hard disk space, etc.
- system resources are of particular value in private networks, where such system resources are typically paid for by a single system operator.
- a method of distributing files over a network operates in a system having a master and a plurality of slaves, interconnected by a communications network.
- Each slave determines a current file length for each of a plurality of files, the current file length of each respective file representing an amount of the respective file currently stored at the slave.
- Each slave sends slave status information to the master, the slave status information including the current file length for each of the plurality of files.
- the master schedules copy operations based on the slave status information received from the slaves. Each scheduled copy operation is for copying a specified portion of a specified file, from a first specified slave to a second specified slave of the plurality of slaves.
- the master also stores bandwidth capability information indicating data transmission bandwidth capabilities for the resources required to transmit data between the slaves, and stores bandwidth usage information indicating amounts of data transmission bandwidth allocated to copy operations scheduled by the master.
- the stored bandwidth usage information indicates a total allocated bandwidth for each resource. While scheduling a copy operation, the master also allocates an amount of data transmission bandwidth with respect to each resource required by the copy operation, and updates the stored bandwidth usage information accordingly. The master only schedules copy operations that do not cause the total allocated bandwidth of any resource to exceed the bandwidth capability of that resource.
- some files are configured as a sequential set of blocks.
- a generator at one of the slaves generates blocks of a file after at least one block of the file has already been copied to another one of the slaves by the execution of a respective copy operation scheduled by the master.
- portions of a file are distributed among the slaves even before the generation of the file is completed. This overlaps data generations with data distribution, making efficient use of the system resources.
- copy operations are scheduled so as to make the invention address the drawbacks of the prior art by providing a system and method for data distribution over a widely distributed network while limiting scheduled file transmissions in accordance with available bandwidth resources.
- data is distributed virally from a source where it was first generated. For example, data generated on a first machine in a first rack is first distributed to the other machines in its rack, then to the machines in its data center, then to the nearest datacenter, etc, so as to utilize the least resources. Accordingly, valuable resources, such as long distance circuits, are not continually and inefficiently utilized.
- FIG. 1 is a block diagram of a data distribution network, according to an embodiment of the invention.
- FIG. 2A is a block diagram of a slave shown in FIG. 1 ;
- FIG. 2B is a block diagram of a the file set file shown in FIG. 2A ;
- FIG. 2C is a block diagram of the slave state shown in FIG. 2A ;
- FIG. 2D is a schematic of a data file and a checksum file, according to an embodiment of the invention.
- FIG. 3A is a block diagram of the master shown in FIG. 1 ;
- FIG. 3B is a block diagram of a system hierarchy table shown in FIG. 3A ;
- FIG. 3C is a block diagram of a system resources table shown in FIG. 3A
- FIG. 3D is a block diagram of a pending transaction table shown in FIG. 3A ;
- FIG. 3E is a block diagram of a State table shown in FIG. 3A ;
- FIG. 4 is a flow chart of a method for distributing data
- FIGS. 5A-5D are flow charts of different types of scheduling shown in FIG. 4 , according to different embodiments.
- FIG. 1 is a block diagram of a data distribution network 100 , according to an embodiment of the invention. It should be appreciated that the layout of this network 100 is merely exemplary and the network 100 may take on any other suitable layout or configuration.
- the network 100 is any suitable system for transmitting data between datacenters, and may include dedicated optical links or other dedicated communication channels, as well as supporting hardware such as modems, bridges, routers, switches, wireless antennas and towers, and the like.
- the network 100 preferably includes one or more wide area networks (WANs) as well as multiple local area networks (LANs).
- WANs wide area networks
- LANs local area networks
- the network 100 is preferably a private network, i.e., the network is designed and operated exclusively for a particular customer, in which the customer leases circuits and sometimes switching capacity for the customer's exclusive use. Alternately, a public network with a fixed, known topology may be used.
- the network 100 preferably includes multiple dispersed datacenters (DC 1 -DC 4 ). Some of the datacenters may be located, geographically, close to each other, and others may be located far from the other datacenters. Furthermore, one of the datacenters may be the primary source of new files to be distributed to the other datacenters, or alternately, the generation of new files may be shared by two or more of the datacenters.
- Each datacenter (DC 1 -DC 4 ) preferably includes multiple racks.
- datacenter one (DC 1 ) includes multiple racks (Rack 1 -N). Physically, the racks may include frames or cabinets into which the slaves and other components, such as switches, are mounted. Each rack preferably includes multiple slaves.
- the first rack (Rack 1 ) includes multiple slaves (Slave 1 - 16 ), the second rack (Rack 2 ) includes multiple slaves (Slave 17 - 31 , and the third rack (Rack 3 ) includes multiple slaves (Slave 32 M).
- the slaves can include any electronic or electromechanical unit of equipment, but in a preferred embodiment are computers. In other embodiments the slaves may include computer controlled devices, such as network attached storage devices.
- At least one of the racks also includes one or more master(s) 102 and at least one the racks contains one or more generators 104 .
- the generators generate the data that will ultimately be distributed throughout the network 100 , while the master(s) schedule and control the data distribution throughout the network 100 .
- some of the slaves may take on one or more roles, such as a generator 104 , master 102 , and/or slave 106 .
- a slave and a generator are combined in a single device 107 .
- many of the generators in a system may also function as slaves for purposes of the data distribution system. This dual functionality can be used to simplify the configuration and management of the system.
- a rack may include a shared storage or disk 108 that is shared by the one or more slaves and/or generators.
- a generator stores the files it generates in a disk 108 that is shared with a slave. The slave has full access to the files written to the disk by the generator, and thus acts as a source of those files to the system.
- the generators 104 , 107 produce new files and new versions of files in a way that enables distribution of portions of the files before generation of the files has been completed. This is important when the generation of a file may take hours, because the process of distributing copies of the file can be largely overlapped with the generation of the file.
- a generator when a generator is producing a file whose length is greater than a predefined block size (e.g., 16 MBytes), the generator produces a checksum associated with each successive block of the file, where each block (except possibly the last block) of the file has the predefined block size.
- the slave associated with the generator 104 / 107 publishes to the master 102 status information indicating the new size of the file being generated. The master uses that size information to identify other slaves which need copies of the newly generated blocks of the file. As a result, in many instances copy operations for copying the blocks of the file are scheduled and executed before the generator generates the last block of the file.
- the generator also produces a final flag or other indicator value when the last block of a file has been generated.
- the final flag (or equivalent indicator value) is stored in combination with the checksum for the last block of the file.
- all masters, generators, and slaves within each rack are interconnected to one another through a rack switch 112 .
- the Slaves 1 - 16 in Rack 1 are interconnected through the Rack Switch 1
- the master and the Slaves 17 - 31 are interconnected through the Rack Switch 2 .
- each slave is coupled to its respective rack switch 112 through separate and distinct uplinks and downlinks, depicted in FIG. 1 by the up and down arrows coupling each slave to its respective rack switch.
- these uplinks and downlinks communicate unidirectionally.
- all racks within each datacenter are also preferably interconnected via a datacenter switch 114 .
- the Racks 1 , 2 , and 3 are interconnected through the Datacenter Switch 1 .
- each rack is coupled to its respective datacenter switch through separate and distinct uplinks and downlinks, depicted in FIG. 1 by the up and down arrows coupling each rack to its respective datacenter switch.
- these uplinks and downlinks communicate unidirectionally.
- the rack switches 112 and datacenter switches 114 are crosspoint switches, also known as a crossbar or N ⁇ N switches, which are switching devices that provide for a fixed number of inputs and outputs, where the inputs and outputs communicate at full speed with one another.
- the bandwidth of each of the connections provided by the rack and datacenter switches is significantly higher than that along the trunks, as described below. Furthermore, the bandwidth of each connection provided by the rack switches and datacenter switches is greater than the input and output bandwidth of the slave devices. Accordingly, the rack and datacenter switches are preferably treated as allowing infinite bandwidth connections between slaves directly connected to them, because these switches are never the locus of a bandwidth limitation when scheduling data transfers between slave devices.
- each datacenter (DC 1 -DC 4 ) is preferably coupled to another datacenter along dedicated trunks (T 2 S, T 2 N, T 1 , T 3 W, T 3 E, and T 4 ).
- these trunks communicate substantially unidirectionally.
- each trunk line may be asymmetric, providing a much larger bandwidth in one direction (the primary direction) than the other (the reverse direction).
- the reverse direction of each trunk is used primarily for TCP/IP control messages and the like, and can also be used for sending short messages such as slave status messages from slaves to the master.
- the uplinks, downlinks, and trunks may include any suitable links, such as copper twisted pairs, coaxial cables, optical fibers, wireless connections, and the like.
- each of the slave, master and generator devices is also connected (via i/o ports 214 , FIG. 2A ) to at least one additional communication network, not shown in FIG. 1 .
- This network may be a local area network, wide area network or a combination thereof.
- the slave devices communicate status information to the master 102 through this additional network, while file transfers are conveyed through the switched network shown in FIG. 1 .
- the additional network is only for sending status information to the master 102 from slaves in data centers that do not have a trunk line connection back to the data center in which the master 102 resides.
- a goal of the system 100 shown in FIG. 1 is for the master to schedule data copy operations among the slaves so that newly generated files and newly generated portions of files are quickly and efficiently copied to all the slaves requiring copies of those files and file portions.
- the master takes into account bandwidth limitations of the various resources in the system when making scheduling decisions, so as to avoid overloading any system resource.
- File copy operations (herein also called file transfers) are only scheduled over resources having sufficient bandwidth to enable the file copy operation to be performed at full speed, where full speed may be defined as the maximum speed compatible with the operation of the sending and receiving slaves. Because long distance trunk lines are typically the most limited resources, the master will typically transfer each file or file portion over a trunk line just once.
- the master 102 selects file copy operations to schedule using one or more cost functions to determine the best file copy operation to schedule for each slave that does not have a complete set of files.
- the master 102 is configured to apply a predefined cost function to a plurality of potential copy operations to produce a respective score for each potential copy operation. The master then selects and schedules the copy operation having a best score.
- file copy operations are selected using a multiple factor cost function.
- the factors in the cost function include: 1) the hierarchical distance between the source and target slaves (e.g., the number of rack and datacenter boundaries that the copy operation would have to cross); 2) the amount or percentage of a file which is needed by the requesting slave (e.g., a function of the file size in the requesting slave and the file size of the slave having the largest amount of the data for the file); 3) a factor that favors slaves (as the source of a file copy operation) that have the least amount of data (e.g., a function of the total of the file sizes reported by each slave that is a potential source of a next block of a file); 4) a factor that favors slaves having the fewest outstanding copy operations (e.g., a function of the number of copy operations listed in the pending transmission table 384 for each slave that is a potential source of a next block of a file); and 5) a factor favoring copy operations using the fewest trunk lines
- the master uses two cost functions to select copy operations to schedule.
- One of the cost functions is used for global scheduling, discussed below, and the other cost function is used for local scheduling, also discussed below.
- FIG. 2A is a block diagram of one of the slaves shown in FIG. 1 .
- Each slave preferably includes: at least one data processor or central processing unit (CPU) 204 ; a memory 218 ; communications circuitry 210 for communicating with other devices in the network 100 ( FIG. 1 ) via the rack and datacenter switches; input and output (I/O) ports 214 coupled to the communication circuitry 210 , to a respective rack switch, and to a backplane or shared bus (not shown); a power source 202 configured to be coupled to a source of power; and at least one bus 212 that interconnects these components.
- Each slave may optionally include one or more user interface devices, such as a monitor 208 and a keyboard/mouse 206 .
- Memory 218 preferably includes high-speed random access memory and may include non-volatile memory, such as one or more magnetic disk storage devices. In some implementations, memory 218 may include mass storage that is remotely located from the central processing unit(s) 204 , such as the shared storage 108 shown in FIG. 1 . Memory 218 preferably stores an operating system 220 , such as Linux, Unix or Windows (a trademark of Microsoft), that includes procedures for handling basic system services and for performing hardware dependent tasks. Memory 218 , or alternatively one or more storage devices (e.g. one or more nonvolatile storage devices) within memory 218 , includes a computer readable storage medium. Memory 218 , or computer readable storage medium, also preferably stores communications procedures 222 used for communicating with other devices on the network 100 ( FIG.
- the communication procedures 222 are used for transmitting the slave state to the master, receiving instructions from the master, requesting and/or receiving data from other slaves or the generator, as described below in relation to FIG. 4 .
- Memory 218 also preferably includes: a file set file (FileSet) 224 ; the slave state 226 ; data files 228 ; checksum files 230 ; verification procedures 240 ; failure detection procedures 242 ; state determination and reporting procedures 244 ; and a cache 246 for temporarily storing data.
- the file set file (FileSet) 224 contains a list of files required by the particular slave. In some embodiments, there are multiple distinct sets of slaves, each of which has a different set of files. For instance, if the system 100 ( FIG.
- 1 is a search engine system, there may be anywhere from five to one hundred (or perhaps a few hundred) distinct types or classes of slaves, each of which performs a distinct function or set of functions in the process of responding to search engine queries.
- the index of documents to be searched may be divided over a small or large number of slaves, and therefore each of these slaves will have a distinct FileSet.
- one or more of the datacenters of the system may have more than one copy of each type of slave, thereby providing multiple replicas of the entire search engine so as to be able to service large numbers of search requests.
- the system has slaves of types 1 through N, and a datacenter has M replicas of the search engine, where M is an integer greater than 1, there will be M slaves of each type in the data center.
- These slaves will preferably be spread over multiple racks so that no single rack failure will prevent the system from being able to respond to search queries.
- FIG. 2B is a block diagram of an example of a file set file 224 .
- the required files are listed by file identifier (FileID) and file version number (Version).
- FileSet 224 includes a file identifier and a file version number for one of the files that the slave needs in order to have a full and current set of files.
- the FileSet 224 does not list itself as one of the required files, and the slave is originally deployed without a FileSet 224 stored in its memory. Instead, the Slave State of the slave is initialized to include an entry for the FileSet required by the slave, as will be explained in more detail below.
- the FileSet file does list itself as one of the required files.
- the slave is originally deployed without a FileSet 224 stored in its memory.
- the master instructs the slave to copy the FileSet 224 required for that slave, and the slave obtains and stores the FileSet
- the status information reported by the slave enables the master to determine whether that slave has the latest version of that FileSet and schedule the file set file to be updated if necessary, as explained below in relation to FIG. 4 .
- the slave state 226 contains a list of all the files or portions of the files that have already been distributed to and stored by the slave as data files 228 .
- FIG. 2C is a block diagram of the slave state 226 shown in FIG. 2A .
- the slave state 226 lists all the files required by the slave, as well as the version of each such file currently stored by the slave, as well as the current size of each such file.
- Each file is preferably listed by file identifier (FileID), version number (Version), and file size (File Size). Except for the FileSet file, the file identifier and version number are copied from the FileSet.
- the data files 228 distributed include very large files. To make the transfer of the large files more manageable, and to avoid having to retransmit an entire file when a failure occurs midway through a file transfer, the data files are preferably segmented into blocks, as shown in FIG. 2D .
- each block has a size of 16 MB (Megabytes) or less. More specifically, large files are both generated and copied in blocks of 16 MB, until the last block of the file has been received, which has whatever size (of 16 MB or less) required to complete the file copy process. It should, however, be appreciated that the block sizes may be any suitable size and/or of different sizes, as determined in accordance with predefined criteria.
- each slave preferably receives the files it requires as a sequence of blocks, until such time that each data file has been completely received.
- the slave may receive one or more blocks of a file before the generator of the file has generated the last block of the file.
- the slave state of the slave indicates the amount of data for each file that has been successfully received and stored by the slave.
- each block of the data file may be received from a different source (e.g., a different slave) than the other blocks of the file. While in practice many blocks of a file may be copied in sequence from one slave to another, as network loading conditions change, the source of the blocks being copied to a slave may be changed by the master. In fact, the resource allocation and scheduling procedures 394 ( FIG.
- the preferred schedule preferably contains one or more instructions for requesting and/or obtaining data, where the transmissions to request and obtain the data are directed so as to consume available system resources and avoid attempting to consume system resources that are already fully committed.
- the preferred schedule will instruct the receiving slave to obtain a block from a sending slave, which has the required block, that is physically closest to the receiving slave, because that will typically use the least system resources.
- Another constraint is that the sending slave have sufficient available bandwidth on its outgoing link to service the data transfer. This scheduling is described in further detail below.
- the verification procedures 240 are used to verify the integrity of the data received by, or stored on, the slaves.
- the verification procedures 228 include a procedure for comparing each received block with a received checksum file 230 .
- each data file 228 has a corresponding checksum file 230 , as shown in FIG. 2D .
- the checksum file 230 includes a checksum value for each block of the data file.
- each checksum is a cumulative checksum that is a function of (1) the data in the data file block corresponding to the checksum, and (2) the cumulative checksum for all the previous blocks of the file, if any.
- the checksum value may be a cyclic redundancy check (CRC) checksum, generated using a predefined polynomial function (e.g., the well-known 32-bit Ethernet CRC polynomial) or any other suitable checksum function.
- CRC cyclic redundancy check
- the receiving slave device computes a checksum for the received block, using a predefined checksum function that is initialized with the cumulative checksum for the prior blocks of the file (in this case Block 1 ). If the computed checksum matches the checksum in the checksum file 230 for Block 2 , then the transmission of Block 2 was successful and the receiving slave can rely upon that data. The slave reports the successful block transfer to the master. If the computed checksum does not match the checksum in the checksum file 230 for Block 2 , then the transmission of Block 2 failed, and the receiving slave will report the failure to the master.
- a predefined checksum function that is initialized with the cumulative checksum for the prior blocks of the file (in this case Block 1 ). If the computed checksum matches the checksum in the checksum file 230 for Block 2 , then the transmission of Block 2 was successful and the receiving slave can rely upon that data. The slave reports the successful block transfer to the master. If the computed checksum does not match the checksum in the checksum file 230 for Block 2 , then the
- the corresponding checksum value must also be conveyed to the slave.
- the checksum is conveyed in a header portion of the transmission of the data file block.
- the slave is configured to retrieve the checksum from the header, compare the received checksum value with the one computed for the received block, and if the comparison indicates a successful data transfer, store the checksum in the respective checksum file.
- a new copy of the checksum file 230 is transmitted by the sending slave to the receiving slave each time a block is transmitted to the receiving slave.
- the new copy of the checksum file includes the checksum corresponding to the block being transmitted to the receiving slave. Since the checksum file is generally very small (e.g., less than 4 KB, and usually much smaller), even for very large data files, transmitting the checksum file does not significantly affect the performance of the system.
- the verification procedures 240 ( FIG. 2A ), described below, only calculate the integrity of the block received with respect to the checksum of the previous block received, without having to recalculate the integrity of all the blocks received for a particular data file. It should, however, be appreciated that any suitable checksum(s) and/or verification procedures may be used to verify the integrity of the blocks received.
- the verification procedures 240 may also be used to verify the integrity of a block when read from disk storage to ensure that the block has been successfully read.
- blocks are verified both by the sending slave when it reads the block from disk storage, or other memory device, and by the receiving slave when receiving the block.
- the failure detection procedures 242 are generally used to detect failure of a data transfer.
- these failure detection procedures 242 are preferably configured to assess the cause and/or source of the failure. For example, where a data transfer between slaves fails, the failure detection procedures 242 attempt to identify the source of the failure, such as the slave, switch, or trunk responsible for the failure.
- the state determination and reporting procedures 244 are used to determine the slave state 226 , and to then report the slave state to the master. These procedures 244 may include one or more incremental state update procedures, for updating the slave state 226 as blocks are received, verified, and stored to disk. These procedures 244 may also include one or more procedures for comparing the data files 228 received and stored, with the data required, i.e., from the FileSet 224 . For instance, when the FileSet 224 indicates a different file version than what is stored in a slave, the state determination procedures 244 update the slave state 226 to indicate the new file version and to indicate that zero bytes of that file are stored by the slave. When the slave state has been updated, it is sent by the procedures 244 to the master. The slave state is also sent to the master each time the slave reports to the master the completion of a file block transmission.
- FIG. 3A is a block diagram of the master 102 shown in FIG. 1 .
- the master preferably includes: at least one data processor or central processing unit (CPU) 360 ; a memory 362 ; communications circuitry 364 for communicating with the slaves through one or more local area networks, a wide area network (e.g., the Internet), or a combination thereof; input and output (I/O) ports 366 coupled to the communication circuitry 364 ; a power source 368 configured to be coupled to a source of power; and at least one bus 374 that interconnects these components.
- the master 102 communicates with the slaves through a rack switch (see FIG. 1 ). In other embodiments, it could communicate with the slaves through another network.
- Each master may optionally include one or more user interface devices, such as a monitor 370 and keyboard/mouse 372 .
- user interface devices such as a monitor 370 and keyboard/mouse 372 .
- Memory 362 preferably includes an operating system (OS) 376 , such as Linux, Unix or Windows (a trademark of Microsoft) that includes procedures for handling basic system services and for performing hardware dependent tasks.
- OS operating system
- Memory 362 or alternatively one or more storage devices (e.g. one or more nonvolatile storage devices) within memory 362 , includes a computer readable storage medium.
- Memory 362 or computer readable storage medium, also preferably includes communications procedures 378 that are generally used for communicating with the network 100 ( FIG. 1 ) and slaves. In particular, the communication procedures 378 are used for receiving a slave's state and for transmitting instructions to slaves, as described below in relation to FIG. 4 .
- Memory 362 also preferably includes a system hierarchy table 380 , a system resources table 382 , a pending transmissions table 384 , a state table 390 , failure determination procedures 392 , resource allocation and scheduling procedures 394 , and a cache 396 .
- the system hierarchy table 380 is used by the master to determine the proximity of slaves to one another.
- the system resources table 382 is used by the master to determine the bandwidth resources for each communication path in the switched network, and the resources currently in use or reserved for use.
- the pending transmissions table 384 is used to keep track of all current or pending transmissions.
- the state table 390 is used by the master to determine which system resources have failed, as well as which files or blocks of files have been received by each slave and while files or file blocks are still needed by each slave.
- the failure determination procedures 392 are used by the master for determining whether a resource has failed.
- the resource allocation and scheduling procedures 394 are used to schedule data transfers between slaves, as described in further detail below in relation to FIGS. 4 and 5 A- 5 D.
- the cache 396 is used for temporarily storing data.
- the system hierarchy table 380 contains a slave level graph 386 and a rack level graph 388 .
- the slave level graph 386 lists all slaves in the network 100 ( FIG. 1 ) by slave identifier (Slave ID).
- the slave level graph also lists a corresponding slave location (Slave Location) for each slave. For example, Slave 134 is located in rack R 35 . From another viewpoint, the slave level graph identifies the parent of each slave in the system hierarchy, where each slave's parent is an identified rack.
- the rack level graph 388 lists all racks in the network 100 ( FIG. 1 ) by rack identifier (Rack ID).
- the rack level graph also lists a corresponding rack location (Rack Location) 393 for each rack.
- rack 35 (Rack 35 ) is located in datacenter 2 (DC 2 ).
- This system hierarchy table 380 allows the master to determine the relative locations and/or distances between slaves and/or racks, and thereby increase distribution efficiency by transferring data between slaves that are physically close to one another.
- FIG. 3C is a block diagram of the system resources table 382 shown in FIG. 3A .
- the system resources table 382 includes multiple graphs, including a slave level bandwidth graph 321 , a rack level bandwidth graph 325 , a datacenter level bandwidth graph 327 , and a trunk level bandwidth graph 328 . Each of these graphs lists the maximum available bandwidth resources and the bandwidth currently being used, or reserved for use, for each slave, rack, datacenter, and trunk.
- the master is configured to include a bandwidth reservation procedure 395 for receiving bandwidth reservations and bandwidth relinquishments from applications that make use of the same communication paths as the communication paths used for conveying copies of file blocks (i.e., for file replication). For instance, if an application running somewhere in the system will require up to 10 units (e.g., MB/Sec) of bandwidth for several minutes, over a specified path through the network, it sends a corresponding bandwidth reservation to the reservation procedure 395 of the master.
- the master stores the bandwidth reservation information in its system resources table 382 and its pending transmissions table 384 , as described below.
- the application may optionally specify a termination time at which the reservation is to expire.
- a termination time at which the reservation is to expire.
- the application no longer requires the use of the network it sends a corresponding bandwidth relinquishment to the master, which then releases the corresponding resources by revising the entries in its system resources table 382 and pending transmission table 384 .
- the tables are updated at or after the specified termination time.
- these embodiments reduce the bandwidth available for file replication operations only when other applications notify the master that they require the use of a portion of the bandwidth of specified communication paths through the network.
- the master is not configured to receive bandwidth reservations and bandwidth relinquishments from such other applications, and therefore the “available bandwidth” values stored in the system resources table 382 (as described below) are adjusted values less than the actual full bandwidths of the various communication paths in the network so as to take into account the fact that other applications share the use of those communication paths.
- the slave level bandwidth graph 321 lists the bandwidth resources for each slave.
- the record for each slave identified by its slave identifier (Slave ID), includes fields specifying the maximum available inbound bandwidth (Inbound BW) and maximum available outbound bandwidth (Outbound BW) for that slave.
- Slave 160 has a maximum inbound bandwidth (Inbound BW) of 100 MB/sec and a maximum outbound bandwidth (Outbound BW) of 100 MB/sec.
- the record for each slave also includes fields specifying the inbound bandwidth currently being used or reserved for use (In BW in use), and the outbound bandwidth currently being used or reserved for use (Out BW in use), is also listed for each slave.
- the trunk level bandwidth graph 328 lists, for each respective trunk line, a trunk identifier (Trunk ID), the maximum available bandwidth (BW); the bandwidth currently in use or reserved for use (BW in use); the starting point of the trunk (Start); the end point of the trunk (End); and the latency along that trunk (Latency).
- the start and end points identify the direction of data travel over each respective trunk, as each trunk is preferably configured for unidirectional communication.
- the latency is the delay between the time that a packet is transmitted and the time that it is received.
- the maximum available bandwidth is 30 MBs, 20 MBs are in use, or reserved for use, and the latency along the trunk is 35 ms.
- the latency is used to calculate the optimal or preferred window size, as described below.
- FIG. 3D is a block diagram of the pending transmissions table 384 shown in FIG. 3A .
- the pending transmissions table 384 lists all current data distribution transmissions, as well as any bandwidth reservations, within the network 100 ( FIG. 1 ).
- Each entry in the table includes fields for identifying: the slave from which the data is being copied (Copy From); the slave to which the data is being copied (Copy To); the amount of bandwidth used by the transmission or reservation (BW used); a termination time by which the transmission is expected to have completed (End Time); and a reserve flag (Reserve Flag) indicating whether the entry represents a bandwidth reservation (as opposed to a file replication operation).
- the End Time (or termination time) for each transmission is preferably significantly later than the expected completion time (e.g., a transmission expected to take 2 seconds may be assigned an End Time that is 6 to 10 seconds after the start of the transmission), and is used primarily for cleaning up failed transmissions.
- An exemplary entry in FIG. 3D shows that Slave 34 is currently copying data to slave 35 ; is using 10 MB/sec of bandwidth; and is expected to complete the operation no later than 8:22.
- Another exemplary entry in FIG. 3D shows that an application has reserved 10 MB/sec of the bandwidth in the path from Slave 89 to Slave 191 until 9:25.
- the End Time for a transmission is preferably the estimated time that the transmission will terminate. If a transmission completes prior to the End Time specified in the pending transmission table 384 , which will generally be the case for a successful transmission, the master will delete the transmission entry from the pending transmission table 384 when it receives a message from the receiving slave indicating that the file block transmission was successfully completed.
- the master will delete the transmission entry from the tending transmission table at or shortly after the specified transmission expiry time.
- the master will delete the transmission entry from the tending transmission table at or shortly after the specified transmission expiry time.
- the master deletes a transmission entry from the pending transmissions table 384 , the corresponding bandwidth use entry or entries in the system resources table are decreased by the amount of bandwidth denoted in the transmission table entry.
- the “bandwidth in use” amount of each resource having a non-zero “bandwidth in use” is automatically decreased at the end of each period (e.g., each successive 2 second period) by either a predefined amount, such as the greater of 10 Bytes/sec and fifty percent of the “bandwidth in use”, or by a fixed amount, or by a function of the available bandwidth (e.g., 10% of the available bandwidth).
- a predefined amount such as the greater of 10 Bytes/sec and fifty percent of the “bandwidth in use”
- a fixed amount or by a function of the available bandwidth (e.g., 10% of the available bandwidth).
- no adjustment is made to the bandwidth in use values when each file block transmission completes. This alternate implementation is particularly useful when most copy operations are expected to take a relatively fixed, short amount of time.
- a single entry in the pending transmissions table 384 can specify bandwidth used over several communication channels.
- a transmission table entry for a transmission between two slaves (slave A and slave B) on different racks (rack 1 and rack 2 ) of a datacenter will correspond to four bandwidth usage entries in the system resources table: slave A (Out BW in use), rack 1 (Out BW in use), rack 2 (In BW in use) and slave B (In BW in use).
- Each of these four bandwidth usage entries in the system resources table will be increased by the amount of bandwidth specified in the transmission table entry, and this same amount will be subtracted from those bandwidth usage entries when the transmission terminates.
- FIG. 3E is a block diagram of the state table 390 shown in FIG. 3A .
- the state table lists all resources in the network 100 ( FIG. 1 ) by resource identifier (Resource ID).
- Resources include all trunks (T 1 -TJ), datacenters (DC 1 -DCK), racks (Rack 123 -Rack N), and slaves (Slave 1 -Slave M).
- Each resource has an associated state (State) that indicates whether that resource is functioning or not. For example, trunk 1 (T 1 ) is functioning, while the second slave (Slave 2 ) is not, where a one (1) indicates that the resource is functioning and a zero (0) indicates that the resource is not functioning.
- each resource has an associated transmission attempts count (Attempts), as well as a failure count (Failure Count) that can be incremented or decremented by the failure determination procedures 392 ( FIG. 3A ).
- the failure determination procedures 392 ( FIG. 3A ) are used to determine if a particular resource is likely to have failed. Each time a copy operation is attempted, the corresponding Attempts Count for each device or resource involved in the data transfer is incremented. Each time a copy operation fails, the Failure Count is incremented for each resource that is potentially responsible for the failure.
- the master receives an indication from the receiving slave indicating which resource(s) are believed to be responsible for the failure.
- the failure determination procedures 392 ( FIG.
- the slave requesting a copy of a file block may determine that the failure was due to insufficient disk space in the requesting slave, or that it was unable to read the received file block from its own storage, in which case the slave's message will indicate that the failure occurred in the requesting slave.
- the presumption will be that the failure occurred elsewhere, in which case the Failure Count for all other resources involved in the copy operation will be incremented.
- a predetermined upper threshold or limit that resource is identified as a failed or bad device. For example, when the failure count of the second slave (Slave 2 ) reached a predefined failure threshold (e.g., 50), the state (State) of the second slave was changed from a one (1) to a zero (0), thereby indicating that the second slave (Slave 2 ) has failed.
- the failure count (Failure Count) and the Attempts count (Attempts) preferably have a half-life, such that the failure count and attempts count degrade over time. In some embodiments, every attempts count and every failure count value in the state table is decreased by a predefined amount at scheduled times.
- each non-zero Attempts Count and each non-zero Failure Count is decreased by 10 percent, rounded down to the next closest integer.
- each non-zero Failure Count is decreased by a fixed amount (e.g., 1), and each non-zero Attempts Count is decreased by another fixed amount (e.g., 5). If a device is reported to have suffered several failures, its Failure Count is increased with each report. If the device then recovers or is repaired, and no additional failures are reported for the device, the system will periodically decrease the Failure Count for the device.
- the state of the device is changed to “functioning” (denoted by a “1” in the exemplary table of FIG. 3E ) and the master resumes scheduling file transfers in which the device is a participant.
- the predefined failure and recovery thresholds are preferably defined so as to provide a period of time for failed devices to perform a reset or recovery operation, or for the device to be serviced or replaced by a technician.
- the predefined failure and recovery thresholds are a function of both the Failure Count and the Attempts Count for each resource. By factoring in the Attempts Count as well as the Failure Count, resources with much larger numbers of successful copy operations than failed operations are kept in service.
- the status table also records the last time the master received a communication from the slave (Last Contact). In some embodiments, the termination of whether the state of a slave device is alive (“functioning”) or dead (“not functioning”) takes into account the difference between the current system time and the Last Contact time denoted for the slave.
- the slaves are configured to periodically send a status message to the master (e.g., twice per minute), in addition to sending a message each time a copy operation completes or fails. If the difference between the current system time and the Last Contact time is substantially longer than the amount of time between periodic slave status reports, then this is further evidence that a slave device's state should be denoted as “not functioning”.
- each entry for a slave listed in the state table 390 also includes state pointers (State Pointers).
- State Pointers Each state pointer points to a copy of the slave state sent by the respective slave.
- the copy of the slave state includes a list of all the files required by the respective slave, the version of each such required file, and the current size of that file in the slave device.
- Each slave reports is slave state to the master both periodically and whenever a transmission to the slave is completed.
- the reported slave state is stored in the master's main memory at a location denoted by the corresponding State Pointer in the state table 392 .
- the state pointer for slave 3 (Slave 3 ) points to the master's copy of the slave state for Slave 3 -R 2 -DC 1 .
- the state table 392 in the master provides the master with a record of all files blocks that have already received by each slave.
- the state table includes a pointer 600 for each slave that points to a list 602 of slaves that store, or are configured to store, the same set of files. If there are N different types of slaves in the system, each configured to store a different set of files, then there will be N different lists 602 of slaves.
- the state information for each file listed in the slave state, includes an Attempts count, a Failure Count and a State, each of which serves the same purpose as for other resources in the system. The meaning of these fields is described above.
- the state information for each listed file for a particular slave may optionally include a “Want Newest” flag, for indicating that the slave should be sent the latest version of the marked file, even if the slave's state did not indicate that it has requested that file.
- the master checks the list 602 of slaves configured to store the same files to determine if any store a newer version of the file than the version stored by the slave in question. When a newer version is found, the master attempts to schedule a copy operation for sending the slave the first block of the newer version of that file.
- the FileSet file is always marked with a “Want Newest” flag, thereby ensuring that each slave receives the current version of its FileSet file.
- the new version of the FileSet file may list new versions of other files that the slave needs to copy.
- FIG. 4 is a flow chart of a data distribution method 400 , according to an embodiment of the invention.
- the method 400 describes a method whereby a single master (Master) ( FIG. 1 ) controls the distribution of files between a first slave (1st Slave) and a second slave (2nd Slave).
- Master a master
- a master may control the distribution of any type of data between any number of slaves.
- the first slave and the second slave may be any of the slaves in the network 100 ( FIG. 1 ).
- a generator 104 ( FIG. 1 ) or a generator/slave 106 ( FIG. 1 ) initially generates a block of a file, at step 402 .
- the generator or generator/slave generates a 16 MB block of a file.
- the generator preferably also generates a checksum of the block at step 404 .
- the generator may also encrypt the block using digital signatures or the like.
- the block and checksum are then stored at step 408 .
- the block and checksum are stored by the generator 104 ( FIG. 1 ) in the shared storage 108 ( FIG. 1 ), which can be accessed by one or more of the slaves in the generator's rack (Rack N).
- the generator/slave stores the block and checksum in a common internal memory.
- the block and checksum are preferably stored in a data file 228 and a checksum file 230 in the memory of a slave device ( FIG. 2A ), herein identified as the second slave (2nd Slave), which has direct access to the memory in which the block is stored.
- the state determination procedures 244 ( FIG. 2A ) on the second slave then updates the second slave's slave state (Slave State 226 ) ( FIG. 2A ) at step 410 .
- the slave state is transmitted to the master either as soon as a new block is added, or periodically, or both, at step 416 .
- Each slave stores a file set file (FileSet) 224 ( FIGS. 2A and 2B ), containing a list of files that it must obtain and store. This file is periodically replaced with new versions as the versions of the files listed in the FileSet are updated.
- the first slave receives either an initial FileSet or replacement FileSet.
- the process for receiving the FileSet file is actually the same as for any other data file, and thus follows the methodology shown in FIG. 4 .
- step 418 is a conceptual representation of the process by which the first slave receives the current version of its FileSet.
- the state determination procedures 242 ( FIG. 2A ) on the first slave determine the slave state 226 ( FIGS. 2A and 2C ) of the first slave at step 420 . To do this, the state determination procedures 242 ( FIG. 2A ) determine what blocks have already been received for each file listed in the FileSet. The information in the slave state is described above.
- the slave state of the first slave is transmitted to the master at step 422 .
- the slave transmits the slave state periodically, as well as each time a file block transfer completes, whether successfully or in failure.
- the slave state is received by the master at step 424 .
- the resource allocation and scheduling procedures 394 ( FIG. 3A ) subsequently schedule a data transfer of the aforementioned block from the second slave block to the first slave at step 430 , in accordance with a predefined bandwidth resource allocation method. This scheduling is described in further detail below in relation to FIGS. 5A-5D .
- the master transmits instructions to the first slave, at step 432 , instructing the first slave (1st Slave) to obtain the block from the second slave (2nd Slave).
- the first slave requests the block from the second slave in accordance with the instructions at step 436 .
- the second slave receives the request for the block at step 438 , and locates the block, and its associated checksum.
- the second slave verifies the requested block using its verification procedures, at step 439 .
- the failure detection procedures 242 ( FIG. 2A ) on the second slave preferably determine whether the block has been successfully read from storage, at step 440 . If the requested block is verified, the second slave transmits the request block and its associated checksum to the first slave at step 441 .
- the block and its checksum are received by the first slave at step 442 .
- the verification procedures 240 ( FIG. 2A ) then preferably verify the integrity of the received block using the received checksum at step 444 . As described above, the verification procedures preferably use a cumulative checksum.
- the failure detection procedures 242 ( FIG. 2A ) then determine, at step 446 , whether the block was received and whether the block was verified. In some embodiments, if a failure occurred, the failure detection procedures 242 ( FIG. 2A ) determine which resource was most likely to be responsible for the failure. For instance, if the block was not received, the failure detection procedures may specify that an unknown resource other than the receiving slave is likely to be responsible for the failure.
- the block is then stored in the first slave's memory in the appropriate data file, and the checksum for the block is added to the corresponding checksum file (step 448 ). If required, the block may be verified at any time thereafter by the verification procedures at step 450 .
- the slave state of the first slave is then updated by the state determination procedures 244 ( FIG. 2A ) at step 454 . In particular, if the block transfer was successful, the size of the data file for which the block was received is updated. If the block transfer failed, the slave state must not be updated, because none of the files denoted in the slave state have increased in size. The slave state and any failure data is subsequently transmitted to the master at step 456 .
- the slave state and/or any failure data is received by the master at step 424 . If a failure occurred, as evidenced from the failure data, the failure determination procedures 392 ( FIG. 3A ) on the master increment the failure count (Failure Count) ( FIG. 3E ) for the resource blamed with the failure at step 426 .
- the master increments the failure count of every resource involved in the block transfer, since the source of the failure is unknown.
- the failure count for the blamed resource reaches a predetermined limit, that resource is identified as failed or bad.
- the failure count decreases over time and the resource may again be determined to be good and allowed to re-enter service. Steps 420 - 432 are repeated until all the required blocks have been received. However, the identity of the first and second slaves may differ with each repetition of this process.
- the master instructions specified slaves to send data (i.e., file blocks) to other specified slaves, instead of instructing specified slaves to copy file blocks from other specified slaves.
- data i.e., file blocks
- the use of a push methodology is substantially equivalent to the use of the pull methodology (for copying file blocks) described above.
- a slave receives instructions from the master indicating a specified file block to transmit to a specified target slave.
- the target slave upon receiving the transmission, verifies the received file block, stores the file block if verified, and sends a confirmation or error message, depending on the outcome of the verification, to either the master, the sending slave, or both.
- FIGS. 5A-5D are flow charts of different types of scheduling that can be used to implement the schedule step 430 of FIG. 4 .
- FIG. 5A shows local scheduling 500 .
- the resource allocation and scheduling procedures 394 ( FIG. 3A ) in the master's memory, determine: the overall system hierarchy from the system hierarchy table 380 (FIGS. 3 A and 3 B); the overall system resources from the system resources table 382 ( FIGS. 3A and 3C ) for all resources in the network; all pending transmissions in the network from the pending transmissions table 384 ( FIGS. 3A and 3D ); and the overall states of all resources in the network from the state table 390 ( FIGS.
- the resource allocation and scheduling procedures 394 ( FIG. 3A ) then determine a preferred schedule for transferring a block to the first slave, at step 512 .
- the first slave may require file 0001v1 block 5 .
- the scheduling procedures examine the states of all the slaves configured to store the specified file (as identified by the list 602 of such slave, FIG. 3E ) to determine a set of candidate slaves, to assess what resources would be required to transmit the required block from each of the candidate slaves to the first slave, and also assesses whether those resources are available for allocation. For each candidate slave where the required resources for a file copy operation are available, a cost function is evaluated to produce a score. The copy operation may then be scheduled from the candidate slave having the best score to the first slave.
- the cost function evaluation is applied, with respect to a requesting slave, to every file listed in the requesting slave's status where at least one other slave has a longer copy of the file (i.e., has at least one more block of the file than the requesting slave).
- the master identifies every file in the slave's status that is a candidate for a copy operation because there is another slave with more data for that file.
- the master identifies a set of candidate slaves, identifies the required resources that would be needed to make a copy of each such slave, checks whether the required resources are available, and if so, calculates a cost function score.
- the master selects the copy operation, for copying a block of a file F from a slave S, having the best score. If the master is using local scheduling, the master sends the requesting slave instructions to execute the selected copy operation.
- the scheduling procedures may cut short these determinations if there is at least one candidate slave in the same datacenter as the first slave, and if the resources required to transmit the required block from that slave to the first slave are available for allocation.
- the scheduling procedures select as the source of the file block transmission a best candidate slave, where the “best slave” is determined in accordance with a predefined cost function. If the second slave has file 0001v1 block 5 , and transmission of the file block from the second slave to the first slave has the best cost function score compared to the other candidate slaves, the second slave is selected as the source slave. Once the source slave is selected, the master schedules the transfer by sending file block copying instructions to the receiving slave device, and by updating the system resources table and pending transmissions table to denote the scheduled file block transmission.
- FIG. 5B shows incremental scheduling 502 .
- the resource allocation and scheduling procedures 394 ( FIG. 3A ) in the master's memory determine the incremental system resources and incremental states at step 514 .
- the resource allocation and scheduling procedures 394 ( FIG. 3A ) determine: the system hierarchy from the system hierarchy table 380 ( FIGS. 3A and 3B ); the system resources that have changed since the last time that the master performed scheduling, from the system resources table 382 ( FIGS. 3A and 3C ); all pending transmissions in the network that have changed since the last time that the master performed scheduling, from the pending transmissions table 384 ( FIGS.
- the resource allocation and scheduling procedures 394 ( FIG. 3A ) only look at changes to the system resources since the last scheduling occurred. Based on the above, the resource allocation and scheduling procedures 394 ( FIG. 3A ) determine a preferred schedule for transferring a block to the first slave, at step 516 . In other words, the scheduler assumes that the network has remained the same except for changes the master made when last performing scheduling. Less work is performed by the master as the overall states and resources are not examined each time scheduling occurs.
- the master When using incremental scheduling, the master will often schedule successive file block transmissions for a particular file between the same two slaves until either all the blocks of the file in the sending slave have been transmitted, or another transaction uses the resources required to transmit a file block from a first one of these two slaves to the second.
- FIG. 5C shows global scheduling 504 .
- the resource allocation and scheduling procedures 394 determine the overall system hierarchy, the overall system resources, all pending transmissions, and the overall states of all resources for the entire network at step 518 . This is similar to step 510 , but may occur at any time, not only when a slave contacts the master.
- the resource allocation and scheduling procedures 394 determine a preferred schedule for transferring blocks to each of the slaves at step 520 . This schedule is then stored in the scheduler at step 522 .
- the master simply locates the next schedule for that slave from its stored schedules and transmits that schedule to the slave. This allows the master to periodically compute the sequence or schedule of all copies to all slaves, and then as each slave requests a file the master sends the slave instructions. This also reduces the computational demands on the system by utilizing the master to perform scheduling when it would otherwise be idle.
- global scheduling is performed only for copy operations requiring use of the trunk lines. Global scheduling is performed periodically (e.g., once every N seconds), using a cost function designed to optimize use (or make efficient use) of the trunk lines.
- the master checks to see if a copy operation has been scheduled for that slave device based on global scheduling. If so, instructions are sent to the requesting slave to perform that copy operation. Otherwise, local scheduling is performed to select a best copy operation to be performed by the requesting slave.
- global scheduling is performed only at the FileSet level.
- the global schedule specifies, for each slave, only that a file (i.e., any file) within a FileSet should be copied, as well as the datacenter, and optionally rack, from which the file should be copied.
- Global scheduling is performed periodically, using a cost function designed to optimize use (or make efficient use) of the trunk lines.
- the slaves which have the same FileSet as the requesting slave, and which are within the specified datacenter, and optionally rack, are considered, and local scheduling is performed (but only considering the slaves in the specified datacenter or datacenter and rack) to select a best copy operation (e.g., a specific slave and a specified file block) to be performed by the requesting slave.
- a best copy operation e.g., a specific slave and a specified file block
- FIG. 5D shows hierarchical or partitioned scheduling 506 .
- the master every time a slave contacts the master, the master only examines the states and resources of potential copy sources that are nearby, thereby processing fewer potential copy sources.
- the allocation and scheduling procedures 394 determine: the incremental system hierarchy of all slaves and racks in a particular virtual or physical part of the network 100 ( FIG. 1 ) in which the requesting slave (the first slave) is located, from the system hierarchy table 380 ( FIGS. 3A and 3B ); the system resources from the system resources table 382 ( FIGS.
- part of determining the preferred schedule involves determining the optimal window size for each data transfer.
- the window size is determined and sent by the master to the slave as part of the copy instructions to be executed.
- the window size is the maximum number of data packets that can be transmitted without additional authorization from the receiving device. If the window size is too small, data is transferred too slowly, while if the window size is too large excessive resources are consumed, as packet losses require excessive retransmits. Therefore, during scheduling, the resource allocation and scheduling procedures 394 ( FIG. 3A ) preferably calculate the window size as follows.
- the path between the sending slave (1st Slave) and the receiving slave (2nd Slave) is determined from the system hierarchy table 380 ( FIG. 3B ) and/or the trunk level bandwidth graph 328 ( FIG.
- the lowest bandwidth along the path i.e., the bottleneck in the transmission, is determined from the system resources table 382 ( FIG. 3C ).
- the latency along the path is calculated by summing all latencies along the path from the trunk level bandwidth graph 328 ( FIG. 3C ) of the system resources table 382 ( FIG. 3C ), i.e., summing the latencies of all trunks along the path.
- latencies associated with the rack switches and datacenter switches in the path are also included in the summing of latencies.
- the window size is then calculated as a function of the lowest bandwidth along the path and the sum of all the latencies along the path.
- window size Function((lowest bandwidth along path)*( ⁇ latencies along path)) Latencies, may be input manually or automatically into the trunk level bandwidth graph 328 ( FIG. 3C ), such as by periodically performing pings, etc.
- the window size for a particular transfer may be set manually and stored within the master.
- the window size may be manually set for transfers between any slave in datacenter DC 1 to any slave in datacenter DC 2 .
- window sizes may be manually set for all possible data transfers between datacenters.
- a separate window size may be manually set for all data transfers within a datacenter.
- a further feature of the above described system allows for slaves and masters to fail, be removed from service, be replaced, etc. For example, if a slave fails or is removed from service, the failure count for that slave will rise until the slave is designated as bad. If the slave is replaced or comes back online, one of two things happen. First, the failure count for the slave will eventually drop below the predetermined failure threshold (or recovery threshold, if different) and the master again assumes that the slave is functioning. Second, if the slave sends its state to the master, the master then knows that the slave is operating. Similarly, if a master is replaced, the new master starts off without any slave states of any of the slaves in the system. However, over a short period of time all the slaves send their states to the master, which eventually assembles the states of all the slaves and begins scheduling data distribution.
- the predetermined failure threshold or recovery threshold, if different
Abstract
Description
window size=Function((lowest bandwidth along path)*(Σlatencies along path))
Latencies, may be input manually or automatically into the trunk level bandwidth graph 328 (
Claims (42)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/613,626 US7568034B1 (en) | 2003-07-03 | 2003-07-03 | System and method for data distribution |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/613,626 US7568034B1 (en) | 2003-07-03 | 2003-07-03 | System and method for data distribution |
Publications (1)
Publication Number | Publication Date |
---|---|
US7568034B1 true US7568034B1 (en) | 2009-07-28 |
Family
ID=40887354
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US10/613,626 Active 2026-09-17 US7568034B1 (en) | 2003-07-03 | 2003-07-03 | System and method for data distribution |
Country Status (1)
Country | Link |
---|---|
US (1) | US7568034B1 (en) |
Cited By (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7908178B2 (en) | 2004-05-04 | 2011-03-15 | Paul Nykamp | Methods for interactive and synchronous displaying session |
WO2011142733A1 (en) * | 2010-05-11 | 2011-11-17 | Thomson Licensing | A configurable computing architecture |
US8134992B1 (en) | 2008-09-24 | 2012-03-13 | Qualcomm Atheros, Inc. | Message-based coexistence interface between wireless devices |
US8249031B1 (en) * | 2009-11-17 | 2012-08-21 | Qualcomm Atheros, Inc. | Aggregation coexistence mechanism for wireless devices |
US8462622B2 (en) | 2009-12-08 | 2013-06-11 | Qualcomm Incorporated | Detection of co-located interference in a multi-radio coexistence environment |
US8520586B1 (en) | 2009-12-16 | 2013-08-27 | Qualcomm Incorporated | Discovery and connection coexistence mechanism for wireless devices |
US8606184B1 (en) | 2009-12-08 | 2013-12-10 | Qualcomm Incorporated | Coexistence message processing mechanism for wireless devices |
WO2013192077A1 (en) * | 2012-06-18 | 2013-12-27 | OpenQ, Inc. | Methods and apparatus for analyzing social media for enterprise compliance issues |
US20150100712A1 (en) * | 2013-10-07 | 2015-04-09 | Qualcomm Incorporated | Camera control interface slave device to slave device communication |
US9390120B1 (en) * | 2013-12-31 | 2016-07-12 | Google Inc. | System and methods for organizing hierarchical database replication |
CN108243348A (en) * | 2016-12-23 | 2018-07-03 | 航天星图科技（北京）有限公司 | A kind of stream process asks distribution server |
US20190250992A1 (en) * | 2013-12-05 | 2019-08-15 | Google Llc | Distributing Data on Distributed Storage Systems |
US20200007653A1 (en) * | 2018-06-28 | 2020-01-02 | Interdigital Ce Patent Holdings | Multi-path management |
US10693709B2 (en) * | 2018-03-28 | 2020-06-23 | Salesforce.Com, Inc. | Processing gap events in a change detection management system |
CN116319762A (en) * | 2023-05-18 | 2023-06-23 | 华夏卓越(天津)科技有限公司 | File batch copying method and device in local area network, electronic equipment and storage medium |
Citations (60)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
USD295632S (en) | 1985-10-28 | 1988-05-10 | Xerox Corporation | Icon for wastebasket or the like |
USD295764S (en) | 1985-12-09 | 1988-05-17 | Xerox Corporation | Icon for telephone or the like |
US5721914A (en) * | 1995-09-14 | 1998-02-24 | Mci Corporation | System and method for hierarchical data distribution |
USD396455S (en) | 1994-01-21 | 1998-07-28 | Xerox Corporation | Display object for a display screen |
US5884031A (en) * | 1996-10-01 | 1999-03-16 | Pipe Dream, Inc. | Method for connecting client systems into a broadcast network |
US5913026A (en) | 1994-07-19 | 1999-06-15 | Sarnoff Corporation | Method of allocating multiple users to a multiple user data distribution system |
US5920701A (en) * | 1995-01-19 | 1999-07-06 | Starburst Communications Corporation | Scheduling data transmission |
US6070191A (en) * | 1997-10-17 | 2000-05-30 | Lucent Technologies Inc. | Data distribution techniques for load-balanced fault-tolerant web access |
US6098078A (en) * | 1995-12-29 | 2000-08-01 | Lucent Technologies Inc. | Maintaining consistency of database replicas |
US6134596A (en) | 1997-09-18 | 2000-10-17 | Microsoft Corporation | Continuous media file server system and method for scheduling network resources to play multiple files having different data transmission rates |
US6141762A (en) * | 1998-08-03 | 2000-10-31 | Nicol; Christopher J. | Power reduction in a multiprocessor digital signal processor based on processor load |
US6189039B1 (en) * | 1997-04-10 | 2001-02-13 | International Business Machines Corporation | Selective tunneling of streaming data |
US6223226B1 (en) | 1998-03-09 | 2001-04-24 | Mitsubishi Denki Kabushiki | Data distribution system and method for distributing data to a destination using a distribution device having a lowest distribution cost associated therewith |
US6370580B2 (en) * | 1996-07-25 | 2002-04-09 | Clearway Acquisition, Inc. | Web serving system that coordinates multiple servers to optimize file transfers |
US6374336B1 (en) | 1997-12-24 | 2002-04-16 | Avid Technology, Inc. | Computer system and process for transferring multiple high bandwidth streams of data between multiple storage units and multiple applications in a scalable and reliable manner |
US20020059416A1 (en) * | 1999-07-09 | 2002-05-16 | Heikki Tuunanen | Management of performance of intelligent network services |
US20020059592A1 (en) * | 1999-02-19 | 2002-05-16 | Chaincast, Inc. | Method and system for implementing an internet radio device for receiving and /or transmitting media information |
US6427172B1 (en) | 1999-07-20 | 2002-07-30 | Cyberstar, L.P. | Method and apparatus for internet cache content delivery via a data distribution system |
US20020103968A1 (en) * | 2001-01-31 | 2002-08-01 | Grover Rajiv K. | Mirroring agent accessible to remote host computers, and accessing remote data-storage devices, via a communcations medium |
US20020138559A1 (en) * | 2001-01-29 | 2002-09-26 | Ulrich Thomas R. | Dynamically distributed file system |
US6467046B1 (en) * | 1996-05-06 | 2002-10-15 | Sun Microsystems, Inc. | System and method for automatically distributing copies of a replicated database in a computer system |
US6477548B1 (en) * | 1998-09-30 | 2002-11-05 | Casio Computer Co., Ltd. | Server computer capable of automatically notifying updated database content to client computer, and program storage medium |
US6477566B1 (en) | 1997-12-10 | 2002-11-05 | Nortel Networks Limited | Method and system of providing improved network management data between a plurality of network elements and a management system for increasing a flow and decreasing an amount of data transfer |
US6484204B1 (en) * | 1997-05-06 | 2002-11-19 | At&T Corp. | System and method for allocating requests for objects and managing replicas of objects on a network |
US20020194324A1 (en) * | 2001-04-26 | 2002-12-19 | Aloke Guha | System for global and local data resource management for service guarantees |
US20030014523A1 (en) * | 2001-07-13 | 2003-01-16 | John Teloh | Storage network data replicator |
USD469444S1 (en) | 2000-01-04 | 2003-01-28 | Apple Computer, Inc. | User interface for computer display |
US20030028726A1 (en) * | 2001-08-06 | 2003-02-06 | Gaertner Mark A. | Synchronized mirrored data in a data storage device |
US20030046283A1 (en) | 1997-04-15 | 2003-03-06 | Gracenote Inc. | Method and system for finding approximate matches in database |
USD472244S1 (en) | 2001-01-08 | 2003-03-25 | Apple Computer, Inc. | User interface for computer display |
USD473565S1 (en) | 1999-09-03 | 2003-04-22 | Cognos Incorporated | User interface for a computer screen |
USD474201S1 (en) | 2000-01-04 | 2003-05-06 | Apple Computer, Inc. | User interface for computer display |
US20030101278A1 (en) * | 2000-03-16 | 2003-05-29 | J.J. Garcia-Luna-Aceves | System and method for directing clients to optimal servers in computer networks |
US20030131193A1 (en) * | 2001-03-21 | 2003-07-10 | Hitachi, Ltd. | Multiple processor data processing system with mirrored data for distributed access |
US6606643B1 (en) * | 2000-01-04 | 2003-08-12 | International Business Machines Corporation | Method of automatically selecting a mirror server for web-based client-host interaction |
US6618752B1 (en) * | 2000-04-18 | 2003-09-09 | International Business Machines Corporation | Software and method for multicasting on a network |
US20030200307A1 (en) * | 2000-03-16 | 2003-10-23 | Jyoti Raju | System and method for information object routing in computer networks |
US20030204571A1 (en) * | 2002-04-24 | 2003-10-30 | International Business Machines Corporation | Distributed file system using scatter-gather |
US20030212869A1 (en) * | 2002-05-09 | 2003-11-13 | Burkey Todd R. | Method and apparatus for mirroring data stored in a mass storage system |
US20030225884A1 (en) * | 2002-05-31 | 2003-12-04 | Hayden Mark G. | Distributed network storage system with virtualization |
US6671705B1 (en) * | 1999-08-17 | 2003-12-30 | Emc Corporation | Remote mirroring system, device, and method |
US20040019721A1 (en) * | 2002-07-25 | 2004-01-29 | International Business Machines Corporation | Data path master/slave data processing device apparatus and method |
US6718361B1 (en) * | 2000-04-07 | 2004-04-06 | Network Appliance Inc. | Method and apparatus for reliable and scalable distribution of data files in distributed networks |
US20040093361A1 (en) * | 2002-09-10 | 2004-05-13 | Therrien David G. | Method and apparatus for storage system to provide distributed data storage and protection |
US20040143672A1 (en) * | 2003-01-07 | 2004-07-22 | Microsoft Corporation | System and method for distributing streaming content through cooperative networking |
US20040223615A1 (en) * | 2003-04-09 | 2004-11-11 | Dhawan Atam P. | Methods and apparatus for multi-level dynamic security system |
US20040260873A1 (en) * | 2003-06-17 | 2004-12-23 | Hitachi, Ltd. | Method and apparatus for managing replication volumes |
US20050015436A1 (en) * | 2003-05-09 | 2005-01-20 | Singh Ram P. | Architecture for partition computation and propagation of changes in data replication |
US6859839B1 (en) * | 1999-08-06 | 2005-02-22 | Wisconsin Alumni Research Foundation | Bandwidth reduction of on-demand streaming data using flexible merger hierarchies |
US6901604B1 (en) * | 1999-02-19 | 2005-05-31 | Chaincast, Inc. | Method and system for ensuring continuous data flow between re-transmitters within a chaincast communication system |
US6970937B1 (en) * | 2000-06-15 | 2005-11-29 | Abacast, Inc. | User-relayed data broadcasting |
US20050273514A1 (en) * | 2000-12-22 | 2005-12-08 | Ray Milkey | System and method for automated and optimized file transfers among devices in a network |
US6990667B2 (en) * | 2001-01-29 | 2006-01-24 | Adaptec, Inc. | Server-independent object positioning for load balancing drives and servers |
US7035933B2 (en) * | 2001-09-13 | 2006-04-25 | Network Foundation Technologies, Inc. | System of distributing content data over a computer network and method of arranging nodes for distribution of data over a computer network |
US20060129775A1 (en) * | 2003-04-08 | 2006-06-15 | Hulsey John A | Method, system and apparatus for releasing storage in a fast replication environment |
US7065618B1 (en) * | 2003-02-14 | 2006-06-20 | Google Inc. | Leasing scheme for data-modifying operations |
US20070174471A1 (en) * | 2003-04-30 | 2007-07-26 | Cedric Van Rossum | Secure, continous, proxy-optimized, device-to-device data download reception system and method of use |
US20080005780A1 (en) * | 2001-10-02 | 2008-01-03 | Singleton Richard B | Master security policy server |
US20080016028A1 (en) * | 2002-03-20 | 2008-01-17 | International Business Machines Corporation | Dynamic Cluster Database Architecture |
US7325073B2 (en) * | 2002-06-28 | 2008-01-29 | International Business Machines Corporation | Peer to peer video on demand system selectively using client and server to forward the requested video program to another client |
-
2003
- 2003-07-03 US US10/613,626 patent/US7568034B1/en active Active
Patent Citations (63)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
USD295632S (en) | 1985-10-28 | 1988-05-10 | Xerox Corporation | Icon for wastebasket or the like |
USD295764S (en) | 1985-12-09 | 1988-05-17 | Xerox Corporation | Icon for telephone or the like |
USD396455S (en) | 1994-01-21 | 1998-07-28 | Xerox Corporation | Display object for a display screen |
US5913026A (en) | 1994-07-19 | 1999-06-15 | Sarnoff Corporation | Method of allocating multiple users to a multiple user data distribution system |
US5920701A (en) * | 1995-01-19 | 1999-07-06 | Starburst Communications Corporation | Scheduling data transmission |
US5721914A (en) * | 1995-09-14 | 1998-02-24 | Mci Corporation | System and method for hierarchical data distribution |
US6098078A (en) * | 1995-12-29 | 2000-08-01 | Lucent Technologies Inc. | Maintaining consistency of database replicas |
US6467046B1 (en) * | 1996-05-06 | 2002-10-15 | Sun Microsystems, Inc. | System and method for automatically distributing copies of a replicated database in a computer system |
US6370580B2 (en) * | 1996-07-25 | 2002-04-09 | Clearway Acquisition, Inc. | Web serving system that coordinates multiple servers to optimize file transfers |
US5884031A (en) * | 1996-10-01 | 1999-03-16 | Pipe Dream, Inc. | Method for connecting client systems into a broadcast network |
US6189039B1 (en) * | 1997-04-10 | 2001-02-13 | International Business Machines Corporation | Selective tunneling of streaming data |
US20030046283A1 (en) | 1997-04-15 | 2003-03-06 | Gracenote Inc. | Method and system for finding approximate matches in database |
US6484204B1 (en) * | 1997-05-06 | 2002-11-19 | At&T Corp. | System and method for allocating requests for objects and managing replicas of objects on a network |
US6134596A (en) | 1997-09-18 | 2000-10-17 | Microsoft Corporation | Continuous media file server system and method for scheduling network resources to play multiple files having different data transmission rates |
US6070191A (en) * | 1997-10-17 | 2000-05-30 | Lucent Technologies Inc. | Data distribution techniques for load-balanced fault-tolerant web access |
US6477566B1 (en) | 1997-12-10 | 2002-11-05 | Nortel Networks Limited | Method and system of providing improved network management data between a plurality of network elements and a management system for increasing a flow and decreasing an amount of data transfer |
US6374336B1 (en) | 1997-12-24 | 2002-04-16 | Avid Technology, Inc. | Computer system and process for transferring multiple high bandwidth streams of data between multiple storage units and multiple applications in a scalable and reliable manner |
US6223226B1 (en) | 1998-03-09 | 2001-04-24 | Mitsubishi Denki Kabushiki | Data distribution system and method for distributing data to a destination using a distribution device having a lowest distribution cost associated therewith |
US6141762A (en) * | 1998-08-03 | 2000-10-31 | Nicol; Christopher J. | Power reduction in a multiprocessor digital signal processor based on processor load |
US6477548B1 (en) * | 1998-09-30 | 2002-11-05 | Casio Computer Co., Ltd. | Server computer capable of automatically notifying updated database content to client computer, and program storage medium |
US6901604B1 (en) * | 1999-02-19 | 2005-05-31 | Chaincast, Inc. | Method and system for ensuring continuous data flow between re-transmitters within a chaincast communication system |
US20020059592A1 (en) * | 1999-02-19 | 2002-05-16 | Chaincast, Inc. | Method and system for implementing an internet radio device for receiving and /or transmitting media information |
US7039173B2 (en) * | 1999-07-09 | 2006-05-02 | Nokia Corporation | Management of performance of intelligent network services |
US20020059416A1 (en) * | 1999-07-09 | 2002-05-16 | Heikki Tuunanen | Management of performance of intelligent network services |
US6427172B1 (en) | 1999-07-20 | 2002-07-30 | Cyberstar, L.P. | Method and apparatus for internet cache content delivery via a data distribution system |
US6859839B1 (en) * | 1999-08-06 | 2005-02-22 | Wisconsin Alumni Research Foundation | Bandwidth reduction of on-demand streaming data using flexible merger hierarchies |
US6671705B1 (en) * | 1999-08-17 | 2003-12-30 | Emc Corporation | Remote mirroring system, device, and method |
USD473565S1 (en) | 1999-09-03 | 2003-04-22 | Cognos Incorporated | User interface for a computer screen |
USD474201S1 (en) | 2000-01-04 | 2003-05-06 | Apple Computer, Inc. | User interface for computer display |
USD469444S1 (en) | 2000-01-04 | 2003-01-28 | Apple Computer, Inc. | User interface for computer display |
US6606643B1 (en) * | 2000-01-04 | 2003-08-12 | International Business Machines Corporation | Method of automatically selecting a mirror server for web-based client-host interaction |
US20060271705A1 (en) * | 2000-03-16 | 2006-11-30 | Garcia-Luna-Aceves J J | System and method for discovering information objects and information object repositories in computer networks |
US20030101278A1 (en) * | 2000-03-16 | 2003-05-29 | J.J. Garcia-Luna-Aceves | System and method for directing clients to optimal servers in computer networks |
US20030200307A1 (en) * | 2000-03-16 | 2003-10-23 | Jyoti Raju | System and method for information object routing in computer networks |
US6718361B1 (en) * | 2000-04-07 | 2004-04-06 | Network Appliance Inc. | Method and apparatus for reliable and scalable distribution of data files in distributed networks |
US6618752B1 (en) * | 2000-04-18 | 2003-09-09 | International Business Machines Corporation | Software and method for multicasting on a network |
US6970937B1 (en) * | 2000-06-15 | 2005-11-29 | Abacast, Inc. | User-relayed data broadcasting |
US20050273514A1 (en) * | 2000-12-22 | 2005-12-08 | Ray Milkey | System and method for automated and optimized file transfers among devices in a network |
USD472244S1 (en) | 2001-01-08 | 2003-03-25 | Apple Computer, Inc. | User interface for computer display |
US20020138559A1 (en) * | 2001-01-29 | 2002-09-26 | Ulrich Thomas R. | Dynamically distributed file system |
US6990667B2 (en) * | 2001-01-29 | 2006-01-24 | Adaptec, Inc. | Server-independent object positioning for load balancing drives and servers |
US20020103968A1 (en) * | 2001-01-31 | 2002-08-01 | Grover Rajiv K. | Mirroring agent accessible to remote host computers, and accessing remote data-storage devices, via a communcations medium |
US20030131193A1 (en) * | 2001-03-21 | 2003-07-10 | Hitachi, Ltd. | Multiple processor data processing system with mirrored data for distributed access |
US20020194324A1 (en) * | 2001-04-26 | 2002-12-19 | Aloke Guha | System for global and local data resource management for service guarantees |
US20030014523A1 (en) * | 2001-07-13 | 2003-01-16 | John Teloh | Storage network data replicator |
US20030028726A1 (en) * | 2001-08-06 | 2003-02-06 | Gaertner Mark A. | Synchronized mirrored data in a data storage device |
US6996668B2 (en) * | 2001-08-06 | 2006-02-07 | Seagate Technology Llc | Synchronized mirrored data in a data storage device |
US7035933B2 (en) * | 2001-09-13 | 2006-04-25 | Network Foundation Technologies, Inc. | System of distributing content data over a computer network and method of arranging nodes for distribution of data over a computer network |
US20080005780A1 (en) * | 2001-10-02 | 2008-01-03 | Singleton Richard B | Master security policy server |
US20080016028A1 (en) * | 2002-03-20 | 2008-01-17 | International Business Machines Corporation | Dynamic Cluster Database Architecture |
US20030204571A1 (en) * | 2002-04-24 | 2003-10-30 | International Business Machines Corporation | Distributed file system using scatter-gather |
US20030212869A1 (en) * | 2002-05-09 | 2003-11-13 | Burkey Todd R. | Method and apparatus for mirroring data stored in a mass storage system |
US20030225884A1 (en) * | 2002-05-31 | 2003-12-04 | Hayden Mark G. | Distributed network storage system with virtualization |
US7325073B2 (en) * | 2002-06-28 | 2008-01-29 | International Business Machines Corporation | Peer to peer video on demand system selectively using client and server to forward the requested video program to another client |
US20040019721A1 (en) * | 2002-07-25 | 2004-01-29 | International Business Machines Corporation | Data path master/slave data processing device apparatus and method |
US20040093361A1 (en) * | 2002-09-10 | 2004-05-13 | Therrien David G. | Method and apparatus for storage system to provide distributed data storage and protection |
US20040143672A1 (en) * | 2003-01-07 | 2004-07-22 | Microsoft Corporation | System and method for distributing streaming content through cooperative networking |
US7065618B1 (en) * | 2003-02-14 | 2006-06-20 | Google Inc. | Leasing scheme for data-modifying operations |
US20060129775A1 (en) * | 2003-04-08 | 2006-06-15 | Hulsey John A | Method, system and apparatus for releasing storage in a fast replication environment |
US20040223615A1 (en) * | 2003-04-09 | 2004-11-11 | Dhawan Atam P. | Methods and apparatus for multi-level dynamic security system |
US20070174471A1 (en) * | 2003-04-30 | 2007-07-26 | Cedric Van Rossum | Secure, continous, proxy-optimized, device-to-device data download reception system and method of use |
US20050015436A1 (en) * | 2003-05-09 | 2005-01-20 | Singh Ram P. | Architecture for partition computation and propagation of changes in data replication |
US20040260873A1 (en) * | 2003-06-17 | 2004-12-23 | Hitachi, Ltd. | Method and apparatus for managing replication volumes |
Non-Patent Citations (15)
Title |
---|
Adam et al., "Regeneration with Virtual Copies for Distributed Systems", IEEE vol. 19, No. 6, Jun. 1993. * |
Bawa, M. et al., Transience of Peers and Streaming Media, Hotnets, 2002. |
Cerf et al., "RFC 675-Specification of Internet Transmission Control Program" Dec. 1974, 141 pages. * |
Chawathe, Y. et al., "An Architecture for Internet Content Distribtion as an Infrastructure Service," Feb. 2000. Unpublished work. http://www.citeseer.ist.psu.edu/chawathe00architecture.html. |
Chawathe, Y., "Scattercast: An Architecture for Internet Broadcast Distribution as an Infrastructure Service," Ph.D. Dissertation, University of California at Berkeley, Fall 2000, http://berkeley.chawathe.com/thesis/thesis-single.pdf. |
Cohen, B., "Incentives Build Robustness in BitTorrent," May 22, 2003, http://bitconjurer.org/BitTorrent/bittorrentecon.pdf. |
Nguyen, T. and Zahkor, A., "Distributed Video Streaming Over Internet," Multimedia Computing and Networking. Jan. 2002. |
Osokine, S., "Search Optimization in the Distributed Networks," Oct. 15, 2002, www.grouter.net/gnutella/search.htm. |
Padmanabhan, V., et al. "Distributing Streaming Media Content Using Cooperative Networking," NOSSDAV '02, May 12, 2002, and Microsoft Research Technical Report, MSR-TR-2002-37, Apr. 2002. |
RFC 959: File Transfer Protocol (FTP), Oct. 1985. |
Rowstron, A. and Druschel, P., "Storage management and caching in PAST, a large-scale, persistent peer-to-peer storage utility," ACM SOSP, Oct. 2001. |
Stoica, I. et al., "Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications," ACM SIGCOMM, Aug. 2001. |
Wolfson, Ouri, "An Adaptive Data Replication Algorithm" ACM Transactions on Database Systems, vol. 22, No. 2, Jun. 1997, pp. 255-314. * |
Wolfson, Ouri, "Distributed Algorithms For Dynamic Replication of Data", 1992. * |
Zhao, B., et al., "Tapestry: An Infrastructure for Fault-tolerant Wide-area Location and Routing," Technical Report UCB/CSD-01-1141, Computer Science Division, University of California at Berkeley, Apr. 2001. |
Cited By (27)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7908178B2 (en) | 2004-05-04 | 2011-03-15 | Paul Nykamp | Methods for interactive and synchronous displaying session |
US8069087B2 (en) | 2004-05-04 | 2011-11-29 | Paul Nykamp | Methods for interactive and synchronous display session |
US8311894B2 (en) | 2004-05-04 | 2012-11-13 | Reliable Tack Acquisitions Llc | Method and apparatus for interactive and synchronous display session |
US8134992B1 (en) | 2008-09-24 | 2012-03-13 | Qualcomm Atheros, Inc. | Message-based coexistence interface between wireless devices |
US8249031B1 (en) * | 2009-11-17 | 2012-08-21 | Qualcomm Atheros, Inc. | Aggregation coexistence mechanism for wireless devices |
US8462622B2 (en) | 2009-12-08 | 2013-06-11 | Qualcomm Incorporated | Detection of co-located interference in a multi-radio coexistence environment |
US8606184B1 (en) | 2009-12-08 | 2013-12-10 | Qualcomm Incorporated | Coexistence message processing mechanism for wireless devices |
US8520586B1 (en) | 2009-12-16 | 2013-08-27 | Qualcomm Incorporated | Discovery and connection coexistence mechanism for wireless devices |
WO2011142733A1 (en) * | 2010-05-11 | 2011-11-17 | Thomson Licensing | A configurable computing architecture |
WO2013192077A1 (en) * | 2012-06-18 | 2013-12-27 | OpenQ, Inc. | Methods and apparatus for analyzing social media for enterprise compliance issues |
US8972511B2 (en) | 2012-06-18 | 2015-03-03 | OpenQ, Inc. | Methods and apparatus for analyzing social media for enterprise compliance issues |
CN105683933A (en) * | 2013-10-07 | 2016-06-15 | 高通股份有限公司 | Slave device to slave device communication of camera control interface |
CN108763122A (en) * | 2013-10-07 | 2018-11-06 | 高通股份有限公司 | Camera control interface is communicated from equipment to from equipment |
US20150100712A1 (en) * | 2013-10-07 | 2015-04-09 | Qualcomm Incorporated | Camera control interface slave device to slave device communication |
EP3055776A1 (en) * | 2013-10-07 | 2016-08-17 | Qualcomm Incorporated | Camera control interface slave device to slave device communication |
US9892077B2 (en) * | 2013-10-07 | 2018-02-13 | Qualcomm Incorporated | Camera control interface slave device to slave device communication |
CN105683933B (en) * | 2013-10-07 | 2018-09-18 | 高通股份有限公司 | Camera control interface is communicated from equipment to from equipment |
US20190250992A1 (en) * | 2013-12-05 | 2019-08-15 | Google Llc | Distributing Data on Distributed Storage Systems |
US10678647B2 (en) * | 2013-12-05 | 2020-06-09 | Google Llc | Distributing data on distributed storage systems |
US9390120B1 (en) * | 2013-12-31 | 2016-07-12 | Google Inc. | System and methods for organizing hierarchical database replication |
CN108243348A (en) * | 2016-12-23 | 2018-07-03 | 航天星图科技（北京）有限公司 | A kind of stream process asks distribution server |
CN108243348B (en) * | 2016-12-23 | 2019-03-15 | 中科星图股份有限公司 | A kind of stream process request distribution server |
US10693709B2 (en) * | 2018-03-28 | 2020-06-23 | Salesforce.Com, Inc. | Processing gap events in a change detection management system |
US20200007653A1 (en) * | 2018-06-28 | 2020-01-02 | Interdigital Ce Patent Holdings | Multi-path management |
US11115498B2 (en) * | 2018-06-28 | 2021-09-07 | Interdigital Ce Patent Holdings | Multi-path management |
CN116319762A (en) * | 2023-05-18 | 2023-06-23 | 华夏卓越(天津)科技有限公司 | File batch copying method and device in local area network, electronic equipment and storage medium |
CN116319762B (en) * | 2023-05-18 | 2023-08-04 | 华夏卓越(天津)科技有限公司 | File batch copying method and device in local area network, electronic equipment and storage medium |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8959144B2 (en) | System and method for scalable data distribution | |
US7568034B1 (en) | System and method for data distribution | |
EP0891585B1 (en) | A method and apparatus for client managed flow control on a limited memory computer system | |
US6662213B1 (en) | System and method for ensuring delivery of a single communication between nodes | |
KR101365838B1 (en) | Improved distributed kernel operating system | |
KR101363167B1 (en) | Improved distributed kernel operating system | |
US7801029B2 (en) | System for selecting routes for retransmission in a network | |
US9210073B2 (en) | System and method for message routing in a network | |
US8428065B2 (en) | Group communication system achieving efficient total order and state synchronization in a multi-tier environment | |
US20150341205A1 (en) | Method for guaranteeing service continuity in a telecommunication network and system thereof | |
EP1305924B1 (en) | Method and apparatus for reliable and scalable distribution of data files in distributed networks | |
US6912586B1 (en) | Apparatus for journaling during software deployment and method therefor | |
US6772158B1 (en) | Apparatus for data depoting and method therefor | |
CN109525496B (en) | Method and device for updating link state information | |
CN116455963A (en) | Cluster node registration method, medium, device and computing equipment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:DULITZ, DANIEL;GHEMAWAT, SANJAY;YANG, BWOLEN PO-JEN;AND OTHERS;REEL/FRAME:015094/0795;SIGNING DATES FROM 20031125 TO 20040219 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0610Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 12 |