US9619521B1 - Classification using concept ranking according to negative exemplars - Google Patents
Classification using concept ranking according to negative exemplars Download PDFInfo
- Publication number
- US9619521B1 US9619521B1 US14/142,976 US201314142976A US9619521B1 US 9619521 B1 US9619521 B1 US 9619521B1 US 201314142976 A US201314142976 A US 201314142976A US 9619521 B1 US9619521 B1 US 9619521B1
- Authority
- US
- United States
- Prior art keywords
- concept
- segment
- segments
- media item
- closest
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G06F17/3053—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/75—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7837—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content
-
- G06N99/005—
Definitions
- a system includes a database for storing a one or more media items and a processor connected thereto.
- the processor may be configured to obtain the one or more media items.
- Each media item may be identified as a concept media item or a non-concept media item.
- the processor may segment each of the media items. For each non-concept segment, the processor may identify a closest concept segment and rank each concept segment based on a number of instances in which it is closest to a non-concept segment.
- a processor may obtain one or more media items. Each media items may be identified as a concept media item or a non-concept media item. The processor may segment each of the media items. For each non-concept segment, a closest concept segment may be identified. Each concept segment may be ranked based on a number of instances in which it is closest to a non-concept segment.
- FIG. 1 shows a computer according to an implementation of the disclosed subject matter.
- FIG. 2 shows a network configuration according to an implementation of the disclosed subject matter.
- FIG. 3 is an example of a concept video that shows segmentation based on time and space in one step and semantic object segmentation in another.
- FIG. 4 is an example representation of how a CRANE technique may operate in embedded space as disclosed herein.
- FIG. 5 is an example system to rank segments according to an implementation disclosed herein.
- FIG. 6 is an example process to rank segments according to an implementation disclosed herein.
- Spatiotemporal segments may be leveraged for a variety of tasks in video understanding, including event detection, human motion volume generation, human activity recognition, and object segmentation.
- object segmentations may be generated on a weakly-labeled content (e.g., video) data.
- Weakly-labeled content may refer to a label applied to a particular content for which the label may refer to only a portion of the content.
- a video labeled as “Sebas Lettev wins F1 race” may only contain a few frames that show Sebas Lettev or his racecar. Typically, this utilizes variations on standard supervised methods (e.g., linear classifiers and multiple-instance learning).
- a noisy label may refer to instances where the label is unreliable.
- the video of Sebas Lettev may actually be a video of Linus Benedict.
- MIL Multiple Instance Learning
- labeled bags of instances are provided, where a positive bag contains at least one positive instance, and a negative bag contains no positive instances.
- MIL is more constrained than the disclosed technique, since the assumptions regarding the contents of each “bag” may not be true due to label noise (which is typically present in video-level tags, for example).
- label noise which is typically present in video-level tags, for example.
- algorithms must contend with positive videos that actually contain no concept segments as well as rare cases where some concept segments appear in negative videos.
- methods and systems may generate pixel-level concept annotations for a weakly labeled video or other type of content (such as an image or an audio segment).
- An example of the overall process will be described in reference to a video that is weakly tagged with a concept “dog.”
- the video-level tag suggests that at least some segments of the video correspond to the concept (i.e., “dog”) while most segments probably do not.
- the video may show a dog on a mat on a floor.
- the video may be processed using a standard unsupervised spatiotemporal segmentation method that aims to preserve object boundaries. In each frame post spatiotemporal segmentation, segments representing the floor, the mat, and the dog may be obtained or delineated.
- the dog may be defined by multiple segments based on the movement of the dog from one frame to the next and/or variation in its coat color.
- each segment within the video may be classified as containing the concept with which the entire video was labeled, which may be denoted herein as concept segments, or as not containing the concept, which may be denoted herein as background segments, such as segments showing the floor or mat.
- concept segments the concept with which the entire video was labeled
- background segments such as segments showing the floor or mat.
- background segments such as segments showing the floor or mat.
- each video may contain more than a single instance of the concept.
- the dog may not separable from a complex background using unsupervised methods.
- spatiotemporal segments for each video may be obtained.
- Each segment may be a spatiotemporal (3D) volume that is represented as a point in a high-dimensional feature space using a set of standard features computed over the segment.
- 3D spatiotemporal
- a dataset represented by ⁇ s 1 ,y 1 , . . . , s N ,y N ⁇ may be obtained, where s i refers to segment i, and y i ⁇ 1,1 ⁇ is the label for segment i.
- the label may be positive if the segment was extracted from a video being weakly-labeled as containing concept c and negative otherwise.
- the set P represents the set of all instances with a positive label
- the set N the set of all negative instances. Since the negative data was weakly labeled with concepts other than c, it may be assumed that the segments labeled as negative are, with rare exceptions, correctly labeled.
- the positive segments P that are concept segments and those that are background segments may be determined as disclosed herein.
- a concept e.g., the dog
- P b P ⁇ P C
- the elements of P C may be ranked in decreasing order of a score, S(s i ) such that top-ranked elements correspond to P C . Thresholding at a particular rank generates a partition that separates segments that contain the concept and segments that do not contain the concept (e.g., may contain non-concept segments and those concept segments that are below the threshold).
- Standard, fully-supervised methods such as Support Vector Machines (“SVM”), may learn a discriminative classifier to separate positive from negative data, given instance-level labels.
- SVM Support Vector Machines
- Such methods can be similar to the weakly supervised setting of segment annotation by propagating video-level labels to segments.
- a discriminative classifier may be used to separate P from N.
- P P C ⁇ P b
- the background segments from positively tagged videos, P b (which are typically the majority), may be label noise.
- a CRANE technique as disclosed herein may operate on the distances between weakly tagged positive and negative segments. Unlike NM, CRANE iterates through the segments in N, and each such negative instance penalizes nearby segments in P. Concept segments in P are those that are far from negatives (and therefore less penalized).
- an indicator function may be defined to evaluate as 1 if the argument of the function is true, and 0 if the argument is false.
- FIG. 4 is a schematic representation of segments in feature space that illustrates operation of a CRANE technique.
- Background segments in positive content e.g., videos
- the nearest neighbor to every negative instance is assigned a penalty f cut ( ⁇ ). Consequently, such segments may be ranked lower than other positives. Since concept segments are rarely the closest to negative instances, they may be ranked higher due to being penalized less often than background segments.
- FIG. 4 also shows an example of why a CRANE process may be more robust than NM or similar techniques to label noise among negative videos. Consider the points in the box shown at 430 of FIG. 4 .
- an unknown segment 410 s i is very close to a negative instance 420 that may have come from an incorrectly tagged video, for example.
- This single noisy instance will cause NM to irrecoverably reject s i .
- a CRANE process as disclosed herein will assign a small penalty s i for its proximity and, in the absence of corroborating evidence, from other negative instances 420 , s i 's rank will not change significantly.
- the associated segment still may be properly considered as a concept segment, despite being close to a single non-concept segment.
- CRANE techniques and systems as disclosed herein possess several properties that make them particularly suitable to practical implementations.
- CRANE may be employed to identify concept segments from weakly labeled content. For example, CRANE may be applied directly in instances where target videos are already weakly tagged. CRANE may be used to rank all of the segments in the positive set according to this score. Thresholding the list at a particular rank may create a partitioning into P C and P b . As another example, CRANE may be used to generate strongly supervised training data from a weakly labeled dataset in instances where the target videos are untagged. The classifier obtained in this case may be used to separate concept from non-concept segments in the target videos. A large number of weakly tagged positive and negative videos may be obtained and from which a set of segment-level classifiers may be learned that can be applied to arbitrary weakly tagged test videos.
- a system in an implementation, an example of which is provided in FIG. 5 , includes a database 510 and a processor 520 .
- the database may store one or media items 530 .
- a media item 530 may refer to, for example, an audio file, a video, or an image.
- the database 510 may contain a collection of user-provided or -created videos.
- a processor 520 may be connected, directly or indirectly, to the database 510 .
- the processor 520 may obtain the one or more media items 530 , for example, from the database.
- Obtaining one or more media items 530 may refer selecting a portion of the total media items stored in the database. It may refer to uploading or placing the media items 530 in the database 510 for storage.
- the processor 520 may identify each of the media items 530 as a concept media item or a non-concept media item.
- a concept may refer to a semantic tag that refers to some visual feature in the media item 530 .
- a database 510 may store user generated video content. Users may have tagged the uploaded content. The tags provided by the users for the videos may or may not be correct and may be referred to as weakly labeled. Thus, a video that may be identified as being a non-concept media item may in fact contain the concept. However, because such false negatives are relatively low compared to the entire pool of non-concept-identified videos, the overall impact on subsequent steps is minimal.
- the processor 520 may obtain segments for each of the media items. Segmentation may refer to spatiotemporal segmentation in which a segment of, for example, a video frame is internally consistent. For example, a mat shown in a video might be of a relatively uniform texture and color and would therefore likely form a single segment. Segmentation of the media items may be performed using other methods as well. For example, video frame or image may be segmented into defined “chunks” such as an eight by eight pixel block. Thus, segments may be derived from concept media items and non-concept media items.
- the processor 520 may identify a closest concept segment. As shown in FIG. 4 , the segments may be represented in a feature space. As described above, any time a non-concept media item has as its closest media item a concept media item, it may penalize (see Equation 2) the concept segment. In some configurations, it may simply be that the concept segment accrues a point and in a subsequent ranking step, concept segments with the lowest number of points are considered as more likely to contain the concept. Each concept segment may be ranked based on a number of instances in which it is closest to a non-concept segment. As FIG.
- those concept segments that are closest to other concept segments likely represent the concept while those concept segments that are nearest to non-concept segments likely represent background portions of the concept media item. For example, in a video labeled by a user as, “dog playing in pool,” most of the video frames (and segments) may not contain the concept, “dog.” Thus, most segments arising from such a concept video may be closer in feature space to non-concept segments. Those portions of the concept video that are closest to other concept video segments where “dog” is the concept are likely actual segments of the concept or “dog” in this case. In some configurations, a portion of ranked concept segments may be selected below a threshold value and each ranked concept segment within the selected portion may be labeled as a non-concept media item.
- a classifier may be trained based on at least a portion of the ranked concept segments and the non-concept segments. Higher-ranked concept segments may correspond to a higher likelihood of a segment containing the concept. As stated above, a threshold value may be selected based on how discriminating of a classifier is desired.
- the classifier may be trained on all of the ranked concept segments in some configurations. Concept segments below a threshold rank may be used as non-concept segments to train the classifier.
- the closest concept segment may be identified based upon a pairwise distance matrix of all segments of the media items in the feature space.
- a new media item may be classified based on the trained classifier. For example, a new video for which no information is available may be labeled as containing a dog concept using the classifier trained according to an implementation disclosed herein. More specifically, the classifier may denote specific frames and/or portions within the frames in which a dog was detected.
- a processor may obtain one or more media items at 610 .
- each of the media items may be identified as a concept media item or a non-concept media item.
- the processor may obtain segments for each of the media items at 620 .
- the processor may segment the media items using, for example, spatiotemporal segmentation, edge recognition, color patterns, defined segment sizes, etc. as disclosed above.
- a closest concept segment may be identified at 630 .
- Each concept segment may be ranked at 640 based on a number of instances in which it is closest to a non-concept segment.
- a classifier may be trained using a machine learning technique as described earlier. For example, if concept segments are ranked based on their distance from non-concept segments (i.e., the highest ranked concepts segments are those that are least often nearest to a non-concept segment), a classifier may be trained by applying a machine learning technique to a portion of the highest ranked concept segments and/or the non-concept segments.
- FIG. 1 is an example computer system 20 suitable for implementing embodiments of the presently disclosed subject matter.
- the computer 20 includes a bus 21 which interconnects major components of the computer 20 , such as one or more processors 24 , memory 27 such as RAM, ROM, flash RAM, or the like, an input/output controller 28 , and fixed storage 23 such as a hard drive, flash storage, SAN device, or the like.
- a user display such as a display screen via a display adapter
- user input interfaces such as controllers and associated user input devices
- keyboard, mouse, touchscreen, or the like and other components known in the art to use in or in conjunction with general-purpose computing systems.
- the bus 21 allows data communication between the central processor 24 and the memory 27 .
- the RAM is generally the main memory into which the operating system and application programs are loaded.
- the ROM or flash memory can contain, among other code, the Basic Input-Output system (BIOS) which controls basic hardware operation such as the interaction with peripheral components.
- BIOS Basic Input-Output system
- Applications resident with the computer 20 are generally stored on and accessed via a computer readable medium, such as the fixed storage 23 and/or the memory 27 , an optical drive, external storage mechanism, or the like.
- Each component shown may be integral with the computer 20 or may be separate and accessed through other interfaces.
- Other interfaces such as a network interface 29 , may provide a connection to remote systems and devices via a telephone link, wired or wireless local- or wide-area network connection, proprietary network connections, or the like.
- the network interface 29 may allow the computer to communicate with other computers via one or more local, wide-area, or other networks, as shown in FIG. 2 .
- FIG. 1 Many other devices or components (not shown) may be connected in a similar manner, such as document scanners, digital cameras, auxiliary, supplemental, or backup systems, or the like. Conversely, all of the components shown in FIG. 1 need not be present to practice the present disclosure. The components can be interconnected in different ways from that shown. The operation of a computer such as that shown in FIG. 1 is readily known in the art and is not discussed in detail in this application. Code to implement the present disclosure can be stored in computer-readable storage media such as one or more of the memory 27 , fixed storage 23 , remote storage locations, or any other storage mechanism known in the art.
- FIG. 2 shows an example arrangement according to an embodiment of the disclosed subject matter.
- One or more clients 10 , 11 such as local computers, smart phones, tablet computing devices, remote services, and the like may connect to other devices via one or more networks 7 .
- the network may be a local network, wide-area network, the Internet, or any other suitable communication network or networks, and may be implemented on any suitable platform including wired and/or wireless networks.
- the clients 10 , 11 may communicate with one or more computer systems, such as processing units 14 , databases 15 , and user interface systems 13 .
- clients 10 , 11 may communicate with a user interface system 13 , which may provide access to one or more other systems such as a database 15 , a processing unit 14 , or the like.
- the user interface 13 may be a user-accessible web page that provides data from one or more other computer systems.
- the user interface 13 may provide different interfaces to different clients, such as where a human-readable web page is provided to web browser clients 10 , and a computer-readable API or other interface is provided to remote service clients 11 .
- the user interface 13 , database 15 , and processing units 14 may be part of an integral system, or may include multiple computer systems communicating via a private network, the Internet, or any other suitable network.
- Processing units 14 may be, for example, part of a distributed system such as a cloud-based computing system, search engine, content delivery system, or the like, which may also include or communicate with a database 15 and/or user interface 13 .
- an analysis system 5 may provide back-end processing, such as where stored or acquired data is pre-processed by the analysis system 5 before delivery to the processing unit 14 , database 15 , and/or user interface 13 .
- a machine learning system 5 may provide various prediction models, data analysis, or the like to one or more other systems 13 , 14 , 15 .
Abstract
Description
S MIN(s i)=mintεN(dist(s i ,t))
S CRANE(s i)=−ΣzεN1[s i =arg mintεP(dist(t,z))]·f cut(dist(s i ,z)) Equation 2
Where 1(·) in Equation 2 denotes the indicator function and fcut(dist(si,z)) is a cutoff function over an input distance. As in other contexts, an indicator function may be defined to evaluate as 1 if the argument of the function is true, and 0 if the argument is false.
Claims (13)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/142,976 US9619521B1 (en) | 2013-12-30 | 2013-12-30 | Classification using concept ranking according to negative exemplars |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/142,976 US9619521B1 (en) | 2013-12-30 | 2013-12-30 | Classification using concept ranking according to negative exemplars |
Publications (1)
Publication Number | Publication Date |
---|---|
US9619521B1 true US9619521B1 (en) | 2017-04-11 |
Family
ID=58460552
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/142,976 Expired - Fee Related US9619521B1 (en) | 2013-12-30 | 2013-12-30 | Classification using concept ranking according to negative exemplars |
Country Status (1)
Country | Link |
---|---|
US (1) | US9619521B1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9971940B1 (en) * | 2015-08-10 | 2018-05-15 | Google Llc | Automatic learning of a video matching system |
US10419773B1 (en) * | 2018-03-22 | 2019-09-17 | Amazon Technologies, Inc. | Hybrid learning for adaptive video grouping and compression |
US20210125339A1 (en) * | 2020-01-09 | 2021-04-29 | Beijing Dajia Internet Information Technology Co., Ltd. | Method and device for segmenting image, and storage medium |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110029529A1 (en) * | 2009-07-28 | 2011-02-03 | Knight William C | System And Method For Providing A Classification Suggestion For Concepts |
US20130097176A1 (en) * | 2011-10-12 | 2013-04-18 | Ensequence, Inc. | Method and system for data mining of social media to determine an emotional impact value to media content |
US8819024B1 (en) * | 2009-11-19 | 2014-08-26 | Google Inc. | Learning category classifiers for a video corpus |
-
2013
- 2013-12-30 US US14/142,976 patent/US9619521B1/en not_active Expired - Fee Related
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110029529A1 (en) * | 2009-07-28 | 2011-02-03 | Knight William C | System And Method For Providing A Classification Suggestion For Concepts |
US8819024B1 (en) * | 2009-11-19 | 2014-08-26 | Google Inc. | Learning category classifiers for a video corpus |
US20130097176A1 (en) * | 2011-10-12 | 2013-04-18 | Ensequence, Inc. | Method and system for data mining of social media to determine an emotional impact value to media content |
Non-Patent Citations (2)
Title |
---|
Siva et al.,"In Defence of Negative Mining for Annotating Weakly Labelled Data", Proceedings of the 12th European Conference on Computer Vision, pp. 594-608, Oct. 7-13, 2012. |
Yagnik et al.,"Learning People Annotation from the Web via Consistency Learning", Proceedings of the International Workshop on Multimedia Information Retrieval, pp. 285-290, Sep. 2007. |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9971940B1 (en) * | 2015-08-10 | 2018-05-15 | Google Llc | Automatic learning of a video matching system |
US10419773B1 (en) * | 2018-03-22 | 2019-09-17 | Amazon Technologies, Inc. | Hybrid learning for adaptive video grouping and compression |
US20210125339A1 (en) * | 2020-01-09 | 2021-04-29 | Beijing Dajia Internet Information Technology Co., Ltd. | Method and device for segmenting image, and storage medium |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10621727B1 (en) | Label and field identification without optical character recognition (OCR) | |
Yang et al. | Visual sentiment prediction based on automatic discovery of affective regions | |
CN107209860B (en) | Method, system, and computer storage medium for processing weakly supervised images | |
US8625887B2 (en) | Systems and methods for matching visual object components | |
US10891465B2 (en) | Methods and apparatuses for searching for target person, devices, and media | |
US20170351905A1 (en) | Learning model for salient facial region detection | |
JP2017062781A (en) | Similarity-based detection of prominent objects using deep cnn pooling layers as features | |
US20190019052A1 (en) | Text Region Detection in Digital Images using Image Tag Filtering | |
Ye et al. | Scene text detection via integrated discrimination of component appearance and consensus | |
JP2016042346A (en) | Method, system, and program for determining social type of person | |
Chang et al. | A Bayesian approach for object classification based on clusters of SIFT local features | |
CN110008365B (en) | Image processing method, device and equipment and readable storage medium | |
US11600088B2 (en) | Utilizing machine learning and image filtering techniques to detect and analyze handwritten text | |
Bekhet et al. | Gender recognition from unconstrained selfie images: a convolutional neural network approach | |
US9619521B1 (en) | Classification using concept ranking according to negative exemplars | |
Alam et al. | A cost-effective computer vision-based vehicle detection system | |
Masaki et al. | Distant traffic light recognition using semantic segmentation | |
Cabrera-Gámez et al. | Exploring the use of local descriptors for fish recognition in lifeclef 2015 | |
Xiong et al. | Parallel tracking and detection for long-term object tracking | |
Wang et al. | Locality constrained joint dynamic sparse representation for local matching based face recognition | |
Liu et al. | A discriminative structural model for joint segmentation and recognition of human actions | |
Wang et al. | Learning optimal seeds for ranking saliency | |
Ye et al. | Robust scene text detection using integrated feature discrimination | |
Feng et al. | Adaptive all-season image tag ranking by saliency-driven image pre-classification | |
Prathiba et al. | ALMEGA-VIR: face video retrieval system |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SUKTHANKAR, RAHUL;TAGNIK, JAY;SIGNING DATES FROM 20131221 TO 20140102;REEL/FRAME:032642/0205 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044097/0658Effective date: 20170929 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Lapsed due to failure to pay maintenance fee |
Effective date: 20210411 |