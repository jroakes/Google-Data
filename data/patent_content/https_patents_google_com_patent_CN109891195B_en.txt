CN109891195B - System and method for using visual landmarks in initial navigation - Google Patents
System and method for using visual landmarks in initial navigation Download PDFInfo
- Publication number
- CN109891195B CN109891195B CN201780007032.6A CN201780007032A CN109891195B CN 109891195 B CN109891195 B CN 109891195B CN 201780007032 A CN201780007032 A CN 201780007032A CN 109891195 B CN109891195 B CN 109891195B
- Authority
- CN
- China
- Prior art keywords
- navigation
- landmark
- initial
- instructions
- landmarks
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 title claims description 61
- 230000000007 visual effect Effects 0.000 title description 39
- 238000012545 processing Methods 0.000 claims description 13
- 230000004044 response Effects 0.000 claims description 10
- 230000001052 transient effect Effects 0.000 claims description 5
- 230000005236 sound signal Effects 0.000 claims 2
- 241000208140 Acer Species 0.000 description 4
- 238000010586 diagram Methods 0.000 description 3
- 230000002452 interceptive effect Effects 0.000 description 3
- 239000003550 marker Substances 0.000 description 3
- 239000008186 active pharmaceutical agent Substances 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 230000002085 persistent effect Effects 0.000 description 2
- 230000010267 cellular communication Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 230000001934 delay Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 230000001932 seasonal effect Effects 0.000 description 1
- 238000012358 sourcing Methods 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3626—Details of the output of route guidance instructions
- G01C21/3644—Landmark guidance, e.g. using POIs or conspicuous other objects
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3626—Details of the output of route guidance instructions
- G01C21/3629—Guidance using speech or audio output, e.g. text-to-speech
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3626—Details of the output of route guidance instructions
- G01C21/3635—Guidance using 3D or perspective road maps
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3626—Details of the output of route guidance instructions
- G01C21/3647—Guidance involving output of stored or live camera images or video streams
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3626—Details of the output of route guidance instructions
- G01C21/3661—Guidance output on an external device, e.g. car radio
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3664—Details of the user input interface, e.g. buttons, knobs or sliders, including those provided on a touch screen; remote controllers; input using gestures
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3691—Retrieval, searching and output of information related to real-time traffic, weather, or environmental conditions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/50—Context or environment of the image
- G06V20/56—Context or environment of the image exterior to a vehicle by using sensors mounted on the vehicle
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/04817—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance using icons
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2200/00—Indexing scheme for image data processing or generation, in general
- G06T2200/24—Indexing scheme for image data processing or generation, in general involving graphical user interfaces [GUIs]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2215/00—Indexing scheme for image rendering
- G06T2215/16—Using real world measurements to influence rendering
Abstract
A route is determined from a current location of the portable device to the destination, wherein the route includes a series of guiding portions. Navigation instructions are generated to guide a user of the portable device along a route to a destination. To this end, candidate navigation landmarks perceptible within 360 degrees of the current location of the portable device are identified, navigation landmarks arranged in a direction substantially opposite to the direction of the first guiding portion of the series of guiding portions are selected from the candidate navigation landmarks, and initial instructions are generated in the navigation instructions and provided via a user interface of the portable device. The initial instruction references the selected navigation landmark.
Description
Technical Field
The present disclosure relates to navigation instructions, and in particular to using images in navigation instructions.
Background
The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.
Navigation systems that automatically plan routes for drivers, cyclists, and pedestrians between geographic locations typically utilize indications of distance, street names, building numbers, etc., to generate navigation instructions based on the routes. For example, these systems may provide instructions to the driver such as "forward one-quarter mile, then turn right into maple street". For users en route to a destination, the portable component of the navigation system may provide relative navigation instructions, i.e., instructions based on the direction of movement. Examples of such navigation instructions include "turn left into main street". However, at the beginning of navigation, when the user is stationary and has not yet moved in direction, the navigation system typically provides an absolute direction such as "walk north into the state street". Users may find absolute directions difficult to use because they do not always know their orientation. However, when the portable device is stationary, it is difficult for the portable device to determine its operation or heading.
Disclosure of Invention
In general, the system of the present disclosure identifies candidate navigation landmarks at the initial location of the portable device over 360 degrees of that location. The candidate navigation landmarks may be visible within a 360 degree view, audible within a 360 degree range, or perceivable in any direction from the initial position. The system then generates initial navigation instructions that reference one or more of these landmarks without limiting the selection of those landmarks that are arranged in the initial direction of travel. When the portable device is stationary, the system may generate such initial navigation instructions without relying on the portable device's sensors to determine the current orientation of the portable device or user. In some cases, the system generates initial navigation instructions that reference landmarks that will be located behind the user once he or she begins traveling. For example, the initial navigation instruction may be "leave fountain and walk to hillside". Further, the landmark or landmarks selected by the system need not be closest and may include remote natural objects such as mountains as well as man-made structures such as bridges, towers, tall buildings, etc. Using crowdsourcing or automation techniques, the system may also recognize audible landmarks and reference the audible landmarks in the initial navigation instructions, e.g., "go to music" if an outdoor concert is reported at a nearby location.
Example embodiments of these techniques are methods for providing initial navigational guidance, which may be performed, at least in part, by one or more processors. The method includes determining a route from a current location of the portable device to the destination, wherein the route includes a series of guide portions, and generating navigation instructions to guide a user of the portable device along the route to the destination. Generating the navigation instruction includes identifying candidate navigation landmarks perceptible within 360 degrees of a current location of the portable device, selecting a navigation landmark from the candidate navigation landmarks that is disposed in a direction substantially opposite to a direction of a first guiding portion of the series of guiding portions, generating an initial instruction in the navigation instruction, the initial instruction referencing the selected navigation landmark, and causing the initial navigation instruction to be provided via a user interface of the portable device.
Another example embodiment of these techniques is a portable computing device that includes a positioning module, a user interface module, processing hardware coupled to the positioning module and the user interface module, and non-transitory memory readable by the processing hardware. The memory stores instructions that, when executed by the processing hardware, cause the portable computing device to obtain navigation instructions for guiding a user to a destination along a route that includes a series of guide portions, wherein the navigation instructions include initial navigation instructions that reference navigation landmarks arranged in a direction substantially opposite to a direction of a first guide portion of the series of guide portions, and provide the initial navigation instructions via the user interface module.
Another example embodiment of these techniques is a non-transitory computer-readable medium having instructions stored thereon for providing initial navigational guidance. The instructions, when executed by the one or more processors, cause the one or more processors to determine a route from a current location of the portable device to the destination, the route including a series of guide portions, and generate navigation instructions to guide a user of the portable device along the route to the destination. To generate navigation instructions, the instructions cause the one or more processors to identify candidate navigation landmarks that are perceivable within 360 degrees of a current location of the portable device, select a navigation landmark disposed in a direction substantially opposite to a direction of a first guiding portion of the series of guiding portions, and generate an initial instruction in the navigation instructions, the initial instruction referencing the selected navigation landmark. The instructions also cause the one or more processors to provide initial navigation instructions via a user interface of the portable device.
Another example embodiment of these techniques is a non-transitory computer-readable medium having instructions stored thereon for providing initial navigational guidance. The instructions, when executed by the one or more processors, cause the one or more processors to receive a request for navigation instructions to direct a user of the portable computing device to a destination; obtaining a route from a current location of the portable device to the destination in response to the request, the route including a series of guide portions; obtaining navigation instructions to guide a user of the portable device along a route to a destination, wherein the navigation instructions include initial navigation instructions that reference navigation landmarks arranged in a direction substantially opposite to a direction of a first guide portion of the series of guide portions; and providing an initial navigation instruction via a user interface of the portable device for output.
Other example embodiments of these techniques are methods and systems of selecting a navigation landmark that is arranged in a direction substantially aligned with a direction of a first guiding portion in a series of guiding portions. Other exemplary methods and systems select navigation landmarks that are disposed in a direction that is neither aligned with nor opposite to the direction of the first guiding portion (e.g., the direction in which the navigation landmarks are located may be a direction that is substantially perpendicular to the first guiding portion).
Drawings
FIG. 1 is a block diagram of an example computing system that generates initial navigation instructions by referencing perceptible landmarks within a 360 degree range of a current location of a portable device, according to one embodiment;
FIG. 2 is a flow chart of an example method for generating initial navigational instructions for a driver using perceptible landmarks in a 360 degree range that may be implemented in the system of FIG. 1;
FIG. 3 is a flow chart of an example method for selecting landmarks for an initial navigation direction based on a plurality of metrics that may be implemented in the system of FIG. 1;
4A-C illustrate a plurality of screen shots of an example user interface of a portable device that a navigation software module of the present disclosure may generate to interactively provide initial navigation instructions;
FIG. 5 is a block diagram schematically illustrating the use of remote and closest navigation landmarks in generating initial navigation instructions in accordance with an example embodiment of the system of FIG. 1;
FIG. 6 is a flow chart of an example method for generating an initial navigation instruction that references two navigation landmarks arranged in different directions relative to a current location of a portable device; and
FIG. 7 is a flowchart of an example method for using photographs captured from certain locations to discern the location of a navigation landmark that can be used as a navigation landmark for an initial navigation instruction.
Detailed Description
General overview
For users en route to a destination, the navigation system of the present disclosure may provide relative navigation instructions, i.e., instructions based on the direction of movement. Examples of such navigation instructions include "turn left into main street". However, at the beginning of navigation, absolute directions such as "walk north into the state street" may be difficult to use when the user is stationary and has not yet moved in direction. To generate an initial navigation instruction, the navigation system identifies candidate navigation landmarks at the initial location of the portable device over 360 degrees of that location. The navigation system then generates initial navigation instructions referencing one or more of the navigation landmarks without restricting selection of those navigation landmarks arranged in the initial direction of travel.
In some cases, the navigation system generates an initial navigation instruction that references a navigation landmark that will be located behind the user once he or she begins traveling. For example, the initial navigation instruction may be "leave fountain and walk to hillside". Further, the landmark or landmarks selected by the system need not be closest and may include remote natural objects such as mountains as well as man-made structures such as bridges, towers, tall buildings, etc. Using crowd sourcing, the navigation system may also recognize and reference audible landmarks for initial navigation instructions, e.g. "go to music" if an outdoor concert is reported at a nearby location.
Navigation landmarks may be permanent or transient. The navigation system may generate various metrics for navigating landmarks and use the metrics when selecting a navigation landmark for a certain position and a certain orientation. Metrics may relate to observability, salience, and uniqueness of the navigation landmark. To evaluate these factors, the system may utilize machine vision and other automated techniques, crowdsourcing, and the like.
Example computing Environment
FIG. 1 illustrates an environment 10 in which at least some techniques for generating initial navigation instructions may be implemented in the environment 10. The environment 10 includes a portable system 12 and a server system 14 interconnected via a communications network 16. The server system 14, in turn, may communicate with various databases and, in some embodiments, with third party systems, such as on-site traffic services or weather services (not shown to avoid clutter). The navigation system 18 operating in the environment 10 includes components configured to select visual and/or audible landmarks included in the navigation instructions and, in particular, for generating initial navigation instructions. Navigation system 18 may be implemented in portable system 12, server system 14, or partially in portable system 12 and partially in server system 14.
The portable system 12 may include a portable electronic device such as a smart phone, a wearable device such as a smart watch or a head mounted display, or a tablet computer. In some embodiments or scenarios, the portable system 12 also includes components embedded or installed in the vehicle. For example, a driver of a vehicle equipped with an electronic component, such as a head unit with a touch screen, may navigate using her smartphone. The smart phone may be connected to the mobile phone via, for example Is connected to the head unit to access the sensors of the vehicle and/or project navigation instructions onto the screen of the head unit. In general, modules of portable or wearable user devices, modules of vehicles, and devices or modules external to the devices may operate as components of portable system 12.
Portable system 12 may include a processing module 22, which processing module 22 may include one or more Central Processing Units (CPUs), one or more Graphics Processing Units (GPUs) for efficiently rendering graphics content, an Application Specific Integrated Circuit (ASIC), or any other suitable type of processing hardware. In addition, portable system 12 may include memory 24 comprised of persistent (e.g., hard disk, flash drive) and/or non-persistent (e.g., RAM) components. The portable system 12 also includes a user interface 28 and a network interface 30. Depending on the scenario, the user interface 28 may correspond to a user interface of a portable electronic device or a user interface of a vehicle. In either case, the user interface 28 may include one or more input components, such as a touch screen, microphone, keyboard, etc., and one or more output components, such as a screen or speaker.
The network interface 30 may support short-range and/or long-range communications. For example, the network interface 30 may support cellular communications, personal area network protocols such as IEEE 802.11 (e.g., wi-Fi) or 802.15 (Bluetooth). In some embodiments, portable system 12 includes a plurality of network interface modules to interconnect a plurality of devices within portable system 12 and connect portable system 12 to network 16. For example, the portable system 12 may include a smart phone, a head unit of a vehicle, and a camera mounted on a windshield. The smart phone and head unit may communicate using bluetooth, the smart phone and camera may communicate using USB, and the smart phone may communicate with the server 14 via the network 16 using 4G cellular services to transfer information to and from the various components of the portable system 12.
Further, the network interface 30 may support geolocation in some cases. For example, the network interface 30 may support Wi-Fi trilateration. In other cases, the portable system 12 may include a dedicated positioning module 32, such as a Global Positioning Service (GPS) module. In general, portable system 12 may include various additional components including redundant components such as positioning modules implemented in both vehicles and smart phones.
In the example embodiment shown in fig. 1, memory 24 stores instructions that implement an initial navigation instruction API25, a geographic application 26, and a third party application 27. However, in another embodiment, the memory 24 stores only the API25 and a third party application 27, such as a web browser or game application, which third party application 27 obtains initial navigation instructions by calling the API25, referencing the navigation landmarks, and uses in the initial navigation instructions in an application-specific manner. For example, in another embodiment, the memory 24 stores only the geographic application 26, the geographic application 26 obtains initial navigation instructions, and these initial navigation instructions are provided as part of the driving direction via the user interface 28. More generally, the memory 24 stores the API25, the geographic application 36, or both.
With continued reference to fig. 1, the portable system 12 may communicate with the server system 14 via a network 16, which network 16 may be a wide area network such as the internet. Server system 14 may be implemented in one or more server devices that include devices distributed across multiple geographic locations. The server system 14 may implement a route engine 40, a navigation instruction generator 42, and a landmark selection module 44. The components 40-44 may be implemented using any suitable combination of hardware, firmware, and software. The server system 15 may access databases such as a map database 50, a visual landmark database 52, and a user profile database 54, which may be implemented using any suitable data storage and access technology.
In operation, route engine 40 may receive a request for navigation instructions from portable system 12. For example, the request may include a source, a destination, and constraints such as a request to avoid toll roads. For example, the request may come from a geographic application 26 or an API 25. In response, the route engine 40 may retrieve road geometry data, road and intersection limits (e.g., unidirectional, left-hand turn prohibited), road type data (e.g., highway, local road), speed limit data, etc. from the map database 50 to generate a route from the source to the destination. In some implementations, the route engine 40 also obtains on-site traffic data when the optimal route is selected. In addition to the best or "primary" route, route engine 40 may also generate one or more alternative routes.
In addition to road data, map database 50 may also store descriptions of indications of geometry and location for various natural geographic features such as rivers, mountains, and forests, as well as artificial geographic features such as buildings and parks. The map data may include vector graphics data, raster image data, and text data, among other data. In an example embodiment, the map database 50 organizes map data into map tiles (map tiles) that generally correspond to two-dimensionally organizing geospatial data into traversable data structures such as quadtrees.
Navigation instruction generator 42 may use one or more routes generated by route engine 40 and generate a series of navigation instructions. Examples of navigational instructions include "at 500 feet, turn right into elm street" and "continue straight four miles". The navigation instruction generator 42 may implement natural language generation techniques to construct these and similar phrases in the language of the driver associated with the portable system 12. The instructions may include text, audio, or both. As discussed in more detail below, the navigation instruction generator 42 and/or software components implemented in the portable system 12 generate initial navigation instructions that reference the navigation landmarks in a different manner than the navigation instructions provided when the portable device is en route.
The landmark selection module 44 may operate as part of the navigation system 18, and the navigation system 18 may also include the navigation application 26. Landmark selection module 44 may augment navigation instructions generated by navigation instruction generator 42 with reference to visual landmarks such as prominent buildings, billboards, traffic lights, parking signs, statues and monuments, and symbols representing businesses. To this end, the landmark selection module 44 may access the visual landmark database 52 to select a set of visual landmarks that are placed along the navigation route. In view of additional signals, such as an indication that the portable computing device is stationary, the landmark selection module 44 selects one or more landmarks included in the initial navigation direction. As discussed below, when selecting a landmark for an initial navigation direction, the landmark selection module 44 may apply different selection criteria (e.g., visibility over 360 degrees).
The visual landmark database 52 may store information about significant geographic entities that are visible while driving (or riding, walking, or otherwise moving along a navigation route) and thus act as visual landmarks. For each visual landmark, the visual landmark database 52 may store one or more photographs, geographic coordinates, text descriptions, comments submitted by the user, and numerical metrics indicative of the practicality of the visual landmark and/or a particular image of the visual landmark. In some implementations, the landmark specific records in the visual landmark database 52 store multiple views of visual landmarks from the same vantage point, i.e., multiple views of visual landmarks captured from the same location and with the same camera orientation. However, the multiple views of the visual landmark may differ depending on the time of day, weather conditions, seasons, and the like. The data record may include metadata specifying these parameters for each image. For example, the data record may include a photograph of the billboard at night when the billboard is illuminated, with a time stamp indicating when the photograph was captured, and another photograph of the billboard from the same vantage point during the day with a corresponding time stamp. Further, the data record may include a photograph of the billboard captured during snowy weather, during rainy weather, during foggy weather, etc., and a corresponding indicator for each photograph. Further, the data records may include photographs captured during different seasons.
In short, the visual landmark database 52 may store a large set of visual landmarks that are, in some cases, redundant in terms of the number of landmarks available for the same maneuver (e.g., billboards to the right and churches to the left of the same intersection) as well as in terms of images available for the same landmark. The navigation system 18 may determine which redundant landmarks are useful for particular lighting conditions, weather conditions, traffic conditions (because the driver may find it difficult to identify certain visual landmarks while driving quickly), and to what extent the corresponding scene may be seen from the driver's vantage point (inferred from real-time images).
In addition to multiple images of the same visual landmark, the visual landmark database 52 may also store multiple descriptions of the same landmark, such as "large glass building," "building with large 'M' in front of it," "building with the national flag of strait," and so forth. An operator of server system 14 and/or a user submitting landmark information as part of a crowdsourcing activity may submit these descriptions, and server system 14 may use feedback processing techniques discussed in more detail below to determine which description the driver finds more helpful. To track driver feedback, the visual landmark database 52 in one example embodiment stores an overall digital metric for visual landmarks, separate digital metrics for different times of day, different weather conditions, etc., and/or separate digital metrics for different images, which may be used to evaluate whether visual landmarks should be referenced in the navigation instructions.
To populate the visual landmark database 52, the server system 14 may receive satellite images, photographs, and videos submitted by various users, street images collected by automobiles equipped with dedicated panoramic cameras, street and pavement images collected by pedestrians and cyclists, crowd-sourced information from users (e.g., "street marts in state and main streets"), and the like. Similarly, the visual landmark database 52 may receive descriptions of landmarks from various sources, such as an operator of the server system 14 and a person submitting user-generated content. In addition to or in lieu of using crowdsourcing techniques, the server system 14 may utilize 3D reconstruction, computer vision, and other automated techniques to generate numerical metrics for observability, saliency, and uniqueness of potential visual landmarks, and utilize various audio processing techniques to determine similarity metrics (e.g., volume, frequency range, uniqueness) for auditory landmarks. For example, in some implementations, server system 14 supplements or corrects automatically generated metrics using metrics derived from user-generated content.
As shown in fig. 1, the visual landmark database 52 may store images 70 of very unique famous buildings visible in relatively few locations, images 72 of large structures visible from multiple locations, and images 74 of buildings that are generally unobtrusive, yet may be fully observable, significant, and unique in some environment (e.g., next to multiple smaller buildings).
In another embodiment, the visual landmark database 52 stores only references to images in other databases. For example, one or more databases may store images 70, 72, and 74, and various electronic services may use these images for various purposes (such as providing user albums, online photo libraries, virtual travel, or supplementary business data). Because these images are updated frequently, the visual landmark database 52 in this embodiment may store only references to images to avoid storing redundancy and storing delays associated with synchronizing the databases. In a similar manner, for a landmark, the visual landmark database 52 may store references to corresponding map data in the map database 50, rather than storing a copy of the map data.
With continued reference to FIG. 1, the user profile database 54 may store user preferences regarding the types of visual landmarks that the user prefers to see. For example, a user's profile may indicate that she prefers to use a billboard as a landmark. The landmark selection module 44 may use user preferences as at least one factor when selecting a visual landmark from the redundant visual landmarks. In some embodiments, the user provides an indication that he or she allows the navigation system 18 to utilize the data. Other factors for selecting a visual landmark from the redundant visual landmarks may include objective metrics such as numerical metrics that are saliency, observability, and uniqueness. In an example embodiment, the objective metric takes precedence over the user preference.
In an example operation, portable system 12 generates a request for navigation instructions and transmits the request to server system 14. The request may include an indication of the current location of the portable system 12, as well as an indication that the portable system 12 is currently stationary, and the indication may be explicit or implicit. The navigation system 18 determines which navigation landmarks are visible or otherwise perceivable at the current location of the portable system 12 without limiting the selection of navigation landmarks to any single direction relative to the current location of the portable system 12.
As described above, the functionality of navigation system 18 may be distributed between portable system 12 and server system 14 in any suitable manner. In some implementations, for example, server system 14 may provide an indication of a plurality of landmarks surrounding the current location of portable system 12, as well as navigation instructions for traveling to a destination, and geographic application 26 and/or API 25 may locally format the initial navigation instructions.
Next, example methods for generating initial navigation instructions with navigation landmarks and populating a database with appropriate navigation landmark data are discussed with reference to fig. 2 and 3, respectively, followed by a discussion of example interfaces that may be provided by the portable system 12. Additional examples and methods for selecting navigation landmarks and providing navigation landmarks in initial navigation are also discussed with reference to fig. 5-7.
Example method for generating initial navigation instructions
FIG. 2 is a flow chart of an example method 100 for generating initial navigational instructions for a driver. The navigation system 18 in question can implement the method 100 as a set of instructions executable on one or more processors in one or more devices. For example, some or all of the acts making up method 100 may be implemented in modules 40, 42, and 44.
The method 100 begins at block 102 where a route to a destination is determined at block 102. For example, a request to navigate a user from a current location of a portable device to a location may be received from a portable device, such as portable system 12 of FIG. 1. The route may be determined according to any suitable technique in view of road geometry, various road characteristics such as speed limits and charging requirements, current road conditions, weather, etc. The route may include a series of guiding portions between waypoints. An example series may include the following guide portions: (i) along the main street, from the current position north-wise at the fifth major road and the main street to the corner of the ninth major road and the main street, (ii) along the ninth major road, from the corner of the ninth major road and the main street to the corner of the ninth major road and the maple street, (iii) along the maple street, from the corner of the ninth major road and the maple street to the south-100 yards.
At block 104, candidate navigation landmarks that are perceivable within 360 degrees of the current location of the portable device are identified. As discussed in more detail with reference to fig. 3, the database 52 of fig. 1 or a similar database may store data related to various navigation landmarks that may be significantly and uniquely observed from various geographic locations. For example, the database may indicate that for the fifth thoroughfare and the main street corner, the navigational landmarks that are significantly and uniquely observable include the municipal hall of 200 yards north, the railroad bridge of one quarter mile west, the mountain of far south (e.g., > 10 miles), and the fountain of 50 yards east.
Next, at block 106, one or more navigation landmarks are selected for inclusion in the initial navigation instruction, and a corresponding initial navigation instruction is generated at block 108. To continue the above example, the initial guiding portion in the series corresponding to a certain route defines going north from the fifth thoroughfare and main street. The initial navigation instruction for the segment may include a reference to a mountain disposed south from the starting location. As a more specific example, the initial navigation instruction may include the text "start traveling away from mountain". Further, the initial navigation instruction may refer to the town hall arranged in the opposite direction, and the text may be "start to leave the mountain and travel to the town hall".
At block 110, the initial navigation instructions may be provided visually via a user interface or in the form of a broadcast. As shown in more detail in fig. 4A-C, initial navigation instructions may also be provided interactively to allow a user to view additional details regarding the navigation landmarks being referenced. Once the user is provided with the initial navigation instruction in the manner described above, the subsequent navigation direction may refer to relative maneuver, such as "turn left two blocks in the century".
Referring now to fig. 3, for example, an example method 150 for selecting landmarks for an initial navigation direction based on a plurality of metrics may be implemented in module 44 as a set of instructions executable on one or more processors. More generally, the method 150 may be implemented in one or more modules that, in some embodiments, are distributed across multiple devices or systems. According to some embodiments, the method 150 is performed in an offline or batch mode to generate metrics for various candidate navigation landmarks, and to update the respective database records for subsequent selection. In other implementations, for example, the method 150 is performed in real-time as part of the block 106 discussed above with reference to fig. 2.
At block 152, an indication of candidate navigation markers for a geographic location is received. For example, for a city location, any discernable structure near the location, any natural geographic features known to be visible at the location, noise sources known to be perceptible at the location, and so forth. Next, at blocks 154-158, various numerical metrics for the candidate navigation landmarks may be determined.
At block 154, a measure of observability is determined. As used herein, observability refers to the probability that a passenger can locate a navigational landmark without changing his or her position. For example, a fountain that may be hidden from one side of a street may not always be observable, and the corresponding metric may be relatively low. Similarly, seasonal or other transient features may not be always observable. Thus, the observability metric may also be relatively low (or, in some embodiment, the observability metric for the appropriate season may be relatively high, and the observability metric for the remaining seasons may be relatively low). Instead, observability metrics for large monuments in the middle of a city square may be assigned relatively high values for locations in the square.
One or more signals may be used to calculate a value of a measure of observability. For example, a photograph of a landmark may be marked with the name and pose data (an indication of the camera position or camera orientation) of the landmark. A certain number of photographs of the same landmark captured from the same location, reflected in the gesture data, is typically a powerful indicator that the navigation landmark should have a high observability metric. Furthermore, three-dimensional (3D) models constructed using scanning, 3D reconstruction from user photographs, panoramic images, aerial images, etc. may be used to determine a longer distance line of sight.
Furthermore, transient features may act as navigation landmarks and be assigned high observability metrics for appropriate time periods. For example, the farmer market may be seasonally open for days of the week, and public art facilities may be exhibited for a limited period of time. Referring back to fig. 1, a map database 50 or similar database may include business data for farmer markets at a location. The business data and/or crowd-sourced information related to the farmer market may be used when generating the observability metrics.
At block 156, a module implementing the method 150 determines a measure of significance for the candidate navigation landmarks. To this end, the module may use such signals as an indication of how widely captured or mentioned in various documents the candidate navigation landmarks are. For example, the numerical value of the measure of saliency may be calculated by calculating the number of index photos of the candidate navigation landmark and/or the number of search hits, comments, and queries related to the candidate navigation landmark.
Another signal that may be used by the module is to indicate whether the candidate navigation landmarks can be easily selected from the environmental elements. As one example, the computer visual feature detector/classifier may attempt to discern candidate navigation markers within a photograph of a scene that includes the candidate navigation markers, where the photograph was captured at a given geographic location. If the candidate navigation landmarks can be distinguished with high confidence, this can be used as a signal indicating saliency.
With continued reference to FIG. 3, a measure of uniqueness may be calculated at block 158. To this end, for example, the proximity of the candidate navigation landmarks to other landmarks may be evaluated to determine whether nearby candidate navigation landmarks or points of interest are of the same type (e.g., fountain, traffic lights). Some types of points of interest may naturally have an expected place of attention (e.g., a monument). Other types of points of interest may be expected to be substantially similar to each other (e.g., traffic signs).
The metrics determined at blocks 154, 156, and 158 may be weighted in any suitable manner to generate a total score. At block 160, one or more landmarks may be selected for the initial navigation instruction in view of the total score, or if desired, in view of only one or two metrics determined at blocks 154, 156, and 158.
Example user interface
Referring back to fig. 1, the geographic application 26 or the third party application 27 may provide initial navigation instructions via an interactive user interface. An example embodiment of the interface is discussed with reference to fig. 4A-C. Each of the screen shots 200, 220 and 240 illustrates an example output of a software application displayed on a touch screen of the portable device 201. For example, while these examples depict a typical rectangular layout for a smart phone or tablet, other types of devices, such as a smart watch or dedicated navigation device, square, circular, oval, and other types of layouts may be implemented in a substantially similar manner.
Referring first to the example screen shot 200 depicted in fig. 4A, the instruction window 202 partially overlays the digital map 204, with initial navigation instructions shown using markers showing the user's current location, the first waypoint, and the direction along which the user should move. As described above, when the user is stationary, it is difficult for the sensor of the portable device to reliably determine the user's orientation and, therefore, to assist the user in aligning the digital map 204 with the user's environment. In response to a user requesting navigation instructions, or in some implementations, the screenshot 200 may be generated directly after presenting an overview of the route to the user and beginning to present a series of navigation instructions in response to a user command.
The instruction window 202 in this example includes text referencing both navigation landmarks (banks) arranged in the direction of travel along a first guiding portion of the route and navigation landmarks (fountain) arranged in the opposite direction. Thus, the text instructs the user to walk to the bank and leave the fountain. In some implementations, the digital map 204 is interactive and accessible via direct contact, such as a tap. In response to a detected tap event within the digital map 204, the instruction window 202 may be dismissed.
As shown in fig. 4A, the instruction window 202 may include an icon depicting the type of navigation landmark referenced in the initial navigation instruction. The landmark selection module 44 or another module operating in the system of fig. 1 may select these icons from a list of available landmark types. The icons may be individually selectable to access additional information about the respective navigation landmarks. Alternatively, the user may select the dismissal instruction window 202 to cause the digital map 204 to occupy the entire screen, or in any case a larger portion of the screen.
Referring to fig. 4B and 4C, a user may select an icon corresponding to a bank to access an information screen 210, which information screen 210 may partially cover the digital map 204, similar to the instruction window 202. Selection of the icon may correspond to a tap event, a double tap event, or any suitable type of interaction. The information screen 210 may include a conventional (planar) photograph of the navigation landmark, a panoramic photograph of the navigation landmark, an interactive 3D view of the landmark, or any other available image. The information screen 210 may also include business data for navigating landmarks, if desired.
Next, block diagram 300 of fig. 5 schematically illustrates the use of remote and closest navigation landmarks in generating initial navigation instructions. In this example scenario, the current location of the portable device is represented by marker 302 and the destination of the navigation instructions is represented by marker 304. The route 306 between the current location and the destination is made up of a series of guiding parts including a first guiding part 310.
Referring to fig. 1 and 5, as a user of portable system 12 begins navigating along route 306, navigation system 18 may generate an initial navigation direction that references mountain 320 disposed away from location 302 (e.g., 40 miles) and gas station 322 disposed at the same intersection as represented by marker 302. More specifically, the navigation system 18 can generate text 350 to instruct the user of the portable system 12 to begin to leave the mountain 320 and drive or walk toward the gas station 322.
For greater clarity, fig. 6 illustrates an example method 400 for generating an initial navigation instruction that references two navigation landmarks arranged in different directions relative to a current location of a portable device. The method 400 may be implemented in the navigation system 18 discussed above. As a more specific example, the method 400 may be implemented in the navigation instruction generator 42 as a set of instructions stored on a computer-readable medium and executable by one or more processors.
At block 402, a first navigation landmark is selected within 360 degrees in view of the observability, salience, and uniqueness metrics discussed above. User preferences (e.g., "use billboards when available", "do not use auditory landmarks") may also be used to select from among a plurality of candidates. In general, the first navigation landmark may be selected in any direction relative to the direction of the first guiding portion of the series of guiding portions. For example, the first navigation landmark may be located on the left hand side relative to the direction of travel. In the example scenario discussed above with reference to fig. 4A-C, a bank disposed in a general direction serves as the first navigation landmark. In the example scenario discussed above with reference to fig. 5, the mountain serves as the first navigation landmark.
At block 404, although the initial navigation instruction may include a reference to only one landmark, the method 400 may include selecting a second navigation landmark within 360 degrees. The first and second navigation landmarks may be arranged in different directions relative to the initial direction of travel. At block 406, text is generated referencing the initial navigation instruction relative to the movement of the first navigation landmark and the second navigation landmark. For example, the movement may be away from the first navigation landmark and toward the second navigation landmark. In some implementations, text is generated according to the template "travel from < first navigation landmark > to < second navigation landmark > direction >".
At block 408, icons corresponding to the types of the first navigation landmark and the second navigation landmark are selected. As shown in fig. 4A-C, the icons may operate as selectable controls for retrieving additional information about the navigation landmarks. An initial navigational instruction is generated using the text generated at block 406 and the icon obtained at block 408 (block 410). The initial navigation instructions may then be provided to the portable system via the user interface for display or otherwise for output.
Updating navigation landmark database
FIG. 7 is a flowchart of an example method 450 for using photographs captured from certain locations to discern locations of navigation landmarks that may be used as initial navigation instructions. Referring back to fig. 1, for example, the method 450 may be implemented in the landmark selection module 44 as one of the techniques for populating the visual database 52. The method 450 may be implemented as a set of instructions stored on a computer-readable medium and executed by one or more processors.
Referring to fig. 7, a set of photographs of a landmark is identified at block 452. For example, a large number of people may share photographs of their washington d.c. national art museum building, and many people label ("tag") their photographs with the heading "national art museum". In addition, many of these users share an indication of where their photos were captured.
At block 454, the locations at which the plurality of photographs of the landmark were captured are determined. For example, photographs of national art buildings may be gathered in certain locations where people tend to capture photographs. As a more specific example, 40% of the 1000 example photographs may be captured at the first intersection, another 30% may be captured at the twenty-first intersection, and the remaining photographs may be scattered in a plurality of other locations. Thus, at block 454, the first and second cross points may be selected as locations where the landmark is visible.
At block 456, respective directions from the first and second crossroads to the national art building are determined using the map data. At block 458, a database (such as database 52 of fig. 1) is updated to indicate that at the first intersection, the national art museum building is a candidate navigation landmark, which is disposed in a direction from the first intersection. Similarly, the database may be updated to indicate that the building is also a candidate navigation landmark at the twentieth intersection, the candidate navigation landmarks being arranged in respective directions. The navigation system 18 of fig. 1 may then use these stored indications in selecting candidate navigation tags.
More generally, navigation system 18 also employs various techniques for populating database 52. Examples of such techniques include receiving an indication from an organizer of a temporary show, street bazaar, musical festival, etc. that is occurring at some location, or receiving a similar indication from a user according to crowdsourcing techniques to obtain an indication of a temporary event. For auditory landmarks, the user may report sound sources that exceed a certain noise level so that the navigation system 18 may generate an initial navigation instruction of the type "away from subway station to music".
Other considerations
Various operations of the example methods described herein may be performed, at least in part, by one or more processors that are temporarily configured (e.g., via software) or permanently configured to perform related operations. Whether temporarily or permanently configured, such a processor may constitute a processor-implemented module that operates to perform one or more operations or functions. In some example embodiments, the modules referred to herein may comprise processor-implemented modules.
Similarly, the methods or routines described herein may be at least partially processor-implemented. For example, at least some operations of the methods may be performed by one or more processors or processor-implemented hardware modules. The performance of certain operations may be distributed among one or more processors, residing not only within a single machine, but also across multiple machines. In some example embodiments, one or more processors may be located in a single location (e.g., within a home environment, an office environment, or as a server farm), while in other embodiments, processors may be distributed across multiple locations.
The one or more processors may also be operative to support performance of related operations in a cloud computing environment or as a software as a service (SaaS). For example, at least some of the operations may be performed by a set of computers (as examples of machines including processors) that are accessible via a network (e.g., the internet) and via one or more suitable interfaces (e.g., application Program Interfaces (APIs)).
Further alternative structural and functional designs of the system for generating an initial navigation direction will be appreciated by those of ordinary skill in the art upon reading this disclosure. Thus, while particular embodiments and applications have been illustrated and described, it is to be understood that the disclosed embodiments are not limited to the precise construction and components disclosed herein. Various modifications, changes and variations apparent to those skilled in the art may be made in the arrangement, operation and details of the methods and apparatus disclosed herein without departing from the spirit and scope as defined in the appended claims.
Claims (19)
1. A method for providing initial navigational guidance, the method comprising:
determining that the portable device is stationary at the current location;
Determining, by one or more processors, a route from the current location of the portable device to a destination, the route including a series of guide portions;
in response to determining that the portable device is stationary at the current location:
generating, by the one or more processors, navigation instructions to guide a user of the portable device along the route to the destination, comprising:
identifying candidate navigation landmarks that are perceivable within 360 degrees of the current location of the portable device,
selecting a navigation landmark from the candidate navigation landmarks arranged in a direction substantially opposite to the direction of the first guiding portion of the series of guiding portions, and
generating an initial navigation instruction in the navigation instruction, wherein the initial navigation instruction refers to the selected navigation landmark; and
such that the initial navigation instructions are provided via a user interface of the portable device.
2. The method of claim 1, wherein the navigation landmark selected is a first navigation landmark, and wherein generating the navigation instruction further comprises:
selecting a second navigation landmark from the candidate navigation landmarks arranged in a direction substantially aligned with the direction of the first guiding portion of the series of guiding portions, and
Generating the initial navigation instruction that also references the second navigation landmark.
3. The method of claim 2, wherein generating the initial navigation instruction includes traveling < direction > from < the first navigation landmark > to < the second navigation landmark > using a format.
4. The method of claim 2, wherein generating the initial navigational instruction further comprises:
for each of the first navigation landmark and the second navigation landmark, a first icon and a second icon are selected, respectively, each icon indicating the type of landmark,
annotating the initial navigation instruction with the first icon and the second icon.
5. The method of claim 1, wherein identifying the candidate navigation markers comprises:
accessing an electronic database storing photographs, annotating the photographs with (i) landmarks captured in the photographs and (ii) locations at which the photographs were taken,
using the electronic database to discern a set of photographs depicting the same landmark captured from the current location of the portable device, and
the landmark depicted in the set of photographs is selected as one of the candidate navigation landmarks.
6. The method of claim 1, wherein identifying the candidate navigational landmark comprises identifying an auditory navigational landmark corresponding to a source of an audio signal exceeding a noise floor.
7. The method of claim 1, wherein identifying the candidate navigation landmark comprises identifying a transient navigation landmark corresponding to a temporary geolocation event.
8. The method of claim 1, wherein identifying the candidate navigational landmark comprises identifying a natural feature as a candidate navigational landmark.
9. The method of claim 1, further comprising determining that the portable device is stationary at the current location.
10. A portable computing device, comprising:
a positioning module that determines a current geographic location of the portable computing device;
a user interface module;
processing hardware coupled to the positioning module and the user interface module; and
a non-transitory memory readable by the processing hardware and storing instructions thereon that, when executed by the processing hardware, cause the portable computing device to:
obtaining navigation instructions for guiding a user from the current geographic location of the portable computing device to a destination along a route including a series of guide portions, wherein the navigation instructions include initial navigation instructions that reference navigation landmarks arranged in a direction substantially opposite to a direction of a first guide portion of the series of guide portions, and
In response to determining that the portable computing device is stationary at the current geographic location, the initial navigation instructions are provided via the user interface module.
11. The portable computing device of claim 10, wherein the navigation landmark selected is a first navigation landmark, and wherein the initial navigation instruction further references a second navigation landmark arranged in a direction substantially aligned with a direction of the first guiding portion of the series of guiding portions.
12. The portable computing device of claim 10, wherein to provide the initial navigation instruction via the user interface, the instruction further causes the portable computing device to display a selectable icon corresponding to a type of the navigation landmark.
13. The portable computing device of claim 12, wherein the instructions further cause the portable computing device to display additional information related to the navigation landmark in response to selecting the icon via the user interface module.
14. A non-transitory computer-readable medium having instructions stored thereon that, when executed on one or more processors, are configured to:
Receiving a request for navigation instructions to direct a user of the portable computing device to a destination;
obtaining a route from a current location of the portable device to the destination in response to the request, the route including a series of guiding portions;
obtaining navigation instructions to guide a user of the portable device along the route to the destination, wherein the navigation instructions include initial navigation instructions that reference navigation landmarks arranged in a direction substantially opposite to a direction of a first guide portion of the series of guide portions; and
in response to determining that the portable computing device is stationary at the current location, the initial navigation instructions are provided for output via a user interface of the portable device.
15. The non-transitory computer readable medium of claim 14, wherein the instructions implement an Application Program Interface (API), wherein a third party software application executing on the portable device invokes the API to (i) specify the current location and the destination of the portable device, and (ii) obtain the initial navigation instructions and display the initial navigation instructions via the user interface of the portable device.
16. The non-transitory computer-readable medium of claim 15, wherein the instructions further implement a task executing on one or more web servers, wherein the task is configured to query a database to identify candidate navigation landmarks perceptible within 360 degrees of the current location of the portable device, wherein for each of the candidate navigation landmarks, the database stores one or more of (i) a measure of observability, (ii) a measure of salience, and (iii) a measure of uniqueness.
17. The non-transitory computer-readable medium of claim 14, wherein the navigation landmark selected is a first navigation landmark, and wherein the initial navigation instruction further references a second navigation landmark disposed in a direction substantially aligned with a direction of the first guiding portion of the series of guiding portions.
18. The non-transitory computer-readable medium of claim 17, wherein the initial navigational instruction is annotated with a first icon corresponding to a type of the first navigational landmark and a second icon corresponding to a type of the second navigational landmark.
19. The non-transitory computer-readable medium of claim 14, wherein the navigational landmark is one of (i) an auditory navigational landmark corresponding to a source of an audio signal that exceeds a noise floor, or (ii) a transient navigational landmark corresponding to a temporary geolocation event.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202311051255.6A CN117029868A (en) | 2016-10-26 | 2017-10-12 | System and method for using visual landmarks in initial navigation |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/335,100 | 2016-10-26 | ||
US15/335,100 US10168173B2 (en) | 2016-10-26 | 2016-10-26 | Systems and methods for using visual landmarks in initial navigation |
PCT/US2017/056254 WO2018080798A1 (en) | 2016-10-26 | 2017-10-12 | Systems and methods for using visual landmarks in initial navigation |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202311051255.6A Division CN117029868A (en) | 2016-10-26 | 2017-10-12 | System and method for using visual landmarks in initial navigation |
Publications (2)
Publication Number | Publication Date |
---|---|
CN109891195A CN109891195A (en) | 2019-06-14 |
CN109891195B true CN109891195B (en) | 2023-09-08 |
Family
ID=60162316
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780007032.6A Active CN109891195B (en) | 2016-10-26 | 2017-10-12 | System and method for using visual landmarks in initial navigation |
CN202311051255.6A Pending CN117029868A (en) | 2016-10-26 | 2017-10-12 | System and method for using visual landmarks in initial navigation |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202311051255.6A Pending CN117029868A (en) | 2016-10-26 | 2017-10-12 | System and method for using visual landmarks in initial navigation |
Country Status (6)
Country | Link |
---|---|
US (4) | US10168173B2 (en) |
EP (3) | EP4194814A1 (en) |
JP (3) | JP6495553B2 (en) |
CN (2) | CN109891195B (en) |
GB (1) | GB2556696B (en) |
WO (1) | WO2018080798A1 (en) |
Families Citing this family (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2018042862A (en) * | 2016-09-15 | 2018-03-22 | 株式会社三洋物産 | Game machine |
JP2018042860A (en) * | 2016-09-15 | 2018-03-22 | 株式会社三洋物産 | Game machine |
US10168173B2 (en) * | 2016-10-26 | 2019-01-01 | Google Llc | Systems and methods for using visual landmarks in initial navigation |
CN112368546A (en) | 2018-09-06 | 2021-02-12 | 谷歌有限责任公司 | Displaying personalized landmarks in mapping applications |
KR102542491B1 (en) | 2018-09-06 | 2023-06-12 | 구글 엘엘씨 | Navigation directions using familiar locations as intermediate destinations |
US20210240762A1 (en) * | 2018-10-22 | 2021-08-05 | Google Llc | Finding Locally Prominent Semantic Features for Navigation and Geocoding |
KR102096078B1 (en) * | 2018-12-05 | 2020-04-02 | 네이버랩스 주식회사 | Method, apparatus, system and computer program for providing route guide |
JP6995077B2 (en) * | 2019-03-29 | 2022-01-14 | 本田技研工業株式会社 | Road management device |
JP6995078B2 (en) | 2019-03-29 | 2022-01-14 | 本田技研工業株式会社 | Road management device |
JP6995080B2 (en) * | 2019-03-29 | 2022-01-14 | 本田技研工業株式会社 | Information acquisition device |
CN110081902A (en) * | 2019-05-14 | 2019-08-02 | 腾讯科技（深圳）有限公司 | Direction indicating method, device and terminal in navigation |
US20200393835A1 (en) * | 2019-06-17 | 2020-12-17 | Toyota Research Institute, Inc. | Autonomous rideshare rebalancing |
US11334174B2 (en) | 2019-07-18 | 2022-05-17 | Eyal Shlomot | Universal pointing and interacting device |
CN113015887A (en) * | 2019-10-15 | 2021-06-22 | 谷歌有限责任公司 | Navigation directions based on weather and road surface type |
CN113155117A (en) * | 2020-01-23 | 2021-07-23 | 阿里巴巴集团控股有限公司 | Navigation system, method and device |
CN113532456A (en) * | 2020-04-21 | 2021-10-22 | 百度在线网络技术（北京）有限公司 | Method and device for generating navigation route |
JP6828934B1 (en) * | 2020-08-18 | 2021-02-10 | 株式会社ビーブリッジ | Navigation devices, navigation systems, navigation methods, navigation programs |
CN112729310A (en) * | 2020-12-25 | 2021-04-30 | 张欣 | Indoor landmark object-based navigation method and system |
US20220316906A1 (en) * | 2021-04-03 | 2022-10-06 | Naver Corporation | Apparatus and Method for Generating Navigational Plans |
JP2023019344A (en) * | 2021-07-29 | 2023-02-09 | 株式会社アイシン | Information processing system |
US20230213351A1 (en) * | 2021-12-30 | 2023-07-06 | Omnieyes Co., Ltd. Taiwan Branch | System and method for navigation |
JP7438497B2 (en) * | 2022-03-23 | 2024-02-27 | 本田技研工業株式会社 | Information processing system and information processing method |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102168982A (en) * | 2010-02-26 | 2011-08-31 | 富士通天株式会社 | Navigation system, vehicle-mounted machine, program and navigation method |
CN103968846A (en) * | 2014-03-31 | 2014-08-06 | 小米科技有限责任公司 | Positioning and navigation method and device |
Family Cites Families (24)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH09304090A (en) | 1996-05-21 | 1997-11-28 | Canon Inc | Map information processor |
JP3587691B2 (en) * | 1998-07-30 | 2004-11-10 | 日本電信電話株式会社 | Navigation method and apparatus, and recording medium recording program for processing this method |
JP2002260160A (en) * | 2001-02-28 | 2002-09-13 | Toshiba Corp | Method and device for guidance |
US7383123B2 (en) | 2003-06-03 | 2008-06-03 | Samsung Electronics Co., Ltd. | System and method of displaying position information including an image in a navigation system |
JP2005099418A (en) | 2003-09-25 | 2005-04-14 | Casio Comput Co Ltd | Object display device and program |
CN103398718B (en) | 2004-03-23 | 2017-04-12 | 咕果公司 | Digital mapping system |
US7831387B2 (en) | 2004-03-23 | 2010-11-09 | Google Inc. | Visually-oriented driving directions in digital mapping system |
US7480567B2 (en) * | 2004-09-24 | 2009-01-20 | Nokia Corporation | Displaying a map having a close known location |
JP4675811B2 (en) | 2006-03-29 | 2011-04-27 | 株式会社東芝 | Position detection device, autonomous mobile device, position detection method, and position detection program |
JP4737549B2 (en) * | 2006-08-22 | 2011-08-03 | ソニー株式会社 | POSITION DETECTION DEVICE, POSITION DETECTION METHOD, POSITION DETECTION PROGRAM, AND NAVIGATION DEVICE |
EP1965172B1 (en) | 2007-03-02 | 2010-11-17 | Alpine Electronics, Inc. | Information display system and method for displaying information associated with map related data |
US8478515B1 (en) | 2007-05-23 | 2013-07-02 | Google Inc. | Collaborative driving directions |
US8224395B2 (en) | 2009-04-24 | 2012-07-17 | Sony Mobile Communications Ab | Auditory spacing of sound sources based on geographic locations of the sound sources or user placement |
US20110098910A1 (en) | 2009-10-22 | 2011-04-28 | Nokia Corporation | Method and apparatus for intelligent guidance using markers |
US8417448B1 (en) * | 2010-04-14 | 2013-04-09 | Jason Adam Denise | Electronic direction technology |
US8762041B2 (en) * | 2010-06-21 | 2014-06-24 | Blackberry Limited | Method, device and system for presenting navigational information |
US8698843B2 (en) * | 2010-11-02 | 2014-04-15 | Google Inc. | Range of focus in an augmented reality application |
US20150112593A1 (en) * | 2013-10-23 | 2015-04-23 | Apple Inc. | Humanized Navigation Instructions for Mapping Applications |
JP6258981B2 (en) | 2014-07-29 | 2018-01-10 | ヤマハ株式会社 | Program and information processing method |
JP2016099270A (en) * | 2014-11-25 | 2016-05-30 | セイコーエプソン株式会社 | Position calculation method, position calculation unit and position calculation program |
WO2017024308A1 (en) | 2015-08-06 | 2017-02-09 | Walc Inc. | Method and system for real-time sensory-based navigation |
US9464914B1 (en) * | 2015-09-01 | 2016-10-11 | International Business Machines Corporation | Landmark navigation |
CN105890608B (en) | 2016-03-31 | 2020-08-21 | 百度在线网络技术（北京）有限公司 | Navigation reference point determining method and device and navigation method and device |
US10168173B2 (en) * | 2016-10-26 | 2019-01-01 | Google Llc | Systems and methods for using visual landmarks in initial navigation |
-
2016
- 2016-10-26 US US15/335,100 patent/US10168173B2/en active Active
-
2017
- 2017-10-12 JP JP2018534579A patent/JP6495553B2/en active Active
- 2017-10-12 EP EP23154082.4A patent/EP4194814A1/en active Pending
- 2017-10-12 CN CN201780007032.6A patent/CN109891195B/en active Active
- 2017-10-12 EP EP20181744.2A patent/EP3734228A1/en not_active Withdrawn
- 2017-10-12 EP EP17788066.3A patent/EP3405751B1/en active Active
- 2017-10-12 WO PCT/US2017/056254 patent/WO2018080798A1/en active Application Filing
- 2017-10-12 CN CN202311051255.6A patent/CN117029868A/en active Pending
- 2017-10-26 GB GB201717605A patent/GB2556696B/en active Active
-
2018
- 2018-12-31 US US16/237,629 patent/US10739158B2/en active Active
-
2019
- 2019-03-05 JP JP2019039718A patent/JP6833886B2/en active Active
-
2020
- 2020-08-10 US US16/989,853 patent/US11604077B2/en active Active
-
2021
- 2021-01-21 JP JP2021007926A patent/JP7127165B2/en active Active
-
2023
- 2023-03-14 US US18/183,947 patent/US20230213350A1/en active Pending
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102168982A (en) * | 2010-02-26 | 2011-08-31 | 富士通天株式会社 | Navigation system, vehicle-mounted machine, program and navigation method |
CN103968846A (en) * | 2014-03-31 | 2014-08-06 | 小米科技有限责任公司 | Positioning and navigation method and device |
Also Published As
Publication number | Publication date |
---|---|
US10739158B2 (en) | 2020-08-11 |
GB2556696A (en) | 2018-06-06 |
US20200370914A1 (en) | 2020-11-26 |
US20190137295A1 (en) | 2019-05-09 |
US20180112993A1 (en) | 2018-04-26 |
CN109891195A (en) | 2019-06-14 |
US10168173B2 (en) | 2019-01-01 |
EP3405751B1 (en) | 2020-06-24 |
EP3734228A1 (en) | 2020-11-04 |
GB2556696B (en) | 2020-01-01 |
CN117029868A (en) | 2023-11-10 |
WO2018080798A1 (en) | 2018-05-03 |
EP4194814A1 (en) | 2023-06-14 |
JP2021063839A (en) | 2021-04-22 |
JP7127165B2 (en) | 2022-08-29 |
EP3405751A1 (en) | 2018-11-28 |
JP6495553B2 (en) | 2019-04-03 |
JP2019504316A (en) | 2019-02-14 |
US11604077B2 (en) | 2023-03-14 |
JP2019109252A (en) | 2019-07-04 |
JP6833886B2 (en) | 2021-02-24 |
GB201717605D0 (en) | 2017-12-13 |
US20230213350A1 (en) | 2023-07-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109891195B (en) | System and method for using visual landmarks in initial navigation | |
US10126141B2 (en) | Systems and methods for using real-time imagery in navigation | |
US8364397B2 (en) | Pictorial navigation | |
US11501104B2 (en) | Method, apparatus, and system for providing image labeling for cross view alignment | |
EP4089370B1 (en) | Method and device for verifying a current location and orientation of a user using landmarks | |
US20140288827A1 (en) | Guiding server, guiding method and recording medium recording guiding program | |
JP7485824B2 (en) | Method, computer device, and computer readable memory for verifying a user's current location or orientation using landmarks - Patents.com | |
Gautam et al. | Multimedia for mobile environment: image enhanced navigation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |