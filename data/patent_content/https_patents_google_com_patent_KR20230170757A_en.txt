KR20230170757A - Application-specific machine learning accelerator creation and global tuning - Google Patents
Application-specific machine learning accelerator creation and global tuning Download PDFInfo
- Publication number
- KR20230170757A KR20230170757A KR1020237039235A KR20237039235A KR20230170757A KR 20230170757 A KR20230170757 A KR 20230170757A KR 1020237039235 A KR1020237039235 A KR 1020237039235A KR 20237039235 A KR20237039235 A KR 20237039235A KR 20230170757 A KR20230170757 A KR 20230170757A
- Authority
- KR
- South Korea
- Prior art keywords
- architecture
- hardware
- accelerator
- neural network
- application
- Prior art date
Links
- 238000010801 machine learning Methods 0.000 title claims abstract description 128
- 238000013528 artificial neural network Methods 0.000 claims abstract description 117
- 238000000034 method Methods 0.000 claims abstract description 61
- 238000004364 calculation method Methods 0.000 claims abstract description 19
- 230000004044 response Effects 0.000 claims abstract description 13
- 238000012545 processing Methods 0.000 claims description 62
- 238000013507 mapping Methods 0.000 claims description 51
- 238000002922 simulated annealing Methods 0.000 claims description 10
- 238000004458 analytical method Methods 0.000 claims description 7
- 238000013461 design Methods 0.000 abstract description 86
- 238000005457 optimization Methods 0.000 description 39
- 230000008569 process Effects 0.000 description 22
- 230000001537 neural effect Effects 0.000 description 16
- 238000004422 calculation algorithm Methods 0.000 description 13
- 230000006870 function Effects 0.000 description 13
- 238000004590 computer program Methods 0.000 description 12
- 238000010586 diagram Methods 0.000 description 10
- 230000000750 progressive effect Effects 0.000 description 10
- 238000004891 communication Methods 0.000 description 9
- 230000004913 activation Effects 0.000 description 8
- 238000001994 activation Methods 0.000 description 8
- 230000015654 memory Effects 0.000 description 8
- 238000013527 convolutional neural network Methods 0.000 description 6
- 230000003993 interaction Effects 0.000 description 6
- 238000013139 quantization Methods 0.000 description 5
- 230000002123 temporal effect Effects 0.000 description 5
- 238000013459 approach Methods 0.000 description 4
- 238000003491 array Methods 0.000 description 4
- 230000015572 biosynthetic process Effects 0.000 description 4
- 238000003786 synthesis reaction Methods 0.000 description 4
- 238000012549 training Methods 0.000 description 4
- 230000009466 transformation Effects 0.000 description 4
- 239000003638 chemical reducing agent Substances 0.000 description 3
- 230000008901 benefit Effects 0.000 description 2
- 238000013499 data model Methods 0.000 description 2
- 238000012938 design process Methods 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 238000003062 neural network model Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000011176 pooling Methods 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000004088 simulation Methods 0.000 description 2
- 238000000844 transformation Methods 0.000 description 2
- 230000002411 adverse Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000000903 blocking effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000011156 evaluation Methods 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 238000012804 iterative process Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 238000003909 pattern recognition Methods 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000002787 reinforcement Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000000638 solvent extraction Methods 0.000 description 1
- 238000010561 standard procedure Methods 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000008685 targeting Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/042—Knowledge-based neural networks; Logical representations of neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/01—Dynamic search techniques; Heuristics; Dynamic trees; Branch-and-bound
Abstract
컴퓨터 판독 가능 매체를 포함한 방법, 시스템 및 장치가 ML 하드웨어 가속기를 글로벌적으로 튜닝하고 생성하기 위해 설명된다. 디자인 시스템은 베이스라인 프로세서 구성을 나타내는 아키텍처를 선택한다. 시스템의 ML 코스트 모델은 적어도 아키텍처가 여러 계층을 포함하는 신경망의 계산을 실행하는 방법을 모델링하여 아키텍처에 대한 성능 데이터를 생성한다. 성능 데이터에 기초하여, 아키텍처는 신경망을 구현하고 타겟 애플리케이션에 대한 기계 학습 계산을 실행할 때 성능 목표를 충족하도록 동적으로 튜닝된다. 아키텍처를 동적으로 튜닝하는 것에 응답하여, 시스템은 신경망의 여러 계층 각각을 구현하기 위한 맞춤형 하드웨어 구성을 지정하는 ML 가속기 구성을 생성한다.Methods, systems, and apparatus, including computer-readable media, are described for globally tuning and generating ML hardware accelerators. The design system selects an architecture that represents the baseline processor configuration. The ML cost model of a system generates performance data for an architecture by at least modeling how the architecture executes the computations of a neural network containing multiple layers. Based on performance data, the architecture is dynamically tuned to meet performance goals when implementing neural networks and executing machine learning calculations for target applications. In response to dynamically tuning the architecture, the system generates ML accelerator configurations that specify custom hardware configurations for implementing each of the multiple layers of the neural network.
Description
본 명세서는 일반적으로 기계 학습 계산을 수행하는 데 사용되는 집적 회로와 관련이 있다. This specification generally relates to integrated circuits used to perform machine learning calculations.
신경망은 하나 이상의 노드 계층을 사용하여 수신된 입력에 대한 출력(예: 분류)을 생성하는 기계 학습 모델이다. 일부 신경망에는 출력 계층 외에 하나 이상의 은닉 계층이 포함되어 있다. 일부 신경망은 이미지 처리용으로 구성된 CNN(컨벌루션 신경망)이거나 음성 및 언어 처리용으로 구성된 순환 신경망(RNN)일 수 있다. 다양한 유형의 신경망 아키텍처를 사용하여 분류 또는 패턴 인식, 데이터 모델링과 관련된 예측 및 정보 클러스터링과 관련된 다양한 태스크를 수행할 수 있다.A neural network is a machine learning model that uses one or more layers of nodes to generate an output (e.g. classification) for the input received. Some neural networks include one or more hidden layers in addition to the output layer. Some neural networks may be convolutional neural networks (CNNs) configured for image processing or recurrent neural networks (RNNs) configured for speech and language processing. Different types of neural network architectures can be used to perform a variety of tasks related to classification or pattern recognition, predictions related to data modeling, and clustering of information.
신경망 계층에는 해당 파라미터 또는 가중치 세트가 있을 수 있다. 가중치는 신경망 추론을 계산하기 위한 계층의 해당 출력을 생성하기 위해 신경망 계층을 통해 입력(예: 입력 배치)을 프로세싱하는 데 사용된다. 입력 배치(batch)와 커널 세트는 입력과 가중치의 다차원 배열인 텐서로 표현될 수 있다. 하드웨어 가속기는 신경망을 구현하기 위한 특수 목표의 집적 회로이다. 회로는 회로의 제어 논리를 사용하여 탐색하거나 액세스할 수 있는 텐서 요소에 해당하는 위치를 가진 메모리를 포함한다. A neural network layer may have a set of corresponding parameters or weights. Weights are used to process inputs (e.g. batches of inputs) through a neural network layer to produce the corresponding output of the layer for computing neural network inference. Input batches and kernel sets can be represented as tensors, which are multidimensional arrays of inputs and weights. Hardware accelerators are specially targeted integrated circuits for implementing neural networks. The circuit contains a memory whose locations correspond to tensor elements that can be navigated or accessed using the circuit's control logic.
특수 하드웨어 가속기를 설계하는 것은 작업 집약적이고 시간 소모적이다. 예를 들어 설계 프로세스에는 수개월의 노력이 필요한 경우가 많으며 여러 번의 설계 반복이 포함될 수 있다. 또한 애플리케이션-특정 성능 및 전력 목표를 충족하려면 설계 프로세스에 타겟 애플리케이션을 기본 하드웨어에 매핑하는 전략이 필요하다. 신경망의 계산 그래프는 정적이지만 매핑 작업에는 회로의 실제 성능에 영향을 미치는 여러 설계 파라미터가 포함될 수 있다. 또한 다양한 설정의 크기와 다양한 파라미터 간의 상호 관계로 인해 설계 공간을 수동으로 탐색하는 것이 종종 불가능하다.Designing specialized hardware accelerators is work-intensive and time-consuming. For example, the design process often requires months of effort and may include multiple design iterations. Additionally, to meet application-specific performance and power goals, the design process requires a strategy for mapping target applications to the underlying hardware. Although the computational graph of a neural network is static, the mapping operation can involve several design parameters that affect the actual performance of the circuit. Additionally, manual exploration of the design space is often impossible due to the size of the various settings and the interrelationships between the various parameters.
본 명세서에서는 데이터 프로세싱 아키텍처를 글로벌적으로 튜닝(globally-tuning)하고 튜닝된 아키텍처에 기초하여 애플리케이션-특정 기계 학습(ML) 가속기를 자동으로 생성하는 기술을 설명한다. 아키텍처는 애플리케이션 레벨 목표(application level objectives)의 세트에 기초하여 선택된 후보 아키텍처일 수 있다. 예시적인 애플리케이션 레벨 목표에는 프로세서 활용도, 전력 소비, 데이터 처리량 및 지연시간이 포함될 수 있다. 경우에 따라 목표는 사용자가 원하는 ML 가속기 예시의 성능 속성을 나타낸다. 목표 중 일부(또는 전부)는 예시적인 하드웨어 가속기 설계 시스템에 대한 사용자 입력으로 수신될 수 있다. 디자인 시스템은 사용자 입력과 관계없이 하나 이상의 목표를 결정할 수도 있다. This specification describes a technique for globally-tuning a data processing architecture and automatically generating application-specific machine learning (ML) accelerators based on the tuned architecture. The architecture may be a candidate architecture selected based on a set of application level objectives. Example application level goals may include processor utilization, power consumption, data throughput, and latency. In some cases, the goal represents the performance properties of the ML accelerator example that the user wants. Some (or all) of the goals may be received as user input to the example hardware accelerator design system. A design system may determine one or more goals independent of user input.
시스템은 애플리케이션 레벨 목표(예: 하나 이상의 입력)를 사용하여 후보 아키텍처를 글로벌적으로 튜닝하고 동적으로 최적화한다. 예를 들어, 전력 소비 및 프로세서 활용과 같은 영역에서 효율성을 실현하기 위해 특정 유형의 신경망을 실행하도록 아키텍처를 튜닝하고 최적화할 수 있다. 가속기 설계 시스템은 아키텍처별 코스트 모델을 사용하여 아키텍처의 다양한 양태를 튜닝한다. 코스트 모델의 출력은 가속기의 최종 구성을 정의하는 데 사용된다. 최적화 및 튜닝 후 시스템은 하드웨어에서 지정된 신경망을 구현하도록 최적화된 애플리케이션-특정(ML) 가속기를 생성하기 위해 예약/매핑 옵션을 포함한 다양한 아키텍처 특징을 포함하는 하드웨어 구성을 자동으로 생성한다. The system uses application-level goals (e.g., one or more inputs) to globally tune and dynamically optimize candidate architectures. For example, the architecture can be tuned and optimized to run specific types of neural networks to achieve efficiencies in areas such as power consumption and processor utilization. Accelerator design systems use architecture-specific cost models to tune various aspects of the architecture. The output of the cost model is used to define the final configuration of the accelerator. After optimization and tuning, the system automatically generates a hardware configuration containing various architectural features, including scheduling/mapping options, to create an application-specific (ML) accelerator optimized to implement the specified neural network in hardware.
본 명세서에 설명된 요지의 한 양태는 애플리케이션-특정 기계 학습(ML) 가속기를 생성하기 위한 컴퓨터로 구현되는 방법으로 구현될 수 있다. 이 방법은 베이스라인 프로세서 구성을 나타내는 아키텍처를 선택하고, ML 코스트 모델을 통해 적어도 아키텍처가 여러 계층을 포함하는 제1 신경망의 계산을 실행하는 방법을 모델링하여 아키텍처에 대한 성능 데이터를 생성하는 것을 포함한다. 이 방법에는 성능 데이터에 기초하여, 아키텍처가 제1 신경망을 구현하고 타겟 애플리케이션에 대한 기계 학습 계산을 실행할 때 성능 목표를 충족하도록 아키텍처를 동적으로 튜닝하는 단계가 포함된다. 이 방법에는 아키텍처를 동적으로 튜닝하는 것에 응답하여 ML 가속기의 구성을 생성하는 단계도 포함된다. 구성은 제1 신경망의 여러 계층 각각을 구현하기 위한 맞춤형 하드웨어 구성을 지정한다. One aspect of the subject matter described herein may be implemented in a computer-implemented method for creating an application-specific machine learning (ML) accelerator. The method includes selecting an architecture representing a baseline processor configuration and generating performance data for the architecture by modeling, through an ML cost model, how the architecture executes computations of at least a first neural network comprising multiple layers. . The method includes dynamically tuning the architecture, based on the performance data, to meet performance goals when the architecture implements a first neural network and executes machine learning calculations for a target application. The method also includes generating a configuration of the ML accelerator in response to dynamically tuning the architecture. The configuration specifies a custom hardware configuration for implementing each of the multiple layers of the first neural network.
이러한 구현과 기타 구현에는 다음 기능 중 하나 이상이 선택적으로 포함될 수 있다. 예를 들어, 일부 구현에서, 방법은 맞춤형 하드웨어 구성에 기초하여 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 단계를 더 포함한다. 또한, 신경망을 사용하여 타겟 애플리케이션에 대한 계산을 실행할 때 애플리케이션-특정 하드웨어 ML 가속기를 최적화하여 신경망의 다양한 계층을 각각 구현할 수 있다. These and other implementations may optionally include one or more of the following features: For example, in some implementations, the method further includes creating an application-specific hardware ML accelerator based on a custom hardware configuration. Additionally, when using a neural network to execute computations for a target application, application-specific hardware ML accelerators can be optimized to implement each of the various layers of the neural network.
성능 목표에는 여러 이산적 목표가 포함되며, 애플리케이션-특정 ML 가속기를 생성하는 것은, 애플리케이션-특정 하드웨어 ML 가속기가 타겟 애플리케이션에 대한 계산을 실행할 때, 복수의 이산적 목표 중 각각의 이산적 목표를 충족시키도록 구성된 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 것을 포함할 수 있다. 일부 구현에서, 성능 데이터를 생성하는 것은 ML 코스트 모델에 의해 제1 신경망의 다중 계층의 각 계층을 실행하기 위한 아키텍처의 사용을 모델링하는 것; 그리고 각 계층을 실행하기 위한 아키텍처 사용 모델링하는 것에 응답하여 ML 코스트 모델을 통해 각 계층에 대한 아키텍처의 성능 파라미터를 생성하는 것을 포함할 수 있다. 성능 파라미터는 복수의 이산적 목표(discrete objectives) 중 각각의 이산적 목표에 대응할 수 있다. 복수의 이산적 목표는 임계 프로세싱 지연시간, 임계 전력 소비, 임계 데이터 처리량, 임계 프로세서 활용 중 적어도 하나를 포함한다. 일부 구현에서, 아키텍처를 동적으로 튜닝하는 것은 애플리케이션-특정 하드웨어 ML 가속기가 하드웨어 ML 가속기의 하드웨어 컴퓨팅 유닛의 임계 백분율을 활용하게 하는 입력 텐서에 대한 계산의 매핑을 결정하는 것; 결정된 매핑에 기초하여 아키텍처를 동적으로 튜닝하는 것을 포함한다.Performance goals include multiple discrete goals, and creating an application-specific ML accelerator ensures that the application-specific hardware ML accelerator satisfies each of the multiple discrete goals when executing computations for the target application. It may include creating an application-specific hardware ML accelerator configured to In some implementations, generating performance data includes modeling the use of the architecture for executing each layer of multiple layers of the first neural network by an ML cost model; and generating performance parameters of the architecture for each layer through an ML cost model in response to modeling the architecture usage for executing each layer. Performance parameters may correspond to each discrete objective among a plurality of discrete objectives. The plurality of discrete goals include at least one of threshold processing latency, threshold power consumption, threshold data throughput, and threshold processor utilization. In some implementations, dynamically tuning the architecture includes determining a mapping of computations to input tensors that causes an application-specific hardware ML accelerator to utilize a critical percentage of the hardware compute units of the hardware ML accelerator; It involves dynamically tuning the architecture based on the determined mapping.
아키텍처를 동적으로 튜닝하는 것은 글로벌 튜너의 여러 ML 코스트 모델 각각에 의해 수행되는 오퍼레이션에 기초하여 아키텍처를 동적으로 튜닝하는 것, 그리고 글로벌 튜너의 시뮬레이트된 어닐링 튜너 또는 랜덤 튜너 중 적어도 하나에 의해 수행되는 오퍼레이션에 기초하여 아키텍처를 동적으로 튜닝하는 것을 포함할 수 있다. 일부 구현에서, 아키텍처는 집적 회로의 하나 이상의 하드웨어 블록을 나타내며, 아키텍처를 동적으로 튜닝하는 것은 아키텍처가 타겟 애플리케이션에 대한 계산을 실행하기 위해 제1 신경망을 구현할 때 하나 이상의 하드웨어 블록 각각에 대한 각각의 성능 목표를 충족하도록 아키텍처를 동적으로 튜닝하는 것을 포함한다.Dynamically tuning the architecture is based on operations performed by each of several ML cost models of the global tuner, and operations performed by at least one of the simulated annealing tuner or the random tuner of the global tuner. It may include dynamically tuning the architecture based on . In some implementations, the architecture represents one or more hardware blocks of an integrated circuit, and dynamically tuning the architecture includes adjusting the respective performance of each of the one or more hardware blocks when the architecture implements a first neural network to execute computations for the target application. It involves dynamically tuning the architecture to meet your goals.
하드웨어 ML 가속기의 구성은 제1 신경망에 대한 맞춤형 소프트웨어 구성을 지정하며; 그리고 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 것은 맞춤형 하드웨어 구성 및 맞춤형 소프트웨어 구성에 기초하여 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 것을 포함한다. 일부 구현에서, ML 코스트 모델은 하나 이상의 개별 분석 모델을 포함하는 아키텍처-웨어 코스트 모델이며; 그리고 아키텍처-웨어 코스트 모델은 아키텍처를 사용하여 프로세싱되는 데이터의 결정론적 데이터 흐름에 기초하여 아키텍처의 성능을 추정하도록 구성된다.The configuration of the hardware ML accelerator specifies a custom software configuration for the first neural network; And creating an application-specific hardware ML accelerator includes creating an application-specific hardware ML accelerator based on a custom hardware configuration and a custom software configuration. In some implementations, the ML cost model is an architecture-ware cost model that includes one or more individual analysis models; And the architecture-wear cost model is constructed to estimate the performance of the architecture based on the deterministic data flow of data processed using the architecture.
이 양태와 다른 양태의 다른 구현에는 방법의 동작을 수행하도록 구성되고 컴퓨터 저장 장치에 인코딩된 대응 시스템, 장치 및 컴퓨터 프로그램이 포함된다. 하나 이상의 컴퓨터로 구성된 시스템은 작동 시 시스템이 동작 세트를 수행하도록 하는 시스템에 설치된 소프트웨어, 펌웨어, 하드웨어 또는 이들의 조합에 의해 구성될 수 있다. 하나 이상의 컴퓨터 프로그램은 데이터 프로세싱 장치에 의해 실행될 때 장치가 통작 세트를 수행하게 하는 명령어를 가짐으로써 구성될 수 있다.Other implementations of this and other aspects include corresponding systems, devices, and computer programs encoded in a computer storage device and configured to perform the operations of the methods. A system comprised of one or more computers may be comprised of software, firmware, hardware, or a combination thereof installed on the system that causes the system to perform a set of operations when activated. One or more computer programs may consist of instructions that, when executed by a data processing device, cause the device to perform a set of operations.
본 명세서에 설명된 요지는 다음 장점 중 하나 이상을 실현하기 위해 특정 실시예에서 구현될 수 있다. The subject matter described herein may be implemented in certain embodiments to realize one or more of the following advantages.
개시된 기술은 하드웨어 회로에서 신경망을 구현하기 위한 오퍼레이션의 효율적인 스케줄링/매핑을 포함하여, 최적화된 하드웨어 및 소프트웨어 구성을 정의하기 위한 아키텍처 탐색 프로세스를 촉진하는 데 사용될 수 있는 프레임워크를 제공한다. 이 프로세스에 기초하여, 하드웨어 설계 시스템은 주어진 PPA(성능(performance), 전력(power), 영역(area)) 제약 세트에 대해 시스템별로 최적화된 하드웨어 매핑을 정의하는 출력 구성을 자동으로 생성할 수 있다. PPA 제약 조건은 최소한 프로세서 활용도(processor utilization), 전력 소비, 지연시간(latency), 블록 크기 및/또는 데이터 처리량과 관련된 하드웨어 가속기 성능 임계값일 수 있다. The disclosed technology provides a framework that can be used to facilitate the architecture exploration process to define optimized hardware and software configurations, including efficient scheduling/mapping of operations for implementing neural networks in hardware circuits. Based on this process, a hardware design system can automatically generate an output configuration that defines a system-specific optimized hardware mapping for a given set of performance, power, and area (PPA) constraints. . PPA constraints may at a minimum be hardware accelerator performance thresholds related to processor utilization, power consumption, latency, block size, and/or data throughput.
설계 시스템은 고정된 수의 계층으로 예시적인 네트워크 모델을 식별하고 블록 연결, 하드웨어 레이아웃 또는 메모리와 같은 마이크로-아키텍처의 속성을 포함하여 식별된 하드웨어 아키텍처의 최적 속성(예: 싸이스톨릭 어레이(systolic array), 계산 타일 등)을 결정할 수 있다. 이러한 최적화된 하드웨어 속성 외에도, 설계 시스템은 계층별 프로세싱을 위한 효율적인 스케줄링 및 데이터 할당을 결정하므로, 따라서 계층별 프로세싱에 대한 사용자 또는 시스템 정의 요구 사항을 충족(또는 초과)하는 동시에 최소한의 전력 및 회로 영역을 소비하도록 애플리케이션-특정 ML 가속기를 생성할 수 있다. The design system identifies an exemplary network model with a fixed number of layers and determines optimal properties of the identified hardware architecture, including properties of the micro-architecture such as block connectivity, hardware layout, or memory, such as a systolic array. ), calculation tiles, etc.) can be determined. In addition to these optimized hardware properties, the design system determines efficient scheduling and data allocation for layer-by-layer processing, thus meeting (or exceeding) user- or system-defined requirements for layer-by-layer processing while minimizing power and circuit area. You can create application-specific ML accelerators to consume .
본 명세서에 설명된 요지의 하나 이상의 구현의 세부 사항은 첨부 도면 및 아래 설명에 설명되어 있다. 요지의 다른 잠재적인 특징, 양태 및 이점은 설명, 도면 및 청구범위로부터 명백해질 것이다.The details of one or more implementations of the subject matter described herein are set forth in the accompanying drawings and the description below. Other potential features, aspects and advantages of the subject matter will become apparent from the description, drawings and claims.
도 1은 기계 학습 가속기를 생성하고 글로벌적으로 튜닝하기 위한 예시적인 컴퓨팅 시스템의 블록도이다.
도 2는 애플리케이션-특정 기계 학습 가속기를 글로벌적으로 튜닝하기 위한 예시적인 시스템을 보여주는 블록도이다.
도 3은 다층 신경망을 튜닝하기 위한 예시적인 프레임워크를 도시한다.
도 4는 다층 신경망의 그래프 실행 스케쥴을 튜닝하고 최적화하기 위한 예시적인 프로세스의 흐름도이다.
도 5는 기계 학습 가속기를 생성하고 글로벌적으로 튜닝하는 데 사용되는 예시적인 프로세스의 흐름도이다.
도 6은 도 1의 시스템을 사용하여 생성된 예시적인 애플리케이션-특정 하드웨어 가속기의 블록도이다.
도 7은 입력 텐서, 가중치 텐서 및 출력 텐서의 예를 도시한다.
다양한 도면에서 유사한 참조 번호 및 명칭은 유사한 요소를 나타낸다.1 is a block diagram of an example computing system for creating and globally tuning machine learning accelerators.
Figure 2 is a block diagram showing an example system for globally tuning an application-specific machine learning accelerator.
3 shows an example framework for tuning a multilayer neural network.
4 is a flow diagram of an example process for tuning and optimizing the graph execution schedule of a multilayer neural network.
Figure 5 is a flow diagram of an example process used to create and globally tune a machine learning accelerator.
FIG. 6 is a block diagram of an example application-specific hardware accelerator created using the system of FIG. 1.
Figure 7 shows examples of input tensors, weight tensors, and output tensors.
Like reference numbers and designations in the various drawings indicate like elements.
도 1은 예시적인 하드웨어 가속기 설계(디자인) 시스템(100)("시스템(100)")의 블록도이다. 일반적으로, 시스템(100)은 프로세서(예를 들어, 중앙 프로세싱 장치(CPU), 그래픽 프로세싱 장치(GPU), 특수 목표 프로세서 등), 메모리 및/또는 맞춤형(customized) 하드웨어 기계 학습 가속기를 글로벌적으로 튜닝하고 생성하기 위한 기능을 실행하는 데 사용되는 프로세싱 리소스를 집합적으로 형성하는 데이터 저장 장치를 포함한다. 1 is a block diagram of an exemplary hardware accelerator design (design) system 100 (“System 100”). Typically, system 100 includes processors (e.g., central processing units (CPUs), graphics processing units (GPUs), special target processors, etc.), memory, and/or customized hardware machine learning accelerators globally. It includes data storage devices that collectively form processing resources used to execute functions for tuning and generating.
아래에 설명된 바와 같이, 하나 이상의 입력 목표(input objectives)(102)를 사용하여 시스템(100)은 예시적인 하드웨어 가속기를 생성하기 위한 설계 구성을 개발하고 출력하도록 구성된다. 하드웨어 가속기는 특정 유형의 기계 학습 태스크를 실행하도록 최적화된 특수 목표 또는 애플리케이션-특정 하드웨어 회로로 구현될 수 있다. 예를 들어, 애플리케이션-특정 회로는 다층 신경망을 구현하거나 실행하도록 구성된 ML(기계 학습) 하드웨어 가속기일 수 있다. As described below, using one or more input objectives 102, system 100 is configured to develop and output a design configuration for creating an example hardware accelerator. Hardware accelerators can be implemented as special target or application-specific hardware circuits optimized to execute specific types of machine learning tasks. For example, an application-specific circuit could be a machine learning (ML) hardware accelerator configured to implement or run a multilayer neural network.
더 구체적으로, 애플리케이션-특정(application-specific) 회로는 사용자에 의해 지정된 하나 이상의 입력과 같은 다양한 애플리케이션 목표에 따라 고유하게 튜닝 및/또는 최적화될 수 있다. 예를 들어, 특정 유형의 신경망(예: 다층 CNN)을 구현할 때, 애플리케이션-특정 ML 회로에 대한 후보 데이터 프로세싱 아키텍처는 프로세서 활용도, 전력 소비, 데이터 처리량 및/또는 지연시간과 관련된 임계 성능 목표(threshold performance objectives)를 달성(또는 초과)하도록 최적화될 수 있다. More specifically, application-specific circuits may be uniquely tuned and/or optimized according to various application goals, such as one or more inputs specified by the user. For example, when implementing a particular type of neural network (e.g., a multilayer CNN), candidate data processing architectures for application-specific ML circuits may have critical performance goals related to processor utilization, power consumption, data throughput, and/or latency. Can be optimized to achieve (or exceed) performance objectives.
이 문서에서 사용된 데이터 프로세싱 "아키텍처"는 하드웨어 회로 아키텍처, 소프트웨어/신경 아키텍처 또는 둘 다를 의미할 수 있다. 이러한 방식으로 아키텍처 튜닝 및 최적화에는 하드웨어 아키텍처의 튜닝 속성뿐만 아니라 신경 아키텍처의 튜닝 속성도 포함될 수 있다. 결과적인 아키텍처는 시스템(100)에 의해 수신되거나 결정될 수 있는 각각의 서로 다른 애플리케이션 목표에 따라 주어진 기계 학습 태스크를 수행하도록 최적화(예를 들어, 완전히 최적화)된다.As used in this document, data processing "architecture" may mean hardware circuit architecture, software/neural architecture, or both. In this way, architectural tuning and optimization can include tuning properties of the neural architecture as well as tuning properties of the hardware architecture. The resulting architecture is optimized (e.g., fully optimized) to perform a given machine learning task according to each different application goal that may be received or determined by system 100.
시스템(100)은 디자인 공간(104)을 구성하고 관리하기 위한 제어 로직을 포함한다. 디자인 공간(104)은 시스템(100)에서 실행되는 하드웨어 장치와 소프트웨어 루틴의 조합에 기초하여 구성될 수 있다. 예를 들어, 제어 로직은 다양한 설계 공간 오퍼레이션을 관리하기 위해 프로그래밍된 명령어를 실행하는 시스템 제어기 또는 호스트 장치로 구현될 수 있다. 디자인 공간(104)의 오퍼레이션은 후보 아키텍처를 튜닝하는데 필요한 복수의 디자인 아이템 또는 파라미터를 프로세싱하는 것을 포함할 수 있다. System 100 includes control logic to configure and manage design space 104. Design space 104 may be constructed based on a combination of hardware devices and software routines executing on system 100. For example, control logic may be implemented as a system controller or host device that executes programmed instructions to manage various design space operations. Operations of design space 104 may include processing a plurality of design items or parameters necessary to tune a candidate architecture.
일반적으로, 시스템(100)은 제어 로직을 사용하여 디자인 공간(104)의 활동 및 오퍼레이션을 관리한다. 주어진 ML 태스크에 대한 아키텍처를 최적화하는 것 외에도 일부 구현에서는 시스템(100)의 제어 로직 자체가 ML 모델에 기초하여 할 수 있다. 예를 들어, ML 모델은 입력 목표(input objectives)의 세트에 기초하여 후보 아키텍처를 튜닝하는 데 필요한 설계 입력 및 제어 파라미터를 프로세싱하도록 훈련될 수 있다. 일부 구현에서, 제어 로직은 예시적인 코스트 모델(아래 설명됨)에 의해 수행되는 오퍼레이션뿐만 아니라 입력 목표 세트에 따라 후보 아키텍처를 튜닝하는 예시적인 최적화 알고리즘을 실행하거나 적용한다.Generally, system 100 uses control logic to manage the activities and operations of design space 104. In addition to optimizing the architecture for a given ML task, in some implementations the control logic of system 100 itself may be based on an ML model. For example, a ML model can be trained to process design input and control parameters needed to tune a candidate architecture based on a set of input objectives. In some implementations, the control logic executes or applies example optimization algorithms that tune candidate architectures according to a set of input objectives as well as operations performed by example cost models (described below).
후보 아키텍처는 적어도 시스템(100)의 아키텍처 저장소(106)에서 선택된다. 시스템(100)은 적어도 입력 객체(102)에 기초하여 아키텍처 저장소(106)로부터 후보 아키텍처를 식별하거나 선택할 수 있다. 아키텍처 저장소(106)는 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 데 사용되는 복수의 서로 다른 하드웨어 아키텍처를 설명하는 정보를 포함한다. A candidate architecture is selected from at least the architecture repository 106 of system 100. System 100 may identify or select a candidate architecture from architecture repository 106 based at least on input object 102 . Architecture repository 106 contains information describing a plurality of different hardware architectures used to create application-specific hardware ML accelerators.
예를 들어, 아키텍처 저장소(106)를 통해 액세스되는 제1 하드웨어 아키텍처는 싸이스톨릭 어레이(배열) 아키텍처(systolic array architecture)를 정의할 수 있는 반면, 아키텍처 저장소(106)를 통해 액세스되는 제2의 다른 하드웨어 아키텍처는 컴퓨팅 타일의 배열에 기초하여 하드웨어 아키텍처를 정의할 수 있다. 유사하게, 아키텍처 저장소(106)를 통해 액세스되는 제3 아키텍처는 이산적 벡터(distinct vector) 프로세싱 장치(VPU)를 형성하는 긴밀하게 결합된 데이터 프로세싱 레인의 각각의 세트에 기초하여 하드웨어 아키텍처를 정의할 수 있는 반면, 아키텍처 저장소(106)를 통해 액세스되는 제4 아키텍처는 대형 공유 스크래치패드 메모리(large shared scratchpad memory) 및 매트릭스 계산 유닛과 상호작용하는 적어도 2개의 벡터 프로세서 코어를 포함하는 하드웨어 아키텍처를 정의할 수 있다. For example, a first hardware architecture accessed through architecture repository 106 may define a systolic array architecture, while a second hardware architecture accessed through architecture repository 106 Other hardware architectures may define hardware architectures based on the arrangement of computing tiles. Similarly, a third architecture accessed through architecture repository 106 may define a hardware architecture based on each set of tightly coupled data processing lanes forming a discrete vector processing unit (VPU). While a fourth architecture accessed through architecture repository 106 may define a hardware architecture that includes at least two vector processor cores interacting with a large shared scratchpad memory and a matrix computation unit. You can.
최적화 및 튜닝을 위해 선택된 후보 아키텍처는 예를 들어 아키텍처 저장소(106)로부터 획득된 하드웨어 회로 아키텍처와 신경 아키텍처의 조합일 수 있다. 신경 아키텍처는 복수의 서로 다른 유형의 신경망 그래프를 포함하는 네트워크 그래프 모듈(108)로부터 획득될 수 있다. 예를 들어, 시스템(100)은 입력 목표(102), 집적 회로(IC)의 예시적인 하드웨어 레이아웃, 예시적인 신경망 그래프에 기초하여 후보 아키텍처를 선택할 수 있다. The candidate architecture selected for optimization and tuning may be a combination of a neural architecture and a hardware circuit architecture obtained from architecture repository 106, for example. The neural architecture may be obtained from the network graph module 108, which includes a plurality of different types of neural network graphs. For example, system 100 may select a candidate architecture based on input target 102, an example hardware layout of an integrated circuit (IC), and an example neural network graph.
일부 구현에서, 시스템(100)은 주어진 신경망 아키텍처에 대한 특정 하드웨어 아키텍처의 선택 쪽으로 시스템을 바이어스시키는 하나 이상의 입력 목표(102)에 기초하여 후보 아키텍처를 선택한다. 예를 들어, 시스템(100)은 하나 이상의 하드웨어 변수에 기초하여 후보 아키텍처를 선택할 수 있다. 하드웨어 변수는 아키텍처 선택을 제한하고 디자인 공간(104)이 예를 들어 그래프 모듈(108)로부터 획득된 주어진 신경 아키텍처에 대해 저장소(106)로부터 특정 유형의 하드웨어 아키텍처를 선택하게 하는 제어 파라미터를 나타낼 수 있다. In some implementations, system 100 selects a candidate architecture based on one or more input objectives 102 that bias the system toward selection of a particular hardware architecture for a given neural network architecture. For example, system 100 may select a candidate architecture based on one or more hardware variables. Hardware variables may represent control parameters that constrain architecture selection and cause design space 104 to select a particular type of hardware architecture from storage 106 for a given neural architecture, e.g., obtained from graph module 108. .
시스템(100)은 예시적인 데이터 프로세싱 아키텍처를 글로벌적으로 튜닝하기 위해 하나 이상의 코스트 모델과 상호작용하는 최적화 및 튜닝 모듈(112)을 포함한다. 예를 들어, 시스템(100)은 하나 이상의 개별 데이터 모델(114)을 포함할 수 있는 아키텍처-웨어(architecture-aware) 코스트 모델(114)을 포함한다. 일부 경우에, 이들 개별 데이터 모델 각각은 입력 목표의 세트에 기초하여 후보 아키텍처를 튜닝하기 위해 ML 기반 분석을 실행하도록 구성된 각각의 코스트 모델(114)이다. 아키텍처-웨어 코스트 모델(114)은 아키텍처를 사용하여 프로세싱되는 데이터의 결정론적 데이터 흐름에 기초하여 후보 아키텍처의 성능을 추정한다. System 100 includes an optimization and tuning module 112 that interacts with one or more cost models to globally tune the example data processing architecture. For example, system 100 includes an architecture-aware cost model 114, which may include one or more individual data models 114. In some cases, each of these individual data models is a respective cost model 114 configured to run ML-based analysis to tune a candidate architecture based on a set of input objectives. The architecture-wear cost model 114 estimates the performance of a candidate architecture based on the deterministic data flow of data processed using the architecture.
일부 구현에서, 시스템(100)은 두 가지 유형의 코스트 모델, 즉 분석 코스트 모델 또는 ML 기반 코스트 모델 중 하나에 기초한 각각의 코스트 모델(114)을 포함한다. 아래 설명된 최적화 루프에서 설명한 것처럼 두 모델 모두 동일한 입력을 받고 동일한 출력을 생성할 수 있다. 일반적으로 이 두 가지 코스트 모델 유형의 차이점은 각 모델이 내부적으로 코스트를 예측하는 방식이다. 분석 코스트 모델과 ML 기반 코스트 모델에는 다양한 차이점이 있다.In some implementations, system 100 includes each cost model 114 based on one of two types of cost models: analytical cost models or ML-based cost models. As demonstrated in the optimization loop described below, both models can take the same input and produce the same output. In general, the difference between these two types of cost models is how each model predicts costs internally. There are various differences between analytical cost models and ML-based cost models.
예를 들어, 분석 코스트 모델은 일련의 하드웨어 매핑 파라미터와 신경망 그래프에 기초하여 다양한 "세일링(ceilings)"을 고려하는 루프라인(roofline) 기반 모델일 수 있다. 분석 코스트 모델에는 훈련 데이터가 필요하지 않다. 주어진 입력을 통해 분석 코스트 모델은 "내부 로직(internal logic)"를 사용하여 병목 현상(bottlenecks)을 도출하고 코스트를 출력한다. 내부적으로 분석 코스트 모델을 구현하는 데 사용되는 하나 이상의 하드웨어 블록을 구성하여 "코스트 모듈(cost module)"을 공유할 수 있다. For example, an analytical cost model could be a roofline-based model that considers various "ceilings" based on a set of hardware mapping parameters and a neural network graph. The analytical cost model does not require training data. Given the input, the analytical cost model uses “internal logic” to derive bottlenecks and output costs. Internally, a "cost module" can be shared, comprising one or more hardware blocks used to implement an analytical cost model.
공유 코스트 모듈은 하드웨어 매핑 파라미터와 하드웨어 블록에서 실행되는 신경망 계산을 고려하여 코스트를 생성하도록 작동 가능하다. 경우에 따라 분석 코스트 모델은 결정적 데이터 흐름이 있는 애플리케이션에 대해 특히 정확한 코스트 출력을 생성한다. The shared cost module is operable to generate costs by considering hardware mapping parameters and neural network calculations running on hardware blocks. In some cases, analytical cost models produce cost outputs that are particularly accurate for applications with deterministic data flows.
ML 기반 코스트 모델에는 최소한 지연시간과 처리량을 예측할 수 있는 기계 학습 모델을 훈련하기 위해 레이블이 지정된 데이터가 필요하다. 예를 들어, 기계 학습 모델을 학습(훈련)하여 하나 이상의 PPA 제약 조건을 포함하여 다양한 애플리케이션 레벨 목표에 대한 코스트 값을 예측할 수 있다. ML 기반 코스트 모델은 지도 학습(supervised learning) 및 다단계 퍼셉트론(multi-level perceptrons)을 사용하여 구현할 수 있다. 일부 구현에서, ML 기반 코스트 모델의 훈련 데이터가 높은 레벨의 합성 및 RTL 시뮬레이션을 통해 획득된다. 입력의 이산적 특성(discrete nature)을 극복하기 위해, ML 기반 코스트 모델의 입력을 확률적 경사하강법(stochastic gradient descent)과 같은 표준 기술을 사용하여 학습된 임베딩으로 변환할 수 있다. 일부 경우, ML 기반 코스트 모델이 오프라인으로 학습된다. 학습된 ML 기반 코스트 모델은 최적화 루프(아래 설명) 중에 사용되어 후보 아키텍처를 동적으로 최적화한다. ML-based cost models require labeled data to train a machine learning model that can at least predict latency and throughput. For example, a machine learning model can be learned (trained) to predict cost values for various application-level goals, including one or more PPA constraints. ML-based cost models can be implemented using supervised learning and multi-level perceptrons. In some implementations, training data for ML-based cost models are obtained through high-level synthesis and RTL simulations. To overcome the discrete nature of the input, the input of an ML-based cost model can be transformed into learned embeddings using standard techniques such as stochastic gradient descent. In some cases, ML-based cost models are trained offline. The learned ML-based cost model is used during the optimization loop (described below) to dynamically optimize the candidate architecture.
최적화 및 튜닝 모듈(112)과 코스트 모델 세트(114) 각각은 설계 공간(104)의 확장으로서 기능할 수 있다. 일부 구현에서, 최적화 및 튜닝 모듈(112)과 코스트 모델 세트(114)는 후보 아키텍처의 신경망과 하드웨어 블록 모두의 속성을 튜닝하는 글로벌 튜너(global tuner)를 나타낸다. 디자인 공간(104)의 제어 로직은 글로벌 튜너의 동작을 제어하거나 관리하는데 사용될 수 있다. 예를 들어, 글로벌 튜너는 제어 로직을 사용하여 생성된 제어 신호에 기초하여 후보 아키텍처를 튜닝하기 위해 디자인 공간(104)의 다양한 에스펙트(aspects)(예를 들어, 변수 및 제약조건)과 상호작용할 수 있다. 이에 대해서는 도 2를 참조하여 아래에서 더 자세히 설명한다. Optimization and tuning module 112 and cost model set 114 may each function as an extension of design space 104. In some implementations, optimization and tuning module 112 and cost model set 114 represent a global tuner that tunes properties of both the neural network and hardware blocks of the candidate architecture. Control logic in design space 104 may be used to control or manage the operation of the global tuner. For example, the global tuner may interact with various aspects of the design space 104 (e.g., variables and constraints) to tune candidate architectures based on control signals generated using control logic. You can. This will be explained in more detail below with reference to FIG. 2.
최적화 및 튜닝 모듈(112)은 예시적인 튜너(116) 및 예시적인 스케줄러/매퍼(scheduler/mapper)(118)를 포함한다. 일부 구현에서, 튜너(116)와 스케줄러/매퍼(118)는 모듈(112)의 예시적인 튜닝 및 최적화 태스크(아래 설명됨)를 실행하기 위해 상호작용한다. 위에서 언급한 바와 같이, 데이터 프로세싱 아키텍처는 예를 들어 아키텍처 저장소(106)로부터 획득된 하드웨어 회로 아키텍처와 신경망 그래프 모듈(108)로부터 획득된 신경 아키텍처의 조합일 수 있다. 하드웨어 아키텍처에는 싸이스톨릭 배열(어레이) 셀(systolic array cells), 벡터 프로세서 레인 또는 개별 컴퓨팅 타일과 같은 하드웨어 기능을 각각 포함하는 여러 개별 하드웨어 블록이 포함될 수 있다. Optimization and tuning module 112 includes an exemplary tuner 116 and an exemplary scheduler/mapper 118. In some implementations, tuner 116 and scheduler/mapper 118 interact to execute example tuning and optimization tasks (described below) of module 112. As mentioned above, the data processing architecture may be a combination of a hardware circuit architecture obtained from architecture repository 106 and a neural architecture obtained from neural network graph module 108, for example. A hardware architecture may include several individual hardware blocks, each containing hardware functionality such as systolic array cells, vector processor lanes, or individual compute tiles.
튜너(116)와 스케줄러/매퍼(118)는 i) 신경망 계층의 후보 매핑을 하나 이상의 하드웨어 블록으로 구성하고, ii) 이 후보 매핑을 위해 하나 이상의 애플리케이션 목표(102)에 기초하여 각 하드웨어 블록의 각각의 마이크로-아키텍처(micro-architecture)를 튜닝하도록 협력한다. 이러한 방식으로, 최적화 및 튜닝 모듈(112)은 주어진 하드웨어 블록이 신경망의 하나 이상의 계층을 실행하도록 최적화되도록 각 하드웨어 블록의 각각의 마이크로-아키텍처를 튜닝하도록 구성된다.The tuner 116 and the scheduler/mapper 118 i) configure a candidate mapping of the neural network layer into one or more hardware blocks, and ii) configure each of the hardware blocks based on one or more application goals 102 for this candidate mapping. Collaborate to tune the micro-architecture of In this manner, optimization and tuning module 112 is configured to tune the respective micro-architecture of each hardware block such that a given hardware block is optimized to execute one or more layers of the neural network.
원하는 성능 목표를 달성하기 위해, 최적화 및 튜닝 모듈(112)은 아키텍처-웨어 코스트 모델(114)과 상호작용하여 후보 매핑을 구성하고 각 하드웨어 블록의 마이크로-아키텍처를 튜닝하는 프로세스를 반복할 수 있다. 이러한 튜닝 반복은 예를 들어 최적화 및 튜닝 모듈(112)로부터 설계 공간(104)까지 선택적 데이터 경로(120)를 통한 신호 통신을 포함할 수 있다. 통신은 예를 들어 코스트 모델(114)에 의해 생성된 성능 추정에 기초하여 후보 아키텍처의 하드웨어 블록을 강화하기 위한 새로운 입력, 변수, 제약 또는 아키텍처 특징을 얻는 것일 수 있다. 시스템(100)은 반복 프로세스를 나타내는 튜닝 루프(122)를 포함할 수 있다. To achieve desired performance goals, optimization and tuning module 112 may interact with architecture-wear cost model 114 to construct candidate mappings and iterate the process to tune the micro-architecture of each hardware block. These tuning iterations may include, for example, signal communication over optional data path 120 from optimization and tuning module 112 to design space 104. The communication may be to obtain new inputs, variables, constraints or architectural features to enhance hardware blocks of the candidate architecture based on performance estimates generated by cost model 114, for example. System 100 may include a tuning loop 122 that represents an iterative process.
시스템(100)은 디자인 공간(104), 최적화 및 튜닝 모듈(112), 아키텍처-웨어(architecture-ware) 코스트 모델(114)의 프로세싱 오퍼레이션에 기초하여 예시적인 출력 구성(130)을 생성한다. 아래 설명된 바와 같이, 시스템(100)은 출력 구성(130)에 기초하여 애플리케이션-특정(application-specific) ML 하드웨어 가속기(예를 들어, 집적 회로)를 자동으로 생성할 수 있다.System 100 generates an example output configuration 130 based on processing operations of design space 104, optimization and tuning module 112, and architecture-ware cost model 114. As described below, system 100 may automatically generate an application-specific ML hardware accelerator (e.g., integrated circuit) based on output configuration 130.
도 2는 글로벌 튜너(202)를 포함하는 예시적인 시스템(200)을 도시하는 블록도이다. 일부 경우에 시스템(200)은 하나 이상의 프로세싱 장치에 의해 실행될 수 있는 프로그래밍된 명령어를 갖는 소프트웨어/계산 모듈 또는 하드웨어 회로의 서브시스템으로서 시스템(100) 내에 포함된다. 2 is a block diagram illustrating an example system 200 including a global tuner 202. In some cases, system 200 is included within system 100 as a subsystem of software/computation modules or hardware circuits having programmed instructions that can be executed by one or more processing devices.
시스템(200)의 동작은 타겟 애플리케이션에 대한 훈련 및 추론과 같은 학습 태스크를 수행하도록 맞춤화된 애플리케이션-특정 IC를 자동으로 생성하기 위한 글로벌 튜닝 프레임워크를 제공한다. 일부 구현에서, 타겟 애플리케이션(또는 장치)은 고정된 하드웨어 구성을 갖춘 맞춤형 하드웨어 가속기이다. 일부 다른 구현에서 타겟 애플리케이션은 이미지 분류, 물체 감지, 자율 차량 내비게이션, 그래픽 처리 또는 과학 컴퓨팅과 관련된 워크로드 유형이다. The operation of system 200 provides a global tuning framework for automatically generating application-specific ICs tailored to perform learning tasks such as training and inference for a target application. In some implementations, the target application (or device) is a custom hardware accelerator with a fixed hardware configuration. In some other implementations, target applications are workload types related to image classification, object detection, autonomous vehicle navigation, graphics processing, or scientific computing.
글로벌 튜너(202)는 애플리케이션-특정 ML 하드웨어 가속기를 생성하기 위해 다양한 애플리케이션 목표(102)에 따라 후보 아키텍처를 글로벌적으로 튜닝/최적화하도록 구성된다. 글로벌 튜너(202)는 하나 이상의 튜너 변수 및 제약 조건(210)에 기초하여 디자인 공간(104)을 구성하는 디자인 공간 빌더(design space builder)(204)를 포함한다. 디자인 공간 빌더(204)는 디자인 공간 익스플로러(explorer)(212) 및 글로벌 튜너(202)의 하나 이상의 코스트 모델(214)과 통신한다. 코스트 모델(214)은 전술한 아키텍처-웨어 코스트 모델(114)의 개별 모델에 대응한다. Global tuner 202 is configured to globally tune/optimize candidate architectures according to various application goals 102 to create application-specific ML hardware accelerators. The global tuner 202 includes a design space builder 204 that constructs a design space 104 based on one or more tuner variables and constraints 210. Design space builder 204 communicates with one or more cost models 214 of design space explorer 212 and global tuner 202. Cost model 214 corresponds to the individual model of the architecture-wear cost model 114 described above.
모듈(108)의 파싱된(parsed) 신경망 그래프에 기초하여, 디자인 공간 빌더(204)와 디자인 공간 익스플로러(212)는 타겟 애플리케이션에 대해 최적으로 수행될 신경망 아키텍처("신경 아키텍처")를 선택하기 위한 신경 아키텍처 검색(NAS: neural architecture search) 시스템을 구현하기 위해 상호작용할 수 있다. NAS는 강화학습(Reinforcement Learning) 기반 기술, 진화 탐색(Evolutionary Search), 미분 탐색(Differentiable Search) 등 다양한 탐색 기술을 채용할 수 있다. 디자인 공간 빌더(204) 및 디자인 공간 익스플로러(212)는 타겟 애플리케이션에 대해 효율적으로 튜닝되고 최적화될 수 있는 다양한 하드웨어 아키텍처를 탐색하기 위해 유사한 접근 방식을 사용할 수 있다.Based on the parsed neural network graph of module 108, design space builder 204 and design space explorer 212 are used to select a neural network architecture (“neural architecture”) that will perform optimally for the target application. They can interact to implement neural architecture search (NAS) systems. NAS can employ various search techniques such as reinforcement learning-based techniques, evolutionary search, and differential search. Design space builder 204 and design space explorer 212 may use similar approaches to explore various hardware architectures that can be efficiently tuned and optimized for a target application.
디자인 공간 빌더(204) 및 디자인 공간 익스플로러(212)는 하나 이상의 튜너 변수 및 제약 조건(210)에 기초하여 NAS 및 하드웨어 아키텍처 검색 기술을 구현한다. 튜너 변수 및 제약사항(210)은 다양한 언롤 인자(unroll factors), 최대 매퍼 입력/출력 데이터 폭, 또는 최대 리듀서(reducer) 입력/출력 데이터 폭을 포함한다. 위에서 설명한 것처럼 신경망 계층은 해당 커널 세트(예: 가중치/파라미터)를 가질 수 있다. 커널은 4가지 차원을 갖는 컨볼루션 커널일 수 있다(C-입력 채널; K-출력 채널; R-커널 높이; S-커널 너비(폭)). 컨볼루션 오퍼레이션의 예는 4차원 파라미터(C, K, R, S)를 사용하여 중첩 루프로 표현될 수 있다. 커널 세트는 다차원 텐서로 표현되며 중첩 루프를 사용하여 텐서의 다양한 차원을 탐색할 수 있다. 이 컨텍스트에서 언롤 인자는 각 중첩 루프(nested loops)의 언롤링에 해당한다. 글로벌 튜너(202)는 모든 언롤 인자에 대한 중첩 루프의 언롤링을 지원하고 이들 인자에 관해 후보 아키텍처를 튜닝할 수 있다. Design space builder 204 and design space explorer 212 implement NAS and hardware architecture discovery techniques based on one or more tuner variables and constraints 210. Tuner variables and constraints 210 include various unroll factors, maximum mapper input/output data width, or maximum reducer input/output data width. As explained above, a neural network layer can have a corresponding set of kernels (e.g. weights/parameters). The kernel can be a convolution kernel with four dimensions (C-input channels; K-output channels; R-kernel height; S-kernel width). An example of a convolution operation can be expressed as a nested loop using four-dimensional parameters (C, K, R, S). The kernel set is represented as a multidimensional tensor, and nested loops can be used to explore different dimensions of the tensor. In this context, the unroll argument corresponds to the unrolling of each nested loop. The global tuner 202 supports unrolling of nested loops for all unroll arguments and can tune candidate architectures with respect to these arguments.
매퍼 및 리듀서(reducer) 입력/출력 데이터 너비는 큰 텐서를 주어진 컴퓨팅 타일 또는 셀에 매핑되는 더 작은 조각(pieces)으로 줄이는 방법에 영향을 미친다. 예를 들어, 입력 텐서와 출력 텐서는 상당히 클 수 있으며 이러한 텐서는 한꺼번에 생성되지 않는다. 이러한 텐서를 프로세싱하는 하드웨어 가속기의 영역과 전력을 줄이기 위해 시스템(100)은 텐서 타일링을 활용하여 입력 텐서와 출력 텐서를 여러 개의 작은 조각으로 나눌 수 있다. 예를 들어, 시스템(100)은 매핑 제약에 기초하여 큰 입력 텐서를 더 작은 조각으로 분해(또는 축소)할 수 있다. 매핑 제약은 전력, 영역, 지연시간 및/또는 처리량과 같은 목표(objectives)와 연결(tied)될 수 있다. 글로벌 튜너(202)는 이러한 목표를 사용하여 후보 아키텍처에 대한 컴퓨팅 타일 세트의 구성 및 크기를 결정할 수 있다. 글로벌 튜너(202)는 입력 텐서의 서로 다른 부분에 대한 계산을 계산 타일 세트의 주어진 타일에 매핑할 수 있다. Mappers and reducers input/output data width affects how large tensors are reduced into smaller pieces that are mapped to given compute tiles or cells. For example, input tensors and output tensors can be quite large, and these tensors are not created all at once. To reduce the area and power of hardware accelerators that process these tensors, system 100 may utilize tensor tiling to divide the input tensor and output tensor into multiple smaller pieces. For example, system 100 may decompose (or reduce) a large input tensor into smaller pieces based on mapping constraints. Mapping constraints may be tied to objectives such as power, area, latency, and/or throughput. Global tuner 202 can use these goals to determine the configuration and size of the compute tile set for the candidate architecture. Global tuner 202 may map computations on different portions of the input tensor to given tiles in the computation tile set.
최대 매퍼 입력/출력 데이터 너비와 최대 리듀서 입력/출력 데이터 너비는 후보 아키텍처의 데이터 처리량에 직접적인 영향을 미치는 제약 조건이다. 튜너 변수 및 제약 조건(210)은 타겟 애플리케이션에 대해 주어진 신경망을 실행하도록 맞춤화된 하드웨어 ML 가속기를 생성하기 위한 후보 아키텍처를 탐색하는 것과 관련된 다른 아이템을 포함할 수 있다. 일부 구현에서, 타일 크기가 작을수록 데이터 전송 시간이 길어지므로 여기에서도 전반적인 칩 성능이 영향을 미칠 수 있다. 이러한 모든 다른 튜너 변수 및 제약(210)은 성능, 전력 및 영역에 영향을 미치는 다른 하드웨어 설계를 초래할 수 있다. 따라서, 글로벌 튜너(202)는 이러한 변수/제약으로부터 설계 공간을 형성하고 하드웨어 및 신경 아키텍처를 맞춤화하기 위한 최적의 파라미터를 선택함으로써 성능, 전력 및 영역(area) 사이의 균형을 유지한다. The maximum mapper input/output data width and maximum reducer input/output data width are constraints that directly affect the data throughput of the candidate architecture. Tuner variables and constraints 210 may include other items related to exploring candidate architectures for creating a hardware ML accelerator tailored to run a given neural network for a target application. In some implementations, smaller tile sizes result in longer data transfer times, so overall chip performance can also be impacted here. All of these different tuner variables and constraints 210 can result in different hardware designs affecting performance, power, and area. Accordingly, the global tuner 202 maintains a balance between performance, power and area by forming a design space from these variables/constraints and selecting optimal parameters to customize the hardware and neural architecture.
글로벌 튜너(202)는 적어도 각각의 개별 ML 코스트 모델(214)에 의해 수행되는 오퍼레이션에 기초하여 후보 아키텍처를 동적으로 튜닝할 수 있다. 일부 구현에서, 글로벌 튜너(202)는 i) 랜덤 검색(탐색) 튜너; ii) 시뮬레이션된 어닐링 튜너; 또는 iii) 프로그레시브 튜너(progressive tuner) 중 적어도 하나에 의해 수행된 오퍼레이션에 기초하여 후보 아키텍처를 동적으로 튜닝한다. 랜덤 검색(탐색) 튜너, 시뮬레이션된 어닐링 튜너(simulated annealing tuner), 프로그레시브 튜너는 각각 전술한 튜너(116)에 대응한다. 블록 분할 모델의 경우, 글로벌 튜너(202)는 시뮬레이션된 어닐링 튜너와 연관된 특정 튜닝 궤적을 구현한다. 랜덤 튜너, 시뮬레이션된 어닐링 튜너, 프로그레시브 튜너 각각은 소프트웨어, 하드웨어 또는 둘 다로 구현될 수 있다. 이들 튜너 각각과 관련된 기능은 글로벌 튜너(202)에 구현되는 튜너(116)에 통합될 수 있다. Global tuner 202 may dynamically tune candidate architectures based at least on the operations performed by each individual ML cost model 214. In some implementations, global tuner 202 may include: i) a random search (search) tuner; ii) simulated annealing tuner; or iii) dynamically tune the candidate architecture based on operations performed by at least one of a progressive tuner. The random search tuner, the simulated annealing tuner, and the progressive tuner each correspond to the tuner 116 described above. For the block partitioning model, the global tuner 202 implements a specific tuning trajectory associated with the simulated annealing tuner. Each of the random tuner, simulated annealing tuner, and progressive tuner can be implemented in software, hardware, or both. Functions associated with each of these tuners may be integrated into tuner 116 implemented in global tuner 202.
글로벌 튜너(202)는 후보 아키텍처의 베이스라인 프로세서 구성과 같은 트라이얼 구성(trial configuration)을 얻도록 검색 공간을 무작위로 샘플링하기 위해 랜덤 검색 튜너를 사용한다. ML 코스트 모델(214)의 성능 및 전력 코스트 모델을 쿼리하여 트라이얼 구성/아키텍처에서 타겟 애플리케이션을 실행하는 코스트를 얻는다. The global tuner 202 uses a random search tuner to randomly sample the search space to obtain a trial configuration that is equal to the baseline processor configuration of the candidate architecture. Query the performance and power cost models of ML cost model 214 to obtain the cost of running the target application in the trial configuration/architecture.
시뮬레이션된 어닐링은 글로벌 튜너(202)의 튜너로서 구현될 수 있으며, 주어진 함수의 글로벌 최적을 근사화하기 위한 확률적 기술이다. 각 단계에서 이 튜너는 현재 하드웨어 설계점 d'의 이웃 하드웨어 설계점 d'를 고려하고 현재 설계점을 설계점 d' 쪽으로 이동할지 아니면 설계점 d를 유지할지 확률론적으로 결정한다. 수락 확률(acceptance probability)을 제어하기 위해 온도 변수가 생성된다. 시뮬레이션된 어닐링 튜너는 확률 결과가 타겟 애플리케이션에 대한 최적의 설계 지점에 도달했음을 나타낼 때까지 이러한 단계를 반복하도록 구성된다. 예를 들어, 임계 점수를 초과하는 확률 점수는 특정 디자인 포인트가 주어진 제약 조건 세트와 관련하여 타겟 애플리케이션에 대해 최적으로 수행된다는 것을 나타낼 수 있다. Simulated annealing can be implemented as a tuner of global tuners 202, a stochastic technique for approximating the global optimum of a given function. At each step, the tuner considers the neighboring hardware design point d' of the current hardware design point d' and probabilistically decides whether to move the current design point toward design point d' or maintain design point d. A temperature variable is created to control the acceptance probability. The simulated annealing tuner is configured to repeat these steps until the probability results indicate that the optimal design point for the target application has been reached. For example, a probability score exceeding a threshold score may indicate that a particular design point performs optimally for the target application with respect to a given set of constraints.
인접 하드웨어 설계 포인트는 무작위로 생성될 수 있다. 일부 구현에서, 이웃 하드웨어 설계 포인트는 현재 하드웨어 설계 포인트와 유사하거나 매우 유사한 하드웨어 파라미터 선택(예를 들어, 언롤링(unrolling), 타일링(tiling), 매핑(mapping) 또는 스케줄링(scheduling))을 갖는다. 파라미터 선택의 유사성은 두 디자인 포인트 사이의 하드웨어 파라미터 선택의 중첩 정도(또는 백분율)로 특징지어질 수 있다. 일부 다른 구현에서, 이웃 하드웨어 설계 포인트는 현재 하드웨어 설계 포인트와 동일한 하드웨어 파라미터 선택 중 하나 이상을 가질 수 있다.Adjacent hardware design points can be randomly generated. In some implementations, the neighboring hardware design point has similar or very similar hardware parameter selections (e.g., unrolling, tiling, mapping, or scheduling) as the current hardware design point. Similarity in parameter selection can be characterized as the degree (or percentage) of overlap in hardware parameter selection between two design points. In some other implementations, neighboring hardware design points may have one or more of the same hardware parameter selections as the current hardware design point.
글로벌 튜너(202)는 프로그레시브 튜너를 사용하여 NAS의 디자인 공간과 같은 예시적인 디자인 공간의 프로그레시브 검색 방법론을 구현한다. 이 프로그레시브 검색(progressive search) 방법론을 사용하면 후보 아키텍처 튜닝을 위한 설계 공간 탐색 시간을 줄일 수 있다. 일부 구현에서, 글로벌 튜너(202)는 집적 회로의 기계 학습 블록에 대한 고정 데이터 속도 입력과 같은 특정 처리량 요구 사항을 충족(또는 초과)하도록 ML 하드웨어를 설계하고 튜닝하는 단계로서 설계 공간을 탐색하기 위해 프로그레시브 검색 방법론을 실행한다. 프로그레시브 검색 방법론은 적어도 i) 모든 신경망 계층에 대한 최소 설계로 베이스라인 설계를 초기화하고 ii) 데이터 속도 요구 사항보다 낮은 데이터 처리량을 갖는 병목(병목 현상)(bottleneck) 계층을 식별하기 위해 코스트 모델(214)에 쿼리하는 단계를 포함할 수 있다. 코스트 모델(214)이 병목 현상을 식별 또는 나타내지 않고 및/또는 글로벌 튜너(202)가 신경망의 어떤 계층도 병목 현상으로 동작하지 않는다고 결정하면, 검색 방법론의 실행이 종료된다.The global tuner 202 uses a progressive tuner to implement a progressive search methodology of an exemplary design space, such as that of a NAS. This progressive search methodology can be used to reduce design space exploration time for tuning candidate architectures. In some implementations, global tuner 202 is used to explore the design space as a step in designing and tuning ML hardware to meet (or exceed) specific throughput requirements, such as fixed data rate input to a machine learning block of an integrated circuit. Implement progressive search methodology. The progressive search methodology involves at least i) initializing a baseline design with a minimal design for all neural network layers and ii) constructing a cost model (214) to identify bottleneck layers with data throughput lower than the data rate requirements. ) can include a querying step. If the cost model 214 does not identify or indicate a bottleneck and/or the global tuner 202 determines that no layer of the neural network is operating as a bottleneck, execution of the search methodology ends.
프로그레시브 검색 방법론은 iii) 전체 모델 성능에 대한 코스트를 최소화하면서 처리량 요구 사항을 충족(또는 초과)하여 병목 현상을 최소화하는 설계 구성을 결정하기 위해 병목 현상을 참조하여 검색 공간을 철저하게 탐색하고 그리고 iv) iii) 단계에서 결정된 설계 구성을 새로운 베이스라인(기본) 설계로 사용한 다음 ii) 단계로 다시 진행하는 단계를 더 포함할 수 있다. 일부 구현에서, 기본 설계는 주어진 신경망의 모든 계층을 실행하기 위한 최소 하드웨어(및 신경) 아키텍처/설계 파라미터를 포함하는 베이스라인 프로세서 구성이다. 검색 공간을 철저하게 탐색하려면 각 설계 구성을 사용하여 다층 신경망을 구현하고, 각 설계 구성의 해당 데이터 처리량을 평가하고, 다양한 설계 구성 각각에 대한 해당 코스트 값을 계산함으로써 다양한 설계 구성을 반복적으로 탐색하는 것이 포함된다. Progressive search methodologies iii) exhaustively explore the search space with reference to bottlenecks to determine a design configuration that minimizes bottlenecks by meeting (or exceeding) throughput requirements while minimizing the cost to overall model performance, and iv ) It may further include using the design configuration determined in step iii) as a new baseline (basic) design and then proceeding again to step ii). In some implementations, the base design is a baseline processor configuration that includes the minimum hardware (and neural) architecture/design parameters to run all layers of a given neural network. To explore the search space exhaustively, iteratively explores various design configurations by implementing a multilayer neural network using each design configuration, evaluating the corresponding data throughput of each design configuration, and calculating the corresponding cost value for each of the various design configurations. It includes
도 2의 예에서, 입력 목표(102)은 사용자 정의, 시스템 정의 또는 둘 다일 수 있다. 예를 들어, 입력 목표(102)는 사용자 구성 파일(user configuration file)로서 또는 시스템 생성 입력 파일(input file)로서 수신될 수 있다. 구성 또는 입력 파일은 예를 들어 PPA 제약 세트에서 파생되는 다양한 애플리케이션 레벨 목표(102)를 지정할 수 있다. 예를 들어 입력 파일에는 프로세서 활용도, 전력 소비, 데이터 처리량, 하드웨어 블록 크기 및/또는 지연시간과 같은 애플리케이션 레벨 목표의 세트가 포함될 수 있다. 입력 파일에는 각 애플리케이션 레벨 목표에 대한 각각의 하드웨어 가속기 성능 임계값도 포함되어 있다. In the example of Figure 2, input target 102 may be user-defined, system-defined, or both. For example, input target 102 may be received as a user configuration file or as a system generated input file. The configuration or input file may specify various application level goals 102, for example derived from a set of PPA constraints. For example, the input file may include a set of application-level goals such as processor utilization, power consumption, data throughput, hardware block size, and/or latency. The input file also contains each hardware accelerator performance threshold for each application-level goal.
일부 구현에서, 입력 파일은 타겟 애플리케이션에 다중 벡터 오퍼레이션이 필요함을 나타내는 목표(102)를 포함한다. 이러한 표시에 기초하여, 제어 로직은 디자인 공간(104)의 하드웨어 변수(110)가 벡터 파라미터(vector_ctrl)로서 설정되도록 트리거할 수 있다. 디자인 공간(104)은 후보 아키텍처의 선택을 예를 들어 긴밀하게 결합된(tightly coupled) VPU를 형성하는 다중 벡터 프로세싱 레인을 포함하는 아키텍처로 제한하기 위해 "vector_ctrl" 파라미터를 사용할 수 있다. In some implementations, the input file includes a goal 102 indicating that the target application requires multiple vector operations. Based on this indication, control logic can trigger hardware variable 110 in design space 104 to be set as a vector parameter (vector_ctrl). Design space 104 may use the “vector_ctrl” parameter to limit the selection of candidate architectures to, for example, architectures that include multiple vector processing lanes forming a tightly coupled VPU.
도 2의 예에서, 코스트 모델(214) 중 일부(또는 전부)는 후보 아키텍처를 튜닝하기 위해 ML 기반 분석을 실행한다. 입력 목표 세트(102)에 따라, 글로벌 튜너(202)는 하나 이상의 최적화 알고리즘에 기초하여 후보 아키텍처의 하드웨어 및 신경 아키텍처를 튜닝한다. 예를 들어, 글로벌 튜너는 신경망의 특정 하드웨어 블록(들)을 참조하여 다층 신경망의 각 계층을 실행하기 위한 후보 아키텍처의 사용을 모델링하기 위해 코스트 모델(214)을 사용한다. 각 계층을 실행하기 위한 아키텍처의 사용을 모델링하는 것에 응답하여, ML 코스트 모델(214)은 아키텍처가 각 계층에 대해 어떻게 수행되는지 설명하는 성능 파라미터를 생성한다. In the example of Figure 2, some (or all) of cost models 214 run ML-based analysis to tune candidate architectures. Depending on the input target set 102, the global tuner 202 tunes the hardware and neural architecture of the candidate architecture based on one or more optimization algorithms. For example, the global tuner uses the cost model 214 to model the use of a candidate architecture for executing each layer of a multilayer neural network with reference to specific hardware block(s) of the neural network. In response to modeling the use of the architecture to execute each layer, ML cost model 214 generates performance parameters that describe how the architecture performs for each layer.
일부 구현에서, 최적화 알고리즘은 코스트 모델 상호작용 루프, 예를 들어 최적화 루프를 구현하는 데 사용된다. 예를 들어, 최적화기 또는 글로벌 튜너(202)(예를 들어, 시뮬레이션된 어닐링, 프로그레시브, 랜덤 등)는 PE 수, 싸이스톨릭 배열 차원 등과 같은 하드웨어 매핑 파라미터 세트를 생성할 수 있다. 계층 종속성 및 양자화 방식(예: 고정)을 포함하는 신경망 그래프와 함께 하드웨어 매핑 파라미터가 코스트 모델(214)로 전송된다. 코스트 모델(214)은 입력에 기초하여 지연시간, 처리량 및 전력과 같은 코스트를 생성한다. 코스트 모델의 코스트 출력은 최적화 루프의 한 단계로 최적화 프로그램에 피드백될 수 있다. 최적화 프로그램은 코스트 출력을 프로세싱하고 탐색할 다음 하드웨어 매핑 전략을 결정할 수 있다. 글로벌 튜너(202)는 수렴 조건이 충족되거나 검색 공간이 완전히 탐색될 때까지 이 최적화 루프를 반복할 수 있다.In some implementations, an optimization algorithm is used to implement a cost model interaction loop, for example an optimization loop. For example, an optimizer or global tuner 202 (e.g., simulated annealing, progressive, random, etc.) may generate a set of hardware mapping parameters such as number of PEs, cystolic array dimensions, etc. Hardware mapping parameters are sent to the cost model 214 along with the neural network graph including layer dependencies and quantization scheme (e.g. fixed). Cost model 214 generates costs such as latency, throughput, and power based on the input. The cost output from the cost model can be fed back to the optimizer as a step in the optimization loop. The optimizer can process the cost output and determine the next hardware mapping strategy to explore. Global tuner 202 may repeat this optimization loop until convergence conditions are met or the search space is fully explored.
일부 구현에서, 글로벌 튜너(202)의 제1 코스트 모델(214)은 후보 아키텍처의 하드웨어 속성에 대한 성능 추정/파라미터를 계산하는 데 사용되는 반면, 제2 코스트 모델(214)은 후보 아키텍처에 구현된 신경망에 대한 성능 추정/파라미터를 계산하는 데 사용된다. 제1 및 제2 코스트 모델(214)은 동일하거나 다를 수 있다. 코스트 모델(214)은 단일 최적화 알고리즘을 사용하여 아키텍처를 튜닝하고 후보 아키텍처의 성능을 최적화하기 위한 성능 추정치를 계산할 수 있다. 일부 다른 구현에서, 코스트 모델(214)은 아키텍처 성능의 다양한 에스펙트를 최적화하기 위한 성능 추정치를 계산하기 위해 다양한 최적화 알고리즘을 사용한다. In some implementations, the first cost model 214 of the global tuner 202 is used to calculate performance estimates/parameters for hardware properties of the candidate architecture, while the second cost model 214 is used to calculate performance estimates/parameters for hardware properties of the candidate architecture. Used to calculate performance estimates/parameters for neural networks. The first and second cost models 214 may be the same or different. Cost model 214 may use a single optimization algorithm to compute performance estimates to tune the architecture and optimize the performance of the candidate architecture. In some other implementations, cost model 214 uses various optimization algorithms to calculate performance estimates for optimizing various aspects of architecture performance.
글로벌 튜너(202)는 탐구되는 다양한 하드웨어 및 신경망 아키텍처에 대한 다양한 디자인 공간 및 최적화 전략을 구현하기 위해 적어도 디자인 공간 빌더(204), 디자인 공간 익스플로러(212) 및 코스트 모델(214)을 사용할 수 있다. 예를 들어, 후보 아키텍처의 각 하드웨어 블록 내에서 글로벌 튜너(202)는 계층별 타일링 및 시스톨릭 배열 차원의 튜닝과 같이 구체적으로 하나의 계층을 목표로 하는 다양한 구현을 탐색한다. 글로벌 튜너(202)는 병렬화를 증가시키기 위해 계층 변환을 탐색할 수 있다. 예를 들어, 글로벌 튜너(202)는 하나 이상의 하드웨어 블록에 걸쳐 컴퓨팅 유닛의 처리량 및/또는 활용도를 증가시키기 위해 조밀한 /1 x 1 컨볼루션을 n x n 컨볼루션으로 변환할 수 있다.The global tuner 202 may use at least a design space builder 204, a design space explorer 212, and a cost model 214 to implement various design spaces and optimization strategies for the various hardware and neural network architectures being explored. For example, within each hardware block of a candidate architecture, the global tuner 202 explores various implementations targeting one layer specifically, such as layer-wise tiling and systolic array-level tuning. Global tuner 202 may explore layer transformations to increase parallelism. For example, global tuner 202 may convert a dense /1 x 1 convolution to an n x n convolution to increase throughput and/or utilization of compute units across one or more hardware blocks.
일부 구현에서, 최적화 알고리즘에 기초하여, 코스트 모델(214)은 조밀한 컨볼루션이 여러 계산 유닛을 포함하는 하드웨어 블록의 단일 계산 유닛에 할당된다는 표시로부터 활용 추정치를 계산한다. 글로벌 튜너(202)는 활용 추정치를 애플리케이션 목표(application objective)(102)(또는 제약(210))에 의해 지정된 활용 임계값과 비교할 수 있다. 글로벌 튜너(202)는 계산된 활용 추정치가 임계값 미만인지 여부를 결정한다. 글로벌 튜너(202)는 계산된 활용 추정치가 임계값 아래에 있다는 결정에 응답하여 주어진 하드웨어 블록에 걸쳐 계산 유닛의 활용을 증가시키기 위해 조밀한/1 x 1 컨볼루션을 n x n 컨볼루션으로 변환할 수 있다. 활용 추정치는 코스트 모델(214)에 의해 생성된 성능 파라미터(또는 추정치)이다. In some implementations, based on an optimization algorithm, cost model 214 calculates a utilization estimate from an indication that a dense convolution is allocated to a single compute unit of a hardware block containing multiple compute units. Global tuner 202 may compare the utilization estimate to a utilization threshold specified by application objective 102 (or constraint 210). Global tuner 202 determines whether the calculated utilization estimate is below a threshold. Global tuner 202 may convert the dense/1 x 1 convolution to an n x n convolution to increase utilization of computational units across a given hardware block in response to determining that the computed utilization estimate is below a threshold. . The utilization estimate is a performance parameter (or estimate) generated by the cost model 214.
프로세싱 엔진(예를 들어, 셀, 타일 또는 프로세싱 레인)의 다차원 어레이의 경우, 글로벌 튜너(202)는 원하는 성능 목표를 달성하는 데 필요한 최적의 크기/영역 및 예상 전력 밀도를 결정할 수 있다. 글로벌 튜너(202)는 결정된 크기에 기초하여 어레이(배열)의 각 차원에서 PE(processing engines)의 개수를 변경할 수 있다. 시스템(100, 200)은 신경망의 한 계층에 대한 하나 이상의 심층적인 하드웨어 맞춤화가 신경망의 다른 계층의 효율적인 실행 또는 오퍼레이션을 방해하거나 악영향을 미치지 않도록 구성된다. For multi-dimensional arrays of processing engines (e.g., cells, tiles or processing lanes), global tuner 202 may determine the optimal size/area and expected power density needed to achieve desired performance goals. The global tuner 202 may change the number of processing engines (PEs) in each dimension of the array based on the determined size. Systems 100, 200 are configured such that one or more deep hardware customizations to one layer of the neural network do not interfere with or adversely affect the efficient execution or operation of other layers of the neural network.
글로벌 튜너(202)는 후보 아키텍처 튜닝에 응답하여 출력 구성(230)을 생성한다. 출력 구성(230)은 애플리케이션-특정 ML 가속기를 자동으로 생성하는 데 사용된다. 출력 구성(230)은 ML 모델(또는 알고리즘) 및 해당 아키텍처 구성을 나타낼 수 있다. 시스템(200)은 예시 코드 생성 모듈(240)을 사용하여 출력 구성(230)을 나타내는 데이터를 상위 레벨 합성(HLS: High level synthesis) 코드로 변환한다. 예를 들어, 코드 생성 모듈(240)은 HLS를 사용하여 하드웨어 가속기에 대한 ML 알고리즘의 펌웨어 구현을 생성할 수 있다. Global tuner 202 generates output configuration 230 in response to candidate architecture tuning. Output configuration 230 is used to automatically generate application-specific ML accelerators. Output configuration 230 may represent an ML model (or algorithm) and its architectural configuration. System 200 uses example code generation module 240 to convert data representing output configuration 230 into high level synthesis (HLS) code. For example, code generation module 240 can use HLS to generate firmware implementations of ML algorithms for hardware accelerators.
일반적으로, 글로벌 튜너(202)는 타겟 애플리케이션에 대해 완전히 맞춤화된 하나 이상의 애플리케이션-특정 ML 가속기를 생성하는 데 사용된다. 예를 들어, 사용자 정의에는 하나 이상의 신경망 계층에 맞게 튜닝된 이종 양자화(heterogeneous quantization) 및 마이크로-아키텍처와 같은 아이템이 포함될 수 있다. 일부 구현에서, 글로벌 튜너(202) 및 시스템(200)은 적어도 PPA 제약 세트(예를 들어 목표(102))에 대한 전체 아키텍처를 최적화하기 위한 마이크로-아키텍처, 공간 매핑 및 시간 매핑과 같은 최적의 하드웨어 파라미터를 식별함으로써 맞춤형 아키텍처를 생성하는 데 사용된다. Typically, global tuner 202 is used to create one or more application-specific ML accelerators that are fully customized for the target application. For example, customizations may include items such as heterogeneous quantization and micro-architecture tuned for one or more neural network layers. In some implementations, global tuner 202 and system 200 provide optimal hardware, such as micro-architecture, spatial mapping, and temporal mapping to optimize the overall architecture for at least a set of PPA constraints (e.g., goals 102). It is used to create a custom architecture by identifying parameters.
하드웨어 기능은 칩 위 또는 내부에서 분리될 수 있다. 아키텍처의 공간 매핑을 최적화하려면 칩 또는 통합 프로세서 블록 내부에서 공간적으로 분리된 다양한 신경망 오퍼레이션을 실행하는 데 사용되는 하드웨어 블록이 필요하다. 예를 들어, 후보 아키텍처는 신경망에서 전용 오퍼레이션을 실행하기 위해 전용 하드웨어 블록의 특정 배열을 사용하여 공간 매핑에 최적화될 수 있다. 이 매핑을 통해 하드웨어 블록을 특정 알고리즘이나 계산 패턴에 맞게 튜닝할 수 있다. Hardware functions can be isolated on or within the chip. Optimizing the spatial mapping of the architecture requires hardware blocks used to execute the various neural network operations that are spatially separated within a chip or integrated processor block. For example, a candidate architecture may be optimized for spatial mapping using a specific arrangement of dedicated hardware blocks to execute dedicated operations in a neural network. This mapping allows hardware blocks to be tuned to specific algorithms or computation patterns.
다른 설계에 비해 최적화된 공간 매핑을 갖춘 아키텍처는 성능과 에너지 효율성을 향상시킬 수 있다. 개선 사항은 적어도 특정 알고리즘이나 컴퓨팅 패턴을 구현하도록 맞춤화된 전용 하드웨어 블록의 배열을 통해 실현될 수 있다. 일부 구현에서, 하나 이상의 전용 하드웨어 블록은 고정 차원 텐서를 프로세싱하고, 고정 양자화 방식을 지원하고, 특정 신경망 계층에 맞게 조정되도록 구성된다. Compared to other designs, architectures with optimized spatial mapping can improve performance and energy efficiency. Improvements can be realized through at least an array of dedicated hardware blocks tailored to implement specific algorithms or computing patterns. In some implementations, one or more dedicated hardware blocks are configured to process fixed-dimensional tensors, support fixed quantization schemes, and be tailored to specific neural network layers.
아키텍처의 시간적 매핑(307)을 최적화하는 것은 신경망의 다양한 오퍼레이션 간에 시간 공유되는 하드웨어 블록을 포함한다. 예를 들어, 후보 아키텍처는 동일한 하드웨어 블록을 재사용하여 신경망에서 다양한 오퍼레이션을 실행함으로써 시간적 매핑에 최적화될 수 있다. 주어진 하드웨어 블록을 보다 일반적으로 사용함으로써 이 접근 방식은 하드웨어의 프로그래밍 가능성을 향상시킬 수 있다. 또한 이 접근 방식은 애플리케이션 개발자에게 하드웨어에서 실행될 수 있는 신경망 측면에서 더 많은 유연성을 제공할 수 있다. 일부 예에서, 최적화된 시간 매핑은 동일한 하드웨어 블록에서 서로 다른 계층의 시간 공유를 제공하고 다중 양자화 방식(multiple quantization schemes)을 지원한다. Optimizing the temporal mapping 307 of the architecture involves hardware blocks being time-shared between the various operations of the neural network. For example, a candidate architecture can be optimized for temporal mapping by reusing the same hardware blocks to execute various operations in a neural network. By making more general use of a given hardware block, this approach can improve the programmability of the hardware. This approach can also give application developers more flexibility in terms of the neural networks that can run on hardware. In some examples, optimized time mapping provides time sharing of different layers on the same hardware block and supports multiple quantization schemes.
사용자 정의(customization)를 통해 타겟 애플리케이션에 맞게 사용자 정의되지 않은 다른 프로세싱 장치에 비해 훨씬 적은 전력과 영역(area)을 소비하는 애플리케이션-특정 ML 가속기가 생성될 수 있다.Customization can create application-specific ML accelerators that consume significantly less power and area than other processing devices that are not customized for the target application.
도 3은 다층 신경망을 튜닝하기 위한 예시적인 프레임워크(300)를 도시한다. 이 프레임워크 시스템(100)을 사용하면 신경망 그래프의 계산 노드를 주어진 하드웨어 블록의 마이크로-아키텍처(또는 프로세싱 엔진)의 다양한 기능에 반복적으로 매핑할 수 있다. 예를 들어, 프레임워크(300)는 신경망 그래프의 다양한 계산 노드 사이의 종속성을 결정하고 구축하기 위해 글로벌 튜너(202) 또는 최적화 및 튜닝 모듈(112)에서 구현될 수 있다. 종속성은 예를 들어 ML 코스트 모델(214)이 후보 아키텍처에 의해 신경망의 각 계층(layer) 실행을 모델링할 때 결정될 수 있다. ML 코스트 모델(214)은 신경망의 각 계층을 실행할 때 후보 아키텍처가 어떻게 수행되는지에 대한 평가를 제공하는 성능 파라미터를 생성한다.Figure 3 shows an example framework 300 for tuning a multilayer neural network. This framework system 100 allows computational nodes of a neural network graph to be iteratively mapped to various functions of the micro-architecture (or processing engine) of a given hardware block. For example, framework 300 may be implemented in global tuner 202 or optimization and tuning module 112 to determine and build dependencies between various computational nodes of a neural network graph. Dependencies may be determined, for example, when ML cost model 214 models the execution of each layer of the neural network by a candidate architecture. ML cost model 214 generates performance parameters that provide an assessment of how a candidate architecture will perform when executing each layer of the neural network.
도 3의 예에서, 신경망(302)은 5개의 계층(L1-L5)을 포함하며, 여기서 제1 계층은 L1이고, 제2 계층은 L2 등이다. 이들 5개 계층은 후보 아키텍처의 다양한 하드웨어 기능(예: 프로세싱 엔진)에 대한 초기 매핑을 가질 수 있다. 예를 들어, 5개 계층 각각은 싸이스톨릭 어레이의 서로 다른 셀, 서로 다른 싸이스톨릭 어레이 블록, 컴퓨팅 타일의 서로 다른 MAC(Multiply-Accumulate Cell), 또는 서로 다른 컴퓨팅 타일에 매핑될 수 있다. 일부 구현에서, 싸이스톨릭 어레이의 개별 셀과 컴퓨팅 타일의 개별 MAC는 후보 아키텍처의 마이크로아키텍처의 에스펙트를 나타낸다.In the example of Figure 3, neural network 302 includes five layers (L1-L5), where the first layer is L1, the second layer is L2, and so on. These five layers may have initial mappings to various hardware functions (e.g. processing engines) of the candidate architecture. For example, each of the five tiers may be mapped to a different cell of the cystolic array, a different cystolic array block, a different Multiply-Accumulate Cell (MAC) of the compute tile, or a different compute tile. In some implementations, the individual MACs of individual cells and compute tiles of the cystolic array represent aspects of the microarchitecture of the candidate architecture.
코스트 모델(214)은 신경망(302)을 실행하는 후보 아키텍처에 대한 성능 추정을 계산할 수 있다. 성능 추정에는 특정 계층 프로세싱에 소요되는 시간(time durations), 전체 프로세싱 지연시간 및 PE 활용도를 나타내는 파라미터가 포함된다. 코스트 모델(214)은 시간(time durations)을 프로세싱하여 타이밍 제약 세트에 대해 최적화된 신경 아키텍처 스케줄(304)을 생성한다. 성능 추정에 기초하여, 글로벌 튜너(202)는 계층 L1 + L2 + L5를 계산하는 데 필요한 시간이 계층 L3 + L4를 계산하는 데 필요한 시간과 대략 동일하다고 결정할 수 있다. Cost model 214 may calculate performance estimates for candidate architectures running neural network 302. Performance estimates include parameters representing time durations for specific layer processing, overall processing latency, and PE utilization. Cost model 214 processes time durations to generate a neural architecture schedule 304 optimized for a set of timing constraints. Based on performance estimates, global tuner 202 may determine that the time required to compute layers L1 + L2 + L5 is approximately the same as the time required to compute layers L3 + L4.
이 결정에 기초하여, 글로벌 튜너(202)는 동일한 하드웨어 기능 B1을 재사용하기 위해 계층 L1, L2 및 L5를 리매핑할 수 있는 반면, 계층 L3 및 L4는 동일한 하드웨어 기능 B2를 재사용하기 위해 리매핑할 수 있다(306). 일부 예에서, B1 및 B2는 각각 계산 타일 또는 싸이스톨릭 어레이, MAC, 싸이스톨릭 어레이 셀, 심지어 VPU의 벡터 프로세싱 레인의 산술 논리 장치(ALU)와 같은 프로세싱 엔진(308, 310)이다. 글로벌 튜너(202)는 프로세싱 지연시간을 줄이고 후보 아키텍처를 최적화하여 목표(102)에 지정된 지연시간 요구 사항에 따라 신경망 모델을 실행하기 위해 튜닝 오퍼레이션의 일부로 리매핑을 수행할 수 있다.Based on this decision, the global tuner 202 may remap layers L1, L2, and L5 to reuse the same hardware function B1, while layers L3 and L4 may remap the same hardware function B2. (306). In some examples, B1 and B2 are processing engines 308, 310, such as compute tiles or arithmetic logic units (ALUs) of a cystolic array, MAC, cystolic array cells, or even vector processing lanes of a VPU, respectively. Global tuner 202 may perform remapping as part of a tuning operation to reduce processing latency and optimize candidate architectures to execute the neural network model according to the latency requirements specified in objective 102.
특정 신경망의 경우 각 계층에는 서로 다른 계산 주기가 필요할 수 있다. 예를 들어, 공간 재매핑 후에 일부 PE는 계산 불균형으로 인해 다른 PE보다 더 많은 유휴 시간을 경험할 수 있다. 이를 로드 불균형(load imbalance)이라고 할 수 있다. 시스템(100)은 적어도 일시적인 방식으로 여러 계층에 걸쳐 PE 재사용을 허용하는 튜닝 및 최적화 메커니즘을 활용하여 로드 불균형을 설명하거나 극복할 수 있다. 예를 들어, 튜너(116)와 스케줄러/매퍼(118)는 로드 불균형을 감지하고 후보 아키텍처의 속성을 튜닝하여 각 PE의 계산 주기의 균형을 균등하게 맞출 수 있다.For a particular neural network, each layer may require different computation cycles. For example, after spatial remapping, some PEs may experience more idle time than others due to computational imbalance. This can be called load imbalance. System 100 may account for or overcome load imbalances by utilizing tuning and optimization mechanisms that allow PE reuse across multiple layers, at least in a temporary manner. For example, tuner 116 and scheduler/mapper 118 can detect load imbalances and tune properties of candidate architectures to evenly balance the compute cycles of each PE.
위에서 언급한 바와 같이, 신경망(302)의 5개 계층은 각 계층이 후보 아키텍처의 서로 다른 하드웨어 기능(예: 프로세싱 엔진)에 매핑되는 초기 매핑을 가질 수 있다. 이 초기 매핑에 대한 성능 추정에는 계층이 매핑될 수 있는 각 프로세싱 엔진에서 전체 컴퓨팅 기능의 낮은 활용도를 나타내는 활용 파라미터가 포함될 수 있다. 이러한 추정치와 파라미터에 기초하여, 글로벌 튜너(202)는 또한 예를 들어 동일한 프로세싱 엔진 B1을 재사용하기 위해 계층 L1, L2 및 L5를 재매핑하고, 동일한 프로세싱 엔진 B2를 재사용하기 위해 계층 L3 및 L4를 재매핑함으로써 프로세싱 활용도를 높이기 위해 재매핑을 수행할 수도 있다. 이러한 재매핑은 B1 및 B2 각각에서 전체 활용도를 증가시키고 목표(102)에 지정된 활용도(및 지연시간) 요구 사항에 따라 신경망 모델을 실행하도록 후보 아키텍처를 최적화하기 위해 수행될 수 있다.As mentioned above, the five layers of neural network 302 may have an initial mapping in which each layer maps to a different hardware function (e.g., processing engine) of the candidate architecture. The performance estimate for this initial mapping may include a utilization parameter that indicates low utilization of the overall computing power in each processing engine to which the layer may be mapped. Based on these estimates and parameters, the global tuner 202 also remaps layers L1, L2, and L5 to reuse the same processing engine B1, for example, and layers L3 and L4 to reuse the same processing engine B2. Remapping can also be performed to increase processing utilization. This remapping may be performed to increase overall utilization in B1 and B2, respectively, and optimize the candidate architecture to execute the neural network model according to the utilization (and latency) requirements specified in goal 102.
글로벌 튜너(202)는 후보 아키텍처를 튜닝하여 임의의 나머지 PE(예를 들어, B3, B4, B5)에 다른 오퍼레이션을 재할당할 수 있다. 일부 경우에, 글로벌 튜너(202)는 PE의 수를 줄이기 위해(예를 들어, 5에서 2로) 후보 아키텍처의 하드웨어 레이아웃을 늘리기 위해 디자인 공간 익스플로러(212)를 사용한다. 일부 다른 경우에, 글로벌 튜너(202)는 적어도 B1 및 B2에 걸쳐 병렬성의 양을 증가시키도록 PE를 재구성하기 위해 디자인 공간 익스플로러(212)를 사용한다. 글로벌 튜너(202)는 나머지 PE(예를 들어, B3, B4, B5)가 리매핑 후에 더 작은 데이터세트를 프로세싱하는 데 필요하다고 결정할 수 있다. 이 결정에 기초하여, 글로벌 튜너(202)는 예를 들어 이들 PE의 마이크로아키텍처의 메모리 비율에 대한 계산을 조정하여 더 작은 데이터세트를 프로세싱하기 위해 PE의 크기와 활용도를 최적화할 수 있다.The global tuner 202 may tune the candidate architecture and reallocate different operations to any remaining PEs (e.g., B3, B4, B5). In some cases, global tuner 202 uses design space explorer 212 to increase the hardware layout of the candidate architecture to reduce the number of PEs (e.g., from 5 to 2). In some other cases, global tuner 202 uses design space explorer 212 to reorganize PEs to increase the amount of parallelism across at least B1 and B2. Global tuner 202 may determine that the remaining PEs (e.g., B3, B4, B5) are needed to process the smaller dataset after remapping. Based on this determination, global tuner 202 can optimize the size and utilization of PEs for processing smaller datasets, for example, by adjusting calculations for the memory ratio of the microarchitecture of these PEs.
프레임워크(300)는 애플리케이션 레벨 목표(예를 들어 추론 시간, 처리량, 전력 등) 및 적용 가능한 하드웨어 제약 조건(110, 210)과 함께 신경망 그래프를 입력으로 취하는 예시적인 알고리즘 또는 계산 시퀀스에 대응할 수 있다. 글로벌 튜너(202)는 다양한 아키텍처 노브(architectural knobs)에 대한 계층별 공간 매핑 탐색을 수행하기 위한 기초로서 프레임워크(300)를 사용할 수 있다. 다양한 아키텍처 노브가 프레임워크(300)에 의해 지원될 수 있으며, 이러한 아키텍처 노브는 i) 싸이스톨릭 배열 또는 완전히 언롤링된 디자인(fully unrolled design)과 같은 디자인 스타일; ii) 복수의 매퍼(예: 싸이스톨릭 배열 클러스터); iii) 클러스터당 싸이스톨릭 어레이의 수; iv) 입력 및 출력 타일링; v) 밀집 계층(dense layers)에 대한 하드웨어 차원 변환을 포함할 수 있다. Framework 300 may correspond to an example algorithm or computation sequence that takes as input a neural network graph along with application-level goals (e.g., inference time, throughput, power, etc.) and applicable hardware constraints 110, 210. . The global tuner 202 may use the framework 300 as a basis for performing a hierarchical spatial mapping search for various architectural knobs. A variety of architectural knobs may be supported by framework 300, including i) a design style such as a systolic arrangement or a fully unrolled design; ii) multiple mappers (e.g. cystolic array clusters); iii) number of cystolic arrays per cluster; iv) input and output tiling; v) May include hardware dimension transformation for dense layers.
주어진 제약(210)에 대한 최적화를 달성하기 위한 각각의 리매핑 또는 튜닝은 다른 제약과 관련하여 후보 아키텍처에 대한 대응 조절을 트리거할 수 있다. 예를 들어, 주어진 타이밍 또는 지연시간 제약 조건(제약)을 최적화하기 위해 B1 및 B2에 대한 재매핑에는 PE에 대한 처리량 요구 사항의 증가가 필요할 수 있다. 따라서 새로운(또는 기타 기존) 요구 사항을 충족하기 위해 다양한 아키텍처 노브를 개선해야 하는 경우가 많다. 일부 구현에서, 시스템(100)은 이들 제약 각각에 대해 후보 아키텍처를 최적화하기 위해 적어도 지연시간, 타이밍 및 활용도 사이의 상호 작용의 균형을 맞추기 위해 후보 아키텍처의 튜닝을 반복한다. 일부 다른 구현에서, 시스템(100)은 여러 제약, 변수 및 목표 간의 상호 작용의 균형을 맞춘다.Each remapping or tuning to achieve optimization for a given constraint 210 may trigger corresponding adjustments to the candidate architecture with respect to other constraints. For example, remapping B1 and B2 to optimize given timing or latency constraints may require an increase in throughput requirements for PE. Therefore, various architectural knobs often need to be improved to meet new (or other existing) requirements. In some implementations, system 100 iterates tuning of the candidate architecture to at least balance the interaction between latency, timing, and utilization to optimize the candidate architecture for each of these constraints. In some other implementations, system 100 balances interactions between multiple constraints, variables, and goals.
각 아키텍처 노브는 엔드투엔드(end-to-end) 애플리케이션 성능에 긍정적이거나 부정적인 영향을 미칠 수 있다. 또한 각 아키텍처 노브는 다른 계층 매핑에서 아키텍처 노브의 효과에도 영향을 미칠 수 있다. 따라서, 적어도 제어 로직의 기계 학습 에스펙트와 아키텍처-웨어 코스트 모델(114)에 기초하여, 시스템(100)은 이러한 긍정적인 영향과 부정적인 영향을 정확하게 예측하기 위해 평가 중인 후보 아키텍처에 대한 종합적인 뷰(holistic view)를 제공하도록 구성된다.Each architectural knob can have a positive or negative impact on end-to-end application performance. Additionally, each architecture knob can also affect the effect of architecture knobs on other layer mappings. Accordingly, based at least on the machine learning aspects of the control logic and the architecture-wear cost model 114, the system 100 provides a comprehensive view of the candidate architecture under evaluation to accurately predict these positive and negative impacts. It is configured to provide a holistic view.
후보 아키텍처에는 여러 프로세싱 엔진이 포함될 수 있으며, 하나 이상의 계층은 사전 정의된 병합 규칙(예: conv2d + BN + 활성화 병합(activation merging), conv2d + 맥스풀링(maxpooling) 병합)에 기초하여 별도의 프로세싱 엔진에 매핑될 수 있다. 병합 규칙은 예를 들어 네트워크 그래프 모듈(108)의 명령어나 코딩된 규칙으로 미리 정의될 수 있다. 일부 구현에서, 두 개 이상의 그래프 노드(또는 계층)는 다음 계층의 계산이 이전 계층의 계산과 일치하여 수행는 경우 병합된다(예: conv2d (+ BN) + 활성화). 예를 들어, 배치 정규화(BN: batch normalization) 계층에 대한 계산은 2D 컨벌루션 계층에 대한 계산과 병합될 수 있다. 또한, 후속 계층의 입력으로 제공되는 각 계층 출력에 대해, 후속 계층에 대한 입력 및 계산량이 임계 크기이고 특정 공간적 및 시간적 지역성을 갖는 경우 이 후속 계층은 계층 출력을 생성한 이전 계층와 병합될 수 있다. 이에 대한 예시는 풀링 계층의 입력으로 제공되는 2D 컨볼루셔널 계층의 계층 출력(예를 들어, conv2d + pooling)에 해당할 수 있다.A candidate architecture may include multiple processing engines, with one or more layers merging into separate processing engines based on predefined merging rules (e.g., conv2d + BN + activation merging, conv2d + maxpooling merging). can be mapped to Merge rules may be predefined, for example, as commands or coded rules of the network graph module 108. In some implementations, two or more graph nodes (or layers) are merged if the computation of the next layer is performed in agreement with the computation of the previous layer (e.g. conv2d (+ BN) + activation). For example, calculations for a batch normalization (BN) layer can be merged with calculations for a 2D convolution layer. Additionally, for each layer output provided as input to a subsequent layer, this subsequent layer may be merged with the previous layer that produced the layer output if the input and computation volume for the subsequent layer is of critical size and has certain spatial and temporal locality. An example of this may correspond to the layer output of a 2D convolutional layer (e.g., conv2d + pooling) provided as the input of the pooling layer.
일부 구현에서, 후보 아키텍처를 튜닝하기 위해, 글로벌 튜너(202)는 대응하는 PE에 대한 각 계층의 초기 매핑을 수행하고 초기 매핑에 대한 성능 추정치를 생성한다. 초기 매핑에 대한 성능 추정치에 기초하여, 글로벌 튜너(202)는 계층의 다양한 조합을 PE에 반복적으로 매핑하여 초기 매핑을 튜닝할 수 있다. 글로벌 튜너(202)는 각 반복에 대한 성능 추정치를 생성하고 성능 추정치가 목표(102)의 PPA 제약 세트와 일치하는 매핑을 식별한다. In some implementations, to tune candidate architectures, global tuner 202 performs an initial mapping of each layer to its corresponding PE and generates performance estimates for the initial mapping. Based on the performance estimate for the initial mapping, the global tuner 202 may tune the initial mapping by iteratively mapping various combinations of layers to PEs. Global tuner 202 generates a performance estimate for each iteration and identifies mappings for which the performance estimate matches the PPA constraint set of target 102.
후보 아키텍처를 튜닝할 때, 글로벌 튜너(202)는 하나 이상의 코스트 모델(214)을 사용하여 다양한 매핑을 통해 반복하고 각 매핑에 대한 성능 파라미터를 계산한다. 성능 파라미터로부터, 시스템(100)은 주어진 PPA 제약 세트(210)에 대해 최적으로 수행되는 계산의 매핑을 식별한다. 일부 구현에서, 시스템(100)은 프로세싱 레인 내에서 작동하는 노드의 시퀀스를 지정하는 시간적 매핑을 사용하여 다양한 벡터 오퍼레이션에 대한 계산 노드를 VPU의 벡터 프로세싱 레인의 서브세트에 반복적으로 매핑할 수 있다. When tuning a candidate architecture, global tuner 202 uses one or more cost models 214 to iterate through various mappings and calculate performance parameters for each mapping. From the performance parameters, system 100 identifies a mapping of computations that performs optimally for a given set of PPA constraints 210. In some implementations, system 100 may recursively map compute nodes for various vector operations to subsets of the VPU's vector processing lanes using temporal mapping that specifies the sequence of nodes operating within the processing lanes.
일부 구현에서, 프레임워크(300)는 아키텍처-웨어 분석 코스트 모델(114)을 사용하여, (1) 각 트라이얼(trial)에 대한 CAS(cycle accurate simulation)는 시간이 많이 걸리고 평가할 고유한 설계 포인트가 수백만에서 수십억에 달하는 경우가 많으며; (2) 신경망의 계산은 계산 집약적이며 중첩 루프(nested loops)로 표현될 수 있으므로 분석 모델이 높은 충실도로 구성될 수 있으므로 인해 각 트라이얼(trial)(하드웨어/신경 구성)의 코스트를 예측한다. 최적화 및 튜닝 모듈(112)은 검색 공간을 샘플링하고, 코스트 모델(114)에 각 설계(디자인) 포인트의 코스트를 쿼리하고, 특정 탐색 궤적(exploration trajectory)을 따라 설계 공간(104)을 검색한다. 각 디자인(설계) 포인트의 코스트와 디자인(설계) 공간(104)의 탐색 궤적은 적어도 각 디자인 포인트의 프로세싱 코스트를 최소화하도록 아키텍처를 튜닝함으로써 후보 아키텍처를 최적화하도록 구현된다. 일부 경우에, 탐색 궤적이 튜너(116)에 의해 사용되는 서로 다른 튜너 알고리즘에 따라 다르다.In some implementations, framework 300 uses architecture-ware analysis cost model 114 to: (1) cycle accurate simulation (CAS) for each trial is time-consuming and has unique design points to evaluate; Often in the millions or billions; (2) The calculation of the neural network is computationally intensive and can be expressed as nested loops, so the analysis model can be constructed with high fidelity to predict the cost of each trial (hardware/neural configuration). The optimization and tuning module 112 samples the search space, queries the cost model 114 for the cost of each design point, and searches the design space 104 along a specific exploration trajectory. The cost of each design point and the search trajectory of the design space 104 are implemented to optimize the candidate architecture by tuning the architecture to at least minimize the processing cost of each design point. In some cases, the search trajectory varies depending on the different tuner algorithms used by tuner 116.
도 4는 다층(복수의) 신경망의 그래프 실행 스케쥴에 관한 예시적인 프로세스(400)의 흐름도이다. 전술한 바와 같이, 글로벌 튜너(202)는 애플리케이션-특정 ML 가속기를 자동으로 생성하는 데 사용되는 출력 구성(230)을 생성한다. 시스템(200)은 예시 코드 생성 모듈(240)을 사용하여 출력 구성(230)을 나타내는 데이터를 HLS 코드로 변환한다. Figure 4 is a flow diagram of an example process 400 for graph execution scheduling of a multi-layer (multiple) neural network. As described above, global tuner 202 generates output configuration 230 that is used to automatically generate application-specific ML accelerators. System 200 uses example code generation module 240 to convert data representing output configuration 230 to HLS code.
신경망 그래프(402)는 맞춤형 애플리케이션-특정 ML 가속기에 대한 것이며 신경망 계층 세트에 대한 예시적인 할당 또는 매핑을 나타낸다. 도 4의 예에서, 제1 신경망 계층 L1은 특정 하드웨어 구성(HW Config)(404a) 및 소프트웨어 구성(SW Config)(404b)에 기초하여 주어진 PE에 매핑될 수 있는 반면, 제2의 다른 신경망 계층 L2는 특정 하드웨어 구성(406a) 및 소프트웨어 구성(406b)에 기초하여 주어진 PE에 매핑될 수 있다. 일부 구현에서, L1 및 L2는 동일한 PE 또는 다른 PE에 매핑될 수 있다. Neural network graph 402 is for a custom application-specific ML accelerator and represents an example assignment or mapping to a set of neural network layers. In the example of Figure 4, a first neural network layer L1 may be mapped to a given PE based on a specific hardware configuration (HW Config) 404a and software configuration (SW Config) 404b, while a second, different neural network layer L2 may be mapped to a given PE based on specific hardware configuration 406a and software configuration 406b. In some implementations, L1 and L2 may be mapped to the same PE or different PEs.
도 5는 애플리케이션-특정 기계 학습 가속기를 생성하고 글로벌적으로 튜닝하기 위한 예시적인 프로세스(500)를 예시하는 흐름도이다. 프로세스(500)는 위에서 설명된 시스템(100)을 사용하여 구현되거나 실행될 수 있다. 프로세스(500)의 설명은 위에서 언급한 시스템(100)의 컴퓨팅 자원(리소스)을 참조할 수 있다. 프로세스(500)의 단계 또는 동작은 본 문서에 설명된 장치 및 리소스의 하나 이상의 프로세서에 의해 실행 가능한 프로그래밍된 펌웨어 또는 소프트웨어 명령어에 의해 활성화될 수 있다. 5 is a flow diagram illustrating an example process 500 for creating and globally tuning an application-specific machine learning accelerator. Process 500 may be implemented or executed using system 100 described above. The description of the process 500 may refer to the computing resources (resources) of the system 100 mentioned above. The steps or operations of process 500 may be activated by programmed firmware or software instructions executable by one or more processors of the devices and resources described herein.
프로세스(500)를 참조하면, 시스템(100)은 아키텍처를 선택한다(502). 예를 들어, 시스템(100)의 제어기는 베이스라인 프로세서 구성을 나타내는 후보 아키텍처를 선택할 수 있다. 후보 아키텍처는 하드웨어 아키텍처와 신경망 그래프에 대응하는 신경 아키텍처를 포함할 수 있다. 일부 구현에서, 아키텍처는 아키텍처 저장소(104)의 하드웨어 레이아웃과 네트워크 그래프 모듈(108)의 신경 아키텍처에 대해 디자인 공간 빌더(204) 및 디자인 공간 익스플로러(212)에 의해 수행되는 검색 오퍼레이션에 기초하여 식별되고 선택된다.Referring to process 500, system 100 selects an architecture (502). For example, a controller of system 100 may select a candidate architecture that represents a baseline processor configuration. Candidate architectures may include hardware architectures and neural architectures corresponding to neural network graphs. In some implementations, the architecture is identified based on a search operation performed by the design space builder 204 and the design space explorer 212 against the hardware layout of the architecture repository 104 and the neural architecture of the network graph module 108; is selected.
시스템(200)은 하나 이상의 튜너 변수 또는 PPA 제약(constraints)(210)에 기초하여 NAS 및 하드웨어 아키텍처 검색 기술을 구현할 수 있다. PPA 제약은 하드웨어 가속기의 성능 요구 사항(performance requirements)을 정의하는 사용자 지정 목표(102)일 수 있다. 예를 들어 요구 사항은 프로세서 사용률, 전력 소비, 프로세싱 지연시간 및 데이터 처리량에 대한 임계값이 될 수 있다. 일부 구현에서, 아키텍처 선택에는 성능 목표(performance objective)를 지정하는 입력 기준을 획득하고, 특수 목표 프로세서를 구현하기 위한 여러 후보 아키텍처를 식별하는 것이 포함된다. 예를 들어, 디자인 공간 빌더(204) 및 익스플로러(212)를 포함하는 디자인 공간(104)을 관리하기 위한 제어 로직은 입력 기준에 기초하여 복수의 후보 아키텍처들 중에서 후보 아키텍처를 선택할 수 있다. System 200 may implement NAS and hardware architecture discovery techniques based on one or more tuner variables or PPA constraints 210. PPA constraints may be custom goals 102 that define the performance requirements of the hardware accelerator. For example, requirements could be thresholds for processor utilization, power consumption, processing latency, and data throughput. In some implementations, architecture selection involves obtaining input criteria that specify a performance objective and identifying several candidate architectures for implementing the special objective processor. For example, control logic for managing the design space 104, including the design space builder 204 and the explorer 212, may select a candidate architecture from a plurality of candidate architectures based on input criteria.
시스템(100)은 아키텍처에 대한 성능 데이터를 생성한다(504). 예를 들어, ML 코스트 모델(214)은 적어도 아키텍처가 다중 신경망 계층을 포함하는 제1 신경망의 계산을 실행하는 방법을 모델링함으로써 후보 아키텍처에 대한 성능 데이터를 생성한다. 일부 구현에서, 신경망은 50 계층 깊이(50 layers deep)의 컨벌루션 신경망인 다층 ResNet-50과 같은 알려진 신경망이다.System 100 generates performance data for the architecture (504). For example, ML cost model 214 generates performance data for a candidate architecture by modeling at least how the architecture executes the computations of a first neural network that includes multiple neural network layers. In some implementations, the neural network is a known neural network, such as multilayer ResNet-50, a convolutional neural network 50 layers deep.
시스템(100)은 성능 데이터에 기초하여 아키텍처를 동적으로 튜닝한다(506). 예를 들어, 성능 데이터에 기초하여, 최적화 및 튜닝 모듈(112)은 하나 이상의 성능 목표를 충족시키기 위해 후보 아키텍처를 동적으로 튜닝한다. 보다 구체적으로, 최적화 및 튜닝 모듈(112)은 아키텍처-웨어 코스트 모델(114)과 상호작용하여 신경망의 각 계층에 대한 후보 아키텍처의 실행을 모델링한다. 예를 들어, ML 코스트 모델(214)은 신경망의 각 계층을 실행할 때 후보 아키텍처가 어떻게 수행되는지에 대한 평가를 제공하는 성능 파라미터를 생성한다. System 100 dynamically tunes the architecture based on performance data (506). For example, based on performance data, optimization and tuning module 112 dynamically tunes the candidate architecture to meet one or more performance goals. More specifically, optimization and tuning module 112 interacts with architecture-wear cost model 114 to model the execution of candidate architectures for each layer of the neural network. For example, ML cost model 214 generates performance parameters that provide an assessment of how a candidate architecture performs when executing each layer of the neural network.
시스템(100)은 성능 파라미터에 기초하여 제1 신경망의 아키텍처 구현을 평가, 튜닝 및 최적화하기 위해 튜닝 루프(tuning loop)(122)를 사용한다. 일부 구현에서, 시스템(100)은 타겟 하드웨어 플랫폼에서 효율적인 신경망 실행을 위해 시스템별로 최적화된 오퍼레이션별 매핑을 발견하기 위해 글로벌 튜닝(예를 들어, 글로벌 튜너(202)을 통해)을 사용한다. 일부 다른 구현에서, 시스템(100)은 다중 계층에 걸친 프로세싱 엔진(PE) 재사용과 같이 허용될 때마다 최적화된 그래프 실행 스케쥴을 발견하기 위해 글로벌 튜닝을 사용한다. 이에 대해서는 도 3을 참조하여 위에서 설명하였다. System 100 uses tuning loop 122 to evaluate, tune, and optimize the architectural implementation of the first neural network based on performance parameters. In some implementations, system 100 uses global tuning (e.g., via global tuner 202) to discover operation-specific mappings that are optimized on a per-system basis for efficient neural network execution on the target hardware platform. In some other implementations, system 100 uses global tuning to discover an optimized graph execution schedule whenever permitted, such as processing engine (PE) reuse across multiple layers. This was explained above with reference to FIG. 3.
예를 들어, 글로벌 튜너(202)는 타겟 애플리케이션에 대해 선택된 신경 아키텍처를 최적화하기 위해 두 개 이상의 계층(예를 들어, L1, L2, L5)을 MAC 또는 컴퓨팅 타일의 동일한 서브세트에 다시 매핑함으로써 후보 아키텍처를 튜닝하도록 구성된다. 아키텍처는 훈련/추론 장치 또는 이미지 분류 워크로드(workload)와 같은 예시적인 애플리케이션에 대해 최적화될 수 있다. 시스템(100)의 제어 로직은 클록 신호의 타이밍을 사용하여, 적절한 시간에, 명령어 및 제어 신호를 최적화 및 튜닝 모듈(112) 및 아키텍처-웨어 코스트 모델(114) 각각에 전송하여 리매핑을 수행하는 데 사용되는 성능 데이터를 생성할 수 있다. 최적화 및 튜닝 모듈(112)은 ML 워크로드를 가속화하는 집적 회로의 하드웨어 레이아웃을 생성하기 위한 애플리케이션-특정 튜닝 및 최적화를 수행하도록 구성된다. 최적화 및 튜닝 모듈(112)(및 코스트 모델(114))은 글로벌 튜너(202)에 의해 수행되는 오퍼레이션의 설명(descriptions of operations)이 최적화 및 튜닝 모듈(112)의 오퍼레이션으로 변환되도록 글로벌 튜너(202)의 일부(또는 모든) 기능을 통합할 수 있다. For example, the global tuner 202 may select candidates by remapping two or more layers (e.g., L1, L2, L5) to the same subset of MAC or compute tiles to optimize the selected neural architecture for the target application. It is configured to tune the architecture. The architecture may be optimized for example applications such as training/inference devices or image classification workloads. The control logic of system 100 uses the timing of clock signals to perform remapping by sending instructions and control signals to optimization and tuning module 112 and architecture-wear cost model 114, respectively, at appropriate times. Performance data can be generated for use. Optimization and tuning module 112 is configured to perform application-specific tuning and optimization to create a hardware layout of an integrated circuit that accelerates ML workloads. The optimization and tuning module 112 (and cost model 114) configures the global tuner 202 such that descriptions of operations performed by the global tuner 202 are converted into operations of the optimization and tuning module 112. ) can be integrated with some (or all) of the functions.
시스템(100)은 아키텍처를 동적으로 튜닝하는 것에 응답하여 ML 가속기의 구성을 생성한다(508). 일부 구현에서, 단계 506의 튜닝 및 최적화는 계층별로 맞춤화되는 하드웨어 아키텍처를 갖는 특수 목표의 집적 회로를 생성하는 것을 허용하는 출력 구성(output configuration)(230)으로 구현된다. 이러한 맞춤화 에스펙트(aspect)를 통해 하드웨어 ML 가속기 회로는 단일 일반 하드웨어 블록에 기초하여 이전 접근 방식에 비해 에너지 효율성을 대폭 향상시킬 수 있다. System 100 generates a configuration of an ML accelerator in response to dynamically tuning the architecture (508). In some implementations, the tuning and optimization of step 506 is implemented with an output configuration 230 that allows creating a special target integrated circuit with a hardware architecture that is customized layer by layer. These custom aspects allow hardware ML accelerator circuits to significantly improve energy efficiency compared to previous approaches based on a single generic hardware block.
예를 들어, 후보 아키텍처를 최적화하고 튜닝한 후, 시스템(100)은 애플리케이션-특정 ML 가속기를 생성하기 위해 적어도 코드 생성 모듈(240)에 의해 사용될 수 있도록 다양한 아키텍처 특징 및 스케줄링/매핑 전략을 포함하는 호환 가능한 하드웨어 구성(230)을 생성한다. 시스템(200)은 코드 생성 모듈(240)을 사용하여 구성(230)을 나타내는 데이터를 HLS(High-level synthesis) 코드로 변환한다. 코드 생성 모듈(240)은 HLS(High-level synthesis)를 사용하여 하드웨어 가속기에 대한 ML 알고리즘의 펌웨어 구현을 생성할 수 있다. 그런 다음 시스템(100)은 펌웨어 구현 및 HLS 오퍼레이션에 기초하여 애플리케이션-특정 하드웨어 ML 가속기를 생성할 수 있다(510). For example, after optimizing and tuning a candidate architecture, system 100 may include various architectural features and scheduling/mapping strategies to be used by at least code generation module 240 to generate an application-specific ML accelerator. Create a compatible hardware configuration (230). System 200 uses code generation module 240 to convert data representing configuration 230 into high-level synthesis (HLS) code. Code generation module 240 may use high-level synthesis (HLS) to generate a firmware implementation of an ML algorithm for a hardware accelerator. System 100 may then create an application-specific hardware ML accelerator based on the firmware implementation and HLS operations (510).
도 6은 예시적인 애플리케이션-특정 하드웨어 ML 가속기(600)의 블록도이다. 하드웨어 가속기(600)는 적어도 시스템(100 및 200)의 예시적인 동작을 포함하여 본 문서에 개시된 기술을 사용하여 생성된다. 코드 생성기(240)를 사용하여, 시스템(100)은 하드웨어 회로의 각 부분을 지정하는 애플리케이션-특정 ML 가속기(600)에 대한 하드웨어 레이아웃을 생성하도록 구성되며, 각 하드웨어 회로는 신경망의 특정 계층을 실행하도록 맞춤화될 수 있다. Figure 6 is a block diagram of an example application-specific hardware ML accelerator 600. Hardware accelerator 600 is created using techniques disclosed herein, including at least example operations of systems 100 and 200. Using code generator 240, system 100 is configured to generate a hardware layout for application-specific ML accelerator 600 specifying each portion of hardware circuitry, each hardware circuit executing a particular layer of a neural network. It can be customized to:
하드웨어 가속기(600)는 스트리밍 및 파이프라인 방식으로 하나 이상의 계층(예를 들어 공통 속성을 공유하는 경우)을 실행하기 위해 별도의 하드웨어 블록(603a, 603b, 603c, 603d, 603e, 603f)을 사용할 수 있다. 각각의 하드웨어 블록(603)은 예를 들어 하드웨어 가속기(600) 전반에 걸쳐 낮은 전력 및 높은 활용을 가능하게 하기 위해 이러한 계층(예를 들어, 양자화, 계층 특정 타일링, 싸이스톨릭 배열 차원 등)에 특별히 맞춰져(tailored) 있다. 일부 구현에서, 각 하드웨어 블록(103)은 신경망의 특정 계층과의 연관 또는 매핑을 갖고, 신경망의 계층(예: 위에서 설명한 L1, L2, L3, L4 또는 L5)과 하드웨어 블록(103)의 연관은 부분적으로 신경망의 해당 계층과 관련된 기능 및 최적화 노력에 기초한다. Hardware accelerator 600 may use separate hardware blocks 603a, 603b, 603c, 603d, 603e, 603f to execute one or more layers (e.g., if they share common properties) in a streaming and pipelined manner. there is. Each hardware block 603 may be configured to provide information on these layers (e.g., quantization, layer-specific tiling, systolic array dimensions, etc.), for example, to enable low power and high utilization across hardware accelerators 600. It is specially tailored. In some implementations, each hardware block 103 has an association or mapping with a particular layer of the neural network, and the association of the hardware block 103 with a layer of the neural network (e.g., L1, L2, L3, L4, or L5 described above) It is based in part on the functionality and optimization efforts associated with that layer of the neural network.
데이터 흐름 표시(601a, 601b, 601c, 601d, 601e, 601f)는 하드웨어 블록(603) 사이의 신경망 데이터 통신의 예시적인 시퀀스를 제공한다. 일부 구현에서, 이들 데이터 흐름 표시(601a, 601b, 601c, 601d, 601e, 601f)는 예를 들어 글로벌 튜너(202)의 최적화 및 튜닝 오퍼레이션에 기초하여 미리 구성된 통신 시퀀스이다. 전달되는 신경망 데이터는 특정 하드웨어 블록(603)에서의 계산 유닛의 출력, 신경망 입력/활성화, 파라미터 가중치 데이터 및 기타 신경망 파라미터 관련 데이터와 같은 계산 결과 데이터를 포함할 수 있다. Data flow representations 601a, 601b, 601c, 601d, 601e, 601f provide example sequences of neural network data communication between hardware blocks 603. In some implementations, these data flow representations 601a, 601b, 601c, 601d, 601e, 601f are pre-configured communication sequences, for example based on optimization and tuning operations of global tuner 202. The transmitted neural network data may include calculation result data, such as the output of a calculation unit in a specific hardware block 603, neural network input/activation, parameter weight data, and other neural network parameter-related data.
각각의 하드웨어 블록(603)은 타겟 애플리케이션에 맞게 맞춤화된 마이크로아키텍처를 포함할 수 있다. 글로벌 튜너(202)는 시스템 레벨에서 아키텍처 설계의 균형을 맞추기 위해 글로벌 튜닝 오퍼레이션에서 서로 다른 하드웨어 블록에 걸친 통신을 최적화하도록 구성된다. 이러한 최적화에는 데이터 전송 속도 일치를 위한 인터페이스 타일링(interface tiling), 계산 시 등급 일치(rating matching)를 위한 계산 블록 수(예: 입력 채널 차단), 버퍼 크기 튜닝 등이 포함된다. 예를 들어, 하드웨어 블록(603a)은 인터-다이(inter-die) 입력 블록(606a, 609b), 인터-다이 출력 블록(611a, 611b) 및 호스트 인터페이스 유닛(613)을 포함할 수 있으며, 하드웨어 블록(603b)은 인터-다이 입력 블록(621a, 621b), 인터-다이 출력 블록(623a, 623b) 및 호스트 인터페이스 유닛(614)을 포함한다.Each hardware block 603 may include a microarchitecture customized for the target application. The global tuner 202 is configured to optimize communication across different hardware blocks in global tuning operations to balance the architectural design at the system level. These optimizations include interface tiling to match data rates, number of compute blocks for rating matching in computations (e.g. blocking input channels), and buffer size tuning. For example, hardware block 603a may include inter-die input blocks 606a, 609b, inter-die output blocks 611a, 611b, and host interface unit 613, Block 603b includes inter-die input blocks 621a and 621b, inter-die output blocks 623a and 623b, and host interface unit 614.
가속기(600)의 맞춤형 구성은 하드웨어 블록(603a)에 매핑되는 신경망의 제1 계층과 하드웨어 블록(603d)에 매핑되는 신경망의 마지막 계층을 포함할 수 있다. 글로벌 튜너(202)는 효율적인 신경망 실행을 위한 오퍼레이션별 공간 매핑(per-op spatial mappings)과 PPA 제약(210)의 크기/영역 제약 사이의 상호 작용의 균형을 맞추기 위해 예를 들어 하드웨어 블록(603a, 603d)들 사이에 피드백 계층을 통합하도록 이 아키텍처를 구성할 수 있다. 예를 들어, 하드웨어 가속기(600)는 최소한의 하드웨어를 사용하여 신경망 계산을 효율적으로 수행하는 동시에 애플리케이션-특정 요구 사항에 따라 처리량/지연시간을 일치시킬 수 있도록 구성된다. A custom configuration of accelerator 600 may include a first layer of the neural network mapped to hardware block 603a and a last layer of the neural network mapped to hardware block 603d. The global tuner 202 may use, for example, hardware blocks 603a, to balance the interaction between per-op spatial mappings for efficient neural network execution and the size/area constraints of the PPA constraints 210. This architecture can be configured to integrate a feedback layer between 603d). For example, hardware accelerator 600 is configured to efficiently perform neural network calculations using minimal hardware while matching throughput/latency according to application-specific requirements.
도 7은 입력 텐서(704), 출력 텐서(708) 및 가중치 텐서(706)의 변형을 포함하는 텐서 또는 다차원 행렬(700)의 예를 도시한다. 텐서(700)는 가속기(600)와 같은 ML 하드웨어 가속기를 사용하여 프로세싱되거나 생성되는 예시적인 기계 학습 데이터 구조이다. 예를 들어, 시스템(100)은 적어도 텐서(704 및 706)를 프로세싱하기 위한 후보 아키텍처를 튜닝 및 최적화하고, 이러한 텐서와 연관된 데이터를 수신하고 프로세싱하는 신경망을 구현하도록 구성된 맞춤형 하드웨어 ML 가속기(600)를 자동으로 생성하는 데 사용될 수 있다. 7 shows an example of a tensor or multidimensional matrix 700 that includes transformations of an input tensor 704, an output tensor 708, and a weight tensor 706. Tensor 700 is an example machine learning data structure that is processed or generated using an ML hardware accelerator, such as accelerator 600. For example, system 100 may include a custom hardware ML accelerator 600 configured to tune and optimize candidate architectures for processing at least tensors 704 and 706 and implement neural networks to receive and process data associated with these tensors. Can be used to automatically generate .
각각의 텐서(700)는 신경망의 특정 계층에서 수행되는 계산을 위한 데이터 값에 대응하는 요소를 포함한다. 계산은 다른 신경망 계층에 입력으로 제공될 수 있는 활성화/출력 값과 같은 출력을 생성하기 위해 하나 이상의 클록 사이클(주기)에서 입력/활성화 텐서(704)와 파라미터/가중치 텐서(706)를 곱하는 것을 포함할 수 있다. 도 7의 예에서, 출력 세트의 각 출력은 출력 텐서(708)의 각 요소에 대응할 수 있다. 일부 예에서 입력 텐서(704)는 활성화 텐서(activation tensor)이다. 활성화 텐서(704)를 해당 가중치 텐서(706)와 곱하는 것은 텐서(704)의 요소로부터의 활성화를 텐서(706)의 요소로부터의 가중치와 곱하여 부분합(들)을 생성하는 것을 포함한다. Each tensor 700 includes elements corresponding to data values for calculations performed in a specific layer of the neural network. The computation involves multiplying the input/activation tensor 704 and the parameter/weight tensor 706 in one or more clock cycles (cycles) to produce an output equal to the activation/output value that can be provided as input to another neural network layer. can do. In the example of Figure 7, each output in the output set may correspond to each element of output tensor 708. In some examples, input tensor 704 is an activation tensor. Multiplying the activation tensor 704 with the corresponding weight tensor 706 includes multiplying the activations from the elements of the tensor 704 with the weights from the elements of the tensor 706 to produce subtotal(s).
일부 구현에서, ML 가속기(600)의 하드웨어 블록(603)은 벡터에서 오퍼레이팅하는 각각의 프로세서 코어이며, 이는 일부 다차원 텐서의 동일한(또는 다른) 차원을 따라 여러 개별 요소를 포함할 수 있다. 여러 요소 각각은 텐서의 차원에 따라 X,Y 좌표(2D) 또는 X,Y,Z 좌표(3D)를 사용하여 표현될 수 있다. ML 가속기(600)의 하드웨어 레이아웃은 주어진 PPA 제약 세트에 따라 여러 부분합을 계산하도록 최적화될 수 있다. 부분합은 배치 입력(batch inputs)에 해당 가중치 값을 곱하여 생성된 곱셈 결과에 해당한다.In some implementations, hardware block 603 of ML accelerator 600 is each processor core operating on a vector, which may contain multiple individual elements along the same (or different) dimensions of some multidimensional tensor. Each of the multiple elements can be expressed using X,Y coordinates (2D) or X,Y,Z coordinates (3D) depending on the dimension of the tensor. The hardware layout of ML accelerator 600 can be optimized to compute multiple subtotals depending on a given set of PPA constraints. Subtotals correspond to the multiplication result created by multiplying batch inputs by the corresponding weight values.
입력 가중치 곱셈은 입력 텐서(704)의 행 또는 슬라이스와 같은 입력 볼륨의 이산 입력(discrete inputs)과 곱해진 각 가중치 요소의 곱의 합으로 기록될 수 있다. 이 행 또는 슬라이스는 입력 텐서(704)의 제1 차원(710) 또는 입력 텐서(704)의 다른 제2 차원(715)과 같은 주어진 차원을 나타낼 수 있다. 차원은 하드웨어 블록(603)에 걸쳐 다양한 벡터 프로세싱 장치에 매핑될 수 있으므로, ML 가속기(600)는 주어진 입력 목표 세트(102)에 따라, 로드 불균형(load imbalances)을 방지하고 그리고 각 하드웨어 블록(603)에서 임계 프로세싱 활용도를 달성하는 방식으로 계산을 정기적으로 수행한다. Input weight multiplication can be written as the sum of the products of each weight element multiplied by discrete inputs of the input volume, such as rows or slices of the input tensor 704. This row or slice may represent a given dimension, such as a first dimension 710 of the input tensor 704 or another second dimension 715 of the input tensor 704. Dimensions can be mapped to various vector processing units across hardware blocks 603, so that ML accelerator 600 can prevent load imbalances and, depending on a given set of input targets 102, ), the computation is performed periodically in a way that achieves a critical processing utilization.
일부 구현에서, 예시적인 계산 세트를 사용하여 컨벌루션 신경망 계층에 대한 출력을 계산할 수 있다. CNN 계층에 대한 계산은 3D 입력 텐서(704)와 적어도 하나의 3D 필터(가중치 텐서(706)) 간의 2D 공간 컨볼루션을 수행하는 것을 포함할 수 있다. 예를 들어, 3D 입력 텐서(704)에 대해 하나의 3D 필터(706)를 컨볼루션하면 2D 공간 평면(720 또는 725)이 생성될 수 있다. 계산에는 입력 볼륨의 특정 차원에 대한 내적 합계 계산이 포함될 수 있다. 예를 들어, 공간 평면(720)은 차원(710)을 따른 입력으로부터 계산된 곱의 합에 대한 출력 값을 포함할 수 있는 반면, 공간 평면(725)은 차원(715)을 따른 입력으로부터 계산된 곱의 합에 대한 출력 값을 포함할 수 있다. 각각의 공간 평면(720 및 725)의 출력 값에 대한 곱의 합을 생성하기 위한 계산은 이 문서에 설명된 기술을 사용하여 생성되고 튜닝되는 하드웨어 블록(603)을 사용하여 수행될 수 있다. In some implementations, an example set of computations may be used to compute the output for a convolutional neural network layer. Computations for the CNN layer may include performing a 2D spatial convolution between the 3D input tensor 704 and at least one 3D filter (weight tensor 706). For example, convolving one 3D filter 706 with a 3D input tensor 704 may produce a 2D spatial plane 720 or 725. The calculation may include calculating the dot product over certain dimensions of the input volume. For example, spatial plane 720 may contain output values for the sum of products calculated from input along dimension 710, while spatial plane 725 may contain output values for the sum of products calculated from input along dimension 715. Can include output values for the sum of products. Calculations to generate the sum of the products of the output values of each spatial plane 720 and 725 may be performed using hardware block 603, which is generated and tuned using techniques described in this document.
본 명세서에 기술된 요지 및 기능적 동작의 실시예는 디지털 전자 회로, 유형으로 구현된 컴퓨터 소프트웨어 또는 펌웨어, 본 명세서에 개시된 구조 및 그 구조적 등가물을 포함하는 컴퓨터 하드웨어, 또는 이들 중 하나 이상의 조합으로 구현될 수 있다. 본 명세서에 설명된 요지의 실시예는 하나 이상의 컴퓨터 프로그램, 즉 데이터 프로세싱 장치에 의해 실행되거나 데이터 프로세싱 장치의 동작을 제어하기 위해 유형의 비일시적 프로그램 캐리어에 인코딩된 컴퓨터 프로그램 명령어의 하나 이상의 모듈로 구현될 수 있다. Embodiments of the subject matter and functional operations described herein may be implemented in digital electronic circuitry, tangible computer software or firmware, computer hardware including the structures disclosed herein and structural equivalents thereof, or a combination of one or more of these. You can. Embodiments of the subject matter described herein may be embodied in one or more computer programs, i.e., one or more modules of computer program instructions encoded in a tangible, non-transitory program carrier for execution by or to control the operation of the data processing device. It can be.
대안적으로 또는 추가적으로, 프로그램 명령어는 데이터 프로세싱 장치에 의한 실행을 위해 적절한 수신기 장치로 전송하기 위한 정보를 인코딩하기 위해 생성된 인공적으로 생성된 전파 신호, 예를 들어 기계 생성 전기, 광학 또는 전자기 신호에 인코딩될 수 있다. 컴퓨터 저장 매체는 기계 판독 가능 저장 장치, 기계 판독 가능 저장 기판, 랜덤 또는 직렬 액세스 메모리 장치, 또는 이들 중 하나 이상의 조합일 수 있다. Alternatively or additionally, the program instructions may be translated into artificially generated radio signals, e.g., machine-generated electrical, optical, or electromagnetic signals, to encode information for transmission to a suitable receiver device for execution by a data processing device. Can be encoded. A computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of these.
"컴퓨팅 시스템"이라는 용어는 예를 들어 프로그래밍 가능한 프로세서, 컴퓨터 또는 다중 프로세서 또는 컴퓨터를 포함하여 데이터를 프로세싱하기 위한 모든 종류의 장치, 장치 및 기계를 포함한다. 장치는 예를 들어 FPGA(field programmable gate array) 또는 ASIC(application specific integrated circuit)와 같은 특수 목표 논리 회로를 포함할 수 있다. 장치는 또한 하드웨어에 추가하여 언급한 컴퓨터 프로그램에 대한 실행 환경을 생성하는 코드, 예를 들어 프로세서 펌웨어, 프로토콜 스택, 데이터베이스 관리 시스템, 운영 체제 또는 이들의 조합을 구성하는 코드를 포함할 수 있다. The term “computing system” includes all types of devices, devices and machines for processing data, including, for example, programmable processors, computers or multiprocessors or computers. The device may include special target logic circuitry, for example, a field programmable gate array (FPGA) or an application specific integrated circuit (ASIC). The device may also include, in addition to hardware, code that creates an execution environment for the mentioned computer programs, for example, code that constitutes processor firmware, protocol stack, database management system, operating system, or combinations thereof.
컴퓨터 프로그램 (프로그램, 소프트웨어, 소프트웨어 애플리케이션, 모듈, 소프트웨어 모듈, 스크립트 또는 코드라고도 함)은 컴파일 언어, 해석 언어, 선언적 언어, 절차적 언어 등 모든 형태의 프로그래밍 언어로 작성될 수 있으며, 독립 실행형 프로그램, 모듈, 컴포넌트, 서브루틴 또는 컴퓨팅 환경에서 사용하기에 적합한 기타 장치를 포함하여 모든 형태로 배포될 수 있다.A computer program (also known as a program, software, software application, module, software module, script, or code) can be written in any form of programming language, including a compiled language, an interpreted language, a declarative language, a procedural language, or a stand-alone program. , may be distributed in any form, including as a module, component, subroutine, or other device suitable for use in a computing environment.
컴퓨터 프로그램은 파일 시스템의 파일에 해당할 수 있지만 반드시 그럴 필요는 없다. 프로그램은 다른 프로그램이나 데이터를 보유하는 파일의 일부(예: 마크업 언어 문서에 저장된 하나 이상의 스크립트), 해당 프로그램 전용 단일 파일 또는 여러 개의 튜닝된 파일(예: 하나 이상의 모듈, 서브 프로그램 또는 코드 일부를 저장하는 파일이다. 컴퓨터 프로그램은 하나의 컴퓨터 또는 한 사이트에 위치하거나 여러 사이트에 걸쳐 분산되고 통신 네트워크로 연결된 여러 컴퓨터에서 실행되도록 배포될 수 있다. Computer programs can, but do not have to, correspond to files in a file system. A program may contain other programs or parts of files that hold data (such as one or more scripts stored in a markup language document), a single file dedicated to that program, or multiple tuned files (such as one or more modules, subprograms, or portions of code). It is a file that stores a computer program that can be distributed to run on a single computer or site, or distributed across multiple sites and connected by a communications network.
본 명세서에 설명된 프로세스 및 논리 흐름은 입력 데이터에 대해 작동하고 출력을 생성함으로써 기능을 수행하는 하나 이상의 컴퓨터 프로그램을 실행하는 하나 이상의 프로그래밍 가능한 컴퓨터에 의해 수행될 수 있다. 프로세스 및 논리 흐름은 장치에 의해 수행될 수도 있으며, 장치는 FPGA(field programmable gate array), ASIC(application specific integrated circuit) 또는 GPGPU(General purpose graphics processing unit)와 같은 특수 목표 논리 회로로 구현될 수도 있다. The processes and logic flows described herein may be performed by one or more programmable computers executing one or more computer programs that perform functions by operating on input data and producing output. Processes and logic flows may be performed by devices, which may be implemented as special target logic circuits such as field programmable gate arrays (FPGAs), application specific integrated circuits (ASICs), or general purpose graphics processing units (GPGPUs). .
컴퓨터 프로그램의 실행에 적합한 컴퓨터는 예를 들어 범용 또는 특수 목표의 마이크로프로세서 또는 둘 다에 기반할 수 있거나 다른 종류의 중앙 프로세싱 장치에 기초하여 할 수 있다. 일반적으로 중앙 프로세싱 장치는 읽기 전용 메모리나 랜덤 액세스 메모리 또는 둘 다로부터 명령어와 데이터를 수신한다. 컴퓨터의 일부 요소는 명령어를 수행하거나 실행하기 위한 중앙 프로세싱 장치와 명령어 및 데이터를 저장하기 위한 하나 이상의 메모리 장치이다. 일반적으로, 컴퓨터는 또한 데이터를 저장하기 위한 하나 이상의 대용량 저장 장치, 예를 들어 자기, 광자기 디스크 또는 광 디스크로부터 데이터를 수신하거나 전송하거나 둘 모두를 포함하거나 작동 가능하게 결합될 것이다. 그러나 컴퓨터에 그러한 장치가 있을 필요는 없다. 더욱이, 컴퓨터는 다른 장치, 예를 들어 휴대폰, PDA(Personal Digital Assistant), 모바일 오디오 또는 비디오 플계층, 게임 콘솔, GPS(Global Positioning System) 수신기 또는 휴대용 저장 장치(예를 들어, USB(범용 직렬 버스) 플래시 드라이브 등)에 내장될 수 있다. A computer suitable for the execution of computer programs may be based, for example, on a general-purpose or special-purpose microprocessor, or both, or on another type of central processing unit. Typically, the central processing unit receives instructions and data from read-only memory, random access memory, or both. Some elements of a computer are a central processing unit to execute or execute instructions and one or more memory devices to store instructions and data. Typically, a computer will also include, or be operably coupled to, one or more mass storage devices for storing data, such as magnetic, magneto-optical or optical disks, or both. However, your computer does not need to have such a device. Moreover, computers can connect to other devices, such as cell phones, personal digital assistants (PDAs), mobile audio or video processors, game consoles, Global Positioning System (GPS) receivers, or portable storage devices (e.g., Universal Serial Bus (USB) devices). ) can be embedded in a flash drive, etc.
컴퓨터 프로그램 명령어 및 데이터를 저장하기에 적합한 컴퓨터 판독 가능 매체는 예를 들어 EPROM, EEPROM 및 플래시 메모리 장치와 같은 반도체 메모리 장치; 자기 디스크(예: 내부 하드 디스크 또는 이동식 디스크); 광자기 디스크; CD ROM 및 DVD-ROM 디스크를 포함하는, 모든 형태의 비휘발성 메모리, 매체 및 메모리 장치를 포함한다. 프로세서와 메모리는 특수 목표 논리 회로로 보완되거나 통합될 수 있다. Computer-readable media suitable for storing computer program instructions and data include, for example, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; Magnetic disks (such as internal hard disks or removable disks); magneto-optical disk; Includes all forms of non-volatile memory, media and memory devices, including CD ROM and DVD-ROM disks. Processors and memories can be supplemented or integrated with special target logic circuits.
사용자와의 상호작용을 제공하기 위해, 본 명세서에 설명된 요지의 실시예는 사용자에게 정보를 표시하기 위한 디스플레이 장치, 예를 들어 LCD(액정 디스플레이) 모니터, 사용자가 정보를 제공할 수 있는 키보드 및 포인팅 장치(예: 사용자가 컴퓨터에 입력을 제공할 수 있는 마우스 또는 트랙볼)를 갖춘 컴퓨터에서 구현될 수 있다. 사용자와의 상호작용을 제공하기 위해 다른 종류의 장치도 사용될 수 있다(예를 들어, 사용자에게 제공되는 피드백은 시각적 피드백, 청각 피드백 또는 촉각 피드백과 같은 임의의 형태의 감각 피드백일 수 있으며; 그리고 사용자의 입력은 음향, 음성 또는 촉각 입력을 포함한 모든 형태로 수신될 수 있다). 또한 컴퓨터는 사용자가 사용하는 장치와 문서를 주고받는 방식으로 사용자와 상호 작용할 수 있으며; 예를 들어, 웹 브라우저에서 받은 요청에 대한 응답으로 사용자 클라이언트 장치의 웹 브라우저로 웹 페이지를 보낸다.To provide interaction with a user, embodiments of the subject matter described herein may include a display device for displaying information to a user, such as an LCD (liquid crystal display) monitor, a keyboard through which a user can present information, and It may be implemented on a computer equipped with a pointing device (e.g., a mouse or trackball) that allows the user to provide input to the computer. Other types of devices may also be used to provide interaction with the user (for example, the feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and The input may be received in any form, including acoustic, voice, or tactile input). The computer can also interact with the user by sending and receiving documents to and from the device the user is using; For example, sending a web page to a web browser on a user's client device in response to a request received from the web browser.
본 명세서에 설명된 요지의 실시예는 예를 들어 데이터 서버로서 백엔드 컴포넌트를 포함하고, 미들웨어 컴포넌트(예: 애플리케이션 서버)를 포함하고, 프런트 엔드 컴포넌트(예를 들어 사용자가 본 명세서에 설명된 요지의 구현과 상호 작용할 수 있는 그래픽 사용자 인터페이스 또는 웹 브라우저를 갖는 클라이언트 컴퓨터)를 포함하고, 백엔드, 미들웨어 또는 프런트엔드 컴포넌트 중 하나 이상의 조합을 포함하는 컴퓨팅 시스템에서 구현될 수 있다. 시스템의 컴포넌트는 통신 네트워크와 같은 디지털 데이터 통신의 모든 형태나 매체를 통해 상호 연결될 수 있다. 통신 네트워크의 예로는 근거리 통신망("LAN") 및 광역 통신망("WAN"), 예를 들어 인터넷이 포함된다. Embodiments of the subject matter described herein include a backend component, e.g., as a data server, a middleware component (e.g., an application server), and a front-end component (e.g., a server that allows users to access the subject matter described herein). and a client computer having a graphical user interface or web browser capable of interacting with the implementation. The components of the system may be interconnected through any form or medium of digital data communication, such as a telecommunications network. Examples of communications networks include local area networks (“LANs”) and wide area networks (“WANs”), such as the Internet.
컴퓨팅 시스템에는 클라이언트와 서버가 포함될 수 있다. 클라이언트와 서버는 일반적으로 서로 멀리 떨어져 있으며 일반적으로 통신 네트워크를 통해 상호 작용한다. 클라이언트와 서버의 관계는 각 컴퓨터에서 실행되고 서로 클라이언트-서버 관계를 갖는 컴퓨터 프로그램으로 인해 발생한다. A computing system may include clients and servers. Clients and servers are usually remote from each other and typically interact through a communications network. The relationship between client and server arises due to computer programs running on each computer and having a client-server relationship with each other.
본 명세서는 많은 특정 구현 세부사항을 포함하고 있지만, 이는 임의의 발명의 범위 또는 청구될 수 있는 범위에 대한 제한으로 해석되어서는 안 되며, 오히려 특정 발명의 특정 실시예에 특정할 수 있는 특징에 대한 설명으로 해석되어야 한다. 별도의 실시예와 관련하여 본 명세서에 설명된 특정 특징은 단일 실시예에서 조합하여 구현될 수도 있다. 반대로, 단일 실시예의 맥락에서 설명된 다양한 특징은 다중 실시예에서 개별적으로 또는 임의의 적절한 하위 조합으로 구현될 수도 있다. 더욱이, 기능은 위에서 특정 조합으로 작용하는 것으로 설명될 수 있고 처음에는 그렇게 주장되기도 하지만, 청구된 조합의 하나 이상의 기능은 경우에 따라 조합에서 삭제될 수 있으며, 청구된 조합은 하위 조합 또는 하위 조합의 변형에 관한 것일 수 있다. Although this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. It should be interpreted as Certain features described herein in relation to separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features described in the context of a single embodiment may also be implemented in multiple embodiments individually or in any suitable sub-combination. Moreover, although features may be described above and initially claimed to function in certain combinations, one or more features of a claimed combination may in some cases be deleted from the combination, and the claimed combination may be modified from a sub-combination or sub-combination. It could be about transformation.
유사하게, 동작이 특정 순서로 도면에 도시되어 있지만, 이는 그러한 동작이 도시된 특정 순서 또는 순차적인 순서로 수행되거나 도시된 모든 동작이 수행되어 바람직한 결과를 달성할 것을 요구하는 것으로 이해되어서는 안 된다. 특정 상황에서는 멀티태스킹과 병렬 처리가 유리할 수 있다. 더욱이, 전술한 실시예에서 다양한 시스템 모듈 및 컴포넌트의 분리는 모든 실시예에서 그러한 분리를 요구하는 것으로 이해되어서는 안 된다. 기술된 프로그램 컴포넌트 및 시스템은 일반적으로 단일 소프트웨어 제품에 함께 통합되거나 여러 소프트웨어 제품에 패키지될 수 있다는 점을 이해해야 한다.Similarly, although operations are shown in the drawings in a particular order, this should not be construed to require that such operations be performed in the specific order or sequential order shown or that all operations shown be performed to achieve the desired results. . In certain situations, multitasking and parallel processing can be advantageous. Moreover, the separation of various system modules and components in the foregoing embodiments should not be construed as requiring such separation in all embodiments. It should be understood that the program components and systems described generally may be integrated together in a single software product or may be packaged into multiple software products.
요지의 특정 실시예가 설명되었다. 다른 실시예는 다음 청구범위의 범위 내에 있다. 예를 들어, 청구범위에 인용된 작업은 다른 순서로 수행될 수 있으며 여전히 원하는 결과를 얻을 수 있다. 일례로서, 첨부 도면에 도시된 프로세스는 바람직한 결과를 달성하기 위해 도시된 특정 순서 또는 순차적 순서를 반드시 필요로 하는 것은 아니다. 특정 구현에서는 멀티태스킹 및 병렬 처리가 유리할 수 있다.Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the operations recited in the claims can be performed in a different order and still obtain the desired results. By way of example, the processes depicted in the accompanying drawings do not necessarily require the specific order or sequential order shown to achieve desirable results. Multitasking and parallel processing may be advantageous in certain implementations.
Claims (20)
베이스라인 프로세서 구성을 나타내는 아키텍처를 선택하는 단계;
ML 코스트 모델에 의해, 적어도 아키텍처가 복수의 계층을 포함하는 제1 신경망의 계산을 실행하는 방법을 모델링함으로써 아키텍처에 대한 성능 데이터를 생성하는 단계;
상기 성능 데이터에 기초하여, 상기 아키텍처가 상기 제1 신경망을 구현하고 그리고 타겟 애플리케이션에 대한 기계 학습 계산을 실행할 때 성능 목표를 충족하도록 상기 아키텍처를 동적으로 튜닝하는 단계;
상기 아키텍처를 동적으로 튜닝하는 것에 응답하여, 상기 제1 신경망의 복수의 계층 각각을 구현하기 위한 맞춤형 하드웨어 구성을 지정하는 ML 가속기의 구성을 생성하는 단계를 포함하는, 컴퓨터로 구현되는 방법.1. A computer-implemented method for creating an application-specific machine learning (ML) accelerator, comprising:
selecting an architecture representing a baseline processor configuration;
generating performance data for the architecture by modeling, by an ML cost model, at least how the architecture executes computations of a first neural network including a plurality of layers;
Based on the performance data, dynamically tuning the architecture to meet performance goals when the architecture implements the first neural network and executes machine learning calculations for a target application;
In response to dynamically tuning the architecture, generating a configuration of an ML accelerator specifying a custom hardware configuration for implementing each of the plurality of layers of the first neural network.
상기 맞춤형 하드웨어 구성에 기초하여 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 단계를 더 포함하며,
상기 애플리케이션-특정 하드웨어 ML 가속기는 신경망이 타겟 애플리케이션에 대한 계산을 실행하는 데 사용될 때 신경망의 서로 다른 계층 각각을 구현하도록 최적화되는, 컴퓨터로 구현되는 방법.The method of claim 1, wherein
further comprising creating an application-specific hardware ML accelerator based on the customized hardware configuration,
The computer-implemented method of claim 1, wherein the application-specific hardware ML accelerator is optimized to implement each of the different layers of the neural network when the neural network is used to perform computations for a target application.
애플리케이션-특정 하드웨어 ML 가속기가 타겟 애플리케이션에 대한 계산을 실행할 때 복수의 이산적 목표 중 각각의 이산적 목표를 충족시키도록 구성된 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 단계를 포함하는, 컴퓨터로 구현되는 방법.3. The method of claim 2, wherein the performance objective includes a plurality of discrete objectives, and generating an application-specific ML accelerator comprises:
A computer-implemented method comprising generating an application-specific hardware ML accelerator configured to satisfy each discrete objective of a plurality of discrete objectives when the application-specific hardware ML accelerator executes a computation for a target application. .
ML 코스트 모델에 의해, 제1 신경망의 복수의 계층 중 각 계층을 실행하기 위한 상기 아키텍처의 사용을 모델링하는 단계; 그리고
각 계층을 실행하기 위한 아키텍처의 사용을 모델링하는 것에 응답하여, 상기 ML 코스트 모델에 의해 복수의 계층 각각에 대한 상기 아키텍처의 성능 파라미터를 생성하는 단계를 포함하는, 컴퓨터로 구현되는 방법.The method of claim 3, wherein generating the performance data comprises:
modeling, by an ML cost model, use of the architecture to execute each layer of the plurality of layers of a first neural network; and
In response to modeling the use of the architecture to execute each layer, generating performance parameters of the architecture for each of a plurality of layers by the ML cost model.
상기 성능 파라미터는 상기 복수의 이산적 목표 중 각각의 이산적 목표에 대응하며; 그리고
상기 복수의 이산적 목표는 임계 프로세싱 지연시간, 임계 전력 소비, 임계 데이터 처리량 및 임계 프로세서 활용 중 적어도 하나를 포함하는, 컴퓨터로 구현되는 방법.According to paragraph 4,
the performance parameter corresponds to each discrete objective of the plurality of discrete objectives; and
The computer-implemented method of claim 1, wherein the plurality of discrete goals include at least one of threshold processing latency, threshold power consumption, threshold data throughput, and threshold processor utilization.
상기 애플리케이션-특정 하드웨어 ML 가속기가 하드웨어 ML 가속기의 하드웨어 컴퓨팅 유닛의 임계 백분율을 활용하게 하는 입력 텐서에 대한 계산의 매핑을 결정하는 단계; 그리고
상기 결정된 매핑에 기초하여 상기 아키텍처를 동적으로 튜닝하는 단계를 포함하는, 컴퓨터로 구현되는 방법.The method of claim 2, wherein dynamically tuning the architecture comprises:
determining a mapping of computations to input tensors that causes the application-specific hardware ML accelerator to utilize a threshold percentage of hardware compute units of the hardware ML accelerator; and
Dynamically tuning the architecture based on the determined mapping.
글로벌 튜너의 복수의 ML 코스트 모델 각각에 의해 수행되는 오퍼레이션에 기초하여 상기 아키텍처를 동적으로 튜닝하는 단계; 그리고
상기 글로벌 튜너의 시뮬레이트된 어닐링 튜너 또는 랜덤 튜너 중 적어도 하나에 의해 수행되는 오퍼레이션에 기초하여 상기 아키텍처를 동적으로 튜닝하는 단계를 포함하는, 컴퓨터로 구현되는 방법.The method of claim 6, wherein dynamically tuning the architecture comprises:
dynamically tuning the architecture based on operations performed by each of a plurality of ML cost models of a global tuner; and
Dynamically tuning the architecture based on operations performed by at least one of a simulated annealing tuner or a random tuner of the global tuner.
상기 아키텍처가 상기 타겟 애플리케이션에 대한 계산을 실행하기 위해 제1 신경망을 구현할 때 상기 하나 이상의 하드웨어 블록 각각에 대한 각각의 성능 목표를 충족하도록 상기 아키텍처를 동적으로 튜닝하는 단계를 포함하는, 컴퓨터로 구현되는 방법.7. The method of claim 6, wherein the architecture represents one or more hardware blocks of an integrated circuit, and dynamically tuning the architecture comprises:
dynamically tuning the architecture to meet respective performance goals for each of the one or more hardware blocks when the architecture implements a first neural network to execute computations for the target application. method.
상기 하드웨어 ML 가속기의 구성은 상기 제1 신경망에 대한 맞춤형 소프트웨어 구성을 지정하며; 그리고
상기 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 단계는 상기 맞춤형 하드웨어 구성 및 상기 맞춤형 소프트웨어 구성에 기초하여 상기 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 단계를 포함하는, 컴퓨터로 구현되는 방법.According to clause 6,
The configuration of the hardware ML accelerator specifies a custom software configuration for the first neural network; and
The computer-implemented method of claim 1, wherein generating the application-specific hardware ML accelerator includes generating the application-specific hardware ML accelerator based on the custom hardware configuration and the custom software configuration.
상기 ML 코스트 모델은 하나 이상의 개별 분석 모델을 포함하는 아키텍처-웨어 코스트 모델이며; 그리고
상기 아키텍처-웨어 코스트 모델은 상기 아키텍처를 사용하여 프로세싱되는 데이터의 결정론적 데이터 흐름에 기초하여 상기 아키텍처의 성능을 추정하도록 구성되는, 컴퓨터로 구현되는 방법.According to clause 6,
The ML cost model is an architecture-ware cost model that includes one or more individual analysis models; and
wherein the architecture-wear cost model is configured to estimate performance of the architecture based on a deterministic data flow of data processed using the architecture.
베이스라인 프로세서 구성을 나타내는 아키텍처를 선택하는 동작;
ML 코스트 모델에 의해, 적어도 아키텍처가 복수의 계층을 포함하는 제1 신경망의 계산을 실행하는 방법을 모델링함으로써 아키텍처에 대한 성능 데이터를 생성하는 동작;
상기 성능 데이터에 기초하여, 상기 아키텍처가 상기 제1 신경망을 구현하고 그리고 타겟 애플리케이션에 대한 기계 학습 계산을 실행할 때 성능 목표를 충족하도록 상기 아키텍처를 동적으로 튜닝하는 동작;
상기 아키텍처를 동적으로 튜닝하는 것에 응답하여, 상기 제1 신경망의 복수의 계층 각각을 구현하기 위한 맞춤형 하드웨어 구성을 지정하는 ML 가속기의 구성을 생성하는 동작을 포함하는, 시스템.A system comprising a processing device and a non-transitory machine readable storage device storing instructions for generating an application-specific machine learning (ML) accelerator, wherein the instructions are executed by the processing device to perform a set of operations, The operation set is,
selecting an architecture representing a baseline processor configuration;
generating performance data for the architecture by modeling, by an ML cost model, at least how the architecture executes computations of a first neural network including a plurality of layers;
Based on the performance data, dynamically tuning the architecture to meet performance goals when the architecture implements the first neural network and executes machine learning calculations for a target application;
In response to dynamically tuning the architecture, generating a configuration of an ML accelerator specifying a custom hardware configuration for implementing each of the plurality of layers of the first neural network.
상기 맞춤형 하드웨어 구성에 기초하여 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 동작을 더 포함하며,
상기 애플리케이션-특정 하드웨어 ML 가속기는 신경망이 상기 타겟 애플리케이션에 대한 계산을 실행하는 데 사용될 때 신경망의 서로 다른 계층 각각을 구현하도록 최적화되는, 시스템.12. The method of claim 11, wherein the operation set is:
further comprising creating an application-specific hardware ML accelerator based on the customized hardware configuration,
The system of claim 1, wherein the application-specific hardware ML accelerator is optimized to implement each of the different layers of the neural network when the neural network is used to perform computations for the target application.
상기 애플리케이션-특정 하드웨어 ML 가속기가 상기 타겟 애플리케이션에 대한 계산을 실행할 때 복수의 이산적 목표 중 각각의 이산적 목표를 충족시키도록 구성된 애플리케이션-특정 하드웨어 ML 가속기를 생성하는 동작을 포함하는, 시스템.13. The method of claim 12, wherein the performance goal includes a plurality of discrete goals, and creating the application-specific ML accelerator comprises:
and generating an application-specific hardware ML accelerator configured to satisfy each discrete goal of a plurality of discrete goals when the application-specific hardware ML accelerator executes a computation for the target application.
상기 ML 코스트 모델에 의해, 상기 제1 신경망의 복수의 계층 중 각 계층을 실행하기 위한 아키텍처의 사용을 모델링하는 동작; 그리고
각 계층을 실행하기 위한 아키텍처의 사용을 모델링하는 것에 응답하여, 상기 ML 코스트 모델에 의해 상기 복수의 계층 각각에 대한 상기 아키텍처의 성능 파라미터를 생성하는 동작을 포함하는, 시스템.The method of claim 13, wherein the operation of generating the performance data includes:
modeling the use of an architecture for executing each layer of the plurality of layers of the first neural network by the ML cost model; and
In response to modeling the use of the architecture to execute each layer, generating performance parameters of the architecture for each of the plurality of layers by the ML cost model.
상기 성능 파라미터는 복수의 이산적 목표 중 각각의 이산적 목표에 대응하며; 그리고
상기 복수의 이산적 목표는 임계 프로세싱 지연시간, 임계 전력 소비, 임계 데이터 처리량 및 임계 프로세서 활용 중 적어도 하나를 포함하는, 시스템.According to clause 14,
the performance parameter corresponds to each discrete objective of the plurality of discrete objectives; and
The system of claim 1, wherein the plurality of discrete goals include at least one of threshold processing latency, threshold power consumption, threshold data throughput, and threshold processor utilization.
상기 애플리케이션-특정 하드웨어 ML 가속기가 하드웨어 ML 가속기의 하드웨어 컴퓨팅 유닛의 임계 백분율을 활용하게 하는 입력 텐서에 대한 계산의 매핑을 결정하는 동작; 그리고
상기 결정된 매핑에 기초하여 상기 아키텍처를 동적으로 튜닝하는 동작을 포함하는, 시스템.The method of claim 12, wherein dynamically tuning the architecture comprises:
determining a mapping of computations to input tensors that causes the application-specific hardware ML accelerator to utilize a threshold percentage of hardware compute units of the hardware ML accelerator; and
Dynamically tuning the architecture based on the determined mapping.
글로벌 튜너의 복수의 ML 코스트 모델 각각에 의해 수행되는 오퍼레이션에 기초하여 상기 아키텍처를 동적으로 튜닝하는 동작; 그리고
상기 글로벌 튜너의 시뮬레이트된 어닐링 튜너 또는 랜덤 튜너 중 적어도 하나에 의해 수행되는 오퍼레이션에 기초하여 상기 아키텍처를 동적으로 튜닝하는 동작을 포함하는, 시스템.The method of claim 16, wherein dynamically tuning the architecture comprises:
dynamically tuning the architecture based on operations performed by each of a plurality of ML cost models of a global tuner; and
Dynamically tuning the architecture based on operations performed by at least one of a simulated annealing tuner or a random tuner of the global tuner.
상기 아키텍처가 상기 타겟 애플리케이션에 대한 계산을 실행하기 위해 제1 신경망을 구현할 때 하나 이상의 하드웨어 블록 각각에 대한 각각의 성능 목표를 충족하도록 상기 아키텍처를 동적으로 튜닝하는 동작을 포함하는, 시스템.17. The method of claim 16, wherein the architecture represents one or more hardware blocks of an integrated circuit, and dynamically tuning the architecture comprises:
Dynamically tuning the architecture to meet respective performance goals for each of one or more hardware blocks when the architecture implements a first neural network to execute calculations for the target application.
상기 ML 코스트 모델은 하나 이상의 개별 분석 모델을 포함하는 아키텍처-웨어 코스트 모델이며; 그리고
상기 아키텍처-웨어 코스트 모델은 상기 아키텍처를 사용하여 프로세싱되는 데이터의 결정론적 데이터 흐름에 기초하여 상기 아키텍처의 성능을 추정하도록 구성되는, 시스템.According to clause 16,
The ML cost model is an architecture-ware cost model that includes one or more individual analysis models; and
The system of claim 1, wherein the architecture-wear cost model is configured to estimate performance of the architecture based on a deterministic data flow of data processed using the architecture.
베이스라인 프로세서 구성을 나타내는 아키텍처를 선택하는 동작;
ML 코스트 모델에 의해, 적어도 아키텍처가 복수의 계층을 포함하는 제1 신경망의 계산을 실행하는 방법을 모델링함으로써 아키텍처에 대한 성능 데이터를 생성하는 동작;
상기 성능 데이터에 기초하여, 상기 아키텍처가 상기 제1 신경망을 구현하고 그리고 타겟 애플리케이션에 대한 기계 학습 계산을 실행할 때 성능 목표를 충족하도록 상기 아키텍처를 동적으로 튜닝하는 동작;
상기 아키텍처를 동적으로 튜닝하는 것에 응답하여, 상기 제1 신경망의 복수의 계층 각각을 구현하기 위한 맞춤형 하드웨어 구성을 지정하는 ML 가속기의 구성을 생성하는 동작을 포함하는, 비일시적 기계 판독 가능 저장 장치.1. A non-transitory machine-readable storage device storing instructions for generating an application-specific machine learning (ML) accelerator, the instructions being executed by a processing device to perform a set of operations; The operation set is,
selecting an architecture representing a baseline processor configuration;
generating performance data for the architecture by modeling, by an ML cost model, at least how the architecture executes computations of a first neural network including a plurality of layers;
Based on the performance data, dynamically tuning the architecture to meet performance goals when the architecture implements the first neural network and executes machine learning calculations for a target application;
In response to dynamically tuning the architecture, generating a configuration of an ML accelerator specifying a custom hardware configuration for implementing each of the plurality of layers of the first neural network.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/030416 WO2022235251A1 (en) | 2021-05-03 | 2021-05-03 | Generating and globally tuning application-specific machine learning accelerators |
Publications (1)
Publication Number | Publication Date |
---|---|
KR20230170757A true KR20230170757A (en) | 2023-12-19 |
Family
ID=76270031
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
KR1020237039235A KR20230170757A (en) | 2021-05-03 | 2021-05-03 | Application-specific machine learning accelerator creation and global tuning |
Country Status (6)
Country | Link |
---|---|
EP (1) | EP4315173A1 (en) |
JP (1) | JP2024517833A (en) |
KR (1) | KR20230170757A (en) |
CN (1) | CN117355843A (en) |
TW (1) | TW202244792A (en) |
WO (1) | WO2022235251A1 (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116451757B (en) * | 2023-06-19 | 2023-09-08 | 山东浪潮科学研究院有限公司 | Heterogeneous acceleration method, heterogeneous acceleration device, heterogeneous acceleration equipment and heterogeneous acceleration medium for neural network model |
CN116501504B (en) * | 2023-06-27 | 2023-09-12 | 上海燧原科技有限公司 | Space-time mapping method and device for data stream, electronic equipment and storage medium |
CN116980423B (en) * | 2023-09-21 | 2024-02-09 | 浪潮电子信息产业股份有限公司 | Model scheduling method, device, computing system, equipment and readable storage medium |
-
2021
- 2021-05-03 KR KR1020237039235A patent/KR20230170757A/en unknown
- 2021-05-03 WO PCT/US2021/030416 patent/WO2022235251A1/en active Application Filing
- 2021-05-03 EP EP21729985.8A patent/EP4315173A1/en active Pending
- 2021-05-03 CN CN202180097806.5A patent/CN117355843A/en active Pending
- 2021-05-03 JP JP2023568049A patent/JP2024517833A/en active Pending
-
2022
- 2022-02-11 TW TW111104992A patent/TW202244792A/en unknown
Also Published As
Publication number | Publication date |
---|---|
JP2024517833A (en) | 2024-04-23 |
TW202244792A (en) | 2022-11-16 |
CN117355843A (en) | 2024-01-05 |
WO2022235251A1 (en) | 2022-11-10 |
EP4315173A1 (en) | 2024-02-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Akhlaghi et al. | Snapea: Predictive early activation for reducing computation in deep convolutional neural networks | |
Mittal | A survey of techniques for approximate computing | |
US11256698B2 (en) | Automated provisioning for database performance | |
Dave et al. | Hardware acceleration of sparse and irregular tensor computations of ml models: A survey and insights | |
Zhang et al. | BoostGCN: A framework for optimizing GCN inference on FPGA | |
KR20230170757A (en) | Application-specific machine learning accelerator creation and global tuning | |
JP2023522567A (en) | Generation of integrated circuit layouts using neural networks | |
Lin et al. | Naas: Neural accelerator architecture search | |
Kim et al. | Full stack optimization of transformer inference: a survey | |
CN110520834A (en) | Alternative circulation limitation | |
Hadjis et al. | Tensorflow to cloud FPGAs: Tradeoffs for accelerating deep neural networks | |
WO2022087415A1 (en) | Runtime task scheduling using imitation learning for heterogeneous many-core systems | |
Zhang et al. | Dna: Differentiable network-accelerator co-search | |
Fasfous et al. | Anaconga: Analytical hw-cnn co-design using nested genetic algorithms | |
Müller et al. | Accelerating noisy VQE optimization with Gaussian processes | |
Russo et al. | MEDEA: A multi-objective evolutionary approach to DNN hardware mapping | |
Gui et al. | Developing subdomain allocation algorithms based on spatial and communicational constraints to accelerate dust storm simulation | |
Boopathy et al. | A multivariate interpolation and regression enhanced kriging surrogate model | |
Sohrabizadeh et al. | StreamGCN: Accelerating graph convolutional networks with streaming processing | |
US11954580B2 (en) | Spatial tiling of compute arrays with shared control | |
Indirli et al. | A tile-based fused-layer CNN accelerator for FPGAs | |
Shi et al. | NASA: Neural architecture search and acceleration for hardware inspired hybrid networks | |
US20240095309A1 (en) | System and method for holistically optimizing dnn models for hardware accelerators | |
Chang et al. | Designing a Framework for Solving Multiobjective Simulation Optimization Problems | |
US11972349B1 (en) | Flexible compute array utilization in a tensor processor |