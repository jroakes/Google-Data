CN116018639A - Method and system for text-to-speech synthesis of streaming text - Google Patents
Method and system for text-to-speech synthesis of streaming text Download PDFInfo
- Publication number
- CN116018639A CN116018639A CN202080102489.7A CN202080102489A CN116018639A CN 116018639 A CN116018639 A CN 116018639A CN 202080102489 A CN202080102489 A CN 202080102489A CN 116018639 A CN116018639 A CN 116018639A
- Authority
- CN
- China
- Prior art keywords
- substring
- text
- punctuation
- point
- trigger point
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/08—Text analysis or generation of parameters for speech synthesis out of text, e.g. grapheme to phoneme translation, prosody generation or stress or intonation determination
Abstract
A method and system for speech synthesis of streaming text is disclosed. In a text-to-speech (TTS) system, a real-time streaming text string having a start point and an end point may be received, and a first substring including a first portion of the text string received from the start point to a first trigger point may be accumulated. The initial point is not earlier than the starting point and is before the first trigger point, and the first trigger point does not exceed the end point. The punctuation model of the TTS system may be applied to the first substring to generate a preprocessed first substring that includes the first substring with the grammar punctuation determined by the punctuation model added. The TTS synthesis process may be applied to at least the preprocessed first substring to generate a first synthesized speech and to generate an audio playback of the first synthesized speech.
Description
Technical Field
Unless otherwise indicated herein, the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.
The goal of Automatic Speech Recognition (ASR) techniques is to map a particular utterance or speech sample to an accurate textual or other symbolic representation of the utterance. For example, an ASR performed on the utterance "My dog has fleas" would ideally map to the text string "My dog has fleas" rather than the meaningless text string "My dog has ice" or the reasonable but inaccurate text string "My marshland tree".
The goal of speech synthesis techniques is to convert written language into speech that can be output in an audio format, for example directly for audio output or stored as an audio file suitable for audio output. Such speech synthesis may be performed by a text-to-speech (TTS) system. The written language may take the form of a textual or symbolic language representation. The speech may be generated as waveforms by a speech synthesizer that produces artificial human speech. Natural sounding human speech may also be the target of a speech synthesis system.
Background
Various techniques including computers, web servers, telephones, and Personal Digital Assistants (PDAs) may be used to implement an ASR system and/or a speech synthesis system, or one or more components of such a system. The communication network may in turn provide communication paths and links between some or all of such devices, supporting speech synthesis system capabilities and services that may utilize ASR and/or speech synthesis system capabilities.
Disclosure of Invention
In one aspect, example embodiments presented herein provide a method comprising: receiving, at a text-to-speech (TTS) system, a real-time streaming text string having a start point and an end point; accumulating, at the TTS system, a first substring comprising a first portion of the text string received from an initial point to a first trigger point, wherein the initial point is not earlier than the initial point and before the first trigger point, and the first trigger point does not exceed the end point; at the TTS system, applying a punctuation model of the TTS system to the first substring to generate a preprocessed first substring, the preprocessed first substring including the first substring with the grammar punctuation as determined by the punctuation model added; applying, at the TTS system, TTS synthesis processing to at least the pre-processed first substring to generate a first synthesized speech; and generating an audio playback of the first synthesized speech.
In another aspect, example embodiments presented herein provide a system comprising a text-to-speech (TTS) system implemented on a device, comprising: one or more processors; a memory; and machine-readable instructions stored in the memory, which when executed by the one or more processors, cause the TTS system to perform operations comprising: receiving a real-time streaming text string having a start point and an end point; accumulating a first substring comprising a first portion of the text string received from an initial point to a first trigger point, wherein the initial point is not earlier than the start point and before the first trigger point, and the first trigger point does not exceed the end point; applying a punctuation model of the TTS system to the first substring to generate a preprocessed first substring comprising the first substring with the addition of grammar punctuation as determined by the punctuation model; applying a TTS synthesis process to at least the preprocessed first substring to generate a first synthesized speech; and generating an audio playback of the first synthesized speech.
In yet another aspect, example embodiments presented herein provide an article of manufacture comprising a computer-readable storage medium having stored thereon program instructions that, when executed by one or more processors of a system comprising a text-to-speech (TTS) system, cause the system to perform operations comprising: receiving a real-time streaming text string having a start point and an end point; accumulating a first substring comprising a first portion of the text string received from an initial point to a first trigger point, wherein the initial point is not earlier than the start point and before the first trigger point, and the first trigger point does not exceed the end point; applying a punctuation model of the TTS system to the first substring to generate a preprocessed first substring comprising the first substring with the addition of grammar punctuation as determined by the punctuation model; applying a TTS synthesis process to at least the preprocessed first substring to generate a first synthesized speech; and generating an audio playback of the first synthesized speech.
These and other aspects, advantages, and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with appropriate reference to the accompanying drawings. Moreover, it should be understood that this summary and other descriptions and drawings provided herein are intended to be illustrative of embodiments by way of example only, and that many variations are possible as such. For example, structural elements and process steps may be rearranged, combined, distributed, eliminated, or otherwise altered while remaining within the scope of the claimed embodiments.
Drawings
FIG. 1 depicts a simplified block diagram of an example text-to-speech system according to an example embodiment.
FIG. 2 is a block diagram of an example network and computing architecture according to an example embodiment.
Fig. 3A is a block diagram of a server device according to an example embodiment.
Fig. 3B depicts a cloud-based server system according to an example embodiment.
Fig. 4 depicts a block diagram of a client device in accordance with an illustrative embodiment.
FIG. 5 depicts example operations of text-to-speech synthesis according to an example embodiment.
FIG. 6 illustrates a simplified block diagram of an example text-to-speech system including punctuation models, according to an example embodiment.
FIG. 7A depicts an example timing diagram of string accumulation during text-to-speech synthesis using punctuation models, according to an example embodiment.
FIG. 7B depicts a first example process flow for text-to-speech synthesis using punctuation models in accordance with an example embodiment.
FIG. 7C depicts a second example process flow for text-to-speech synthesis using punctuation models in accordance with an illustrative embodiment.
FIG. 7D depicts a third example process flow for text-to-speech synthesis using punctuation models in accordance with an example embodiment.
FIG. 8 depicts example operations for text-to-speech synthesis including punctuation models in accordance with an example embodiment.
Fig. 9 is a flowchart illustrating an example method according to an example embodiment.
Detailed Description
1. Summary of the invention
The speech synthesis system may be a processor-based system configured to convert written language into artificially generated speech or spoken speech. The written language may be written text, such as one or more written sentences or text strings. The written language may also take the form of other symbolic representations, such as a speech synthesis markup language, which may include information indicative of speaker emotion, speaker gender, speaker identity, and speaking style. The source of the written text may be entered from a keyboard or keypad of a computing device, such as a portable computing device (e.g., PDA, smart phone, etc.), or may be from a file stored on one form or another of computer-readable storage medium, or from a remote resource (such as a web page) accessed via a network. The artificially generated speech may be generated as waveforms from a signal generating device or module (e.g., a speech synthesizer device) and output and/or formatted by an audio playback device and recorded as an audio file on a tangible recording medium. The synthesized speech is played via a network connection to an audio device, such as a landline telephone or smart phone. Such a system may also be referred to as a "text-to-speech" (TTS) system, although written form is not necessarily limited to text only.
The speech synthesis system may operate by receiving input text (or other form of written language) and translating that written text into a "phonetic transcription" corresponding to a symbolic representation of how a spoken presentation of that text sounds or should sound. The phonetic transcription may then be mapped to phonetic features that parameterize the acoustic rendering of the phonetic transcription, and which then serve as input data for a signal generation module device or element that may generate an audio waveform suitable for playback by an audio output device. For example, playing may sound like human speech that is a word (or sound) that is descriptive of the input text string. In the case of speech synthesis, the more natural the sound of the synthesized speech (e.g., for the human ear), the better the speech quality ranking of the system is in general. In some cases, the more natural sound may also reduce computing resources, as subsequent communication with the user's clarified output meaning may be reduced. The audio waveform may also be generated as an audio file that may be stored or recorded on a storage medium suitable for subsequent playback. In some embodiments, speech may be synthesized directly from text without having to generate a phonetic transcription.
In operation, the TTS system may be used to convey information from an apparatus (e.g., a processor-based device or system) to a user, such as messages, prompts, answers to questions, instructions, news, email, and speech-to-speech translation. The speech signal itself may carry various forms or types of information, including linguistic content, emotional states (e.g., emotion and/or mood), physical states (e.g., physical sound features), and speaker identity, among others.
In an example embodiment, speech synthesis may use a parametric representation of speech with textual speech and symbolic descriptions of the linguistic content. TTS systems can be trained using data consisting essentially of a large number of speech samples and corresponding text strings (or other symbol renderings). For practical reasons, speech samples are typically recorded, although in principle they are not required. By construction, the corresponding text string is in a written storage format, or is generally adapted to a written storage format. Thus, the recorded speech samples and their corresponding text strings may constitute training data for the TTS system.
One example of TTS is based on a Hidden Markov Model (HMM). In this approach, HMMs are used to model the following statistical probabilities: the statistical probability correlates the phonetic transcription of the input text string with a parametric representation of the corresponding speech to be synthesized. As another example, the TTS may generate a parametric representation of speech based on some form of machine learning to synthesize speech. For example, an Artificial Neural Network (ANN) is trained to associate known phonetic transcriptions with known parametric representations of speech sounds to generate speech parameters using the ANN. Both HMM-based speech synthesis and ANN-based speech synthesis may facilitate the use of one form or another of statistical adaptation to alter or adjust the characteristics of the synthesized speech. Other forms of TTS systems are also possible.
In normal operation, examples of text for TTS training data include grammatical punctuation such as commas, periods, question marks, and exclamation marks. In this way, the TTS system may be trained to generate "predicted" speech at runtime that may convey (e.g., at tone and/or volume) meaning, intent, or content, e.g., not just written words of text at runtime. However, in some applications of TTS, the runtime text may contain little or no grammar punctuation. A non-limiting example is a text application on a smart phone, where typical user inputs may lack grammar punctuation, either partially or completely. TTS processing of text in this form, which may be referred to as "streaming text" or "real-time" text, may present challenges for conventionally trained TTS systems, and in such cases the resulting synthesized speech may sound monotonous or unnatural, or worse. Thus, it is desirable to be able to synthesize naturally sounding speech from text that is partially or entirely devoid of grammar punctuation. The inventors have found how this can be done.
According to an example embodiment, a "punctuation model" may be added to or integrated into a TTS system. Punctuation models may be applied to runtime input text to add grammatical punctuation to the text prior to the synthesis process. The resulting synthesized speech may then sound more natural than the synthesized punctuation-free input text. In an example embodiment, the punctuation model may be based on machine learning and/or other artificial intelligence techniques and is trained to generate output text including grammatical punctuation from input text that contains little or no punctuation. In addition to improving the quality of the synthesized speech, punctuation may be incrementally added in real-time as the streaming text is received and used to subdivide the arriving streaming text into successive sub-strings that may be incrementally processed into synthesized speech. This piecewise, incremental processing enables TTS synthesis of one substring while receiving subsequent substrings, thereby reducing the time taken to generate synthesized speech from the first to the last streaming text character.
2. Example text-to-Speech System
TTS synthesis systems (or more generally, speech synthesis systems) may operate by: the method includes receiving input text, processing the text into phonetic and linguistic representations of the text strings, generating a sequence of phonetic features corresponding to the symbolic representations, and providing the phonetic features as input to a speech synthesizer to produce a verbal presentation of the input text. The phonetic and symbolic representations of the linguistic content of the text may take the form of a sequence of tags, each tag identifying a low-level phonetic unit (such as a phoneme) and further identifying or encoding the low-level language and/or syntactic context, temporal parameters, and other information specifying how the sound of the symbolic representation is rendered into meaningful speech for a given language. Other speech features may include pitch, frequency, pace, and intonation (e.g., statement tones, query tones, etc.). At least some of these features are sometimes referred to as "prosody".
According to an example embodiment, the phonetic units of the phonetic transcription may be phonemes. A phoneme may be considered to be the smallest acoustic segment of speech in a given language that contains meaningful comparisons with other speech segments in the given language. Thus, a word typically includes one or more phonemes. For simplicity, phonemes may be considered to be the pronunciation of letters, although this is not a perfect analogy, as some phonemes may represent multiple letters. In written form, the phonemes are typically represented as some type of separation Fu Na representing one or more letters or symbols representing the text of the phonemes. For example, the phonetic spelling of the American English pronunciation of the word "cat" is/k// ae// t/, and consists of phonemes/k/,/ae/and/t/s. Another example is that the phonetic spelling of the word "dog" is/d// aw// g/, and is made up of phonemes/d/,/aw/and/g/. Different phoneme letters exist and other phoneme representations are possible. The common phonetic letters of american english contain about 40 different phones. Other languages may be described by different phones containing different phones.
The phonetic properties of the phonemes in the utterance may depend on or be influenced by the context that was spoken (or intended to be spoken). For example, a "triphone" is a triplet of phonemes in which the spoken rendering of a given phoneme is formed of a temporally preceding phoneme (referred to as a "left context") and a temporally following phoneme (referred to as a "right context"). Thus, the phoneme order of the english triphones corresponds to the direction of english reading. Other phoneme contexts, such as five phonemes, are also contemplated.
For example, in addition to the context of the phoneme hierarchy, phonetic properties may also depend on higher-level contexts, such as words, phrases, and sentences. Higher level contexts are generally associated with language usage and can be characterized by a language model. In written text, the use of language may be conveyed, at least in part, by grammatical punctuation. Specifically low, grammar punctuation can provide a high level context related to phonetic rhythms, intonation, and other pronunciation nuances.
The speech features represent acoustic properties of the speech as parameters and, in the context of speech synthesis, may be used to drive the generation of a synthesized waveform corresponding to the output speech signal. In general, the features of speech synthesis take into account three main components of the speech signal, namely the spectral envelope of the vocal tract-like effect, the excitation of the simulated glottal source, and the prosody describing the pitch contour ("melody") and the beat (rhythm) as described above. In practice, features may be represented as multi-dimensional feature vectors corresponding to one or more time frames. One of the basic operations of TTS synthesis systems is to map a phonetic transcription (e.g., a tag sequence) to an appropriate feature vector sequence.
For example, the features may include mel-filter cepstral coefficient (MFCC) coefficients. The MFCC may represent a short-term power spectrum of a portion of an input utterance and may be based on, for example, a linear cosine transform of a logarithmic power spectrum on a nonlinear mel frequency scale. (the Mel scale may be a pitch scale subjectively perceived by a listener as being approximately equal in distance from each other, even if the actual frequencies of these pitches are not equal in distance from each other.)
In some embodiments, the feature vector may include MFCC, a first order cepstral coefficient derivative, and a second order cepstral coefficient derivative. For example, the feature vector may contain 13 coefficients, 13 first derivatives ("δ") and 13 second derivatives ("δ") and thus be 39 in length. However, in other possible embodiments, the feature vectors may use different feature combinations. As another example, the feature vector may include Perceptual Linear Prediction (PLP) coefficients, relative spectrum (RASTA) coefficients, filter bank logarithmic energy coefficients, or some combination thereof. Each feature vector may be considered to include a quantized representation of the acoustic content of a corresponding time frame of the utterance (or more generally, the audio input signal).
FIG. 1 depicts a simplified block diagram of an example text-to-speech (TTS) synthesis system 100 according to an example embodiment. In addition to the functional components, FIG. 1 also shows selected example inputs, outputs, and intermediates of example operations. The functional components of TTS synthesis system 100 include: a text analysis module 102 for converting the input text 101 into a phonetic transcription 103; a TTS subsystem 104 for generating data representing acoustic properties 105 of speech to be synthesized from the phonetic transcription 103; and a speech generator 106 for generating synthesized speech 107 from the acoustic properties 105. These functional components may be implemented as machine language instructions in a centralized and/or distributed manner on one or more computing platforms or systems, such as those described above. The machine language instructions may be stored on one or another form of tangible, non-transitory computer-readable medium (or other article of manufacture), such as a magnetic disk or optical disk; and makes it available to the processing elements of the system as part of, for example, a manufacturing process, a configuration process, and/or performing a start-up process.
It should be noted that the discussion in this section and the accompanying figures are given for illustrative and exemplary purposes. For example, TTS subsystem 104 may be implemented using an HMM model for generating speech features at runtime based on a learning (training) association between known tags and known parameterized speech. As another example, TTS subsystem 104 may be implemented using a machine learning model, such as an Artificial Neural Network (ANN), for generating speech features at runtime from associations between known tags and known parameterized speech, where the associations are learned through training with known associations. In yet another example, the TTS subsystem may employ a hybrid HMM-ANN model.
According to an example embodiment, the text analysis module 102 may receive input text 101 (or other form of text-based input) and generate as output a phonetic transcription 103. For example, the input text 101 may be a text message, email, chat input, book paragraph, article, or other text-based communication. As described above, the phonetic transcription may correspond to a tag sequence identifying a phonetic unit (such as a phoneme), possibly with context information.
As shown, TTS subsystem 104 may employ HMM-based or ANN-based speech synthesis to generate feature vectors corresponding to phonetic transcription 103. The feature vector may include an amount representing the acoustic properties 105 of the speech to be generated. For example, acoustic properties may include pitch, fundamental frequency, pace (e.g., speech rate), and prosody. Other acoustic properties are also possible.
The acoustic properties may be input to a speech generator 106, which speech generator 106 generates as output a synthesized speech 107. The synthesized speech 107 may be generated as an actual audio output, such as from an audio device (e.g., headphones, earpieces, speakers, etc.) having one or more speakers; and/or digital data that can be recorded and played from a data file (e.g., a waveform file, etc.).
Although not necessarily explicitly shown in fig. 1, TTS system 100 may also use a language model to predict a high-level context for interpretation of phonetic transcription 103 and generation of acoustic features 105 that may be rendered into natural-sounding speech by speech generator 106. The accuracy of the predictions of the language model may depend at least in part on structural features in the input text 101, including grammar punctuation. As described above, the lack of grammar punctuation in the written text can dilute or eliminate these aspects of the high-level context, resulting in poor or inadequate determination of phonetic properties. Thus, TTS systems typically trained using punctuated text and corresponding speech samples may not be able to generate naturally sounding speech from text input lacking grammar punctuation. A non-limiting example of input text that is lacking or insufficient in grammar punctuation is streaming text, such as that generated by a text application.
The example embodiments described herein employ conventional TTS processing to enable natural-sounding speech to be generated from text input that lacks or lacks grammatical punctuation. In particular, example embodiments introduce punctuation models that can create grammatical punctuation renderings of input text, which can then be processed by a TTS subsystem to generate naturally pronounced speech. Before describing example embodiments of a TTS system suitable for adapting to punctuation-deficient text, a discussion of example communication systems and device architectures in which example embodiments of TTS synthesis with punctuation modeling may be implemented is presented.
3. Example communication System and device architecture
According to the method of example embodiments, such as one of the embodiments described above, devices may be implemented using so-called "thin client" and "cloud-based" server devices, as well as other types of clients and server devices. Under various aspects of this example, a client device, such as a mobile phone and a tablet computer, may transfer some of the processing and storage responsibilities to a remote server device. At least at some time, these client services are able to communicate with the server device over a network such as the internet. As a result, applications running on the client device may also have permanent, server-based components. It should be noted, however, that at least some of the methods, processes, and techniques disclosed herein may be capable of operating entirely on a client device or a server device.
This section describes a general system and device architecture for such client devices and server devices. However, the methods, apparatus and systems presented in the subsequent sections may also operate under different examples. Thus, the embodiments of this section are merely examples of how such methods, apparatuses, and systems may be performed.
a. Example communication System
Fig. 2 is a simplified block diagram of a communication system 200 in which various embodiments described herein may be employed. Communication system 200 includes client devices 202, 204, and 206, which represent desktop Personal Computers (PCs), tablet computers, and mobile phones, respectively. The client device may also include, for example, a wearable computing device, such as a head-mounted display and/or an augmented reality display. Each of these client devices may be capable of communicating with other devices (including with each other) via network 208 using wired connections (represented by solid lines) and/or wireless connections (represented by dashed lines).
The network 208 may be, for example, the Internet, or some other form of public or private Internet Protocol (IP) network. Thus, client devices 202, 204, and 206 may communicate using packet-switched technology. Nonetheless, the network 208 may also incorporate at least some circuit-switched technology, and the client devices 202, 204, and 206 may alternatively or in addition to packet-switched communicate via circuit-switched.
Although only three client devices, one server device, and one server data store are shown in fig. 2, communication system 200 may include any number of each of these components. For example, communication system 200 may include millions of client devices, thousands of server devices, and/or thousands of server data stores. Furthermore, the client device may take a form different from that in fig. 2.
b. Example server device and server System
Fig. 3A is a block diagram of a server device according to an example embodiment. In particular, the server device 300 shown in fig. 3A may be configured to perform one or more functions of the server device 210 and/or the server data store 212. The server device 300 may include a user interface 302, a communication interface 304, a processor 306, and a data store 308, all of which may be linked together via a system bus, network, or other connection mechanism 314.
The user interface 302 may include a user input device such as a keyboard, keypad, touch screen, computer mouse, trackball, joystick, and/or other similar devices now known or later developed. The user interface 302 may also include a user display device such as one or more Cathode Ray Tubes (CRTs), liquid Crystal Displays (LCDs), light Emitting Diodes (LEDs), displays using Digital Light Processing (DLP) technology, printers, light bulbs, and/or other similar devices now known or later developed. Additionally, the user interface 302 may be configured to generate audible output via speakers, speaker jacks, audio output ports, audio output devices, headphones, and/or other similar devices now known or later developed. In some embodiments, user interface 302 may comprise software, circuitry, or another form of logic capable of transmitting data to and/or receiving data from an external user input/output device.
Communication interface 304 may include one or more wireless interfaces and/or wired interfaces that may be configured to communicate via a network, such as network 208 shown in fig. 2. The wireless interface, if present, may include one or more wireless transceivers, such as Bluetooth
In some embodiments, communication interface 304 may be configured to provide reliable, secure, and/or authenticated communication. For each communication described herein, information may be provided for ensuring reliable communication (e.g., guaranteed messaging), possibly as part of a message header and/or trailer (e.g., packet/message ordering information, encapsulation header and/or trailer, size/time information, and transmission verification information such as Cyclic Redundancy Check (CRC) and/or parity values). One or more cryptographic protocols and/or algorithms, such as, but not limited to, the Data Encryption Standard (DES), advanced Encryption Standard (AES), rivest, shamir, and Adleman (RSA) algorithms, diffie-Hellman algorithms, and/or Digital Signature Algorithms (DSA), may be used to secure (e.g., encode or encrypt) and/or decrypt/decode the communication. Other encryption protocols and/or algorithms may be used instead of or in addition to those listed herein to secure (and then decrypt/decode) the communication.
Processor 306 may include one or more general-purpose processors (e.g., microprocessors) and/or one or more special-purpose processors (e.g., digital Signal Processors (DSPs), graphics Processing Units (GPUs), floating point processing units (FPUs), network processors, or Application Specific Integrated Circuits (ASICs)). Processor 306 may be configured to execute computer-readable program instructions 310 and/or other instructions contained in data store 308 to perform the various functions described herein.
Data store 308 may include one or more non-transitory computer-readable storage media that may be read or accessed by processor 306. One or more computer-readable storage media may include volatile and/or nonvolatile storage components, such as optical, magnetic, organic, or other memory or disk storage, which may be integrated in whole or in part with the processor 306. In some embodiments, data store 308 may be implemented using a single physical device (e.g., one optical, magnetic, organic, or other memory or disk storage unit), while in other embodiments data store 308 may be implemented using two or more physical devices.
The data store 308 may also include program data 312, which program data 312 may be used by the processor 306 to perform the functions described herein. In some embodiments, the data store 308 may include or have access to additional data store components or devices (e.g., a cluster data store described below).
Referring briefly again to fig. 2, server device 210 and server data storage device 212 may store applications and application data at one or more sites accessible via network 208. These sites may be data centers that contain a large number of servers and storage devices. The exact physical location, connectivity, and configuration of server device 210 and server data storage device 212 may be unknown and/or unimportant to the client device. Accordingly, server device 210 and server data storage device 212 may be referred to as "cloud-based" devices, which are housed in various remote locations. One possible advantage of such "cloud-based" computing is load transfer processing and data storage from client devices, simplifying the design and requirements of these client devices.
In some embodiments, server device 210 and server data storage device 212 may be a single computing device residing in a single data center. In other embodiments, server device 210 and server data storage device 212 may include multiple computing devices in a data center, or even multiple computing devices in multiple data centers, where the data centers are located in different geographic locations. For example, fig. 2 depicts each of a server device 210 and a server data storage device 212, which may be located in different physical locations.
Fig. 3B depicts an example of a cloud-based server cluster. In fig. 3B, the functionality of server device 210 and server data storage device 212 may be distributed among three server clusters 320A, 320B, and 320C. Server cluster 320A may include one or more server devices 300A, a cluster data store 322A, and a cluster router 324A connected by a local cluster network 326A. Similarly, server cluster 320B may include one or more server devices 300B, a cluster data store 322B, and a cluster router 324B connected by a local cluster network 326B. Likewise, server cluster 320C may include one or more server devices 300C, a cluster data store 322C, and a cluster router 324C connected by a local cluster network 326C. Server clusters 320A, 320B, and 320C may communicate with network 308 via communication links 328A, 328B, and 328C, respectively.
In some embodiments, each of server clusters 320A, 320B, and 320C may have an equal number of server devices, an equal number of cluster data stores, and an equal number of cluster routers. However, in other embodiments, some or all of server clusters 320A, 320B, and 320C may have a different number of server devices, a different number of cluster data stores, and/or a different number of cluster routers. The number of server devices, cluster data stores, and cluster routers in each server cluster may depend on the computing tasks and/or applications assigned to each server cluster.
For example, in server cluster 320A, server device 300A may be configured to perform various computing tasks such as servers of server device 210. In one embodiment, these computing tasks may be distributed among one or more of server devices 300A. Server devices 300B and 300C in server clusters 320B and 320C may be configured the same or similar to server device 300A in server cluster 320A. On the other hand, in some embodiments, server devices 300A, 300B, and 300C may each be configured to perform different functions. For example, server device 300A may be configured to perform one or more functions of server device 210, while server device 300B and server device 300C may be configured to perform the functions of one or more other server devices. Similarly, the functionality of the server data storage device 212 may be dedicated to a single server cluster or distributed across multiple server clusters.
Similar to the manner in which the functionality of server device 210 and server data storage device 212 may be distributed across server clusters 320A, 320B, and 320C, the various active portions and/or backup/redundant portions of these components may be distributed across cluster data stores 322A, 322B, and 322C. For example, some of the clustered data stores 322A, 322B, and 322C may be configured to store backup versions of the data stored in other clustered data stores 322A, 322B, and 322C.
Additionally, the configuration of cluster routers 324A, 324B, and 324C may be based, at least in part, on data communication requirements of the server devices and the cluster storage arrays, data communication capabilities of network devices in cluster routers 324A, 324B, and 324C, delays and throughput of local cluster networks 326A, 326B, 326C, delays, throughput, and cost of wide area network connections 328A, 328B, and 328C, and/or other factors that may contribute to cost, speed, fault tolerance, resilience, efficiency, and/or other design goals of the system architecture.
c. Example client device
Fig. 4 is a simplified block diagram illustrating some of the components of an example client device 400. The client device 400 may be configured to perform one or more functions of the client devices 202, 204, 206. By way of example and not limitation, client device 400 may be or include a "plain old telephone system" (POTS) telephone, a cellular mobile telephone, a still camera, a video camera, a facsimile machine, a answering machine, a computer (such as a desktop, notebook, or tablet computer), a personal digital assistant, a wearable computing device, a home automation component, a Digital Video Recorder (DVR), a digital television, a remote control, or some other type of device equipped with one or more wireless or wired communication interfaces. The client device 400 may also take the form of interactive virtual and/or augmented reality glasses, such as a head-mounted display device, sometimes referred to as a "heads-up" display device. Although not necessarily shown in fig. 4, the head-mounted device may include a display assembly for displaying images on the display assembly of the head-mounted device. The head-mounted device may also include one or more eye-facing cameras or other devices configured to track eye movements of a wearer of the head-mounted device. The eye tracking camera may be used to determine in real time the eye gaze direction and the movement of the wearer's eyes. The eye gaze direction may be provided as input to various operations, functions, and/or applications, such as tracking the wearer's gaze direction and movement on text displayed in a display device.
As shown in fig. 4, a client device 400 may include a communication interface 402, a user interface 404, a processor 406, and a data store 408, all of which may be communicatively linked together by a system bus, network, or other connection mechanism 410.
The communication interface 402 is used to allow the client device 400 to communicate with other devices, access networks, and/or transport networks using analog or digital modulation. Thus, the communication interface 402 may facilitate circuit-switched and/or packet-switched communications, such as POTS communications and/or IP or other packetized communications. For example, the communication interface 402 may comprise a chipset and an antenna arranged for wireless communication with a radio access network or access point. In addition, the communication interface 402 may take the form of a wired interface, such as an Ethernet, token ring, or USB port. The communication interface 402 may also take the form of a wireless interface, such as Wifi, bluetoothcommunication interface 402. In addition, communication interface 402 may include multiple physical communication interfaces (e.g., wifi interface, bluetooth +. >
The user interface 404 may be used to allow the client device 400 to interact with a human or non-human user, such as receiving input from a user and providing output to the user. Accordingly, the user interface 404 may include input components such as a keypad, keyboard, touch-sensitive or presence-sensitive panel, computer mouse, trackball, joystick, microphone, still camera, and/or video camera. The user interface 404 may also include one or more output components such as a display screen (e.g., which may be combined with a touch sensitive panel), CRT, LCD, LED, a display using DLP technology, a printer, a light bulb, and/or other similar devices now known or later developed. The user interface 404 may also be configured to generate audible output via speakers, speaker jacks, audio output ports, audio output devices, headphones, and/or other similar devices now known or later developed. In some embodiments, the user interface 404 may comprise software, circuitry, or another form of logic capable of transmitting data to and/or receiving data from an external user input/output device. Additionally or alternatively, the client device 400 may support remote access from another device via the communication interface 402 or via another physical interface (not shown). The user interface 404 may be configured to receive user input, the position and movement of which may be indicated by an indicator or cursor described herein. The user interface 404 may additionally or alternatively be configured as a display device to render or display the text snippets.
The processor 406 may include one or more general-purpose processors (e.g., a microprocessor) and/or one or more special purpose processors (e.g., DSP, GPU, FPU, a network processor, or an ASIC). The data store 408 can include one or more volatile and/or nonvolatile storage components, such as magnetic, optical, flash, or organic storage, and can be integrated in whole or in part with the processor 406. The data store 408 can include removable and/or non-removable components.
In general, the processor 406 may be capable of executing program instructions 418 (e.g., compiled or non-compiled program logic and/or machine code) stored in the data store 408 to perform the various functions described herein. The data store 408 may include a non-transitory computer readable medium having stored thereon program instructions that, when executed by the client device 400, cause the client device 400 to perform any of the methods, processes, or functions disclosed in the present specification and/or figures. Execution of the program instructions 418 by the processor 406 may cause the processor 406 to use the data 412.
By way of example, program instructions 418 may include an operating system 422 (e.g., an operating system kernel, device drivers, and/or other modules) and one or more application programs 420 (e.g., address book, email, web browsing, social networking, and/or gaming applications) installed on client device 400. Similarly, data 412 may include operating system data 416 and application data 414. Operating system data 416 may be primarily accessible to operating system 422, and application data 414 may be primarily accessible to one or more application programs 420. The application data 414 may be arranged in a file system that is visible or hidden to the user of the client device 400.
In some jargon, the application 420 may be simply referred to as an "app (application)". Additionally, the application 420 may be downloaded to the client device 400 through one or more online application stores or application markets. However, applications may also be installed on the client device 400 in other ways, such as via a web browser or through a physical interface (e.g., a USB port) on the client device 400.
4. Example System and operation
An example of a usage scenario for a TTS in which a grammar punctuation is absent or absent in the input text is shown in fig. 5, in fig. 5 a smartphone 502 is used to input text via a text application and convert the text to speech, which can then be transmitted to a telephone 506 over a communication network 504 and played by an audio component 506-1. Both smartphone 502 and phone 506 are examples of communication devices that are communicatively connected by way of network 504, and thus are considered remote from each other. Other devices may also be used. The "lightning" line in the figure represents the communication connection of each device to the network.
In the illustration, the user may type in an input text that includes, obviously and by way of example, the string 501 "hi you want to and me see lunch at the face" i can subscribe to me know (hi do you want to meet for lunch i can make a reservation at pizza palace let me know) in pizza palace (pizza palace) ". The sending user may click a virtual "send" button on the smartphone 502 (as represented by the pointing finger in fig. 5) to invoke the TTS system 502-1 of the smartphone 502, which TTS system 502-1 generates a synthesized voice, represented in the figure by waveform 503, which is then transmitted to the cellular phone 506 as indicated. The curved dashed arrow represents the transmission to the smart phone 506. In some embodiments, text may be converted to speech at cellular phone 506 using TTS processing residing on the cellular phone instead of smart phone 502. Alternatively, the TTS process may be remotely hosted on a third-party computing system (not shown) configured to receive text data from the smartphone 502 over the communication network 504, convert it to speech using the TTS process, and transmit the speech to the cellular phone 506 over the communication network or another communication network 504.
The lack of grammar punctuation in the input text stream may cause TTS system 501-2 to synthesize flat, unnatural sounding output speech 505. This is visually represented in fig. 5 by: each word of the input text 501 is placed on a separate line of written words intended to represent the word spoken in the output speech 505. Thus, as rendered in synthesized speech, each word of output 505 sounds as if spoken one at a time and is isolated from each other. The inventors have found that by introducing a punctuation model, it is possible to compensate for the lack of grammatical punctuation in the input text and to generate naturally sounding speech.
a. Example text-to-speech System with punctuation model
FIG. 6 illustrates a simplified block diagram of an example text-to-speech system 600 including punctuation models, according to an example embodiment. Like TTS system 100 in fig. 1, TTS system 600 includes text analysis module 606, TTS subsystem 608, and speech generator 610. However, TTS system 600 also includes a substring accumulation module 602 followed by punctuation model 604 before text analysis module 606. Like the TTS system 100, the elements and modules of the TTS shown in fig. 6 do not necessarily correspond exactly to the actual or particular components of a particular implementation of the TTS system, but rather represent at least a convenient conceptualization of operations performed during TTS processing, including punctuation predictions of an input text string that may otherwise lack grammatical punctuation.
In general, TTS system 600 applies a punctuation model to an input string or portion thereof to generate a pre-processed sub-string 605, which may then be processed by text analysis module 606 and other downstream processing elements in a manner similar to TTS 100 shown in fig. 1 to process input text string 101. In more detail, as described below, the substring accumulation module 602 and punctuation model 604 together may segment or subdivide the input-stream text 601 into two or more consecutive substrings for separate processing by the TTS subsystem 608 and/or the speech generator 610.
According to an example embodiment, the substring accumulation module 602 may be used to accumulate successive sub-portions of the input-stream text 610 into an accumulated substring 603, the accumulated substring 603 then being processed by the punctuation model 604 to produce a preprocessed substring 605. For example, the accumulated substrings may correspond to some number of input text objects, such as letters (e.g., text characters), words (e.g., syntactic groupings of text characters), or phrases. A given substring may be incrementally accumulated from the incoming streaming text and input to punctuation model 604 to generate a punctuation version 603 of the accumulated substring. If the accumulated substring 603 corresponds to the entire input-stream text string, the punctuation version of the substring may be passed to the text analysis module 606. If the accumulated substring 603 corresponds to less than the entire input-stream text string, punctuation versions of the accumulated substring 603 may be searched for punctuation of the delimited accumulated substring 603 for the TTS synthesis process. If a suitable punctuation is found in the punctuation version of the accumulated substring 603, the accumulated substring 603 may be passed to a text analysis module 606. If no suitable punctuation is found in the punctuation version of the accumulated substring 603, additional incoming streaming text may be accumulated into a larger substring, which may again be tested for bounding punctuation. This incremental accumulation process, represented by the arrow labeled "decide how much to accumulate" in FIG. 6, may be iteratively repeated until punctuation model 604 may generate a punctuation version of accumulated substring 603, which contains the appropriate punctuations for delimiting accumulated substring 603.
In fig. 6, the sub-string containing the appropriate punctuation for delimitation (including the case of an entire streaming text string) is shown as a pre-processing sub-string 605, which may then be processed into phonetic transcription 607 by text analysis module 606. TTS subsystem 608 then applies TTS synthesis to generate acoustic properties 609, and speech generator 610 may generate an audio output in the form of synthesized speech 611 from acoustic properties 609, as shown.
In an example embodiment, substring accumulation may be performed incrementally one input word at a time, where space characters between letter groupings may be used as separators (relimiers). In such an approach, the substring may be built one word at a time and effectively tested by punctuation model 604 as each subsequent word is appended to the existing substring.
In general, an input text stream, whether from a streaming source such as a text application or from a static source such as a text file or copy-and-paste from archived text, may be subdivided into any two or more sub-strings that may be individually synthesized into speech. In practice, it may be more common to have only two or possibly three substrings subdivided. As mentioned, the entire input text string may be processed by the TTS synthesized punctuation model and not subdivided at all.
One advantage of sub-dividing into sub-strings is that it enables TTS processing of incoming streaming text as it arrives, thereby reducing the delay caused by waiting for the entire streaming text string to arrive before processing. For example, in the case of a streaming text string generated by a text application, TTS processing may begin at an initial portion of the streaming text, even though the user is still typing in a later portion. It is also possible to play a synthesized portion of the audio while synthesizing the latter portion, even while the user is still typing the latter portion. Details of these different modes are described in the context of the following example operations.
According to example embodiments, punctuation models may be based on Artificial Neural Networks (ANNs) or other forms or machine learning. For example, an ANN may be trained to predict punctuation text as output from non-punctuation text as input. In an example embodiment, the input may be a sequence of characters of a text string, and the output may be a calculated probability that each character of the input string is output as the same character or punctuation. The training data may include labeled pairs of text strings, where one element of each pair is a punctuation-free version of the other element. The non-punctuation elements may represent input data, while the punctuation elements may represent "ground truth" for comparison with predicted output during training. Training may require adjustment of model parameters to achieve a statistically determined "best fit" between predicted and "true" punctuations.
b. Example operations
As described above, the substring process may require any number of consecutive or sequential substrings. For purposes of discussion herein, the only cases considered in detail are the cases where there are no substrings (i.e., complete input strings) or there are two substrings. The expansion from two substrings to more than two substrings is straightforward and by considering only two substrings there is no general penalty for more than two substrings. In the following discussion, an example case of processing the entire received string (i.e., without substring) is first described. Next is a description of two example cases, each case having two substrings. The first example shows the audio playback of a first substring while synthesizing a second substring. The second example shows the audio playback of a first substring while receiving a second substring, followed by the simultaneous synthesis of the second substring.
FIG. 7A depicts an example timing diagram of string accumulation during text-to-speech synthesis using punctuation models, according to an example embodiment. For all example cases, the streaming text string is considered to be received in real-time, measured from the start point to the end point, as shown by timeline 732. Example timeline 732-1 illustrates the accumulation of an entire incoming text string; i.e. no substring (or one substring equal to the whole string). In this case, the initial point is equal to the starting point, the first trigger point is equal to the end point, and there is no second trigger point. Example timelines 732-2, 732-3, and 732-4 each illustrate an example case where two (or more) substrings are accumulated. In these cases, the first substring is considered to accumulate from an initial point to a first trigger point, where the initial point is greater than or equal to the initial point and the first trigger point is greater than the initial point and less than the second trigger point. The second trigger point is less than or equal to the endpoint.
The relationship between the timing elements discussed herein and shown in FIG. 7A may be represented briefly as t start ≤t initial ≤trigger 1 <trigger 2 ≤t ending As summarized at the top of fig. 7A. Note that t is shown for timelines 732-4 and 732-4 start ≤t initial May include sub-string 0 preceding sub-string 1. This possibility is shown in the gray illustration of fig. 7A.
The term "trigger point" is introduced for ease of discussion only. According to an example embodiment, the trigger point marks the end of one sub-string and the beginning of the next sub-string, if there is a next sub-string. The trigger points may be text delimiters, such as punctuation marks separating words and/or phrases. Non-limiting examples of such punctuation marks include commas, periods, question marks, and exclamation marks. For example, the trigger point may also be the end of a fully entered string and/or the detection of a "send" command from a text application.
Fig. 7B, 7C, and 7D illustrate a process flow of TTS processing using a punctuation model according to an example embodiment. In each example, the streaming text is presented as input to the TTS system for processing, composition, and playback. For example, in a typical embodiment, the source of the streaming text may be a text application. However, the source may also or alternatively be a text file or saved text from a text application. In the examples of fig. 7B, 7C, and 7D, receiving streaming text at the TTS system may be considered the arrival of text characters because they were typed in using a text application or other real-time streaming text generator. With this description, the term "accumulation" may be considered incremental receipt of characters and/or words at the TTS system. Clicking on the "send" button, or a similar trigger or command from the streaming text program, may be considered the following signal: the entire text string is complete and should be converted to speech (synthesized) and its audio rendering produced and played. In the example of fig. 5, this corresponds to transmitting an audio playback to a remote communication device.
The main difference of the example operations shown in fig. 7B, 7C and 7D is that: before clicking the "send" button, the TTS process of accumulating text is started or not and which TTS process is started. Specifically, starting the process before clicking the "send" button may reduce the delay associated with waiting until clicking the "send" button. For real-time streaming text applications and other real-time text streaming applications, this can advantageously make voice communications (where the source of the voice is the text application) sound more natural in terms of quality of synthesized voice and reduced end-to-end delay.
FIG. 7B depicts an example process flow for text-to-speech synthesis in which an entire streaming text string is received prior to processing by a punctuation model, according to an example embodiment. The process flow is shown at the top of the figure and the process timelines 714-B and 716-B are shown below the process flow. The streaming text string 702 arrives at the TTS system and is accumulated 704 in real time. As described above, this may correspond to receiving text characters generated (e.g., typed) by, for example, a streaming text program. When the entire text string is accumulated, as signaled by clicking on the send button 706, the entire text string is input to the punctuation model 708, which punctuation model 708 generates a punctuation text string that includes the added grammatical punctuation determined by the punctuation model 708.
In the process flow of FIG. 7B, real-time text string accumulation 704 is input to punctuation model 708 at a first trigger point. The output of the punctuation model is then input to TTS synthesis 710, which then generates audio output 712.
In this example, timeline 714-B shows that the initial point coincides with the beginning at the initial point and the first trigger point coincides with the end point. For example, the first trigger point may correspond to a "send" button signal.
As shown in timeline 716-B, the entire text string is accumulated over the interval from the initial point to the first trigger point. Also as shown, the accumulation or receipt of the entire text string is followed by punctuation of the entire text string, synthesizing speech from the punctuation text string, and finally playing the synthesized text string. It should be noted that the apparent relative durations of each operation in timeline 716-B are for illustration purposes and are not necessarily to scale and/or intended to convey an actual quantitative relationship.
FIG. 7C depicts an example process flow for text-to-speech synthesis using a punctuation model in which two substrings are accumulated, according to an example embodiment. In this example, the TTS synthesis process of the first substring is performed simultaneously with the accumulation of the second substring, and the audio playback of the first substring is performed simultaneously with the TTS process of the second substring. The process flow is shown at the top of the figure and the process timelines 714-C, 716-C1, and 716-C2 are shown below the process flow. The streaming text string 702 arrives at the TTS system and is accumulated 704 in real time. Again, this may correspond to receiving text characters generated (e.g., typed) by, for example, a streaming text program. In this example, the first trigger point occurs before the endpoint, and the tag second trigger point coincides with the endpoint. As a result, the entire text string is accumulated in two consecutive substrings, as described below.
In the process flow of FIG. 7C, partial accumulation of the real-time text string 704 produces a real-time text sub-string 722, which real-time text sub-string 722 is input to punctuation model 708 when some threshold amount of text has been accumulated. In an example embodiment, the threshold may correspond to a accumulation of one or more complete words. For example, as described above, the output 724 of the punctuation model is then evaluated against "acceptable" punctuation that includes the bounding punctuation. If the fruit time text substring 722 can be delimited based on the output of the punctuation model, then the real-time text substring 722 is input to the TTS synthesis 710. If text substring 722 cannot be delimited at the time of the fruit, additional text is accumulated and punctuation model testing is applied again. This cycle is repeated until the real-time text substring 722 can be delimited, followed by TTS synthesis 710 and audio playback 712.
When the real-time text substring 722 is input to the TTS synthesis 710, accumulation of the next sequential substring begins. Note that in practice, the accumulation may be continuous from one substring to the next. Once the accumulation of the next sequential sub-string is complete, the generation of the audio output 712 of the initial sub-string may begin. This is indicated on timeline 716-C1 by the "wait" gap between TTS synthesis and audio playback.
The substring accumulation process just described may be repeated for: as many consecutive substrings as possible can be accumulated in the repeatedly arrived streaming text. The boundary between consecutive substrings is the trigger point. For the present example, only the first sub-string and the second sub-string are considered. The end of the first substring and the beginning of the second substring are marked by a first trigger point. In this example, the end of the second substring is marked by a second trigger point. In the illustration of FIG. 7C, the second substring corresponds to the last substring 726, which is directly input to the TTS composition 710 upon receipt of the send button 706. Thus, the second trigger point coincides with the end point of the arriving streaming text.
In this example, timeline 714-C shows that the initial point coincides with the starting point, and that the first trigger point occurs before the ending point. The first trigger point marks the end of the first sub-string and the beginning of the second sub-string, and the second trigger point marks the end of the second sub-string. For example, the second trigger point may correspond to a "send" button signal.
As shown in timeline 716-C1, the first substring accumulates over an interval from the initial point to the first trigger point. As marked on the timeline 716-C1, the accumulation is assumed to include punctuation and testing delimited in the manner described above, where the result of the accumulation and punctuation is referred to as the "preprocessed first substring". Speech is then synthesized from the preprocessed first substring, and the synthesized first substring is finally played.
As shown in timeline 716-C2, the second substring accumulates over the interval from the first trigger point to the second trigger point. As marked on the timeline 716-C2, the accumulation is also assumed to include punctuation and test for delimitation and/or receive "send" button signal 706, where the result of the accumulation and punctuation is referred to as the "preprocessed second substring". Speech is then synthesized from the preprocessed second substring, and the synthesized second substring is finally played. The play of the second sub-string corresponds to the completion of the play of the entire text string, although in the play of two consecutive sub-strings. Comparison of the timelines 716-C1 and 716-C2 shows that the accumulation of the second substring occurs simultaneously with the TTS synthesis of the first substring, and that the TTS synthesis of the second substring occurs simultaneously with the playback of the first substring. Note that the accumulation of the second (and first) substrings may correspond to the typing (or generation) of streaming text. Thus, the processing of the first substring occurs simultaneously with the typing of the second substring.
For comparison with the TTS processing of the entire text string (as shown in fig. 7B), the time stamp of the completion of the play of the example of the processing for the entire text string is shown on the timeline 716-C2. It can be seen that the corresponding time to complete playing the second sub-string is earlier than the time to process and play the entire text string. This illustrates the reduction in delay. As with the timeline of FIG. 7B, the apparent relative durations of each operation in the timeline 716-C are for purposes of illustration and are not necessarily to scale and/or intended to convey an actual quantitative relationship.
FIG. 7D depicts another example process flow for text-to-speech synthesis using a punctuation model in which two substrings are accumulated, according to an example embodiment. In this example, the TTS synthesis process (and possibly also at least partial playback of the first substring) is performed simultaneously with the accumulation of the second substring, and at least partial audio playback of the first substring is performed simultaneously with the TTS process of the second substring. The process flow is shown at the top of the figure and the process timelines 714-D, 716-D1, and 716-D2 are shown below the process flow. The streaming text string 702 arrives at the TTS system and is accumulated 704 in real time. Again, this may correspond to receiving text characters generated (e.g., typed) by, for example, a streaming text program. In this example, the first trigger point occurs before the endpoint and the second trigger point coincides with the endpoint. As a result, the entire text string is accumulated in two consecutive substrings, as described below.
In the process flow of FIG. 7D, partial accumulation of the real-time text string 704 produces a real-time text sub-string 722, which real-time text sub-string 722 is input to punctuation model 708 when some threshold amount of text has been accumulated. In an example embodiment, the threshold may correspond to a accumulation of one or more complete words. For example, as described above, the output 724 of the punctuation model is then evaluated against "acceptable" punctuation that includes the bounding punctuation. If the fruit time text substring 722 can be delimited based on the output of the punctuation model, then the real-time text substring 722 is input to the TTS synthesis 710. If text substring 722 cannot be delimited at the time of the fruit, additional text is accumulated and punctuation model testing is applied again. This cycle is repeated until the real-time text substring 722 can be delimited, followed by TTS synthesis 710 and audio playback 712.
When the real-time text substring 722 is input to the TTS synthesis 710, accumulation of the next sequential substring begins. As described above, the accumulation may be continuous from one substring to the next. In some cases, completion of TTS synthesis process 710 may be completed before the accumulation of the next consecutive substring ends. For example, when the initial substring has been synthesized and can be played, the real-time streaming text application may still be generating text, e.g., the user may still be typing streaming text. In this case, a determination 728 is made as to whether the synthesized speech is "ready to send" before playback can begin. If so, playback may begin. If not, playback is delayed until more arriving streaming text strings are received and synthesized. This operation allows playback to begin while the streaming text is still being received, but only when the "ready to send" condition is satisfied.
In an example embodiment, the "ready to send" condition may correspond to criteria for evaluating: the likelihood that the source text of the streaming text that has been received and synthesized prior to the signaling of the send button 706 will be edited, revised and/or modified. Again for the case of a streaming text application, the user entering the text message may decide to make a change before clicking the send button. If the initial portion of the input text has been synthesized and played, then the user modifies the played portion of the text message too late. The "ready to send" criteria may thus be used to evaluate the likelihood that a change will be made. If the likelihood is below a "ready to send" threshold (or conversely if the likelihood of not making a change is above a complementary "ready to send" threshold), then playback may be made while the streaming text is still accumulating. Otherwise, playback is delayed until more text is received and synthesized such that the threshold is met and/or if a send button signal is received.
The substring accumulation process may be repeated for: as many consecutive substrings as possible can be accumulated in the repeatedly arrived streaming text. The boundary between consecutive substrings is the trigger point. For the present example, only the first sub-string and the second sub-string are considered. The end of the first substring and the beginning of the second substring are marked by a first trigger point. In this example, the end of the second substring is marked by a second trigger point. In the illustration of FIG. 7D, the second substring corresponds to the last substring 726, which is directly input to the TTS composition 710 upon receipt of the send button 706. Thus, the second trigger point coincides with the end point of the arriving streaming text.
In this example, timeline 714-D shows that the initial point coincides with the starting point and that the first trigger point occurs before the ending point. The first trigger point marks the end of the first sub-string and the beginning of the second sub-string, and the second trigger point marks the end of the second sub-string. For example, the second trigger point may correspond to a "send" button signal.
As shown in timeline 716-D1, the first substring accumulates over an interval from the initial point to the first trigger point. As marked on the timeline 716-D1, the accumulation is assumed to include punctuation and testing delimited in the manner described above, where the result of the accumulation and punctuation is referred to as the "preprocessed first substring". Speech is then synthesized from the preprocessed first substring, and if a "ready to send" criterion is met, the synthesized first substring is played.
As shown in timeline 716-D2, the second substring accumulates over an interval from the first trigger point to the second trigger point. As marked on the timeline 716-D2, the accumulation is also assumed to include punctuation and test for delimitation and/or receive "send" button signal 706, where the result of the accumulation and punctuation is referred to as the "preprocessed second substring". Speech is then synthesized from the preprocessed second substring, and the synthesized second substring is finally played. The play of the second sub-string corresponds to the completion of the play of the entire text string, although in the play of two consecutive sub-strings. Comparison of the timelines 716-D1 and 716-D2 shows that the accumulation of the second substring occurs simultaneously with the TTS synthesis and at least partial play of the first substring, and that the TTS synthesis of the second substring occurs simultaneously with any remaining play of the first substring. Note that the accumulation of the second (and first) substrings may correspond to the typing (or generation) of streaming text. Thus, processing of the first substring and at least a portion of typing of the second substring occur simultaneously.
For comparison with the TTS processing of the entire text string (as shown in fig. 7B), the time stamp of the completion of the play of the example of the processing for the entire text string is shown on the timeline 716-D2. It can be seen that the corresponding time to complete playing the second sub-string is earlier than the time to process and play the entire text string. This again illustrates the reduction in delay. As with the timelines of FIGS. 7B and 7C, the apparent relative durations of each operation in the timelines 716-D are for purposes of illustration and are not necessarily to scale and/or intended to convey an actual quantitative relationship.
FIG. 8 depicts the example usage scenario shown in FIG. 5, but now with operations including text-to-speech synthesis of punctuation models, according to an example embodiment. Again, the smartphone 502 is used to input text, e.g., via a text application, and convert the text to speech, which can then be transmitted to the phone 506 over the communication network 504 and played by the audio component 506-1.
The user may type in text consisting of, for example, again a string 501 "hi you want to and me see the lunch at the face and may subscribe to pizza palace to let me know". The sending user may click a virtual "send" button on the smartphone 502 (as represented by the pointing finger in fig. 5), this time invoking the TTS system 805 of the smartphone 502, which TTS system 805 generates a synthesized voice, represented in the figure by waveform 803, which is then transferred to the cellular phone 506 as indicated. The curved dashed arrow represents the transmission to the smart phone 506.
In this example, the absence of grammar punctuation in the input text stream is compensated for by TTS system 802, which includes a punctuation model. By adding punctuation to the text string prior to TTS synthesis, the system can now synthesize output speech 805 that sounds natural. This is visually represented in fig. 5 by: each word of the input text 501 is placed in a meaningful phrase and the font size and style are intended to represent the word spoken in the output speech 805. The arrangement shown in fig. 8 may be particularly beneficial when the user of telephone 506 is visually impaired and may have difficulty reading plain text messages. Without the advantageous improvement in speech quality of synthesized speech produced by the techniques and methods of the example embodiments herein, a visually impaired user of cellular telephone 506 would have to meet the defective quality illustrated in fig. 5 and the like.
c. Example method
In example embodiments, the example methods may be implemented as machine readable instructions, which when executed by one or more processors of a system, cause the system to perform the various functions, operations, and tasks described herein. In addition to the one or more processors, the system may also include one or more forms of memory for storing machine-readable instructions (and possibly other data) for the example methods, as well as one or more input devices/interfaces, one or more output devices/interfaces, and possibly other components. Some or all aspects of the example methods may be implemented in a TTS synthesis system, which may include functionality and capabilities specific to TTS synthesis. However, not all aspects of the example method need depend on implementation in a TTS synthesis system.
In an example embodiment, a TTS synthesis system including a punctuation model may be implemented in an apparatus including one or more processors, one or more forms of memory, one or more input devices/interfaces, one or more output devices/interfaces, and machine-readable instructions, which when executed by the one or more processors, cause the TTS synthesis system including the punctuation model to perform the various functions and tasks described herein. The TTS synthesis system may also include an implementation based on one or more hidden markov models. In particular, TTS synthesis systems may employ methods that incorporate HMM-based speech synthesis as well as other possible components. Additionally or alternatively, the TTS synthesis system may further include one or more Artificial Neural Network (ANN) based implementations. In particular, TTS synthesis systems may employ methods that incorporate ANN-based speech synthesis as well as other possible components. In addition, punctuation models can be implemented using methods that incorporate ANN-based speech synthesis as well as other possible components.
In an example embodiment, the apparatus may be a communication device, such as a smart phone, PDA, tablet device, laptop computer, or the like. In operation, the communication device may be communicatively connected to a remote communication device by way of a communication network, such as a telephone network, the public internet, or a wireless communication network (e.g., a cellular broadband network). Streaming text applications such as interactive text/messaging applications may also be implemented on a communication device and may be a streaming text input source for TTS systems.
Fig. 9 is a flowchart illustrating an example method 900 according to an example embodiment. In step 902, the tts system may receive a real-time streaming text string, for example, from a streaming text application. The real-time streaming text string may have a start point and an end point. The start and end points may correspond to both the text string itself and the time interval in which the TTS system receives the entire streaming text string. For example, the first character of the streaming text string may be received at the time marked by the start point, while the last character and/or the "send" button signal may be received at the time marked by the end point.
At step 904, the tts system may accumulate a first substring comprising a first portion of the text string received from an initial point to a first trigger point. The initial point may be no earlier than the starting point and may precede the first trigger point, and the first trigger point may not exceed the end point.
In step 906, the TTS system can apply a punctuation model of the TTS system to the first substring to generate a preprocessed first substring comprising the first substring with the grammar punctuation determined by the punctuation model added. Non-limiting examples of grammar punctuation may include commas, periods, question marks, exclamation marks, semicolons, and colon.
At step 908, the TTS system may perform TTS synthesis processing on at least the pre-processed first substring to generate a first synthesized speech.
Finally, at step 910, an audio playback of the first synthesized speech may be generated.
According to an example embodiment, the first substring may be: (a) A completely received text string, wherein the initial point is a start point and the first trigger point is an end point and marks the end of the text string; (b) Less than the fully received text string, wherein the initial point is a starting point and the first trigger point is before the end point; (c) Less than the fully received text string, wherein the initial point is after the start point and the first trigger point is the end point; or (d) fewer than the complete received text string, wherein the initial point is after the start point and the first trigger point is before the end point. Case (b) corresponds to a first substring starting at the start point and ending before the end point. For this case, the subsequent substring may follow the first substring. Case (c) corresponds to the first substring beginning after the start point and ending at the end point. For this case, the preceding substring may precede the first substring. Case (d) corresponds to the first substring starting after the start point and ending before the end point. For this case, the preceding substring may precede the first substring, and the subsequent substring may follow the first substring.
According to an example embodiment, as described above, receiving a real-time streaming text string may entail receiving streaming text output from an interactive text application executing on a communication device communicatively connected to a remote device. For this example, the first trigger point may correspond to a command from the interactive text application to send a text string to the remote device. The audio playback that produced the first synthesized speech may then be transmitted from the communication device to the remote device over the communication connection.
According to an example embodiment, when the first trigger point is before the endpoint, the method 900 may further include, while applying the TTS synthesis process to the preprocessed first substring to generate the first synthesized speech, accumulating a second substring comprising a second portion of the text string received from the first trigger point to a second trigger point, wherein the second trigger point is after the first trigger point and does not exceed the endpoint. The example method 900 may also further include applying a punctuation model to the second substring to generate a preprocessed second substring. Further, the operations may also include, while generating the audio playback of the first synthesized speech, concurrently applying TTS synthesis processing to the preprocessed second substring to generate the second synthesized speech, and generating the audio playback of the second synthesized speech.
Further according to the example embodiment, the first substring may be: less than a fully received text string, wherein the initial point is a starting point; or less than a fully received text string, with the initial point after the start point.
According to an example embodiment, as described above, receiving a real-time streaming text string may entail receiving streaming text output from an interactive text application executing on a communication device. In this case, the first trigger point and the second trigger point may each correspond to the end of a different respective word of the streaming text output.
According to an example embodiment, accumulating the first substring may require incrementally accumulating from the received real-time streaming text into the first intermediate substring one consecutive word at a time when the first trigger point may be before the endpoint, and after each consecutive accumulation of consecutive words into the first intermediate substring, applying a punctuation model to the first intermediate substring to generate a preprocessed first intermediate substring. A first specific punctuation added by the punctuation model may be searched in each pre-processed first intermediate substring, which delimits the first intermediate substring for the TTS synthesis process. The first trigger point may then be set to the occurrence of the first particular punctuation in the preprocessed first intermediate substring, and the first substring may be determined to be the delimited first intermediate substring. With this arrangement, applying a punctuation model of the TTS system to the first substring to generate a preprocessed first substring may require generating a preprocessed first intermediate substring with a first specific punctuation occurrence. Non-limiting examples of specific punctuations may include commas, periods, question marks, exclamation marks, semicolons, and colon.
According to an example embodiment, the example method 900 may further include operations performed concurrently with applying the TTS synthesis process to the preprocessed first substring to generate the first synthesized speech. The operations may include incrementally accumulating one continuous word at a time from the first trigger point into the second intermediate substring from the received real-time streaming text, and after each continuous accumulation of continuous words into the second intermediate substring, applying a punctuation model to the second intermediate substring to generate a preprocessed second intermediate substring. Then, setting the second trigger point may be set to: (i) Occurrence of a second specific punctuation in the pre-processed second intermediate substring that delimits the second intermediate substring for the TTS synthesis process, or (ii) a signal indicative of an endpoint of the received real-time streaming text. The second substring may then be set to a second intermediate substring from the first trigger point to the second trigger point.
Further in accordance with the example embodiment, the example method may entail simultaneously applying TTS synthesis to the second substring to generate the second synthesized speech while generating the audio playback of the first synthesized speech. This may then produce an audio playback of the second synthesized speech.
According to an example embodiment, the example method 900 may further require operations performed concurrently with the audio playback that produced the first synthesized speech. The operations may include incrementally accumulating one continuous word at a time from the received real-time streaming text from the first trigger point into the second intermediate substring, and after each continuous accumulation of continuous words into the second intermediate substring, applying a punctuation model to the second intermediate substring to generate a preprocessed second intermediate substring. The second trigger point may then be set to: (i) Occurrence of a second specific punctuation in the pre-processed second intermediate substring, the second specific punctuation delimiting the second intermediate substring for the TTS synthesis process, or (ii) a signal indicative of an end point of the received real-time streaming text. The second substring may be set as a second intermediate substring from the first trigger point to the second trigger point, and TTS synthesis may be applied to the second substring to generate a second synthesized speech. In an operation after the audio playback of the first synthesized speech is generated, the audio playback of the second synthesized speech may be generated.
According to an example embodiment, as described above, receiving a real-time streaming text string may entail receiving streaming text output from an interactive text application executing on a communication device. The interactive text application may include an interactive display configured to display text entered by a user and provide text editing functionality. With this arrangement, the first trigger point and the second trigger point may each correspond to a different, corresponding word end of the streaming text output. The example method 900 may further entail, at the beginning of audio playback of the first synthesized speech, causing the text editing function to be disabled for any displayed user-entered text corresponding to the first substring.
According to an example embodiment, the punctuation model may include or be based on an Artificial Neural Network (ANN) trained to add grammatical punctuation to an input text string that includes a plurality of words but lacks any grammatical punctuation. Adding grammar punctuation may then include predicting the particular grammar punctuation and its corresponding location before and/or after the word of the input text string.
It will be appreciated that the steps shown in fig. 9 are intended to illustrate a method according to an example embodiment. In this way, various steps may be changed or modified, the order of certain steps may be changed, and additional steps may be added while still achieving the overall desired operation. The method may be performed by a client device, or by a server, or by a combination of a client device and a server. The method may be performed by any suitable computing device.
Conclusion(s)
Illustrative embodiments have been described herein by way of example. However, those skilled in the art will understand that changes and modifications may be made to the embodiment without departing from the true scope and spirit of the elements, products, and methods for which the embodiment is defined by the claims.
Claims (21)
1. A method, comprising:
Receiving, at a text-to-speech (TTS) system, a real-time streaming text string having a start point and an end point;
accumulating, at the TTS system, a first substring comprising a first portion of the text string received from an initial point to a first trigger point, wherein the initial point is not earlier than the initial point and before the first trigger point, and the first trigger point does not exceed the end point;
at the TTS system, applying a punctuation model of the TTS system to the first substring to generate a preprocessed first substring, the preprocessed first substring comprising the first substring with the grammar punctuation determined by the punctuation model added;
applying, at the TTS system, TTS synthesis processing to at least the pre-processed first substring to generate a first synthesized speech; and
an audio playback of the first synthesized speech is generated.
2. The method of claim 1, wherein the first substring is one of:
a completely received text string, wherein the initial point is a start point and the first trigger point is an end point and marks the end of the text string;
less than the fully received text string, wherein the initial point is a starting point and the first trigger point is before the end point;
less than the fully received text string, wherein the initial point is after the start point and the first trigger point is the end point; or alternatively
Less than the fully received text string, wherein the initial point is after the start point and the first trigger point is before the end point.
3. The method of claim 1 or 2, wherein receiving the real-time streaming text string comprises receiving streaming text output from an interactive text application executing on a communication device communicatively connected to the remote device,
wherein the first trigger point corresponds to a command from the interactive text application to send a text string to the remote device,
and wherein generating the audio playback of the first synthesized speech comprises transmitting the audio playback from the communication device to the remote device over the communication connection.
4. The method of claim 1 or 2, wherein the first trigger point is before the endpoint, and wherein the method further comprises:
while applying TTS synthesis processing to the preprocessed first substring to generate first synthesized speech, accumulating simultaneously a second substring comprising a second portion of the text string received from a first trigger point to a second trigger point, the second trigger point being subsequent to the first trigger point and not exceeding an endpoint;
applying a punctuation model to the second substring to generate a preprocessed second substring;
simultaneously applying TTS synthesis processing to the preprocessed second substring to generate second synthesized speech while generating audio playback of the first synthesized speech; and
An audio playback of the second synthesized speech is generated.
5. The method of claim 4, wherein the first substring is one of:
less than the fully received text string, wherein the initial point is a starting point; or alternatively
Less than the fully received text string, wherein the initial point is after the start point.
6. The method of claim 4 or 5, wherein receiving the real-time streaming text string comprises receiving streaming text output from an interactive text application executing on the communication device,
and wherein the first trigger point and the second trigger point each correspond to a different, corresponding end of word of the streaming text output.
7. The method of any of the preceding claims, wherein the first trigger point is prior to the endpoint,
wherein accumulating the first substring comprises:
incrementally accumulating one continuous word at a time from the received real-time streaming text into the first intermediate substring;
after each successive accumulation of successive words into the first intermediate substring, applying a punctuation model to the first intermediate substring to generate a preprocessed first intermediate substring, and searching the preprocessed first intermediate substring for a first specific punctuation added by the punctuation model, the first specific punctuation delimiting the first intermediate substring for the TTS synthesis process;
Setting the first trigger point to be the occurrence of a first specific punctuation in the preprocessed first intermediate substring; and
determining the first sub-string as a delimited first intermediate sub-string;
and wherein applying the punctuation model of the TTS system to the first substring to generate a preprocessed first substring comprises: a preprocessed first intermediate substring is generated having occurrences of a first particular punctuation.
8. The method of claim 7, further comprising, concurrently with applying TTS synthesis processing to the preprocessed first substring to generate the first synthesized speech:
incrementally accumulating, from the first trigger point, from the received real-time streaming text, one continuous word at a time into a second intermediate substring;
after each successive accumulation of successive words into the second intermediate substring, applying a punctuation model to the second intermediate substring to generate a preprocessed second intermediate substring;
setting the second trigger point to one of: (i) Occurrence of a second specific punctuation in the pre-processed second intermediate substring, the second specific punctuation delimiting the second intermediate substring for the TTS synthesis process, or (ii) a signal indicative of an end point of the received real-time streaming text; and
the second sub-string is determined as a second intermediate sub-string from the first trigger point to the second trigger point.
9. The method of claim 8, further comprising:
simultaneously applying TTS synthesis to the second substring to generate a second synthesized speech while generating an audio playback of the first synthesized speech;
and generating an audio playback of the second synthesized speech.
10. The method of claim 7, further comprising, concurrently with the audio playback that produces the first synthesized speech:
incrementally accumulating, from the first trigger point, from the received real-time streaming text, one continuous word at a time into a second intermediate substring;
after each successive accumulation of successive words into the second intermediate substring, applying a punctuation model to the second intermediate substring to generate a preprocessed second intermediate substring;
setting the second trigger point to one of: (i) Occurrence of a second specific punctuation in the pre-processed second intermediate substring, the second specific punctuation delimiting the second intermediate substring for the TTS synthesis process, or (ii) a signal indicative of an end point of the received real-time streaming text;
determining the second substring as a second intermediate substring from the first trigger point to the second trigger point; and
the TTS synthesis is applied to the second substring to generate a second synthesized speech,
and wherein after the audio playback of the first synthesized speech is generated, the audio playback of the second synthesized speech is generated.
11. The method of claim 10, wherein receiving the real-time streaming text string comprises receiving streaming text output from an interactive text application executing on the communication device, the interactive text application comprising an interactive display configured to display user input text and provide text editing functionality,
wherein the first trigger point and the second trigger point each correspond to an end of a different, corresponding word of the streaming text output,
and wherein the method further comprises: at the beginning of audio playback of the first synthesized speech, the text editing function is disabled for any displayed user-entered text corresponding to the first substring.
12. The method of any of claims 1-11, wherein the punctuation model comprises an Artificial Neural Network (ANN) trained to add grammatical punctuation to an input text string that includes both a plurality of words and lacks any grammatical punctuation,
and wherein adding the grammar punctuation includes predicting the particular grammar punctuation and its corresponding location before and/or after the word of the input text string.
13. A system comprising a text-to-speech (TTS) system implemented on a device, the device comprising:
One or more processors;
a memory; and
machine-readable instructions stored in memory, which when executed by one or more processors, cause a TTS system to perform operations comprising:
receiving a real-time streaming text string having a start point and an end point;
accumulating a first substring comprising a first portion of the text string received from an initial point to a first trigger point, wherein the initial point is not earlier than the start point and before the first trigger point, and the first trigger point does not exceed the end point;
applying a punctuation model of the TTS system to the first substring to generate a preprocessed first substring, the preprocessed first substring including the first substring with the grammar punctuation added as determined by the punctuation model;
applying a TTS synthesis process to at least the preprocessed first substring to generate a first synthesized speech; and
an audio playback of the first synthesized speech is generated.
14. The system of claim 13, wherein the apparatus is a communication device communicatively coupled to a remote device,
wherein receiving the real-time streaming text string includes receiving streaming text output from an interactive text application executing on the communication device,
wherein the first trigger point corresponds to at least one of: the end of a word of the streaming text output, or a command from the interactive text application to send a text string to the remote device,
And wherein generating the audio playback of the first synthesized speech comprises transmitting the audio playback from the communication device to the remote device over the communication connection.
15. The system of claim 13 or 14, wherein the first trigger point is before the endpoint, and wherein the operations further comprise:
while applying TTS synthesis processing to the preprocessed first substring to generate first synthesized speech, accumulating simultaneously a second substring comprising a second portion of the text string received from a first trigger point to a second trigger point, the second trigger point being subsequent to the first trigger point and not exceeding an endpoint;
applying a punctuation model to the second substring to generate a preprocessed second substring;
simultaneously applying TTS synthesis processing to the preprocessed second substring to generate second synthesized speech while generating audio playback of the first synthesized speech; and
an audio playback of the second synthesized speech is generated.
16. The system of any of claims 13-15, wherein the first trigger point is prior to the endpoint
Wherein accumulating the first substring comprises:
incrementally accumulating one continuous word at a time from the received real-time streaming text into the first intermediate substring;
After each successive accumulation of successive words into the first intermediate substring, applying a punctuation model to the first intermediate substring to generate a preprocessed first intermediate substring, and searching the preprocessed first intermediate substring for a first specific punctuation added by the punctuation model, the first specific punctuation delimiting the first intermediate substring for the TTS synthesis process;
setting the first trigger point to be the occurrence of a first specific punctuation in the preprocessed first intermediate substring; and
determining the first sub-string as a delimited first intermediate sub-string;
and wherein applying the punctuation model of the TTS system to the first substring to generate a preprocessed first substring comprises: a preprocessed first intermediate substring is generated having occurrences of a first particular punctuation.
17. The system of claim 16, wherein the operations further comprise, concurrently with applying TTS synthesis processing to the preprocessed first substring to generate the first synthesized speech:
incrementally accumulating, from the first trigger point, from the received real-time streaming text, one continuous word at a time into a second intermediate substring;
after each successive accumulation of successive words into the second intermediate substring, applying a punctuation model to the second intermediate substring to generate a preprocessed second intermediate substring;
Setting the second trigger point to one of: (i) Occurrence of a second specific punctuation in the pre-processed second intermediate substring, the second specific punctuation delimiting the second intermediate substring for the TTS synthesis process, or (ii) a signal indicative of an end point of the received real-time streaming text; and
the second substring is determined as a second intermediate substring from the first trigger point to the second trigger point,
and wherein the operations further comprise:
simultaneously applying TTS synthesis to the second substring to generate a second synthesized speech while generating an audio playback of the first synthesized speech;
and generating an audio playback of the second synthesized speech.
18. The system of claim 16, wherein the operations further comprise, concurrently with the audio playback that produces the first synthesized speech:
incrementally accumulating, from the first trigger point, from the received real-time streaming text, one continuous word at a time into a second intermediate substring;
after each successive accumulation of successive words into the second intermediate substring, applying a punctuation model to the second intermediate substring to generate a preprocessed second intermediate substring;
setting the second trigger point to one of: (i) Occurrence of a second specific punctuation in the pre-processed second intermediate substring, the second specific punctuation delimiting the second intermediate substring for the TTS synthesis process, or (ii) a signal indicative of an end point of the received real-time streaming text;
Determining the second substring as a second intermediate substring from the first trigger point to the second trigger point; and
the TTS synthesis is applied to the second substring to generate a second synthesized speech,
and wherein after the audio playback of the first synthesized speech is generated, the audio playback of the second synthesized speech is generated.
19. The system of claim 18, wherein the apparatus is a communication device communicatively coupled to a remote device,
wherein receiving the real-time streaming text string comprises receiving streaming text output from an interactive text application executing on the communication device, the interactive text application comprising an interactive display configured to display user input text and provide text editing functionality,
wherein the first trigger point and the second trigger point each correspond to an end of a different, corresponding word of the streaming text output,
and wherein the method further comprises: at the beginning of audio playback of the first synthesized speech, the text editing function is disabled for any displayed user input text corresponding to the first substring.
20. The system of any of claims 13-19, wherein the punctuation model comprises an Artificial Neural Network (ANN) trained to add grammatical punctuation to an input text string that includes both a plurality of words and lacks any grammatical punctuation,
And wherein adding the grammar punctuation includes predicting the particular grammar punctuation and its corresponding location before and/or after the word of the input text string.
21. An article of manufacture comprising a computer-readable storage medium having stored thereon program instructions that, when executed by one or more processors of a system comprising a text-to-speech (TTS) system, cause the system to perform operations comprising:
receiving a real-time streaming text string having a start point and an end point;
accumulating a first substring comprising a first portion of the text string received from an initial point to a first trigger point, wherein the initial point is not earlier than the start point and before the first trigger point, and the first trigger point does not exceed the end point;
applying a punctuation model of the TTS system to the first substring to generate a preprocessed first substring, the preprocessed first substring including the first substring with the grammar punctuation added as determined by the punctuation model;
applying a TTS synthesis process to at least the preprocessed first substring to generate a first synthesized speech; and
an audio playback of the first synthesized speech is generated.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/057529 WO2022093192A1 (en) | 2020-10-27 | 2020-10-27 | Method and system for text-to-speech synthesis of streaming text |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116018639A true CN116018639A (en) | 2023-04-25 |
Family
ID=73476256
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080102489.7A Pending CN116018639A (en) | 2020-10-27 | 2020-10-27 | Method and system for text-to-speech synthesis of streaming text |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230335111A1 (en) |
EP (1) | EP4176431A1 (en) |
CN (1) | CN116018639A (en) |
WO (1) | WO2022093192A1 (en) |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5634084A (en) * | 1995-01-20 | 1997-05-27 | Centigram Communications Corporation | Abbreviation and acronym/initialism expansion procedures for a text to speech reader |
US20090313020A1 (en) * | 2008-06-12 | 2009-12-17 | Nokia Corporation | Text-to-speech user interface control |
US10573312B1 (en) * | 2018-12-04 | 2020-02-25 | Sorenson Ip Holdings, Llc | Transcription generation from multiple speech recognition systems |
CN110119514A (en) * | 2019-04-02 | 2019-08-13 | 杭州灵沃盛智能科技有限公司 | The instant translation method of information, device and system |
-
2020
- 2020-10-27 EP EP20808583.7A patent/EP4176431A1/en active Pending
- 2020-10-27 CN CN202080102489.7A patent/CN116018639A/en active Pending
- 2020-10-27 WO PCT/US2020/057529 patent/WO2022093192A1/en unknown
- 2020-10-27 US US17/914,010 patent/US20230335111A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230335111A1 (en) | 2023-10-19 |
EP4176431A1 (en) | 2023-05-10 |
WO2022093192A1 (en) | 2022-05-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8527276B1 (en) | Speech synthesis using deep neural networks | |
CN108573693B (en) | Text-to-speech system and method, and storage medium therefor | |
US9542927B2 (en) | Method and system for building text-to-speech voice from diverse recordings | |
US9183830B2 (en) | Method and system for non-parametric voice conversion | |
US9177549B2 (en) | Method and system for cross-lingual voice conversion | |
US8805684B1 (en) | Distributed speaker adaptation | |
US8442821B1 (en) | Multi-frame prediction for hybrid neural network/hidden Markov models | |
US8996366B2 (en) | Multi-stage speaker adaptation | |
KR20210146368A (en) | End-to-end automatic speech recognition for digit sequences | |
US20130289989A1 (en) | Sampling Training Data for an Automatic Speech Recognition System Based on a Benchmark Classification Distribution | |
KR102375115B1 (en) | Phoneme-Based Contextualization for Cross-Language Speech Recognition in End-to-End Models | |
US20230122824A1 (en) | Method and system for user-interface adaptation of text-to-speech synthesis | |
CN116250038A (en) | Transducer of converter: unified streaming and non-streaming speech recognition model | |
US20200394258A1 (en) | Generation of edited transcription for speech audio | |
US20220310073A1 (en) | Mixture Model Attention for Flexible Streaming and Non-Streaming Automatic Speech Recognition | |
US20220122581A1 (en) | Using Speech Recognition to Improve Cross-Language Speech Synthesis | |
JP2024510817A (en) | Efficient streaming non-recurrent on-device end-to-end model | |
CN112785667A (en) | Video generation method, device, medium and electronic equipment | |
US11823697B2 (en) | Improving speech recognition with speech synthesis-based model adapation | |
US20230335111A1 (en) | Method and system for text-to-speech synthesis of streaming text | |
US20230017892A1 (en) | Injecting Text in Self-Supervised Speech Pre-training | |
US20230013587A1 (en) | Advancing the Use of Text and Speech in ASR Pretraining With Consistency and Contrastive Losses | |
US20220310061A1 (en) | Regularizing Word Segmentation | |
JP2024017562A (en) | Information processing device, information processing method and program | |
WO2023205132A1 (en) | Machine learning based context aware correction for user input recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |