EP4145803A1 - Off-chip memory backed reliable transport connection cache hardware architecture - Google Patents
Off-chip memory backed reliable transport connection cache hardware architecture Download PDFInfo
- Publication number
- EP4145803A1 EP4145803A1 EP22157939.4A EP22157939A EP4145803A1 EP 4145803 A1 EP4145803 A1 EP 4145803A1 EP 22157939 A EP22157939 A EP 22157939A EP 4145803 A1 EP4145803 A1 EP 4145803A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- cache
- connection
- chip memory
- rta
- asic
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/50—Network services
- H04L67/56—Provisioning of proxy services
- H04L67/568—Storing data temporarily at an intermediate stage, e.g. caching
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L49/00—Packet switching elements
- H04L49/10—Packet switching elements characterised by the switching fabric construction
- H04L49/109—Integrated on microchip, e.g. switch-on-chip
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2455—Query execution
- G06F16/24552—Database cache management
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L49/00—Packet switching elements
- H04L49/25—Routing or path finding in a switch fabric
- H04L49/253—Routing or path finding in a switch fabric using establishment or release of connections between ports
- H04L49/254—Centralised controller, i.e. arbitration or scheduling
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L49/00—Packet switching elements
- H04L49/30—Peripheral units, e.g. input or output ports
- H04L49/3063—Pipelined operation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L69/00—Network arrangements, protocols or services independent of the application payload and not provided for in the other groups of this subclass
- H04L69/30—Definitions, standards or architectural aspects of layered protocol stacks
- H04L69/32—Architecture of open systems interconnection [OSI] 7-layer type protocol stacks, e.g. the interfaces between the data link level and the physical level
- H04L69/322—Intralayer communication protocols among peer entities or protocol data unit [PDU] definitions
- H04L69/326—Intralayer communication protocols among peer entities or protocol data unit [PDU] definitions in the transport layer [OSI layer 4]
Definitions
- the Internet protocol suite is a set of communication protocols used for servicing data transmissions between two devices communicating information over the Internet or other computer networks.
- Transmission Control Protocol (“TCP") is a part of the Internet protocol suite that provides for connection-oriented, reliable, and ordered delivery of a stream of data packets between, for example, a web-browser application running on a client device and a web-server application running on a server device over a local or wide area network.
- Datacenters using communication protocols such as TCP encounter certain issues. For instance, incast is a many-to-one communication pattern commonly found in datacenters, which may result in incast congestion when multiple synchronized computing devices send data to a same receiver computing device in parallel. Further, because TCP protocols require ordered delivery of packets over a connection, a long tail latency, which is the amount of time for the last few packets among a series of packets to be transmitted, may prevent transmission of the next series of packets.
- Reliable transport protocols rely on tracking data according to an identifier and maintaining more active connections in a cache for faster processing. Less active, or inactive connections, may be stored off cache. However, given that there may be millions of connections, processing data and managing the connections may require significant processing power.
- the present disclosure provides a network interface card configured to track active and stored connection contexts within on-chip and off-chip memory.
- One aspect of the technology is directed to an application specific integrated circuit (ASIC), including a reliable transport accelerator (RTA), the RTA including a cache lookup database.
- the RTA may be configured to: determine, from a received data packet, a connection identifier; query the cache lookup database for a cache entry corresponding to a connection context having the connection identifier; and receive, in response to the query, a cache hit or a cache miss.
- the ASIC further includes on-chip memory storing one or more connection contexts associated with one or more connection identifiers.
- the ASIC is attached to a network interface card.
- the network interface card includes off-chip memory storing one or more connection contexts associated with one or more connection identifiers, wherein the off-chip memory is connected to the ASIC.
- a cache miss indicates that the connection context having the connection identifier is not stored in the cache lookup database.
- the RTA retrieves the connection context from the on-chip memory or the off-chip memory.
- the RTA stores the connection context in the on-chip memory or the off-chip memory based on a predefined user configuration.
- a cache hit indicates that the connection context having the connection identifier is stored in the cache lookup database.
- a cache policy is stored by the cache lookup database, wherein the cache policy defines when one or more connection contexts are evicted from the cache lookup database.
- evicted connection contexts are moved to on-chip or off-chip memory.
- the system includes an upper-layer protocol accelerator (ULP) and a packet process pipeline accelerator.
- ULP upper-layer protocol accelerator
- UPF packet process pipeline accelerator
- the method may include determining, by a reliable transport accelerator (RTA) including a cache lookup database, a connection identifier for a received data packet; querying, by the RTA, the cache lookup database for a cache entry corresponding to a connection context having the connection identifier; and receiving, from the cache lookup database, in response to the query, a cache hit or a cache miss.
- RTA reliable transport accelerator
- a cache miss indicates that the connection context having the connection identifier is not stored in the cache lookup database.
- the RTA retrieves the connection context from on-chip memory or off-chip memory.
- the RTA stores the connection context in the on-chip memory or the off-chip memory based on a predefined user configuration.
- a cache hit indicates that the connection context having the connection identifier is stored in the cache lookup database.
- the RTA further includes a cache policy stored by the cache lookup database, wherein the cache policy defines when one or more connection contexts are evicted from the cache lookup database.
- evicted connection contexts are moved to on-chip or off-chip memory.
- the on-chip memory is located on an ASIC with the RTA.
- the off-chip memory is located off the ASIC.
- the technology generally relates to a network interface card configured to track active and stored connection contexts within on-chip and off-chip memory.
- the particular Reliable Transport protocol, described herein, referred to as "RT,” employs a connection-oriented architecture that provides reliable packet delivery over a lossy and out-of-order network. Every reliable transport packet may be associated with a connection. The states of each connection may be tracked in a connection context, which includes sliding windows for packet reliability, transaction ordering information, security protection and congestion control, etc.
- the network interface card described herein may support thousands of active connections within an on-chip cache and a million, or more, connections stored in off-chip memory.
- connection context refers to the state of a connection, which may include sliding windows for packet reliability, transaction ordering information, security protection, and congestion control. Other information about a connection may also be included in a connection context.
- FIGURE 1 shows an example network 100.
- the network 100 includes various entities, such as entity A, entity B, and entity C.
- connections are formed between the entities, such as connection 110 between entities A and B, and connection 120 between entities A and C.
- the entities may communicate over the connections using one or more protocols.
- RT is a protocol that notifies the sender whether or not the delivery of data to an intended receiver was successful.
- a sender and a receiver are considered peers of a communication protocol, thus entities A and B may be reliable transport peers, and entities A and C may be reliable transport peers.
- a connection over which RT is used is an end-to-end construct that describes a bidirectional communication channel between two reliable transport peers.
- a connection may be identified by a pair of Connection IDs ("CIDs"), one in each direction of communication. CIDs may be allocated by a receiver entity during the connection setup process and have no global significance outside of the parties involved.
- CIDs may be allocated by a receiver entity during the connection setup process and have no global significance outside of the parties involved.
- the connection 110 between entities A and B may have a CID with value 5 for the direction from A to B, and a CID with value 10 for the direction from B to A.
- the connection 120 between entities A and C may have a CID value 5 for the direction from A to C and a CID with value 11 for the direction from C to A.
- CIDs assigned by an entity or "Source CIDs" of an entity must have different values.
- the CIDs assigned by entity A or Source CIDs of entity A have different values 10 and 11.
- "Destination CIDs" of an entity are assigned by other entities and may have the same value.
- the Destination CIDs of entity A are assigned by
- Packets may be transmitted over the connections between the entities.
- a packet is a basic unit of communication across a connection.
- a packet may have a predetermined size, for example up to a maximum transfer unit ("MTU") in length.
- a packet may have a header including information about the packet and its transmission, and a payload of data.
- a reliable transport packet may include the Destination CID, such as in a header. For example, when entity B receives a packet over the connection 110 with the Destination CID of 5, entity B may identify the packet as coming from entity A, and may then notify A that the packet has been received by sending an acknowledgment over the connection 110 referencing this packet and its CID of 5. The acknowledgment itself may be sent as a packet including the Destination CID of 10.
- Entities A, B, and C may be any type of device capable of communicating over a network, such as personal computing devices, server computing devices, mobile devices, wearable devices, virtual machines, etc.
- FIGURE 2 is a block diagram of some components in an example system 200 that can communicate using RT protocol.
- the system 200 includes at least two entities having one or more connections between them. It should not be considered as limiting the scope of the disclosure or usefulness of the features described herein.
- the system 200 is shown with two entities, one or more computing devices 210 and one or more computing devices 260, with a connection 250 between them.
- computing devices 210 may be entity A and computing devices may be entity B of FIGURE 1
- connection 250 may be connection 110 of FIGURE 1 .
- the computing devices 210 and 260 may be configured with similar components as shown or may include additional and/or different components.
- the computing devices 210 contain one or more processors 220, memory 230, and one or more network interface cards 252.
- the one or more processors 270, memory 280, and the one or more network interface cards 292 of computing device 260 may be configured similarly to one or more processors 220, memory 230, and one or more network interface cards 252 of computing devices 210.
- the one or more processors 220 can be any conventional processor, such as a commercially available CPU. Alternatively, the processors can be dedicated components such as an application-specific integrated circuit ("ASIC") or other hardware-based processor. Although not necessary, one or more of the computing devices 210 may include specialized hardware components to perform specific computing processes.
- ASIC application-specific integrated circuit
- the memory 230 can be of any non-transitory type capable of storing information accessible by the processor, such as a hard-drive, memory card, ROM, RAM, DRAM, DVD, CD-ROM, write-capable, and read-only memories.
- Memory 230 of the computing devices 210 can store information accessible by the one or more processors 220, including data 232 and instructions 234.
- Memory 230 can include data 232 that can be retrieved, manipulated or stored by the processors 220.
- data such as communication protocols, connection information such as CIDs, definitions of headers, etc., as described herein with respect to FIGURE 1 and FIGURES 3-9 may be retrieved, manipulated, or stored by the processors 220.
- Memory 230 of the computing devices 210 can also store instructions 234 that can be executed by the one or more processors 220. For instance, instructions such as communication protocols as described with reference to FIGURES 1 and 3-9 may be performed by the one or more processors 220 according to instructions 234 and data 232 in memory 230.
- Data 232 may be retrieved, stored, or modified by the one or more processors 220 in accordance with the instructions 234.
- the data can be stored in computer registers, in a relational database as a table having many different fields and records, or XML documents.
- the data can also be formatted in any computing device-readable format such as, but not limited to, binary values, ASCII, or Unicode.
- the data can comprise any information sufficient to identify the relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories such as at other network locations, or information that is used by a function to calculate the relevant data.
- the instructions 234 can be any set of instructions to be executed directly, such as machine code, or indirectly, such as scripts, by the one or more processors.
- the terms "instructions,” “application,” “steps,” and “programs” can be used interchangeably herein.
- the instructions can be stored in object code format for direct processing by a processor, or in any other computing device language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance.
- computing devices 210 may further include other components typically present in general purpose computing devices.
- computing devices 210 may include output devices, such as displays (e.g., a monitor having a screen, a touch-screen, a projector, a television, or another device that is operable to display information), speakers, haptics, etc.
- the computing devices 210 may also include user input devices, such as a mouse, keyboard, touch-screen, microphones, sensors, etc.
- FIGURE 2 functionally illustrates the processor, memory, and other elements of computing devices 210 as being within the same block
- the processor, computer computing device, or memory can actually comprise multiple processors, computers, computing devices, or memories that may or may not be stored within the same physical housing.
- the memory can be a hard drive or other storage media located in housings different from that of the computing devices 210.
- references to a processor, computer, computing device, or memory will be understood to include references to a collection of processors, computers, computing devices, or memories that may or may not operate in parallel.
- the computing devices 210 may include server computing devices operating as a load-balanced server farm, distributed system, etc.
- some functions described below are indicated as taking place on a single computing device having a single processor, various aspects of the subject matter described herein can be implemented by a plurality of computing devices, for example, communicating information over a network.
- the computing devices 210 may be capable of directly and indirectly communicating with other entities, such as computing devices 260, of a network through connection 250.
- Computing devices 210 and 260 may be interconnected using various protocols and systems, such that computing devices in the network can be part of the Internet, World Wide Web, specific intranets, wide area networks, or local networks.
- Computing devices in the network can utilize standard communication protocols, such as Ethernet, WiFi and HTTP, protocols that are proprietary to one or more companies, and various combinations of the foregoing.
- FIGURE 3 shows an example communication protocol system 300.
- the communication protocol system 300 may be implemented on two or more entities in a network, such as two or more of entities A, B, C of network 100 of FIGURE 1 , for example by network interface cards 252 and 292 of FIGURE 2 , as further described below with reference to at least FIGURE 7 .
- each entity may include multiple layers of communication protocols.
- entity A may include upper-layer protocol ("ULP") 310 and reliable transport (RT) protocol 330
- entity B may include upper-layer protocol 320 and reliable transport (RT) protocol layer 340.
- Peers may be formed between protocols of each layer.
- ULP 310 and ULP 320 are ULP peers
- reliable transport protocol layer 330 and reliable transport protocol layer 340 are RT peers.
- the upper-layer protocols are configured to communicate with the RT protocols.
- the upper-layer protocols 310, 320 may be responsible for implementing the hardware/software interface, processing of messages, completion notifications, and/or end-to-end flow control.
- the upper-layer protocols may be implemented on any of a number of hardware or software devices.
- the upper-layer protocols may be implemented as Remote Direct Memory Access ("RDMA") operation.
- RDMA Remote Direct Memory Access
- NVMe NonVolatile Memory Express
- the RT protocols 330, 340 may be responsible for reliable delivery of packets, congestion control, admission control, and/or ordered or unordered delivery of packets.
- Each RT protocol 330, 340 may logically be partitioned into two sublayers of protocols.
- reliable transport protocol layer 330 is partitioned into a solicitation sublayer 332 that is responsible for end-point admission control and optionally ordered delivery of packets, and a sliding window sublayer 334 that is responsible for end-to-end reliable delivery and congestion control.
- the reliable transport protocol layer 340 is also divided into a solicitation sublayer 342 and a sliding window sublayer 344.
- FIGURE 4 shows example sliding windows 410 and 420.
- the sliding windows 410 and 420 are used by entities to keep track of a predetermined number of packets to be transmitted and acknowledged over a connection.
- entity A may use the TX sliding window 410 for keeping track of packets sent to entity B over the connection 110.
- Entity B may use the RX sliding window 420 for keeping track of packets received from entity B.
- delays may occur between the TX sliding window 410 and RX sliding window 420 due to network latency. As a result, the TX sliding window 410 and RX sliding 420 window may go out-of-sync temporarily as the network out-of-order and/or loss.
- the sliding windows 410 and 420 may be respectively implemented in the sliding window sublayer 334 that is part of the reliable transport protocol layer 330 of FIGURE 3 .
- the TX sliding window and the RX sliding window may have different sizes as shown, or may alternatively have the same size.
- each packet is assigned a Packet Sequence Number ("PSN") by the sender entity A. As shown, the bit number increases from left to right.
- the receiver entity B may acknowledge the packets it has received within the sliding window by communicating to the sender entity A the PSN it has received within the window in an acknowledgment packet.
- a Sequence Number Bitmap (SNB) may be provided on both the sender entity A and the receiver entity B.
- SNB Sequence Number Bitmap
- Each bit of the Sequence Number Bitmap (SNB) represents one packet within a sliding window at the entity. For example, for the TX sliding window 410, a bit is set to 1 if a sent packet has been acknowledged. Otherwise the bit is 0.
- the sender entity A may move the sliding window 410 forward to the next set of packets to be transmitted.
- the sliding window moves forward once the base sequence number (BSN) packet is acknowledged.
- BSN base sequence number
- PSN for the sender entity may include Base Sequence Number ("BSN") and Next Sequence Number (“NSN").
- BSN is the PSN value of the oldest packet that is yet to be acknowledged by the receiver entity B.
- Bit 0 represents a PSN value of BSN
- Bit n represents a PSN value of (BSN+n).
- the receiver entity may also keep one or more sliding windows.
- an RX sliding window may be kept by receiver entity B for the packets received, where each bit represents a packet to be received with the sliding window. The bit is set to 1 if the packet has been received by the receiver entity B. Otherwise the bit is 0.
- the receiver entity B may also use PSN to keep track of received packets.
- BSN may be the PSN value of the oldest packet that is yet to be received by the receiver entity.
- the update of the BSN may clear the bits in the Sequence Number Bitmap corresponding to packets from the previous BSN to the PSN.
- Bit 0 represents a PSN value of BSN
- Bit n represents a PSN value of (BSN+n). Because sender entity A does not acknowledge the acknowledgments sent by receiver entity B, that is, PSN is not used for the acknowledgment packets, the receiver entity B need not keep a TX sliding window for the acknowledgments it sends.
- the sender entity and receiver entity may handle the packets and the respective acknowledgments according to a set of rules. For instance, if the receiver BSN in a received packet is smaller than the sender entity's BSN, the sender entity discards the ACK information; otherwise, the sender entity updates its BSN to match the receiver entity's BSN. After adjusting its BSN, the sender entity applies an OR operation on the receiver entity's Sequence Number Bitmap in the ACK packet with its own Sequence Number Bitmap. After a packet is transmitted, it is buffered by the sender entity until it is acknowledged by the receiver entity. Further, upon per packet retransmit timer expiry, the sender entity retransmits the packet with the same PSN as the original packet and increment a retransmission counter for that packet.
- the receiver entity may also implement a number of rules. For instance, if the PSN value of the received packet is less than the BSN of the received packet, the receiver entity discards the packet and sends an ACK packet with the current BSN. If the PSN value falls within the receiver entity's sliding window, the receiver entity updates the Sequence Number Bitmap by setting the bit at location (PSN-BSN) to 1. If the bit at location (PSN-BSN) was already 1, the packet is discarded; otherwise, the packet is delivered to the ULP of the receiver entity, and a cumulative ACK counter is incremented. If the PSN of the received packet is equal to the BSN of the received packet, the receiver entity updates the BSN to be equal to the next highest PSN that has not been received.
- PSN-BSN Sequence Number Bitmap
- the sliding windows are configured to allow the entities to keep track of packets received and/or acknowledged out-of-order within the respective sliding window.
- packets represented by bits 3 and 4 may be sent by entity A before the packets represented by bits 0, 1, and 2, the packets represented by bits 3 and 4 may be received and/or acknowledged before the packets represented by bits 0, 1, 2 in the TX sliding window 410.
- Network congestion may be detected by monitoring packet retransmission and/or packet round-trip latencies.
- the size of the one or more sliding windows may be adjusted. For example, if congestion is high, it may take longer for all packets within the TX sliding window 410 to be received and/or acknowledged by entity B. As such, to reduce congestion, the number of outstanding packets in the network may be reduced by decreasing the size of the sliding window 410.
- the retransmission timer expiry value in response to network congestion status may be adjusted. For example, retransmitting less frequently might reduce network congestion.
- the communication protocol system 300 of FIGURE 3 may support various transactions, including both pull and push transactions.
- the communication protocol system 300 of FIGURE 3 may be configured to perform the transactions using an initiator-target approach, where an "initiator" is the entity that requests a transaction, and a "target” is the entity that responds to the request.
- Such a transaction may involve multiple packets to be transmitted between the initiator and target entities, thus the initiator and the target entities may be both sender and receiver of packets in the transaction, and may keep track of packets and/or acknowledgments using TX and/or RX sliding windows as described with reference to FIGURE 4 .
- FIGURE 5 shows an example timing diagram for a push transaction according to aspects of the technology.
- pull transaction may be similarly constructed without the push grant (PushGrnt) feedback.
- the push transaction depicted in Figure 5 is a solicitated push request.
- Other types of push transactions, such as unsolicited push requests, may be similarly constructed without the push grant (PushGrnt) feedback as needed.
- the example timing diagrams of FIGURE 5 may be implemented by two entities in a network, such as entities A and B over connection 110 of FIGURE 1 , for example by processors 220 and 270 of FIGURE 2 .
- FIGURE 5 shows a timing diagram 500 for a push request, such as a solicited push transaction.
- the push transaction is performed by various communication protocol layers of both the initiator entity and the target entity.
- entity A may be the initiator entity and initiator ULP 510 and initiator RT 530 may be communication protocol layers configured as upper-layer protocol 310 and reliable transport protocol layer 330 of FIGURE 3
- entity B may be the target entity and target ULP 520 and target RT 540 may be communication protocol layers configured as upper-layer 320 and reliable transport protocol layer 340 of FIGURE 3 .
- a push request (“pushReq”) may originate from the initiator entity A at the initiator ULP 510, which may be sent to the initiator RT 530.
- the initiator RT 530 only sends a request to the target entity B, for instance over the connection 110, which may or may not be granted by the target entity B.
- This request and grant process or "solicitation" process may be performed by the respective RTs, which for example may be performed by their respective solicitation sublayers.
- the initiator RT 530 is shown sending a push request (“pushSlctdReq") to the target RT 540, and the target RT 540 may decide whether and/or when to grant the pushSlctdReq.
- entity B may limit the total number of outstanding granted pushSlctdData to prevent incast to entity B that causes congestion in the network. If and when the target RT 540 grants the request, the target RT 540 may send a push grant ("pushGrnt") back to the initiator RT 530. Once the initiator RT 530 receives the pushGrnt, the initiator entity A may then push solicited data ("pushSlctdData”) onto the target entity B, for instance over the connection 110. This may be performed by the respective RTs, thus the initiator RT 530 is shown pushing solicited data (“pushslctdData”) to the target RT 540.
- the data here is effectively solicited by the pushGrnt from the target RT 540.
- the target RT 540 may request for the received data to be placed or stored at the target entity B and does so by sending a pushReq to the target ULP 520.
- the target ULP 520 may place or store the received data, and then sends an acknowledgment message ("ULP-ACK") to the target RT 540 acknowledging that the received data has been placed or stored according to the pushReq.
- ULP-ACK acknowledgment message
- the target entity B sends an acknowledgment message ("ACK") to notify initiator entity A of the receipt and placement of the pushed data, for instance over the connection 110.
- the target RT 540 sends the ACK message to the initiator RT 530.
- the initiator RT 530 may send a push complete message (“pushCmpl") to initiator ULP 510 to notify that the data packet has been received and placed by the target entity.
- pushCmpl push complete message
- the initiator entity A and the target entity B may communicate with each other by transmitting packets.
- the pushSlctdReq, pushGrnt, pushslctdData and ACK may each be a packet transmitted over the connection 110.
- reliable transport packets may be tracked by sliding windows.
- the pushslctdData packet may be part of a data TX sliding window kept by entity A (indicated by dash-dot line).
- the pushGrnt packet may be part of a data TX sliding window kept by entity B (indicated by dash dot line), and the pushSlctdData packet may be part of a data TX sliding window kept by entity A (indicated by dash-dot line).
- the ACK packet sent by entity B may reference the PSN of the pushSlctdData, which entity A may keep track of using the data TX sliding window.
- entity A may send ACK for the pushGrnt packet, which entity B may keep track of using its data TX sliding window
- entity B may send ACK for the pushSlctdReq, which entity A may keep track of using its request TX sliding window.
- acknowledgment messages such as the ACK packet shown (indicated by dot line) are not reliable transport packets, and thus may not be part of any sliding window at the sender entity B.
- the solicited push transaction allows an initiator entity to solicit a grant to send data from a target entity before actually sending the data.
- the target entity may have control over the incoming data, which may be especially helpful when multiple initiator entities are attempting to push data onto the target entity, and also if the pushed data is large or if the network is congested. Since incast congestion may be caused by packets not being delivered to the receiver as fast as transmitted, and/or by multiple entities attempting to send packets simultaneously to the same entity, such a solicitation process may reduce incast congestion.
- an unsolicited push request does not require a pushGrnt from the target entity to send the push unsolicited data.
- a pull request does not need a pull grant from target RT.
- the target RT may then send the pull request to the target ULP to request permission.
- the target ULP may then send an acknowledgment message ("ULP-ACK”) to the target RT acknowledging the pull request, as well as a pull response (“pullResp”) instructing the target RT to pull the requested data.
- the target RT may pull the requested data ("pullData"), and send the pulled data to the initiator RT, for instance over the connection 110.
- the initiator RT may send a pullResp to the initiator ULP so that the initiator ULP may place or store the received data packet.
- the communication protocol system may be configured to perform both of the solicited push transactions and/or the unsolicited push transactions.
- the system may be configured to determine which push transaction to use based on one or more factors. For instance, whether a push request should be sent as a solicited or unsolicited request may be determined by the initiator RT.
- the initiator RT may determine whether to send a solicited push request or an unsolicited push based on a length of the push request from the initiator ULP.
- a solicited push request may be used to make sure that the large request does not cause congestion; otherwise, an unsolicited push may be used.
- whether to use solicited request or unsolicited push may be based on network conditions, such as level of congestion, where a solicited request may be used when congestion meets a predetermined threshold level.
- FIGURE 6 shows an example timing diagram for ordered transactions over a connection according to aspects of the technology.
- the example timing diagrams of FIGURE 6 may be implemented by two entities in a network, such as entities A and B over connection 110 of FIGURE 1 , for example by processors 220 and 270 of FIGURE 2 .
- various transactions may be performed by various communication protocol layers of both the initiator entity and the target entity.
- entity A may be the initiator entity and initiator ULP 610 and initiator RT 630 may be communication protocol layers configured as upper-layer protocol 610 and reliable transport protocol layer 630 of FIGURE 6
- entity B may be the target entity and target ULP 620 and target RT 640 may be communication protocol layers configured as upper-layer protocol 620 and reliable transport protocol layer 640 of FIGURE 6 .
- a number of requests may originate from the initiator entity A, including pull requests such as pullReq_1, shown as communication path 602, and push requests such as pushReq_0, pushReq_2, and pushReq_3, shown as communication paths 601, 603, 604. As described above, these requests may be sent by the initiator ULP 610 to the initiator RT 630. Once the initiator RT 630 receives these requests, initiator RT 630 may optionally determine whether the push requests should be sent as solicited or unsolicited. Thus, in this example, the initiator RT 630 may determine that pushReq_0 and pushReq_2 are to be sent as solicited, while pushReq_3 is to be sent as unsolicited. The initiator RT 630 may then send these pull and push requests to the target RT 640, for example over the connection 110.
- pull requests such as pullReq_1, shown as communication path 602
- push requests such as pushReq_0, pushReq_2, and pushReq_3, shown as communication paths 601,
- the requests may be sent by the initiator ULP 610 in a particular order as indicated by the Request Sequence Numbers ("RSN"), which may be assigned by the initiator RT 630, so as to track the transaction orders.
- RSN Request Sequence Numbers
- the initiator RT 630 may also assign Solicited Sequence Numbers ("SSN") specifically to solicited push requests, which may be an incremental number as shown.
- SSN Solicited Sequence Numbers
- the requests may be assigned with a sequence of numbers in ascending order according to the order of the RSN.
- the requests may be assigned PSNs within one or more TX sliding windows maintained by initiator entity A according to the RSNs.
- pushSlctdReq_0 shown as a communication path 611
- pullReq_1 shown as a communication path 614
- PSN 1
- pushSlctdReq_2 shown as a communication path 612
- pushReq_3 from the initiator ULP 610 does not require solicitation, there is no corresponding pushUnslctdReq being sent between the RTs. While RSNs and SSNs may be known to the ULPs, the PSNs may be unknown to the ULPs but only used by the RTs in packets.
- push grants may be sent by the target RT 640 to the initiator RT 630 in the order of the received requests, such as pushGnt_0 and pushGnt_2, shown as communication paths 605, 606 respectively.
- the push grants may not be received in the same order by the initiator RT 630 as the order of transmission for the push requests.
- pushGrnt_2 is received by the initiator RT 630 before the pushGmt_0.
- the reorder engine 256, 296 may assist reassembling the order of the requests prior to sending to ULPs.
- the initiator RT 630 may determine the correct order of the push grants based on their respective RSNs and push the data packets based on that order. Such order may be determined by performing a look-up operation in the reorder engine to determine the correct order. As such, although pushGrnt_2 was received by the initiator RT 630 before pushGrnt_0, the initiator RT 630 may first push the data solicited by pushGrnt_0 with pushSlctdData_0 and then push the data solicited by pushGrnt_2 with pushSlctdData_2 to target RT 640.
- the pushed data packets are also assigned PSNs in ascending order within one or more TX sliding windows maintained by initiator entity A according to the order of transmission.
- the pushReq_3 does not require a grant, thus as indicated by the curved arrow 655 skips directly to pushUnslctdData_3, shown as the communication path 613, which pushes the unsolicited data.
- Target RT 640 receives the requests, and then sends corresponding requests to the target ULP 620 in the order of ULP-Req-0-1-2-3, which is in the same order as the transmission order of the requests from the initiator ULP 610 shown at the top of the timing diagram 800.
- These ULP-Reqs ask the target ULP 620 for permission to pull data, or to place the pushed data at the target entity B. Note that the pull request pullReq_1 does not require a grant as described, thus as indicated by the curved arrow 656 skips directly to the ULP-Req.
- the target ULP 620 may send acknowledgment ULP-ACKs to the target RT 640. In this ordered system, the ULP-ACKs are sent in the order of ULP-ACK-0-1-2-3, which is the same as the order of transmission for the requests from the initiator ULP 610.
- ACKs acknowledging the data packets are then sent by target RT 640 to initiator RT 630 to notify the safe receipt and placement of the reliable transport data packets.
- initiator RT 630 may send a completion message pushCompl_0 to initiator ULP 610.
- acknowledgment packets may be opportunistically piggybacked on other reliable transport packets.
- the requests pushSlctdReq_0, pullReq_1, and pushSlctdReq_2 are reliable transport packets requiring an ACK, but these acknowledgments to requests (or request ACKs) are not explicitly shown in timing diagram 800 because they may be piggybacked on reliable transport packets such as pushGrnt_0 and pushGrnt_2.
- pull requests may also be responded to.
- the target ULP 620 may send a pullResp_1 instructing target RT 640 to pull the requested data.
- Target RT 640 then sends the pulled data to the initiator RT 630 with pullData_1.
- the initiator RT 630 then sends a pullResp_1, shown as the communication path 657, to the initiator ULP 610 so that the initiator ULP 610 may place or store the received data packet at entity A.
- an acknowledgment may be sent to notify entity B of safe receipt.
- the completion messages received by the initiator ULP 610 near the bottom of timing diagram 600 are in the same order as the requests that were sent by initiator ULP 610 near the top of the timing diagram 600.
- This order is maintained on ULPs of both initiator and target entities, where the target RT presents requests to the target ULP in the same order as the initiator ULP sends requests to the initiator RT.
- This ordered system ensures that the requests are delivered once and only once over the connection. In contrast, there may not be an ordering requirement between transactions going in different directions over the connection.
- the requests transmitted between the initiator RT and the target RT may be out of order due to different paths or other factors such as unexpected delay or equipment failure, the requests may arrive at the target RT out of sequence or out of order, or the request may be dropped or corrupted. This may become problematic when the target RT is configured to receive the requests in the same order as transmitted from the initiator RT.
- FIGURE 7 depicts an ASIC 700, for use on a network interface card (not shown), with an integrated reliable transport accelerator (RTA) 710 configured to control data path operations and connection contexts.
- RTA integrated reliable transport accelerator
- the RTA 710 interfaces with upstream Upper-Layer Protocol (ULP) 702 and downstream packet process pipeline 703 for managing or otherwise controlling data path operations.
- ULP Upper-Layer Protocol
- FIG. 7 illustrates the RTA 710, ULP 702, and packet process pipeline 703 as individual accelerators. However, in some instances the RTA 710, ULP 702, and/or packet process pipeline 703 may be any number accelerators.
- the RTA further communicates with memory through a Network-on-Chip (NoC) 704 for managing connection contexts within on-chip or off-chip memory.
- NoC Network-on-Chip
- the network interface card supports external memory, referred to herein as off-chip memory 720.
- the network interface card also supports on-chip memory, referred to herein as system-level cache (SLC) 706. Both SLC 706 and off-chip memory 720 can be configured as user memory.
- the RTA 710 may communicate with the SLC 706 through the NoC 704. Alternatively, or simultaneously, the RTA 710 may communicate with off-chip memory 720 through the NoC 704 and a memory controller 708.
- off-chip memory 720 is memory that is positioned off of the ASIC 700, such as on the same network card (not shown) as the ASIC. Although off-chip memory 720 is illustrated as DRAM in FIGURE 7 , any type of memory may be used.
- RT connection contexts can be stored either in the SLC 706 as a cache entry or in off-chip memory 720.
- the memory subsystem on the ASIC 700 may implement a relaxed memory model which does not guarantee ordering of load/store to the same memory address.
- the memory subsystem may be an ARM memory subsystem.
- a "connection context" may include sliding windows for packet reliability, transaction ordering information, security protection, and congestion control.
- the RTA 710 is partitioned into four main units including a Rate Update (RUE) & NoC Interface 711, Connection Context Pipeline (CTX) 712, Retransmission Engine (TX) 713, and Reorder Engine (RX) 714.
- RUE Rate Update
- CTX Connection Context Pipeline
- TX Retransmission Engine
- RX Reorder Engine
- Each of the units 711, 712, 713, and 714 may be accelerators comprised of one or more accelerators capable of providing the functionalities described herein.
- the RTA 710 may be comprised of one accelerator capable of providing all of the functionalities of each of the four units, 711-714.
- the accelerator(s) may each be comprised of one or more processors capable of providing particular acceleration functionality.
- the CTX 712 is configured to provide connection context cache management.
- the CTX 712 may also implement packet admission controls based on connection receiving sliding window state and security configurations. As illustrated in FIGURE 7 , the CTX 712 may receive packets from the ULP directly, as illustrated by arrow 771. Similarly, the CTX 712 may receive packets from the network through the ingress packet process pipeline 703, as illustrated by arrow 772.
- the retransmission engine (TX) 713 may receive packets from the CTX 712, as illustrated by arrow 775.
- the TX 713 may transmit the packets to the network through the egress packet process pipeline 703, based on a per-connection packet format configuration, as illustrated by arrow 773.
- the TX 713 may implement packet reliability and serve as the main retransmission data buffer.
- the Reorder Engine (RX) 714 may utilize per connection ordering states to reorder transactions.
- the RX 714 may send packet payload and transaction completion data to ULP 702, as illustrated by arrow 774.
- RX 714 may also serve as the main receiver data buffer from the CTX, as illustrated by arrow 779.
- the Rate Update Engine (RUE) & NoC Interface 711 supports reliable transport congestion control (CC) and at the same time implements the NoC interfaces for the internal cache management pipeline in CTX 712, described herein.
- FIGURE 8 illustrates a detailed illustration of the flow of data through the CTX 712.
- Packets including variable packet data, illustrated as ULP tx-intf 804, may be received from ULP 702, as illustrated by arrow 891.
- the CTX 712 may parse the variable packet data and extract a fixed format command through the internal cache pipeline, illustrated by dashed box 880.
- any event such as a packet received from the network, a transaction received from the ULP 702, or an internal cache management event may be formatted into a "command".
- Each packet received may be of a different type, for example, pull request packets, push data packets, etc. Each of these packets may be mapped/converted into a fixed format command for internal processing.
- the packet data associated with the command is directly sent to TX data buffers, as illustrated by line 893.
- the direct transmission of the command to the TX data buffers may eliminate the need for a large data buffer in the CTX pipeline 880.
- connection ID (CID) of the packets
- CID Lookup 811 a cache entry lookup key within a connection identifier lookup database, illustrated by CID Lookup 811, for existing connections in the cache, stored in the CID Lookup 811.
- the RUE & NoC interface 711 may communicate with the NoC 704, as illustrated by arrow 777, which in turn may communicate with SLC 706.
- CTX 712 may allocate a free cache entry within the CID Lookup 811 to the connection and issue a memory read to attempt and fetch the connection, and the corresponding connection context data from off-chip memory 720 (or on-chip memory 706) through the NOC 704.
- the memory controller 708 memory controller 708 may communicate with the off-chip memory 720, as illustrated by arrow 778.
- the current command may be deposited, by the command process 813, into a per-connection cache miss queue 810.
- the CTX 712 may install the connection context data into a cache entry within the on-chip cache of CID Lookup 811.
- Command entries associated with the newly installed connection may become eligible for dequeue from the miss queue 810.
- the command entries may go through the internal cache pipeline 880 for a second-pass process, as further illustrated in Figure 8 .
- the command process 813 of CTX 712 may enqueue all ULP commands to the connection scheduler 812.
- the connection scheduler 812 may schedule transmission to the network based on per connection congestion control.
- the command process 813 of CTX 712 may also pass control information regarding the command to TX 713 as illustrated by arrow 823, and transaction completion information to RX 714, as illustrated by arrow 824.
- miss queue 810 In case of a cache hit for a particular CID associated with a connection, and when no pending commands associated with the same CID is within the miss queue 810, the command is processed immediately without going through the miss queue 810. For ordering purposes, some or all later ULP transactions associated with a CID may also go through the miss queue 813 if there are one or more commands in the miss queue 813 for the same CID.
- the CTX 712 may similarly process packets received from the network, which are subjected to additional admission controls including per connection receiving sliding window and security checks, as described herein.
- data associated with data packets illustrated as pkt-RX 802 may be received from the network, as illustrated by arrow 890.
- the data associated with data packets 802 may be are stored directly in RX 714, as illustrated by arrow 892.
- packets received from the network 802 may trigger congestion control events to the RUE and NoC Interface, as illustrated by arrow 894.
- ordering may not be strictly required, hence they may bypass cache miss queue 810 as long as cache is hit during the CID lookup 811.
- the cache policy 814 may choose and evict connection context entries from the cache, stored on CID Lookup 811, when cache occupancy goes above a configured threshold.
- the threshold may be preprogrammed or configured by a user or application.
- RT 714 may implement both random and serial eviction (e.g., first in, first out (FIFO)), based on the cache index.
- FIFO first in, first out
- Connection contexts may be eligible for eviction from the cache store in the SLC 706 when they become idle with pending transaction counts equal to 0.
- TX 713 may decrement resource counters to CTX 712 when packets in the retransmission engine 713 are acknowledged by a remote node.
- the reorder engine 714 may decrement transaction counters to CTX 712 when a transaction has been completed to or acknowledged by local ULP.
- the transaction counters may be decremented by TX when a transmitted packet has been acknowledged and hence deleted from the retransmission buffer.
- the transaction counters may be decremented by RX when a transaction is completed to ULP and hence deleted from the reorder buffer.
- the CTX cache pipeline 880 may serialize access to the same CID to avoid read/write hazards in the relaxed memory model.
- HW hardware
- SW software
- RT may provide a SW context flush interface through Control/Status Register (CSR) 816, as further illustrated in FIGURE 8 .
- the SW may trigger one connection context update through the flush control CSR.
- HW may then write the content through to the main memory where all the connection contexts may be stored, including the off-chip memory 720 and on-chip memory (SLC 706).
- a user may define which main memory the connection contexts should be stored.
- HW may search the on-chip cache (CID Lookup 811), for the CID, and, if found, update content in the cache.
- SW polls the flush status CSR for HW completion status of previous flush operation and readiness for next operation.
- FIGURE 9 illustrates a cache entry finite state machine (FSM) 900.
- the FSM includes four states: Invalid 902, Allocated 904, Valid 906, and Evicting 908.
- Commands received from the network or ULP are processed as cache hit when the CID associated with the commands match a CID associated with cache entry, storing connection context data, in a Valid state 906.
- Each cache entry may start as Invalid 902.
- HW may allocate a cache entry within the CID Lookup 811 for the connection context, as identified by the CID in the command.
- the HW e.g., RTA 710,
- RTA 710 may also issue a memory read for the connection context.
- the cache entry for the connection context may transition into the Allocated state 904.
- connection context in the Allocated state 904 may be blocked. By blocking access to connection contexts in the Allocated state 904, memory access hazards may be avoided. As a result, any SW flush or RUE result write is stalled in this state.
- CTX cache pipeline 880 may attempt an eviction of the cache entry by writing the content to off-chip memory 720 and transitioning the cache entry into the Evicting state 908. Similarly, all other access to the same connection context is stalled and serialized to prevent multiple outstanding read/write to the same memory address. If more ULP or network commands directed to a connection context in the Evicting state 908, the cache entry for the connection context in the Evicting state 908 may transition back to the Allocated state 904, so that the connection context data can be retrieved. Otherwise, when all write completions are returned from the off-chip memory 720, the cache entry transition to the Invalid state and the on-chip resources associated with that cache entry may be freed up.
- RTA 710 may support a mode where SW flush is able to install a valid cache entry directly even from an initial state of Invalid 902.
- the cache CID lookup process is a multi-clock-cycle event, and the command processing itself takes multiple cycles. As such, multiple hazard conditions are presented during cache entry state transitions. For RTA 710, the lookup may involve two memory accesses and a total of 6-cycle latency. Additionally, command processing may take 2 cycles. HW hazard condition avoidance is handled in two separate stages. Firstly, the 6 cycle memory lookup pipeline implements bypass logic, such that all lookups always return the latest result after all prior modifications. This tight loop includes a non-existent cache entry (in Invalid state) transiting to Allocated or Evicting state that involves free entry allocation. Secondly, HW detects and stalls access to the same CID to be every other cycle since bypass does not address the 2-cycle cache entry state modification latency.
- the HW may stall access to the same connection for 6 cycles after a memory-write completion is being processed, which is the only type of operation that might lead to entry deallocation.
- the performance impact of such stalling is negligible since memory-write completion is not a common case command.
Abstract
Description
- The present application claims the benefit of the filing date of
U.S. Provisional Patent Application No. 63/239,603 filed September 1, 2021 - The Internet protocol suite is a set of communication protocols used for servicing data transmissions between two devices communicating information over the Internet or other computer networks. Transmission Control Protocol ("TCP") is a part of the Internet protocol suite that provides for connection-oriented, reliable, and ordered delivery of a stream of data packets between, for example, a web-browser application running on a client device and a web-server application running on a server device over a local or wide area network. Datacenters using communication protocols such as TCP encounter certain issues. For instance, incast is a many-to-one communication pattern commonly found in datacenters, which may result in incast congestion when multiple synchronized computing devices send data to a same receiver computing device in parallel. Further, because TCP protocols require ordered delivery of packets over a connection, a long tail latency, which is the amount of time for the last few packets among a series of packets to be transmitted, may prevent transmission of the next series of packets.
- To address these issues, alternative reliable transport protocols have been developed as communication protocols. Reliable transport protocols rely on tracking data according to an identifier and maintaining more active connections in a cache for faster processing. Less active, or inactive connections, may be stored off cache. However, given that there may be millions of connections, processing data and managing the connections may require significant processing power.
- The present disclosure provides a network interface card configured to track active and stored connection contexts within on-chip and off-chip memory. One aspect of the technology is directed to an application specific integrated circuit (ASIC), including a reliable transport accelerator (RTA), the RTA including a cache lookup database. The RTA may be configured to: determine, from a received data packet, a connection identifier; query the cache lookup database for a cache entry corresponding to a connection context having the connection identifier; and receive, in response to the query, a cache hit or a cache miss.
- In some instances, the ASIC further includes on-chip memory storing one or more connection contexts associated with one or more connection identifiers.
- In some instances, the ASIC is attached to a network interface card.
- In some examples, the network interface card includes off-chip memory storing one or more connection contexts associated with one or more connection identifiers, wherein the off-chip memory is connected to the ASIC.
- In some examples, a cache miss indicates that the connection context having the connection identifier is not stored in the cache lookup database.
- In some instances, after receiving a cache miss, the RTA retrieves the connection context from the on-chip memory or the off-chip memory.
- In some examples, the RTA stores the connection context in the on-chip memory or the off-chip memory based on a predefined user configuration.
- In some instances, a cache hit indicates that the connection context having the connection identifier is stored in the cache lookup database.
- In some examples, a cache policy is stored by the cache lookup database, wherein the cache policy defines when one or more connection contexts are evicted from the cache lookup database.
- In some instances, evicted connection contexts are moved to on-chip or off-chip memory.
- In some examples, the system includes an upper-layer protocol accelerator (ULP) and a packet process pipeline accelerator.
- Another aspect of the disclosure is directed to a method for managing connection contexts. The method may include determining, by a reliable transport accelerator (RTA) including a cache lookup database, a connection identifier for a received data packet; querying, by the RTA, the cache lookup database for a cache entry corresponding to a connection context having the connection identifier; and receiving, from the cache lookup database, in response to the query, a cache hit or a cache miss.
- In some instances, a cache miss indicates that the connection context having the connection identifier is not stored in the cache lookup database.
- In some examples, after receiving a cache miss, the RTA retrieves the connection context from on-chip memory or off-chip memory.
- In some examples, the RTA stores the connection context in the on-chip memory or the off-chip memory based on a predefined user configuration.
- In some instances, a cache hit indicates that the connection context having the connection identifier is stored in the cache lookup database.
- In some instances, the RTA further includes a cache policy stored by the cache lookup database, wherein the cache policy defines when one or more connection contexts are evicted from the cache lookup database.
- In some examples, evicted connection contexts are moved to on-chip or off-chip memory.
- In some instances, the on-chip memory is located on an ASIC with the RTA.
- In some examples, the off-chip memory is located off the ASIC.
-
-
FIGURE 1 is a schematic diagram of a network according to aspects of the technology. -
FIGURE 2 is a block diagram of an example system according to aspects of the technology. -
FIGURE 3 is a block diagram of communication layers according to aspects of the technology. -
FIGURE 4 illustrates an example sliding window according to aspects of the technology. -
FIGURE 5 is an example timing diagram of solicited push transaction according to aspects of the technology. -
FIGURE 6 is an example timing diagram of transactions according to aspects of the technology. -
FIGURE 7 is a block diagram of an example network interface card architecture according to aspects of the technology. -
FIGURE 8 is a block diagram of a connection context pipeline architecture according to aspects of the technology. -
FIGURE 9 is an example finite state machine illustrating the possible states for a cache entry and how transitions between states may occur, according to aspects of the technology. - The technology generally relates to a network interface card configured to track active and stored connection contexts within on-chip and off-chip memory. The particular Reliable Transport protocol, described herein, referred to as "RT," employs a connection-oriented architecture that provides reliable packet delivery over a lossy and out-of-order network. Every reliable transport packet may be associated with a connection. The states of each connection may be tracked in a connection context, which includes sliding windows for packet reliability, transaction ordering information, security protection and congestion control, etc. The network interface card described herein may support thousands of active connections within an on-chip cache and a million, or more, connections stored in off-chip memory.
- As used herein the term "connection context" refers to the state of a connection, which may include sliding windows for packet reliability, transaction ordering information, security protection, and congestion control. Other information about a connection may also be included in a connection context.
-
FIGURE 1 shows an example network 100. The network 100 includes various entities, such as entity A, entity B, and entity C. In order to communicate with one another, connections are formed between the entities, such asconnection 110 between entities A and B, andconnection 120 between entities A and C. The entities may communicate over the connections using one or more protocols. For example, RT is a protocol that notifies the sender whether or not the delivery of data to an intended receiver was successful. A sender and a receiver are considered peers of a communication protocol, thus entities A and B may be reliable transport peers, and entities A and C may be reliable transport peers. A connection over which RT is used is an end-to-end construct that describes a bidirectional communication channel between two reliable transport peers. - A connection may be identified by a pair of Connection IDs ("CIDs"), one in each direction of communication. CIDs may be allocated by a receiver entity during the connection setup process and have no global significance outside of the parties involved. Thus, the
connection 110 between entities A and B may have a CID withvalue 5 for the direction from A to B, and a CID withvalue 10 for the direction from B to A. Theconnection 120 between entities A and C may have aCID value 5 for the direction from A to C and a CID withvalue 11 for the direction from C to A. Further, CIDs assigned by an entity or "Source CIDs" of an entity must have different values. Thus in the example shown, the CIDs assigned by entity A or Source CIDs of entity A havedifferent values same value 5. - Packets may be transmitted over the connections between the entities. In this regard, a packet is a basic unit of communication across a connection. A packet may have a predetermined size, for example up to a maximum transfer unit ("MTU") in length. A packet may have a header including information about the packet and its transmission, and a payload of data. To ensure reliable transport, a reliable transport packet may include the Destination CID, such as in a header. For example, when entity B receives a packet over the
connection 110 with the Destination CID of 5, entity B may identify the packet as coming from entity A, and may then notify A that the packet has been received by sending an acknowledgment over theconnection 110 referencing this packet and its CID of 5. The acknowledgment itself may be sent as a packet including the Destination CID of 10. - Entities A, B, and C may be any type of device capable of communicating over a network, such as personal computing devices, server computing devices, mobile devices, wearable devices, virtual machines, etc.
FIGURE 2 is a block diagram of some components in anexample system 200 that can communicate using RT protocol. Thesystem 200 includes at least two entities having one or more connections between them. It should not be considered as limiting the scope of the disclosure or usefulness of the features described herein. In this example, thesystem 200 is shown with two entities, one ormore computing devices 210 and one ormore computing devices 260, with aconnection 250 between them. For example,computing devices 210 may be entity A and computing devices may be entity B ofFIGURE 1 , andconnection 250 may beconnection 110 ofFIGURE 1 . Thecomputing devices computing devices 210 contain one or more processors 220,memory 230, and one or morenetwork interface cards 252. The one ormore processors 270, memory 280, and the one or morenetwork interface cards 292 ofcomputing device 260 may be configured similarly to one or more processors 220,memory 230, and one or morenetwork interface cards 252 ofcomputing devices 210. - The one or more processors 220 can be any conventional processor, such as a commercially available CPU. Alternatively, the processors can be dedicated components such as an application-specific integrated circuit ("ASIC") or other hardware-based processor. Although not necessary, one or more of the
computing devices 210 may include specialized hardware components to perform specific computing processes. - The
memory 230 can be of any non-transitory type capable of storing information accessible by the processor, such as a hard-drive, memory card, ROM, RAM, DRAM, DVD, CD-ROM, write-capable, and read-only memories.Memory 230 of thecomputing devices 210 can store information accessible by the one or more processors 220, includingdata 232 andinstructions 234. -
Memory 230 can includedata 232 that can be retrieved, manipulated or stored by the processors 220. For example, data such as communication protocols, connection information such as CIDs, definitions of headers, etc., as described herein with respect toFIGURE 1 andFIGURES 3-9 may be retrieved, manipulated, or stored by the processors 220. -
Memory 230 of thecomputing devices 210 can also storeinstructions 234 that can be executed by the one or more processors 220. For instance, instructions such as communication protocols as described with reference toFIGURES 1 and3-9 may be performed by the one or more processors 220 according toinstructions 234 anddata 232 inmemory 230. -
Data 232 may be retrieved, stored, or modified by the one or more processors 220 in accordance with theinstructions 234. For instance, although the subject matter described herein is not limited by any particular data structure, the data can be stored in computer registers, in a relational database as a table having many different fields and records, or XML documents. The data can also be formatted in any computing device-readable format such as, but not limited to, binary values, ASCII, or Unicode. Moreover, the data can comprise any information sufficient to identify the relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories such as at other network locations, or information that is used by a function to calculate the relevant data. - The
instructions 234 can be any set of instructions to be executed directly, such as machine code, or indirectly, such as scripts, by the one or more processors. In that regard, the terms "instructions," "application," "steps," and "programs" can be used interchangeably herein. The instructions can be stored in object code format for direct processing by a processor, or in any other computing device language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. - Although not shown,
computing devices 210 may further include other components typically present in general purpose computing devices. For example,computing devices 210 may include output devices, such as displays (e.g., a monitor having a screen, a touch-screen, a projector, a television, or another device that is operable to display information), speakers, haptics, etc. Thecomputing devices 210 may also include user input devices, such as a mouse, keyboard, touch-screen, microphones, sensors, etc. - Although
FIGURE 2 functionally illustrates the processor, memory, and other elements ofcomputing devices 210 as being within the same block, the processor, computer computing device, or memory can actually comprise multiple processors, computers, computing devices, or memories that may or may not be stored within the same physical housing. For example, the memory can be a hard drive or other storage media located in housings different from that of thecomputing devices 210. Accordingly, references to a processor, computer, computing device, or memory will be understood to include references to a collection of processors, computers, computing devices, or memories that may or may not operate in parallel. For example, thecomputing devices 210 may include server computing devices operating as a load-balanced server farm, distributed system, etc. Yet further, although some functions described below are indicated as taking place on a single computing device having a single processor, various aspects of the subject matter described herein can be implemented by a plurality of computing devices, for example, communicating information over a network. - The
computing devices 210 may be capable of directly and indirectly communicating with other entities, such ascomputing devices 260, of a network throughconnection 250. -
Computing devices - Referring to
FIGURE 1 , packets may be transmitted between the entities A, B, and/or C over the connections using one or more communication protocols.FIGURE 3 shows an example communication protocol system 300. The communication protocol system 300 may be implemented on two or more entities in a network, such as two or more of entities A, B, C of network 100 ofFIGURE 1 , for example bynetwork interface cards FIGURE 2 , as further described below with reference to at leastFIGURE 7 . As shown, each entity may include multiple layers of communication protocols. For example, entity A may include upper-layer protocol ("ULP") 310 and reliable transport (RT)protocol 330, and entity B may include upper-layer protocol 320 and reliable transport (RT)protocol layer 340. Peers may be formed between protocols of each layer. Thus,ULP 310 andULP 320 are ULP peers, and reliabletransport protocol layer 330 and reliabletransport protocol layer 340 are RT peers. Further as shown, within each entity, the upper-layer protocols are configured to communicate with the RT protocols. - As described with reference to
FIGURES 4 -12, the upper-layer protocols - Also described with reference to
FIGURES 4-9 , theRT protocols RT protocol transport protocol layer 330 is partitioned into asolicitation sublayer 332 that is responsible for end-point admission control and optionally ordered delivery of packets, and a slidingwindow sublayer 334 that is responsible for end-to-end reliable delivery and congestion control. Likewise, the reliabletransport protocol layer 340 is also divided into asolicitation sublayer 342 and a slidingwindow sublayer 344. -
FIGURE 4 showsexample sliding windows windows TX sliding window 410 for keeping track of packets sent to entity B over theconnection 110. Entity B may use theRX sliding window 420 for keeping track of packets received from entity B. In some examples, delays may occur between theTX sliding window 410 andRX sliding window 420 due to network latency. As a result, theTX sliding window 410 and RX sliding 420 window may go out-of-sync temporarily as the network out-of-order and/or loss. As such, the slidingwindows window sublayer 334 that is part of the reliabletransport protocol layer 330 ofFIGURE 3 . The TX sliding window and the RX sliding window may have different sizes as shown, or may alternatively have the same size. - Referring to the
TX sliding window 410, to keep track of the packets, each packet is assigned a Packet Sequence Number ("PSN") by the sender entity A. As shown, the bit number increases from left to right. The receiver entity B may acknowledge the packets it has received within the sliding window by communicating to the sender entity A the PSN it has received within the window in an acknowledgment packet. In this regard, a Sequence Number Bitmap (SNB) may be provided on both the sender entity A and the receiver entity B. Each bit of the Sequence Number Bitmap (SNB) represents one packet within a sliding window at the entity. For example, for theTX sliding window 410, a bit is set to 1 if a sent packet has been acknowledged. Otherwise the bit is 0. Once all packets within theTX sliding window 410 are received and acknowledged, the sender entity A may move the slidingwindow 410 forward to the next set of packets to be transmitted. The sliding window moves forward once the base sequence number (BSN) packet is acknowledged. Thus, referring to the example inFIGURE 4 , the sliding window moves by one once the left most 0 is marked, and moves by another one once the second-left-most 0 is marked, and by three once the third 0 is marked (since the two following bits are already set). - PSN for the sender entity may include Base Sequence Number ("BSN") and Next Sequence Number ("NSN"). As shown, BSN is the PSN value of the oldest packet that is yet to be acknowledged by the receiver entity B. Further as shown, NSN is the PSN value that should be assigned to the next packet transmitted over the connection to receiver entity B. For instance, when a packet is received from
ULP 310 for transmission, the current PSN may be updated to NSN. Then when the packet is transmitted over the connection, NSN may be incremented, for example with NSN = (NSN+1)mod 232. As such, within the slidingwindow 410,Bit 0 represents a PSN value of BSN, and Bit n represents a PSN value of (BSN+n). - Although not shown, the receiver entity may also keep one or more sliding windows. For example, an RX sliding window may be kept by receiver entity B for the packets received, where each bit represents a packet to be received with the sliding window. The bit is set to 1 if the packet has been received by the receiver entity B. Otherwise the bit is 0. The receiver entity B may also use PSN to keep track of received packets. For instance, BSN may be the PSN value of the oldest packet that is yet to be received by the receiver entity. When a packet is received with a PSN value of BSN, the BSN may be updated to the next lowest PSN of the packet that has not yet been received, for example with BSN = (BSN+1)
mod 232. The update of the BSN may clear the bits in the Sequence Number Bitmap corresponding to packets from the previous BSN to the PSN. As such, within the RX sliding window for the receiver entity B,Bit 0 represents a PSN value of BSN and Bit n represents a PSN value of (BSN+n). Because sender entity A does not acknowledge the acknowledgments sent by receiver entity B, that is, PSN is not used for the acknowledgment packets, the receiver entity B need not keep a TX sliding window for the acknowledgments it sends. - The sender entity and receiver entity may handle the packets and the respective acknowledgments according to a set of rules. For instance, if the receiver BSN in a received packet is smaller than the sender entity's BSN, the sender entity discards the ACK information; otherwise, the sender entity updates its BSN to match the receiver entity's BSN. After adjusting its BSN, the sender entity applies an OR operation on the receiver entity's Sequence Number Bitmap in the ACK packet with its own Sequence Number Bitmap. After a packet is transmitted, it is buffered by the sender entity until it is acknowledged by the receiver entity. Further, upon per packet retransmit timer expiry, the sender entity retransmits the packet with the same PSN as the original packet and increment a retransmission counter for that packet.
- The receiver entity may also implement a number of rules. For instance, if the PSN value of the received packet is less than the BSN of the received packet, the receiver entity discards the packet and sends an ACK packet with the current BSN. If the PSN value falls within the receiver entity's sliding window, the receiver entity updates the Sequence Number Bitmap by setting the bit at location (PSN-BSN) to 1. If the bit at location (PSN-BSN) was already 1, the packet is discarded; otherwise, the packet is delivered to the ULP of the receiver entity, and a cumulative ACK counter is incremented. If the PSN of the received packet is equal to the BSN of the received packet, the receiver entity updates the BSN to be equal to the next highest PSN that has not been received.
- Note that, because the packets are tracked according to bitmaps, the sliding windows are configured to allow the entities to keep track of packets received and/or acknowledged out-of-order within the respective sliding window. Thus as shown, although packets represented by
bits 3 and 4 may be sent by entity A before the packets represented bybits bits 3 and 4 may be received and/or acknowledged before the packets represented bybits TX sliding window 410. - Network congestion may be detected by monitoring packet retransmission and/or packet round-trip latencies. To perform congestion control, the size of the one or more sliding windows may be adjusted. For example, if congestion is high, it may take longer for all packets within the
TX sliding window 410 to be received and/or acknowledged by entity B. As such, to reduce congestion, the number of outstanding packets in the network may be reduced by decreasing the size of the slidingwindow 410. In addition to or as an alternative to changing the size of the sliding window, the retransmission timer expiry value in response to network congestion status may be adjusted. For example, retransmitting less frequently might reduce network congestion. - The communication protocol system 300 of
FIGURE 3 may support various transactions, including both pull and push transactions. The communication protocol system 300 ofFIGURE 3 may be configured to perform the transactions using an initiator-target approach, where an "initiator" is the entity that requests a transaction, and a "target" is the entity that responds to the request. Such a transaction may involve multiple packets to be transmitted between the initiator and target entities, thus the initiator and the target entities may be both sender and receiver of packets in the transaction, and may keep track of packets and/or acknowledgments using TX and/or RX sliding windows as described with reference toFIGURE 4 .FIGURE 5 shows an example timing diagram for a push transaction according to aspects of the technology. It is noted that pull transaction may be similarly constructed without the push grant (PushGrnt) feedback. The push transaction depicted inFigure 5 is a solicitated push request. Other types of push transactions, such as unsolicited push requests, may be similarly constructed without the push grant (PushGrnt) feedback as needed. The example timing diagrams ofFIGURE 5 may be implemented by two entities in a network, such as entities A and B overconnection 110 ofFIGURE 1 , for example byprocessors 220 and 270 ofFIGURE 2 . -
FIGURE 5 shows a timing diagram 500 for a push request, such as a solicited push transaction. As shown, the push transaction is performed by various communication protocol layers of both the initiator entity and the target entity. For example, entity A may be the initiator entity andinitiator ULP 510 andinitiator RT 530 may be communication protocol layers configured as upper-layer protocol 310 and reliabletransport protocol layer 330 ofFIGURE 3 , while entity B may be the target entity andtarget ULP 520 and targetRT 540 may be communication protocol layers configured as upper-layer 320 and reliabletransport protocol layer 340 ofFIGURE 3 . - As depicted in
FIGURE 5 , a push request ("pushReq") may originate from the initiator entity A at theinitiator ULP 510, which may be sent to theinitiator RT 530. At this stage, theinitiator RT 530 only sends a request to the target entity B, for instance over theconnection 110, which may or may not be granted by the target entity B. This request and grant process or "solicitation" process may be performed by the respective RTs, which for example may be performed by their respective solicitation sublayers. Thus, theinitiator RT 530 is shown sending a push request ("pushSlctdReq") to thetarget RT 540, and thetarget RT 540 may decide whether and/or when to grant the pushSlctdReq. In some examples, entity B may limit the total number of outstanding granted pushSlctdData to prevent incast to entity B that causes congestion in the network. If and when thetarget RT 540 grants the request, thetarget RT 540 may send a push grant ("pushGrnt") back to theinitiator RT 530. Once theinitiator RT 530 receives the pushGrnt, the initiator entity A may then push solicited data ("pushSlctdData") onto the target entity B, for instance over theconnection 110. This may be performed by the respective RTs, thus theinitiator RT 530 is shown pushing solicited data ("pushslctdData") to thetarget RT 540. The data here is effectively solicited by the pushGrnt from thetarget RT 540. Once the data is received by the target entity B, thetarget RT 540 may request for the received data to be placed or stored at the target entity B and does so by sending a pushReq to thetarget ULP 520. In response, thetarget ULP 520 may place or store the received data, and then sends an acknowledgment message ("ULP-ACK") to thetarget RT 540 acknowledging that the received data has been placed or stored according to the pushReq. For reliable transport, the target entity B sends an acknowledgment message ("ACK") to notify initiator entity A of the receipt and placement of the pushed data, for instance over theconnection 110. This is performed by the respective RTs, thus as shown thetarget RT 540 sends the ACK message to theinitiator RT 530. Once the ACK message is received by theinitiator RT 530, theinitiator RT 530 may send a push complete message ("pushCmpl") toinitiator ULP 510 to notify that the data packet has been received and placed by the target entity. - As described with reference to
FIGURE 1 , the initiator entity A and the target entity B may communicate with each other by transmitting packets. Thus, the pushSlctdReq, pushGrnt, pushslctdData and ACK may each be a packet transmitted over theconnection 110. Further as described with reference toFIGURE 4 , reliable transport packets may be tracked by sliding windows. As such, the pushslctdData packet may be part of a data TX sliding window kept by entity A (indicated by dash-dot line). The pushGrnt packet may be part of a data TX sliding window kept by entity B (indicated by dash dot line), and the pushSlctdData packet may be part of a data TX sliding window kept by entity A (indicated by dash-dot line). For reliable transport, the ACK packet sent by entity B may reference the PSN of the pushSlctdData, which entity A may keep track of using the data TX sliding window. Likewise, though not shown, entity A may send ACK for the pushGrnt packet, which entity B may keep track of using its data TX sliding window, and entity B may send ACK for the pushSlctdReq, which entity A may keep track of using its request TX sliding window. However, acknowledgment messages such as the ACK packet shown (indicated by dot line) are not reliable transport packets, and thus may not be part of any sliding window at the sender entity B. - As illustrated by
FIGURE 5 , the solicited push transaction allows an initiator entity to solicit a grant to send data from a target entity before actually sending the data. As such, the target entity may have control over the incoming data, which may be especially helpful when multiple initiator entities are attempting to push data onto the target entity, and also if the pushed data is large or if the network is congested. Since incast congestion may be caused by packets not being delivered to the receiver as fast as transmitted, and/or by multiple entities attempting to send packets simultaneously to the same entity, such a solicitation process may reduce incast congestion. - It is noted that other types of the requests, such as an unsolicited push request and/or pull request may also be utilized in the communication protocol system, as shown in
Figure 6 , with different feedback mechanisms. For example, an unsolicited push request does not require a pushGrnt from the target entity to send the push unsolicited data. Similarly, a pull request does not need a pull grant from target RT. Instead, the target RT may then send the pull request to the target ULP to request permission. The target ULP may then send an acknowledgment message ("ULP-ACK") to the target RT acknowledging the pull request, as well as a pull response ("pullResp") instructing the target RT to pull the requested data. In response to the pull request, the target RT may pull the requested data ("pullData"), and send the pulled data to the initiator RT, for instance over theconnection 110. Once the requested data is received by the initiator RT, the initiator RT may send a pullResp to the initiator ULP so that the initiator ULP may place or store the received data packet. - Thus, the communication protocol system may be configured to perform both of the solicited push transactions and/or the unsolicited push transactions. Where the communication protocol system is configured to perform both of the push transactions, the system may be configured to determine which push transaction to use based on one or more factors. For instance, whether a push request should be sent as a solicited or unsolicited request may be determined by the initiator RT. The initiator RT may determine whether to send a solicited push request or an unsolicited push based on a length of the push request from the initiator ULP. As an example, if a push request requires a large amount of data to be pushed, such as meeting a predetermined size threshold, a solicited push request may be used to make sure that the large request does not cause congestion; otherwise, an unsolicited push may be used. As another example, whether to use solicited request or unsolicited push may be based on network conditions, such as level of congestion, where a solicited request may be used when congestion meets a predetermined threshold level.
-
FIGURE 6 shows an example timing diagram for ordered transactions over a connection according to aspects of the technology. The example timing diagrams ofFIGURE 6 may be implemented by two entities in a network, such as entities A and B overconnection 110 ofFIGURE 1 , for example byprocessors 220 and 270 ofFIGURE 2 . - As depicted in
FIGURE 6 , various transactions, such as the pull and push transactions, may be performed by various communication protocol layers of both the initiator entity and the target entity. For example, entity A may be the initiator entity andinitiator ULP 610 andinitiator RT 630 may be communication protocol layers configured as upper-layer protocol 610 and reliabletransport protocol layer 630 ofFIGURE 6 , while entity B may be the target entity andtarget ULP 620 and targetRT 640 may be communication protocol layers configured as upper-layer protocol 620 and reliabletransport protocol layer 640 ofFIGURE 6 . - Referring to the timing diagram 600, a number of requests may originate from the initiator entity A, including pull requests such as pullReq_1, shown as
communication path 602, and push requests such as pushReq_0, pushReq_2, and pushReq_3, shown ascommunication paths initiator ULP 610 to theinitiator RT 630. Once theinitiator RT 630 receives these requests,initiator RT 630 may optionally determine whether the push requests should be sent as solicited or unsolicited. Thus, in this example, theinitiator RT 630 may determine that pushReq_0 and pushReq_2 are to be sent as solicited, while pushReq_3 is to be sent as unsolicited. Theinitiator RT 630 may then send these pull and push requests to thetarget RT 640, for example over theconnection 110. - The requests may be sent by the
initiator ULP 610 in a particular order as indicated by the Request Sequence Numbers ("RSN"), which may be assigned by theinitiator RT 630, so as to track the transaction orders. In some instances, theinitiator RT 630 may also assign Solicited Sequence Numbers ("SSN") specifically to solicited push requests, which may be an incremental number as shown. When the requests are sent as packets between two entities, the requests may be assigned with a sequence of numbers in ascending order according to the order of the RSN. Thus, as shown, the requests may be assigned PSNs within one or more TX sliding windows maintained by initiator entity A according to the RSNs. For example, pushSlctdReq_0, shown as acommunication path 611, is assigned PSN=0, pullReq_1, shown as acommunication path 614, is assigned PSN=1, pushSlctdReq_2, shown as acommunication path 612, is assigned PSN=2 within a request TX sliding window of entity A (indicated by dash lines pointing towards B). Note that since pushReq_3 from theinitiator ULP 610 does not require solicitation, there is no corresponding pushUnslctdReq being sent between the RTs. While RSNs and SSNs may be known to the ULPs, the PSNs may be unknown to the ULPs but only used by the RTs in packets. - In response to the solicited push requests, push grants may be sent by the
target RT 640 to theinitiator RT 630 in the order of the received requests, such as pushGnt_0 and pushGnt_2, shown ascommunication paths initiator RT 630 as the order of transmission for the push requests. Thus, as shown, pushGrnt_2 is received by theinitiator RT 630 before the pushGmt_0. In this regard, the reorder engine 256, 296 may assist reassembling the order of the requests prior to sending to ULPs. - The
initiator RT 630 may determine the correct order of the push grants based on their respective RSNs and push the data packets based on that order. Such order may be determined by performing a look-up operation in the reorder engine to determine the correct order. As such, although pushGrnt_2 was received by theinitiator RT 630 before pushGrnt_0, theinitiator RT 630 may first push the data solicited by pushGrnt_0 with pushSlctdData_0 and then push the data solicited by pushGrnt_2 with pushSlctdData_2 to targetRT 640. The pushed data packets are also assigned PSNs in ascending order within one or more TX sliding windows maintained by initiator entity A according to the order of transmission. For example, pushSlctdData_0 is assigned PSN=200 and pushSlctdData_2 is assigned PSN=201 within a data TX sliding window of entity A (indicated by dash-dot lines pointing towards B shown as thecommunication paths curved arrow 655 skips directly to pushUnslctdData_3, shown as thecommunication path 613, which pushes the unsolicited data. In this example, pushUnslctdData_3 is assigned PSN=202 also in the data TX sliding window of entity A. -
Target RT 640 receives the requests, and then sends corresponding requests to thetarget ULP 620 in the order of ULP-Req-0-1-2-3, which is in the same order as the transmission order of the requests from theinitiator ULP 610 shown at the top of the timing diagram 800. These ULP-Reqs ask thetarget ULP 620 for permission to pull data, or to place the pushed data at the target entity B. Note that the pull request pullReq_1 does not require a grant as described, thus as indicated by thecurved arrow 656 skips directly to the ULP-Req. In response to the ULP-Reqs, thetarget ULP 620 may send acknowledgment ULP-ACKs to thetarget RT 640. In this ordered system, the ULP-ACKs are sent in the order of ULP-ACK-0-1-2-3, which is the same as the order of transmission for the requests from theinitiator ULP 610. - Following the ULP-ACKs, with respect to the push transactions, ACKs acknowledging the data packets (or data acknowledgments) are then sent by
target RT 640 toinitiator RT 630 to notify the safe receipt and placement of the reliable transport data packets. As an example, ACK-eBSN=3, 203, shown as thecommunication path 608, is sent by entity B to notify entity A that all request packets up to PSN=3 and all data packets up to PSN=203 have been received and placed. Once the ACK is received,initiator RT 630 may send a completion message pushCompl_0 to initiatorULP 610. Further, in some instances, acknowledgment packets may be opportunistically piggybacked on other reliable transport packets. For example, the requests pushSlctdReq_0, pullReq_1, and pushSlctdReq_2, are reliable transport packets requiring an ACK, but these acknowledgments to requests (or request ACKs) are not explicitly shown in timing diagram 800 because they may be piggybacked on reliable transport packets such as pushGrnt_0 and pushGrnt_2. - Also following the ULP-ACKs, pull requests may also be responded to. Thus, as shown, the
target ULP 620 may send a pullResp_1instructing target RT 640 to pull the requested data.Target RT 640 then sends the pulled data to theinitiator RT 630 with pullData_1. In this example, pullData_1 is assigned PSN=1002 within the same data TX sliding window of entity B as the pushGrnts (indicated by dash-dot line pointing towards A). Theinitiator RT 630 then sends a pullResp_1, shown as thecommunication path 657, to theinitiator ULP 610 so that theinitiator ULP 610 may place or store the received data packet at entity A. After the data packet is placed or stored at entity A, an acknowledgment may be sent to notify entity B of safe receipt. Thus, as shown, ACK-1002-eBSN=0, 1003 is sent by entity A to notify entity B that the pullData_1 packet has been safely received. - In this ordered system, the completion messages received by the
initiator ULP 610 near the bottom of timing diagram 600 are in the same order as the requests that were sent byinitiator ULP 610 near the top of the timing diagram 600. This order is maintained on ULPs of both initiator and target entities, where the target RT presents requests to the target ULP in the same order as the initiator ULP sends requests to the initiator RT. This ordered system ensures that the requests are delivered once and only once over the connection. In contrast, there may not be an ordering requirement between transactions going in different directions over the connection. - However, in some situations wherein the requests transmitted between the initiator RT and the target RT may be out of order due to different paths or other factors such as unexpected delay or equipment failure, the requests may arrive at the target RT out of sequence or out of order, or the request may be dropped or corrupted. This may become problematic when the target RT is configured to receive the requests in the same order as transmitted from the initiator RT.
-
FIGURE 7 depicts anASIC 700, for use on a network interface card (not shown), with an integrated reliable transport accelerator (RTA) 710 configured to control data path operations and connection contexts. TheRTA 710 interfaces with upstream Upper-Layer Protocol (ULP) 702 and downstreampacket process pipeline 703 for managing or otherwise controlling data path operations.Figure 7 illustrates theRTA 710,ULP 702, andpacket process pipeline 703 as individual accelerators. However, in some instances theRTA 710,ULP 702, and/orpacket process pipeline 703 may be any number accelerators. - The RTA further communicates with memory through a Network-on-Chip (NoC) 704 for managing connection contexts within on-chip or off-chip memory. In this regard, the network interface card supports external memory, referred to herein as off-
chip memory 720. The network interface card also supports on-chip memory, referred to herein as system-level cache (SLC) 706. BothSLC 706 and off-chip memory 720 can be configured as user memory. In this regard, theRTA 710 may communicate with theSLC 706 through theNoC 704. Alternatively, or simultaneously, theRTA 710 may communicate with off-chip memory 720 through theNoC 704 and amemory controller 708. - As further illustrated in
FIGURE 7 , off-chip memory 720 is memory that is positioned off of theASIC 700, such as on the same network card (not shown) as the ASIC. Although off-chip memory 720 is illustrated as DRAM inFIGURE 7 , any type of memory may be used. - Depending on user configurations, RT connection contexts can be stored either in the
SLC 706 as a cache entry or in off-chip memory 720. The memory subsystem on theASIC 700 may implement a relaxed memory model which does not guarantee ordering of load/store to the same memory address. The memory subsystem may be an ARM memory subsystem. As previously explained, a "connection context" may include sliding windows for packet reliability, transaction ordering information, security protection, and congestion control. - The
RTA 710 is partitioned into four main units including a Rate Update (RUE) &NoC Interface 711, Connection Context Pipeline (CTX) 712, Retransmission Engine (TX) 713, and Reorder Engine (RX) 714. Each of theunits RTA 710 may be comprised of one accelerator capable of providing all of the functionalities of each of the four units, 711-714. The accelerator(s) may each be comprised of one or more processors capable of providing particular acceleration functionality. - The
CTX 712 is configured to provide connection context cache management. TheCTX 712 may also implement packet admission controls based on connection receiving sliding window state and security configurations. As illustrated inFIGURE 7 , theCTX 712 may receive packets from the ULP directly, as illustrated byarrow 771. Similarly, theCTX 712 may receive packets from the network through the ingresspacket process pipeline 703, as illustrated byarrow 772. - The retransmission engine (TX) 713 may receive packets from the
CTX 712, as illustrated byarrow 775. TheTX 713 may transmit the packets to the network through the egresspacket process pipeline 703, based on a per-connection packet format configuration, as illustrated byarrow 773. TheTX 713 may implement packet reliability and serve as the main retransmission data buffer. - The Reorder Engine (RX) 714 may utilize per connection ordering states to reorder transactions. In this regard, the
RX 714 may send packet payload and transaction completion data toULP 702, as illustrated byarrow 774.RX 714 may also serve as the main receiver data buffer from the CTX, as illustrated byarrow 779. - The Rate Update Engine (RUE) &
NoC Interface 711 supports reliable transport congestion control (CC) and at the same time implements the NoC interfaces for the internal cache management pipeline inCTX 712, described herein. -
FIGURE 8 illustrates a detailed illustration of the flow of data through theCTX 712. Packets, including variable packet data, illustrated as ULP tx-intf 804, may be received fromULP 702, as illustrated byarrow 891. Upon receipt, theCTX 712 may parse the variable packet data and extract a fixed format command through the internal cache pipeline, illustrated by dashedbox 880. In this regard, any event, such as a packet received from the network, a transaction received from theULP 702, or an internal cache management event may be formatted into a "command". Each packet received may be of a different type, for example, pull request packets, push data packets, etc. Each of these packets may be mapped/converted into a fixed format command for internal processing. Upon receipt of packet data, per connection pending transaction counters, within theCTX 712, may be immediately incremented for both cache entry management purposes and RT resource management purposes. The packet data associated with the command is directly sent to TX data buffers, as illustrated byline 893. The direct transmission of the command to the TX data buffers may eliminate the need for a large data buffer in theCTX pipeline 880. - As explained,
RTA 710 implements a fully-associative cache for connection contexts. In this regard, the connection ID (CID) of the packets, may be used as a cache entry lookup key within a connection identifier lookup database, illustrated byCID Lookup 811, for existing connections in the cache, stored in theCID Lookup 811. In this regard, the RUE &NoC interface 711 may communicate with theNoC 704, as illustrated byarrow 777, which in turn may communicate withSLC 706. - In case of a cache miss,
CTX 712 may allocate a free cache entry within theCID Lookup 811 to the connection and issue a memory read to attempt and fetch the connection, and the corresponding connection context data from off-chip memory 720 (or on-chip memory 706) through theNOC 704. In the case the connection context data is retrieved from off-chip memory, thememory controller 708memory controller 708 may communicate with the off-chip memory 720, as illustrated byarrow 778. The current command may be deposited, by thecommand process 813, into a per-connectioncache miss queue 810. In the event connection context data is returned from the off-chip memory 720, theCTX 712 may install the connection context data into a cache entry within the on-chip cache ofCID Lookup 811. - Command entries associated with the newly installed connection may become eligible for dequeue from the
miss queue 810. In this regard, the command entries may go through theinternal cache pipeline 880 for a second-pass process, as further illustrated inFigure 8 . During the second-pass, thecommand process 813 ofCTX 712 may enqueue all ULP commands to theconnection scheduler 812. Theconnection scheduler 812 may schedule transmission to the network based on per connection congestion control. Thecommand process 813 ofCTX 712 may also pass control information regarding the command toTX 713 as illustrated byarrow 823, and transaction completion information toRX 714, as illustrated byarrow 824. - In case of a cache hit for a particular CID associated with a connection, and when no pending commands associated with the same CID is within the
miss queue 810, the command is processed immediately without going through themiss queue 810. For ordering purposes, some or all later ULP transactions associated with a CID may also go through themiss queue 813 if there are one or more commands in themiss queue 813 for the same CID. - The
CTX 712 may similarly process packets received from the network, which are subjected to additional admission controls including per connection receiving sliding window and security checks, as described herein. In this regard, data associated with data packets, illustrated as pkt-RX 802, may be received from the network, as illustrated byarrow 890. The data associated withdata packets 802 may be are stored directly inRX 714, as illustrated byarrow 892. Furthermore, packets received from thenetwork 802 may trigger congestion control events to the RUE and NoC Interface, as illustrated byarrow 894. For packets received from the network, ordering may not be strictly required, hence they may bypasscache miss queue 810 as long as cache is hit during theCID lookup 811. - The
cache policy 814, illustrated inFIGURE 8 , may choose and evict connection context entries from the cache, stored onCID Lookup 811, when cache occupancy goes above a configured threshold. The threshold may be preprogrammed or configured by a user or application.RT 714 may implement both random and serial eviction (e.g., first in, first out (FIFO)), based on the cache index. - Connection contexts may be eligible for eviction from the cache store in the
SLC 706 when they become idle with pending transaction counts equal to 0. In this regard,TX 713 may decrement resource counters toCTX 712 when packets in theretransmission engine 713 are acknowledged by a remote node. Thereorder engine 714 may decrement transaction counters toCTX 712 when a transaction has been completed to or acknowledged by local ULP. The transaction counters may be decremented by TX when a transmitted packet has been acknowledged and hence deleted from the retransmission buffer. In another example, the transaction counters may be decremented by RX when a transaction is completed to ULP and hence deleted from the reorder buffer. - As explained in the Cache Entry State Transitions Section, the
CTX cache pipeline 880 may serialize access to the same CID to avoid read/write hazards in the relaxed memory model. - Since hardware (HW), such as the
RTA 710, dynamically fetches and updates connection contexts on-chip at run-time, a mechanism for software (SW) to coherently and atomically update the connection contexts is required. RT may provide a SW context flush interface through Control/Status Register (CSR) 816, as further illustrated inFIGURE 8 . The SW may trigger one connection context update through the flush control CSR. HW may then write the content through to the main memory where all the connection contexts may be stored, including the off-chip memory 720 and on-chip memory (SLC 706). A user may define which main memory the connection contexts should be stored. HW may search the on-chip cache (CID Lookup 811), for the CID, and, if found, update content in the cache. SW polls the flush status CSR for HW completion status of previous flush operation and readiness for next operation. -
FIGURE 9 illustrates a cache entry finite state machine (FSM) 900. As shown, the FSM includes four states: Invalid 902, Allocated 904, Valid 906, andEvicting 908. Commands received from the network or ULP are processed as cache hit when the CID associated with the commands match a CID associated with cache entry, storing connection context data, in a Valid state 906. - Each cache entry may start as Invalid 902. When a command is received from either ULP or the network, HW may allocate a cache entry within the
CID Lookup 811 for the connection context, as identified by the CID in the command. The HW (e.g.,RTA 710,) may also issue a memory read for the connection context. Upon completion of these steps, the cache entry for the connection context may transition into the Allocatedstate 904. - Memory access to a connection context in the Allocated
state 904 may be blocked. By blocking access to connection contexts in the Allocatedstate 904, memory access hazards may be avoided. As a result, any SW flush or RUE result write is stalled in this state. - Return of the read data may cause the cache entry to transition from an Allocated
state 904 to a Valid state 906 and any pending writes to the connection context in on-chip memory can then be processed serially. After all pending transactions are completed for the connection,CTX cache pipeline 880 may attempt an eviction of the cache entry by writing the content to off-chip memory 720 and transitioning the cache entry into theEvicting state 908. Similarly, all other access to the same connection context is stalled and serialized to prevent multiple outstanding read/write to the same memory address. If more ULP or network commands directed to a connection context in theEvicting state 908, the cache entry for the connection context in theEvicting state 908 may transition back to the Allocatedstate 904, so that the connection context data can be retrieved. Otherwise, when all write completions are returned from the off-chip memory 720, the cache entry transition to the Invalid state and the on-chip resources associated with that cache entry may be freed up. - When a SW flush or RUE result is received for a cache entry in the Invalid state, the cache entry may transition directly from
Invalid state 902 into theEvicting state 908. Additionally,RTA 710 may support a mode where SW flush is able to install a valid cache entry directly even from an initial state of Invalid 902. - The cache CID lookup process is a multi-clock-cycle event, and the command processing itself takes multiple cycles. As such, multiple hazard conditions are presented during cache entry state transitions. For
RTA 710, the lookup may involve two memory accesses and a total of 6-cycle latency. Additionally, command processing may take 2 cycles. HW hazard condition avoidance is handled in two separate stages. Firstly, the 6 cycle memory lookup pipeline implements bypass logic, such that all lookups always return the latest result after all prior modifications. This tight loop includes a non-existent cache entry (in Invalid state) transiting to Allocated or Evicting state that involves free entry allocation. Secondly, HW detects and stalls access to the same CID to be every other cycle since bypass does not address the 2-cycle cache entry state modification latency. - In one hazard condition, when cache entry transitions into
Invalid state 602 and the cache entry is being deallocated, the HW may stall access to the same connection for 6 cycles after a memory-write completion is being processed, which is the only type of operation that might lead to entry deallocation. The performance impact of such stalling is negligible since memory-write completion is not a common case command. - Unless otherwise stated, the foregoing alternative examples are not mutually exclusive, but may be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject matter defined by the claims, the foregoing description of the embodiments should be taken by way of illustration rather than by way of limitation of the subject matter defined by the claims. In addition, the provision of the examples described herein, as well as clauses phrased as "such as," "including" and the like, should not be interpreted as limiting the subject matter of the claims to the specific examples; rather, the examples are intended to illustrate only one of many possible embodiments. Further, the same reference numbers in different drawings can identify the same or similar elements.
Claims (15)
- An application specific integrated circuit, ASIC, including:
a reliable transport accelerator, RTA, the RTA including a cache lookup database, wherein the RTA is configured to:determine, from a received data packet, a connection identifier;query the cache lookup database for a cache entry corresponding to a connection context having the connection identifier; andreceive, in response to the query, a cache hit or a cache miss. - The ASIC of claim 1, wherein the ASIC further includes on-chip memory storing one or more connection contexts associated with one or more connection identifiers.
- The ASIC of claim 1 or 2, wherein the ASIC is attached to a network interface card.
- The ASIC of one of claims 1 to 3, wherein the network interface card includes off-chip memory storing one or more connection contexts associated with one or more connection identifiers, wherein the off-chip memory is connected to the ASIC.
- The ASIC of one of claims 1 to 4, wherein a cache miss indicates that the connection context having the connection identifier is not stored in the cache lookup database;wherein, after receiving a cache miss, the RTA retrieves the connection context from the on-chip memory or the off-chip memory; andwherein the RTA optionally stores the connection context in the on-chip memory or the off-chip memory based on a predefined user configuration.
- The ASIC of one of claims 1 to 5, wherein a cache hit indicates that the connection context having the connection identifier is stored in the cache lookup database.
- The ASIC of one of claims 1 to 6, further including a cache policy stored by the cache lookup database, wherein the cache policy defines when one or more connection contexts are evicted from the cache lookup database; and
wherein evicted connection contexts optionally are moved to on-chip or off-chip memory. - The ASIC of one of claims 1 to 7 further comprising an upper-layer protocol accelerator, ULP, and a packet process pipeline accelerator.
- A method for managing connection contexts, the method including:determining, by a reliable transport accelerator, RTA, including a cache lookup database, a connection identifier for a received data packet;querying, by the RTA, the cache lookup database for a cache entry corresponding to a connection context having the connection identifier; andreceiving, from the cache lookup database, in response to the query, a cache hit or a cache miss.
- The method of claim 9, wherein a cache miss indicates that the connection context having the connection identifier is not stored in the cache lookup database.
- The method of claim 9 or 10, wherein, after receiving a cache miss, the RTA retrieves the connection context from on-chip memory or off-chip memory; and
wherein the RTA optionally stores the connection context in the on-chip memory or the off-chip memory based on a predefined user configuration. - The method of one of claims 9 to 11, wherein a cache hit indicates that the connection context having the connection identifier is stored in the cache lookup database.
- The method of one of claims 9 to 12, wherein the RTA further includes a cache policy stored by the cache lookup database, wherein the cache policy defines when one or more connection contexts are evicted from the cache lookup database.
- The method of one of claims 9 to 13, wherein evicted connection contexts are moved to on-chip or off-chip memory.
- The method of claim 14, wherein the on-chip memory is located on an ASIC with the RTA; and
wherein the off-chip memory optionally is located off the ASIC.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163239603P | 2021-09-01 | 2021-09-01 | |
US17/553,387 US20230062889A1 (en) | 2021-09-01 | 2021-12-16 | Off-Chip Memory Backed Reliable Transport Connection Cache Hardware Architecture |
Publications (1)
Publication Number | Publication Date |
---|---|
EP4145803A1 true EP4145803A1 (en) | 2023-03-08 |
Family
ID=80447183
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP22157939.4A Pending EP4145803A1 (en) | 2021-09-01 | 2022-02-22 | Off-chip memory backed reliable transport connection cache hardware architecture |
Country Status (2)
Country | Link |
---|---|
US (1) | US20230062889A1 (en) |
EP (1) | EP4145803A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN115982091B (en) * | 2023-03-21 | 2023-06-23 | 深圳云豹智能有限公司 | RDMA engine-based data processing method and system, medium and equipment |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040156393A1 (en) * | 2003-02-12 | 2004-08-12 | Silverback Systems, Inc. | Architecture and API for of transport and upper layer protocol processing acceleration |
US20050226238A1 (en) * | 2004-03-31 | 2005-10-13 | Yatin Hoskote | Hardware-based multi-threading for packet processing |
US20060274787A1 (en) * | 2005-06-07 | 2006-12-07 | Fong Pong | Adaptive cache design for MPT/MTT tables and TCP context |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9628406B2 (en) * | 2013-03-13 | 2017-04-18 | Cisco Technology, Inc. | Intra switch transport protocol |
US9996498B2 (en) * | 2015-09-08 | 2018-06-12 | Mellanox Technologies, Ltd. | Network memory |
KR20210148586A (en) * | 2020-06-01 | 2021-12-08 | 삼성전자주식회사 | Scheduler, method for operating the same and accelerator system including the same |
US20210119730A1 (en) * | 2020-09-18 | 2021-04-22 | Intel Corporation | Forward error correction and cyclic redundancy check mechanisms for latency-critical coherency and memory interconnects |
US20220400123A1 (en) * | 2021-06-11 | 2022-12-15 | Mellanox Technologies Ltd. | Secure network access device |
US20230061794A1 (en) * | 2021-08-31 | 2023-03-02 | Intel Corporation | Packet transmission scheduling |
-
2021
- 2021-12-16 US US17/553,387 patent/US20230062889A1/en active Pending
-
2022
- 2022-02-22 EP EP22157939.4A patent/EP4145803A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040156393A1 (en) * | 2003-02-12 | 2004-08-12 | Silverback Systems, Inc. | Architecture and API for of transport and upper layer protocol processing acceleration |
US20050226238A1 (en) * | 2004-03-31 | 2005-10-13 | Yatin Hoskote | Hardware-based multi-threading for packet processing |
US20060274787A1 (en) * | 2005-06-07 | 2006-12-07 | Fong Pong | Adaptive cache design for MPT/MTT tables and TCP context |
Also Published As
Publication number | Publication date |
---|---|
US20230062889A1 (en) | 2023-03-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3707882B1 (en) | Multi-path rdma transmission | |
US10430374B2 (en) | Selective acknowledgement of RDMA packets | |
US8233483B2 (en) | Communication apparatus, communication system, absent packet detecting method and absent packet detecting program | |
US7013419B2 (en) | Reliable message transmission with packet-level resend | |
US8213315B2 (en) | Dynamically-connected transport service | |
US8549170B2 (en) | Retransmission system and method for a transport offload engine | |
CA2548966C (en) | Increasing tcp re-transmission process speed | |
US7733875B2 (en) | Transmit flow for network acceleration architecture | |
US20230421657A1 (en) | Reliable Transport Protocol and Hardware Architecture for Datacenter Networking | |
US20220385587A1 (en) | Acknowledgement Coalescing Module Utilized In Content Addressable Memory (CAM) Based Hardware Architecture For Data Center Networking | |
EP4145803A1 (en) | Off-chip memory backed reliable transport connection cache hardware architecture | |
US7499452B2 (en) | Self-healing link sequence counts within a circular buffer | |
EP3941007A1 (en) | Content addressable memory (cam) based hardware architecture for datacenter networking | |
US20060004933A1 (en) | Network interface controller signaling of connection event | |
US7319670B2 (en) | Apparatus and method for transmitting data to a network based on retransmission requests | |
US20080062991A1 (en) | Buffer Management for Communication Protocols | |
US7089282B2 (en) | Distributed protocol processing in a data processing system | |
US7822051B1 (en) | Method and system for transmitting packets | |
US8005971B2 (en) | Apparatus for communicating with a network | |
US20220382783A1 (en) | High Bandwidth Content Addressable Memory (CAM) Based Hardware Architecture For Datacenter Networking | |
US7450599B2 (en) | Apparatus and method for communicating with a network | |
CA2985674A1 (en) | Method and computer product for operating a memory buffer system | |
CN114143268A (en) | TCP-based message receiving method and device, electronic equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE APPLICATION HAS BEEN PUBLISHED |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: REQUEST FOR EXAMINATION WAS MADE |
|
17P | Request for examination filed |
Effective date: 20230901 |
|
RBV | Designated contracting states (corrected) |
Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: HKRef legal event code: DERef document number: 40089884Country of ref document: HK |