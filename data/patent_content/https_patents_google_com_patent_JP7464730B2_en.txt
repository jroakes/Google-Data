JP7464730B2 - Spatial Audio Enhancement Based on Video Information - Google Patents
Spatial Audio Enhancement Based on Video Information Download PDFInfo
- Publication number
- JP7464730B2 JP7464730B2 JP2022547129A JP2022547129A JP7464730B2 JP 7464730 B2 JP7464730 B2 JP 7464730B2 JP 2022547129 A JP2022547129 A JP 2022547129A JP 2022547129 A JP2022547129 A JP 2022547129A JP 7464730 B2 JP7464730 B2 JP 7464730B2
- Authority
- JP
- Japan
- Prior art keywords
- audio
- auditory
- video frame
- auditory event
- audio segment
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 claims description 161
- 230000000007 visual effect Effects 0.000 claims description 132
- 239000013598 vector Substances 0.000 claims description 26
- 238000000926 separation method Methods 0.000 claims description 12
- 230000004044 response Effects 0.000 claims description 4
- 230000005236 sound signal Effects 0.000 description 14
- 238000013459 approach Methods 0.000 description 13
- 230000006870 function Effects 0.000 description 7
- 230000008859 change Effects 0.000 description 5
- 230000008569 process Effects 0.000 description 5
- 238000003860 storage Methods 0.000 description 5
- 208000023514 Barrett esophagus Diseases 0.000 description 4
- 206010011469 Crying Diseases 0.000 description 4
- 238000013507 mapping Methods 0.000 description 4
- 241000405217 Viola <butterfly> Species 0.000 description 3
- 239000000284 extract Substances 0.000 description 3
- 238000004519 manufacturing process Methods 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000004091 panning Methods 0.000 description 3
- 238000001228 spectrum Methods 0.000 description 3
- 238000004590 computer program Methods 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000009792 diffusion process Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 238000010801 machine learning Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 229910001369 Brass Inorganic materials 0.000 description 1
- 241001465754 Metazoa Species 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 239000010951 brass Substances 0.000 description 1
- 239000000470 constituent Substances 0.000 description 1
- 238000013497 data interchange Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000006073 displacement reaction Methods 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 230000008451 emotion Effects 0.000 description 1
- 238000004880 explosion Methods 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 238000013213 extrapolation Methods 0.000 description 1
- 238000002372 labelling Methods 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000012545 processing Methods 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/165—Management of the audio stream, e.g. setting of volume, audio stream path
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0272—Voice signal separating
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04S—STEREOPHONIC SYSTEMS
- H04S5/00—Pseudo-stereo systems, e.g. in which additional channel signals are derived from monophonic signals by means of phase shifting, time delay or reverberation
- H04S5/005—Pseudo-stereo systems, e.g. in which additional channel signals are derived from monophonic signals by means of phase shifting, time delay or reverberation of the pseudo five- or more-channel type, e.g. virtual surround
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/008—Multichannel audio signal coding or decoding using interchannel correlation to reduce redundancy, e.g. joint-stereo, intensity-coding or matrixing
Description
本開示は、空間オーディオに関し、より詳細には、非空間化オーディオレコーディングから空間オーディオ情報を取得することに関する。 This disclosure relates to spatial audio, and more particularly to obtaining spatial audio information from non-spatialized audio recordings.
生活の中で、典型的な設定(たとえば、シーン)は、複数の異なる聴覚イベントを含む。聴覚イベントは、音源(すなわち、音のプロデューサまたはジェネレータ)および方向(すなわち、音が聞こえる方向)を有する音と考えることができる。拡散音は特定の方向からは始まらない。 In life, a typical setting (e.g., a scene) contains multiple different auditory events. An auditory event can be thought of as a sound that has a source (i.e., a producer or generator of the sound) and a direction (i.e., the direction from which the sound is heard). Diffuse sound does not originate from a specific direction.
聴覚イベントは、スピーチ、音楽、および楽器(たとえば、ピアノ、バイオリン、コンサート、チェロなど)、自然音(たとえば、雨、風、雷など)、人間の感情(たとえば、泣く、笑う、歓声など)、動物の発声(たとえば、咆哮、泣き声など)、他のアーティファクト(たとえば、爆発、車、およびドアベルなど)などを含むことができる。 Auditory events can include speech, music, and musical instruments (e.g., piano, violin, concert, cello, etc.), natural sounds (e.g., rain, wind, thunder, etc.), human emotions (e.g., crying, laughing, cheering, etc.), animal vocalizations (e.g., roaring, crying, etc.), other artifacts (e.g., explosions, cars, doorbells, etc.), and so on.
典型的な設定のビデオレコーディングは、設定の聴覚イベントのすべて(または、少なくともほとんど)を含む。ビデオレコーディングは、単純なハンドヘルドモバイルデバイス(たとえば、スマートフォン)から高度なレコーディング装置(たとえば、球面カメラまたは360°カメラ)まで、任意の数のレコーディングデバイスを使用して取得され得る。いくつかのレコーディングデバイスは、空間オーディオ情報(たとえば、レコーディングに含まれるオーディオイベントの方向および/または位置)をキャプチャできない場合がある。 A typical video recording of a setting will contain all (or at least most) of the auditory events of the setting. Video recordings may be obtained using any number of recording devices, ranging from simple handheld mobile devices (e.g., smartphones) to advanced recording equipment (e.g., spherical or 360° cameras). Some recording devices may not be able to capture spatial audio information (e.g., the direction and/or location of audio events contained in the recording).
ある設定のレコーディングが、実際の生活においてその設定が聞こえた(たとえば、経験した)であろうあり方を模倣するようなあり方で再生され得るように、空間オーディオ情報を含まないレコーディングについて、空間オーディオ情報が導出される(たとえば、抽出される、識別されるなど)ことが望ましい。 It is desirable for spatial audio information to be derived (e.g., extracted, identified, etc.) for recordings that do not contain spatial audio information, so that a recording of a setting can be played back in a manner that mimics the way that setting would have sounded (e.g., experienced) in real life.
本明細書に開示されているのは、ビデオ情報に基づく空間オーディオ拡張の実装形態である。 Disclosed herein is an implementation of spatial audio enhancement based on video information.
本明細書で説明する第1の例には、空間情報をオーディオセグメントに割り当てる方法がある。本方法は、非空間化されており、第1のビデオフレームに関連付けられている、第1のオーディオセグメントを受信するステップと、第1のビデオフレームにおいて視覚オブジェクトを識別するステップと、第1のオーディオセグメントにおいて聴覚イベントを識別するステップと、視覚オブジェクトのうちのある視覚オブジェクトと聴覚イベントのうちのある聴覚イベントとの間の一致を識別するステップと、前記ある視覚オブジェクトの位置に基づいて、空間的位置を前記ある聴覚イベントに割り当てるステップとを含む。 A first example described herein is a method of assigning spatial information to an audio segment. The method includes receiving a first audio segment that is despatialized and associated with a first video frame, identifying a visual object in the first video frame, identifying an auditory event in the first audio segment, identifying a match between a visual object of the visual objects and an auditory event of the auditory events, and assigning a spatial location to the auditory event based on a location of the visual object.
本方法は、一致しない聴覚イベントを識別するステップをさらに含み得る。一致しない聴覚イベントとは、第1のビデオフレームにおいて識別された視覚オブジェクトと一致しない聴覚イベントである。一致しない聴覚イベントは、ユーザインターフェースにおいて提示され得る。 The method may further include identifying incongruent auditory events. The incongruent auditory events are auditory events that are incongruent with the visual object identified in the first video frame. The incongruent auditory events may be presented in a user interface.
本方法は、ユーザから、第1のビデオフレームにおいて識別された視覚オブジェクトのうちの別の視覚オブジェクトへの一致しない聴覚イベントの割当てを受信するステップを含み得る。本方法は、ユーザから、一致しない聴覚イベントを拡散音として割り当てるための指示を受信するステップを含み得る。本方法は、ユーザから、一致しない聴覚イベントを指向性音として割り当てるための指示、および一致しない聴覚イベントの空間方向を受信するステップを含み得る。 The method may include receiving from a user an assignment of the incongruent auditory events to another one of the visual objects identified in the first video frame. The method may include receiving from a user an instruction to assign the incongruent auditory events as diffuse sound. The method may include receiving from a user an instruction to assign the incongruent auditory events as directional sound, and a spatial direction of the incongruent auditory events.
第1のビデオフレームは、全天球ビデオ(spherical video)のフレームであり得る。 The first video frame may be a frame of a spherical video.
第1のオーディオセグメントはモノラルであり得る。 The first audio segment may be mono.
第1のオーディオセグメントにおける聴覚イベントを識別するステップは、第1のオーディオセグメントを複数のトラックに分解することによって第1のオーディオセグメントにおける聴覚イベントを識別するために、ブラインド音源分離を使用するステップであって、各トラックがそれぞれの聴覚イベントに対応する、ステップを含み得る。 Identifying auditory events in the first audio segment may include using blind source separation to identify auditory events in the first audio segment by decomposing the first audio segment into a number of tracks, each track corresponding to a respective auditory event.
第1のビデオフレームにおける視覚オブジェクトを識別するステップは、第1のビデオフレームにおける視覚オブジェクトを識別するために、画像認識を使用するステップを備え得る。 Identifying the visual object in the first video frame may include using image recognition to identify the visual object in the first video frame.
本方法は、聴覚イベントのうちのある聴覚イベント、およびある聴覚イベントの空間的位置を備えるオーディオ出力を生成するステップをさらに含み得る。 The method may further include generating an audio output comprising an auditory event of the auditory events and a spatial location of the auditory event.
本方法は、第2のオーディオセグメントを受信するステップであって、第2のオーディオセグメントが聴覚イベントを含む、ステップを含み得る。第2のビデオフレームが受信されてもよく、第2のビデオフレームは視覚オブジェクトを含まない。本方法は、第1のビデオフレームの少なくともサブセットに少なくとも部分的に基づいて視覚オブジェクトの動きベクトルを決定するステップと、動きベクトルに基づいて、周囲空間的位置を聴覚イベントのうちのある聴覚イベントに割り当てるステップとを含み得る。 The method may include receiving a second audio segment, the second audio segment including an auditory event. A second video frame may be received, the second video frame not including a visual object. The method may include determining a motion vector of the visual object based at least in part on at least a subset of the first video frames, and assigning a ambient spatial location to an auditory event of the auditory events based on the motion vector.
本方法は、第2のオーディオセグメントを受信するステップであって、第2のオーディオセグメントが聴覚イベントを含む、ステップを含み得る。第2のビデオフレームが受信されてもよく、第2のビデオフレームは視覚オブジェクトを含まない。本方法は、第1のビデオフレームと第2のビデオフレームとの間の時間差に基づいて、周囲空間的位置または拡散位置のうちの1つを聴覚イベントに割り当てるステップを含み得る。 The method may include receiving a second audio segment, the second audio segment including an auditory event. A second video frame may be received, the second video frame not including a visual object. The method may include assigning one of a surrounding spatial location or a diffuse location to the auditory event based on a time difference between the first video frame and the second video frame.
本明細書で説明される第2の例では、空間情報をビデオ内のオーディオイベントに割り当てる方法がある。本方法は、オーディオトラックとビデオフレームを取得するためにビデオを逆多重化するステップと、それぞれの視覚ラベルをビデオフレーム内の視覚オブジェクトに割り当てるステップと、オーディオトラックを複数のトラックに分割するステップと、それぞれのオーディオラベルを複数のトラックに割り当てるステップと、それぞれのオーディオラベルのうちのいくつかを視覚ラベルのうちのいくつかに自動的に一致させるステップと、視覚オブジェクトのうちのいくつかのそれぞれの位置に基づいて、それぞれの空間的位置をそれぞれのオーディオラベルのうちのいくつかに割り当てるステップとを含む。 In a second example described herein, there is a method for assigning spatial information to audio events in a video. The method includes demultiplexing the video to obtain audio tracks and video frames, assigning respective visual labels to visual objects in the video frames, splitting the audio tracks into a plurality of tracks, assigning respective audio labels to the plurality of tracks, automatically matching some of the respective audio labels to some of the visual labels, and assigning respective spatial locations to some of the respective audio labels based on respective locations of some of the visual objects.
本方法は、一致しないオーディオラベルに対応する残留トラックを識別するステップと、ユーザに、ディスプレイにおいて残留トラックを表示するステップとをさらに含み得る。 The method may further include identifying the remaining tracks that correspond to the mismatched audio labels and displaying the remaining tracks to a user on a display.
本方法は、残留トラックのうちのある残留トラックの拡散音場への第1の割当て、ビデオフレームの任意の空間的位置への前記ある残留トラックの第2の割当て、周囲音としての前記ある残留トラックの第3の割当て、または、ビデオフレーム内の視覚オブジェクトへの前記ある残留トラックの第4の割当てのうちの少なくとも1つを、ユーザから受信するステップをさらに備え得る。 The method may further comprise receiving from a user at least one of a first assignment of a certain one of the residual tracks to a diffuse sound field, a second assignment of the certain one of the residual tracks to an arbitrary spatial location of the video frame, a third assignment of the certain one of the residual tracks as an ambient sound, or a fourth assignment of the certain one of the residual tracks to a visual object within the video frame.
本明細書で説明される第3の例では、空間情報をオーディオセグメントに割り当てるための方法がある。本方法は、非空間化されており、第1のビデオフレームに関連付けられている、第1のオーディオセグメントを受信するステップと、第1のビデオフレームにおいて視覚オブジェクトを識別するステップと、第1のオーディオセグメントにおいて聴覚イベントを識別するステップと、視覚オブジェクトのうちのある視覚オブジェクトと聴覚イベントのうちのある聴覚イベントとの間の一致を識別するステップに応答して、空間情報を前記あるイベントに割り当てるステップと、一致を識別しないステップに応答して、前記ある聴覚イベントを拡散音場に割り当てるステップとを含む。 In a third example described herein, there is a method for assigning spatial information to an audio segment. The method includes receiving a first audio segment that is despatialized and associated with a first video frame, identifying a visual object in the first video frame, identifying an auditory event in the first audio segment, and, in response to identifying a match between a visual object of the visual objects and an auditory event of the auditory events, assigning spatial information to the event, and, in response to not identifying a match, assigning the auditory event to a diffuse sound field.
空間情報を聴覚イベントに割り当てるステップは、視覚オブジェクトの位置に基づいて、空間的位置を聴覚イベントに割り当てるステップを備え得る。 The step of assigning spatial information to the auditory event may include the step of assigning a spatial location to the auditory event based on the location of a visual object.
空間的位置は、視覚オブジェクトの境界ポリゴンの中心に対応し得る。 The spatial location may correspond to the center of a bounding polygon of the visual object.
本方法は、聴覚イベントと聴覚イベントの空間的位置とを含むオーディオファイルを生成するステップを含み得る。 The method may include generating an audio file that includes the auditory events and the spatial locations of the auditory events.
本方法は、聴覚イベントと聴覚イベントに関連する拡散音情報とを含むオーディオファイルを生成するステップを含み得る。 The method may include generating an audio file that includes the auditory event and diffuse sound information associated with the auditory event.
本明細書に記載の第4の例によれば、上記の例のいずれかの方法を実行するように構成されたプロセッサを備える装置がある。 According to a fourth example described herein, there is an apparatus including a processor configured to perform the method of any of the above examples.
本開示のこれらおよび他の態様は、実施形態の以下の詳細な説明、添付の特許請求の範囲、および添付の図面に開示されている。 These and other aspects of the present disclosure are disclosed in the following detailed description of the embodiments, the appended claims, and the accompanying drawings.
本開示は、添付の図面と併せて読むと、以下の詳細な説明から最もよく理解される。一般的な慣行に従って、図面の様々な特徴は原寸に比例していないことが強調される。むしろ、明確にするために、様々な機能の寸法が任意に拡大または縮小される。 The present disclosure is best understood from the following detailed description when read in conjunction with the accompanying drawings. It is emphasized that, according to common practice, the various features of the drawings are not drawn to scale. Rather, the dimensions of various features have been arbitrarily enlarged or reduced for clarity.
シーンの一部のビデオレコーディングは、聴覚イベントの空間オーディオ情報を含むことができる。たとえば、全天球ビデオにおいては、聴覚イベントは通常、視覚オブジェクトまたは聴覚イベントを製作する(たとえば、生成する)イベントに対応する方向から発生する。高解像度および/または高精度の空間オーディオ表現は、いくつかの利用可能な手法のうちの1つを使用して実現することができる。そのような手法の例は、オブジェクトベース、チャネルベース、およびシーンベースの手法を含む。 A video recording of a portion of a scene can contain spatial audio information of an auditory event. For example, in a spherical video, the auditory events typically originate from directions that correspond to the visual objects or events that produce (e.g., generate) the auditory events. High-resolution and/or high-precision spatial audio representations can be achieved using one of several available techniques. Examples of such techniques include object-based, channel-based, and scene-based techniques.
オブジェクトベースの手法において、各オーディオイベントは、(パラメータの中でも特に)イベントの空間的位置を指定するメタデータとともに、オーディオデータ(たとえば、モノラルオーディオレコーディングなど)を使用してオブジェクトとして表すことができる。チャネルベースの手法においては、選択された時間/強度ベースのパンニング法則を使用して、(最も一般的には)水平のみ、または(あまり一般的ではないが)高さのあるスピーカ構成のスピーカのグループ(たとえば、2つまたは3つのスピーカ)間で、多数のオーディオイベントを計画することができる。シーンベースの手法においては、任意の数のオーディオイベントから生じる無限の解像度の音場を、最終的な空間解像度に切り捨てて、有限の数の基底関数で表すことができる。たとえば、高次アンビソニックス(Higher Order Ambisonics、HOA)は、聞き手の周りの球上の有限解像度の音圧分布を表すために、球面調和関数を基底関数として使用する。この手法は、チャネルベースの手法に固有の最終的な再生設定から音場表現を切り離す。 In object-based approaches, each audio event can be represented as an object using audio data (e.g., a mono audio recording) along with metadata that specifies (among other parameters) the spatial location of the event. In channel-based approaches, multiple audio events can be planned across a group of speakers (e.g., two or three speakers) in a (most commonly) horizontal-only or (less commonly) height-altitude speaker configuration using a selected time/intensity-based panning law. In scene-based approaches, the infinite resolution sound field resulting from any number of audio events can be represented by a finite number of basis functions, truncated to the final spatial resolution. For example, Higher Order Ambisonics (HOA) uses spherical harmonics as basis functions to represent a finite resolution sound pressure distribution on a sphere around the listener. This approach decouples the sound field representation from the final playback setting that is inherent in channel-based approaches.
上記の手法の各々は、空間オーディオコンテンツのキャプチャ、製作、記憶、および/または再生のうちの少なくとも1つに関連付けられる特定の欠点があり得る。 Each of the above approaches may have certain drawbacks associated with at least one of the capture, production, storage, and/or playback of spatial audio content.
たとえば、オブジェクトベースの手法においては、各オーディオイベントを個別にキャプチャして記憶するのが理想的である。キャプチャは、個々のイベントをスポットレコーディングし、ポストプロダクション中に空間プロパティを聴覚イベントに割り当てることと同等であり得る。オブジェクトベースのオーディオは、最終的な再生段階から切り離されており、各オブジェクトが個別に空間化される必要があり、それによって再生の複雑さが高くなる。 For example, in an object-based approach, each audio event would ideally be captured and stored individually. Capture could be equivalent to spot recording each individual event and assigning spatial properties to the auditory events during post-production. Object-based audio is decoupled from the final playback stage, requiring each object to be spatialized individually, thereby increasing playback complexity.
たとえば、チャネルベースの手法においては、専用のマルチマイク設定を用いてオーディオをレコーディングすることができ、または、スポットレコーディングを使用することができる。スポットレコーディングの場合、すべてのオーディオイベントは、"Multichannel sound technology in home and broadcasting applications," ITU-R BS.2159.4, [2019年12月27日に取得](インターネット<URL https://www.itu.int/dms_pub/itu-r/opb/rep/R-REP-BS.2159-4-2012-PDF-E.pdf>から取得)において説明されているように、専用のチャネルフォーマットに事前にミキシングすることができる。ストレージコストは使用するチャネルの数によって制限され、レンダリングの複雑さは低くなるが、空間解像度もまた、所与の空間領域におけるスピーカの物理的な可用性によって制限される可能性がある。さらに、特定のスピーカ設定用にミックスされたコンテンツは、通常、他のスピーカ設定と互換性がない可能性がある。 For example, in a channel-based approach, audio can be recorded using a dedicated multi-microphone setup, or spot recording can be used. In the case of spot recording, all audio events can be premixed into a dedicated channel format as described in "Multichannel sound technology in home and broadcasting applications," ITU-R BS.2159.4, [Retrieved December 27, 2019] (Retrieved from the Internet <URL https://www.itu.int/dms_pub/itu-r/opb/rep/R-REP-BS.2159-4-2012-PDF-E.pdf>). Storage costs are limited by the number of channels used, and rendering complexity is low, but spatial resolution may also be limited by the physical availability of speakers in a given spatial region. Furthermore, content mixed for a particular speaker setup may not usually be compatible with other speaker setups.
たとえば、アンビソニックス(オーディオをキャプチャ、記憶、および再生するための漸近ホロフォニクス技法)を使用するなどのシーンベースの手法においては、符号化された音場の空間解像度は、音場の球面調和関数展開が無限大に近づくにつれて、元の音場の解像度と一致する可能性がある。しかしながら、音場のキャプチャ、記憶、および再生のコストは、所望の空間解像度(たとえば、スケーラブルな空間解像度)によって異なる可能性がある。たとえば、一次アンビソニックス(FOA)は4つの係数(すなわち、オーディオのチャネル)を必要とする。高解像度の音場表現(たとえば、HOA)は、9、16、またはそれ以上の係数(すなわち、オーディオのチャネル)を必要とする。 For example, in scene-based techniques such as using Ambisonics (an asymptotic holophonic technique for capturing, storing, and reproducing audio), the spatial resolution of the encoded sound field can match that of the original sound field as the spherical harmonic expansion of the sound field approaches infinity. However, the cost of capturing, storing, and reproducing the sound field can vary depending on the desired spatial resolution (e.g., scalable spatial resolution). For example, first-order Ambisonics (FOA) requires four coefficients (i.e., channels of audio). High-resolution sound field representations (e.g., HOA) require 9, 16, or more coefficients (i.e., channels of audio).
キャプチャの観点から、FOAは必要なチャネル数が少ないため、広くアクセス可能である。たとえば、FOAでは4つのチャネルのマイクアレイが一般的である。HOAのキャプチャはより困難であり、HOA球面アレイの従来技術のキャプチャでは、32個のオーディオのチャネルを使用する可能性がある。 From a capture perspective, FOA is widely accessible due to the small number of channels required. For example, a four channel microphone array is common in FOA. HOA is more difficult to capture, and prior art capture of an HOA spherical array might use 32 channels of audio.
製作の観点から、スポットレコーディング(たとえば、モノラルレコーディング)からのオーディオイベントは、FOA/HOA表現に符号化することができ(オブジェクトベースおよびチャネルベースの手法と同様に)、ここで、時間/強度ベースのパンニング法則の代わりに、アンビソニックスエンコーダを使用することができる。したがって、ストレージコストは、選択した音場の解像度に依存する可能性がある。 From a production perspective, audio events from spot recordings (e.g. mono recordings) can be encoded into a FOA/HOA representation (similar to object-based and channel-based approaches), where an Ambisonics encoder can be used instead of time/intensity-based panning laws. Storage costs may therefore depend on the chosen sound field resolution.
上記のように、シーンの一部のビデオレコーディングは、空間オーディオ情報を含まない場合がある。そのようなレコーディングは、本明細書ではモノラルレコーディングと呼ばれ、単一のマイクを使用して、またはスマートフォンなどのハンドヘルドデバイスを使用してキャプチャされ得る。モノラルレコーディングは、特にカジュアルなビデオプロデューサまたはプロシューマの間で一般的である。モノラルレコーディングには、より複雑なチャネルベースまたはシーンベース設定の機器制限はない。 As noted above, some video recordings of a scene may not contain spatial audio information. Such recordings are referred to herein as mono recordings and may be captured using a single microphone or using a handheld device such as a smartphone. Mono recordings are common, especially among casual video producers or prosumers. Mono recordings do not have the equipment limitations of more complex channel-based or scene-based setups.
上記の手法とは対照的に、モノラルレコーディングには空間オーディオ情報がない。モノラルレコーディングは、多数の指向性音源と拡散イベントおよび/または環境
のモノラルダウンミックスを含む。したがって、たとえば、スマートフォンなどのハンドヘルドデバイスを使用してビデオに付随するオーディオをレコーディングする場合、通常、すべての聴覚空間情報が回復不能に失われる。また、モノラルレコーディングはすでに指向性音源と非指向性音源が混在しているため、オブジェクトベースの手法において使用することができない。
In contrast to the above techniques, mono recordings lack spatial audio information. They contain a mono downmix of many directional sound sources and diffuse events and/or environments. Thus, for example, when recording the audio accompanying a video using a handheld device such as a smartphone, all auditory spatial information is typically irretrievably lost. Also, mono recordings cannot be used in object-based techniques because they already contain a mixture of directional and non-directional sound sources.
説明のために、設定は、泣いている女の子(見える)、吠えている犬(これも見える)、テレビで再生されているサッカーの試合(見える)、隣接する部屋にいる(すなわち、見えない)、歌っている母親、および雷の音を含み得る。モノラルレコーディング(すなわち、モノラル音を含むビデオレコーディング)は、これらすべての音を含む。しかしながら、このレコーディングは、これらの音の各々に関する空間情報を含まない。たとえば、このレコーディングは、泣いている聴覚イベントの位置が画像の中央にあること、吠える聴覚イベントが画像の左側から発生し、犬が壁の近くに横たわっていること、および、歌は画像の右側から来ていることをキャプチャしない。 To illustrate, a setting may include a girl crying (seen), a dog barking (also seen), a soccer game playing on a television (seen), a mother singing in an adjacent room (i.e., not seen), and the sound of thunder. A mono recording (i.e., a video recording that includes mono sound) includes all these sounds. However, this recording does not include spatial information about each of these sounds. For example, this recording does not capture that the location of the crying auditory event is in the center of the image, that the barking auditory event occurs from the left side of the image, that the dog is lying near the wall, and that the singing is coming from the right side of the image.
本明細書で使用される場合、ビデオのフレーム(たとえば、画像)において見えるまたは見えないにかかわらず、オブジェクト(たとえば、人、車両など)から発生する音(すなわち、聴覚イベント)は、本明細書では指向性音と呼ばれる。見えないオブジェクトから発生する音は、さらに周囲音として説明することができる。特定の方向から来ない音(たとえば、雨、雷など)は、本明細書では拡散音と呼ばれる。オーディオセグメントのすべての拡散音は、拡散音場と呼ばれる。周囲音と拡散音の違いは、拡散音は特定の方向に関連付けられていないのに対し、周囲音はシーン/フレーム内に見えないオブジェクトからの一般的な方向に関連付けられていると考えられ得る。拡散音は、統計的に複数の方向(たとえば、あらゆる場所)から発生し、特定のオブジェクトに関連付けられていないものと考えることができる。 As used herein, sounds (i.e., auditory events) originating from objects (e.g., people, vehicles, etc.), whether or not visible in a frame (e.g., image) of a video, are referred to herein as directional sounds. Sounds originating from unseen objects may be further described as ambient sounds. Sounds not coming from a specific direction (e.g., rain, thunder, etc.) are referred to herein as diffuse sounds. All diffuse sounds in an audio segment are referred to as the diffuse sound field. The difference between ambient and diffuse sounds is that diffuse sounds are not associated with a specific direction, whereas ambient sounds are associated with a general direction from objects not visible in the scene/frame. Diffuse sounds may be considered as statistically originating from multiple directions (e.g., everywhere) and not associated with a specific object.
本開示による実装形態は、モノラルダウンミックスプロセス(すなわち、たとえばスマートフォンを使用するモノラルオーディオキャプチャ)中に失われた空間オーディオ情報を検索する(たとえば、ヒューリスティックに検索する)ために、ビデオにおけるオーディオおよび視覚的特徴(空間的および時間的)の両方を利用する。すなわち、欠落している空間オーディオ情報を検索(たとえば、再構成、推定など)するために、ビデオ内の視覚的(すなわち、画像および/または動画)情報を使用することができる。 Implementations according to the present disclosure exploit both audio and visual features (spatial and temporal) in the video to search (e.g., heuristically search) for spatial audio information lost during the mono downmix process (i.e., mono audio capture using a smartphone, for example). That is, visual (i.e., image and/or video) information in the video can be used to search (e.g., reconstruct, estimate, etc.) the missing spatial audio information.
空間オーディオ情報を検索(たとえば、導出、再構成、構築など)するために、機械ビジョン(たとえば、画像および/またはオブジェクト認識)を機械聴覚技法(たとえば、オーディオ分類)と組み合わせて使用することができる。いくつかの例では、視聴覚シーン情報を検索するために、ユーザ支援を機械ビジョンおよび/または機械聴覚技法と組み合わせることができる。指向性聴覚イベントと拡散聴覚イベントの両方の選択と抽出を通知するために、ユーザ支援を使用することができる。検索した空間情報は、その後、聴覚イベントを元の方向に再符号化したり、拡散音場および/または環境を合成したりするために使用することができる。 Machine vision (e.g., image and/or object recognition) can be used in combination with machine hearing techniques (e.g., audio classification) to retrieve (e.g., derive, reconstruct, construct, etc.) spatial audio information. In some examples, user assistance can be combined with machine vision and/or machine hearing techniques to retrieve audiovisual scene information. User assistance can be used to inform the selection and extraction of both directional and diffuse auditory events. The retrieved spatial information can then be used to re-encode auditory events back to their original orientation or to synthesize diffuse sound fields and/or environments.
本明細書ではモノラルレコーディングが説明されているが、この用語は、使用される場合、空間情報を含まない、または限定された空間情報を含むレコーディングも包含することに留意されたい。すなわち、本明細書の内容は、十分な空間オーディオ情報(すなわち、「非空間化されたオーディオ情報」)を含まない(たとえば、関連付けられていない)ビデオレコーディングに適用される。たとえば、ステレオレコーディングは空間情報が制限され、これは通常、2つのスピーカ間の弧または左右のヘッドホン間のラインに制限される。たとえば、ステレオレコーディングでは、シーンの後ろにあった音オブジェクトが前に聞こえる場合があり、ユーザの上にあった音オブジェクトが、水平面上にある可能性がある。その結果、一部の指向性音源の元の位置が、ステレオ音場において誤って表現されることがよくある。 Note that although mono recordings are described herein, the term, when used, also encompasses recordings that do not contain spatial information or that contain limited spatial information. That is, the contents of this specification apply to video recordings that do not contain (e.g., are not associated with) sufficient spatial audio information (i.e., "non-spatialized audio information"). For example, stereo recordings have limited spatial information, which is usually restricted to an arc between two speakers or a line between left and right headphones. For example, in a stereo recording, a sound object that was at the back of the scene may be heard as being in front, and a sound object that was above the user may be on the horizontal plane. As a result, the original position of some directional sound sources is often misrepresented in the stereo sound field.
モノラルオーディオトラックは、オーディオトラックに存在する音源に関する空間情報を含まない。スピーカアレイを介して提示された場合、すべての音源は単一のスピーカ(オーディオトラックが割り当てられている)から発生するか、または複数のスピーカに複製される。後者のシナリオでは、時間/強度のパンニング法則により音源の位置が変更される可能性があるため、音源は複数のスピーカ間の位置から発生しているように思われる場合がある。しかしながら、オーディオストリームに存在するすべての音源は、元の音場において空間的に分離されている場合でも、同じ位置から発生すると予想される。ヘッドホンを介して提示された場合、すべての音源は聞き手の頭の中から発生する。 Mono audio tracks do not contain spatial information about the sound sources present in the audio track. When presented through a speaker array, all sound sources either originate from a single speaker (to which the audio track is assigned) or are duplicated across multiple speakers. In the latter scenario, sound sources may appear to originate from a position among multiple speakers, since time/intensity panning laws may change the location of the sound sources. However, all sound sources present in an audio stream are expected to originate from the same location, even if they are spatially separated in the original sound field. When presented through headphones, all sound sources originate from inside the listener's head.
一方、ステレオオーディオトラックは、元の音場に存在する音源に関する非常に限られた情報を含む。通常、2つの標準ステレオスピーカを使用して提示する場合、音源のそれぞれの位置は60度の弧に制限される。ヘッドホンを介して提示される場合、音源は左耳または右耳のいずれか(または、その間の任意の場所)に定位することができる。そのため、ステレオにおいては、球形の音場から発生する音源の360°空間表現は、1D表現に折りたたまれる。 Stereo audio tracks, on the other hand, contain very limited information about the sound sources present in the original sound field. Typically, when presented using two standard stereo speakers, the respective positions of the sound sources are restricted to an arc of 60 degrees. When presented through headphones, the sound sources can be localized at either the left or right ear (or anywhere in between). So in stereo, the 360° spatial representation of the sound sources originating from a spherical sound field is collapsed into a 1D representation.
したがって、繰返しになるが、本開示に関連して、ステレオレコーディングは空間オーディオ情報を含むとは言えず、「モノラルオーディオ」という用語は、モノラルオーディオとステレオオーディオの両方、より一般的には、空間オーディオ情報を含まないレコーディングを包含する。 Therefore, to reiterate, in the context of this disclosure, a stereo recording is not said to contain spatial audio information, and the term "mono audio" encompasses both mono and stereo audio, and more generally, recordings that do not contain spatial audio information.
図1は、本開示の実装形態によるオーディオ情報を空間化するための装置またはシステム100の例を示す図である。オーディオ情報の空間化は、オーディオイベントに空間情報を追加することを意味する。システム100は、ビデオ102を受信し、空間情報を含むオーディオ104を出力する。ビデオ102は、画像(たとえば、フレーム、動画など)および聴覚イベントを含む。ビデオ102は、ビデオファイル(たとえば、以前にレコーディングされて記憶されたビデオファイル)、ビデオストリームなどであり得る。聴覚イベントは非空間化されている。すなわち、ビデオの聴覚イベントに関する空間情報は利用できない。
FIG. 1 illustrates an example of an apparatus or
ビデオ102は、マルチビュービデオであり得る。すなわち、ビデオ102は、複数のカメラを使用して、または全方位カメラを使用して、異なる視点からキャプチャされる設定であり得る。したがって、ビデオ102は、全天球ビデオ、360°ビデオ、パノラマビデオなどであり得る。ビデオ102は、シングルビューカメラビデオであり得る。
システム100は、音の空間情報を出力(たとえば、取得、割当て、決定、計算、推定など)するために、ビデオの画像およびビデオにおいてキャプチャされた(たとえば、含まれるなど)音を使用することができる。
The
システム100は、オーディオ分類装置106、オブジェクト認識装置108、マッチャ110、オーディオ空間化装置(audio spatializer)112、および拡散フィールドシンセサイザ114を含むことができる。システム100の他の実装形態は、より多いモジュール、より少ないモジュール、他のモジュール、またはそれらの組合せを含むことができる。
The
たとえば、拡散フィールドシンセサイザ114は、システム100に含まれなくてもよい。たとえば、システム100は、ユーザインターフェースモジュールを含むことができる。ユーザインターフェースモジュールを通じて、ユーザは、画像内の認識されないオブジェクト(すなわち、オブジェクト認識装置108によって識別または分類されないオブジェクト)を識別することができ、ユーザは、認識されない音(すなわち、オーディオ分類装置106によって識別または分類されない聴覚イベント)を識別することができ、ユーザは、一致しない(すなわち、一致しない、またはマッチャ110によって不正確に一致する)聴覚イベントをオブジェクトに一致させることができ、ユーザは、他のアクション、またはそれらの組合せを実行することができる。別の例では、ユーザインターフェースモジュールのいくつかの態様(たとえば、機能、能力など)は、他のそれぞれのモジュールに実装されるか、その一部となることができる。システム100は、ビデオ102をその構成要素であるビデオストリームおよびオーディオストリームに分割することができる逆多重化モジュールを含むことができる。
For example, the diffuse field synthesizer 114 may not be included in the
システム100のモジュールは、図2に関連して説明される。図2は、本開示の実装形態による、オーディオを空間化するための技法200の例のフローチャートである。音場における指向性オーディオイベントおよび拡散オーディオイベントのモノラルダウンミックス、ならびにオーディオイベントの視覚的表現(球形/360°表現が可能であるが、そうである必要はない)が与えられると、技法200は、モノラルダウンミキシングまたはモノラルレコーディングプロセスにおいて失われた聴覚シーンの空間オーディオ情報を検索する(たとえば、推定する、など)。次いで、抽出された聴覚イベントは、任意の空間表現に空間的に拡張(すなわち、アップミックス)することができる。
The modules of the
ビデオ202は、技法200によって受信される。ビデオ202は、図1のビデオ102に関して説明した通りであり得る。ビデオ202のオーディオトラック204は、オーディオ分類208に入力され、これは、図1のオーディオ分類装置106によって実行することができる。オーディオ分類208は、オーディオトラック204内のオーディオ音を識別する。ビデオ202のビデオフレーム206は、図1のオブジェクト認識装置108によって実行され得るオブジェクト分類210に入力される。オブジェクト分類210は、ビデオフレーム内の視覚オブジェクト(すなわち、見えるオブジェクト)を識別する。
A
一例では、逆多重化モジュールは、ビデオ202をその構成要素(すなわち、オーディオトラック204およびビデオフレーム206)に分割することができる。別の例では、図1のオーディオ分類装置106、または別のモジュールは、ビデオ202からオーディオトラック204を抽出することができ、オブジェクト認識装置108、または別のモジュールは、ビデオ202からビデオフレーム206を抽出することができる。
In one example, the demultiplexing module can split the
一例では、ビデオ202は、技法200によってチャンク(すなわち、セグメント)において処理することができる。すなわち、ビデオ202は、セグメントに分割することができる。各セグメントは、いくつかのフレーム、および対応するオーディオセグメントを含むことができる。したがって、オーディオトラック204はチャンクのオーディオセグメントであり得、ビデオフレーム206はチャンクのフレームであり得る。
In one example, the
たとえば、セグメントの長さが各5秒であり、ビデオが1秒あたり30フレームのフレームレートでキャプチャされたと仮定すると、各セグメントは150フレームおよび対応する5秒のオーディオを含む。セグメントは様々なサイズ(すなわち、長さ)を有することができる。一例では、各セグメントは、ビデオ202のシーンに対応することができる。たとえば、ビデオ202は、ビデオ202内のシーンを識別するために、シーン検出モジュール(図示せず)によって処理することができる。全天球ビデオの例では、各チャンクは、カメラ位置の変化に対応することができる。
For example, assuming the segments are each 5 seconds long and the video was captured at a frame rate of 30 frames per second, each segment includes 150 frames and corresponding 5 seconds of audio. The segments can have a variety of sizes (i.e., lengths). In one example, each segment can correspond to a scene in
オーディオ分類208は、図3に関して説明されている。図3は、本開示の実装形態による、オーディオ分類のための技法300の例のフローチャートである。技法300は、図2のオーディオ分類208によって、または図1のオーディオ分類装置106によって実装することができる。
The
技法300は、1)オーディオトラック204内の異なる音源を識別することと、2)各抽出されたオーディオ音源(または、少なくともその一部)にラベル付けすることと、3)任意でユーザから分類情報を受信することとを含む。
The
302において、技法300は、図2のオーディオトラック204などのオーディオトラック内の異なる音源を識別する。異なる音源は、任意の数の利用可能な音源分離技法を使用して識別することができる。音源は、以下でさらに説明するように、後で再結合(すなわち、アップミックス)できるように分離されている。
At 302, the
音源の分離は、異なるオーディオデータ表現(たとえば、オーディオスペクトログラム)の分析に基づく場合がある。一例では、音源分離技法であるブラインドオーディオ音源分離(Blind Audio Source Separation、BASS)を使用することができる。BASSは、混合信号(すなわち、ダウンミックスされたオーディオトラック)を入力として受信し、ダウンミックスされたオーディオトラックから個々の音源を抽出する。別の言い方をすれば、BASSは混合された音源から元の音源を抽出することを目的としている。基礎となるBASSは、ダウンミックスされたオーディオトラックを生成するために、個々の音源が未知の機能に従ってミックスされたという仮定である。BASSは、観測された(すなわち、ダウンミックスされたオーディオトラック内の)混合信号を使用して混合関数を推定する。 The separation of the sound sources may be based on the analysis of different audio data representations (e.g., audio spectrograms). In one example, the sound source separation technique Blind Audio Source Separation (BASS) can be used. BASS receives a mixed signal (i.e., a downmixed audio track) as input and extracts individual sound sources from the downmixed audio track. In other words, BASS aims to extract the original sound sources from the mixed sound sources. The underlying BASS assumption is that the individual sound sources have been mixed according to unknown functions to generate the downmixed audio track. BASS estimates the mixing function using the observed (i.e., in the downmixed audio track) mixed signal.
たとえば、1人がバイオリンを演奏し、もう1人がピアノを演奏している、2人のモノラルレコーディングが与えられた場合、BASSは、レコーディングに2つのオーディオ音源(すなわち、2つのオーディオオブジェクト)があることを識別することができる。BASSは、第1のオーディオ音源がバイオリンであり、第2のオーディオ音源がピアノであることを明確に識別できない場合がある。 For example, given a mono recording of two people, one playing the violin and another playing the piano, BASS can identify that there are two audio sources (i.e., two audio objects) in the recording. BASS may not be able to clearly identify that the first audio source is the violin and the second audio source is the piano.
304において、技法300は、抽出されたオーディオ音源のうちの少なくともいくつかにラベル付けする。すなわち、ラベル(すなわち、識別、人間が読める文字列、意味論的文字列など)が、抽出されたオーディオ音源のうちの少なくとも一部の各々に割り当てられる。一例では、オーディオ分類を使用することができる。抽出されたオーディオ音源の各々は、オーディオ分類のために(たとえば、別々に)提示することができる。オーディオ分類は、オーディオ音源の識別に関する情報を出力する。一例では、オーディオ分類は、音のサンプルを分類するようにトレーニングされた機械学習モデルであることができる。すなわち、音のサンプルが与えられると、音のサンプルの音源の分類ラベル(たとえば、人間が読める意味論的記述)が出力される。
At 304, the
一例では、オーディオ分類は、オブジェクト(すなわち、オーディオ音源)が何であるかを示す確率評価を出力することができる。たとえば、バイオリンである音源の音のサンプルが提示された場合、オーディオ分類208は、オブジェクトが80%の確率でバイオリンであり、15%の確率でビオラであり、2%の確率でチェロであると出力し得る。一例では、オーディオ分類208は、最も可能性の高い(すなわち、最良の推測)オブジェクトタイプのみを出力することができる。たとえば、オーディオ分類は単に「バイオリン」と出力することができる。いくつかの例では、オーディオ分類はオブジェクトを識別できない場合がある。たとえば、オーディオ分類は「不明」(または、オーディオサンプルが分類できなかったことを示す他のラベル)と出力することができる。
In one example, the audio classifier can output a probability assessment indicating what the object (i.e., audio source) is. For example, when presented with a sample of a sound source that is a violin, the
306において、技法300は、任意で、ユーザから分類情報を受信することができる。たとえば、技法300は、ユーザインターフェースにおいて、識別されたおよび識別されていない音源のリストをユーザに提示することができる。ユーザインターフェースにおいて、ユーザは音源のラベルを選択して、関連付けられる(たとえば、識別された)音を再生することができる。ユーザは、識別された音源のうちのいくつかに割り当てられたラベルに修正をすることができる。説明のために、実際にはビオラである場合に、音源は「バイオリン」として識別される可能性がある。したがって、ユーザは音源に関連付けられているラベルを「ビオラ」に修正することができる。ユーザは、ラベルを識別されていない音源に割り当てることができる。説明のために、ピアノの音源が識別されていない可能性があるため、「不明」というラベルが割り当てられた。ユーザは、ラベル「不明」を「ピアノ」に変更することができる。
At 306,
図6は、音源分類(すなわち、識別)情報を示すユーザインターフェース600の例である。ユーザインターフェース600は、306において、技法300によって提示することができる。ユーザインターフェース600は、コンサートのオーディオセグメント602(たとえば、モノラルオーディオクリップ)が技法300に提示されたことを示す。ラベル604を出力して、オーディオセグメントにおいて識別された(音源分離を使用するなどして)音源のうちの少なくともいくつかに割り当てることができる。一例では、スペクトログラム614は、スペクトルが時間とともに変化するので、オーディオクリップの周波数のスペクトルを表示することができる。ユーザインターフェース600は、音源606が「バイオリン」であることを示している。一例では、それぞれの確実性指標612(たとえば、信頼レベル)は、音源の各々の分類に関連付けることができる。ユーザインターフェース600はまた、音源608および音源610が識別されなかったことを示している。したがって、デフォルトのラベル(たとえば、「Unknown_1」(不明_1)および「Unknown_2」(不明_2)など)が識別されていない音源に割り当てられた。
FIG. 6 is an example of a user interface 600 showing source classification (i.e., identification) information. The user interface 600 may be presented by the
他の例では、ユーザインターフェース600は、より少ないユーザ制御および/または情報、より多いユーザ制御および/または情報、他のユーザ制御および/または情報、あるいはそれらの組合せを含むことができる。たとえば、ユーザインターフェース600は、ラベル604の各々に隣接して、音源を示す代表的な画像を含むことができる。たとえば、ユーザインターフェース600は、ユーザがオーディオセグメントを再生、巻戻し、早送り、または一時停止することを可能にする制御を含むことができる。たとえば、ユーザインターフェース600は、ユーザが、たとえば、ラベル604のうちの1つを選択し、選択されたラベルに対応する音源のみを再生することを可能にする制御を含むことができ、それによって、ユーザは、選択された音源の識別を検証することができる。たとえば、ユーザインターフェース600は、ユーザがラベル604のうちのあるラベルを選択および修正することを可能にする制御を含むことができる。たとえば、音源610がフレンチホルンのものであると仮定すると、ユーザは、音源610を選択し、そのラベルを「フレンチホルン」に変更することができる。 In other examples, the user interface 600 may include fewer user controls and/or information, more user controls and/or information, other user controls and/or information, or combinations thereof. For example, the user interface 600 may include a representative image adjacent each of the labels 604 that indicates a sound source. For example, the user interface 600 may include controls that allow a user to play, rewind, fast forward, or pause an audio segment. For example, the user interface 600 may include controls that allow a user to select, for example, one of the labels 604 and play only the sound source that corresponds to the selected label, thereby allowing the user to verify the identity of the selected sound source. For example, the user interface 600 may include controls that allow a user to select and modify certain ones of the labels 604. For example, assuming that the sound source 610 is of a French horn, the user may select the sound source 610 and change its label to "French horn."
一例では、音源に複数のラベルが割り当てられ得る。たとえば、バイオリン音源は、「バイオリン」(音源606で示されている)および「弦楽器」ラベル(図示せず)というラベルに関連付けられている場合がある。一例では、ユーザは複数のラベルを音源に割り当てることができる。たとえば、「フレンチホルン」というラベルに加えて、ユーザは「金管楽器」というラベルを追加することもできる。 In one example, a sound source may be assigned multiple labels. For example, a violin sound source may be associated with the label "violin" (shown by sound source 606) and a "strings" label (not shown). In one example, a user may assign multiple labels to a sound source. For example, in addition to the label "French horn," a user may also add the label "brass."
一例では、ユーザは音源を拡散オーディオコンポーネントに割り当てることができる。拡散オーディオコンポーネントは、特定の方向を持たない音に関連付けられている。すなわち、拡散オーディオコンポーネントは、シーン内の特定の方向から開始されない1つまたは複数の音を含む。 In one example, a user can assign a sound source to a diffuse audio component. The diffuse audio component is associated with sounds that do not have a specific direction. That is, the diffuse audio component includes one or more sounds that do not originate from a specific direction in the scene.
図2に戻ると、上記のように、ビデオ202のビデオフレーム206は、オブジェクト分類210に入力される。オブジェクト分類210は、図4に関して説明されている。図4は、本開示の実装形態による、視覚的分類のための技法400の例のフローチャートである。技法400は、図2のオブジェクト分類210、または図1のオブジェクト認識装置108によって実装することができる。
Returning to FIG. 2, as described above, video frames 206 of
技法400は、ビデオフレーム206内の視覚オブジェクトを分類することと、識別された視覚オブジェクトの各々(または、少なくともいくつか)について、オブジェクトの座標を推定することと、識別された視覚オブジェクトの各々(または、少なくともいくつか)について、それぞれの動きベクトルを任意で推定することと、未分類のコンポーネントの各々(または、少なくともいくつか)について、ユーザから分類情報を任意で受信することとを含む。
The
402において、技法400は、ビデオフレーム206のフレームのうちの少なくともいくつかのフレーム内のオブジェクトを識別する。技法400は、ビデオフレーム206のフレームのうちの少なくともいくつかを分析して、シーンに存在する視覚オブジェクトを識別するために、オブジェクト認識装置を使用することができる。
At 402, the
一例では、オブジェクト認識装置は、画像内の多くの異なるオブジェクトを認識するようにトレーニングされた機械学習モデルであることができる。一例では、オブジェクト認識装置は、技法400によって直接実装(たとえば、実行)されない場合がある。むしろ、オブジェクト認識装置は、技法400によって使用され得る(たとえば、活用される、呼び出されるなど)サービスであり得る。たとえば、技法400は、ビデオフレーム206のうちの1つまたは複数のフレームをサービスに渡し、オブジェクト認識装置が識別した視覚オブジェクトに関する情報を受信することができる。
In one example, the object recognizer can be a machine learning model trained to recognize many different objects in an image. In one example, the object recognizer may not be directly implemented (e.g., executed) by
図7は、オブジェクト認識装置の出力の例700を示している。例700は、http://cloud.google.com/visionにおいて利用できるGoogle Cloud画像理解サービスの出力を示している。例700は、単なる一例である。他の出力および出力フォーマットも、技法400によって受信されることができる。
FIG. 7 illustrates an example 700 of the output of an object recognizer. Example 700 illustrates the output of the Google Cloud image understanding service available at http://cloud.google.com/vision. Example 700 is merely an example. Other outputs and output formats can also be received by
ビデオフレーム206のうちのフレーム702は、オブジェクト認識装置に提示される。オブジェクト認識装置は、リスト704内のフレーム702のオブジェクトを識別した。オブジェクト認識装置は、識別されたオブジェクトの各々の周囲に境界ポリゴン(たとえば、ボックス)を描画する。たとえば、境界ポリゴン706は、バイオリンの周りに描画される。オブジェクト認識装置はまた、リスト708に示されるように、認識されたオブジェクトにラベルを添付することができる。一例では、リスト704および/またはリスト708内のアイテムの各々は、関連付けられる確実性を有することができる。バイオリンに関して、例700のオブジェクト認識装置は、(リスト708において)「楽器」、「バイオリン」、「弦楽器」、「擦弦楽器」、「弦楽器」、および「バイオリン属」というラベルを識別した。 Frame 702 of the video frames 206 is presented to an object recognizer. The object recognizer has identified objects in frame 702 in list 704. The object recognizer draws bounding polygons (e.g., boxes) around each of the identified objects. For example, bounding polygon 706 is drawn around a violin. The object recognizer may also attach labels to the recognized objects, as shown in list 708. In one example, each of the items in list 704 and/or list 708 may have an associated certainty. For the violin, the object recognizer of example 700 has identified (in list 708) the labels "musical instrument," "violin," "stringed instrument," "bowed string instrument," "stringed instrument," and "violin genus."
技法400は、技法400が動作することができる機械可読データ構造においてオブジェクト認識装置からデータを受信する。データ構造710は、バイオリンオブジェクトに関して技法400によって受信され得る例示的なデータ構造を示している。データ構造710は、JavaScriptオブジェクト表記(JSON)データ交換フォーマットである。しかしながら、他のフォーマットも可能である。データ構造710の境界ポリゴン712は、バイオリンの境界ポリゴン座標を記述する。ラベル714は、境界ポリゴン712によって包含されるオブジェクトに割り当てられた人間が読めるラベル(すなわち、「バイオリン」)である。認識されたオブジェクトのうちのいくつかは音源ではない可能性があることに留意されたい。
状況によっては、オブジェクト認識装置がいくつかのオブジェクトを誤認する場合がある。たとえば、バイオリンが「チェロ」として認識されており、したがって、「チェロ」というラベルが付けられている可能性がある。状況によっては、オブジェクト認識装置は、オブジェクトがフレーム内に存在することを認識し得るが、オブジェクトを分類する(たとえば、ラベルを割り当てる)ことができない場合がある。たとえば、例700において、境界ポリゴン716のバイオリン奏者の衣服は、オブジェクトとして認識され得るが、ラベル718(すなわち、「Unknown_1」)が割り当てられる。 In some situations, the object recognizer may misidentify some objects. For example, a violin may be recognized as a "cello" and therefore labeled as "cello." In other situations, the object recognizer may recognize that an object is present in the frame, but may be unable to classify the object (e.g., assign a label). For example, in example 700, the violinist's clothing in bounding polygon 716 may be recognized as an object, but is assigned label 718 (i.e., "Unknown_1").
一例では、オブジェクトを識別するために、ビデオフレーム206のN番目ごとのフレームを分析することができる。すなわち、オブジェクトを識別するためにすべてのフレームを分析する必要はない。Nの値は、すべてのビデオフレーム206に対して固定することができる。たとえば、5番目ごとのフレームを分析することができる。Nは任意の値(たとえば、1、2など)にすることができる。一例では、Nはビデオのタイプによって異なる。たとえば、多くの動きを含むビデオフレーム206(たとえば、スポーツビデオ)において、かなり静的なビデオ(たとえば、ミュージシャンが舞台上であまり動かないコンサートのビデオ)よりも多くのフレームを分析することができる。
In one example, every Nth frame of the video frames 206 may be analyzed to identify an object. That is, it is not necessary to analyze every frame to identify an object. The value of N may be fixed for all video frames 206. For example, every fifth frame may be analyzed. N may be any value (e.g., 1, 2, etc.). In one example, N varies depending on the type of video. For example, in a
図4に戻ると、404において、技法400は、認識されたオブジェクトのうちの少なくともいくつかに対するそれぞれの空間座標を推定する。一例では、境界ポリゴンのそれぞれの中心が使用される。すなわち、後で音の位置および/または方向を音に割り当てるために、音を放出するオブジェクトの境界ポリゴンの中心が音の音源の位置として使用される。他の例では、境界ポリゴンに関連付けられる異なる位置を使用することができる。
Returning to FIG. 4, at 404,
いくつかの実装形態では、技法400は、動きベクトルを推定するステップを含むことができる。すなわち、406において、技法400は、認識されたオブジェクトのうちの少なくともいくつかについて、それぞれの動きベクトルを推定することができる。動きベクトルを推定するための任意の適切な技法を使用することができる。視覚オブジェクト(したがって、同等に、対応する音)の位置が時間の経過とともにどのように変化するかを追跡するために、動きベクトルを使用することができる。動きベクトルの推定は、一般に、フレーム間の差異を決定することになり得る。たとえば、犬は、第1の位置を中心とする位置の第1のフレームにおいて識別され、第2の位置を中心とする位置の第2のフレームにおいて識別され得る。したがって、犬の動きベクトルは、第1の位置と第2の位置との間の差(たとえば、変位)であり得る。
In some implementations,
一例では、隠されたオブジェクトに音の位置を割り当てるために、動きベクトルを使用することができる。たとえば、ビデオフレーム206の第1のサブセットにおいて、音源であるオブジェクトが見えていた。しかしながら、ビデオフレーム206の第2のサブセットにおいて、オブジェクトは隠されたが、依然として音を発していた。ビデオフレーム206の第2のサブセット内の1つまたは複数の位置をオブジェクトに割り当てるために、フレーム(必ずしも連続するフレームである必要はない)の第1のサブセット内のオブジェクトについて推定された動きベクトルを使用することができる。一例では、オブジェクトが長期間にわたって隠されている場合、オブジェクトに関連付けられる音イベントを拡散音場に割り当てることができる。 In one example, motion vectors can be used to assign sound locations to hidden objects. For example, in a first subset of video frames 206, an object was visible that was the source of sound. However, in a second subset of video frames 206, the object was hidden but still emitted sound. The motion vector estimated for the object in the first subset of frames (not necessarily consecutive frames) can be used to assign one or more locations in the second subset of video frames 206 to the object. In one example, if an object is hidden for an extended period of time, sound events associated with the object can be assigned to a diffuse sound field.
一例では、ビデオフレーム206は、全天球ビデオのフレームであってよい。したがって、オブジェクトが第1の視点(たとえば、第1のカメラのビュー)から第2の視点(たとえば、第2のカメラのビュー)にいつ移動するかを識別するために、動きベクトルを使用することができる。したがって、音の位置は、第1のビュー内の位置から第2のビューの位置に移動することができる。別の例では、オブジェクトが、球面カメラの任意の視点のビューから外れて移動する可能性があるが、それでも音を発している可能性がある。オブジェクトを拡散音場に割り当てることができる。
In one example, the
一例では、モーションベクトルは推定されない。音源に関連付けられる音の位置は、(動きベクトルに従って連続的にではなく)個別に変更することができる。たとえば、15フレームのシーケンスにおいて、第1のフレームと10番目のフレームが分析されると仮定する。オブジェクトは、第1のフレームにおいて第1の位置、および、10番目のフレームの第2の位置において識別される。第1の位置はフレーム1～9の音源として割り当てることができ、第2の位置はフレーム10～15の音源として割り当てることができる。別の例では、音の位置は、第1の位置および第2の位置の補間または外挿のように、各フレームにおいて割り当てることができる。 In one example, motion vectors are not estimated. The sound location associated with the sound source can be changed individually (rather than continuously according to a motion vector). For example, assume that in a sequence of 15 frames, the first and tenth frames are analyzed. An object is identified in the first frame at a first location and in the tenth frame at a second location. The first location can be assigned as the sound source for frames 1-9, and the second location can be assigned as the sound source for frames 10-15. In another example, the sound location can be assigned in each frame, such as an interpolation or extrapolation of the first and second locations.
一例では、技法400は、任意で、ユーザから分類情報を受信することができる(408において)。
In one example,
たとえば、技法400は、ユーザインターフェースにおいて、識別されたオブジェクトおよび識別されていないオブジェクト(視覚オブジェクト)のリストをユーザに提示することができる。ユーザインターフェースにおいて、ユーザは識別されたオブジェクトのラベルを選択することができる。ユーザは、識別されたオブジェクトのうちのいくつかに割り当てられたラベルを修正することができる。ユーザインターフェースは、図7に関して説明したものと同様であり得る。一例では、ユーザインターフェースは、ユーザがビデオフレーム206内で前後に移動することを可能にする制御を含むことができる。ユーザインターフェースは、境界ポリゴンを含むことができる。一例では、ユーザインターフェースに識別されていないオブジェクト(存在する場合)を含むことができる。ユーザは、識別されていないオブジェクトを選択し、1つまたは複数のラベルを識別されていないオブジェクトに割り当てることができる。一例では、「不明」というラベル(または、視覚オブジェクトを分類できなかったことを示す他の何らかのラベル)を識別されていないオブジェクトに割り当てることができる。一例では、オブジェクトを識別し、1つまたは複数のラベルをユーザによって識別されるオブジェクトに割り当てるために、ユーザはフレームの領域の周りに境界ポリゴンを描画することができる。
For example, the
再び図2に戻ると、オーディオ分類208およびオブジェクト分類210の結果は、一致212に組み合わされている。一致212は、図1のマッチャ110によって実装することができる。一致212は、図5に関して説明されている。図5は、本開示の実装形態による、オーディオオブジェクトと視覚オブジェクトを一致させるための技法500の例のフローチャートである。技法500は、図2の一致212、または図1のマッチャ110によって実装することができる。
Returning again to FIG. 2, the results of
502において、技法500は、聴覚オブジェクトを視覚オブジェクトに、またはその逆にマッピングする。一例では、技法500は、識別されたオーディオオブジェクトを識別された視覚オブジェクトに一致させる自動プロセスから開始することができる。
At 502,
聴覚オブジェクトと視覚オブジェクトは、文字列一致を使用してマッピングすることができる。たとえば、図6の音源606は、両方のオブジェクト(すなわち、聴覚オブジェクトおよび視覚オブジェクト)に「バイオリン」というラベルが付いているので、図7のデータ構造710によって表されるオブジェクトにマッピングすることができる。オーディオオブジェクトおよび視覚オブジェクトは、意味論的一致を使用してマッピングすることができる。たとえば、図6のバイオリンが「擦弦楽器」としてのみ識別されたと仮定する。「バイオリン」が「擦弦楽器」の一種であることを識別するために、分類学を使用することができる。したがって、技法500は、聴覚オブジェクトであるバイオリンを視覚オブジェクトである「擦弦楽器」にマッピングすることができる。視覚オブジェクトを聴覚オブジェクトに自動的にマッピングする他の方法も可能である。
Auditory and visual objects can be mapped using string matching. For example, sound source 606 in FIG. 6 can be mapped to the object represented by data structure 710 in FIG. 7 because both objects (i.e., the auditory object and the visual object) are labeled "violin". Audio and visual objects can be mapped using semantic matching. For example, assume that the violin in FIG. 6 was identified only as a "bowed string instrument". Taxonomy can be used to identify that "violin" is a type of "bowed string instrument". Thus,
聴覚オブジェクトと視覚オブジェクトとの間のマッピングを識別するために、他のヒューリスティックを使用することができる。たとえば、マッピングは、視覚オブジェクトのサイズと音源の周波数に基づいて識別することができる。たとえば、図2のオブジェクト分類210は、実際にはオブジェクトがトラックである場合に、オブジェクトを「車」として識別した可能性がある。オーディオのスペクトルが低周波数成分(トラックの音のプロファイルと一致する)を含む場合、車とトラックが意味論的にリンクされているため(たとえば、両方とも車両の例である)、「車」として識別されたオブジェクトは、低周波数に対応する音源に一致する(すなわち、マッピングする)ことができる。
Other heuristics can be used to identify the mapping between auditory and visual objects. For example, the mapping can be identified based on the size of the visual object and the frequency of the sound source. For example, the
一例では、技法500は、一致しないオブジェクトを拡散音信号に割り当てる、および/または一致しないオーディオイベントを残留オーディオ信号に割り当てることができる。残留オーディオ信号は、一致しないオーディオイベントのセットに対応する可能性がある。以下に説明するように、残留オーディオ信号は、ユーザが残留オーディオ信号のオーディオイベントの処理を決定できるように、ユーザに提示することができる。
In one example,
一例では、明らかな誤分類のケースを除外するために、ステレオレコーディングからの部分的な空間情報を使用することができる。たとえば、左のパノラマにおける聴覚イベントは、右半球において視覚的表現を有することができない。 In one example, partial spatial information from stereo recordings can be used to rule out cases of apparent misclassification. For example, an auditory event in the left panorama may not have a visual representation in the right hemisphere.
状況によっては、自動一致が正確ではない(すなわち、不一致)こともあり、行われるべき一致が行われない(つまり、一致しない)がある。不一致および/または一致しないことは、ユーザの介入を使用して解決することができる。たとえば、技法500は、ユーザが利用可能な視覚オブジェクトを閲覧し、聴覚オブジェクトのうちのいずれかが視覚オブジェクトによりよく一致することができるかどうかを決定することができるユーザインターフェースをユーザに提示することができる。ユーザは、音イベントを拡散音場に割り当てることができる。ユーザは、音を視覚イベントに割り当てることなく、音イベントを方向(たとえば、位置)に割り当てることができる。ユーザは、音イベントを視覚オブジェクトにマッピングすることができる。
In some situations, the automatic match may not be accurate (i.e., a mismatch) and a match that should be made is not made (i.e., a mismatch). The mismatch and/or mismatch can be resolved using user intervention. For example,
504において、マッピングされた聴覚イベントの各々について、技法500は、空間座標をマッピングされた聴覚イベントに割り当てる。たとえば、一致した視覚オブジェクトの境界ポリゴンの中心である座標を、聴覚イベントに割り当てることができる。
At 504, for each mapped auditory event,
506において、技法500は、上記のように、隠されたオブジェクトの最も可能性の高い現在の方向を推定するために、任意で、動きベクトルおよび/または補間を使用することができる。隠されたオブジェクトは、シーン内にあるが、シーン内の他のオブジェクトによって隠されている視覚オブジェクトであり得る。隠されたオブジェクトは、シーンを出た視覚オブジェクトであり得る。
At 506,
一例では、聴覚イベント(マッピングされた、またはマッピングされていない)の場合、ユーザは、音イベントの音源(すなわち、位置)として割り当てられるべき画面位置(たとえば、ビデオフレーム206のフレーム内のポイント)を割り当てることができる。ユーザはまた、異なるフレームを選択して、ソース聴覚イベントとして別の位置を選択することができる。第1のフレームと第2のフレームとの間のフレームの各々における聴覚イベントの位置は、上記のように割り当てることができる。 In one example, for an auditory event (mapped or unmapped), the user can assign a screen location (e.g., a point within a frame of video frame 206) to be assigned as the source (i.e., location) of the sound event. The user can also select a different frame to select another location as the source auditory event. The location of the auditory event in each of the frames between the first and second frames can be assigned as described above.
図2に戻ると、空間情報214が生成される(たとえば、出力)。
Returning to FIG. 2,
図1のオーディオ空間化装置112は、オーディオイベントを空間化およびアップミックスして、空間オーディオを生成するために、抽出された空間メタデータ(たとえば、空間的位置)を備えたオーディオ信号を使用する。たとえば、上記の手法のうちの1つ(たとえば、オブジェクトベース、チャネルベース、またはシーンベースの手法)を使用することができる。
The
たとえば、シーンベースの手法(すなわち、高次アンビソニックス再生)を使用して、各指向性オーディオイベントが空間化され(すなわち、必要な順序でHOA表現に符号化され)、すべての空間化されたオーディオイベントは、単一の音場表現に混合される。そのような空間オーディオ表現においては、音イベントを球面調和関数で表すことができる。たとえば、HOA信号の各々は、音イベントに対応し、球面調和関数によって重み付けされた、抽出された(たとえば、分離されたなどの)モノラルオーディオ信号で構成することができ、これらの信号は、所望の音イベントの位置に対応する角度で評価される。 For example, using a scene-based approach (i.e., higher-order Ambisonics reproduction), each directional audio event is spatialized (i.e., encoded into a HOA representation in the required order) and all spatialized audio events are mixed into a single sound field representation. In such a spatial audio representation, the sound events can be represented by spherical harmonic functions. For example, each HOA signal can consist of extracted (e.g., separated, etc.) mono audio signals corresponding to a sound event and weighted by a spherical harmonic function, which are evaluated at an angle corresponding to the position of the desired sound event.
残りの残留オーディオ信号は、拡散(すなわち、無指向性)オーディオストリームであると仮定することができる。残りの残留オーディオ信号は、指向性オーディオストリームとは別に処理することができる。図1の拡散フィールドシンセサイザ114は、残りの残留オーディオ信号を処理する。残留オーディオ信号を拡散音場として処理するために、任意の数の利用可能な技法を使用することができる。一例では、拡散オーディオストリームは、いくつかの非相関フィルタを通過し(たとえば、一時的に拡散するインパルス(Temporarily Diffuse Impulses)を使用して)、上記のHOAチャネルに等しく追加することができる。 The remaining residual audio signal may be assumed to be a diffuse (i.e., omnidirectional) audio stream. The remaining residual audio signal may be processed separately from the directional audio stream. The diffuse field synthesizer 114 of FIG. 1 processes the remaining residual audio signal. Any number of available techniques may be used to process the residual audio signal as a diffuse sound field. In one example, the diffuse audio stream may be passed through several decorrelation filters (e.g., using Temporarily Diffuse Impulses) and added equally to the HOA channels described above.
非相関の各拡散オーディオストリームは、以前に符号化された指向性オーディオストリームに追加することができる。したがって、空間情報214は、1つの全方位およびN-1(それぞれ、指向性音イベントに対応する)指向性チャネルを含むことができる。
Each uncorrelated diffuse audio stream can be added to a previously encoded directional audio stream. Thus, the
図8は、本開示の実装形態による、ビデオ情報に基づく空間オーディオ拡張のための技法800のフローチャートの例である。技法800は、図1～図7に関して上述したステップの別の見方および/または詳細を提示する。技法800は、図1のシステム100などのシステムによって実装することができる。
FIG. 8 is an example flowchart of a
ビデオフレーム802(すなわち、視覚ストリーム)およびオーディオセグメント806を含むビデオ801が受信される。ビデオ801は、ビデオフレーム802およびオーディオセグメント806を取得するために逆多重化される。一例では、ビデオフレーム802およびオーディオセグメント806は、ビデオ801の対応する部分であり得る。たとえば、ビデオフレーム802およびオーディオセグメント806は、ビデオ801のシーンを構成するビデオ801の一部であり得る。
A
視覚オブジェクト804のリストは、図2のオブジェクト分類210に関して上記で説明したように、ビデオフレーム802のフレームから取得することができる。視覚オブジェクト804のリストは、識別されたオブジェクトのラベルを含む。視覚オブジェクト804のリストは、フレームの視覚オブジェクトの各々のラベルを含まない場合がある。さらに、視覚オブジェクトのラベルが識別されない場合がある。視覚オブジェクト804のリストは、識別されラベル付けされた視覚オブジェクト(たとえば、V_OBJECT_1、V_OBJECT_2、およびV_OBJECT_3)、および識別されたがラベル付けされていない視覚オブジェクト(たとえば、V_UNKNOWN_1およびV_UNKNOWN_2)を含むものとして示されている。上記のように、ユーザは、ラベルを追加することと、ラベルを修正することと、および/または視覚オブジェクト804のリストからラベルを削除することとを行うことができる。
The list of
上記のように、動きベクトルおよび/または空間情報819は、ビデオフレーム802および視覚オブジェクト804のリストを使用して取得することができる。たとえば、視覚オブジェクト804のリストのオブジェクトのそれぞれの境界ポリゴンの中心は、視覚オブジェクトに関連付けられる空間情報として使用することができる。
As described above, the motion vectors and/or
聴覚イベントのリスト(すなわち、聴覚オブジェクト810のリスト)は、オーディオセグメント806から取得することができる。上記のように、オーディオセグメント806は、音源分離モジュール808に提示することができる。次いで、分離された音源の各々は、図2のオーディオ分類208で説明したように、分類のために提示することができる。分類されたオブジェクトは、聴覚オブジェクト810のリストに収集される。聴覚オブジェクト810のリストは、識別されたオブジェクト(すなわち、識別された音)のラベルを含む。聴覚オブジェクト810のリストは、オーディオセグメント806の聴覚オブジェクトの各々のラベルを含まない場合がある。さらに、聴覚イベントのラベルを識別されない場合がある。聴覚オブジェクト810のリストは、識別されラベル付けされた聴覚イベント(たとえば、A_OBJECT_1およびA_OBJECT_2)、および識別されているがラベル付けされていない聴覚イベント(たとえば、A_UNKNOWN_1およびA_UNKNOWN_2)を含むものとして示されている。上記のように、ユーザは、ラベルを追加することと、ラベルを修正することと、および/または聴覚オブジェクト810のリストからラベルを削除することとを行うことができる。オーディオセグメント806に特定の音源を有していないオーディオイベントは、残留オーディオ信号812に割り当てることができる。
A list of auditory events (i.e., a list of auditory objects 810) may be obtained from the
自動一致814(すなわち、自動化された一致)のために、視覚オブジェクト804のリストおよび聴覚オブジェクト810のリストが提供される(たとえば、入力されるなど)。自動一致は、図2の一致212に関して上記のように実行することができる。
For automatic matching 814 (i.e., automated matching), a list of
上記のように、手動一致816は、ユーザによって実行することができる。たとえば、ユーザは、聴覚オブジェクトのリスト810のマッピングされた聴覚オブジェクトを、視覚オブジェクトのリスト804の異なる視覚オブジェクトにマッピングすることができる。たとえば、ユーザはマッピングされていない聴覚オブジェクトを視覚オブジェクトにマッピングすることができる。たとえば、ユーザは空間座標(たとえば、位置)をオーディオ音源(たとえば、聴覚イベント)に割り当てることができる。これは、たとえば、聴覚イベントに対応する視覚オブジェクトがオブジェクト認識装置によって識別されなかったが、ユーザが、視覚オブジェクトが聴覚イベントのソースであると確信している場合に役立つ。たとえば、ユーザはマッピングされた聴覚オブジェクトのマッピングを解除し、それを拡散音場に割り当てることができる。
As described above, manual matching 816 can be performed by a user. For example, a user can map a mapped auditory object in the list of
自動一致814および手動一致816の後、ビデオフレーム802内のどの視覚オブジェクトにもマッピングされていないいくつかの聴覚オブジェクトが依然として存在し得る。そのようなオブジェクトは、オーディオ残留818と呼ばれる。オーディオ残留818は、シーンにおいて見えない可能性があるが特定の方向から発生するオブジェクトに対応する第1の聴覚イベントを含むことができる。オーディオ残留818は、特定の方向から発生せず、したがって拡散音である第2の聴覚イベントを含むことができる。したがって、ユーザは、オーディオ残留818のどの聴覚イベントが指向性音イベントであり、どれが拡散音であるかを選択する。ユーザは、視界外の指向性位置を第1の聴覚イベントのうちの少なくともいくつかに割り当てることができる。
After
任意の指向性音がオーディオ空間化装置820に提供される。オーディオ空間化装置820は、空間化手法(たとえば、オブジェクトベース、チャネルベース、またはシーンベースの手法)に従って、動きベクトルおよび/または空間情報819を使用して、指向性聴覚イベントとして識別される聴覚イベントのうちのいくつかを空間化することができる。オーディオ空間化装置820は、第1の聴覚イベントに関してユーザによって提供された指向性位置を使用することができる。
Any directional sounds are provided to an
残留オーディオ信号812およびオーディオ残留818において拡散音イベントとして識別された任意の聴覚イベントは、上記のように、拡散フィールドシンセサイザ822によって処理される。しかしながら、ある方向から拡散音を聞くことが望まれる(たとえば、ユーザによって)場合、拡散音は、オーディオ空間化装置820に供給され得る(たとえば、オーディオ空間化装置820に提供される、入力される、方向付けられる、オーディオ空間化装置820によって処理されるなど)。オーディオ空間化装置820および拡散フィールドシンセサイザ822の出力は、オーディオ出力824に結合される。たとえば、オーディオ出力824は、ステレオファイル、マルチチャネルファイル、またはシーンベースの表現(たとえば、アンビソニックス)、オブジェクト表現ファイルなどであり得る。状況によっては、ステレオファイルに(すなわち、ステレオフォーマットで)保存すると、音源(たとえば、原音)のモノラルオーディオレコーディングよりも改善される場合がある。たとえば、モノラルレコーディングとしてキャプチャされた音楽コンサートについて考えてみる。本明細書で説明されるビデオ情報に基づく空間拡張は、ステレオパノラマ内でミュージシャン(すなわち、ミュージシャンの音イベント)を動かして、ステレオ音楽出力をもたらすのに役立つ可能性がある。上記のように、ステレオは特定の音イベントを誤って表現する場合があるが、依然としてステレオ出力はモノラル音楽レコーディングよりも改善される可能性がある。360°情報を2つのチャネルに符号化するが、ヘッドホン(または、スピーカであるが、チャネル間クロストークなし)での再生が必要なバイノーラルステレオは、ステレオフォーマットでの保存が元のモノラルレコーディングよりも有益な別の例であり得る。
Any auditory events identified as diffuse sound events in the
図9は、空間情報をオーディオセグメントに割り当てるための技法900の例のフローチャートである。技法900のうちの少なくともいくつかの態様は、図1のシステム100のモジュールのうちの1つまたは複数によって実装することができる。技法900のうちの少なくともいくつかの態様は、図2に関して説明したように、部分的または完全に実装することができる。
FIG. 9 is a flow chart of an
技法900は、モノラルオーディオセグメントを受信し、空間情報をオーディオセグメントの少なくとも1つの聴覚イベントに割り当てる。技法900は、空間情報を割り当てるために、オーディオセグメントに対応するビデオフレーム内の視覚情報を使用する。
The
902において、技法900は、第1のオーディオセグメントを受信する。第1のオーディオセグメントは非空間化されている。すなわち、第1のオーディオセグメントにおける聴覚イベントに関して利用できる空間情報はない。第1のオーディオセグメントは、第1のビデオフレームに関連付けられている。
At 902, the
第1のオーディオセグメントは、ネットワークを介して、ケーブルを介して、第1のオーディオセグメントを受信することによって、あるいは、プライマリメモリ、またはディスクドライブもしくはコンパクトフラッシュ(登録商標)(CF)カード、セキュアデジタル(SD)カードなどのリムーバブルメディアを含む他のストレージデバイスからオーディオセグメントを読み取ることなどによって任意の数の方法で受信することができる。第1のオーディオセグメントは、第1のオーディオセグメントおよび第1のビデオフレームの両方を含むストリーム(たとえば、ファイル)において受信することができる。一例では、ストリームのオーディオトラックとビデオトラックを逆多重化することができる。第1のオーディオセグメントと第1のビデオフレームは、ストリームのシーンに対応することができる。一例では、ビデオは全天球ビデオであり得る。 The first audio segment may be received in any number of ways, such as by receiving the first audio segment over a network, over a cable, or by reading the audio segment from primary memory or other storage device, including a disk drive or removable media, such as a Compact Flash (CF) card, a Secure Digital (SD) card, etc. The first audio segment may be received in a stream (e.g., a file) that includes both the first audio segment and the first video frame. In one example, the audio track and the video track of the stream may be demultiplexed. The first audio segment and the first video frame may correspond to a scene of the stream. In one example, the video may be a spherical video.
904において、技法900は、第1のビデオフレーム内の視覚オブジェクトを識別する。視覚オブジェクトは、図1のオブジェクト認識装置108などのオブジェクト認識モジュールによって識別することができる。視覚オブジェクトは、図2のオブジェクト分類210に関して説明したように識別することができる。一例では、視覚オブジェクトを識別するステップは、第1のビデオフレームにおける視覚オブジェクトを識別するために、画像認識を使用するステップを含むことができる。
At 904, the
906において、技法900は、第1のオーディオセグメントにおける聴覚イベントを識別する。聴覚イベントは、図1のオーディオ分類装置106などのオーディオ分類装置106によって識別することができる。聴覚イベントは、図2のオーディオ分類208に関して説明したように識別することができる。一例では、聴覚イベントを識別するステップは、それぞれが聴覚イベントに対応する複数のトラックに第1のオーディオセグメントを分解することによって、第1のオーディオセグメントにおける聴覚イベントを識別するために、ブラインド音源分離を使用するステップを含み得る。
At 906,
908において、技法900は、視覚オブジェクトのうちのある視覚オブジェクトと聴覚イベントのうちのある聴覚イベントとの間の一致を識別する。一致は、図8の自動一致814に関して説明したように、自動的に識別され得る。一致は、図8の手動一致816に関して説明したように、手動で識別され得る。
At 908, the
910において、技法900は、視覚オブジェクトの位置に基づいて、空間的位置を聴覚イベントに割り当てる。空間的位置は、図8のオーディオ空間化装置820に説明されているように割り当てることができる。
At 910, the
一例では、技法900は、図2の空間情報214、または図8のオーディオ出力824に関して説明したように、聴覚イベントと聴覚イベントの空間的位置とを備えるオーディオ出力を生成するステップを含むことができる。
In one example,
一例では、技法900は、一致しない聴覚イベントを識別するステップと、一致しない聴覚イベントをユーザインターフェースにおいて提示するステップとを含むことができる。一致しない聴覚イベントは、第1のビデオフレームにおいて識別された視覚オブジェクトと一致しない聴覚イベントであり得る。たとえば、一致しない聴覚イベントは、図8のオーディオ残留818の聴覚イベントであり得る。一例では、一致しない聴覚イベントは、図8の手動一致816に関して説明したように、一致しない聴覚イベントまたは不一致の聴覚イベントであり得る。したがって、一例では、技法900は、ユーザから、第1のビデオフレームにおいて識別された視覚オブジェクトのうちのある視覚オブジェクトへの一致しない聴覚イベントの割当て(すなわち、マッピング)を受信するステップを含むことができる。
In one example,
一例では、技法900は、ユーザから、一致しないオーディオイベントを拡散音として割り当てるための指示を受信するステップを含むことができる。指示はユーザ入力にすることができる。ユーザは、図8のオーディオ残留818に関して説明したように、一致しないオーディオイベントを拡散音場に割り当てることができる。
In one example,
一例では、技法900は、図8のオーディオ残留818に関して説明したように、ユーザから、一致しない音の一致しない聴覚イベントを指向性音として割り当てるための指示、および一致しない聴覚イベントの空間方向を受信するステップを含むことができる。
In one example,
一例では、技法900は、聴覚イベントを含む第2のオーディオセグメントを受信するステップと、視覚オブジェクトを含まない第2のビデオフレームを受信するステップと、第1のビデオフレームの少なくともサブセットに少なくとも部分的に基づいて視覚オブジェクトの動きベクトルを決定するステップと、動きベクトルに基づいて、周囲空間的位置を聴覚イベントに割り当てるステップとを含むことができる。
In one example,
一例では、技法900は、聴覚イベントを含む第2のオーディオセグメントを受信するステップと、視覚オブジェクトを含まない第2のビデオフレームを受信するステップと、第1のビデオセグメントと第2のビデオセグメントとの間の時間差に基づいて、周囲空間的位置または拡散位置のうちの1つを聴覚イベントに割り当てるステップとを含み得る。たとえば、話しており、シーンにおいて見えている人が見えなくなったが、まだ話しているシナリオを考えてみる。その人が見えなくなってからの時間が長くなるほど、その人の位置は予測しにくくなる可能性がある。たとえば、人が見えなくなったときにその人の位置を推定するために、最初は動きベクトルを使用することができるが、より長期間後に人の位置を予測し続けるために、動きベクトルを妥当に使用することはできない。たとえば、人が最初にシーンの左側から退出して、最初に周囲音の方向が割り当てられている間に、その人はカメラの周り(たとえば、後ろ)を回ったり、カーテンの後ろに隠れたりした可能性がある。したがって、一定の時間(たとえば、2秒など)の後、人の音を拡散音場に割り当てることができる。
In one example,
図10は、空間情報をオーディオセグメントに割り当てるための技法1000の別の例のフローチャートである。技法1000のうちの少なくともいくつかの態様は、図1のシステム100のモジュールのうちの1つまたは複数によって実装することができる。技法1000のうちの少なくともいくつかの態様は、図2に関して説明したように、部分的または完全に実装することができる。
FIG. 10 is a flow chart of another example of a
技法1000は、モノラルオーディオセグメントを受信し、空間情報をオーディオセグメントのうちの少なくとも1つの聴覚イベントに割り当てる。技法1000は、空間情報を割り当てるために、オーディオセグメントに対応するビデオフレーム内の視覚情報を使用する。
The
1002において、技法1000は、上述のように、オーディオトラックとビデオフレームを取得するためにビデオを逆多重化する。1004において、技法1000は、それぞれの視覚ラベルをビデオフレーム内の視覚オブジェクトに割り当てる。一例では、技法1000は、視覚ラベルを割り当てるために、画像認識および/またはオブジェクト分類を使用する。1006において、技法1000は、オーディオトラックを複数のトラックに分割する。一例では、技法1000は、複数のトラックを取得するために、音源分離(ブラインドオーディオ音源分離など)を使用する。1008において、技法1000は、それぞれのオーディオラベルを複数のトラックに割り当てる。一例では、そして上記のように、技法1000は、それぞれのオーディオラベルを割り当てるために、オーディオ分類装置106などのオーディオ分類装置を使用することができる。
At 1002, the
1010において、技法1000は、図8の自動一致814に関して上で説明したように、オーディオラベルのうちの少なくともいくつかを視覚ラベルのうちのいくつかに自動的に一致させる。1012において、技法1000は、図8の運動ベクトルおよび/または空間情報819に関して上で説明したように、視覚オブジェクトのうちのいくつかのそれぞれの位置に基づいて、それぞれの空間的位置をオーディオラベルのうちのいくつかに割り当てる。
At 1010,
一例では、技法1000は、一致しないオーディオラベルに対応する残留トラックを識別するステップと、ユーザに、ディスプレイにおいて残留トラックを表示するステップとを含むことができる。一例では、技法1000は、残留トラックのうちのある残留トラックの拡散音場への第1の割当て、ビデオフレームの任意の空間的位置への前記ある残留トラックの第2の割当て、周囲音としての前記ある残留トラックの第3の割当て、または、ビデオフレーム内の視覚オブジェクトへの前記ある残留トラックの第4の割当てのうちの少なくとも1つを、ユーザから受信するステップを含むことができる。
In one example,
図11は、空間情報をオーディオセグメントに割り当てるための技法1100のさらに別の例のフローチャートである。技法1100のうちの少なくともいくつかの態様は、図1のシステム100のモジュールのうちの1つまたは複数によって実装することができる。技法1100のうちの少なくともいくつかの態様は、図2に関して説明したように、部分的または完全に実装することができる。
FIG. 11 is a flowchart of yet another example of a
1102において、技法1100は、非空間化されており、第1のビデオフレームに関連付けられている、第1のオーディオセグメントを受信する。第1のオーディオセグメントは、モノラルオーディオセグメントであることができる。第1のオーディオセグメントは、図9の902に関して説明したように受信することができる。1104において、技法1100は、第1のビデオフレーム内の視覚オブジェクトを識別する。視覚オブジェクトを識別するステップは、図9の904に関して説明した通りであり得る。1106において、技法1100は、第1のオーディオセグメントにおける聴覚イベントを識別する。聴覚イベントを識別するステップは、図9の906に関して説明した通りであり得る。
At 1102, the
1108において、技法1100は、視覚オブジェクトのうちのある視覚オブジェクトと聴覚イベントのうちのある聴覚イベントとの間に一致があるかどうかを決定する。一致がある場合、技法1100は1110に進み、空間情報を聴覚イベントに割り当てる。一致がない場合、技法1100は1112に進み、聴覚イベントを拡散音場に割り当てる。
At 1108,
一例では空間情報を聴覚イベントに割り当てるステップは、視覚オブジェクトの位置に基づいて、空間的位置を聴覚イベントに割り当てるステップを含むことができる。一例では、空間的位置は、図5の504に関して上記で説明したように、視覚オブジェクトの境界ポリゴンの中心であり得る。 In one example, assigning spatial information to the auditory event may include assigning a spatial location to the auditory event based on a location of a visual object. In one example, the spatial location may be a center of a bounding polygon of the visual object, as described above with respect to 504 of FIG. 5.
一例では、技法1100は、聴覚イベントと聴覚イベントの空間的位置とを含むオーディオファイルを生成するステップを含むことができる。一例では、オーディオファイルを生成するステップは、聴覚イベントと聴覚イベントに関連する拡散音情報とを含むオーディオファイルを生成するステップを含むことができる。オーディオファイルを生成するステップは、図8のオーディオ出力824に関して説明した通りであり得る。
In one example,
説明を簡単にするために、技法200、300、400、500、800、900、1000、および1100はそれぞれ、一連のブロック、ステップ、または動作として描写および説明されている。しかしながら、本開示によるブロック、ステップ、または動作は、様々な順序で、および/または同時に発生する可能性がある。さらに、本明細書に提示および記載されていない他のステップまたは操作も使用され得る。さらに、開示された主題に従って技法を実装するために、図示されたすべてのステップまたは動作が必要とされるわけではない。
For ease of explanation,
「例」または「実装形態」という言葉は、本明細書では、例、実例、または例示として機能することを意味するために使用される。本明細書で「例」または「実装形態」として説明される態様または設計は、必ずしも他の態様または設計よりも好ましいまたは有利であると解釈されるべきではない。むしろ、「例」または「実装形態」という言葉の使用は、概念を具体的に提示することを目的としている。本明細書で使用されているように、「または」という用語は、排他的「または」ではなく、包括的「または」を意味することを意図している。すなわち、特に明記されていない限り、または文脈から明らかでない限り、「XはAまたはBを含む」は、自然な包括的順列のいずれかを意味することを意図している。すなわち、XがAを含む場合、XがBを含む場合、または、XがAとBの両方を含む場合、前述の実例のすべてにおいて「XはAまたはBを含む」が満たされる。さらに、本出願および添付の特許請求の範囲で使用される冠詞「a」および「an」は、特に明記されていない限り、または文脈から明らかに単数形に向けられない限り、一般に「1つまたは複数」を意味すると解釈されるべきである。さらに、全体を通して「実装形態」または「一実装形態」という用語の使用は、そのように説明されない限り、同じ実施形態または実装形態を意味することを意図するものではない。 The word "example" or "implementation" is used herein to mean serving as an example, illustration, or illustration. An aspect or design described herein as an "example" or "implementation" should not necessarily be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" or "implementation" is intended to present a concept in a concrete manner. As used herein, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or." That is, unless otherwise specified or clear from the context, "X includes A or B" is intended to mean any of the natural inclusive permutations. That is, "X includes A or B" is satisfied in all of the foregoing examples if X includes A, if X includes B, or if X includes both A and B. Additionally, the articles "a" and "an" used in this application and the appended claims should generally be construed to mean "one or more" unless otherwise specified or clearly directed to the singular form from the context. Additionally, use of the term "implementation" or "one implementation" throughout is not intended to refer to the same embodiment or implementation unless so described.
システム100の実装形態(および、その上に記憶され、および/またはそれによって実行される、技法200、300、400、500、800、900、1000、および/または1100を含む、アルゴリズム、方法、命令など)は、ハードウェア、ソフトウェア、またはそれらの任意の組合せにおいて実現することができる。ハードウェアは、たとえば、コンピュータ、知的財産(IP)コア、特定用途向け集積回路(ASIC)、プログラム可能なロジックアレイ、光プロセッサ、プログラム可能なロジックコントローラ、マイクロコード、マイクロコントローラ、サーバ、マイクロプロセッサ、デジタル信号プロセッサ、または任意の他の適切な回路を含むことができる。特許請求の範囲において、「プロセッサ」という用語は、単独でまたは組み合わせて、前述のハードウェアのうちのいずれかを包含すると理解されるべきである。「信号」および「データ」という用語は互換的に使用される。さらに、システム100の部分は、必ずしも同じ方法で実装される必要はない。
Implementations of system 100 (and algorithms, methods, instructions, etc., stored thereon and/or executed by
さらに、一態様では、たとえば、システム100は、メモリに命令として記憶することができ、実行されると、本明細書に記載されているそれぞれの方法、アルゴリズム、および/または命令のうちのいずれかを実行するコンピュータプログラムを備えたコンピュータまたはプロセッサを使用して実装することができる。さらに、または代わりに、たとえば、本明細書に記載されている方法、アルゴリズム、または命令のうちのいずれかを実行するための他のハードウェアを含むことができる専用コンピュータ/プロセッサを利用することができる。
Furthermore, in one aspect, for example,
さらに、本開示の実装形態のすべてまたは一部は、たとえば、有形のコンピュータ使用可能またはコンピュータ可読媒体からアクセス可能なコンピュータプログラム製品の形態をとることができる。コンピュータ使用可能またはコンピュータ可読媒体は、たとえば、任意のプロセッサによって、または任意のプロセッサに関連して使用するために、プログラムを具体的に含み、記憶し、通信し、または輸送することができる任意のデバイスであり得る。媒体は、たとえば、電子、磁気、光学、電磁気、または半導体デバイスであり得る。他の適切な媒体も利用可能である。 Furthermore, all or a portion of the implementations of the present disclosure may take the form of a computer program product accessible, for example, from a tangible computer usable or computer readable medium. The computer usable or computer readable medium may be, for example, any device that can tangibly contain, store, communicate, or transport a program for use by or in association with any processor. The medium may be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable media may also be utilized.
上記の実施形態、実装形態、および態様は、本開示の容易な理解を可能にするために説明されており、本開示を限定しない。むしろ、本開示は、添付の特許請求の範囲内に含まれる様々な修正および等価な構成を網羅することを意図しており、その範囲は、法律の下で許可されるすべてのそのような修正および同等の構造を包含するように最も広い解釈を与えられるべきである。 The above embodiments, implementations, and aspects are described to facilitate understanding of the present disclosure, and are not intended to limit the present disclosure. Rather, the present disclosure is intended to cover various modifications and equivalent configurations that fall within the scope of the appended claims, the scope of which should be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures permitted under law.
100 システム
102 ビデオ
104 オーディオ
106 オーディオ分類装置
108 オブジェクト認識装置
110 マッチャ
112 オーディオ空間化装置
114 拡散フィールドシンセサイザ
200 技法
202 ビデオ
204 オーディオトラック
206 ビデオフレーム
208 オーディオ分類
210 オブジェクト分類
212 一致
214 空間情報
300 技法
400 技法
500 技法
600 ユーザインターフェース
602 オーディオセグメント
604 ラベル
606 音源
608 音源
610 音源
612 確実性指標
614 スペクトログラム
700 例
702 フレーム
704 リスト
706 境界ポリゴン
708 リスト
710 データ構造
712 境界ポリゴン
714 ラベル
716 境界ポリゴン
718 ラベル
800 技法
801 ビデオ
802 ビデオフレーム
804 視覚オブジェクト
806 オーディオセグメント
808 音源分離モジュール
810 聴覚オブジェクト
812 残留オーディオ信号
814 自動一致
816 手動一致
818 オーディオ残留
819 空間情報
820 オーディオ空間化装置
822 拡散フィールドシンセサイザ
824 オーディオ出力
900 技法
1000 技法
1100 技法
100 Systems
102 Videos
104 Audio
106 Audio Classifier
108 Object Recognition Device
110 Matcha
112 Audio spatialization device
114 Diffusion Field Synthesizer
200 Techniques
202 Videos
204 Audio Tracks
206 Video Frames
208 Audio Classification
210 Object Classification
212 matches
214 Spatial Information
300 Techniques
400 Techniques
500 Techniques
600 User Interface
602 Audio Segments
604 Label
606 Sound Source
608 Sound Source
610 Sound Source
612 Certainty Index
614 Spectrogram
700 Examples
702 frames
704 List
706 Boundary Polygons
708 List
710 Data Structures
712 Boundary Polygons
714 Label
716 Boundary Polygons
718 Label
800 Techniques
801 Video
802 video frames
804 Visual Objects
806 Audio Segments
808 Sound Source Separation Module
810 Auditory Objects
812 Residual Audio Signal
814 Automatic Match
816 Manual Match
818 Audio Residual
819 Spatial Information
820 Audio Spatializer
822 Diffusion Field Synthesizer
824 Audio Output
900 Techniques
1000 Techniques
1100 Technique
Claims (18)
非空間化されており、第1のビデオフレームに関連付けられている、第1のオーディオセグメントを受信するステップと、
前記第1のビデオフレームにおいて視覚オブジェクトを識別するステップと、
前記第1のオーディオセグメントにおいて聴覚イベントを識別するステップと、
前記視覚オブジェクトのうちのある視覚オブジェクトと前記聴覚イベントのうちのある聴覚イベントとの間の一致を識別するステップと、
前記ある視覚オブジェクトの位置に基づいて、空間的位置を前記ある聴覚イベントに割り当てるステップと
を備える、方法であって、前記方法は、
第2のオーディオセグメントを受信するステップであって、前記第2のオーディオセグメントが前記ある聴覚イベントを含む、ステップと、
第2のビデオフレームを受信するステップであって、前記第2のビデオフレームが前記ある視覚オブジェクトを含まない、ステップと、
前記第1のビデオフレームの少なくともサブセットに少なくとも部分的に基づいて前記ある視覚オブジェクトの動きベクトルを決定するステップと、
前記動きベクトルに基づいて、周囲空間的位置を前記聴覚イベントのうちの前記ある聴覚イベントに割り当てるステップと
をさらに備える、方法。 1. A computer-implemented method for assigning spatial information to audio segments, comprising the steps of:
receiving a first audio segment that is de-spatialized and associated with a first video frame;
identifying a visual object in the first video frame;
identifying an auditory event in the first audio segment;
identifying a correspondence between a visual object of the visual objects and an auditory event of the auditory events;
and assigning a spatial location to the auditory event based on a location of the visual object, the method comprising:
receiving a second audio segment, the second audio segment including the auditory event;
receiving a second video frame, the second video frame not including the certain visual object;
determining a motion vector of the certain visual object based at least in part on at least a subset of the first video frames;
assigning a surrounding spatial location to said one of said auditory events based on said motion vector;
The method further comprises :
非空間化されており、第1のビデオフレームに関連付けられている、第1のオーディオセグメントを受信するステップと、receiving a first audio segment that is despatialized and associated with a first video frame;
前記第1のビデオフレームにおいて視覚オブジェクトを識別するステップと、identifying a visual object in the first video frame;
前記第1のオーディオセグメントにおいて聴覚イベントを識別するステップと、identifying an auditory event in the first audio segment;
前記視覚オブジェクトのうちのある視覚オブジェクトと前記聴覚イベントのうちのある聴覚イベントとの間の一致を識別するステップと、identifying a correspondence between a visual object of the visual objects and an auditory event of the auditory events;
前記ある視覚オブジェクトの位置に基づいて、空間的位置を前記ある聴覚イベントに割り当てるステップとassigning a spatial location to the auditory event based on a location of the visual object;
を備える、方法であって、A method comprising:
第2のオーディオセグメントを受信するステップであって、前記第2のオーディオセグメントが前記ある聴覚イベントを含む、ステップと、receiving a second audio segment, the second audio segment including the auditory event;
第2のビデオフレームを受信するステップであって、前記第2のビデオフレームが前記ある視覚オブジェクトを含まない、ステップと、receiving a second video frame, the second video frame not including the certain visual object;
前記第1のビデオフレームと前記第2のビデオフレームとの間の時間差に基づいて、周囲空間的位置または拡散位置のうちの1つを前記ある聴覚イベントに割り当てるステップとassigning one of a circumferential spatial location or a diffuse location to the certain auditory event based on a time difference between the first video frame and the second video frame;
をさらに備える、方法。The method further comprises:
前記一致しない聴覚イベントをユーザインターフェースにおいて提示するステップと
をさらに備える、請求項1または2に記載の方法。 identifying an inconsistent auditory event, the inconsistent auditory event not matching a visual object identified in the first video frame;
and presenting the incongruent auditory events in a user interface .
前記第1のオーディオセグメントがモノラルである、請求項1から6のいずれか一項に記載の方法。 the first video frame is a frame of a celestial sphere video; or
The method of claim 1 , wherein the first audio segment is mono.
前記第1のオーディオセグメントを複数のトラックに分解することによって前記第1のオーディオセグメントにおける前記聴覚イベントを識別するために、ブラインド音源分離を使用するステップであって、各トラックがそれぞれの聴覚イベントに対応する、ステップを備える、請求項1から7のいずれか一項に記載の方法。 identifying the auditory event in the first audio segment,
8. The method of claim 1, comprising using blind source separation to identify the auditory events in the first audio segment by decomposing the first audio segment into a number of tracks, each track corresponding to a respective auditory event.
前記第1のビデオフレームにおける前記視覚オブジェクトを識別するために、画像認識を使用するステップを備える、請求項1から8のいずれか一項に記載の方法。 identifying the visual object in the first video frame,
9. A method according to any preceding claim, comprising using image recognition to identify the visual object in the first video frame.
第2のビデオフレームを受信するステップであって、前記第2のビデオフレームが前記ある視覚オブジェクトを含まない、ステップと、
前記第1のビデオフレームと前記第2のビデオフレームとの間の時間差に基づいて、周囲空間的位置または拡散位置のうちの1つを前記ある聴覚イベントに割り当てるステップと
をさらに備える、請求項1または、請求項3から10のいずれか一項に記載の方法。 receiving a second audio segment, the second audio segment including the auditory event;
receiving a second video frame, the second video frame not including the certain visual object;
and assigning one of a surrounding spatial location or a diffuse location to the auditory event based on a time difference between the first video frame and the second video frame.
前記第1のオーディオセグメントにおける前記聴覚イベントを識別するステップが、
前記第1のオーディオセグメントを複数のトラックに分割するステップと、
それぞれのオーディオラベルを複数のトラックに割り当てるステップと
を備え、
前記視覚オブジェクトのうちの前記ある視覚オブジェクトと前記聴覚イベントのうちの前記ある聴覚イベントとの間の前記一致を識別するステップが、前記それぞれのオーディオラベルのうちのいくつかを前記視覚ラベルのうちのいくつかに自動的に一致させるステップを備える、請求項1または2に記載の方法。 wherein identifying the visual objects in the first video frame comprises assigning respective visual labels to the visual objects in the first video frame;
identifying the auditory event in the first audio segment,
splitting the first audio segment into a number of tracks;
and assigning respective audio labels to a plurality of tracks;
3. The method of claim 1, wherein identifying the match between the visual object and the auditory event comprises automatically matching some of the respective audio labels to some of the visual labels.
非空間化されており、第1のビデオフレームに関連付けられている、第1のオーディオセグメントを受信することと、
前記第1のビデオフレームにおいて視覚オブジェクトを識別することと、
前記第1のオーディオセグメントにおいて聴覚イベントを識別することと、
前記視覚オブジェクトのうちのある視覚オブジェクトと前記聴覚イベントのうちのある聴覚イベントとの間の一致を識別することに応答して、空間情報を前記ある聴覚イベントに割り当てることと、
前記一致を識別しないことに応答して、前記ある聴覚イベントを拡散音場に割り当てることと
を行うように構成されたプロセッサを備え、前記プロセッサは、
第2のオーディオセグメントを受信することであって、前記第2のオーディオセグメントが前記ある聴覚イベントを含む、ことと、
第2のビデオフレームを受信することであって、前記第2のビデオフレームが前記ある視覚オブジェクトを含まない、ことと、
前記第1のビデオフレームの少なくともサブセットに少なくとも部分的に基づいて前記ある視覚オブジェクトの動きベクトルを決定することと、
前記動きベクトルに基づいて、周囲空間的位置を前記聴覚イベントのうちの前記ある聴覚イベントに割り当てることと
をさらに行うように構成されている、装置。 1. An apparatus for assigning spatial information to an audio segment, comprising:
Receiving a first audio segment, the first audio segment being de-spatialized and associated with a first video frame;
identifying a visual object in the first video frame;
identifying an auditory event in the first audio segment;
in response to identifying a correspondence between a visual object of the visual objects and an auditory event of the auditory events, assigning spatial information to the auditory event;
and in response to not identifying a match, assigning the certain auditory event to a diffuse sound field, the processor configured to :
receiving a second audio segment, the second audio segment including the auditory event; and
receiving a second video frame, the second video frame not including the certain visual object; and
determining a motion vector of the certain visual object based at least in part on at least a subset of the first video frames;
assigning a surrounding spatial location to said one of said auditory events based on said motion vector;
The apparatus is further configured to:
前記ある視覚オブジェクトの位置に基づいて、空間的位置を前記ある聴覚イベントに割り当てることを備える、請求項13に記載の装置。 assigning the spatial information to the one auditory event,
14. The apparatus of claim 13, further comprising assigning a spatial location to the given auditory event based on a position of the given visual object.
前記ある聴覚イベント、および前記ある聴覚イベントの前記空間的位置を含むオーディオファイルを生成するように構成される、請求項14または15に記載の装置。 The processor,
16. An apparatus according to claim 14 or 15, configured to generate an audio file comprising the certain auditory event and the spatial location of the certain auditory event.
前記ある聴覚イベントおよび前記ある聴覚イベントに関連する拡散音情報を含むオーディオファイルを生成するように構成される、請求項13に記載の装置。 The processor,
14. The apparatus of claim 13, configured to generate an audio file comprising the given auditory event and diffuse sound information associated with the given auditory event.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/779,921 US11704087B2 (en) | 2020-02-03 | 2020-02-03 | Video-informed spatial audio expansion |
US16/779,921 | 2020-02-03 | ||
PCT/US2020/055964 WO2021158268A1 (en) | 2020-02-03 | 2020-10-16 | Video-informed spatial audio expansion |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2023514121A JP2023514121A (en) | 2023-04-05 |
JP7464730B2 true JP7464730B2 (en) | 2024-04-09 |
Family
ID=73198490
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022547129A Active JP7464730B2 (en) | 2020-02-03 | 2020-10-16 | Spatial Audio Enhancement Based on Video Information |
Country Status (6)
Country | Link |
---|---|
US (2) | US11704087B2 (en) |
EP (1) | EP4055596A1 (en) |
JP (1) | JP7464730B2 (en) |
KR (1) | KR20220116502A (en) |
CN (1) | CN114981889A (en) |
WO (1) | WO2021158268A1 (en) |
Citations (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030053680A1 (en) | 2001-09-17 | 2003-03-20 | Koninklijke Philips Electronics N.V. | Three-dimensional sound creation assisted by visual information |
JP2006123161A (en) | 2004-09-30 | 2006-05-18 | Samsung Electronics Co Ltd | Audio video sensor fusion device and fusion method for grasping, tracking and separating position |
JP2007272733A (en) | 2006-03-31 | 2007-10-18 | Sony Corp | Image processing device and method, and program |
JP2010117946A (en) | 2008-11-13 | 2010-05-27 | Masafumi Hagiwara | Object tracking method and image processing apparatus |
JP2011071683A (en) | 2009-09-25 | 2011-04-07 | Nec Corp | Video object detection apparatus, video object detection method and program |
US20140314391A1 (en) | 2013-03-18 | 2014-10-23 | Samsung Electronics Co., Ltd. | Method for displaying image combined with playing audio in an electronic device |
JP2015032001A (en) | 2013-07-31 | 2015-02-16 | キヤノン株式会社 | Information processor and information processing method and program |
JP2016513410A (en) | 2013-02-15 | 2016-05-12 | クゥアルコム・インコーポレイテッドＱｕａｌｃｏｍｍ Ｉｎｃｏｒｐｏｒａｔｅｄ | Video analysis support generation of multi-channel audio data |
JP2016062071A5 (en) | 2014-09-22 | 2017-10-19 | ||
JP2019050482A (en) | 2017-09-08 | 2019-03-28 | オリンパス株式会社 | Information acquisition device, display method, and program |
JP2019078864A (en) | 2017-10-24 | 2019-05-23 | 日本電信電話株式会社 | Musical sound emphasis device, convolution auto encoder learning device, musical sound emphasis method, and program |
JP2019523902A (en) | 2016-05-25 | 2019-08-29 | ワーナー ブラザーズ エンターテイメント インコーポレイテッド | Method and apparatus for generating a virtual or augmented reality presentation using 3D audio positioning |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB2414369B (en) * | 2004-05-21 | 2007-08-01 | Hewlett Packard Development Co | Processing audio data |
US8755432B2 (en) | 2010-06-30 | 2014-06-17 | Warner Bros. Entertainment Inc. | Method and apparatus for generating 3D audio positioning using dynamically optimized audio 3D space perception cues |
US9888333B2 (en) * | 2013-11-11 | 2018-02-06 | Google Technology Holdings LLC | Three-dimensional audio rendering techniques |
WO2015105748A1 (en) * | 2014-01-09 | 2015-07-16 | Dolby Laboratories Licensing Corporation | Spatial error metrics of audio content |
US9282399B2 (en) * | 2014-02-26 | 2016-03-08 | Qualcomm Incorporated | Listen to people you recognize |
US9570113B2 (en) * | 2014-07-03 | 2017-02-14 | Gopro, Inc. | Automatic generation of video and directional audio from spherical content |
JP6392051B2 (en) | 2014-09-22 | 2018-09-19 | 株式会社東芝 | Electronic device, method and program |
US9756421B2 (en) * | 2016-01-22 | 2017-09-05 | Mediatek Inc. | Audio refocusing methods and electronic devices utilizing the same |
US20170293461A1 (en) * | 2016-04-07 | 2017-10-12 | VideoStitch Inc. | Graphical placement of immersive audio sources |
EP3503592B1 (en) * | 2017-12-19 | 2020-09-16 | Nokia Technologies Oy | Methods, apparatuses and computer programs relating to spatial audio |
US10649638B2 (en) * | 2018-02-06 | 2020-05-12 | Adobe Inc. | Immersive media content navigation and editing techniques |
-
2020
- 2020-02-03 US US16/779,921 patent/US11704087B2/en active Active
- 2020-10-16 KR KR1020227024392A patent/KR20220116502A/en unknown
- 2020-10-16 JP JP2022547129A patent/JP7464730B2/en active Active
- 2020-10-16 EP EP20804076.6A patent/EP4055596A1/en active Pending
- 2020-10-16 WO PCT/US2020/055964 patent/WO2021158268A1/en unknown
- 2020-10-16 CN CN202080091396.9A patent/CN114981889A/en active Pending
-
2023
- 2023-06-01 US US18/327,134 patent/US20230305800A1/en active Pending
Patent Citations (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030053680A1 (en) | 2001-09-17 | 2003-03-20 | Koninklijke Philips Electronics N.V. | Three-dimensional sound creation assisted by visual information |
JP2006123161A (en) | 2004-09-30 | 2006-05-18 | Samsung Electronics Co Ltd | Audio video sensor fusion device and fusion method for grasping, tracking and separating position |
JP2007272733A (en) | 2006-03-31 | 2007-10-18 | Sony Corp | Image processing device and method, and program |
JP2010117946A (en) | 2008-11-13 | 2010-05-27 | Masafumi Hagiwara | Object tracking method and image processing apparatus |
JP2011071683A (en) | 2009-09-25 | 2011-04-07 | Nec Corp | Video object detection apparatus, video object detection method and program |
JP2016513410A (en) | 2013-02-15 | 2016-05-12 | クゥアルコム・インコーポレイテッドＱｕａｌｃｏｍｍ Ｉｎｃｏｒｐｏｒａｔｅｄ | Video analysis support generation of multi-channel audio data |
US20140314391A1 (en) | 2013-03-18 | 2014-10-23 | Samsung Electronics Co., Ltd. | Method for displaying image combined with playing audio in an electronic device |
JP2015032001A (en) | 2013-07-31 | 2015-02-16 | キヤノン株式会社 | Information processor and information processing method and program |
JP2016062071A5 (en) | 2014-09-22 | 2017-10-19 | ||
JP2019523902A (en) | 2016-05-25 | 2019-08-29 | ワーナー ブラザーズ エンターテイメント インコーポレイテッド | Method and apparatus for generating a virtual or augmented reality presentation using 3D audio positioning |
JP2019050482A (en) | 2017-09-08 | 2019-03-28 | オリンパス株式会社 | Information acquisition device, display method, and program |
JP2019078864A (en) | 2017-10-24 | 2019-05-23 | 日本電信電話株式会社 | Musical sound emphasis device, convolution auto encoder learning device, musical sound emphasis method, and program |
Also Published As
Publication number | Publication date |
---|---|
US20230305800A1 (en) | 2023-09-28 |
US11704087B2 (en) | 2023-07-18 |
JP2023514121A (en) | 2023-04-05 |
US20210240431A1 (en) | 2021-08-05 |
WO2021158268A1 (en) | 2021-08-12 |
EP4055596A1 (en) | 2022-09-14 |
KR20220116502A (en) | 2022-08-23 |
CN114981889A (en) | 2022-08-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Gao et al. | 2.5 d visual sound | |
US11887578B2 (en) | Automatic dubbing method and apparatus | |
Zhou et al. | Sep-stereo: Visually guided stereophonic audio generation by associating source separation | |
Yang et al. | Telling left from right: Learning spatial correspondence of sight and sound | |
US20210217436A1 (en) | Data driven audio enhancement | |
Xu et al. | Visually informed binaural audio generation without binaural audios | |
JP7116424B2 (en) | Program, apparatus and method for mixing sound objects according to images | |
US11431887B2 (en) | Information processing device and method for detection of a sound image object | |
JP5618043B2 (en) | Audiovisual processing system, audiovisual processing method, and program | |
CN108141695A (en) | The screen correlation of high-order ambiophony (HOA) content adapts to | |
Chen et al. | Audio-visual synchronisation in the wild | |
US11212637B2 (en) | Complementary virtual audio generation | |
JP2013171089A (en) | Voice correction device, method, and program | |
US10153002B2 (en) | Selection of an audio stream of a video for enhancement using images of the video | |
Wang et al. | Self-supervised learning of audio representations from audio-visual data using spatial alignment | |
JP7464730B2 (en) | Spatial Audio Enhancement Based on Video Information | |
CN112995530A (en) | Video generation method, device and equipment | |
JP2014195267A (en) | Video and audio processing system, video and audio processing method, and program | |
Lv et al. | A TCN-based primary ambient extraction in generating ambisonics audio from Panorama Video | |
US20230308823A1 (en) | Systems and Methods for Upmixing Audiovisual Data | |
Dimoulas et al. | Spatial audio content management within the MPEG-7 standard of ambisonic localization and visualization descriptions | |
GB2601114A (en) | Audio processing system and method | |
CN117099159A (en) | Information processing device, information processing method, and program | |
CN112562687A (en) | Audio and video processing method and device, recording pen and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20220802 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20220802 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20230726 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20230904 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20231127 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20240304 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20240328 |