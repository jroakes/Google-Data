JP7469407B2 - Exploiting sparsity of input data in neural network computation units - Google Patents
Exploiting sparsity of input data in neural network computation units Download PDFInfo
- Publication number
- JP7469407B2 JP7469407B2 JP2022138360A JP2022138360A JP7469407B2 JP 7469407 B2 JP7469407 B2 JP 7469407B2 JP 2022138360 A JP2022138360 A JP 2022138360A JP 2022138360 A JP2022138360 A JP 2022138360A JP 7469407 B2 JP7469407 B2 JP 7469407B2
- Authority
- JP
- Japan
- Prior art keywords
- input
- neural network
- inputs
- network layer
- activation
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000013528 artificial neural network Methods 0.000 title claims description 72
- 230000004913 activation Effects 0.000 claims description 263
- 238000012545 processing Methods 0.000 claims description 49
- 239000011159 matrix material Substances 0.000 claims description 48
- 238000000034 method Methods 0.000 claims description 42
- 238000004364 calculation method Methods 0.000 claims description 30
- 230000008569 process Effects 0.000 claims description 15
- 238000004590 computer program Methods 0.000 claims description 9
- 238000001994 activation Methods 0.000 description 254
- 230000004044 response Effects 0.000 description 11
- 238000013507 mapping Methods 0.000 description 8
- 238000004891 communication Methods 0.000 description 7
- 230000003405 preventing effect Effects 0.000 description 7
- 238000010801 machine learning Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000013527 convolutional neural network Methods 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 238000009825 accumulation Methods 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 2
- 238000003491 array Methods 0.000 description 2
- 230000005055 memory storage Effects 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000000638 solvent extraction Methods 0.000 description 2
- 244000141359 Malus pumila Species 0.000 description 1
- 240000008790 Musa x paradisiaca Species 0.000 description 1
- 230000009471 action Effects 0.000 description 1
- 235000021016 apples Nutrition 0.000 description 1
- 235000021015 bananas Nutrition 0.000 description 1
- 239000000872 buffer Substances 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 239000002131 composite material Substances 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000009977 dual effect Effects 0.000 description 1
- 230000008030 elimination Effects 0.000 description 1
- 238000003379 elimination reaction Methods 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000026676 system process Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/14—Handling requests for interconnection or transfer
- G06F13/16—Handling requests for interconnection or transfer for access to memory bus
- G06F13/1668—Details of memory controller
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F15/00—Digital computers in general; Data processing equipment in general
- G06F15/76—Architectures of general purpose stored program computers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/15—Correlation function computation including computation of convolution operations
- G06F17/153—Multidimensional correlation or convolution
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/16—Matrix or vector computation, e.g. matrix-matrix or matrix-vector multiplication, matrix factorization
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F7/00—Methods or arrangements for processing data by operating upon the order or content of the data handled
- G06F7/38—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation
- G06F7/48—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation using non-contact-making devices, e.g. tube, solid state device; using unspecified devices
- G06F7/544—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation using non-contact-making devices, e.g. tube, solid state device; using unspecified devices for evaluating functions by calculation
- G06F7/5443—Sum of products
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/30003—Arrangements for executing specific machine instructions
- G06F9/30007—Arrangements for executing specific machine instructions to perform operations on data operands
- G06F9/3001—Arithmetic instructions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/38—Concurrent instruction execution, e.g. pipeline, look ahead
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/38—Concurrent instruction execution, e.g. pipeline, look ahead
- G06F9/3824—Operand accessing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
- G06N20/10—Machine learning using kernel methods, e.g. support vector machines [SVM]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/10—Interfaces, programming languages or software development kits, e.g. for simulating neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y02—TECHNOLOGIES OR APPLICATIONS FOR MITIGATION OR ADAPTATION AGAINST CLIMATE CHANGE
- Y02D—CLIMATE CHANGE MITIGATION TECHNOLOGIES IN INFORMATION AND COMMUNICATION TECHNOLOGIES [ICT], I.E. INFORMATION AND COMMUNICATION TECHNOLOGIES AIMING AT THE REDUCTION OF THEIR OWN ENERGY USE
- Y02D10/00—Energy efficient computing, e.g. low power processors, power management or thermal management
Description
背景
この明細書は、特殊用途の計算ユニットを用いて機械学習計算を実行することに関する。
FIELD This specification relates to performing machine learning calculations using special-purpose computing units.
ニューラルネットワークは機械学習モデルであって、モデルのうちの１つ以上の層を用いて、受信された入力のための出力（たとえば、分類）を生成する。いくつかのニューラルネットワークは、出力層に加えて１つ以上の隠された層を含む。各々の隠された層の出力は、ネットワークにおける次の層への入力（すなわち、ネットワークにおける次の隠された層または出力層）として用いられる。ネットワークの各層は、対応するそれぞれのパラメータのセットの現在値に従って、受信された入力から出力を生成する。 A neural network is a machine learning model that uses one or more layers of the model to generate an output (e.g., a classification) for a received input. Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as an input to the next layer in the network (i.e., the next hidden layer or the output layer in the network). Each layer of the network generates an output from the received input according to the current values of its respective set of parameters.
いくつかのニューラルネットワークは、１つ以上の畳み込みニューラルネットワーク層を含む。各々の畳み込みニューラルネットワーク層は、関連するカーネルのセットを有する。各々のカーネルは、ユーザによって作成されたニューラルネットワークモデルによって設定された値を含む。いくつかの実現例においては、カーネルは特定の画像外形、形状または色を識別する。カーネルは、重み入力のマトリックス構造として表わすことができる。各々の畳み込み層はまた、１セットのアクティベーション入力を処理することもできる。当該１セットのアクティベーション入力もマトリックス構造として表わすことができる。 Some neural networks include one or more convolutional neural network layers. Each convolutional neural network layer has a set of associated kernels. Each kernel contains values set by a neural network model created by a user. In some implementations, the kernels identify specific image shapes, shapes, or colors. The kernels can be represented as a matrix structure of weight inputs. Each convolutional layer can also process a set of activation inputs. The set of activation inputs can also be represented as a matrix structure.
概要
畳み込み計算を実行する一方法は、大次元空間における多数の行列乗算を必要とする。計算ユニットのプロセッサまたはコントローラデバイスはブルートフォース（brute force）法によって行列乗算を計算することができる。たとえば、計算集約的であるとともに
時間集約的であるが、プロセッサは、畳み込み計算のために個々の和および積を繰返し計算することができる。プロセッサが計算を並列化する度合は、そのアーキテクチャのせいで制限されている。
Overview One method of performing convolution calculations requires a large number of matrix multiplications in a large dimensional space. A processor or controller device of a computation unit can calculate the matrix multiplications by brute force methods. For example, a processor can repeatedly calculate the individual sums and products for a convolution calculation, which is both computationally and time intensive. The degree to which a processor can parallelize the calculations is limited by its architecture.
この明細書中に記載される主題の革新的な局面は、コンピュータにより実現される方法で具体化することができる。当該方法は、コンピューティングデバイスが、少なくとも部分的に、当該コンピューティングデバイス外にあるソースから提供される複数の入力アクティベーションを受信するステップと、当該コンピューティングデバイスのコントローラが、当該複数の入力アクティベーションの各々がゼロ値または非ゼロ値のうちのいずれであるかを判断するステップとを含む。当該方法はさらに、当該コンピューティングデバイスのメモリバンクに少なくとも１つの入力アクティベーションを格納するステップを含む。当該少なくとも１つの入力アクティベーションを格納するステップは、当該コントローラが、非ゼロ値である入力アクティベーション値を有する１つ以上のメモリアドレス位置を含むインデックスを生成するステップを含む。当該方法はさらに、当該コントローラが、当該メモリバンクから、少なくとも１つの入力アクティベーションを、計算アレイのうちの１つ以上のユニットによってアクセス可能なデータバスに対して提供するステップを含む。当該アクティベーションは、少なくとも部分的に、当該インデックスに関連付けら
れたメモリアドレス位置から提供される。
Innovative aspects of the subject matter described herein may be embodied in a computer-implemented method. The method includes a computing device receiving a plurality of input activations provided, at least in part, from a source external to the computing device, and a controller of the computing device determining whether each of the plurality of input activations is one of a zero value or a non-zero value. The method further includes storing at least one input activation in a memory bank of the computing device. Storing the at least one input activation includes the controller generating an index including one or more memory address locations having input activation values that are non-zero values. The method further includes the controller providing, from the memory bank, the at least one input activation to a data bus accessible by one or more units of a computational array. The activation is provided, at least in part, from a memory address location associated with the index.
いくつかの実現例においては、当該インデックスは、複数のビットを含むビットマップに基づいて作成される。当該ビットマップのうちの各々のビットは、非ゼロ入力アクティベーション値またはゼロ入力アクティベーション値のうち少なくとも１つを示す。いくつかの実現例においては、当該方法はさらに、非ゼロ値を有する第１の入力アクティベーションを提供して、少なくとも１つのユニットが当該非ゼロ値を用いて計算を実行し、その後、ゼロ値を有する第２の入力アクティベーションを提供し、少なくとも１つのユニットにおいて、当該ゼロ値を用いて実行される可能性のある計算を防止するステップを含む。 In some implementations, the index is created based on a bitmap including a plurality of bits, each bit of the bitmap indicating at least one of a non-zero input activation value or a zero input activation value. In some implementations, the method further includes providing a first input activation having a non-zero value to cause at least one unit to perform a calculation using the non-zero value, and thereafter providing a second input activation having a zero value to prevent at least one unit from performing a calculation that may be performed using the zero value.
いくつかの実現例においては、当該防止するステップは、当該入力アクティベーションが当該インデックスに関連付けられていないメモリアドレス位置から提供されると当該コントローラが判断することに応じて、行なわれる。いくつかの実現例においては、当該方法はさらに、当該入力アクティベーションが当該インデックスに関連付けられていないメモリアドレス位置から提供されることを、当該コントローラが検出するステップと、当該検出するステップに応じて、当該ゼロ入力アクティベーション値に関連付けられた乗算演算を防止するための制御信号を当該計算アレイのうちの少なくとも１つのユニットに提供するステップとを含む。 In some implementations, the preventing step occurs in response to the controller determining that the input activation is provided from a memory address location not associated with the index. In some implementations, the method further includes the controller detecting that the input activation is provided from a memory address location not associated with the index, and in response to the detecting step, providing a control signal to at least one unit of the computational array to prevent a multiplication operation associated with the zero input activation value.
いくつかの実現例においては、当該方法はさらに、当該コントローラが、テンソル計算のうち第１の入力アクティベーションを用いる第１の部分を第１のユニットにマップし、当該テンソル計算のうち当該第１の入力アクティベーションも用いる第２の部分を当該第１のユニットとは異なる第２のユニットにマップするステップを含む。いくつかの実現例においては、当該方法はさらに、単一の入力アクティベーションを連続的に当該データバスに対して提供するステップを含む。当該単一の入力アクティベーションは、当該インデックスに関連付けられたメモリアドレス位置からアクセスおよび選択される。いくつかの実現例においては、当該提供するステップはさらに、ゼロ値を有する入力アクティベーションを提供しないステップを含む。 In some implementations, the method further includes the controller mapping a first portion of the tensor computation that uses a first input activation to a first unit and mapping a second portion of the tensor computation that also uses the first input activation to a second unit different from the first unit. In some implementations, the method further includes providing a single input activation to the data bus in succession. The single input activation is accessed and selected from a memory address location associated with the index. In some implementations, the providing step further includes not providing an input activation having a zero value.
この明細書中に記載される主題の別の革新的な局面は、１つ以上の処理デバイスによって実行可能であるとともに以下の動作を実行するための命令を格納する１つ以上の機械読取り可能ストレージデバイスにおいて具体化することができる。当該以下の動作は、コンピューティングデバイスが、少なくとも部分的に、当該コンピューティングデバイス外にあるソースから提供される複数の入力アクティベーションを受信する動作と、当該コンピューティングデバイスのコントローラが、当該複数の入力アクティベーションの各々がゼロ値または非ゼロ値のうちのいずれであるかを判断する動作とを含む。当該以下の動作はさらに、当該コンピューティングデバイスのメモリバンクに当該入力アクティベーションのうち少なくとも１つを格納する動作を含む。当該少なくとも１つの入力アクティベーションを格納する動作は、当該コントローラが、非ゼロ値である入力アクティベーション値を有する１つ以上のメモリアドレス位置を含むインデックスを生成する動作を含む。当該以下の動作はさらに、当該コントローラが、当該メモリバンクから、少なくとも１つの入力アクティベーションを、計算アレイのうちの１つ以上のユニットによってアクセス可能なデータバスに対して提供する動作を含む。当該アクティベーションは、少なくとも部分的に、当該インデックスに関連付けられたメモリアドレス位置から提供される。 Another innovative aspect of the subject matter described herein may be embodied in one or more machine-readable storage devices executable by one or more processing devices and storing instructions for performing the following operations: the computing device receiving a plurality of input activations provided at least in part from a source external to the computing device; and a controller of the computing device determining whether each of the plurality of input activations is a zero value or a non-zero value. The following operations further include storing at least one of the input activations in a memory bank of the computing device. The storing of the at least one input activation includes the controller generating an index including one or more memory address locations having an input activation value that is a non-zero value. The following operations further include the controller providing at least one input activation from the memory bank to a data bus accessible by one or more units of a computational array. The activation is provided at least in part from a memory address location associated with the index.
この明細書中に記載される主題の別の革新的な局面は、電子システムにおいて具体化することができる。当該電子システムは、コンピューティングデバイスに配置されるとともに１つ以上の処理デバイスを含むコントローラと、当該１つ以上の処理デバイスによって実行可能であるとともに以下の動作を実行するための命令を格納するための１つ以上の機械読取り可能ストレージデバイスとを含み、当該以下の動作は、当該コンピューティング
デバイスが、少なくとも部分的に、当該コンピューティングデバイス外にあるソースから提供される複数の入力アクティベーションを受信する動作と、当該コントローラが、当該複数の入力アクティベーションの各々がゼロ値または非ゼロ値のうちのいずれであるかを判断する動作とを含む。当該以下の動作はさらに、当該コンピューティングデバイスのメモリバンクに当該入力アクティベーションのうち少なくとも１つを格納する動作を含む。当該少なくとも１つの入力アクティベーションを格納する動作は、非ゼロ値である入力アクティベーション値を有する１つ以上のメモリアドレス位置を含むインデックスを生成する動作を含む。当該以下の動作はさらに、当該コントローラが、当該メモリバンクから、少なくとも１つの入力アクティベーションを、計算アレイのうちの１つ以上のユニットによってアクセス可能なデータバスに対して提供する動作を含む。当該アクティベーションは、少なくとも部分的に、当該インデックスに関連付けられたメモリアドレス位置から提供される。
Another innovative aspect of the subject matter described herein may be embodied in an electronic system. The electronic system includes a controller disposed in a computing device and including one or more processing devices, and one or more machine-readable storage devices executable by the one or more processing devices for storing instructions for performing the following operations: the computing device receiving a plurality of input activations provided, at least in part, from a source external to the computing device, and the controller determining whether each of the plurality of input activations is one of a zero value or a non-zero value. The following operations further include storing at least one of the input activations in a memory bank of the computing device. The storing of the at least one input activation includes generating an index including one or more memory address locations having an input activation value that is a non-zero value. The following operations further include the controller providing, from the memory bank, the at least one input activation to a data bus accessible by one or more units of a computational array, the activation being provided, at least in part, from a memory address location associated with the index.
この明細書中に記載される主題は、以下の利点のうち１つ以上を実現するように特定の実施形態において実現することができる。第１のメモリからアクセス可能なアクティベーションおよび第２のメモリからアクセス可能な重みは、単一の計算システムにおいて、レジスタから検索されたメモリアドレス値に基づいてトラバースすることができる。計算システムのコントローラは、非ゼロ値だけを第１のメモリに格納することによってアクティベーションデータを圧縮することができ、これにより、メモリストレージ空間および対応する帯域幅を節約することができる。行列乗算は、主として非ゼロ入力アクティベーションを提供することに部分的に基づいて計算システムにおいて行なわれる。さらに、計算システムが、主として非ゼロアクティベーション値を含む通信スキーム用いる場合、ゼロ乗を排除することによって計算効率を向上または促進させることができる。 The subject matter described herein may be implemented in certain embodiments to achieve one or more of the following advantages: Activations accessible from a first memory and weights accessible from a second memory may be traversed in a single computing system based on memory address values retrieved from a register. A controller of the computing system may compress activation data by storing only non-zero values in the first memory, thereby saving memory storage space and corresponding bandwidth. Matrix multiplication may be performed in the computing system based in part on providing primarily non-zero input activations. Additionally, when a computing system employs a communication scheme involving primarily non-zero activation values, the elimination of zero powers may improve or facilitate computational efficiency.
この局面および他の局面についての他の実現例は、上記方法の動作を実行するように構成された対応するシステム、装置と、コンピュータストレージデバイス上で符号化されるコンピュータプログラムとを含む。１つ以上のコンピュータのシステムは、システムにインストールされて動作時に当該システムに上記動作を実行させるソフトウェア、ファームウェア、ハードウェア、またはそれらの組合わせによって、このように構成され得る。１つ以上のコンピュータプログラムは、データ処理装置によって実行されると当該装置に上記動作を実行させる命令を有することによって、このように構成され得る。 Other implementations of this and other aspects include corresponding systems, devices, and computer programs encoded on computer storage devices configured to perform the operations of the above methods. One or more computer systems may be configured in this manner by software, firmware, hardware, or combinations thereof that are installed on the system and that, when operated, cause the system to perform the above operations. One or more computer programs may be configured in this manner by having instructions that, when executed by a data processing device, cause the device to perform the above operations.
この明細書中に記載される主題はまた、画像認識および／または分類方法／システムに関する。システムは、ハードウェアコンピューティングシステムのコンピューティングユニットが推論計算を実行するためにニューラルネットワーク層のための入力を処理する場合に入力データのスパース性を活用するための開示技術を用いて実現することができる。 The subject matter described herein also relates to an image recognition and/or classification method/system. The system can be implemented using the disclosed techniques to exploit sparsity of input data when a computing unit of a hardware computing system processes inputs for neural network layers to perform inference calculations.
この明細書中に記載される主題の１つ以上の実現例についての詳細が添付の図面および以下の記載において説明される。主題についての他の潜在的な特徴、局面および利点は、この記載、添付の図面および添付の特許請求の範囲から明らかになるだろう。 Details of one or more implementations of the subject matter described in this specification are set forth in the accompanying drawings and the following description. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the accompanying drawings, and the appended claims.
さまざまな図面における同様の参照番号および参照符号は同様の要素を示している。
詳細な説明
この明細書中に記載される主題は、例示的なニューラルネットワーク・ハードウェアコンピューティングシステムの計算ユニットまたはタイル内において行なわれる計算を減らすことに関する。概して、ニューラルネットワーク推論を計算する一環として、入力アクティベーションにパラメータまたは重み値を乗じて、出力アクティベーションを生成する。ここでは、入力および入力アクティベーションは、ニューラルネットワークにおいて一般に用いられるテンソル、マトリックスおよび／またはデータアレイなどの多次元データ構造に含まれるデータ要素を指し得る。ディープニューラルネットワークのための計算推論のアルゴリズム特性のために、入力アクティベーションの大部分はゼロである。言いかえれば、現在の計算ユニットは、ゼロ（入力アクティベーション値）に対して１つの数（たとえば重み）を乗じることを含む多数の不必要な計算を実行する。
Like reference numbers and designations in the various drawings indicate like elements.
DETAILED DESCRIPTION The subject matter described herein relates to reducing the computations performed within a computation unit or tile of an exemplary neural network hardware computing system. Generally, as part of computing neural network inference, input activations are multiplied by parameters or weight values to generate output activations. Here, inputs and input activations may refer to data elements contained in multi-dimensional data structures such as tensors, matrices and/or data arrays commonly used in neural networks. Due to the algorithmic nature of computational inference for deep neural networks, the majority of input activations are zero. In other words, current computation units perform a large number of unnecessary computations, including multiplying a number (e.g., weights) by zero (input activation values).
この明細書は、一部には、より効率的なアクティベーションストレージおよび通信スキームと、ディープニューラルネットワーク処理のための、特にニューラルネットワークの畳み込み層を処理するための、カスタムのアーキテクチャ設計とを記載している。この明細書では、時間をかけて稠密な行列乗算を実行する従来のハードウェアアクセラレータとは異なり、１）ゼロ入力値を認識すると計算をスキップをするかまたはバイパスすることができ、２）非ゼロ値だけを含む圧縮された入力アクティベーションを格納することにより計算ユニットにおけるメモリ使用を減らすことができる、アーキテクチャを記載している。全体的に、この明細書の教示により、ニューラルネットワーク推論計算のための計算ユニット性能が改善されるとともに、不必要な計算をスキップすることによってエネルギが節約される。 This specification describes, in part, a more efficient activation storage and communication scheme and a custom architecture design for deep neural network processing, particularly for processing the convolutional layers of neural networks. Unlike traditional hardware accelerators that perform time-consuming dense matrix multiplications, this specification describes an architecture that 1) can skip or bypass calculations when it recognizes zero input values, and 2) can reduce memory usage in the compute unit by storing compressed input activations that contain only non-zero values. Overall, the teachings of this specification improve compute unit performance for neural network inference calculations while saving energy by skipping unnecessary calculations.
図１は、アクティベーション構造１０２およびパラメータ構造１０４を含む例示的な計算構造１００を示す。アクティベーション構造１０２は、（下付き文字０によって示される）第１の入力深度に対応する複数のデータ要素を含む第１のデータ構造１０２ａを含み得る。同様に、アクティベーション構造１０２はまた、（下付き文字１によって示される）第２の入力深度に対応する複数のデータ要素を含む第２のデータ構造１０２ｂを含み得る。データ構造１０２ａにおいて示される複数のデータ要素はａ０、ｂ０、ｃ０、ｄ０として示され、データ構造１０２ｂにおいて示される複数のデータ要素はａ１、ｂ１、ｃ１、ｄ１として示される。データ構造１０２ａ／１０２ｂの各データ要素（ａ０、ａ１、ｂ０、ｂ１、ｃ０、ｄ０など）は入力アクティベーション値であって、各々の入力深度はニューラルネットワーク層への入力の深度に対応する。いくつかの実現例においては、ニューラルネットワーク層は１の入力深度を有し得るとともに、他の実現例においては、ニューラルネットワーク層は、２以上の入力深度を有し得る。
1 shows an exemplary
パラメータ構造１０４は、アクティベーション構造１０２と同様の態様で説明することができる。パラメータ構造１０４は、第１のデータ構造１０４ａおよび第２のデータ構造１０４ｂを含む。各々のデータ構造１０４ａ／１０４ｂは、各々がカーネル値を含む複数のデータ要素を含み得る。図１に示されるように、データ構造１０４ａに対応する複数のデータ要素はｘ０、ｙ０、ｚ０として示され、データ構造１０４ｂに対応する複数のデータ要素はｘ１、ｙ１、ｚ１として示される。
The
上述のとおり、ニューラルネットワークの各層は、対応するそれぞれのオペランドのセットの値に従って、受信された入力から出力を生成する。他のニューラルネットワーク層
と同様に、各々の畳み込み層は、マトリックス構造として表わすことができる１セットのアクティベーション入力を処理することができる。畳み込みニューラルネットワーク層はまた、値を含む関連するカーネルのセットを有することとなり、カーネルも、重みのマトリックス構造として表わすことができる。図１において、アクティベーション構造１０２は、１つ以上のアクティベーション入力を有するマトリックス構造に対応し得るとともに、パラメータ構造１０４は、１つ以上のカーネルまたは重みパラメータを有するマトリックス構造に対応し得る。
As mentioned above, each layer of a neural network generates an output from the received input according to the values of the corresponding set of operands. Like other neural network layers, each convolutional layer can process a set of activation inputs that can be represented as a matrix structure. A convolutional neural network layer will also have an associated set of kernels that contain values, and the kernels can also be represented as a matrix structure of weights. In FIG. 1, the
以下においてより詳細に記載されるように、ニューラルネットワークのさまざまな層は、行列乗算を含む大量の計算を実行することによって機械学習推論を処理する。ニューラルネットワーク層（たとえば、畳み込み層）内において実行される計算プロセスは、１以上のサイクルで入力アクティベーション（すなわち、第１オペランド）に重み（すなわち、第２オペランド）を乗じることと、多くのサイクルにわたって積の累積を実行することとを含み得る。出力アクティベーションは、２つのオペランドに対して実行される乗算動作および累積動作に基づいて生成される。 As described in more detail below, various layers of a neural network process machine learning inference by performing a large amount of computations, including matrix multiplication. The computational process performed within a neural network layer (e.g., a convolutional layer) may include multiplying an input activation (i.e., a first operand) by a weight (i.e., a second operand) in one or more cycles and performing an accumulation of products over many cycles. An output activation is generated based on the multiplication and accumulation operations performed on the two operands.
図示のとおり、式１０６は、アクティベーション構造１０２の或るデータ要素に関連付けられた入力アクティベーションに、パラメータ構造１０４の或るデータ要素に関連付けられたカーネル値または重み／パラメータを乗じる場合に実行することができる直列シーケンスベースの数学的演算の一例を提供する。たとえば、式１０６において、インデックス「ｉ」が０と等しい場合、アクティベーション構造１０２のデータ要素ａ０に関連付けられた入力アクティベーションに、パラメータ構造１０４のデータ要素ｘ０に関連付けられた重み／パラメータを乗じる。さらに、式１０６が部分的に直列ベースの式であるので、付加的な乗算演算が、アクティベーション構造１０２およびパラメータ構造１０４の他のデータ要素に対応するオペランドのセット間で行なわれることとなる。いくつかの実現例においては、１セットのオペランドの乗算により、特定の出力特徴またはアクティベーションについての部分和１０６ａ／１０６ｂを生成することができる。したがって、式１０６に示されるように、部分和を追加して出力特徴を生成することができる。
As shown,
ニューラルネットワークは、ネットワーク層の機械学習推論ワークロードを促進するように構成された複数のコンピューティングユニットを含む１つ以上のハードウェアコンピューティングシステムにおいて具体化することができる。各々のコンピューティングユニットは、所与の層のための計算のサブセットを処理することができる。いくつかの実現例においては、構造１００は、少なくとも２つのメモリバンクと、（以下に記載される）ＭＡＣアレイを集合的に形成し得る１つ以上の積和演算（ＭＡＣ）セルとを各々が含む１つ以上のコンピューティングユニットにおいて具体化することができる。
The neural network can be embodied in one or more hardware computing systems that include multiple computing units configured to facilitate the machine learning inference workload of the network layer. Each computing unit can process a subset of the computations for a given layer. In some implementations, the
一例においては、例示的なコンピューティングユニットの第１のメモリバンク１０８は、アクティベーション構造１０２に関連付けられたデータを格納するとともに、入力アクティベーション値を受信してメモリバンク１０８内のメモリアドレス位置に書込むように構成され得る。同様に、例示的なコンピューティングユニットの第２のメモリバンク１１０は、パラメータ構造１０４に関連付けられたデータを格納するとともに、重み値を受信してメモリバンク１１０内のメモリアドレス位置に書込むように構成され得る。この例においては、データ要素１０２ａの各要素（たとえば、ａ０、ｂ０、ｃ０、ｄ０）は、第１のメモリバンク１０８の対応するそれぞれのメモリアドレスに格納することができる。同様に、データ要素１０４ａの各要素（たとえば、ｘ０、ｙ０、ｚ０）は、第２のメモリ１１０の対応するそれぞれのメモリアドレスに格納することができる。
In one example, a
いくつかの実現例においては、第１のメモリバンク１０８および第２のメモリバンク１１０は、各々、１つの揮発性メモリユニットまたは複数の揮発性メモリユニットである。
他のいくつかの実現例においては、メモリバンク１０８およびメモリバンク１１０は各々、１つの不揮発性メモリユニットまたは複数の不揮発性メモリユニットである。メモリバンク１０８および１１０はまた、コンピュータ読取り可能記憶媒体の別の形態、たとえば、フロッピー（登録商標）ディスクデバイス、ハードディスクデバイス、光ディスクデバイス、もしくはテープデバイスなど、フラッシュメモリもしく他の同様のソリッドステートメモリデバイス、または、ストレージエリアネットワークもしくは他の構成におけるデバイスを含むデバイスのアレイであり得る。
In some implementations, the
In some other implementations,
概して、ハードウェアコンピューティングシステムのコンピューティングユニットは、メモリアドレス値を追跡するために１つ以上のレジスタを含み得る。アクティベーション構造１０２に対応するマトリックス構造のデータ要素は、第１のメモリバンク１０８からアクセスすることができるとともに、パラメータ構造１０４に対応するマトリックス構造のデータ要素は第２のメモリバンク１１０からアクセスすることができる。コンピューティングタイル／計算ユニットの例示的な制御デバイスは、１つ以上のレジスタからアクセス可能なアドレス値に基づいてマトリックス構造のデータ要素にアクセスすること、および／またはデータ要素をトラバースすることができる。例示的な制御デバイス、アクティベーション構造１０２、パラメータ構造１０４、第１のメモリバンク１０８および第２のメモリバンク１１０を含む例示的な計算ユニット／タイルが、図３に関連付けて以下においてより詳細に説明される。
In general, a computing unit of a hardware computing system may include one or more registers to track memory address values. The data elements of the matrix structure corresponding to the
さらに、ニューラルネットワーク推論ワークロードのためのニューラルネットワーク・テンソル計算および行列乗算を促進するためのハードウェアコンピューティングシステムに関する付加的な詳細および記載が、２０１６年１０月２７日付で提出された「ニューラルネットワークコンピュートタイル（Neural Network Compute Tile）」と題された米国
特許出願第１５／３３５，７６９号に記載されている。米国特許出願第１５／３３５，７６９号の開示全体は、引用によりその全体がこの明細書中に援用されている。
Further details and description regarding a hardware computing system for facilitating neural network tensor computation and matrix multiplication for neural network inference workloads are described in U.S. patent application Ser. No. 15/335,769, entitled "Neural Network Compute Tile," filed Oct. 27, 2016. The entire disclosure of U.S. patent application Ser. No. 15/335,769 is incorporated herein by reference in its entirety.
図２は、アクティベーション構造と、２以上の出力特徴深度のための複数のパラメータ構造とを含む例示的な計算構造２００を示す。いくつかの実現例においては、ニューラルネットワークは、複数の出力特徴深度を有する出力を生成する複数の層を有し得る。いくつかの実現例においては、各々のパラメータ構造は、出力深度のうち対応する出力深度に関与し得る。したがって、計算構造２００はスケーラブルなコンピューティング構造を示しており、このスケーラブルなコンピューティング構造においては、出力深度のＮの数値に関連付けられた計算を容易にするために、付加的なパラメータ構造１０４ａ／１０４ｂ／１０４ｃが追加される。Ｎは変数であって、たとえば、１～５の範囲の整数値を有し得るか、または代替的には、コンピューティングシステム設計者の好みもしくは必要性に応じて１～Ｎの範囲の整数値を有し得る。
2 illustrates an exemplary computational structure 200 that includes an activation structure and multiple parameter structures for two or more output feature depths. In some implementations, a neural network may have multiple layers that generate outputs with multiple output feature depths. In some implementations, each parameter structure may contribute to a corresponding one of the output depths. Thus, computational structure 200 illustrates a scalable computing structure in which
データ経路１０５によって示されるように、データ構造１０２ａに関連付けられた要素についての個々の入力アクティベーション値は、それぞれのパラメータ構造１０４に関連付けられた複合演算子によって実行される計算で使用するために各々のパラメータ構造１０４ａ／１０４ｂ／１０４ｃに供給することができる。次いで、各々のパラメータ構造１０４は、パイプラインの態様で、その左隣りから受信したアクティベーション値をその右隣りに渡すことができる。代替的には、アクティベーションは、各々のパラメータ構造１０４によって同時に提供および消費することができる。
As shown by data path 105, individual input activation values for elements associated with
マトリックス２０２は、アクティベーション１０２に対応する例示的なマトリックス構造を表わし得る。より具体的には、要素行２０２ａはデータ構造１０２ａに対応し得るとともに、要素行２０２ｂはデータ構造１０２ｂに対応し得る。概して、一例として、第１のパラメータ構造１０４（１）は、空間２０６に関連付けられた計算を実行するためにア
クセスされるとともに、第２のパラメータ構造１０４（２）は、空間２０８に関連付けられた計算を実行するためにアクセスされる。図示されていないが、付加的な計算もｚ次元に対応して実行することができる。一例として、要素行２０２ａはＲＧＢ画像のＲ面にあり得るとともに、要素行２０２ｂは同じＲＧＢ画像のＧ面にあり得る。ニューラルネットワークの例示的な畳み込み層は、典型的には、複数の出力特徴を生成する。例示的な出力特徴は、りんごを分類するための出力特徴とバナナを分類するための別の出力特徴とを含み得る。データ構造２０４に関して、空間２０６および２０８は、さまざまな分類のためのさまざまな面を表わし得る。
Matrix 202 may represent an exemplary matrix structure corresponding to
図３は、入力アクティベーションを１つ以上のパラメータ構造に供給するための例示的な計算システム３００を示す。計算システム３００は概してコントローラ３０２を含む。コントローラ３０２は、アクティベーション構造１０２のための入力アクティベーションをメモリバンク１０８のメモリアドレスに格納させるかまたは当該メモリアドレスから検索させるための１つ以上の制御信号３１０を与える。同様に、コントローラ３０２はまた、パラメータ構造１０４ａ／１０４ｂ／１０４ｃについての重みをメモリバンク１１０のメモリアドレスに格納させるかまたは当該メモリアドレスから検索させるための１つ以上の制御信号３１０を与える。計算システム３００はさらに、１つ以上の積和演算（ＭＡＣ）セル／ユニット３０４、入力アクティベーションバス３０６および出力アクティベーションバス３０８を含む。制御信号３１０は、たとえば、メモリバンク１０８に、１つ以上の入力アクティベーションを入力アクティベーションバス３０６に提供させること、メモリバンク１１０に、１つ以上の重みをパラメータ構造１０４ａ／１０４ｂ／１０４ｃに提供させること、および／または、ＭＡＣユニット３０４に、出力アクティベーションバス３０８に提供される出力アクティベーションを生成する計算を実行させること、ができる。
3 illustrates an
コントローラ３０２は１つ以上の処理ユニットおよびメモリを含み得る。いくつかの実施形態においては、コントローラ３０２の処理ユニットは、１つ以上のプロセッサ（たとえば、マイクロプロセッサまたは中央処理装置（central processing unit：ＣＰＵ））
、グラフィック処理装置（graphics processing unit：ＧＰＵ）、特定用途向け集積回路（application specific integrated circuit：ＡＳＩＣ）、またはさまざまなプロセッ
サの組合せを含み得る。代替的な実施形態においては、コントローラ３０２は、この明細書中に記載される判断および計算のうち１つ以上を実行するための付加的な処理オプションを提供する他のストレージまたはコンピューティングリソース／デバイス（たとえば、バッファ、レジスタ、制御回路など）を含み得る。
The
, a graphics processing unit (GPU), an application specific integrated circuit (ASIC), or a combination of various processors. In alternative embodiments,
いくつかの実現例においては、コントローラ３０２の処理ユニットは、コントローラ３０２および計算システム３００にこの明細書中に記載される１つ以上の機能を実行させるための、メモリに格納されたプログラムされた命令を実行する。コントローラ３０２のメモリは、１つ以上の非一時的な機械読取り可能記憶媒体を含み得る。非一時的な機械読取り可能記憶媒体は、ソリッドステートメモリ、磁気ディスクおよび光ディスク、ポータブルコンピュータディスケット、ランダムアクセスメモリ（random access memory：ＲＡＭ）、読取専用メモリ（read-only memory：ＲＯＭ）、消去可能プログラム可能な読取り専用メモリ（たとえば、ＥＰＲＯＭ、ＥＥＰＲＯＭもしくはフラッシュメモリ）、または情報を格納することができる他の任意の有形媒体を含み得る。
In some implementations, the processing unit of the
概して、計算システム３００は、例示的な計算ユニットまたはタイルであって、テンソル、マトリックスおよび／またはデータアレイなどの多次元データ構造に関連付けられた計算を実行するための付加的なハードウェア構造を含み得る。いくつかの実現例においては、入力アクティベーション値は、アクティベーション構造１０２のためのメモリバンク１０８に予めロードすることができるとともに、重み値は、計算システム３００によって
受信されたデータ値を用いて第２のメモリバンク１１０に予めロードすることができる。これらデータ値は、ニューラルネットワーク・ハードウェアコンピューティングシステムに関連付けられた外部の制御デバイスまたはより上位の制御デバイスから計算システム３００に到達するものである。
In general, the
命令、入力または入力アクティベーション、および重みは、ニューラルネットワーク・ハードウェアコンピューティングシステムに関連付けられた外部入／出力（input/output：Ｉ／Ｏ）デバイスまたは上位制御デバイスなどの外部ソースからシステム３００に提供することができる。いくつかの実現例においては、１つ以上のデータバスが、外部ソース（たとえば、制御デバイス）とシステム３００との間にデータ通信を提供する。データバスは、命令、入力もしくはアクティベーションおよび重みを、例示的なＩ／Ｏデバイスから複数のシステム３００の各々に提供するために、または、ニューラルネットワークのためのハードウェアコンピューティングシステムに含まれる複数のコンピュートタイル（たとえば、複数のシステム３００）間において提供するために、用いられる。
Instructions, inputs or input activations, and weights can be provided to the
システム３００は、システム３００によって実行されるべき特定の計算動作を規定する命令を受信することができる。さらに、コントローラ３０２は、たとえば、受信された入力アクティベーションに関連付けられたデータストリームを分析するために、プログラムされた命令を実行することができる。入力アクティベーションデータストリームを分析することにより、コントローラ３０２が、入力アクティベーションの各々に関連付けられた値がゼロ値であるかまたは非ゼロ値であるかどうかを検出するかまたは判断することが可能となり得る。いくつかの実現例においては、コントローラ３０２は、例示的な入力アクティベーションデータストリームを分析し、各々の検出されたゼロアクティベーション値および各々の検出された非ゼロアクティベーション値をビットベクトルまたはビットマップ３０３にマップする。
The
図３に示されるように、ビットマップ３０３は、バイナリ値を用いて、検出されたゼロ値入力アクティベーションおよび検出された非ゼロ値入力アクティベーションをマップすることができる。たとえば、「０」のバイナリ値は、検出されたゼロ入力アクティベーション値に対応し得るとともに、「１」のバイナリ値は、検出された非ゼロ入力アクティベーション値に対応し得る。たとえば、ビットマップ３０３は８ビットのビットマップであり得る。この場合、バイナリ「１」を含む奇数のビット位置が非ゼロアクティベーション値に対応するとともに、バイナリ「０」を含む偶数のビット位置がゼロアクティベーション値に対応する。
As shown in FIG. 3,
コントローラ３０２は、入力アクティベーションをメモリバンク１０８に格納させることができる。概して、メモリバンク１０８に格納されたデータ値は、典型的には、各々が対応するそれぞれのメモリアドレス位置に書込まれている。次いで、特定の計算動作を実行するために入力アクティベーションなどのデータ値が必要になると、例示的な制御デバイス（たとえば、コントローラ３０２）によってメモリバンク１０８におけるアドレス位置にアクセスすることができる。コントローラ３０２は、ビットマップ３０３を用いて、非ゼロ入力アクティベーション値を含むメモリアドレス位置のインデックスを作成することができる。
The
いくつかの実現例においては、コントローラ３０２は、ビットマップ３０３を用いて、どの入力アクティベーションをメモリバンク１０８に書込むべきかを判断する。たとえば、ビットマップ３０３の分析は、ビットマップ位置１、３、５、７（非ゼロ値）に対応するアクティベーション値だけがメモリバンク１０８におけるアドレス位置に書込まれるべきであることを示し得る。さらに、ビットマップ位置２、４、６、８（ゼロ値）に関連付けられたデータ値は、アクティベーション値が入力バス３０６に提供されたときにコント
ローラ３０２によってアクセスされる可能性があるがアクセスされない可能性もあるメモリアドレス位置に書込むことができるか、または廃棄することができる。これにより、ビットマップ３０３はゼロアクティベーション値を圧縮するための基準として用いることができる。この場合、ゼロ値入力アクティベーションがメモリアドレス位置に書込まれていなければ圧縮が行なわれ、これにより、全体的なメモリ使用を減らして、他のデータ値を格納するためのアドレス位置を解放する。
In some implementations, the
コントローラ３０２は、入力アクティベーションをメモリバンク１０８から入力アクティベーションバス３０６にロードするために１つ以上の制御信号３１０をメモリバンク１０８に提供するとともに、これら値をＭＡＣ３０４を含む計算ユニットのアレイに提供することができる。いくつかの実現例においては、非ゼロアクティベーション値を提供するためにどのメモリアドレス値にアクセスしなければならないかを判断するために、コントローラ３０２によって、ビットマップ３０３、またはビットマップ３０３に対応する非ゼロメモリアドレスインデックスが参照され得る。アクティベーション値は、コントローラ３０２によってメモリバンク１０８からデータバス３０６に対して提供される。
The
いくつかの実現例においては、入力アクティベーションは、少なくとも部分的に、インデックスまたはビットマップ３０３に関連付けられたメモリアドレス位置から提供される。他の実現例においては、コントローラ３０２は、ビットマップ３０３またはインデックスのうちの１つに基づいて、提供される入力アクティベーションがゼロ値を有しているかどうかを検出または判断することができる。この判断の実行に応じて、コントローラ３０２は、さらに、不必要な乗算演算（例えば、ゼロ乗）が行なわれるのを防止するか停止させるかまたは阻止するための制御信号を計算アレイにおけるユニットまたはＭＡＣ３０４に提供することができる。計算システム３００内において、ゼロアクティベーション値を提供し、その後、または同時に、そのアクティベーションに関連付けられた計算動作を不可能化することでエネルギの節約が実現可能となる。
In some implementations, the input activation is provided, at least in part, from a memory address location associated with the index or
上述のとおり、インデックスは、非ゼロ値を備える入力アクティベーションを有するすべてのメモリアドレス位置を含む。データバス３０６は、計算アレイのうちの１つ以上のユニットによってアクセス可能である。計算アレイのこれらユニットは、データバス３０６から１つ以上の非ゼロアクティベーション値を受信して、受信したアクティベーション値に基づいて行列乗算に関連する計算を実行することができる。いくつかの実現例においては、計算システム３００は、単に、インデックス付きアドレスに対応するメモリアドレス位置から入力アクティベーションを提供するだけであるだろう。このため、ゼロアクティベーションが入力バス３０６に提供されることはなく、このため、計算動作の実行が不可能化されたり防止されたりすることはないだろう。計算システム３００がこの通信スキームを用いる場合、ゼロ乗を排除することによって計算効率を向上または促進させることができる。
As described above, the index includes all memory address locations that have input activations with non-zero values. The
所与の計算サイクルにわたって、計算システム３００は、ニューラルネットワーク層についての推論計算に関連付けられた乗算演算を実行するために、アクティベーション構造１０２およびパラメータ構造１０４の要素へのアクセスを必要とし得る。上述のように、メモリバンク１０８およびメモリバンク１１０についての特定のメモリアドレス値は、それぞれ、アクティベーション構造１０２およびパラメータ構造１０４の要素に対応し得る。
Over a given computation cycle, the
計算が実行されるサイクルにわたって、コントローラ３０２は一度に１つの入力アクティベーション値を提供することとなり、ＭＡＣセル３０４を含む計算ユニットのアレイは、アクティベーションに重みを乗じて、所与の入力アクティベーションについてのさまざまな出力アクティベーションを生成することとなる。計算ユニットのアレイの（パラメー
タ構造として上述された）各要素またはＭＡＣセル３０４は、ニューラルネットワーク層のさまざまな出力深度に関与し得る。概して、コントローラ３０２がゼロアクティベーション値を検出するたびに、コントローラ３０２は、１）メモリバンク１０８にそのアクティベーション値を格納しないこと、２）アクティベーション値を提供しないこと、または、３）当該値を提供して、そのゼロアクティベーション値に対応する乗算演算を特定の計算ユニットに実行させないようにするための制御信号をこの特定の計算ユニットに提供すること、を実行し得る。
Over a cycle in which a computation is performed, the
計算ユニットのアレイは、ゼロアクティベーション値の検出に基づいて、いつ特定の計算をスキップするかまたは防ぐことが必要になるかを判断することができるコントローラ３０２によって、十分に制御される。このため、特定の計算をスキップするために計算ユニットのアレイ内に複雑なハードウェア構造を追加する必要はない。さらに、入力アクティベーション値は、メモリバンク１０８への格納のために計算システム３００に到達すると、分析され得る。入力アクティベーションの分析に応じて、コントローラ３０２は、非ゼロ値だけをメモリ１０８に格納することによって効率的にアクティベーションデータを圧縮するための命令を実行することができ、これにより、メモリ格納空間および対応する帯域幅を節約することができる。
The array of computational units is fully controlled by the
計算システム３００が入力アクティベーションおよび重みを受信すると、コントローラ３０２は、たとえば、１つ以上のダイレクトメモリアクセス動作を実行することができる。これらのメモリアクセス動作の実行は、アクティベーション構造１０２の次元要素に対応する入力アクティベーションをメモリバンク１０８のアドレス位置に格納することを含む。同様に、コントローラ３０２はまた、パラメータ構造１０４の次元要素に対応するパラメータをメモリバンク１１０のアドレス位置に格納することができる。ビットマップ３０３に加えて、コントローラ３０２は１つ以上のアドレスレジスタをさらに含み得る。１つ以上のアドレスレジスタが維持しているメモリアドレスからは、（たとえば、ゼロ値または非ゼロ値を有する）特定の入力アクティベーションがフェッチされることとなる。さらに、１つ以上のレジスタはまた、メモリアドレスを格納することとなる。これらメモリアドレスからは、特定の入力アクティベーションが乗算される対応する重みがフェッチされる。
When the
上述のとおり、コントローラ３０２は、部分的にビットマップ３０３に基づいて、非ゼロアクティベーション値についてのメモリアドレスを識別する。いくつかの実現例においては、コントローラ３０２は、ビットマップ３０３を読取り、たとえば、非ゼロアクティベーション値を有する少なくとも２つのメモリアドレスを決定する。コントローラ３０２がゼロアクティベーション値を提供し、その後、ゼロアクティベーション値についての計算をスキップするかまたは不可能にするように構成されている場合、コントローラ３０２はまた、ゼロアクティベーション値を有する少なくとも１つのメモリアドレスを決定してもよい。この実現例においては、コントローラ３０２は、上述のレジスタを参照して、第１の入力アクティベーションについての対応する重み（およびメモリアドレス）を決定するとともに、第２の入力アクティベーションについての対応する重み（およびメモリアドレス）を決定することができる。
As described above, the
上述のとおり、コントローラ３０２はメモリにおいて１つ以上のアドレスレジスタを維持する。このため、オペランド（入力アクティベーションおよび重み）の潜在的な如何なる不整合をも軽減するかまたは防ぐために、ゼロ値入力アクティベーションが検出されると、コントローラ３０２は、対応する計算ユニットを使用不可能にし、特定の重みのロードをスキップし、次の非ゼロ入力アクティベーションのために適切な対応する重み（およびメモリアドレス）を検索して、所与のニューラルネットワーク層のための出力アクティベーションの計算を再開することができる。
As mentioned above, the
いくつかの実現例においては、第１のニューラルネットワーク層において計算された出力アクティベーションは、ネットワークにおける次の第２の層（たとえば、ネットワークのうち次の隠された層または出力層）への入力アクティベーションとして用いられる。概して、ニューラルネットワークの各層は、対応するそれぞれのパラメータのセットの現在値に従って、受信された入力から出力を生成する。いくつかの場合には、コントローラ３０２は、出力アクティベーションバス３０８に提供された出力アクティベーションに関連付けられたデータストリームを分析するために、プログラムされた命令（すなわち、出力論理）を実行することができる。出力アクティベーションデータストリームを分析することにより、コントローラ３０２が、出力アクティベーションの各々に関連付けられた値がゼロ値であるかまたは非ゼロ値であるかを検出または判断することが可能となり得る。コントローラ３０２は、例示的な出力アクティベーションデータストリームを分析するとともに、検出された非ゼロアクティベーション値の各々をビットマップ３０５にマップすることができる。ビットマップ３０５におけるマップされた非ゼロアクティベーション値を用いることにより、ネットワークにおける次の第２の層に関連付けられた計算に関与する後続の計算システム３００に対して、入力アクティベーションとして非ゼロ値だけを供給することができる。
In some implementations, the output activations calculated in the first neural network layer are used as input activations to the next second layer in the network (e.g., the next hidden layer or output layer of the network). Generally, each layer of a neural network generates outputs from received inputs according to the current values of a corresponding respective set of parameters. In some cases, the
代替的な実現例においては、いくつかの計算動作があってもよく、これら計算動作においては、単一の非ゼロ入力アクティベーションが、パラメータ構造１０４の所与の次元要素についてのさまざまな重みを包含する（すなわち、「ｘ」または「ｙ」次元を繰り返す）いくつかの乗算演算のためのオペランドとして用いられる。たとえば、コントローラ３０２がメモリバンク１０８に第１の入力アクティベーション（たとえば、非ゼロ値）を提供させると、パラメータ構造１０４ａがアクティベーションを受信し、所与のアドレスにおける対応する重みがまたパラメータ構造１０４ａにロードされる。パラメータ構造１０４ａは、第１の入力アクティベーションがＫ回の計算サイクルにわたって影響を及ぼす（たとえば、変数「Ｋ」によって示される）部分和の特定の数値を更新し始めることとなる。結果として、これらのＫ回のサイクルの間、パラメータ構造１０４ａは追加の入力アクティベーションを受信することはないだろう。次いで、コントローラ３０２は、次の入力アクティベーションを入力アクティベーションバス３０６に提供させるための制御信号をメモリバンク１０８に提供することができる。
In an alternative implementation, there may be several computation operations in which a single non-zero input activation is used as an operand for several multiplication operations involving various weights for a given dimension element of the parameter structure 104 (i.e., repeating the "x" or "y" dimension). For example, when the
図４は、アクティベーション４０４を入力バス３０６を介して１つ以上の積和演算（ＭＡＣ）演算子に提供するメモリバンク１０８を含む例示的なアーキテクチャを示す。シフトレジスタ４０４はシフト機能を提供することができ、これにより、アクティベーション４０４が、ＭＡＣセル３０４における１つ以上のＭＡＣ演算子によって受信されるように、一度に１つずつ入力バス３０６に発送される。図示のとおり、一実現例においては、アクティベーション４０６は、ゼロのアクティベーション値を有している可能性もあり、したがって、ＭＡＣセル３０４によって消費されない可能性がある。
4 illustrates an exemplary architecture including a
概して、ＭＡＣ演算子を含むＭＡＣセル３０４は、部分和を計算するとともに、いくつかの実現例において、出力バス３０８に対して部分和データを書込むように構成された計算ユニットとして規定される。図示のとおり、セル３０４は１つ以上のＭＡＣ演算子から構成されていてもよい。一実現例においては、ＭＡＣセル３０４におけるＭＡＣ演算子の数はセルの発行幅と称される。一例として、デュアル発行セル（dual issue cell）は、
（メモリ１１０からの）２つのパラメータとの（メモリバンク１０８からの）２つのアクティベーション値の乗算を計算することができるとともに、２つの乗数の結果と現在の部分和との加算を実行することができる、２つのＭＡＣ演算子を備えたセルを指している。
In general, a
It refers to a cell with two MAC operators that can calculate the multiplication of two activation values (from memory bank 108) with two parameters (from memory 110) and perform an addition of the results of the two multipliers with the current partial sum.
上述のとおり、入力バス３０６は、線形ユニット（すなわち、ＭＡＣアレイ３０４）の
ＭＡＣ演算子に入力アクティベーションを提供する通信バスである。いくつかの実現例においては、同じ入力がすべてのＭＡＣ演算子間で共有される。入力バス３０６の幅は、入力アクティベーションを所与のＭＡＣアレイ３０４のための対応する数のセルに供給するのに十分に広くなければならない。入力バス３０６の構造を例示するために以下の例を検討する。線形ユニットにおけるセルの数が４に等しく、アクティベーション幅が８ビットに等しい場合、入力バス３０６は、サイクルごとに最大で４つまでの入力アクティベーションを提供するように構成することができる。この例においては、ＭＡＣアレイ３０４におけるすべてのセルが、提供される４つのアクティベーションのうちの１つにアクセスするだけとなるだろう。
As mentioned above, the
いくつかの例においては、命令データ３１２は、ＭＡＣアレイ３０４のセルが同じ入力アクティベーションを用いて計算を実行する必要があるだろうことを示し得る。これは、ＭＡＣアレイ３０４のセル内のＺｏｕｔパーティショニングと称されてもよい。同様に、ＭＡＣアレイ３０４のセルが計算を実行するために異なるアクティベーションを必要とする場合、セル内においてＺｉｎパーティショニングが行なわれる。前者の場合、単一の入力アクティベーションが４回複製されるとともに、メモリバンク１０８から読取られた４つのアクティベーションが４サイクルにわたって提供される。後者の場合、サイクルごとにメモリバンク１０８の読取りが必要となる。
In some examples, the instruction data 312 may indicate that cells of the
図５は、パラメータ計算を減らして入力データのスパース性を活用するためのプロセスの例示的なフローチャートである。ブロック５０２において、計算システム３００は、ゼロアクティベーション値または非ゼロアクティベーション値のいずれかを有する入力アクティベーションを受信する。上述のとおり、いくつかの実現例においては、計算システム３００は、例示的なニューラルネットワークハードウェアシステムのホストインターフェイスデバイスまたは上位コントローラから入力アクティベーションを受信することができる。
FIG. 5 is an example flow chart of a process for reducing parameter calculations to exploit sparsity in input data. At
ブロック５０４において、コントローラ３０２は、入力アクティベーションの各々がゼロ値であるかまたは非ゼロ値であるかを判断する。いくつかの実現例においては、コントローラ３０２は、入力アクティベーションデータストリームを分析するとともに、ゼロ入力アクティベーション値（「０」）および非ゼロ入力アクティベーション値（「１」）に対応するバイナリ値を含むビットマップ３０３に対して、検出されたゼロ値および非ゼロ値の各々をマップする。
At block 504, the
ブロック５０６において、コントローラ３０２は、受信した入力アクティベーションをメモリバンク１０８に格納する。入力アクティベーションを格納することは、コントローラ３０２が、非ゼロ値を含む入力アクティベーションを有する１つ以上のメモリアドレス位置のインデックスを生成することを含み得る。いくつかの実現例においては、インデックスはビットマップ３０３に基づいて作成される。たとえば、ビットマップ３０３の各ビットが非ゼロアクティベーション値またはゼロアクティベーション値のいずれかを示しているので、入力アクティベーションをメモリバンク１０８に書込む際に非ゼロ値を有するメモリアドレス位置のインデックスを作成するためにコントローラ３０２によってビットマップ３０３が参照され得る。
At
ブロック５０８において、コントローラ３０２が、少なくとも１つの入力アクティベーションをメモリバンク１０８からデータバス３０６に対して提供する。いくつかの実現例においては、入力アクティベーションは、少なくとも部分的に、インデックスにおいて識別されたメモリアドレス位置から提供される。上述のとおり、インデックスは、非ゼロ値を備えた入力アクティベーションを格納するすべてのメモリアドレス位置を識別する。データバス３０６は、計算アレイのうちの１つ以上のユニットによってアクセス可能である
。計算アレイのユニットは、行列乗算に関する計算を実行するために１つ以上の非ゼロアクティベーション値をデータバス３０６から受信する。いくつかの実現例においては、計算システム３００は、単に、インデックス付きアドレスに対応するメモリアドレスから入力アクティベーションを提供するだけであるだろう。計算システム３００がこの通信スキームを用いる場合、ゼロ乗算を排除することよって計算効率を向上させることができる。
At
ブロック５１０において、インデックス付きアドレスから入力アクティベーションだけではなくすべてのアクティベーション値が提供される実現例においては、コントローラ３０２は、入力アクティベーションが、非ゼロアクティベーション値を含むいずれのインデックス付きアドレスにも関連付けられていないメモリアドレスから提供されていることを検出する。この検出するステップに応じて、コントローラ３０２は、ゼロ入力に関連付けられた乗算演算を防ぐための制御信号を計算アレイのうちの少なくとも１つのユニットに提供することができる。計算システム３００がこの通信スキームを用いる場合、有用な結果（たとえば、部分和または出力アクティベーションの計算を含む有用な結果）をもたらさない不必要または無駄な計算を防ぐことによって、エネルギの節約を実現することができる。
In block 510, in an implementation in which all activation values are provided from indexed addresses rather than just input activations, the
本明細書に記載されている主題および機能的動作の実施形態は、デジタル電子回路で実現されてもよく、有形的に具体化されたコンピュータソフトウェアもしくはファームウェアで実現されてもよく、本明細書に開示されている構造およびそれらの構造的等価物を含むコンピュータハードウェアで実現されてもよく、またはそれらのうちの１つ以上の組合せで実現されてもよい。本明細書中に記載される主題の実施形態は、１つ以上のコンピュータプログラムとして、すなわちデータ処理装置による実行のために、またはデータ処理装置の動作を制御するために有形の非一時的なプログラムキャリア上で符号化されるコンピュータプログラム命令の１つ以上のモジュールとして、実現されてもよい。代替的または付加的には、プログラム命令は、データ処理装置によって実行されるのに適した受信機装置に送信される情報を符号化するために生成される人為的に生成された伝搬信号（たとえば機械によって生成された電気信号、光学信号または電磁気信号）で符号化することができる。コンピュータ記憶媒体は、機械読取り可能なストレージデバイス、機械読取り可能なストレージ基板、ランダムもしくはシリアルアクセスメモリデバイス、またはこれらのうちの１つ以上の組合せであってもよい。 Embodiments of the subject matter and functional operations described herein may be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware including the structures disclosed herein and their structural equivalents, or in a combination of one or more of them. Embodiments of the subject matter described herein may be implemented as one or more computer programs, i.e., as one or more modules of computer program instructions encoded on a tangible non-transitory program carrier for execution by or to control the operation of a data processing apparatus. Alternatively or additionally, the program instructions may be encoded in an artificially generated propagated signal (e.g., a machine-generated electrical, optical or electromagnetic signal) generated to encode information to be transmitted to a receiver device suitable for execution by the data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of these.
本明細書中に記載されているプロセスおよび論理フローは、入力データ上で動作して出力を生成することによって機能を実行するように１つ以上のコンピュータプログラムを実行する１つ以上のプログラム可能なプロセッサによって実行され得る。また、プロセスおよび論理フローは、特殊用途論理回路、たとえばフィールド・プログラマブル・ゲート・アレイ（field programmable gate array：ＦＰＧＡ）または特殊用途向け集積回路（application specific integrated circuit：ＡＳＩＣ）、汎用グラフィック処理ユニット（General purpose graphics processing unit：ＧＰＧＰＵ）、または他のいくつかの処理ユニットとして実現されてもよい。 The processes and logic flows described herein may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows may also be implemented as special purpose logic circuitry, such as a field programmable gate array (FPGA) or application specific integrated circuit (ASIC), a general purpose graphics processing unit (GPGPU), or some other processing unit.
コンピュータプログラムの実行に適したコンピュータは、汎用マイクロプロセッサ、特殊用途マイクロプロセッサもしくはこれら両方、または、他の任意の種類の中央処理装置を含み、一例として、汎用マイクロプロセッサ、特殊用途マイクロプロセッサもしくはこれら両方、または、他の任意の種類の中央処理装置に基づいていてもよい。概して、中央処理装置は、読出し専用メモリまたはランダムアクセスメモリまたはそれら両方から命令およびデータを受信するであろう。コンピュータの必須の要素は、命令を実行または実施するための中央処理装置と、命令およびデータを格納するための１つ以上のメモリデバイスとである。一般に、コンピュータは、データを格納するための１つ以上の大容量記憶装置、たとえば磁気ディスク、光磁気ディスクまたは光ディスクも含み得るか、または、当
該１つ以上の大容量記憶装置からデータを受信するもしくは当該１つ以上の大容量記憶装置にデータを転送するように、もしくは受信も転送も行なうように動作可能に結合されるであろう。しかし、コンピュータは、このようなデバイスを有する必要はない。
A computer suitable for executing a computer program includes, and may be based on, a general purpose microprocessor, a special purpose microprocessor, or both, or any other type of central processing unit, by way of example. In general, the central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or carrying out instructions, and one or more memory devices for storing instructions and data. In general, a computer may also include one or more mass storage devices, such as magnetic disks, magneto-optical disks, or optical disks, for storing data, or will be operatively coupled to receive data from or transfer data to the one or more mass storage devices, or both. However, a computer need not have such devices.
コンピュータプログラム命令およびデータを格納するのに適したコンピュータ読取り可能な媒体は、すべての形態の不揮発性メモリ、媒体およびメモリデバイスを含み、一例としてたとえばＥＰＲＯＭ、ＥＥＰＲＯＭおよびフラッシュメモリデバイスといった半導体メモリデバイス、たとえば内蔵ハードディスクまたは取外し可能なディスクといった磁気ディスクを含む。プロセッサおよびメモリは、特殊用途論理回路によって補完されてもよく、または特殊用途論理回路に組込まれてもよい。 Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media, and memory devices, including, by way of example, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices, and magnetic disks, such as internal hard disks or removable disks. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
本明細書は多くの具体的な実現例の詳細を含んでいるが、これらは、いずれかの発明の範囲またはクレームされ得るものの範囲を限定するものとして解釈されるべきではなく、むしろ特定の発明の特定の実施形態に特有となり得る特徴の説明として解釈されるべきである。別々の実施形態の文脈において本明細書に記載されている特定の特徴は、単一の実施形態において組合わせて実現されてもよい。逆に、単一の実施形態の文脈において記載されているさまざまな特徴は、複数の実施形態において別々に、または任意の好適なサブコンビネーションで実現されてもよい。さらに、特徴は特定の組合せで作用するものとして記載され得るとともに、さらにはそのようなものとして最初にクレームされ得るが、クレームされている組合せのうちの１つ以上の特徴は、場合によっては、当該組合せから削除されてもよく、クレームされている組合せは、サブコンビネーションまたはサブコンビネーションの変形例に向けられてもよい。 While the specification contains many specific implementation details, these should not be construed as limiting the scope of any invention or what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of a particular invention. Certain features that are described in this specification in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment may also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described as working in a particular combination, and even initially claimed as such, one or more features of a claimed combination may, in some cases, be deleted from the combination, and the claimed combination may be directed to a subcombination or a variation of the subcombination.
同様に、動作は特定の順序で図面に示されているが、これは、このような動作が、望ましい結果を達成するために、示されている特定の順序もしくは連続した順序で実行されなければならないと理解されるべきではなく、または、望ましい結果を達成するために、すべての示されている動作が実行されなければならないと理解されるべきではない。特定の状況では、マルチタスクおよび並列処理が有利である可能性がある。さらに、上述の実施形態におけるさまざまなシステムモジュールおよび構成要素の分離は、すべての実施形態においてこのような分離が必要であると理解されるべきではなく、記載されているプログラム構成要素およびシステムが、一般に単一のソフトウェア製品に一体化されてもよく、または複数のソフトウェア製品にパッケージングされてもよいことが理解されるべきである。 Similarly, although operations are shown in the figures in a particular order, this should not be understood as requiring such operations to be performed in the particular order or sequential order shown to achieve a desired result, or that all of the operations shown must be performed to achieve a desired result. In certain circumstances, multitasking and parallel processing may be advantageous. Furthermore, the separation of various system modules and components in the above-described embodiments should not be understood as requiring such separation in all embodiments, and it should be understood that the program components and systems described may generally be integrated into a single software product or packaged in multiple software products.
さらなる実現例が以下の例において要約される。
例１：コンピュータにより実現される方法であって、コンピューティングデバイスが、少なくとも部分的に、当該コンピューティングデバイス外にあるソースから提供される複数の入力アクティベーションを受信するステップと、当該コンピューティングデバイスのコントローラが、当該複数の入力アクティベーションの各々がゼロ値または非ゼロ値のうちのいずれを有するかを判断するステップと、当該コンピューティングデバイスのメモリバンクに当該入力アクティベーションのうち少なくとも１つを格納するステップと、当該コントローラが、非ゼロ値である入力アクティベーション値を有する１つ以上のメモリアドレス位置を含むインデックスを生成するステップと、当該コントローラが、当該メモリバンクから、少なくとも１つの入力アクティベーションを、計算アレイのうちの１つ以上のユニットによってアクセス可能なデータバスに対して提供するステップとを含む。当該アクティベーションは、少なくとも部分的に、当該インデックスに関連付けられたメモリアドレス位置から提供される。
Further implementations are summarized in the following examples.
Example 1: A computer-implemented method comprising: a computing device receiving a plurality of input activations provided at least in part from a source external to the computing device, a controller of the computing device determining whether each of the plurality of input activations has either a zero value or a non-zero value, storing at least one of the input activations in a memory bank of the computing device, the controller generating an index including one or more memory address locations having input activation values that are non-zero values, and the controller providing at least one input activation from the memory bank to a data bus accessible by one or more units of a computational array, the activation provided at least in part from a memory address location associated with the index.
例２：例１の方法であって、当該インデックスは、複数のビットを含むビットマップに基づいて作成され、当該ビットマップのうちの各々のビットは、非ゼロ入力アクティベー
ション値またはゼロ入力アクティベーション値のうち少なくとも１つを示す。
Example 2: The method of example 1, wherein the index is created based on a bitmap including a plurality of bits, each bit of the bitmap indicating at least one of a non-zero-input activation value or a zero-input activation value.
例３：例１または例２の方法であって、非ゼロ値を有する第１の入力アクティベーションを提供して、少なくとも１つのユニットが当該非ゼロ値を用いて計算を実行し、その後、ゼロ値を有する第２の入力アクティベーションを提供し、少なくとも１つのユニットにおいて、当該ゼロ値を用いて実行される可能性のある計算を防止するステップをさらに含む。 Example 3: The method of Example 1 or Example 2, further comprising providing a first input activation having a non-zero value such that at least one unit performs a calculation using the non-zero value, and thereafter providing a second input activation having a zero value such that at least one unit prevents a calculation that may be performed using the zero value.
例４：例３の方法であって、当該防止するステップは、当該入力アクティベーションが当該インデックスに関連付けられていないメモリアドレス位置から提供されると当該コントローラが判断することに応じて、行なわれる。 Example 4: The method of example 3, wherein the preventing step is performed in response to the controller determining that the input activation is provided from a memory address location that is not associated with the index.
例５：例４の方法であって、当該入力アクティベーションが当該インデックスに関連付けられていないメモリアドレス位置から提供されることを、当該コントローラが検出するステップと、当該検出するステップに応じて、当該ゼロ入力アクティベーション値に関連付けられた乗算演算を防止するための制御信号を当該計算アレイのうちの少なくとも１つのユニットに提供するステップとをさらに含む。 Example 5: The method of Example 4, further comprising: the controller detecting that the input activation is provided from a memory address location not associated with the index; and, in response to the detecting, providing a control signal to at least one unit of the computational array to prevent a multiplication operation associated with the zero input activation value.
例６：例１から例５のうちのいずれか１つの方法であって、当該方法は、当該コントローラが、テンソル計算のうち第１の入力アクティベーションを用いる第１の部分を第１のユニットにマップし、当該テンソル計算のうち当該第１の入力アクティベーションも用いる第２の部分を当該第１のユニットとは異なる第２のユニットにマップするステップをさらに含む。 Example 6: The method of any one of Examples 1 to 5, further comprising the step of the controller mapping a first portion of the tensor computation that uses a first input activation to a first unit and mapping a second portion of the tensor computation that also uses the first input activation to a second unit different from the first unit.
例７：例１から例６のうちのいずれか１つの方法であって、単一の入力アクティベーションを連続的に当該データバスに対して提供するステップをさらに含み、当該単一の入力アクティベーションは、当該インデックスに関連付けられたメモリアドレス位置からアクセスおよび選択される。 Example 7: The method of any one of Examples 1 to 6, further comprising the step of continuously providing a single input activation to the data bus, the single input activation being accessed and selected from a memory address location associated with the index.
例８：例１から例７のうちのいずれか１つの方法であって、当該提供するステップはさらに、ゼロ値を有する入力アクティベーションを提供しないステップを含む。 Example 8: The method of any one of Examples 1 to 7, wherein the providing step further includes not providing an input activation having a zero value.
例９：１つ以上の処理デバイスによって実行可能であるとともに以下の動作を実行するための命令を格納する１つ以上の機械読取り可能ストレージデバイスであって、当該以下の動作は、コンピューティングデバイスが、少なくとも部分的に、当該コンピューティングデバイス外にあるソースから提供される複数の入力アクティベーションを受信する動作と、当該コンピューティングデバイスのコントローラが、当該複数の入力アクティベーションの各々がゼロ値または非ゼロ値のうちのいずれを有するかを判断する動作と、当該コンピューティングデバイスのメモリバンクに当該入力アクティベーションのうち少なくとも１つを格納する動作と、当該コントローラが、非ゼロ値である入力アクティベーション値を有する１つ以上のメモリアドレス位置を含むインデックスを生成する動作と、当該コントローラが、当該メモリバンクから、少なくとも１つの入力アクティベーションを、計算アレイのうちの１つ以上のユニットによってアクセス可能なデータバスに対して提供する動作とを含む。当該アクティベーションは、少なくとも部分的に、当該インデックスに関連付けられたメモリアドレス位置から提供される。 Example 9: One or more machine-readable storage devices executable by one or more processing devices and storing instructions for performing the following operations: a computing device receiving a plurality of input activations provided at least in part from a source external to the computing device; a controller of the computing device determining whether each of the plurality of input activations has a zero value or a non-zero value; storing at least one of the input activations in a memory bank of the computing device; the controller generating an index including one or more memory address locations having input activation values that are non-zero values; and the controller providing at least one input activation from the memory bank to a data bus accessible by one or more units of a computational array. The activation is provided at least in part from a memory address location associated with the index.
例１０：例９の機械読取り可能ストレージデバイスであって、当該インデックスは、複数のビットを含むビットマップに基づいて作成され、当該ビットマップのうちの各々のビットは、非ゼロ入力アクティベーション値またはゼロ入力アクティベーション値のうち少なくとも１つを示す。 Example 10: The machine-readable storage device of Example 9, wherein the index is created based on a bitmap including a plurality of bits, each bit of the bitmap indicating at least one of a non-zero input activation value or a zero input activation value.
例１１：例９または例１０の機械読取り可能ストレージデバイスであって、非ゼロ値を有する第１の入力アクティベーションを提供して、少なくとも１つのユニットが当該非ゼロ値を用いて計算を実行し、その後、ゼロ値を有する第２の入力アクティベーションを提供し、少なくとも１つのユニットにおいて、当該ゼロ値を用いて実行される可能性のある計算を防止する動作をさらに含む。 Example 11: The machine-readable storage device of example 9 or example 10, further comprising an operation of providing a first input activation having a non-zero value, causing at least one unit to perform a calculation using the non-zero value, and thereafter providing a second input activation having a zero value, causing at least one unit to prevent a calculation that may be performed using the zero value.
例１２：例１１の機械読取り可能ストレージデバイスであって、当該防止する動作は、当該入力アクティベーションが当該インデックスに関連付けられていないメモリアドレス位置から提供されると当該コントローラが判断することに応じて、行なわれる。 Example 12: The machine-readable storage device of example 11, wherein the preventing action is performed in response to the controller determining that the input activation is provided from a memory address location that is not associated with the index.
例１３：例１２の機械読取り可能ストレージデバイスであって、当該入力アクティベーションが当該インデックスに関連付けられていないメモリアドレス位置から提供されることを、当該コントローラが検出する動作と、当該検出する動作に応じて、当該ゼロ入力アクティベーション値に関連付けられた乗算演算を防止するための制御信号を当該計算アレイのうちの少なくとも１つのユニットに提供する動作とをさらに含む。 Example 13: The machine-readable storage device of Example 12, further comprising: the controller detecting that the input activation is provided from a memory address location not associated with the index; and, in response to the detecting, providing a control signal to at least one unit of the computational array to prevent a multiplication operation associated with the zero input activation value.
例１４：例９から例１３のうちのいずれか１つの機械読取り可能ストレージデバイスであって、当該以下の動作はさらに、当該コントローラが、テンソル計算のうち第１の入力アクティベーションを用いる第１の部分を第１のユニットにマップし、当該テンソル計算のうち当該第１の入力アクティベーションも用いる第２の部分を当該第１のユニットとは異なる第２のユニットにマップする動作を含む。 Example 14: The machine-readable storage device of any one of Examples 9 to 13, wherein the following operations further include the controller mapping a first portion of the tensor computation that uses a first input activation to a first unit and mapping a second portion of the tensor computation that also uses the first input activation to a second unit different from the first unit.
例１５：電子システムは、コンピューティングデバイスに配置されるとともに１つ以上の処理デバイスを含むコントローラと、当該１つ以上の処理デバイスによって実行可能であるとともに以下の動作を実行するための命令を格納するための１つ以上の機械読取り可能ストレージデバイスとを含み、当該以下の動作は、当該コンピューティングデバイスが、少なくとも部分的に、当該コンピューティングデバイス外にあるソースから提供される複数の入力アクティベーションを受信する動作と、当該コントローラが、当該複数の入力アクティベーションの各々がゼロ値または非ゼロ値のうちのいずれを有するかを判断する動作と、当該コンピューティングデバイスのメモリバンクに当該入力アクティベーションのうち少なくとも１つを格納する動作と、当該コントローラが、非ゼロ値である入力アクティベーション値を有する１つ以上のメモリアドレス位置を含むインデックスを生成する動作と、当該コントローラが、当該メモリバンクから、少なくとも１つの入力アクティベーションを、計算アレイのうちの１つ以上のユニットによってアクセス可能なデータバスに対して提供する動作とを含む。当該アクティベーションは、少なくとも部分的に、当該インデックスに関連付けられたメモリアドレス位置から提供される。 Example 15: An electronic system includes a controller disposed in a computing device and including one or more processing devices, and one or more machine-readable storage devices executable by the one or more processing devices and for storing instructions for performing the following operations, the computing device receiving a plurality of input activations provided at least in part from a source external to the computing device, the controller determining whether each of the plurality of input activations has a zero value or a non-zero value, storing at least one of the input activations in a memory bank of the computing device, the controller generating an index including one or more memory address locations having input activation values that are non-zero values, and the controller providing at least one input activation from the memory bank to a data bus accessible by one or more units of a computational array. The activation is provided at least in part from a memory address location associated with the index.
例１６：例１５の電子システムであって、当該インデックスは、複数のビットを含むビットマップに基づいて作成され、当該ビットマップのうちの各々のビットは、非ゼロ入力アクティベーション値またはゼロ入力アクティベーション値のうち少なくとも１つを示す。 Example 16: The electronic system of Example 15, wherein the index is created based on a bitmap including a plurality of bits, each bit of the bitmap indicating at least one of a non-zero input activation value or a zero input activation value.
例１７：例１５または例１６の電子システムであって、非ゼロ値を有する第１の入力アクティベーションを提供して、少なくとも１つのユニットが当該非ゼロ値を用いて計算を実行し、その後、ゼロ値を有する第２の入力アクティベーションを提供し、少なくとも１つのユニットにおいて、当該ゼロ値を用いて実行される可能性のある計算を防止する動作をさらに含む。 Example 17: The electronic system of Example 15 or Example 16, further comprising an operation of providing a first input activation having a non-zero value, causing at least one unit to perform a calculation using the non-zero value, and thereafter providing a second input activation having a zero value, preventing at least one unit from performing a calculation that may be performed using the zero value.
例１８：例１７の電子システムであって、当該防止する動作は、当該入力アクティベー
ションが当該インデックスに関連付けられていないメモリアドレス位置から提供されると当該コントローラが判断することに応じて、行なわれる。
Example 18: The electronic system of example 17, wherein the preventing action is performed in response to the controller determining that the input activation is provided from a memory address location not associated with the index.
例１９：例１７または例１８の電子システムであって、当該入力アクティベーションが当該インデックスに関連付けられていないメモリアドレス位置から提供されることを、当該コントローラが検出する動作と、当該検出する動作に応じて、当該ゼロ入力アクティベーション値に関連付けられた乗算演算を防止するための制御信号を当該計算アレイのうちの少なくとも１つのユニットに提供する動作とをさらに含む。 Example 19: The electronic system of Example 17 or Example 18, further comprising: the controller detecting that the input activation is provided from a memory address location not associated with the index; and, in response to the detecting, providing a control signal to at least one unit of the computational array to prevent a multiplication operation associated with the zero input activation value.
例２０：例１５から例１９のうちのいずれか１つの電子システムであって、当該以下の動作はさらに、当該コントローラが、テンソル計算のうち第１の入力アクティベーションを用いる第１の部分を第１のユニットにマップし、当該テンソル計算のうち当該第１の入力アクティベーションも用いる第２の部分を当該第１のユニットとは異なる第２のユニットにマップする動作を含む。 Example 20: The electronic system of any one of Examples 15 to 19, wherein the following operations further include the controller mapping a first portion of the tensor calculation that uses a first input activation to a first unit and mapping a second portion of the tensor calculation that also uses the first input activation to a second unit different from the first unit.
主題の特定の実施形態が記載されてきた。他の実施形態は添付の特許請求の範囲内である。たとえば、特許請求の範囲に記載されている動作は、異なる順序で実行することもで、依然として望ましい結果を達成することができる。一例として、添付の図面に示されるプロセスは、所望の結果を達成するために、必ずしも、図示される特定の順序または連続的順序を必要とするものではない。いくつかの実現例においては、マルチタスクおよび並列処理が有利であるかもしれない。 Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. By way of example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some implementations, multitasking and parallel processing may be advantageous.
Claims (17)
コントローラを備え、前記コントローラは、
第１のニューラルネットワーク層を介する処理のために入力のバッチを受信し、
前記入力のバッチ内の各入力のそれぞれの値に基づいて入力の圧縮された表現を生成するように構成され、前記入力の圧縮された表現は前記入力のバッチよりも少ない数の入力を含み、前記入力の圧縮された表現内の各入力は非ゼロ値を有し、前記ハードウェア回路はさらに、
前記入力の圧縮された表現内の各入力を格納するように構成されたメモリ位置を識別するアドレスを格納するように構成されたアドレスレジスタと、
積和演算セルとを備え、前記積和演算セルは、
前記アドレスレジスタ内のアドレスによって識別されるメモリ位置から前記入力の圧縮された表現内の入力を受信し、
前記第１のニューラルネットワーク層を介して前記入力を処理するように構成され、前記処理することは、前記入力と前記ニューラルネットワーク層についての対応する重み値との間で乗算を実行して前記ニューラルネットワーク層についての出力を生成することを含む、ハードウェア回路。 1. A hardware circuit configured to implement a neural network including a plurality of neural network layers, comprising:
a controller, the controller comprising:
receiving a batch of inputs for processing through a first neural network layer;
generating a compressed representation of an input based on a respective value of each input in the batch of inputs , the compressed representation of the input including a number of inputs fewer than the batch of inputs, each input in the compressed representation of the input having a non-zero value, the hardware circuit further configured to:
an address register configured to store an address identifying a memory location configured to store each input in the compressed representation of the input;
a multiply-accumulate cell, the multiply-accumulate cell comprising:
receiving an input in a compressed representation of the input from a memory location identified by an address in the address register;
a hardware circuit configured to process the input through the first neural network layer, the processing including performing multiplications between the input and corresponding weight values for the neural network layer to generate outputs for the neural network layer.
前記複数のニューラルネットワーク層の各々についての複数の重み値を格納するように構成されたパラメータメモリとを含む、請求項１に記載のハードウェア回路。 an activation memory configured to store each input in the batch of inputs and each input in a compressed representation of the input;
and a parameter memory configured to store a plurality of weight values for each of the plurality of neural network layers.
前記入力の圧縮された表現内の各入力を前記ハードウェア回路の前記アクティベーションメモリの対応するメモリ位置に格納するために制御信号を生成するように構成され、前記アクティベーションメモリの各メモリ位置は、前記アドレスレジスタに格納されている対応するアドレスによって識別される、請求項２に記載のハードウェア回路。 The controller:
3. The hardware circuit of claim 2, configured to generate control signals to store each input in the compressed representation of the input in a corresponding memory location of the activation memory of the hardware circuit, each memory location of the activation memory being identified by a corresponding address stored in the address register.
前記入力の圧縮された表現についての第１の入力行列を受信するように構成され、前記第１の入力行列は非ゼロ値を有する入力を含み、前記計算ユニットはさらに、
前記第１のニューラルネットワーク層についての重み行列の重み値の１セットを受信し、
前記複数の積和演算セルを用いて前記第１のニューラルネットワーク層を介して前記第１の入力行列の前記入力を処理するように構成され、前記処理することは、前記第１の入力行列内の各入力と前記重み値のセット内の重み値との間で行列乗算を実行して、計算の第１の量に基づいて前記ニューラルネットワーク層についての複数の出力を生成することを含む、請求項２または３に記載のハードウェア回路。 A calculation unit including a plurality of multiply-accumulate cells, the calculation unit comprising:
The computation unit is configured to receive a first input matrix for a compressed representation of the input, the first input matrix including inputs having non-zero values, the computation unit further comprising:
receiving a set of weight values of a weight matrix for the first neural network layer;
4. The hardware circuit of claim 2 or 3, configured to process the inputs of the first input matrix through the first neural network layer with the plurality of multiply-accumulate cells, the processing comprising performing a matrix multiplication between each input in the first input matrix and a weight value in the set of weight values to generate a plurality of outputs for the neural network layer based on a first amount of computation.
前記入力のバッチについての第２の入力行列を受信するように構成され、前記第２の入力行列はゼロ値を有する入力と非ゼロ値を有する入力とを含み、前記計算ユニットはさらに、
前記複数の積和演算セルを用いて、計算の第２の量に基づいて前記第１のニューラルネットワーク層を介して前記第２の入力行列の前記入力を処理するように構成される、請求項４に記載のハードウェア回路。 The computing unit comprises:
a second input matrix for the batch of inputs, the second input matrix including inputs having zero values and inputs having non-zero values, the computation unit further configured to:
5. The hardware circuit of claim 4 , configured to process the inputs of the second input matrix through the first neural network layer based on a second amount of computations using the plurality of multiply-accumulate cells.
前記入力の圧縮された表現内の各入力を前記計算ユニットに配置された対応する積和演算セルに連続的に提供するために制御信号を生成して、前記第１のニューラルネットワーク層を介して前記入力の圧縮された表現内の各入力を処理するように構成される、請求項５または６に記載のハードウェア回路。 The controller:
7. The hardware circuit of claim 5 or 6, configured to generate control signals to sequentially provide each input in the compressed representation of the input to a corresponding multiply-accumulate cell disposed in the computation unit to process each input in the compressed representation of the input through the first neural network layer.
前記ハードウェア回路のコントローラによって、第１のニューラルネットワーク層を介して処理するための入力のバッチを受信するステップと、
前記コントローラによって、前記入力のバッチ内の各入力のそれぞれの値に基づいて入力の圧縮された表現を生成するステップとを含み、前記入力の圧縮された表現は前記入力のバッチよりも少ない数の入力を含み、前記入力の圧縮された表現内の各入力は非ゼロ値を有し、前記方法はさらに、
前記ハードウェア回路のアドレスレジスタを用いて、前記入力の圧縮された表現内の各入力を格納するように構成されたメモリ位置を識別する１つ以上のアドレスを格納するステップと、
前記ハードウェア回路の計算ユニットによって、前記アドレスレジスタ内のアドレスによって識別されるメモリ位置から前記入力の圧縮された表現内の入力を受信するステップと、
前記計算ユニットの積和演算セルを用いて、前記第１のニューラルネットワーク層を介して前記入力を処理するステップとを含み、前記処理するステップは、前記入力と前記ニューラルネットワーク層についての対応する重み値との間で乗算を実行して、前記ニューラルネットワーク層についての出力を生成するステップを含む、方法。 1. A method for performing computations using hardware circuitry configured to implement a neural network including a plurality of neural network layers, comprising:
receiving, by a controller of the hardware circuit, a batch of inputs for processing through a first neural network layer;
and generating, by the controller, a condensed representation of the input based on a respective value of each input in the batch of inputs, the condensed representation of the input including fewer inputs than the batch of inputs, each input in the condensed representation of the input having a non-zero value, the method further comprising :
using an address register of the hardware circuit to store one or more addresses identifying memory locations configured to store each input in the compressed representation of the input;
receiving, by a computational unit of the hardware circuit, an input in a compressed representation of the input from a memory location identified by an address in the address register;
and processing the input through the first neural network layer using a multiply-accumulate cell of the computation unit, the processing step including performing multiplications between the input and corresponding weight values for the neural network layer to generate outputs for the neural network layer.
前記ハードウェア回路のパラメータメモリによって、前記複数のニューラルネットワーク層の各々についての複数の重み値を格納するステップとを含む、請求項８に記載の方法。 storing, by an activation memory of the hardware circuit, each input in the batch of inputs and each input in a compressed representation of the input;
and storing, by a parameter memory of the hardware circuit, a plurality of weight values for each of the plurality of neural network layers.
前記計算ユニットによって、前記パラメータメモリから、前記第１のニューラルネットワーク層についての重み行列の重み値のセットを受信するステップと、
前記計算ユニットによって、前記計算ユニットに配置された複数の積和演算セルを用いて前記第１のニューラルネットワーク層を介して前記第１の入力行列の前記入力を処理するステップとを含み、前記処理するステップは、前記第１の入力行列内の各入力と前記重み値のセット内の重み値との間で行列乗算を実行して、計算の第１の量に基づいて前記ニューラルネットワーク層についての複数の出力を生成するステップを含む、請求項９または１０に記載の方法。 receiving, by the computation unit, from the activation memory a first input matrix for a compressed representation of the input, the first input matrix including inputs having non-zero values;
receiving, by the computation unit, a set of weight values of a weight matrix for the first neural network layer from the parameter memory;
and processing, by the computation unit, the inputs of the first input matrix through the first neural network layer using a plurality of multiply-accumulate cells disposed in the computation unit, the processing comprising performing a matrix multiplication between each input in the first input matrix and a weight value in the set of weight values to generate a plurality of outputs for the neural network layer based on a first amount of computation.
前記複数の積和演算セルを用いて、計算の第２の量に基づいて前記第１のニューラルネットワーク層を介して前記第２の入力行列の前記入力を処理するステップを含む、請求項１１に記載の方法。 receiving, by the computation unit, a second input matrix for the batch of inputs, the second input matrix including inputs having zero values and inputs having non-zero values; and
12. The method of claim 11 , further comprising: processing the inputs of the second input matrix through the first neural network layer based on a second amount of computations using the plurality of multiply-accumulate cells.
前記１つ以上の処理デバイスのコントローラによって、第１のニューラルネットワーク層を介して処理するための入力のバッチを受信することと、
前記コントローラによって、前記入力のバッチ内の各入力のそれぞれの値に基づいて入力の圧縮された表現を生成することとを含み、前記入力の圧縮された表現は前記入力のバッチよりも少ない数の入力を含み、前記入力の圧縮された表現内の各入力は非ゼロ値を有し、前記複数の動作はさらに、
前記１つ以上の処理デバイスのアドレスレジスタを用いて、前記入力の圧縮された表現内の各入力を格納するように構成されたメモリ位置を識別する１つ以上のアドレスを格納することと、
前記１つ以上の処理デバイスの計算ユニットによって、前記アドレスレジスタ内のアドレスによって識別されるメモリ位置から前記入力の圧縮された表現内の入力を受信することと、
前記計算ユニットの積和演算セルを用いて、前記第１のニューラルネットワーク層を介して前記入力を処理することとを含み、前記処理することは、前記入力と前記ニューラルネットワーク層についての対応する重み値との間で乗算を実行して前記ニューラルネットワーク層についての出力を生成することを含む、コンピュータプログラム。 One or more computer programs including instructions for performing computations on a neural network having multiple neural network layers, the instructions being executable by one or more processing devices to cause the processing devices to perform a number of operations, the number of operations including:
receiving , by a controller of the one or more processing devices , a batch of inputs for processing through a first neural network layer;
generating , by the controller, a condensed representation of the input based on a respective value of each input in the batch of inputs, the condensed representation of the input including a fewer number of inputs than the batch of inputs, each input in the condensed representation of the input having a non-zero value, the operations further comprising:
storing , using an address register of the one or more processing devices , one or more addresses identifying memory locations configured to store each input in the compressed representation of the input;
receiving , by a computational unit of the one or more processing devices , an input in a compressed representation of the input from a memory location identified by an address in the address register;
and processing the input through the first neural network layer with a multiply-accumulate cell of the computation unit , the processing including performing multiplications between the input and corresponding weight values for the neural network layer to generate outputs for the neural network layer.
前記１つ以上の処理デバイスのアクティベーションメモリによって、前記入力のバッチ内の各入力および前記入力の圧縮された表現内の各入力を格納することと、
前記１つ以上の処理デバイスのパラメータメモリによって、前記複数のニューラルネットワーク層の各々についての複数の重み値を格納することとを含む、請求項１５に記載のコンピュータプログラム。 The plurality of operations include:
storing , by an activation memory of the one or more processing devices , each input in the batch of inputs and each input in a compressed representation of the input;
and storing, by a parameter memory of the one or more processing devices , a plurality of weight values for each of the plurality of neural network layers.
前記計算ユニットによって、前記アクティベーションメモリから、前記入力の圧縮された表現についての第１の入力行列を受信することを含み、前記第１の入力行列は非ゼロ値を有する入力を含み、前記複数の動作はさらに、
前記計算ユニットによって、前記パラメータメモリから、前記第１のニューラルネットワーク層についての重み行列の重み値のセットを受信することと、
前記計算ユニットによって、前記計算ユニットに配置された複数の積和演算セルを用いて前記第１のニューラルネットワーク層を介して前記第１の入力行列の前記入力を処理することとを含み、前記第１の入力行列の前記入力を処理することは、前記第１の入力行列内の各入力と前記重み値のセット内の重み値との間で行列乗算を実行して、計算の第１の量に基づいて前記ニューラルネットワーク層についての複数の出力を生成することを含む、請求項１６に記載のコンピュータプログラム。 The plurality of operations include:
receiving , by the computation unit, from the activation memory a first input matrix for a compressed representation of the input, the first input matrix including inputs having non-zero values, the operations further comprising:
receiving , by the computation unit, a set of weight values of a weight matrix for the first neural network layer from the parameter memory;
17. The computer program product of claim 16, further comprising: processing, by the computation unit, the inputs of the first input matrix through the first neural network layer using a plurality of multiply-accumulate cells disposed in the computation unit, wherein processing the inputs of the first input matrix comprises performing a matrix multiplication between each input in the first input matrix and a weight value in the set of weight values to generate a plurality of outputs for the neural network layer based on a first amount of computation.
Applications Claiming Priority (6)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/336,066 US10360163B2 (en) | 2016-10-27 | 2016-10-27 | Exploiting input data sparsity in neural network compute units |
US15/336,066 | 2016-10-27 | ||
US15/465,774 US9818059B1 (en) | 2016-10-27 | 2017-03-22 | Exploiting input data sparsity in neural network compute units |
US15/465,774 | 2017-03-22 | ||
JP2019523062A JP7134955B2 (en) | 2016-10-27 | 2017-08-22 | Exploitation of Sparsity of Input Data in Neural Network Computation Units |
PCT/US2017/047992 WO2018080624A1 (en) | 2016-10-27 | 2017-08-22 | Exploiting input data sparsity in neural network compute units |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019523062A Division JP7134955B2 (en) | 2016-10-27 | 2017-08-22 | Exploitation of Sparsity of Input Data in Neural Network Computation Units |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2022172258A JP2022172258A (en) | 2022-11-15 |
JP7469407B2 true JP7469407B2 (en) | 2024-04-16 |
Family
ID=60256363
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019523062A Active JP7134955B2 (en) | 2016-10-27 | 2017-08-22 | Exploitation of Sparsity of Input Data in Neural Network Computation Units |
JP2022138360A Active JP7469407B2 (en) | 2016-10-27 | 2022-08-31 | Exploiting sparsity of input data in neural network computation units |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019523062A Active JP7134955B2 (en) | 2016-10-27 | 2017-08-22 | Exploitation of Sparsity of Input Data in Neural Network Computation Units |
Country Status (9)
Country | Link |
---|---|
US (4) | US10360163B2 (en) |
EP (2) | EP3533003B1 (en) |
JP (2) | JP7134955B2 (en) |
KR (3) | KR20230061577A (en) |
CN (2) | CN108009626B (en) |
DE (2) | DE102017120452A1 (en) |
HK (1) | HK1254700A1 (en) |
SG (1) | SG11201903787YA (en) |
WO (1) | WO2018080624A1 (en) |
Families Citing this family (77)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9959498B1 (en) | 2016-10-27 | 2018-05-01 | Google Llc | Neural network instruction set architecture |
US10360163B2 (en) | 2016-10-27 | 2019-07-23 | Google Llc | Exploiting input data sparsity in neural network compute units |
US10175980B2 (en) | 2016-10-27 | 2019-01-08 | Google Llc | Neural network compute tile |
US10685285B2 (en) * | 2016-11-23 | 2020-06-16 | Microsoft Technology Licensing, Llc | Mirror deep neural networks that regularize to linear networks |
CN108205700B (en) * | 2016-12-20 | 2021-07-30 | 上海寒武纪信息科技有限公司 | Neural network operation device and method |
US11328037B2 (en) * | 2017-07-07 | 2022-05-10 | Intel Corporation | Memory-size- and bandwidth-efficient method for feeding systolic array matrix multipliers |
TWI680409B (en) * | 2017-07-08 | 2019-12-21 | 英屬開曼群島商意騰科技股份有限公司 | Method for matrix by vector multiplication for use in artificial neural network |
US10790828B1 (en) | 2017-07-21 | 2020-09-29 | X Development Llc | Application specific integrated circuit accelerators |
US10879904B1 (en) * | 2017-07-21 | 2020-12-29 | X Development Llc | Application specific integrated circuit accelerators |
US10725740B2 (en) * | 2017-08-31 | 2020-07-28 | Qualcomm Incorporated | Providing efficient multiplication of sparse matrices in matrix-processor-based devices |
US10902318B2 (en) | 2017-11-06 | 2021-01-26 | Neuralmagic Inc. | Methods and systems for improved transforms in convolutional neural networks |
GB2570186B (en) | 2017-11-06 | 2021-09-01 | Imagination Tech Ltd | Weight buffers |
US20190156214A1 (en) | 2017-11-18 | 2019-05-23 | Neuralmagic Inc. | Systems and methods for exchange of data in distributed training of machine learning algorithms |
US10936942B2 (en) * | 2017-11-21 | 2021-03-02 | Google Llc | Apparatus and mechanism for processing neural network tasks using a single chip package with multiple identical dies |
US10553207B2 (en) * | 2017-12-29 | 2020-02-04 | Facebook, Inc. | Systems and methods for employing predication in computational models |
CN111788583A (en) * | 2018-02-09 | 2020-10-16 | 渊慧科技有限公司 | Continuous sparsity pattern neural network |
CN111742331A (en) * | 2018-02-16 | 2020-10-02 | 多伦多大学管理委员会 | Neural network accelerator |
US10572568B2 (en) * | 2018-03-28 | 2020-02-25 | Intel Corporation | Accelerator for sparse-dense matrix multiplication |
US11216732B2 (en) | 2018-05-31 | 2022-01-04 | Neuralmagic Inc. | Systems and methods for generation of sparse code for convolutional neural networks |
US10963787B2 (en) | 2018-05-31 | 2021-03-30 | Neuralmagic Inc. | Systems and methods for generation of sparse code for convolutional neural networks |
US11449363B2 (en) | 2018-05-31 | 2022-09-20 | Neuralmagic Inc. | Systems and methods for improved neural network execution |
US10832133B2 (en) | 2018-05-31 | 2020-11-10 | Neuralmagic Inc. | System and method of executing neural networks |
WO2021054990A1 (en) * | 2019-09-16 | 2021-03-25 | Neuralmagic Inc. | Systems and methods for generation of sparse code for convolutional neural networks |
US10599429B2 (en) * | 2018-06-08 | 2020-03-24 | Intel Corporation | Variable format, variable sparsity matrix multiplication instruction |
US20200019836A1 (en) * | 2018-07-12 | 2020-01-16 | International Business Machines Corporation | Hierarchical parallelism in a network of distributed neural network cores |
CN110796244B (en) * | 2018-08-01 | 2022-11-08 | 上海天数智芯半导体有限公司 | Core computing unit processor for artificial intelligence device and accelerated processing method |
CN109344964B (en) * | 2018-08-08 | 2020-12-29 | 东南大学 | Multiply-add calculation method and calculation circuit suitable for neural network |
CN110826707B (en) * | 2018-08-10 | 2023-10-31 | 北京百度网讯科技有限公司 | Acceleration method and hardware accelerator applied to convolutional neural network |
US20200065659A1 (en) * | 2018-08-24 | 2020-02-27 | Samsung Electronics Co., Ltd. | Method of accelerating training process of neural network and neural network device thereof |
JP6985997B2 (en) * | 2018-08-27 | 2021-12-22 | 株式会社日立製作所 | Machine learning system and Boltzmann machine calculation method |
US11586417B2 (en) | 2018-09-28 | 2023-02-21 | Qualcomm Incorporated | Exploiting activation sparsity in deep neural networks |
US11636343B2 (en) | 2018-10-01 | 2023-04-25 | Neuralmagic Inc. | Systems and methods for neural network pruning with accuracy preservation |
CN111026440B (en) * | 2018-10-09 | 2022-03-29 | 上海寒武纪信息科技有限公司 | Operation method, operation device, computer equipment and storage medium |
JP7115211B2 (en) * | 2018-10-18 | 2022-08-09 | 富士通株式会社 | Arithmetic processing device and method of controlling arithmetic processing device |
CN111126081B (en) * | 2018-10-31 | 2023-07-21 | 深圳永德利科技股份有限公司 | Global universal language terminal and method |
US10768895B2 (en) | 2018-11-08 | 2020-09-08 | Movidius Limited | Dot product calculators and methods of operating the same |
KR20200057814A (en) * | 2018-11-13 | 2020-05-27 | 삼성전자주식회사 | Method for processing data using neural network and electronic device for supporting the same |
US11361050B2 (en) | 2018-11-20 | 2022-06-14 | Hewlett Packard Enterprise Development Lp | Assigning dependent matrix-vector multiplication operations to consecutive crossbars of a dot product engine |
EP3895071A1 (en) * | 2018-12-11 | 2021-10-20 | Mipsology SAS | Accelerating artificial neural network computations by skipping input values |
US10769527B2 (en) | 2018-12-11 | 2020-09-08 | Mipsology SAS | Accelerating artificial neural network computations by skipping input values |
JP7189000B2 (en) * | 2018-12-12 | 2022-12-13 | 日立Astemo株式会社 | Information processing equipment, in-vehicle control equipment, vehicle control system |
KR20200082613A (en) | 2018-12-31 | 2020-07-08 | 에스케이하이닉스 주식회사 | Processing system |
US11544559B2 (en) | 2019-01-08 | 2023-01-03 | Neuralmagic Inc. | System and method for executing convolution in a neural network |
US11604958B2 (en) | 2019-03-13 | 2023-03-14 | Samsung Electronics Co., Ltd. | Method and apparatus for processing computation of zero value in processing of layers in neural network |
KR20200111939A (en) | 2019-03-20 | 2020-10-05 | 에스케이하이닉스 주식회사 | accelerating Appratus of neural network and operating method thereof |
US20210026686A1 (en) * | 2019-07-22 | 2021-01-28 | Advanced Micro Devices, Inc. | Chiplet-integrated machine learning accelerators |
WO2021026225A1 (en) | 2019-08-08 | 2021-02-11 | Neuralmagic Inc. | System and method of accelerating execution of a neural network |
US11635893B2 (en) * | 2019-08-12 | 2023-04-25 | Micron Technology, Inc. | Communications between processors and storage devices in automotive predictive maintenance implemented via artificial neural networks |
KR20210024865A (en) | 2019-08-26 | 2021-03-08 | 삼성전자주식회사 | A method and an apparatus for processing data |
KR20210045225A (en) * | 2019-10-16 | 2021-04-26 | 삼성전자주식회사 | Method and apparatus for performing operation in neural network |
JP7462140B2 (en) | 2019-10-29 | 2024-04-05 | 国立大学法人 熊本大学 | Neural network circuit and neural network operation method |
US11244198B2 (en) | 2019-11-21 | 2022-02-08 | International Business Machines Corporation | Input partitioning for deep learning of large image data |
FR3105659B1 (en) * | 2019-12-18 | 2022-06-24 | Commissariat Energie Atomique | Method and apparatus for binary signal coding for implementing dynamic precision digital MAC operations |
KR102268817B1 (en) * | 2019-12-19 | 2021-06-24 | 국민대학교산학협력단 | Method and device for evaluating machine learning performance in a distributed cloud envirionment |
KR20210086233A (en) * | 2019-12-31 | 2021-07-08 | 삼성전자주식회사 | Method and apparatus for processing matrix data through relaxed pruning |
TWI727641B (en) * | 2020-02-03 | 2021-05-11 | 華邦電子股份有限公司 | Memory apparatus and operation method thereof |
US11586601B2 (en) * | 2020-02-05 | 2023-02-21 | Alibaba Group Holding Limited | Apparatus and method for representation of a sparse matrix in a neural network |
US11604975B2 (en) | 2020-04-09 | 2023-03-14 | Apple Inc. | Ternary mode of planar engine for neural processor |
CN111445013B (en) * | 2020-04-28 | 2023-04-25 | 南京大学 | Non-zero detector for convolutional neural network and method thereof |
KR102418794B1 (en) * | 2020-06-02 | 2022-07-08 | 오픈엣지테크놀로지 주식회사 | Method of accessing parameters for a hardware accelerator from a memory and a device for the same |
CN113835675A (en) * | 2020-06-23 | 2021-12-24 | 深圳市中兴微电子技术有限公司 | Data processing apparatus and data processing method |
US20230267310A1 (en) * | 2020-07-17 | 2023-08-24 | Sony Group Corporation | Neural network processing apparatus, information processing apparatus, information processing system, electronic device, neural network processing method, and program |
KR20220010362A (en) | 2020-07-17 | 2022-01-25 | 삼성전자주식회사 | Neural network apparatus and operating method of the same |
WO2022016257A1 (en) * | 2020-07-21 | 2022-01-27 | The Governing Council Of The University Of Toronto | System and method for using sparsity to accelerate deep learning networks |
US11928176B2 (en) * | 2020-07-30 | 2024-03-12 | Arm Limited | Time domain unrolling sparse matrix multiplication system and method |
US20220075669A1 (en) * | 2020-09-08 | 2022-03-10 | Technion Research And Development Foundation Ltd. | Non-Blocking Simultaneous MultiThreading (NB-SMT) |
US20230333816A1 (en) | 2020-09-30 | 2023-10-19 | Sony Semiconductor Solutions Corporation | Signal processing device, imaging device, and signal processing method |
KR20220049325A (en) * | 2020-10-14 | 2022-04-21 | 삼성전자주식회사 | Accelerator and electronic device including the same |
US11861328B2 (en) * | 2020-11-11 | 2024-01-02 | Samsung Electronics Co., Ltd. | Processor for fine-grain sparse integer and floating-point operations |
US11861327B2 (en) * | 2020-11-11 | 2024-01-02 | Samsung Electronics Co., Ltd. | Processor for fine-grain sparse integer and floating-point operations |
US11556757B1 (en) | 2020-12-10 | 2023-01-17 | Neuralmagic Ltd. | System and method of executing deep tensor columns in neural networks |
KR102541461B1 (en) | 2021-01-11 | 2023-06-12 | 한국과학기술원 | Low power high performance deep-neural-network learning accelerator and acceleration method |
US20220222386A1 (en) * | 2021-01-13 | 2022-07-14 | University Of Florida Research Foundation, Inc. | Decommissioning and erasing entropy in microelectronic systems |
US11853717B2 (en) * | 2021-01-14 | 2023-12-26 | Microsoft Technology Licensing, Llc | Accelerating processing based on sparsity for neural network hardware processors |
US11940907B2 (en) * | 2021-06-25 | 2024-03-26 | Intel Corporation | Methods and apparatus for sparse tensor storage for neural network accelerators |
US11669489B2 (en) * | 2021-09-30 | 2023-06-06 | International Business Machines Corporation | Sparse systolic array design |
US11960982B1 (en) | 2021-10-21 | 2024-04-16 | Neuralmagic, Inc. | System and method of determining and executing deep tensor columns in neural networks |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP5501317B2 (en) | 2011-09-21 | 2014-05-21 | 株式会社半導体理工学研究センター | Time difference amplifier circuit |
Family Cites Families (64)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US3754128A (en) | 1971-08-31 | 1973-08-21 | M Corinthios | High speed signal processor for vector transformation |
JPS4874139A (en) * | 1971-12-29 | 1973-10-05 | ||
JPS5364439A (en) * | 1976-11-20 | 1978-06-08 | Agency Of Ind Science & Technol | Linear coversion system |
JPS58134357A (en) | 1982-02-03 | 1983-08-10 | Hitachi Ltd | Array processor |
EP0156648B1 (en) | 1984-03-29 | 1992-09-30 | Kabushiki Kaisha Toshiba | Convolution arithmetic circuit for digital signal processing |
US5267185A (en) | 1989-04-14 | 1993-11-30 | Sharp Kabushiki Kaisha | Apparatus for calculating matrices |
US5138695A (en) | 1989-10-10 | 1992-08-11 | Hnc, Inc. | Systolic array image processing system |
JPH03167664A (en) | 1989-11-28 | 1991-07-19 | Nec Corp | Matrix arithmetic circuit |
WO1991019248A1 (en) | 1990-05-30 | 1991-12-12 | Adaptive Solutions, Inc. | Neural network using virtual-zero |
WO1991019267A1 (en) | 1990-06-06 | 1991-12-12 | Hughes Aircraft Company | Neural network processor |
US5287464A (en) | 1990-10-24 | 1994-02-15 | Zilog, Inc. | Semiconductor multi-device system with logic means for controlling the operational mode of a set of input/output data bus drivers |
JP3318753B2 (en) | 1991-12-05 | 2002-08-26 | ソニー株式会社 | Product-sum operation device and product-sum operation method |
AU658066B2 (en) * | 1992-09-10 | 1995-03-30 | Deere & Company | Neural network based control system |
JPH06139218A (en) | 1992-10-30 | 1994-05-20 | Hitachi Ltd | Method and device for simulating neural network completely in parallel by using digital integrated circuit |
US6067536A (en) * | 1996-05-30 | 2000-05-23 | Matsushita Electric Industrial Co., Ltd. | Neural network for voice and pattern recognition |
US5742741A (en) | 1996-07-18 | 1998-04-21 | Industrial Technology Research Institute | Reconfigurable neural network |
US5905757A (en) | 1996-10-04 | 1999-05-18 | Motorola, Inc. | Filter co-processor |
US6243734B1 (en) | 1998-10-30 | 2001-06-05 | Intel Corporation | Computer product and method for sparse matrices |
JP2001117900A (en) | 1999-10-19 | 2001-04-27 | Fuji Xerox Co Ltd | Neural network arithmetic device |
US20020044695A1 (en) * | 2000-05-05 | 2002-04-18 | Bostrom Alistair K. | Method for wavelet-based compression of video images |
JP2003244190A (en) | 2002-02-19 | 2003-08-29 | Matsushita Electric Ind Co Ltd | Processor for data flow control switch and data flow control switch |
US7016529B2 (en) * | 2002-03-15 | 2006-03-21 | Microsoft Corporation | System and method facilitating pattern recognition |
US7493498B1 (en) * | 2002-03-27 | 2009-02-17 | Advanced Micro Devices, Inc. | Input/output permission bitmaps for compartmentalized security |
US7426501B2 (en) | 2003-07-18 | 2008-09-16 | Knowntech, Llc | Nanotechnology neural network methods and systems |
US7818729B1 (en) * | 2003-09-15 | 2010-10-19 | Thomas Plum | Automated safe secure techniques for eliminating undefined behavior in computer software |
JP2007518199A (en) | 2004-01-13 | 2007-07-05 | ニューヨーク・ユニバーシティ | Method, system, storage medium, and data structure for image recognition using multiple linear independent element analysis |
GB2436377B (en) | 2006-03-23 | 2011-02-23 | Cambridge Display Tech Ltd | Data processing hardware |
CN101441441B (en) * | 2007-11-21 | 2010-06-30 | 新乡市起重机厂有限公司 | Design method of intelligent swing-proof control system of crane |
JP4513865B2 (en) | 2008-01-25 | 2010-07-28 | セイコーエプソン株式会社 | Parallel computing device and parallel computing method |
CN102037652A (en) | 2008-05-21 | 2011-04-27 | Nxp股份有限公司 | A data handling system comprising memory banks and data rearrangement |
US8321652B2 (en) * | 2008-08-01 | 2012-11-27 | Infineon Technologies Ag | Process and method for logical-to-physical address mapping using a volatile memory device in solid state disks |
EP2290563B1 (en) * | 2009-08-28 | 2017-12-13 | Accenture Global Services Limited | Accessing content in a network |
US8589600B2 (en) | 2009-12-14 | 2013-11-19 | Maxeler Technologies, Ltd. | Method of transferring data with offsets |
US8595467B2 (en) | 2009-12-29 | 2013-11-26 | International Business Machines Corporation | Floating point collect and operate |
US8676874B2 (en) | 2010-12-06 | 2014-03-18 | International Business Machines Corporation | Data structure for tiling and packetizing a sparse matrix |
US8457767B2 (en) * | 2010-12-31 | 2013-06-04 | Brad Radl | System and method for real-time industrial process modeling |
US8806171B2 (en) | 2011-05-24 | 2014-08-12 | Georgia Tech Research Corporation | Systems and methods providing wear leveling using dynamic randomization for non-volatile memory |
US8977629B2 (en) | 2011-05-24 | 2015-03-10 | Ebay Inc. | Image-based popularity prediction |
US8812414B2 (en) | 2011-05-31 | 2014-08-19 | International Business Machines Corporation | Low-power event-driven neural computing architecture in neural networks |
US8909576B2 (en) | 2011-09-16 | 2014-12-09 | International Business Machines Corporation | Neuromorphic event-driven neural computing architecture in a scalable neural network |
US9201828B2 (en) | 2012-10-23 | 2015-12-01 | Analog Devices, Inc. | Memory interconnect network architecture for vector processor |
US9606797B2 (en) | 2012-12-21 | 2017-03-28 | Intel Corporation | Compressing execution cycles for divergent execution in a single instruction multiple data (SIMD) processor |
US9921832B2 (en) | 2012-12-28 | 2018-03-20 | Intel Corporation | Instruction to reduce elements in a vector register with strided access pattern |
US9477628B2 (en) | 2013-09-28 | 2016-10-25 | Intel Corporation | Collective communications apparatus and method for parallel systems |
US9323525B2 (en) | 2014-02-26 | 2016-04-26 | Intel Corporation | Monitoring vector lane duty cycle for dynamic optimization |
CN110110843B (en) * | 2014-08-29 | 2020-09-25 | 谷歌有限责任公司 | Method and system for processing images |
CN104463209B (en) * | 2014-12-08 | 2017-05-24 | 福建坤华仪自动化仪器仪表有限公司 | Method for recognizing digital code on PCB based on BP neural network |
US9666257B2 (en) | 2015-04-24 | 2017-05-30 | Intel Corporation | Bitcell state retention |
US10013652B2 (en) * | 2015-04-29 | 2018-07-03 | Nuance Communications, Inc. | Fast deep neural network feature transformation via optimized memory bandwidth utilization |
US10489703B2 (en) | 2015-05-20 | 2019-11-26 | Nec Corporation | Memory efficiency for convolutional neural networks operating on graphics processing units |
US10671564B2 (en) | 2015-10-08 | 2020-06-02 | Via Alliance Semiconductor Co., Ltd. | Neural network unit that performs convolutions using collective shift register among array of neural processing units |
US9875104B2 (en) | 2016-02-03 | 2018-01-23 | Google Llc | Accessing data in multi-dimensional tensors |
US10552119B2 (en) | 2016-04-29 | 2020-02-04 | Intel Corporation | Dynamic management of numerical representation in a distributed matrix processor architecture |
CN106023065B (en) * | 2016-05-13 | 2019-02-19 | 中国矿业大学 | A kind of tensor type high spectrum image spectral-spatial dimension reduction method based on depth convolutional neural networks |
CN106127297B (en) | 2016-06-02 | 2019-07-12 | 中国科学院自动化研究所 | The acceleration of depth convolutional neural networks based on tensor resolution and compression method |
US10360163B2 (en) | 2016-10-27 | 2019-07-23 | Google Llc | Exploiting input data sparsity in neural network compute units |
US9959498B1 (en) | 2016-10-27 | 2018-05-01 | Google Llc | Neural network instruction set architecture |
US10175980B2 (en) | 2016-10-27 | 2019-01-08 | Google Llc | Neural network compute tile |
US10733505B2 (en) | 2016-11-10 | 2020-08-04 | Google Llc | Performing kernel striding in hardware |
US10037490B2 (en) | 2016-12-13 | 2018-07-31 | Google Llc | Performing average pooling in hardware |
CN106529511B (en) | 2016-12-13 | 2019-12-10 | 北京旷视科技有限公司 | image structuring method and device |
US20180189675A1 (en) | 2016-12-31 | 2018-07-05 | Intel Corporation | Hardware accelerator architecture and template for web-scale k-means clustering |
US11164071B2 (en) | 2017-04-18 | 2021-11-02 | Samsung Electronics Co., Ltd. | Method and apparatus for reducing computational complexity of convolutional neural networks |
US10572409B1 (en) | 2018-05-10 | 2020-02-25 | Xilinx, Inc. | Sparse matrix processing circuitry |
-
2016
- 2016-10-27 US US15/336,066 patent/US10360163B2/en active Active
-
2017
- 2017-03-22 US US15/465,774 patent/US9818059B1/en active Active
- 2017-08-22 JP JP2019523062A patent/JP7134955B2/en active Active
- 2017-08-22 SG SG11201903787YA patent/SG11201903787YA/en unknown
- 2017-08-22 KR KR1020237014253A patent/KR20230061577A/en active IP Right Grant
- 2017-08-22 WO PCT/US2017/047992 patent/WO2018080624A1/en active Search and Examination
- 2017-08-22 KR KR1020227015590A patent/KR102528517B1/en active Application Filing
- 2017-08-22 EP EP17761665.3A patent/EP3533003B1/en active Active
- 2017-08-22 EP EP22153266.6A patent/EP4044071A1/en active Pending
- 2017-08-22 KR KR1020197012085A patent/KR102397415B1/en active IP Right Grant
- 2017-09-06 DE DE102017120452.0A patent/DE102017120452A1/en not_active Ceased
- 2017-09-06 DE DE202017105363.6U patent/DE202017105363U1/en active Active
- 2017-09-29 CN CN201710908258.5A patent/CN108009626B/en active Active
- 2017-09-29 CN CN202210121408.9A patent/CN114595803A/en active Pending
-
2018
- 2018-10-25 HK HK18113685.3A patent/HK1254700A1/en unknown
-
2019
- 2019-07-17 US US16/514,562 patent/US11106606B2/en active Active
-
2021
- 2021-08-24 US US17/410,071 patent/US11816045B2/en active Active
-
2022
- 2022-08-31 JP JP2022138360A patent/JP7469407B2/en active Active
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP5501317B2 (en) | 2011-09-21 | 2014-05-21 | 株式会社半導体理工学研究センター | Time difference amplifier circuit |
Non-Patent Citations (1)
Title |
---|
MOONS, Bert et al.，A 0.3-2.6 TOPS/W PRECISION-SCALABLE PROCESSOR FOR REAL-TIME LARGE-SCALE CONVNETS，Proceedings of 2016 IEEE SYMPOSIUM ON VLSI CIRCUITS (VLSI-CIRCUITS)，IEEE，2016年06月15日，http://dx.doi.org/10.1109/VLSIC.2016.7573525 |
Also Published As
Publication number | Publication date |
---|---|
EP3533003B1 (en) | 2022-01-26 |
KR20190053262A (en) | 2019-05-17 |
JP7134955B2 (en) | 2022-09-12 |
CN108009626B (en) | 2022-03-01 |
SG11201903787YA (en) | 2019-05-30 |
US9818059B1 (en) | 2017-11-14 |
JP2022172258A (en) | 2022-11-15 |
US20220083480A1 (en) | 2022-03-17 |
KR102397415B1 (en) | 2022-05-12 |
KR20220065898A (en) | 2022-05-20 |
KR20230061577A (en) | 2023-05-08 |
US11106606B2 (en) | 2021-08-31 |
CN114595803A (en) | 2022-06-07 |
KR102528517B1 (en) | 2023-05-04 |
WO2018080624A1 (en) | 2018-05-03 |
DE202017105363U1 (en) | 2017-12-06 |
CN108009626A (en) | 2018-05-08 |
US20180121377A1 (en) | 2018-05-03 |
US10360163B2 (en) | 2019-07-23 |
US20200012608A1 (en) | 2020-01-09 |
JP2020500365A (en) | 2020-01-09 |
EP4044071A1 (en) | 2022-08-17 |
EP3533003A1 (en) | 2019-09-04 |
DE102017120452A1 (en) | 2018-05-03 |
US11816045B2 (en) | 2023-11-14 |
HK1254700A1 (en) | 2019-07-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7469407B2 (en) | Exploiting sparsity of input data in neural network computation units | |
TWI699712B (en) | Method and system for performing neural network computations, and related non-transitory machine-readable storage device | |
Zhang et al. | BoostGCN: A framework for optimizing GCN inference on FPGA | |
US11507382B2 (en) | Systems and methods for virtually partitioning a machine perception and dense algorithm integrated circuit | |
Moolchandani et al. | Accelerating CNN inference on ASICs: A survey | |
US10817260B1 (en) | Reducing dynamic power consumption in arrays | |
EP3265907B1 (en) | Data processing using resistive memory arrays | |
US20190228307A1 (en) | Method and apparatus with data processing | |
Ke et al. | Nnest: Early-stage design space exploration tool for neural network inference accelerators | |
Geng et al. | O3BNN: An out-of-order architecture for high-performance binarized neural network inference with fine-grained pruning | |
Wong et al. | Low bitwidth CNN accelerator on FPGA using Winograd and block floating point arithmetic | |
US20220012304A1 (en) | Fast matrix multiplication | |
GB2556413A (en) | Exploiting input data sparsity in neural network compute units | |
CN112132254A (en) | Exploiting input activation data sparseness in micro-neuron network computations | |
CN114595811A (en) | Method and apparatus for performing deep learning operations | |
CN110765413A (en) | Matrix summation structure and neural network computing platform |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20220927 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20220927 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20230621 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20230711 |
|
A601 | Written request for extension of time |
Free format text: JAPANESE INTERMEDIATE CODE: A601Effective date: 20231010 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20231211 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20240305 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20240404 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7469407Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |