CN103714104A - Answering questions using environmental context - Google Patents
Answering questions using environmental context Download PDFInfo
- Publication number
- CN103714104A CN103714104A CN201310394518.3A CN201310394518A CN103714104A CN 103714104 A CN103714104 A CN 103714104A CN 201310394518 A CN201310394518 A CN 201310394518A CN 103714104 A CN103714104 A CN 103714104A
- Authority
- CN
- China
- Prior art keywords
- content
- data
- engine
- computer
- sign
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000007613 environmental effect Effects 0.000 title claims abstract description 63
- 238000013518 transcription Methods 0.000 claims abstract description 124
- 230000035897 transcription Effects 0.000 claims abstract description 124
- 238000000034 method Methods 0.000 claims abstract description 64
- 238000012545 processing Methods 0.000 claims abstract description 37
- 238000013507 mapping Methods 0.000 claims description 50
- 206010038743 Restlessness Diseases 0.000 claims description 21
- 230000001052 transient effect Effects 0.000 claims 1
- 238000004590 computer program Methods 0.000 abstract description 12
- 239000010408 film Substances 0.000 description 28
- 230000008569 process Effects 0.000 description 19
- 238000004891 communication Methods 0.000 description 17
- 230000005540 biological transmission Effects 0.000 description 11
- 230000008878 coupling Effects 0.000 description 10
- 238000010168 coupling process Methods 0.000 description 10
- 238000005859 coupling reaction Methods 0.000 description 10
- 238000005516 engineering process Methods 0.000 description 8
- 238000012360 testing method Methods 0.000 description 6
- 230000009471 action Effects 0.000 description 5
- 231100001261 hazardous Toxicity 0.000 description 5
- 238000000926 separation method Methods 0.000 description 5
- 230000003595 spectral effect Effects 0.000 description 5
- 238000010586 diagram Methods 0.000 description 4
- 230000000694 effects Effects 0.000 description 4
- 230000013011 mating Effects 0.000 description 4
- 241001269238 Data Species 0.000 description 3
- 238000003058 natural language processing Methods 0.000 description 3
- 239000007787 solid Substances 0.000 description 3
- 238000004364 calculation method Methods 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 239000000284 extract Substances 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 239000013598 vector Substances 0.000 description 2
- 208000035126 Facies Diseases 0.000 description 1
- 238000000429 assembly Methods 0.000 description 1
- 230000000712 assembly Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 238000012512 characterization method Methods 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 238000000151 deposition Methods 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 230000002349 favourable effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3329—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/68—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/683—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/685—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using automatically derived transcript of audio data, e.g. lyrics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/432—Query formulation
- G06F16/433—Query formulation using audio data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3344—Query execution using natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/68—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/686—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using information manually generated, e.g. tags, keywords, comments, title or artist information, time, location or usage information, user ratings
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1815—Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/24—Speech recognition using non-acoustical features
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
- G10L25/54—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination for retrieval
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10H—ELECTROPHONIC MUSICAL INSTRUMENTS; INSTRUMENTS IN WHICH THE TONES ARE GENERATED BY ELECTROMECHANICAL MEANS OR ELECTRONIC GENERATORS, OR IN WHICH THE TONES ARE SYNTHESISED FROM A DATA STORE
- G10H2240/00—Data organisation or data communication aspects, specifically adapted for electrophonic musical tools or instruments
- G10H2240/121—Musical libraries, i.e. musical databases indexed by musical parameters, wavetables, indexing schemes using musical parameters, musical rule bases or knowledge bases, e.g. for automatic composing methods
- G10H2240/131—Library retrieval, i.e. searching a database or selecting a specific musical piece, segment, pattern, rule or parameter set
- G10H2240/141—Library retrieval matching, i.e. any of the steps of matching an inputted segment or phrase with musical database contents, e.g. query by humming, singing or playing; the steps may include, e.g. musical analysis of the input, musical feature extraction, query formulation, or details of the retrieval process
Abstract
Embodiments of the present invention relate to answering questions using environmental context. Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for receiving audio data encoding an utterance and environmental data, obtaining a transcription of the utterance, identifying an entity using the environmental data, submitting a query to a natural language query processing engine, wherein the query includes at least a portion of the transcription and data that identifies the entity, and obtaining one or more results of the query.
Description
the cross reference of related application
The application requires the rights and interests of following U.S. Patent application, at this, merges by reference its full content: September 10 in 2012, order was submitted to, and number of patent application is No.61/698,934 U.S. Provisional Patent Application; On September 10th, 2012 submits to, and number of patent application is No.61/698,949 U.S. Provisional Patent Application; On September 25th, 2012 submits to, and number of patent application is No.13/626,439 U.S. Patent application; On September 25th, 2012 submits to, and number of patent application is No.13/626,351 U.S. Patent application; And submission on February 15th, 2013, number of patent application is No.13/768,232 U.S. Patent application.
Technical field
This instructions relates to the Query Result of sign based on natural language querying and environmental information, and for example environment for use information is as context answer problem.
Background technology
Conventionally, search inquiry comprises that when user asks search engine to carry out search user submits to one or more terms of search engine.Except alternate manner, user can by typewriting on keyboard or in the context of voice queries by query term being dictated into the query term of carrying out typing search inquiry in the microphone of mobile device.Can process voice queries with speech recognition technology.
Summary of the invention
According to some innovation aspect of the theme of describing in this instructions, environmental information (such as neighbourhood noise) can nonproductive poll disposal system be answered natural language querying.For example, user can inquire the problem of the TV programme of watching about them, such as " whom the performer in this film is ".User's mobile device detects user's sounding (utterance) and environmental data, and environmental data can comprise the audio frequency of dubbing in background music of TV programme.Mobile computing device is encoded to Wave data by sounding and environmental data, and this Wave data is offered to the computing environment based on server.
Computing environment is separated from the environmental data of Wave data by sounding, and then obtains the transcription text of sounding.Computing environment is such as the further sign of the title solid data relevant to environmental data and sounding that passes through sign film.Computing environment can identify one or more results from transcription text and solid data then, for example, and in response to the result of user's problem.Especially, one or more results can comprise the answer (for example, performer's name) to the problem of user " whom performer in this film is ".Computing environment can provide to the user of mobile computing device this result.
The innovation aspect of the theme described in this instructions can be embodied in method, the method comprises following action: the voice data of received code sounding and environmental data, obtain the transcription text of sounding, environment for use Data Identification entity, to natural language querying processing engine submit Query, wherein inquiry at least comprises the data of transcription text-part and identified entities, and one or more results of obtaining inquiry.
Other embodiment of these aspects comprises corresponding system, device and the computer program of encoding on computer memory device, and it is configured to the action of manner of execution.
These and other embodiment can comprise the one or more features in following feature separately alternatively.For example, the expression of at least one result in Output rusults.Further use sounding identified entities.Generated query.Generated query comprises the data correlation of transcription text and identified entities.The associated data markers transcription text that uses identified entities that further comprises.An associated part that further comprises the data replacement transcription text that uses identified entities.Replace further comprising one or more words of the data replacement transcription text that uses identified entities.Reception environment data further comprise reception environment voice data, ambient image data or the two.Reception environment voice data further comprises that reception comprises the additional voice data of ground unrest.
According to some innovation aspect of the theme of describing in present disclosure, the project of the natural language querying identification medium content based on ambient sound audio data and oral account.For example, user can inquire the problem of the TV programme of watching about them, such as " what we are watching ".Problem can comprise keyword, and such as " watching ", its suggestion problem is the media content about TV programme rather than some other types.User's mobile device detects user's sounding and environmental data, and environmental data can comprise the audio frequency of dubbing in background music of TV programme.Mobile computing device is Wave data by sounding and television environment data encoding, and this Wave data is offered to the computing environment based on server.
Computing environment is separated with the environmental data of Wave data by sounding, and then processes sounding to obtain the transcription text of sounding.Computing environment, from the keyword of this transcription text detection any specific content type, " is watched " such as keyword.Computing environment is the project based on environmental data identification medium content then, and can select from the project of sign the specific project of media content, its with and the associated certain types of content of keyword mate.Computing environment provides the expression of the specific project of media content to the user of mobile computing device.
The innovation aspect of the theme of describing in this instructions can be embodied in method, it comprises following action: the voice data that receives (i) coding oral account natural language querying, and (ii) ambient sound audio data, obtain the transcription text of oral account natural language querying, the associated certain types of content of one or more keywords definite and in transcription text, to content recognition engine, provide an at least part for ambient sound audio data, the content item of certain types of content has been exported and mated to sign by content recognition engine.
Other embodiment of these aspects comprises corresponding system, device and the computer program of encoding on computer memory device, and it is configured to the action of manner of execution.
These and other embodiment can comprise one or more in following feature separately alternatively.For example, certain types of content is movie contents type, music content type, content of TV program type, audio frequency podcast content type, book contents type, artwork content type, trailer content type, video podcast content type, internet video content type or video game content type.Reception environment voice data further comprises that reception comprises the additional voice data of ground unrest.Ground unrest is associated with certain types of content.Reception comprises the additional environmental data of video data or view data.Video data or view data are associated with certain types of content.To content recognition engine, provide at least this part of ambient sound audio data further to comprise this part that ambient sound audio data is provided to audio-frequency fingerprint identification engine.Determine that certain types of content further comprises the one or more keywords of the one or more Database Identifications of use, for each content type in a plurality of content types, database is mapped at least one content type in a plurality of content types by least one keyword in keyword.A plurality of content types comprise certain types of content, and wherein mapping further comprises at least one keyword in keyword is mapped to certain types of content.Output identification content item destination data.
Feature further comprises, for example, provides the data that further comprise sign certain types of content to offer content recognition engine, and sign content project further comprises from content recognition engine reception sign content item destination data.From content identifying system, receive two or more content recognition candidates, and sign content project further comprises based on certain types of content selection certain content identification candidate.Each content recognition candidate in two or more content recognition candidates is associated with rank score, and the method further comprises the rank score of adjusting two or more content recognition candidates based on certain types of content.Rank score based on adjusting is to two or more content recognition candidate ranks.
The details of one or more embodiments of the theme of describing in this instructions is illustrated in the the accompanying drawings and the following description.Other potential feature, aspect and the advantage of theme will become obvious from specification, drawings and the claims.
accompanying drawing is briefly described
Fig. 1 has described for the example system based on ambient sound audio data and oral account natural language querying sign content project data.
Fig. 2 has described the process flow diagram for the instantiation procedure based on ambient sound audio data and oral account natural language querying sign content project data.
Fig. 3 A-Fig. 3 B has described a plurality of parts for the example system of sign content project.
Fig. 4 has described for the example system based on ambient image data and oral account natural language querying identification medium content item.
Fig. 5 has described for identify the system of one or more results based on ambient sound audio data and sounding.
Fig. 6 has described for identify the process flow diagram of the instantiation procedure of one or more results based on environmental data and sounding.
Fig. 7 has described can be for realizing computer equipment and the mobile computer device of technology described herein.
Same reference numbers in different accompanying drawings represents same element.
Embodiment
The computing environment that environment for use information is answered oral account natural language querying as context can be used a plurality of processes to process inquiry.In the example of some processes, if Fig. 1 is to as shown in Fig. 4, computing environment can be carried out identification medium content based on environmental information (such as ambient noise).In the example of other process, as shwon in Figures 5 and 6, for the more satisfied answer to oral account natural language querying is provided, computing environment can be expanded oral account natural language querying with the context (such as the data of identification medium content) that is derived from environmental information.
Fig. 1 has described in more detail for the system 100 based on ambient sound audio data and oral account natural language querying sign content project data.In brief, system 100 can identify the content item data of also mating the certain types of content associated with oral account natural language querying based on ambient sound audio data.System 100 comprises mobile computing device 102, disambiguation engine 104, speech recognition engine 106, keyword mapping engine 108 and content recognition engine 110.Mobile computing device 102 is communicated by letter with disambiguation engine 104 by one or more networks.Mobile device 110 can comprise that microphone, camera or other are for detecting the testing agency of sounding from user 112 and/or with the environmental data that user 112 is closed.
In some instances, user 112 is just in the bright TV programme of seeing.In the example shown, user 112 wants to know that who has directed current in progress TV programme.In some instances, user 112 may not know the title of current in progress TV programme, and " who has directed this program " mobile computing device 102 of for this reason asking a question detect these sounding and with environmentally hazardous ambient sound audio data of the user 112.
The ground unrest that can comprise in some instances, user 112 environment with environmentally hazardous ambient sound audio data of the user 112.For example, ambient sound audio data comprises the sound of TV programme.In some instances, the ambient sound audio data associated with the TV programme of current demonstration can comprise the audio frequency (for example, audio frequency etc. of dubbing in background music of the TV programme association of the dialogue of the TV programme of current demonstration, current demonstration) of the TV programme of current demonstration.
In some instances, mobile computing device 102 testing environment voice data after sounding being detected; Testing environment voice data when detecting sounding; Or two kinds of modes all adopt.During operation (A), the sounding that the sounding that mobile computing device 102 processing detect and ambient sound audio data detect with generation expression and the Wave data 114 of ambient sound audio data, and this Wave data 114 is transferred to disambiguation engine 104 (for example, passing through network).In some instances, from mobile computing device 110 flow transmission ambient sound audio data.
In some instances, disambiguation engine 104 use voice detectors with contribute to by sign Wave data 114 comprise voice activity or with a part for the voice activity of the user-association of computing equipment 102 from the separated sounding of ground unrest.In some instances, sounding relates to inquiry (for example, relating to the inquiry of the TV programme of current demonstration).In some instances, Wave data 114 comprises the sounding detecting.As response, disambiguation engine 104 can be from relating to the mobile computing device 102 request ambient sound audio data of sounding.
For example, speech recognition system 106 transcription sounding are to generate the transcription text of " who has directed this program ".In some embodiments, speech recognition system 106 provides two or more transcription texts of sounding.For example, speech recognition system 106 transcription sounding are to generate the transcription text of " who has directed this program " and " who has directed these footwear ".
For example, keyword mapping engine 108 is from the transcription Text Flag keyword " director " of " who has directed this program ".Keyword " director " is associated with " TV programme " content type.The keyword of the transcription text being identified by keyword mapping engine 108 in some embodiments, is associated with two or more content types.For example, keyword " director " is associated with " TV programme " and " film " content type.
In some embodiments, two or more keywords associated with certain types of content in keyword mapping engine 108 sign transcription texts.For example, keyword mapping engine 108 sign keyword " director " and " program " associated with certain types of content.In some embodiments, two or more keywords of sign are associated with same content type.For example, the keyword of sign " director " is all associated with " TV programme " content type with " program ".In some embodiments, two or more keywords of sign are associated from different content types.For example, the keyword of sign " director " is associated with " film " content type and the keyword " program " of sign is associated with " TV programme " content type.Keyword mapping engine 108 for example, arrives disambiguation engine 108 by certain types of content transmission (, passing through network).
In some embodiments, keyword mapping engine 108 is used the one or more keywords associated with certain types of content in one or more databases (for each content type in a plurality of content types, this database is mapped at least one content type in a plurality of content types by least one keyword in keyword) sign transcription text.Especially, keyword mapping engine 108 comprises database (or a plurality of database) or communicates by letter with database (or a plurality of database).Database comprises mapping between keyword and content type or associated with this mapping.Especially, thereby database provides the connection (for example, mapping) between keyword and content type has made keyword mapping engine 108 can identify the one or more keywords associated with certain types of content in transcription text.
In some embodiments, the one or more mappings in the mapping between keyword and content type can comprise one direction (for example, unidirectional) mapping (that is, the mapping from keyword to content type).In some embodiments, the one or more mappings in the mapping between keyword and content type can comprise twocouese (for example, two-way) mapping (that is, the mapping from keyword to content type and from content type to keyword).In some embodiments, one or more databases are mapped to two or more content types by the one or more keywords in keyword.
For example, keyword mapping engine 108 is used the one or more databases that keyword " director " are mapped to " film " and " TV programme " content type.In some embodiments, the mapping between keyword and content type can comprise the root keyword (for example, word family) of a plurality of different editions and the mapping between content type.The keyword of different editions can comprise different grammer kinds, for example, for example, such as tense (, past tense, present tense, future tense) and part of speech (, noun, verb).For example, database can comprise that the word family (such as " directors (directors) " and " director (direction) ") of handing over root word " to direct (direct) " is mapped to the mapping of one or more content types.
For example, disambiguation engine 104 to content recognition engine 110, transmit the TV programme that comprises current demonstration audio frequency the TV programme that relates to current demonstration ambient sound audio data (for example, the dialogue of the TV programme of current demonstration, the dub in background music audio frequency associated with the TV programme of current demonstration, etc.) and the transcription text (for example, " TV programme " content type) of the sounding of certain types of content.
In some embodiments, disambiguation engine 104 provides a part for ambient sound audio data to content recognition engine 110.In some instances, a part for ambient sound audio data can comprise the ground unrest being detected after detecting sounding by mobile computing device 102.In some instances, a part for ambient sound audio data can comprise the ground unrest being detected when detecting sounding by mobile computing device 102.
In some embodiments, (Wave data 114) ground unrest is associated with certain types of content, and this certain types of content is associated with the keyword of transcription text.For example, the keyword " director " of transcription text " who has directed this program " is associated with " TV programme " content type, and ground unrest (the ambient sound audio data that for example, relates to the TV programme of current demonstration) is also associated with " TV programme " content type.
For example, the ambient sound audio data of TV programme the content item data of further coupling " TV programme " content type of content recognition engine 110 signs based on relating to current demonstration.For this reason, rely on the component environment voice data being received by content recognition engine 110, the dialogue of the TV programme that content recognition engine 110 can be based on current demonstration or the dub in background music audio identification content item data associated with the TV programme of current demonstration.
In some embodiments, content recognition engine 110 is audio-frequency fingerprint engines, and it utilizes and uses the user supplied video content using fingerprints of small echo (wavelet) with sign content project data.Especially, content recognition engine 110 is converted to spectrogram by Wave data 114.Content recognition engine 110 extracts spectral image from spectrogram.Spectral image can be represented as small echo.For each spectral image extracting in the spectral image of spectrogram, the separately value of content recognition engine 110 based on small echo extracts " top " small echo.For each spectral image, the small echo of content recognition engine 110 computed image signature.In some instances, small echo signature is the version that block, that quantize of the wavelet decomposition of image.
For example, in order to use small echo to describe the image of m * n, return m * n small echo and without compression.In addition, content recognition engine 110 utilizes small echo subset that can characterization song.Especially, select t " top " small echo (passing through value), wherein t<<m * n.In addition, content recognition engine 110 creates the compact representation of above-mentioned sparse wavelet vectors, for example, uses Minhash to calculate the sub-fingerprint of these sparse bit vectors.
In some instances, when ambient sound audio data at least comprises associated the dubbing in background music during audio frequency of TV programme with current demonstration, content recognition engine 110 sign content project datas, the audio frequency of dubbing in background music that the TV programme of this content item data based on current demonstration is associated also also matches with " TV programme " content type.Therefore, in some instances, content recognition engine 110 signs relate to the content item data of the television programme title of current demonstration.For example, content recognition engine 110 (for example can be determined particular content item, concrete TV programme) with theme song (for example, the audio frequency of dubbing in background music) association, and particular content item (for example, concrete TV programme) coupling certain types of content (for example, " TV programme " content type).Therefore, content recognition engine 110 can identify and (for example relate to particular content item, the TV programme of current demonstration) data (for example, the title of concrete TV programme), this particular content item based on ambient sound audio data (for example, and further mate certain types of content (for example, " TV programme " content type) audio frequency of dubbing in background music).
In some instances, mobile computing device 102, disambiguation engine 104, one or more can the communication with the subset of mobile computing device 102, disambiguation engine 104, speech recognition engine 106, keyword mapping engine 108 and content recognition engine 110 (or wherein each) in speech recognition engine 106, keyword mapping engine 108 and content recognition engine 110.In some embodiments, the one or more one or more computing equipments (such as one or more calculation servers, distributed computing system or server zone or cluster) that can use in disambiguation engine 104, speech recognition engine 106, keyword mapping engine 108 and content recognition engine 110 are realized.
In some embodiments, as mentioned above, from mobile computing device 110 to disambiguation engine 104 flow transmission ambient sound audio data.When ambient sound audio data is streamed, for example, along with ambient sound audio data receives by disambiguation engine 104 the above-mentioned process (, operation (A)-(H)) (that is, incrementally carrying out) of carrying out.In other words, for example, along with each part of ambient sound audio data receives (, spread and be passed to disambiguation engine 104) by disambiguation engine 104, iteration executable operations (A)-(H) until sign content project data.
Fig. 2 has described the process flow diagram for the instantiation procedure 200 based on ambient sound audio data and oral account natural language querying sign content project data.Can use one or more computing equipments to carry out instantiation procedure 200.For example, mobile computing device 102, disambiguation engine 104, speech recognition engine 106, keyword mapping engine 108 and/or content recognition engine 110 can be for carrying out instantiation procedure 200.
The voice data (202) of received code oral account natural language querying and ambient sound audio data.For example, disambiguation engine 104 receives Wave data 114 from mobile computing device 102.Wave data 114 comprises user's oral account natural language querying (for example, " who has directed this program ") and ambient sound audio data (for example, the audio frequency of the TV programme of current demonstration).Disambiguation engine 104 for example will be given an oral account natural language querying (" who has directed this program "), from ground unrest (, the TV programme of the current demonstration) separation of user 112 environment.
Obtain the transcription text (204) of natural language querying.For example, speech recognition system 106 transcription natural language queryings for example, to generate the transcription text (, " who has directed this program ") of natural language querying.
The associated certain types of content (206) of one or more keywords definite and in transcription text.For example, keyword mapping engine 108 sign transcription texts (for example, " who has directed this program ") in the associated one or more keywords (for example, " director ") of certain types of content (for example, " TV programme " content type).In some embodiments, keyword mapping engine 108 is used the associated certain types of content of one or more databases one or more keywords definite and in transcription text, for each content type in a plurality of content types, this database is mapped at least one content type in a plurality of content types by least one keyword in keyword.Database provides for example, for example, connection between keyword (, " director ") and content type (, " TV programme " content type) (for example, mapping).
To content recognition engine, provide an at least part for ambient sound audio data (208).For example, disambiguation engine 104 provides a part (for example, the audio frequency of the TV programme of current demonstration) at least ambient sound audio data of being encoded by Wave data 114 to content recognition engine 110.In some instances, disambiguation engine 104 also to content recognition engine 110 provide with transcription text in the associated certain types of content (for example, " TV programme " content type) of one or more keywords (for example, " director ").
The content item of content recognition engine output identification, the content matching certain types of content (210) of sign.For example, content recognition engine 110 sign content projects or based on ambient sound audio data (for example, the audio frequency of the TV programme of current demonstration) and with the content item data of certain types of content (for example, " TV programme " content type) coupling.
Fig. 3 A and Fig. 3 B have described respectively part 300a and the 300b for the system of sign content project data.Especially, Fig. 3 A and Fig. 3 B comprise respectively disambiguation engine 3 04a and 304b; And comprise respectively content recognition engine 3 10a and 310b.The disambiguation engine 104 of disambiguation engine 3 04a and 304b and system depicted in figure 1 100 is similar; The content recognition engine 110 of content recognition engine 3 10a and 310b and system depicted in figure 1 100 is similar.
Fig. 3 A has described to comprise the part 300a of content recognition engine 3 10a.Content recognition engine 3 10a can identify based on environmental data and mate the content item data of certain types of content.In other words, content recognition engine 3 10a suitably processing environment data with based on environmental data sign content project data, and the content item data of the one or more signs in the content item data of further selecting to identify, thereby selected content item data is mated with certain types of content.
Especially, during operation (A), disambiguation engine 3 04a provides environmental data and certain types of content to content recognition engine 3 10a.In some embodiments, disambiguation engine 3 04a provides a part for environmental data to content recognition engine 3 10a.
Content recognition engine 3 10a is from disambiguation engine 3 04a reception environment data and certain types of content.During operation (B), content recognition engine 3 10a identifies then based on environmental data the content item data of mating with certain types of content, and the content item data of sign is offered to disambiguation engine 3 04a.Especially, content item data based on environmental data of content recognition engine 3 10a sign (for example, the title of TV programme, the title of song, etc.).Content recognition engine 3 10a selects the content item data of the one or more signs in the content item data of the sign that matches with certain types of content then.In other words, content recognition engine 3 10a filters the content item data of sign based on certain types of content.Content recognition engine 3 10a for example, gives disambiguation engine 3 04a by the content item data transmission (, passing through network) of sign.
In some instances, when environmental data at least comprises associated the dubbing in background music during audio frequency of TV programme with current demonstration, as above mentioned about Fig. 1, content recognition engine 3 10a identifies the content item data of the associated audio frequency of dubbing in background music of TV programme based on current demonstration.Content recognition engine 3 10a filters the content item data of sign then based on " TV programme " content type.For example, content recognition engine 3 10a sign " theme song title " and " television programme title " associated with the audio frequency of dubbing in background music.Content recognition engine 3 10a filters the content item data of sign then, thereby makes the content item data of sign also mate " TV programme " content type.For example, content recognition engine 3 10a selects " television programme title " identification data, and " television programme title " identification data is transferred to disambiguation engine 3 04a.
In some instances, the content-based type of content recognition engine 3 10a (for example, " TV programme " content type) is selected corpus (or index).Especially, content recognition engine 3 10a can access about first index of " TV programme " content type with about second index of " film " content type.Content recognition engine 3 10a suitably selects the first index based on " TV programme " content type.Therefore,, by selecting the first index (and not selecting the second index), content recognition engine 3 10a is sign content project data (for example, the title of TV programme) more efficiently.
Disambiguation engine 3 04a receives content item data from content recognition engine 3 10a.For example, disambiguation engine 3 04a receives " television programme title " identification data from content recognition engine 3 10a.During operation (C), disambiguation engine 3 04a for example, provides identification data to third party's (, mobile computing device 102 of Fig. 1) then.For example, disambiguation engine 3 04a provides " television programme title " identification data to third party.
Fig. 3 b has described to comprise the part 300b of content recognition engine 3 10b.Content recognition engine 3 10b can be based on environmental data sign content project data.In other words, content recognition engine 3 10b suitably processing environment data with based on environmental data sign content project data, and provide content item data to disambiguation engine 3 04b.The content item data of the one or more signs in the content item data that disambiguation engine 3 10b selects to identify, thus make selected content item data coupling certain types of content.
Especially, during operation (A), disambiguation engine 3 04b provides environmental data to content recognition engine 3 10b.In some embodiments, disambiguation engine 3 04b provides a part for environment religion certificate to content recognition engine 3 10b.
Content recognition engine 3 10b is from disambiguation engine 3 04b reception environment data.During operation (B), content recognition engine 3 10b identifies the content item data based on environmental data then, and the content item data of sign is provided to disambiguation engine 3 04b.Especially, content recognition engine 3 10b sign based on environmental data, with the associated content item data of two or more content items (for example, the title of TV programme, the title of song, etc.).Content recognition engine 3 10b transmits (for example, passing through network) to disambiguation engine 3 04b by two or more candidates that represent the content item data of sign.
In some instances, when environmental data at least comprises dubbing in background music during audio frequency of being associated with the TV programme of current demonstration, as mentioned in Fig. 1, content recognition engine 3 10b sign relates to the content item data of two or more content items, the dub in background music audio frequency of this content item based on being associated with the TV programme of current demonstration.For example, " theme song title " and " television programme title " that content recognition engine 3 10b sign is associated with the audio frequency of dubbing in background music, and " theme song title " and " television programme title " identification data are sent to disambiguation engine 3 04b.
Disambiguation engine 3 04b receives two or more candidates from content recognition engine 3 10b.For example, disambiguation engine 3 04b receives " theme song title " and " television programme title " candidate from content recognition engine 3 10b.During operation (C), disambiguation engine 3 04b selects a candidate in two or more candidates based on certain types of content then, and selected candidate is offered to third party's (for example, mobile computing device 102 of Fig. 1).Especially, as described in Figure 1 on, disambiguation engine 3 04b receives certain types of content (for example, the special type associated with sounding) in advance.Disambiguation engine 3 04b selects the particular candidate person in two or more candidates based on certain types of content.Especially, disambiguation engine 3 04b selects to mate in two or more candidates the particular candidate person of certain types of content.For example, disambiguation engine 3 04b selects " television programme title " candidate, because " television programme title " candidate matches with " TV programme " content type.
In some embodiments, associated with rank score from two or more candidates of content recognition engine 3 10b.Rank score can be associated with any scoring tolerance of being determined by disambiguation engine 3 04b.Disambiguation engine 3 04b can further adjust based on certain types of content the rank score of two or more candidates.Especially, when each candidate mates with certain types of content, disambiguation engine 3 04b can increase the rank score of the one or more candidates in candidate.For example, can increase the rank score of candidate " television programme title ", because it mates with " TV programme " content type.In addition,, when each candidate does not mate with certain types of content, disambiguation engine 3 04b can reduce the rank score of one or more candidates.For example, can reduce the rank score of candidate " theme song title ", because it does not mate with " TV programme " content type.
In some embodiments, can each the self-adjusting rank score based on disambiguation engine 3 04b carry out rank to two or more candidates.For example, disambiguation engine 3 04b can be by " television programme title " candidate rank on " theme song title " candidate, because " television programme title " candidate has the rank score of higher adjustment than the rank score of the adjustment of " theme song title " candidate.In some instances, disambiguation engine 3 04b selects the candidate that rank is the highest (candidate namely, with the rank score of height adjustment).
Fig. 4 has described for the system 400 based on ambient image data and oral account natural language querying sign content project data.In brief, system 400 can identify based on ambient image data and with content item data with the associated certain types of content of oral account natural language querying coupling.System 400 comprises mobile computing device 402, disambiguation engine 404, speech recognition engine 406, keyword mapping engine 408 and content recognition engine 410, and they are similar to respectively mobile computing device 102, disambiguation engine 104, speech recognition engine 106, keyword mapping engine 108 and the content recognition engine 110 of the system 100 shown in Fig. 1.
In some instances, user 112 is watching the CD collection of records front cover of dubbing in background music of film.In the example shown, user 112 wants to understand those songs in dubbing in background music.In some instances, user 112 may not know the title of film sound tracks, and therefore may ask that " having what song above this " or " what song what play in film is " mobile computing device 402 detects these sounding, and with environmentally hazardous ambient image data of the user 112.
In some instances, the ambient image data associated with user 112 environmental facies comprise user 112 ambient image data.For example, ambient image data comprise the image of for example having described, with the CD collection of records front cover of film associated picture (, the image of the film poster of relevant film).In some instances, mobile computing device 402 is used the camera of mobile computing device 402 to catch the image (or video) of CD collection of records front cover, with this, carrys out testing environment view data.
During operation (A), the Wave data 414 of the sounding that the sounding that mobile computing device 402 processing detect detects with generation expression, and Wave data 414 and ambient image data are sent to disambiguation engine 404 (for example, passing through network).
For example, speech recognition system 406 transcription sounding are to generate the transcription text of " having what song above this ".In some embodiments, speech recognition system 406 provides two or more transcription texts of sounding.For example, speech recognition system 406 transcription sounding are to generate the transcription text of " having what song above this " and " having what pond above this ".
For example, keyword mapping engine 408 sign from transcription text, " above this, have what song " keyword " song ".Keyword " song " is associated with " music " content type.The keyword of the transcription text being identified by keyword mapping engine 408 in some embodiments, is associated with two or more content types.For example, keyword " song " is associated with " music " and " singer " content type.Keyword mapping engine 408 for example, is given disambiguation engine 408 by certain types of content transmission (, passing through network).
In some embodiments, be similar to above-mentioned, the one or more keywords associated with certain types of content that keyword mapping engine 408 is used in one or more Database Identification transcription texts, for each content type in a plurality of content types, this database is mapped at least one content type in a plurality of content types by least one keyword in keyword.For example, keyword mapping engine 408 is used the one or more databases that keyword " song " are mapped to " music " and " singer " content type.
For example, the ambient image data that disambiguation engine 404 relates to film sound tracks to content recognition engine 410 transmission (for example, the image of film poster CD collection of records front cover) and the certain types of content of the transcription text of sounding (for example, " music " content type).
For example, content recognition engine 410 signs are also further matched with the data of " music " content type based on ambient image data, and these ambient image data relate to the image of film poster CD collection of records front cover.
In some instances, when ambient image data at least comprise the film poster image associated with CD collection of records front cover, the content recognition engine film poster of 410 signs based on associated with CD collection of records front cover the content item data of also mating with " music " content type.Therefore, in some instances, content recognition engine 410 signs relate to the content item data of film sound tracks title.For example, content recognition engine 410 can determine that particular content item (for example, concrete film sound tracks) is associated with film poster, and particular content item (for example, concrete film sound tracks) for example, with certain types of content (, " music " content type) coupling.Therefore, content recognition engine 410 can identify and based on ambient image data (for example relate to, the image of CD collection of records front cover) and further with certain types of content (for example, " music " content type) particular content item of coupling (for example, concrete film sound tracks) data (for example, the title of concrete film sound tracks).
As mentioned above, Fig. 1 shows to Fig. 4 several instantiation procedures that computing environment wherein can be based on environmental information (such as ambient noise) identification medium content (or other content).Also can use other process for sign content.Fig. 5 and Fig. 6 show other instantiation procedure substantially, wherein, for the more satisfied answer to oral account natural language querying is provided, computing environment can be used context (such as the data of identification medium content) the expansion oral account natural language querying that is derived from environmental information.
Fig. 5 has described in more detail for identify the system 500 of one or more results based on ambient sound audio data and sounding.In some instances, one or more results can represent the one or more answers to natural language querying.System 500 comprise mobile computing device 502, cooperation engine 504, speech recognition engine 506, content identification engine 508,, and natural language querying processing engine 510.Mobile computing device 502 is by one or more networks and engine 504 communications that cooperate.Mobile device 510 can comprise that microphone, camera or other are for detecting the detection architecture of sounding from user 512 and/or the environmental data associated with user 512.
Similar with the system 100 of Fig. 1, user 512 is watching TV programme.In the example shown, user 512 wants to know that who has directed in progress TV programme (for example, entity).In some instances, user 512 may not know the title of the TV programme of current broadcasting, and because may put question to problem " who has directed this program " mobile computing device 502 detect these sounding and with environmentally hazardous environmental data of the user 512.
The ground unrest that can comprise in some instances, user 512 environment with environmentally hazardous environmental data of the user 512.For example, environmental data comprises the sound of TV programme (for example, entity).In some instances, the environmental data associated with the TV programme of current demonstration can comprise the audio frequency (for example, the dialogue of the TV programme of current demonstration, the dub in background music audio frequency associated with the TV programme of current demonstration, etc.) of the TV programme of current demonstration.In some instances, environmental data can comprise that ambient sound audio data, ambient image data or the two all comprise.In some instances, mobile computing device 502 testing environment voice data after detecting sounding; Testing environment voice data when detecting sounding; Or the two.During operation (A), mobile computing device 502 is processed the sounding and the environmental data that detect and (is for example represented the sounding detecting and the ambient sound audio data detecting to generate, the sound of TV programme) Wave data 514, and Wave data 514 is transferred to cooperation engine 504 (for example, passing through network).
For example, speech recognition engine 506 transcription sounding are to generate the transcription text of " who has directed this program ".In some embodiments, speech recognition engine 506 provides two or more transcription texts of sounding.For example, speech recognition engine 506 transcription sounding are to generate the transcription text of " who has directed this program " and " who has directed these footwear ".
For example, the TV programme that cooperation engine 504 relates to current demonstration to content identification engine 508 transmission (for example, entity) part corresponding to sounding (" who has directed this program ") of environmental data and waveform 514, this environmental data comprise current demonstration TV programme audio frequency (for example, the dialogue of the TV programme of current demonstration, the dub in background music audio frequency associated with the TV programme of current demonstration, etc.).
In some embodiments, cooperation engine 504 provides a part for environmental data to content identification engine 508.In some instances, a part for environment religion certificate can comprise the ground unrest being detected after detecting sounding by mobile computing device 502.In some instances, a part for environmental data can comprise the ground unrest being detected when detecting sounding by mobile computing device 502.
For example, content identification engine 508 processing environment voice datas are with the sign content item data associated with the TV programme of current demonstration.In some embodiments, content identification engine 508 is systems 100 of Fig. 1.
In some instances, cooperation engine 504 generated queries.In some instances, cooperation engine 504 obtains inquiry (for example,, from third-party server).For example, cooperation engine 504 can be submitted the transcription text of sounding and the data of identified entities to third-party server, and receives back the inquiry of the data based on transcription text and identified entities.
In some embodiments, by cooperation engine 504 generated queries, for example can be comprised, by the data of the transcription text of sounding and identified entities (, content item data) association.In some instances, can comprise the data markers transcription text that uses identified entities by the transcription text of sounding is associated with sign content project data.For example, cooperation engine 504 (for example can use " television programme title " or other identification information associated with content item data, sign (ID) number) mark transcription text " who has directed this program " in some instances, can comprise the data correlation of the transcription text of sounding and identified entities a part for the data replacement transcription text that uses identified entities.For example, cooperation engine 504 can use the data of " television programme title " or sign " television programme title " to replace a part for transcription text " who has directed this program ".In some instances, a part for the data replacement transcription text of use identified entities can comprise one or more words of the transcription text of the data replacement sounding that uses identified entities.For example, cooperation engine 504 can replace " television programme title " or the data of sign " television programme title " in transcription text " who has directed this program ".For example, this replacement can cause comprising the transcription text of " who has directed " television programme title " " or " who has directed " identification number " ".
Natural language querying processing engine 510 receives the inquiry of the data (for example, content item data) that comprise transcription text and identified entities from cooperation engine 504.During operation (G), natural language querying processing engine 510 is suitably processed inquiry, and processes based on this, to cooperation engine 504, provides one or more results (for example, passing through network).In other words, cooperation engine 510 (for example,, from natural language querying processing engine 510) obtains one or more results of inquiry.
Especially, natural language querying processing engine 510 (from collecting of information resources) is obtained the information resources relevant to inquiring about (transcription text and the content item data of sounding).In some instances, natural language querying processing engine 510 will be inquired about and database information (for example, text document, image, audio frequency, video, etc.) coupling, and the score of matching inquiry to what extent of each object in computational data storehouse.The object (object for example, with score threshold value score on) of natural language querying processing engine 510 based on coupling identifies one or more results.
For example, natural language processing engine 510 receives the inquiry of the transcription text that comprises " television programme title " (or other identification information) and sounding " who has directed this program ".Natural language querying processing engine 510 will be inquired about with database information and be mated, and one or more results of matching inquiry are provided.Natural language querying processing engine 510 is calculated the score of each match objects in match objects.
In some instances, mobile computing device 502, cooperation engine 504, speech recognition engine 506, content identification engine 508 and one or more can communication with the subset of mobile computing device 502, the engine 504 that cooperates, speech recognition engine 506, content identification engine 508 and natural language querying processing engine 510 (or wherein each) in natural language querying processing engine 510.In some embodiments, can use one or more computing equipments (such as one or more servers, distributed computing system or server zone or cluster) to realize one or more in cooperation engine 504, speech recognition engine 506, content identification engine 508 and natural language querying processing engine 510.
Fig. 6 has described for identify the process flow diagram of the instantiation procedure 600 of one or more results based on environmental data and sounding.Instantiation procedure 600 can be used one or more computing equipments to carry out.For example, mobile computing device 502, cooperation engine 504, speech recognition engine 506, content identification engine 508 and/or natural language querying processing engine 510 can be for carrying out instantiation procedure 600.
The voice data of received code sounding and environmental data (602).For example, cooperation engine 504 receives Wave data 514 from mobile computing device 502.Wave data 514 comprises user's sounding (for example, " who has directed this program ") and environmental data (for example, the audio frequency of the TV programme of current demonstration).In some instances, reception environment data can comprise reception environment voice data, ambient image data or the two.In some instances, reception environment data comprise that reception comprises the additional voice data of ground unrest.
Obtain the transcription text (604) of sounding.For example, cooperation engine 504 uses speech recognition engine 506 to obtain the transcription text of sounding.Speech recognition engine 506 transcription sounding for example, to generate the transcription text (, " who has directed this program ") of sounding.
Environment for use Data Identification entity (606).For example, cooperation engine 504 uses content identification engine 508 to obtain the data of identified entities.Suitably processing environment data are (for example for content identification engine 508, the ambient sound audio data associated with the TV programme showing) for example, (to identify following data, content item data), this Data Identification and environment religion certificate are (for example, the title of TV programme, the title of song, etc.) associated entity.In some instances, content identification engine 508 can further be processed waveform 514 corresponding to sounding (with processing environment data simultaneously or after processing environment data) with identified entities.
In some instances, cooperation engine 504 generated queries.In some instances, by cooperation engine 504 generated queries, can be comprised the transcription text of sounding and the data correlation of identified entities.In some instances, by the transcription text of the sounding part that can comprise with the data replacement transcription text of identified entities associated with content item data.In some instances, use the data of identified entities to replace a part for transcription text to comprise to use the one or more words in the transcription text of data replacement sounding of identified entities.
To natural language processing engine submit Query (608).For example, cooperation engine 504 is to natural language querying processing engine 510 submit Queries.Inquiry can at least comprise a part for transcription text and the data of identified entities (for example, content item data).For example, cooperation engine 504 is submitted to and is comprised the transcription text of sounding (" who has directed this program ") and the inquiry of content item data (" television programme title ") to natural language querying processing engine 510.
Obtain one or more results (610) of inquiry.For example, cooperation engine 510 obtains one or more results (for example, the director's of TV programme name) of inquiry from natural language querying processing engine 510.In some instances, cooperation engine 504 provides one or more results to mobile computing device 502 then.
Fig. 7 has described the example of general purpose computing device 700 and General Mobile computer equipment 750, and it can use together with technology described herein.Computing equipment 700 is intended to represent various forms of digital machines, such as laptop computer, and desk-top computer, workstation, personal digital assistant, server, blade server, large scale computer and other suitable computing machine.Computing equipment 750 is intended to represent various forms of mobile devices, such as personal digital assistant, cell phone, smart phone and other similar computing equipment.Assembly shown here, their connection and relation and their function are only exemplary, rather than for limiting the implementation of the invention that this document describes and/or ask for protection.
Computing equipment 700 comprises processor 702, storer 704, memory device 706, is connected to the high-speed interface 708 of storer 704 and high speed Extended Capabilities Port 710 and is connected to low speed bus 714 and the low-speed interface 712 of memory device 706.Each assembly in assembly 702,704,706,708,710 and 712 is used various bus interconnections, and can be installed on general mainboard or in other suitable mode and install.Processor 702 can be processed for the instruction in computing equipment 700 interior execution, comprise the instruction being stored on storer 704 or memory device 706, so that externally input-output apparatus (such as the display 716 that is coupled to high-speed interface 708) above shows the graphical information of GUI.In other implementation, when appropriate, can use a plurality of processors and/or a plurality of bus, together with a plurality of storeies and a plurality of type of memory.In addition, a plurality of computer equipments 700 can connect with each equipment (for example,, as server array, blade server group or multicomputer system) of the part of necessary operation is provided.
The information that storer 704 stores in computing equipment 700.In an implementation, storer 704 is one or more volatile memory-elements.In another implementation, storer 704 is one or more Nonvolatile memery units.Storer 704 can also be the computer-readable medium of another form, such as disk or CD.
Memory device 706 can provide Mass storage for computing equipment 700.In an implementation, memory device 706 can be or comprise computer-readable medium, such as floppy device, hard disc apparatus, compact disk equipment, or the array of tape unit, flash memory or other similar solid storage device or equipment, it comprises equipment or other configuration in storage area network.Computer program can visibly be embedded in information carrier.Computer program can also comprise instruction, when carrying out this instruction, carries out a kind of or a plurality of methods, such as above-described those methods.Information carrier is computing machine or machine readable media, such as the storer on storer 704, memory device 706 or processor 702.
The bandwidth intensive operation of high-speed controller 708 Management Calculation equipment 700, and the lower bandwidth intensive operation of low speed controller 712 management.It is only exemplary that this function is distributed.In an implementation, high-speed controller 708 is coupled to storer 704, display 716 (for example, by graphic process unit or accelerator) and high speed Extended Capabilities Port 710, and it can accept various expansion card (not shown).In this implementation, low speed controller 712 is coupled to memory device 706 and low speed Extended Capabilities Port 714.It (for example can comprise various communication port, USB, bluetooth, Ethernet, wireless ethernet) low speed Extended Capabilities Port can for example by network adapter, be coupled to one or more input-output apparatus, such as keyboard, pointing apparatus, scanner or the network equipment (such as switch or router).
As shown in FIG., computing equipment 700 can be realized according to multitude of different ways.For example, it may be implemented as standard server 720, or the repeatedly enforcement in such server zone.It can also be implemented as a part for posture server system 724.In addition, can realize it at personal computer (in laptop computer 722).Alternatively, from the assembly of computing equipment 700 can with mobile device (not shown) (such as equipment 750) in other assembly combination.Each equipment in such equipment can comprise one or more computing equipment in computing equipment 700,750, and whole system can be comprised of a plurality of computing equipments 700,750 that communicate with one another.
Computing equipment 750 comprises processor 752, storer 764, input-output apparatus, such as display 754, communication interface 766 and transceiver 768 and other assemblies.Equipment 750 can have for the memory device of additional memory devices is provided, such as microdrive or miscellaneous equipment.Each assembly in assembly 750,752,754,766 and 768 is used various bus interconnections, and several assembly can be installed on general mainboard or in other suitable mode and installs.
Processor 752 can be carried out the instruction in computing equipment 750, comprises the instruction being stored in storer 764.Processor may be implemented as and comprises separately and the chipset of the chip of a plurality of analog-and digital-processors.Processor can for example provide other parts of Mediation Device 750, such as controlling user interface, the application of equipment 750 operations and the radio communication of equipment 750.
Processor 752 can be by control interface 758 and display interface 756 and the telex network of being coupled to display 754.Display 754 can be for example TFT LCD (Thin Film Transistor-LCD) or OLED (organic light emitting diode) display or other suitable display technique.Display interface device 756 can comprise for driving display 754 to present the suitable circuit arrangement of figure and out of Memory to user.Control interface 758 can receive order and be transformed for submitting to processor 752 from user.In addition, can provide the external interface of communicating by letter with processor 752 to realize equipment 750 and miscellaneous equipment short-range communication.In some implementations, external interface 762 can provide for example wire communication, or provides radio communication in other implementation, can also use a plurality of interfaces.
Information in storer 764 storage computing equipments 750.Storer 764 may be implemented as one or more computer-readable mediums, one or more volatile memory-elements or one or more Nonvolatile memery unit.Extended memory 754 can also be provided and be connected to equipment 750 by expansion interface 752, it can comprise for example SIMM (single-in-line memory module) card interface.This extended memory 754 can provide extra all spaces of depositing for equipment 750, or application or the out of Memory of all right memory device 750.Especially, extended memory 754 can comprise that instruction is to carry out or supplementary said process, and can comprise security information.Therefore, for example, can provide the security module of extended memory 754 as equipment 750, and can to it, programme with the instruction of the safe handling of permission equipment 750.In addition, can safety applications be provided together with additional information via SIMM card, such as the mode with can not hacker, identification information is positioned on SIMM card.
As discussed below, storer can for example comprise flash memory and/or NVRAM storer.In an implementation, in information carrier, visibly realize computer program.Computer program is included in the instruction of carrying out one or more methods (all methods described above) while being performed.Information carrier is computing machine or machine readable media, such as the signal of storer 764, extended memory 774, storer on processor 752 or the propagation that can for example receive by transceiver 768 or external interface 762.
Equipment 750 can can comprise that the communication interface 766 of digital signal processing circuit device wirelessly communicates by letter where necessary by logical.Communication interface 766 can provide communication in various patterns or agreement (under GSM audio call, SMS, EMS or MMS information receiving, CDMA, TDMA, PDC, wCDMA, CDMA2000 or GPRS and other pattern or agreement).Can for example by radio-frequency (RF) transceiver 768, there is such communication.In addition, can be such as with bluetooth, WiFi or other such transceiver (not shown), junction service occurring.In addition, GPS (GPS) receiver module 770 can provide the additional wireless data relevant with navigation and position to equipment 750, and these data can be used by the application of operation on equipment 750 as suitable.
Equipment 750 also can audibly be communicated by letter with audio coder-decoder 760, and this coding decoder can receive oral account speech breath and convert it to usable digital information from user.Audio coder-decoder 760 can be similarly such as generating the sub-audible sound for user by for example loudspeaker in the head phone of equipment 750.Such sound can comprise sound from voice telephone calls, can comprise the sound (for example, speech message, music file etc.) of record and also can comprise the sound that the application by operation on equipment 750 generates.
As shown in FIG., can be with a plurality of multi-form enforcement computing equipments 750.For example, it can be embodied as to cell phone 780.Also it can be embodied as to the part of smart phone 782, personal digital assistant or other similar mobile device.
Can in the ASIC of Fundamental Digital Circuit device, integrated circuit (IC) apparatus, particular design (special IC), computer hardware, firmware, software and/or its combination, realize the various implementations of system described herein and technology.These various implementations can be included on following programmable system, can carry out and/or can one or more computer program of decipher in implementation, this programmable system comprises it can being special or general at least one programmable processor, at least one input equipment and at least one output device, and this programmable processor is coupled to receive data and instruction and transmit data and instruction to storage system from storage system.
These computer programs (being also referred to as program, software, software application or code) comprise for the machine instruction of programmable processor and can and/or implement by compilation/machine language with level process and/or Object-Oriented Programming Language.As used herein, term " machine readable media ", " computer-readable medium " (for example refer to any computer program, device and/or equipment for machine instruction and/or data are provided to programmable processor, disk, CD, storer, programmable logic device (PLD) (PLD)), this computer program, device and/or equipment comprise that reception machine instruction is as the machine readable media of machine-readable signal.Term " machine-readable signal " refers to for any signal of machine instruction and/or data is provided to programmable processor.
For mutual with user is provided, can on following computing machine, implement system described herein and technology, this computing machine has display device (for example CRT (cathode-ray tube (CRT)) or LCD (liquid crystal display) monitor) for show from information to user and user can be used for providing to computing machine keyboard and the pointing apparatus (for example, mouse or tracking ball) of input.The equipment of other kind also can be used to provide mutual with user; The feedback for example providing to user can be any type of sensory feedback (for example, visual feedback, audio feedback or tactile feedback); And can receive the input from user by any form that comprises sound, speech or sense of touch input.
Can in following computing system, implement system described herein and technology, this computing system comprises that back-end component (for example, as data server) or comprise that middleware component is (for example, application server) or comprise that front end component (for example, the client computer with following graphical user interface or Web browser, user can come with the implementation of system described herein and technology mutual by this graphical user interface or Web browser) or any combination of such rear end, middleware or front end component.The parts of system can for example, by any digital data communication form or medium (, communication network) interconnection.The example of communication network comprises LAN (Local Area Network) (" LAN "), wide area network (" WAN ") and the Internet.
Computing system can comprise client and server.Client and server is conventionally each other at a distance of remote and typically mutual by communication network.By the raw client of the computer program product moving on corresponding computer and mutually there is client one relationship server and relationship server.
Although present disclosure comprises many details, these should not be interpreted as the restriction to the scope of content disclosure or can be claimed, but are in fact interpreted as the description to the distinctive feature of specific implementation of disclosure.Some feature described in the background of independent implementation providing in present disclosure also can be provided in single implementation.Conversely, also can be in a plurality of implementations separately or the various features of describing provide the background in single implementation in any suitable sub-portfolio in.In addition; although can Expressive Features be above effect and even originally claimed like this in some combination; but can from claimed combination, remove one or more feature from this combination in some cases, and claimed combination can relate to the variation of sub-portfolio or sub-portfolio.
Similarly, although describe operation by particular order in the accompanying drawings, this should not be construed as require by shown in particular order or by order successively carry out such operation or carry out all shown in operation to realize the result of wishing.In some circumstances, multitask and parallel processing can be favourable.In addition, in above-described implementation, separated various system units should not be construed as and in all implementations, require such separation, and the program element that should be appreciated that description and system generally can together be integrated in single software product or be encapsulated in a plurality of software products.
The specific implementation of present disclosure has been described like this.Other implementation within the scope of the appended claims.For example, the action of recording in the claims can be carried out and still realizes the result of wishing by different order.A plurality of implementations have been described.But, be appreciated that and can carry out without departing from the spirit and scope in the present disclosure various modifications.For example, can use various forms of above-mentioned flow processs, wherein step is resequenced, added or removes.Therefore, other implementation is also in the scope of following claim.
Claims (52)
1. a computer-implemented method, comprising:
The voice data of received code sounding and environmental data;
Obtain the transcription text of described sounding;
Use described environmental data identified entities;
To natural language querying processing engine submit Query, wherein said inquiry at least comprises a part for described transcription text and the data of the described entity of sign; And
Obtain one or more results of described inquiry.
2. computer-implemented method as claimed in claim 1, further comprises the expression of at least one result in the described result of output.
3. computer-implemented method as claimed in claim 1, is wherein further used described sounding to identify described entity.
4. computer-implemented method as claimed in claim 1, further comprises and generates described inquiry.
5. computer-implemented method as claimed in claim 4, wherein generates described inquiry and comprises the described data correlation with the described entity of sign by described transcription text.
6. computer-implemented method as claimed in claim 5, wherein associatedly further comprises transcription text described in the described data markers of using the described entity of sign.
7. computer-implemented method as claimed in claim 5, the wherein associated part of using the described data of the described entity of sign to replace described transcription text that further comprises.
8. computer-implemented method as claimed in claim 7, wherein replaces further comprising the one or more words that use the described data of the described entity of sign to replace described transcription text.
9. computer-implemented method as claimed in claim 1, wherein receives described environmental data and further comprises reception environment voice data, ambient image data or the two.
10. computer-implemented method as claimed in claim 9, wherein receives described ambient sound audio data and further comprises that reception comprises the additional voice data of ground unrest.
11. 1 kinds of systems, comprising:
One or more computing machines and one or more memory devices of storing exercisable instruction, when carrying out described instruction by described one or more computing machines, make described one or more computing machine executable operations, comprising:
The voice data of received code sounding and environmental data;
Obtain the transcription text of described sounding;
Use described environmental data identified entities;
To natural language querying processing engine submit Query, wherein said inquiry at least comprises a part for described transcription text and the data of the described entity of sign; And
Obtain one or more results of described inquiry.
12. systems as claimed in claim 11, described operation further comprises generated query, wherein generates described inquiry and comprises the described data correlation with the described entity of sign by described transcription text.
13. systems as claimed in claim 12, wherein associatedly further comprise transcription text described in the described data markers of using the described entity of sign.
14. systems as claimed in claim 12, wherein association further comprises that the described data of the described entity of use sign replace a part for described transcription text.
15. systems as claimed in claim 14, wherein replacement further comprises that the described data of the described entity of use sign replace one or more words of described transcription text.
16. systems as claimed in claim 11, wherein receive described environmental data and further comprise reception environment voice data, ambient image data or the two.
17. systems as claimed in claim 16, wherein receive described ambient sound audio data and further comprise that reception comprises the additional voice data of ground unrest.
The computer-readable medium of 18. 1 kinds of storing softwares, described software comprises the instruction that can be carried out by one or more computing machines, carries out described instruction and causes described one or more computing machine executable operations, comprising:
The voice data of received code sounding and environmental data;
Obtain the transcription text of described sounding;
Use described environmental data identified entities;
To natural language querying processing engine submit Query, wherein said inquiry at least comprises a part for described transcription text and the data of the described entity of sign; And
Obtain one or more results of described inquiry.
19. computer-readable mediums as claimed in claim 18, described operation further comprises generated query, wherein generates described inquiry and comprises the described data correlation with the described entity of sign by described transcription text.
20. computer-readable mediums as claimed in claim 19, wherein associatedly further comprise transcription text described in the described data markers of using the described entity of sign.
21. computer-readable mediums as claimed in claim 19, wherein association further comprises that the described data of the described entity of use sign replace a part for described transcription text.
22. computer-readable mediums as claimed in claim 21, wherein replacement further comprises that the described data of the described entity of use sign replace one or more words of described transcription text.
23. 1 kinds of computer-implemented methods, comprising:
Receive the voice data of (i) coding oral account natural language querying, and (ii) ambient sound audio data;
Obtain the transcription text of described oral account natural language querying;
The associated certain types of content of one or more keywords definite and in described transcription text;
To content recognition engine provide at least described ambient sound audio data-part; And
The content item that sign has been exported and mated with described certain types of content by described content recognition engine.
24. computer-implemented methods as claimed in claim 23, wherein said certain types of content is movie contents type, music content type, content of TV program type, audio frequency podcast content type, book contents type, artwork content type, trailer content type, video podcast content type, the Internet audio content type or video game content type.
25. computer-implemented methods as claimed in claim 23, wherein receive described ambient sound audio data and further comprise that reception comprises the additional voice data of ground unrest.
26. computer-implemented methods as claimed in claim 23, further comprise and receive the additional environmental data that comprises video data or view data.
27. computer-implemented methods as claimed in claim 23, further comprise content item destination data described in output identification.
28. computer-implemented methods as claimed in claim 23, wherein provide a described part at least described ambient sound audio data further to comprise the described part that described ambient sound audio data is provided to audio-frequency fingerprint engine to described content recognition engine.
29. computer-implemented methods as claimed in claim 23, wherein determine that described certain types of content further comprises and use one or more keywords described in one or more Database Identifications, for each content type in a plurality of content types, described database is mapped at least one content type in described a plurality of content type by least one keyword in described keyword.
30. computer-implemented methods as claimed in claim 29, wherein said a plurality of content types comprise described certain types of content, and wherein mapping further comprises at least one keyword in described keyword is mapped to described certain types of content.
31. computer-implemented methods as claimed in claim 23, wherein provide and further comprise the data that the described certain types of content of sign is provided to described content recognition engine, and
Wherein identifying described content item further comprises from the described content item destination data of described content recognition engine reception sign.
32. computer-implemented methods as claimed in claim 23, further comprise from described content identifying system and receive two or more content recognition candidates, and
Wherein identifying described content item further comprises based on described certain types of content selection certain content identification candidate.
33. computer-implemented methods as claimed in claim 32, each content recognition candidate in wherein said two or more content recognition candidates is associated with rank score, and described method further comprises the described rank score of adjusting described two or more content recognition candidates based on described certain types of content.
34. computer-implemented methods as claimed in claim 33, further comprise that rank score based on adjusting is to described two or more content recognition candidate ranks.
35. 1 kinds of systems, comprising:
One or more computing machines and one or more memory devices of storing exercisable instruction, when carrying out described instruction by described one or more computing machines, make described one or more computing machine executable operations, comprising:
Receive the voice data of (i) coding oral account natural language querying, and (ii) ambient sound audio data;
Obtain the transcription text of described oral account natural language querying;
The associated certain types of content of one or more keywords definite and in described transcription text;
A part at least described ambient sound audio data is provided to content recognition engine; And
The content item that sign has been exported and mated with described certain types of content by described content recognition engine.
36. systems as claimed in claim 35, wherein receive described ambient sound audio data and further comprise that reception comprises the additional voice data of ground unrest.
37. systems as claimed in claim 35, described operation further comprises that reception comprises the additional environmental data of video data or view data.
38. systems as claimed in claim 35, wherein provide a described part at least described ambient sound audio data further to comprise the described part that described ambient sound audio data is provided to audio-frequency fingerprint engine to described content recognition engine.
39. systems as claimed in claim 35, wherein determine that described certain types of content further comprises and use one or more keywords described in one or more Database Identifications, for each content type in a plurality of content types, described database is mapped at least one content type in described a plurality of content type by least one keyword in described keyword.
40. systems as claimed in claim 39, wherein said a plurality of content types comprise described certain types of content, and wherein mapping further comprises at least one keyword in described keyword is mapped to described certain types of content.
41. systems as claimed in claim 35, wherein provide and further comprise the data that the described certain types of content of sign is provided to described content recognition engine, and
Wherein identifying described content item further comprises from the described content item destination data of described content recognition engine reception sign.
42. systems as claimed in claim 35, described operation further comprises from described content identifying system and receives two or more content recognition candidates, and
Wherein identifying described content item further comprises based on described certain types of content selection certain content identification candidate.
43. systems as claimed in claim 42, each content recognition candidate in wherein said two or more content recognition candidates is associated with rank score, and described method further comprises the described rank score of adjusting described two or more content recognition candidates based on described certain types of content.
44. systems as claimed in claim 43, described operation further comprises that rank score based on adjusting is to described two or more content recognition candidate ranks.
The non-transient computer-readable medium of 45. 1 kinds of storing softwares, described software comprises the instruction that can be carried out by one or more computing machines, carries out described instruction and causes described one or more computing machine executable operations, comprising:
Receive the voice data of (i) coding oral account natural language querying, and (ii) ambient sound audio data;
Obtain the transcription text of described oral account natural language querying;
The associated certain types of content of one or more keywords definite and in described transcription text;
A part at least described ambient sound audio data is provided to content recognition engine; And
The content item that sign has been exported and mated with described certain types of content by described content recognition engine.
46. computer-readable mediums as claimed in claim 45, wherein provide a described part at least described ambient sound audio data further to comprise the described part that described ambient sound audio data is provided to audio-frequency fingerprint engine to described content recognition engine.
47. computer-readable mediums as claimed in claim 45, wherein determine that described certain types of content further comprises and use one or more keywords described in one or more Database Identifications, for each content type in a plurality of content types, described database is mapped at least one content type in described a plurality of content type by least one keyword in described keyword.
48. computer-readable mediums as claimed in claim 47, wherein said a plurality of content types comprise described certain types of content, and wherein mapping further comprises at least one keyword in described keyword is mapped to described certain types of content.
49. computer-readable mediums as claimed in claim 45, described operation further comprises content item destination data described in output identification.
50. computer-readable mediums as claimed in claim 45, wherein provide and further comprise the data that the described certain types of content of sign is provided to described content recognition engine, and
Wherein identifying described content item further comprises from the described content item destination data of described content recognition engine reception sign.
51. computer-readable mediums as claimed in claim 45, described operation further comprises from described content identifying system and receives two or more content recognition candidates, and
Wherein identifying described content item further comprises based on described certain types of content selection certain content identification candidate.
52. computer-readable mediums as claimed in claim 51, each content recognition candidate in wherein said two or more content recognition candidates is associated with rank score, and described method further comprises the described rank score of adjusting described two or more content recognition candidates based on described certain types of content.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN201610628594.XA CN106250508B (en) | 2012-09-10 | 2013-04-05 | Use environment context is answered a question |
Applications Claiming Priority (10)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261698934P | 2012-09-10 | 2012-09-10 | |
US201261698949P | 2012-09-10 | 2012-09-10 | |
US61/698,934 | 2012-09-10 | ||
US61/698,949 | 2012-09-10 | ||
US13/626,351 | 2012-09-25 | ||
US13/626,439 US20140074466A1 (en) | 2012-09-10 | 2012-09-25 | Answering questions using environmental context |
US13/626,351 US8484017B1 (en) | 2012-09-10 | 2012-09-25 | Identifying media content |
US13/626,439 | 2012-09-25 | ||
US13/768,232 US8655657B1 (en) | 2012-09-10 | 2013-02-15 | Identifying media content |
US13/768,232 | 2013-02-15 |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201610628594.XA Division CN106250508B (en) | 2012-09-10 | 2013-04-05 | Use environment context is answered a question |
Publications (2)
Publication Number | Publication Date |
---|---|
CN103714104A true CN103714104A (en) | 2014-04-09 |
CN103714104B CN103714104B (en) | 2016-10-05 |
Family
ID=50237523
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201610628594.XA Active CN106250508B (en) | 2012-09-10 | 2013-04-05 | Use environment context is answered a question |
CN201310394518.3A Expired - Fee Related CN103714104B (en) | 2012-09-10 | 2013-04-05 | Use environmental context is answered a question |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201610628594.XA Active CN106250508B (en) | 2012-09-10 | 2013-04-05 | Use environment context is answered a question |
Country Status (3)
Country | Link |
---|---|
KR (3) | KR102029276B1 (en) |
CN (2) | CN106250508B (en) |
WO (1) | WO2014039106A1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN107403619A (en) * | 2017-06-30 | 2017-11-28 | 武汉泰迪智慧科技有限公司 | A kind of sound control method and system applied to bicycle environment |
CN110268469A (en) * | 2017-02-14 | 2019-09-20 | 谷歌有限责任公司 | Server side hot word |
CN111344780A (en) * | 2017-08-30 | 2020-06-26 | 亚马逊技术股份有限公司 | Context-based device arbitration |
Families Citing this family (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9965774B2 (en) * | 2014-06-13 | 2018-05-08 | Flipboard, Inc. | Presenting advertisements in a digital magazine by clustering content |
US10133821B2 (en) * | 2016-01-06 | 2018-11-20 | Google Llc | Search result prefetching of voice queries |
US10049666B2 (en) * | 2016-01-06 | 2018-08-14 | Google Llc | Voice recognition system |
US10453456B2 (en) * | 2017-10-03 | 2019-10-22 | Google Llc | Tailoring an interactive dialog application based on creator provided content |
KR102533443B1 (en) * | 2018-05-04 | 2023-05-17 | 삼성전자 주식회사 | Method for providing content and electronic device using the same |
KR20200115695A (en) * | 2019-03-07 | 2020-10-08 | 삼성전자주식회사 | Electronic device and method for controlling the electronic devic thereof |
CN116806343A (en) * | 2020-10-01 | 2023-09-26 | 克劳德斯玛特有限公司 | Probability graph network |
KR20240012973A (en) * | 2022-07-21 | 2024-01-30 | 삼성전자주식회사 | Display apparatus that provides answer to question based on image and controlling method thereof |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7562392B1 (en) * | 1999-05-19 | 2009-07-14 | Digimarc Corporation | Methods of interacting with audio and ambient music |
Family Cites Families (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7760905B2 (en) * | 1999-06-29 | 2010-07-20 | Digimarc Corporation | Wireless mobile phone with content processing |
US7324943B2 (en) * | 2003-10-02 | 2008-01-29 | Matsushita Electric Industrial Co., Ltd. | Voice tagging, voice annotation, and speech recognition for portable devices with optional post processing |
KR100676863B1 (en) * | 2004-08-31 | 2007-02-02 | 주식회사 코난테크놀로지 | System and method for providing music search service |
US7640160B2 (en) * | 2005-08-05 | 2009-12-29 | Voicebox Technologies, Inc. | Systems and methods for responding to natural language speech utterance |
US7949529B2 (en) * | 2005-08-29 | 2011-05-24 | Voicebox Technologies, Inc. | Mobile systems and methods of supporting natural language human-machine interactions |
US20070073651A1 (en) | 2005-09-23 | 2007-03-29 | Tomasz Imielinski | System and method for responding to a user query |
US10056077B2 (en) * | 2007-03-07 | 2018-08-21 | Nuance Communications, Inc. | Using speech recognition results based on an unstructured language model with a music system |
US20080243788A1 (en) | 2007-03-29 | 2008-10-02 | Reztlaff James R | Search of Multiple Content Sources on a User Device |
CN101431573B (en) * | 2007-11-08 | 2013-02-20 | 上海撼世网络科技有限公司 | Method and equipment for implementing automatic customer service through human-machine interaction technology |
US9009053B2 (en) * | 2008-11-10 | 2015-04-14 | Google Inc. | Multisensory speech detection |
US8055675B2 (en) | 2008-12-05 | 2011-11-08 | Yahoo! Inc. | System and method for context based query augmentation |
KR101042515B1 (en) | 2008-12-11 | 2011-06-17 | 주식회사 네오패드 | Method for searching information based on user's intention and method for providing information |
KR20100067174A (en) * | 2008-12-11 | 2010-06-21 | 한국전자통신연구원 | Metadata search apparatus, search method, and receiving apparatus for iptv by using voice interface |
CN101917553B (en) * | 2009-11-27 | 2013-05-01 | 新奥特（北京）视频技术有限公司 | System for collectively processing multimedia data |
KR20120034378A (en) * | 2010-10-01 | 2012-04-12 | 엔에이치엔(주) | Advertisement information providing system through recognition of sound and method thereof |
KR101369931B1 (en) * | 2010-11-17 | 2014-03-04 | 주식회사 케이티 | System and method for hybrid semantic searching service |
-
2013
- 2013-04-03 WO PCT/US2013/035095 patent/WO2014039106A1/en active Application Filing
- 2013-04-05 CN CN201610628594.XA patent/CN106250508B/en active Active
- 2013-04-05 KR KR1020130037540A patent/KR102029276B1/en active IP Right Grant
- 2013-04-05 CN CN201310394518.3A patent/CN103714104B/en not_active Expired - Fee Related
-
2019
- 2019-09-27 KR KR1020190119592A patent/KR102140177B1/en active IP Right Grant
-
2020
- 2020-07-24 KR KR1020200092439A patent/KR102241972B1/en active IP Right Grant
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7562392B1 (en) * | 1999-05-19 | 2009-07-14 | Digimarc Corporation | Methods of interacting with audio and ambient music |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN110268469A (en) * | 2017-02-14 | 2019-09-20 | 谷歌有限责任公司 | Server side hot word |
CN110268469B (en) * | 2017-02-14 | 2023-05-23 | 谷歌有限责任公司 | Server side hotword |
US11699443B2 (en) | 2017-02-14 | 2023-07-11 | Google Llc | Server side hotwording |
CN107403619A (en) * | 2017-06-30 | 2017-11-28 | 武汉泰迪智慧科技有限公司 | A kind of sound control method and system applied to bicycle environment |
CN111344780A (en) * | 2017-08-30 | 2020-06-26 | 亚马逊技术股份有限公司 | Context-based device arbitration |
Also Published As
Publication number | Publication date |
---|---|
CN103714104B (en) | 2016-10-05 |
CN106250508B (en) | 2019-07-23 |
WO2014039106A1 (en) | 2014-03-13 |
KR20190113712A (en) | 2019-10-08 |
KR102140177B1 (en) | 2020-08-03 |
CN106250508A (en) | 2016-12-21 |
KR20140034034A (en) | 2014-03-19 |
KR102241972B1 (en) | 2021-04-20 |
KR102029276B1 (en) | 2019-10-07 |
KR20200093489A (en) | 2020-08-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN103714104A (en) | Answering questions using environmental context | |
US20220214775A1 (en) | Method for extracting salient dialog usage from live data | |
US9786279B2 (en) | Answering questions using environmental context | |
CN107481720B (en) | Explicit voiceprint recognition method and device | |
CN100468399C (en) | Query to task mapping | |
CN107209905A (en) | For personalized and task completion service, correspondence spends theme and sorted out | |
CN105659230A (en) | Query response using media consumption history | |
US20220083583A1 (en) | Systems, Methods and Computer Program Products for Associating Media Content Having Different Modalities | |
CN103534696B (en) | Domain detection in understanding for conversational language clicks on record using inquiry | |
US9639633B2 (en) | Providing information services related to multimodal inputs | |
CN104598502A (en) | Method, device and system for obtaining background music information in played video | |
CN101305368A (en) | Semantic visual search engine | |
CN108701155A (en) | Expert's detection in social networks | |
US20220229865A1 (en) | Automated content generation and delivery | |
CN105684457A (en) | Video frame selection for targeted content | |
CN104615689A (en) | Searching method and device | |
US20220308987A1 (en) | Debugging applications for delivery via an application delivery server | |
CN111753126A (en) | Method and device for video dubbing | |
CN110347786B (en) | Semantic model tuning method and system | |
EP2706470A1 (en) | Answering questions using environmental context | |
US11385990B2 (en) | Debugging applications for delivery via an application delivery server | |
US10437902B1 (en) | Extracting product references from unstructured text | |
CN111476028A (en) | Chinese phrase identification method, system, storage medium and electronic equipment | |
Liu et al. | TechWare: Mobile Media Search Resources [Best of the Web] | |
CN116994249A (en) | Information processing method, device, equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
C14 | Grant of patent or utility model | ||
GR01 | Patent grant | ||
CP01 | Change in the name or title of a patent holder | ||
CP01 | Change in the name or title of a patent holder |
Address after: California, USAPatentee after: Google Inc.Address before: California, USAPatentee before: Google Inc. |
|
CF01 | Termination of patent right due to non-payment of annual fee | ||
CF01 | Termination of patent right due to non-payment of annual fee |
Granted publication date: 20161005 |