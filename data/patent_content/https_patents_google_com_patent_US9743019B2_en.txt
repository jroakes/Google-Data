US9743019B2 - Multiplane panoramas of long scenes - Google Patents
Multiplane panoramas of long scenes Download PDFInfo
- Publication number
- US9743019B2 US9743019B2 US14/807,387 US201514807387A US9743019B2 US 9743019 B2 US9743019 B2 US 9743019B2 US 201514807387 A US201514807387 A US 201514807387A US 9743019 B2 US9743019 B2 US 9743019B2
- Authority
- US
- United States
- Prior art keywords
- planes
- points
- virtual camera
- fitted
- input images
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- H04N5/3415—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T3/00—Geometric image transformation in the plane of the image
- G06T3/40—Scaling the whole image or part thereof
- G06T3/4038—Scaling the whole image or part thereof for image mosaicing, i.e. plane images composed of plane sub-images
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N25/00—Circuitry of solid-state image sensors [SSIS]; Control thereof
- H04N25/40—Extracting pixel data from image sensors by controlling scanning circuits, e.g. by modifying the number of pixels sampled or to be sampled
- H04N25/41—Extracting pixel data from a plurality of image sensors simultaneously picking up an image, e.g. for increasing the field of view by combining the outputs of a plurality of sensors
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/06—Ray-tracing
-
- H04N13/0282—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/282—Image signal generators for generating image signals corresponding to three or more geometrical viewpoints, e.g. multi-view systems
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/698—Control of cameras or camera modules for achieving an enlarged field of view, e.g. panoramic image capture
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N25/00—Circuitry of solid-state image sensors [SSIS]; Control thereof
- H04N25/40—Extracting pixel data from image sensors by controlling scanning circuits, e.g. by modifying the number of pixels sampled or to be sampled
- H04N25/46—Extracting pixel data from image sensors by controlling scanning circuits, e.g. by modifying the number of pixels sampled or to be sampled by combining or binning pixels
-
- H04N5/23238—
-
- H04N5/347—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2200/00—Indexing scheme for image data processing or generation, in general
- G06T2200/04—Indexing scheme for image data processing or generation, in general involving 3D image data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10028—Range image; Depth image; 3D point clouds
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20212—Image combination
- G06T2207/20221—Image fusion; Image merging
Definitions
- FIG. 1B illustrates an exemplary sketch of three dimensional points of the scene depicted in FIG. 1A and planes fitted to the points in accordance with an embodiment.
- FIG. 2C illustrates a multiplane panoramic view of the long scene street side shown in FIG. 2A , in accordance with an embodiment.
- FIGS. 3A and 3B illustrate, respectively, an exemplary single plane rendering of a street side, and a corresponding multiplane rendering of the same street side in accordance with an embodiment.
- embodiments use multiple planes to project the dominant surfaces, background surfaces, foreground surfaces, and ground surface into the virtual camera to generate the panoramic image of the long scene.
- Reference numeral 212 indicates the ground plane
- reference numeral 214 indicates the dominant facade
- reference numeral 216 indicates a plane fit mostly to protrusions from the facade on the right building and the shrubbery
- reference numerals 218 and 220 correspond to planes fit to buildings on the left of the image.
- FIGS. 3A and 3B illustrate respectively, an exemplary single plane rendering of a street side, and a corresponding multiplane rendering of the same street side in accordance with an embodiment.
- Using multiple planes can alleviate some of the deletion and repetition artifacts seen in the single planes results.
- FIGS. 3A and 3B show a comparison between using a single plane and multiple planes for the illustrated street side.
- the view in FIG. 3A is made with a single plane fit to the dominant facade.
- the view in FIG. 3B is made with planes fit to the facade, the ground plane, and 2 planes fit to foreground objects. Since the multiplane image uses planes for the foreground objects, they do not get compressed or cut in half. Also, the ground plane looks more natural, consistent with viewing it from a single perspective. In the single plane result the ground plane is stretched vertically, making it look artificially wide.
- FIG. 4 illustrates a flowchart 400 of a method to generate panoramic images of long scenes, in accordance with an embodiment.
- input images are accessed.
- Each of the input images may include a view of a portion of a long scene.
- each input image may include a portion of a view of a street side facade.
- the input images may be panoramic images.
- input images may be 360 degree panoramic images used for Google Streetview.
- the input images may be non-panoramic.
- Three-dimensional point information associated with the input images may be another input to generate panoramic images of a long scene.
- the 3D points may be obtained at the same time as the input images are captured.
- a laser scan technique such as LIDAR, may be used to obtain the 3D points.
- Three-dimensional points may also be determined from respective images using a technique such as stereo.
- Each 3D point in effect, indicates the location in 3D of a point on a surface.
- the 3D location includes the depth from the camera or other viewpoint.
- the 3D points may be referred to as a 3D point cloud.
- FIG. 1B above illustrates a horizontal cross-section view of a 3D point cloud corresponding to the street side facade shown in FIG. 1A .
- Camera information associated with the input images may also be an input to the process for generating panoramic images of a long scene.
- the camera information includes camera locations at which each of the input images were captured.
- a single camera location representing the actual capture position for that image may be available.
- the camera location may be determined using the start position and end position of a capture.
- panoramic Streetview input images may have a start position and end position for the rolling shutter camera as it moves down the street, and the capture location may be approximated as the mid-point between the start and end positions.
- a plurality of planes are fitted to the 3D point cloud.
- planes may be fitted to the point cloud using a technique, such as, a mean shift approach and an update rule.
- the approach generates random initial guesses which are then optimized with mean shift to minimize the distance of each 3D point to the plane. Fitting planes to the 3D point cloud is further described below in relation to FIG. 5 .
- a location is determined for the virtual camera.
- the virtual camera may be positioned behind the capture camera locations looking in the direction of the long scene.
- the virtual camera may be considered as being located further away from the street than the capture cameras. As the virtual camera is moved further away a longer facade of the street side is viewable. The positioning of the virtual camera is further described below in relation to FIG. 6 .
- Steps 408 - 414 may be repeated to process all pixels, or a determined portion of the pixels, of the panoramic image of the long scene.
- steps 408 - 414 are repeated for each pixel of the panoramic image of the long scene.
- pixels from the top left corner of the image to the bottom right may be selected in sequence in a left to right and top to bottom manner.
- contiguous areas of pixels may be selected in the image for each iteration of steps 408 - 414 .
- groups of contiguous pixels having the same fitted plane may be selected for each iteration.
- Other techniques of selecting pixels for processing steps 408 - 414 such as, for example, selecting blocks of pixels are contemplated within embodiments. For purposes of discussion of the following steps, and not by way of limitation, it is assumed that, for each iteration of steps 408 - 414 , one pixel is selected (at step 408 ) in the left to right and top to bottom manner.
- a pairing of an input image and a fitted plane is chosen for the selected pixel of the panoramic image.
- each input image is projected into the virtual camera using one or more planes that are associated with the input image. For example, a projection into the virtual camera may be made for each plane that is fitted in the point cloud in a surface viewable in the input image.
- Each projection of an input image and fitted plane into the virtual camera produces a candidate image, which may then be used to select the best pairing of an input image and a fitted plane for the pixel. Choosing of the pairing of an input image and a fitted plane for the pixel is further described below in relation to FIG. 7 .
- the panoramic image of the long scene is stitched using the projection into the virtual camera of the pairing of an input image and a fitted plane for the selected pixel.
- the projections for all of the pixels are stitched to form the long scene panoramic image.
- step 414 it is determined whether more pixels are to be processed, and if so method 400 proceeds to step 408 to select the next pixel. Otherwise, method 400 proceeds to step 416 .
- step 416 blending of areas in the stitched image may be performed. Blending can, for example, reduce the visible seams at the edges of the portions of each input image that is projected to the final panoramic image of the long scene.
- FIG. 5 illustrates a method 500 for fitting planes to the 3D point cloud, in accordance with an embodiment.
- Method 500 may be used, for example, in performing step 404 discussed above.
- Method 500 is used to find a set of planes that substantially optimally fits the 3D point cloud.
- method 500 may result in one or more closely fitting planes respectively for each of the dominant facade, the background buildings and the foreground objects.
- method 500 operates to find a set of planes by starting with an initial set of planes from RANSAC (“RANdom SAmple Consensus”), and then using a mode seeking procedure to remove redundant RANSAC plane estimates. That is, initial planes are fitted to the 3D points using RANSAC, the planes are updated with the mode seeking procedure until convergence, and then any inliers are removed. This procedure is iterated until the desired number of unique planes is found. The entire 3D point cloud is used for mode seeking at each iteration; points are only removed for iterations of the RANSAC estimates.
- RANSAC Random SAmple Consensus
- an initial set of planes is determined.
- an initial set of planes may be determined based on a technique such as RANSAC. All of the 3D points or portions of the 3D points may be selected as points based upon which an initial set of planes is determined to fit those selected points. For example, based upon RANSAC, the 3D points or the selected portions of those points may be considered as inliers, i.e., points that correspond to the desired model (for example, the surface of a street side), and the initial set of planes may be determined based upon a least squares approximation of the initial set to the 3D points considered as initial inliers.
- weights for 3D points are determined for each plane.
- the weights may be based upon the distance from the point to the respective plane. According to an embodiment, the weights decay exponentially as the distance between a point and a corresponding plane increases.
- the set of planes is updated.
- the updating may include a mode seeking step loosely based upon the mean shift technique.
- Mean shift is a clustering algorithm that finds modes (local maxima) of a kernel density estimator. Given an initialization x of a cluster center and a kernel function, x is updated to the mean of the surrounding points weighted by the kernel function. Different initializations can converge to different modes, and all points that converge to the same mode are clustered together.
- planes may be found by starting with an initial estimate, calculating weights for the point cloud based on the distance to the plane, and updating the plane with a weighted least squares fit minimizing the orthogonal distances of the points to the plane.
- Minimizing the orthogonal distances may include reducing the distances towards a minimum, and does not necessarily require obtaining the minimum values.
- the set of planes may be updated with weighted least squares fit minimizing the orthogonal distances of the 3D points to the respective planes.
- ⁇ are the plane parameters
- x i are points from the point cloud
- Dist gives the point to plane distance
- the update of the set of planes with the mode seeking procedure may be repeated until convergence of the set of planes. Any inliers, as determined by the updated set, can then be removed.
- step 502 After the update step, it is determined whether further iterations of steps 502 - 506 are required. If more iterations are required, processing of method 500 proceeds to step 502 with the point cloud updated to remove inlier points and duplicate surfaces. The determination may be based upon whether the desired number of unique planes fitting the 3D point cloud has been found. The desired number of planes may be configured as an upper threshold of planes to be found. The determination may also be based upon characteristics of the remaining 3D points which have not yet been removed from the point cloud considered for the RANSAC step. For example, not having any areas with a distribution density of the 3D points above a configured threshold may be considered as an indication of the lack of additional significant surface areas to be fitted with planes.
- FIG. 6 illustrates a flowchart of a method 600 to determine the virtual camera to be used to generate panoramic images of long scenes.
- Method 600 may, for example, be used in performing step 406 of method 400 discussed above.
- the type of camera to be used as the virtual camera is selected.
- the panoramic image of the long scene is produced by projecting each input image into a new view, or virtual view, using a virtual camera.
- Any camera model may be used as the virtual camera.
- a standard perspective camera, with view direction parallel to the ground plane is used. A view direction that is parallel to the ground ensures vertical lines stay vertical.
- the location of the virtual camera is determined.
- the virtual camera may be positioned by moving back from the center of the input camera's path (from the first input image to the last input image) in a direction perpendicular to the input camera's path and at approximately a 10 degree angle with the ground plane. This procedure generally moves the camera away from the street side, allowing the capture of more of the street facade in a single view.
- the viewing direction of the virtual camera may be back towards the input path center, but parallel to the ground plane. The distance that the virtual camera is pulled back determines the width of the field of view of the final panoramic image.
- FIG. 7 illustrates a flowchart for a method 700 for selecting input images to be rendered for respective pixels or groups of pixels of the panoramic image of the long scene.
- Method 700 can be used, for example, in performing step 410 discussed above.
- Each input image or input panorama can be rendered into the virtual camera using any of the planes fitted to the 3D point cloud, producing a candidate image. These images are good candidates where the plane is close the true surface (e.g., surface of the street facade) and the input image views that surface without occlusion.
- the best input image/fitted plane pair From the candidate images, for each pixel in the virtual image, the best input image/fitted plane pair.
- a function to be optimized can measure how parallel the virtual camera's viewing ray is to the viewing ray of the input image.
- the input images and fitted planes are paired to generate candidate images.
- the horizontal and vertical fields of view of the camera can be determined.
- Each 3D point can be projected into the virtual view to determine the minimum and maximum image coordinates in the vertical and horizontal directions.
- a view is rendered from the virtual camera by projecting rays onto the plane and back into the input image.
- pairings that include input images having areas that are not visible to the virtual camera are discarded.
- the candidate images from step 702 above may be inaccurate when the input image includes areas that are occluded to the virtual camera. Therefore, such input images may be removed from consideration.
- the 3D point cloud can be used to determine the input images that include surfaces or areas that are occluded from the virtual camera. For example, in FIG. 1A , the virtual camera may not view some areas of building surfaces in the dominant facade due the car in the foreground. However, those areas occluded from the virtual camera may still be visible to one or more input cameras because of the differences in viewing rays, and fields of view.
- the candidate image for rendering the pixel or group of pixels in the panoramic image of the long scene is chosen.
- the selection of the candidate image is based upon a graph cut based Markov Random Field (MRF) approach.
- MRF Markov Random Field
- the data term of the MRF may be selected to measure the distance between rays from the virtual camera and the center of projection of the input cameras. This data term encourages using input images that are likely to see the scene geometry from close to the same viewing direction as the virtual camera. In instances where, such as in Streetview panoramic images, there is no fixed center of projection (due to, for example, a rolling shutter), the center of the input camera positions for the input image can be approximated as its center of projection.
- the cost of assigning a camera c with center of projection p c , to a ray x+tv is represented by (2).
- E data (1) ⁇ pixels i C ( P l(i) ,x i ,v i ) (3) where (x i , v i ) is the ray associated with pixel i and l is an assignment of cameras/plane pairs to pixels, i.e. l(i) is the index of the camera/plane pair assigned to pixel i.
- Changing the viewpoint of the virtual camera may change the data costs, resulting in view-dependent rendering effects. This is in contrast to data terms which uses two dimensional image distances, and encourages using views directly in front of the plane along its normal direction.
- a smoothness term is used to encourage seamless transitions between pixels from different input images.
- the smoothness cost for neighboring pixels is shown in (4).
- V ( i,l ( i ), j,l ( j ))
- V ⁇ ( i , l ⁇ ( i ) , j , l ⁇ ( j ) ) c ⁇ ⁇ I l ⁇ ( i ) ⁇ ( i ) - I l ⁇ ( j ) ⁇ ( i ) ⁇ 1 - ⁇ I l ⁇ ( i ) ⁇ ( j ) - I l ⁇ ( j ) ⁇ ( j ) ⁇ 1 1 + ⁇ I l ⁇ ( i ) ⁇ ( i ) - I l ⁇ ( i ) ⁇ ( j ) ⁇ 1 - ⁇ I l ⁇ ( j ) ⁇ ( j ) - I l ⁇ ( j ) ⁇ ( j ) ⁇ 1 ( 5 )
- the denominator in the above equation measures how strong of an edge is between the neighboring pixels in both images, encouraging cuts along strong edges when transitioning between planes.
- the motivation for this term is to encourage transition between planes at depth discontinuities, where there are likely edges in the image.
- the constant c may be set to a value such as 100 in order to prevent the cost of switching planes from being much cheaper than staying on the same plane.
- the total energy to be minimized may be a sum of the data and smoothness terms, and may be represented as in (7). Minimizing the total energy includes reducing the energy towards a minimum, and may not necessarily include reducing to the minimum value.
- E smoothness E data + ⁇ E smoothness (7)
- the camera or input image assignment MRF may be further optimized using techniques such as alpha-expansion techniques.
- alpha-expansion techniques are described in Y. Boykov, O. Veksler, and R. Zabih. “Fast approximate energy minimization via graph cuts,” IEEE Trans. Pattern Anal. Mach. Intell., 23:1222-1239, November 2001.
- the alpha-expansion with support for sparse data terms can be used to obtain a performance boost when each label is only feasible for a small portion of pixels. This condition occurs, for example, when rays from the virtual camera do not project into a particular input image.
- the above described process of selecting candidate images can be optionally further optimized by adding selected constraints.
- the point cloud may be used to add constraints such that if it is known which planes are accurate in a certain area, then only those planes are used. Also, if it is known that a texture (portion of input image) is valid for a particular plane, then that texture shouldn't be used for any other planes.
- Depth agreement constraints may also be added based on whether laser points (i.e., 3D points) agree with each of the planes. If a laser point projects into the virtual camera unoccluded (by the mesh described above, for example) and it is close to any of the planes, then only planes which are close to that point are valid planes for the pixel where that point projects in the virtual view. All other planes are marked as infeasible for the corresponding pixel. This constraint forces objects that are only close to certain planes to use those planes.
- Additional occlusion constraints may be included to prevent images from being used to texture occluded geometry.
- the input camera can be marked as infeasible for all the corresponding pixels in the virtual view.
- the above constraints yield a set of infeasible camera/plane labels.
- the depth agreement constraint marks a set of labels with the same geometry as invalid for each pixel, and the occlusion constraint marks particular camera/label pairs as invalid.
- These constraints can be encoded by assigning the labels very high weights in the MRF, or by removing them as candidate labels for those pixels during the optimization of the MRF.
- FIG. 8 illustrates an example network system 800 for rendering panoramic images of long scenes from input images of portions of the long scene according to an embodiment.
- Network system 800 includes server 802 .
- Server 802 includes a long scene panoramic image forming module 804 .
- Long scene panoramic image forming module 804 includes logic to generate panoramic long scenes from input images of portions of the scene, in accordance with an embodiment. According to an embodiment, logic for method 400 may be included in long scene panoramic image forming module 804 .
- Network 806 can include any network or combination of networks that can carry data communication. These networks can include, for example, a local area network (LAN) or a wide area network (WAN), such as the Internet. LAN and WAN networks can include any combination of wired (e.g., Ethernet) or wireless (e.g., Wi-Fi, 3G, or 4G) network components.
- LAN and WAN networks can include any combination of wired (e.g., Ethernet) or wireless (e.g., Wi-Fi, 3G, or 4G) network components.
- Server 802 , computing device 808 and client device 820 can be implemented on any computing device capable of processing images.
- Devices 802 , 808 and 820 may include, for example, a mobile computing device (e.g. a mobile phone, a smart phone, a personal digital assistant (PDA), a navigation device, a tablet, or other mobile computing devices).
- Devices 802 , 808 and 820 may also include, but are not limited to, a central processing unit, an application-specific integrated circuit, a computer, workstation, a distributed computing system, a computer cluster, an embedded system, a stand-alone electronic device, a networked device, a rack server, a set-top box, or other type of computer system having at least one processor and memory.
- a computing process performed by a clustered computing environment or server farm may be carried out across multiple processors located at the same or different locations.
- Hardware can include, but is not limited to, a processor, memory, and a user interface display.
- Camera 810 may include any digital image capture device such as, for example, a digital camera or an image scanner. While camera 810 is included in computing device 808 , camera 810 is not intended to limit the embodiments in any way. Alternative methods may be used to acquire photographic images such as, for example, retrieving photographic images from a local or networked storage device.
- Client device 820 includes a user interface module 822 and a long scene rendering module 824 .
- User interface module 822 may include input and output devices including keyboard, display, a browser, and the like that enable user interaction with client device 820 .
- Long scene rendering module 824 includes logic to render a long scene on client device 822 based upon information communicated from server 802 . According to an embodiment, long scene rendering module 824 renders a long scene on a browser on client device 822 , where the panoramic image of the long scene is retrieved from server 802 or from another location (not shown) where the long scene generated by server 802 is stored.
- FIG. 9 illustrates a system 900 for long scene panoramic image generation in accordance with an embodiment of the present invention.
- System 900 includes panoramic long scene image forming module 908 which may be configured to implement method 400 .
- long scene panoramic image generation module 908 includes a plane fitting module 910 , a virtual camera module 912 , a pairing selector module 914 , a panoramic image stitcher module 916 , and a panoramic image blender module 918 .
- Plane fitting module 910 includes logic to fit planes into the 3D point cloud.
- plane fitting module 910 can be configured to implement method 500 .
- Virtual camera module 912 may include logic to determine an appropriate camera type, and to position the virtual camera.
- virtual camera module 912 can implement functions described in relation to method 600 .
- Pairing selector 914 includes logic to select candidate images for pixels or groups of pixels.
- pairing selector 914 can implement method 700 .
- Panoramic image stitcher 916 includes logic to stitch the panoramic long scene from the projections of the different input images.
- Panoramic image blender 918 includes logic to blend the panoramic long scene, for example, in order to reduce visibility of seams between portions of the image projected from different input images.
- System 900 may also include input images 902 , 3D point information 904 , and camera information 906 .
- Input images 902 may include panoramic or non-panoramic images of portions of the long scene to be rendered.
- 3D point information 904 includes the point information corresponding to the input images.
- Camera information 906 includes information, such as the capture positions, capture start position, and capture end positions, and/or moving speed information for the camera, for the respective input images.
- FIG. 10 illustrates an example computer 1000 in which the embodiments described herein, or portions thereof, may be implemented as computer-readable code.
- long scene panoramic image forming module 908 or any of its modules can be executed on one or more computer systems 1000 using hardware, software, firmware, computer readable storage media having instructions stored thereon, or a combination thereof.
- a computing device having at least one processor device and a memory may be used to implement the above described embodiments.
- a processor device may be a single processor, a plurality of processors, or combinations thereof.
- Processor devices may have one or more processor “cores.”
- processor device 1004 may be a single processor in a multi-core/multiprocessor system, such system operating alone, or in a cluster of computing devices operating in a cluster or server farm.
- Processor device 1004 is connected to a communication infrastructure 1006 , for example, a bus, message queue, network, or multi-core message-passing scheme.
- Computer system 1000 may also include display interface 1002 and display unit 1030 .
- Computer system 1000 also includes a main memory 1008 , for example, random access memory (RAM), and may also include a secondary memory 1010 .
- Secondary memory 1010 may include, for example, a hard disk drive 1012 , and removable storage drive 1014 .
- Removable storage drive 1014 may include a floppy disk drive, a magnetic tape drive, an optical disk drive, a flash memory drive, or the like.
- the removable storage drive 1014 reads from and/or writes to a removable storage unit 1018 in a well-known manner.
- Removable storage unit 1018 may include a floppy disk, magnetic tape, optical disk, flash memory drive, etc. which is read by and written to by removable storage drive 1014 .
- removable storage unit 1018 includes a computer readable storage medium having stored thereon computer software and/or data.
- secondary memory 1010 may include other similar means for allowing computer programs or other instructions to be loaded into computer system 1000 .
- Such means may include, for example, a removable storage unit 1022 and an interface 1020 .
- Examples of such means may include a program cartridge and cartridge interface (such as that found in video game devices), a removable memory chip (such as an EPROM, or PROM) and associated socket, and other removable storage units 1022 and interfaces 1020 which allow software and data to be transferred from the removable storage unit 1022 to computer system 1000 .
- Computer system 1000 may also include a communications interface 1024 .
- Communications interface 1024 allows software and data to be transferred between computer system 1000 and external devices.
- Communications interface 1024 may include a modem, a network interface (such as an Ethernet card), a communications port, a PCMCIA slot and card, or the like.
- Software and data transferred via communications interface 1024 may be in the form of signals, which may be electronic, electromagnetic, optical, or other signals capable of being received by communications interface 1024 . These signals may be provided to communications interface 1024 via a communications path 1026 .
- Communications path 1026 carries signals and may be implemented using wire or cable, fiber optics, a phone line, a cellular phone link, an RF link or other communications channels.
- Computer storage medium and “computer readable storage medium” are used to generally refer to media such as removable storage unit 1018 , removable storage unit 1022 , and a hard disk installed in hard disk drive 1012 .
- Computer storage medium and computer readable storage medium may also refer to memories, such as main memory 1008 and secondary memory 1010 , which may be memory semiconductors (e.g. DRAMs, etc.).
- Computer programs are stored in main memory 1008 and/or secondary memory 1010 . Computer programs may also be received via communications interface 1024 . Such computer programs, when executed, enable computer system 1000 to implement the embodiments described herein. In particular, the computer programs, when executed, enable processor device 1004 to implement the processes of the embodiments, such as the stages in the methods illustrated by flowcharts of FIGS. 4-7 , described above. Accordingly, such computer programs represent controllers of computer system 1000 . Where an embodiment is implemented using software, the software may be stored in a computer storage medium and loaded into computer system 1000 using removable storage drive 1014 , interface 1020 , and hard disk drive 1012 , or communications interface 1024 .
- Embodiments of the invention also may be directed to computer program products including software stored on any computer readable storage medium.
- Such software when executed in one or more data processing device, causes a data processing device(s) to operate as described herein.
- Examples of computer readable storage mediums include, but are not limited to, primary storage devices (e.g., any type of random access memory) and secondary storage devices (e.g., hard drives, floppy disks, CD ROMS, ZIP disks, tapes, magnetic storage devices, and optical storage devices, MEMS, nanotechnological storage device, etc.).
Abstract
Description
θt+1=arg maxθΣiexp[−βDist2(θt ,x i)]Dist(θ,x i) (1)
C(p c ,x,v)=|p c−(x+<p c −x,v>v)| (2)
Only rays which project into an input image are valid, otherwise the cost is infinite or very high in practice. The total data cost can be represented as in (3).
E data(1)=Σpixels i C(P l(i) ,x i ,v i) (3)
where (xi, vi) is the ray associated with pixel i and l is an assignment of cameras/plane pairs to pixels, i.e. l(i) is the index of the camera/plane pair assigned to pixel i. Changing the viewpoint of the virtual camera may change the data costs, resulting in view-dependent rendering effects. This is in contrast to data terms which uses two dimensional image distances, and encourages using views directly in front of the plane along its normal direction.
V(i,l(i),j,l(j))=|I l(i)(i)−I l(j)(i)|1 −|I l(i)(j)−I l(j)(j)|1 (4)
If l(i) and l(j) correspond to the same plane. If l(i) and l(j) correspond to the different planes the smoothness cost is shown in (5).
The denominator in the above equation measures how strong of an edge is between the neighboring pixels in both images, encouraging cuts along strong edges when transitioning between planes. The motivation for this term is to encourage transition between planes at depth discontinuities, where there are likely edges in the image. The constant c may be set to a value such as 100 in order to prevent the cost of switching planes from being much cheaper than staying on the same plane. The total smoothness cost may be represented as in (6).
E smoothness=ΣiΣΩ(i) V(i,l(i),Ω(i),l(Ω(i))) (6)
where Ω(i) is the set of all neighbors of i assuming the pixels are 4-connected.
E smoothness =E data +αE smoothness (7)
The constant α may be configured according to the proportion of the effect of the data and smoothness terms. According to an embodiment, α=1 may be used when rendering images 2000-3000 pixels wide, and when 3D distances are measured in meters.
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/807,387 US9743019B2 (en) | 2011-12-30 | 2015-07-23 | Multiplane panoramas of long scenes |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201161582109P | 2011-12-30 | 2011-12-30 | |
US13/543,543 US9118905B2 (en) | 2011-12-30 | 2012-07-06 | Multiplane panoramas of long scenes |
US14/807,387 US9743019B2 (en) | 2011-12-30 | 2015-07-23 | Multiplane panoramas of long scenes |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/543,543 Continuation US9118905B2 (en) | 2011-12-30 | 2012-07-06 | Multiplane panoramas of long scenes |
Publications (2)
Publication Number | Publication Date |
---|---|
US20160028972A1 US20160028972A1 (en) | 2016-01-28 |
US9743019B2 true US9743019B2 (en) | 2017-08-22 |
Family
ID=53266377
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/543,543 Active 2033-11-13 US9118905B2 (en) | 2011-12-30 | 2012-07-06 | Multiplane panoramas of long scenes |
US14/807,387 Active 2033-01-09 US9743019B2 (en) | 2011-12-30 | 2015-07-23 | Multiplane panoramas of long scenes |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/543,543 Active 2033-11-13 US9118905B2 (en) | 2011-12-30 | 2012-07-06 | Multiplane panoramas of long scenes |
Country Status (1)
Country | Link |
---|---|
US (2) | US9118905B2 (en) |
Families Citing this family (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8422825B1 (en) | 2008-11-05 | 2013-04-16 | Hover Inc. | Method and system for geometry extraction, 3D visualization and analysis using arbitrary oblique imagery |
US9118905B2 (en) * | 2011-12-30 | 2015-08-25 | Google Inc. | Multiplane panoramas of long scenes |
NL2008639C2 (en) * | 2012-04-13 | 2013-10-16 | Cyclomedia Technology B V | Device, system and vehicle for recording panoramic images, and a device and method for panoramic projection thereof. |
US11670046B2 (en) | 2013-07-23 | 2023-06-06 | Hover Inc. | 3D building analyzer |
US9691138B2 (en) * | 2013-08-30 | 2017-06-27 | Google Inc. | System and method for adjusting pixel saturation |
US10147211B2 (en) * | 2015-07-15 | 2018-12-04 | Fyusion, Inc. | Artificially rendering images using viewpoint interpolation and extrapolation |
KR20170025058A (en) | 2015-08-27 | 2017-03-08 | 삼성전자주식회사 | Image processing apparatus and electronic system including the same |
US10438405B2 (en) | 2017-03-02 | 2019-10-08 | Sony Corporation | Detection of planar surfaces for use in scene modeling of a captured scene |
US10313651B2 (en) | 2017-05-22 | 2019-06-04 | Fyusion, Inc. | Snapshots at predefined intervals or angles |
CN108171715B (en) * | 2017-12-05 | 2020-08-04 | 浙江大华技术股份有限公司 | Image segmentation method and device |
US11012750B2 (en) * | 2018-11-14 | 2021-05-18 | Rohde & Schwarz Gmbh & Co. Kg | Method for configuring a multiviewer as well as multiviewer |
US10986287B2 (en) | 2019-02-19 | 2021-04-20 | Samsung Electronics Co., Ltd. | Capturing a photo using a signature motion of a mobile device |
CN110111378B (en) * | 2019-04-04 | 2021-07-02 | 贝壳技术有限公司 | Point cloud registration optimization method and device based on indoor three-dimensional data |
CN111127536A (en) * | 2019-12-11 | 2020-05-08 | 清华大学 | Light field multi-plane representation reconstruction method and device based on neural network |
WO2023050396A1 (en) * | 2021-09-30 | 2023-04-06 | Oppo广东移动通信有限公司 | Methods and apparatuses for generating, performing data processing on, encoding and decoding multiplane image |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20070076016A1 (en) | 2005-10-04 | 2007-04-05 | Microsoft Corporation | Photographing big things |
US20090179895A1 (en) | 2008-01-15 | 2009-07-16 | Google Inc. | Three-Dimensional Annotations for Street View Data |
US20100201682A1 (en) | 2009-02-06 | 2010-08-12 | The Hong Kong University Of Science And Technology | Generating three-dimensional fadeçade models from images |
US9118905B2 (en) * | 2011-12-30 | 2015-08-25 | Google Inc. | Multiplane panoramas of long scenes |
-
2012
- 2012-07-06 US US13/543,543 patent/US9118905B2/en active Active
-
2015
- 2015-07-23 US US14/807,387 patent/US9743019B2/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20070076016A1 (en) | 2005-10-04 | 2007-04-05 | Microsoft Corporation | Photographing big things |
US20090179895A1 (en) | 2008-01-15 | 2009-07-16 | Google Inc. | Three-Dimensional Annotations for Street View Data |
US20100201682A1 (en) | 2009-02-06 | 2010-08-12 | The Hong Kong University Of Science And Technology | Generating three-dimensional fadeçade models from images |
US9118905B2 (en) * | 2011-12-30 | 2015-08-25 | Google Inc. | Multiplane panoramas of long scenes |
Also Published As
Publication number | Publication date |
---|---|
US20150156415A1 (en) | 2015-06-04 |
US9118905B2 (en) | 2015-08-25 |
US20160028972A1 (en) | 2016-01-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9743019B2 (en) | Multiplane panoramas of long scenes | |
CN111127655B (en) | House layout drawing construction method and device, and storage medium | |
Hedman et al. | Scalable inside-out image-based rendering | |
US10304244B2 (en) | Motion capture and character synthesis | |
Furukawa et al. | Accurate, dense, and robust multiview stereopsis | |
US9305371B2 (en) | Translated view navigation for visualizations | |
US20120081357A1 (en) | System and method for interactive painting of 2d images for iterative 3d modeling | |
Boulch et al. | Piecewise‐planar 3D reconstruction with edge and corner regularization | |
US20150097827A1 (en) | Target Region Fill Utilizing Transformations | |
US8436852B2 (en) | Image editing consistent with scene geometry | |
US20190057532A1 (en) | Realistic augmentation of images and videos with graphics | |
KR20080051158A (en) | Photographing big things | |
Liu et al. | 3D entity-based stereo matching with ground control points and joint second-order smoothness prior | |
Lukasczyk et al. | Voidga: A view-approximation oriented image database generation approach | |
Hao et al. | Slice-based building facade reconstruction from 3D point clouds | |
Saxena et al. | 3-d reconstruction from sparse views using monocular vision | |
Evers‐Senne et al. | Image based interactive rendering with view dependent geometry | |
Deng et al. | Interactive panoramic map-like views for 3D mountain navigation | |
Li et al. | Texture category-based matching cost and adaptive support window for local stereo matching | |
CA2716257A1 (en) | System and method for interactive painting of 2d images for iterative 3d modeling | |
Turner | 3D Modeling of Interior Building Environments and Objects from Noisy Sensor Suites | |
Phothong et al. | Quality improvement of 3D models reconstructed from silhouettes of multiple images | |
Stereopsis | Accurate, dense, and robust multiview stereopsis | |
Ton et al. | 3D Least Squares Based Surface Reconstruction | |
Jiang et al. | A large-scale scene display system based on webgl |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GALLUP, DAVID;SEITZ, STEVEN MAXWELL;AGRAWALA, MANEESH;AND OTHERS;SIGNING DATES FROM 20120621 TO 20120629;REEL/FRAME:036308/0415 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044097/0658Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |