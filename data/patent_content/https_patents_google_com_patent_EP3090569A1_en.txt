EP3090569A1 - Content-adaptive chunking for distributed transcoding - Google Patents
Content-adaptive chunking for distributed transcodingInfo
- Publication number
- EP3090569A1 EP3090569A1 EP14825587.0A EP14825587A EP3090569A1 EP 3090569 A1 EP3090569 A1 EP 3090569A1 EP 14825587 A EP14825587 A EP 14825587A EP 3090569 A1 EP3090569 A1 EP 3090569A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- video clip
- scene
- frames
- chunks
- chunk size
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Withdrawn
Links
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/91—Television signal processing therefor
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/2343—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving reformatting operations of video signals for distribution or compliance with end-user requests or end-user device requirements
- H04N21/234309—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving reformatting operations of video signals for distribution or compliance with end-user requests or end-user device requirements by transcoding between formats or standards, e.g. from MPEG-2 to MPEG-4 or from Quicktime to Realvideo
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/142—Detection of scene cut or scene change
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/179—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being a scene or a shot
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/40—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using video transcoding, i.e. partial or full decoding of a coded input stream followed by re-encoding of the decoded output stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/42—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by implementation details or hardware specially adapted for video compression or decompression, e.g. dedicated software implementation
- H04N19/436—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by implementation details or hardware specially adapted for video compression or decompression, e.g. dedicated software implementation using parallelised computational arrangements
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/44—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs
- H04N21/44008—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics in the video stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/83—Generation or processing of protective or descriptive data associated with content; Content structuring
- H04N21/845—Structuring of content, e.g. decomposing content into time segments
- H04N21/8456—Structuring of content, e.g. decomposing content into time segments by decomposing the content in the time domain, e.g. in time segments
Definitions
- Transcoding is the direct digital-to-digital data conversion of one encoding to another. Transcoding is often utilized in the delivery of video clips to client machines (e.g., desktop computers, smartphones, tablets, etc.) to provide support for various screen resolutions, aspect ratios, file formats, codecs, etc.
- client machines e.g., desktop computers, smartphones, tablets, etc.
- a computer system determines N frames at which to divide a video clip into N+1 consecutive chunks, where N is a positive integer, and where the frames are determined based on the image content of the video clip, a minimum chunk size, and a maximum chunk size.
- N+1 chunks are provided to a respective processor for transcoding, and a transcoded video clip is then generated from the transcoded N+1 chunks.
- Figure 1 depicts a portion of an illustrative video clip and illustrative fixed-size and content-adaptive chunking of the video clip.
- Figure 2 illustrates an exemplary system architecture, in accordance with one implementation of the present disclosure.
- Figure 3 is a block diagram of one implementation of a transcoding manager.
- Figure 4 depicts a flow diagram of aspects of a method for distributed transcoding of video clips.
- Figure 5 depicts a flow diagram of aspects of a method for determining boundary frames at which to divide video into chunks.
- Figure 6 depicts a block diagram of an illustrative computer system operating in accordance with aspects and implementations of the present disclosure.
- implementations of the present disclosure are disclosed for distributed transcoding of video clips.
- implementations of the present disclosure are capable of dividing a video clip into chunks, providing each of the chunks to a respective processor for transcoding (e.g., a central processing unit of a respective server, a respective processor of a multi-processor computer, etc.), and generating a transcoded video clip from the transcoded chunks. Because the chunks can be transcoded in parallel by the processors, the video clip can be transcoded in a fraction of the time required for a single processor transcoding the entire video clip.
- a respective processor for transcoding e.g., a central processing unit of a respective server, a respective processor of a multi-processor computer, etc.
- a problem that may arise with such a strategy is that chunks can vary widely in their video coding complexity. More particularly, when a scene is split across adjacent chunks having different video coding complexities, the result can be discontinuities at chunk boundaries that, when large enough, can be visible to a viewer of the transcoded video clip. For example, there may be a discontinuity in quantization step size between adjacent chunks that, when large enough, causes a visible discontinuity in peak signal-to- noise ratio (PSNR) at the chunk boundary.
- PSNR peak signal-to- noise ratio
- a further problem when using chunking to transcode video arises from the nature of video compression. More particularly, video compression utilizes different types of frames - I-frames containing fully-specified images, and non-I-frames that store only changes between adjacent frames (e.g., predicted picture frames known as P-frames, bi-predictive picture frames known as B-frames, etc.). While the first frame of a chunk is always an I-frame, the final frame of a chunk may be either an I-frame or a non-I-frame. Moreover, I-frames and non-I-frames exhibit different quantization noise patterns.
- the quality difference between a final non-I-frame of a chunk and the initial I-frame of the next chunk can result in a visible flicker known as I-pulsing, particularly in lower bit rate encoding schemes (e.g., lower bit rate H.264/MPEG-4 encodings, etc.).
- lower bit rate encoding schemes e.g., lower bit rate H.264/MPEG-4 encodings, etc.
- Implementations of the present disclosure can mitigate these inherent problems of chunking by using a content-adaptive algorithm. More particularly, instead of naively dividing a video clip into fixed-size (or approximately fixed-size) chunks, implementations of the present disclosure determine chunk boundaries based on the image content of the video clip (e.g., pixel values of frames of the video clip, features of the video clip, etc.), a minimum chunk size, and a maximum chunk size. This approach yields fewer artifacts at chunk boundaries, thereby resulting in an improved viewing experience for users.
- determining chunk boundaries based on the image content of a video clip comprises identifying scene changes in the video clip (e.g., via extraction of effects such as fade in or fade out, via pixel-based differences between frames, via histogram-based differences between frames, via statistical analysis of features, etc.).
- identifying scene changes and, when possible, aligning chunk boundaries with scene changes the quality of the stitched-together transcoded video clip is improved, as artifacts caused by chunking are generally less noticeable to viewers when coinciding with scene changes.
- Figure 1 depicts a portion of an illustrative video clip comprising scenes 101-1 through 101-5 divided by (a) an illustrative fixed-size chunking of the video clip, and by (b) an illustrative content-adaptive chunking of the video clip.
- the content-adaptive chunks have fewer boundaries occurring within a scene compared to the fixed-size chunking, thereby resulting in a higher-quality transcoded video clip.
- the determination of chunk boundaries is also based on a default chunk size, in addition to minimum and maximum chunk sizes.
- the default chunk size is greater than or equal to the minimum chunk size and less than or equal to the maximum chunk size.
- the splitting of the scene at a chunk boundary may be based on image content.
- the chunk boundary may be determined based on a measure of brightness of individual frames of the scene (e.g., splitting the scene at a frame at which a measure of brightness has a minimum rate of change, etc.), or based on a measure of motion across frames of the scene (e.g., splitting the scene at a frame at which a measure of motion has a minimum rate of change, etc.).
- a chunk may first be decoded to an intermediate "universal" format, and then transcoded from the universal format to a target encoding.
- a video clip may be transcoded into a plurality of different encodings (e.g., H.264/MPEG-4, MPEG-2, etc.).
- each chunk is transcoded into the plurality of different encodings, and a transcoded video clip for each encoding is generated by assembling the corresponding transcoded chunks (e.g., an MPEG-2 video clip is assembled from MPEG-2-encoded chunks, an H.264/MPEG-4 video clip is assembled from H.264/MPEG-4-encoded chunks, etc.).
- a transcoded video clip for each encoding is generated by assembling the corresponding transcoded chunks (e.g., an MPEG-2 video clip is assembled from MPEG-2-encoded chunks, an H.264/MPEG-4 video clip is assembled from H.264/MPEG-4-encoded chunks, etc.).
- an MPEG-2 video clip is assembled from MPEG-2-encoded chunks
- an H.264/MPEG-4 video clip is assembled from H.264/MPEG-4-encoded chunks, etc.
- aspects and implementations of the present disclosure are thus capable of improving the quality of video clips that are transcoded via parallel and distributed processing.
- the transcoded video clips possess fewer noticeable artifacts when compared to na ' ive, fixed-size chunking strategies due to a reduction in intra-scene chunk boundaries, intelligent splitting of long scenes (for example, by minimizing the rate of change of brightness, motion, etc. at boundaries falling within such scenes), and an overall reduction in the number of I-frames in the transcoded video clip. Consequently, aspects and
- implementations of the present disclosure provide the speed advantage of transcoding video clips via distributed and parallel processing, while mitigating the reduction in quality incurred by such processing.
- transcoding video clips can be adapted to transcoding other types of media items (e.g., audio clips, images, etc.).
- media items e.g., audio clips, images, etc.
- an analog of a scene change in a video clip might be a silent time interval in an audio clip.
- FIG. 2 illustrates an example system architecture 200, in accordance with one implementation of the present disclosure.
- the system architecture 200 includes a server machine 215, a media store 220, a web page store 230, client machines 202-1 through 202-M, and transcode servers 260-1 through 260-N connected to a network 204, where M and N are positive integers.
- Network 204 may be a public network (e.g., the Internet), a private network (e.g., a local area network (LAN) or wide area network (WAN)), or a combination thereof.
- LAN local area network
- WAN wide area network
- the client machines 202-1 through 202-M may be personal computers (PCs), laptops, mobile phones, tablet computers, set top boxes, televisions, video game consoles, digital assistants or any other computing devices.
- the client machines 202-1 through 202-M may run an operating system (not shown) that manages hardware and software of the client machines 202-1 through 202-M.
- a browser (not shown) may execute on some client machines (e.g., on the OS of the client machines).
- the browser may be a web browser that can access content served by a content server 240 of server machine 215 by navigating to web pages of the content server 240 (e.g., using the hypertext transport protocol (HTTP)).
- the browser may issue commands and queries to the content server 240, such as commands to upload media items (e.g., video clips, audio clips, images, etc.), search for media items, share media items, and so forth.
- media items e.g., video clips, audio clips, images, etc.
- One or more of client machines 202-1 through 202-M may include applications that are associated with a service provided by content server 240.
- Examples of client machines that may use such applications (“apps") include mobile phones, "smart" televisions, tablet computers, and so forth.
- the applications or apps may access content provided by content server 240, issue commands to content server 240, and so forth without visiting web pages of content server 240.
- functions described in one embodiment as being performed by the content server 240 can also be performed on the client machines 202-1 through 202-M in other embodiments if appropriate.
- functionality attributed to a particular component can be performed by different or multiple components operating together.
- the content server 240 can also be accessed as a service provided to other systems or devices through appropriate application programming interfaces, and thus is not limited to use in websites.
- Server machine 215 may be a rackmount server, a router computer, a personal computer, a portable digital assistant, a mobile phone, a laptop computer, a tablet computer, a camera, a video camera, a netbook, a desktop computer, a media center, or any combination of the above.
- Server machine 215 includes a content server 240 and a transcoding manager 250. In alternative implementations, the content server 240 and transcoding manager 250 may run on different machines.
- Media store 220 is a persistent storage that is capable of storing media items (e.g., video clips, audio clips, images, etc.) as well as data structures to tag, organize, and index the media items.
- Media store 220 may be hosted by one or more storage devices, such as main memory, magnetic or optical storage based disks, tapes or hard drives, NAS, SAN, and so forth.
- media store 220 may be a network-attached file server, while in other embodiments media store 220 may be some other type of persistent storage such as an object-oriented database, a relational database, and so forth, that may be hosted by the server machine 215 or one or more different machines coupled to the server machine 215 via the network 204.
- the media items stored in the media store 220 may include user- generated media items that are uploaded by client machines, as well as media items from service providers such as news organizations, publishers, libraries and so forth.
- media store 220 may be provided by a third-party service, while in some other implementations media store 220 may be maintained by the same entity maintaining server machine 215.
- Web page store 230 is a persistent storage that is capable of storing web pages and/or mobile app documents for serving to clients, as well as data structures to tag, organize, and index the web pages and/or mobile app documents (e.g., documents provided to mobile apps for rendering on mobile devices).
- Web page store 230 may be hosted by one or more storage devices, such as main memory, magnetic or optical storage based disks, tapes or hard drives, NAS, SAN, and so forth.
- web page store 230 may be a network-attached file server, while in other embodiments web page store 230 may be some other type of persistent storage such as an object-oriented database, a relational database, and so forth, that may be hosted by the server machine 215 or one or more different machines coupled to the server machine 215 via the network 204.
- the web pages and/or mobile app documents stored in the web page store 230 may have embedded content (e.g., media items stored in media store 220, media items stored elsewhere on the Internet, etc.) that is generated by users and uploaded by client machines, provided by news organizations, and so forth.
- transcoding manager 250 is capable of storing uploaded media items in media store 220, indexing the media items in media store 220, transcoding media items as described below with respect to Figures 3 through 5, and performing image, video and audio processing (e.g., filtering, anti-aliasing, line detection, scene change detection, feature extraction, etc.).
- image, video and audio processing e.g., filtering, anti-aliasing, line detection, scene change detection, feature extraction, etc.
- Each of transcode servers 260-1 through 260-N is a machine comprising a memory and one or more processors and is capable of receiving one or more chunks from server machine 215 via network 204, transcoding chunks into one or more encodings, and transmitting transcoded chunks back to server machine via network 204.
- transcode servers 260-1 through 260-N may be connected to server machine 215 via a network other than network 204 (e.g., a local area network, a privately-owned metropolitan area network or wide-area network, etc.).
- a network other than network 204 e.g., a local area network, a privately-owned metropolitan area network or wide-area network, etc.
- still other implementations might employ a parallel multi-processor machine in lieu of transcode servers 260-1 through 260-N, and that some such
- implementations might use the parallel multi-processor machine to perform some or all of the functions of server machine 215.
- FIG. 3 is a block diagram of one implementation of a transcoding manager.
- the transcoding manager 300 may be the same as the transcoding manager 250 of Figure 2 and may include a demuxer/muxer 302, a scene change identification engine 304, a chunk boundary decision engine 306, a s litter/assembler 308, a controller 309, and a data store 310.
- the components can be combined together or separated in further components, according to a particular implementation. It should be noted that in some implementations, various components of transcoding manager 300 may run on separate machines.
- the data store 310 may be the same as media store 220, or web page store 230, or both, or may be a different data store (e.g., a temporary buffer or a permanent data store) to hold one or more media items (e.g., to be stored in media store 220, to be embedded in web pages, to be processed, etc.), one or more chunks of media items, one or more data structures for indexing media items in media store 220, one or more web pages (e.g., to be stored in web page store 230, to be served to clients, etc.), one or more data structures for indexing web pages in web page store 230, or some combination of these data.
- Data store 310 may be hosted by one or more storage devices, such as main memory, magnetic or optical storage based disks, tapes or hard drives, and so forth.
- the demuxer/muxer 302 is capable of separating the video and audio portions of a video clip, and of combining video data and audio data into a video clip. Some operations of demuxer/muxer 302 are described in more detail below with respect to Figure 4.
- Scene change identification engine 304 is capable of identifying scene changes in a video clip (e.g., via extraction of effects such as fade in or fade out, via pixel-based differences between frames, via histogram-based differences between frames, via statistical analysis of features, etc.). Some operations of scene change identification engine 304 are described in more detail below with respect to Figure 5.
- Chunk boundary decision engine 306 is capable of determining frames of a video clip at which to divide a video clip into consecutive chunks. In one aspect, chunk boundary decision engine 306 determines the chunk boundary frames based on image content of the video clip, a minimum chunk size, and a maximum chunk size. In one implementation, the determination of chunk boundary frames is based on scene changes in the video clip, and a default chunk size in addition to the minimum and maximum chunk sizes. Some operations of chunk boundary decision engine 306 are described in more detail below with respect to Figures 4 and 5.
- Splitter/assembler 308 is capable of splitting a video clip into consecutive chunks in accordance with a set of chunk boundary frames, and of combining chunks into a video clip.
- Controller 309 is capable of providing chunks to respective transcode servers 260 for transcoding, and of receiving transcoded chunks from transcode servers 260.
- controller 309 may contain logic for assigning chunks to particular transcode servers (e.g., load balancing logic, etc.).
- Figure 4 depicts a flow diagram of aspects of a method for dividing a video clip into chunks for distributed transcoding.
- Figure 4 depicts a flow diagram of aspects of a method for distributed transcoding of video clips.
- the method is performed by processing logic that may comprise hardware (circuitry, dedicated logic, etc.), software (such as is run on a general purpose computer system or a dedicated machine), or a combination of both.
- the method is performed by the server machine 215 of Figure 2, while in some other implementations, one or more blocks of Figure 4 may be performed by another machine.
- blocks 401 and 402 are performed by content server 240.
- block 403 the video and audio portions of the video clip are separated.
- block 403 is performed by demuxer/muxer 302 of transcoding manager 250.
- the video portion of the video clip may be decoded to an intermediate "universal" format from which one or more target encodings may be obtained at blocks 406 through 408 below.
- the universal format may be uncompressed, while in some other implementations the universal format may be
- the decoding into universal format may be performed as part of block 403, while in some other aspects the decoding may instead occur at some other point of the method of Figure 4 (e.g., in a separate block not depicted in Figure 4, as part of another block, such as one of blocks 404 through 410, etc.) or at some point in the method of Figure 5, which is performed by transcode servers 260 and is described below.
- chunk boundary frames for dividing the video portion into chunks are determined based on image content of the video clip, a minimum chunk size, and a maximum chunk size.
- the video clip is split into consecutive chunks in accordance with the chunk boundary frames determined at block 404.
- block 405 is performed by splitter/assembler 308 of transcoding manager 250. It should be noted that when the video clip has been decoded into an intermediate "universal" format, the chunks may be obtained by splitting the universal-format video into universal-format chunks.
- the chunks are provided to transcode servers 260 (e.g., the first chunk provided to transcode server 260-1, the second chunk provided to transcode server 260-2, etc.) for transcoding.
- block 406 is performed by controller 309 of transcoding manager 250.
- controller 309 may contain logic for assigning chunks to particular transcode servers in an intelligent manner (e.g., load balancing logic, etc.).
- transcoded chunks are received from transcode servers 260.
- block 407 is performed by controller 309.
- the chunks are transcoded in parallel by transcode servers 260, and each transcode server provides its transcoded chunk(s) to controller 309 upon completion of transcoding.
- transcode servers 260 may transcode each chunk into a plurality of different encodings (e.g., H.264/MPEG-4, MPEG-2, etc.), either directly or via the intermediate universal format, and provide the plurality of transcoded chunks to controller 309.
- the transcode servers 260 may also be responsible for decoding chunks into universal format rather than, as described above, the entire video clip being decoded into universal format prior to being split into chunks.
- one or more transcoded videos are generated from the transcoded chunks. More particularly, when the chunks are transcoded into a single encoding, a single transcoded video may be generated from the transcoded chunks; when chunks are transcoded into a plurality of encodings (e.g., universal format, MPEG-2, H.264/MPEG-4, etc.), a first transcoded video may be generated by assembling the chunks transcoded into the first encoding, a second transcoded video may be generated by assembling the chunks transcoded into the second encoding, and so forth.
- block 408 is performed by controller 309.
- a respective video clip is generated from each transcoded video generated at block 408 and from the audio obtained at block 403.
- a single transcoded video clip is generated from the audio and the transcoded video generated at block 408, while in the case of a plurality of encodings, a first transcoded video clip is generated from the audio and a first transcoded video generated at block 408, a second transcoded video clip is generated from the audio and a second transcoded video generated at block 408, and so forth.
- block 409 is performed by demuxer/muxer 302 of transcoding manager 250.
- the one or more transcoded video clips generated at block 409 are stored in media store 220. It should be noted that when the video clip has been decoded into a universal format, this version of the video clip may also be stored in media store 220. In some implementations, the universal-format video clip may be stored in media store 220 at block 410, while in some other implementations the universal-format video clip may be stored in media store 220 at an earlier point of the method (e.g., immediately following decoding into universal format at block 403 above, etc.). In accordance with one aspect, block 410 is performed by controller 309.
- the video clips to be transcoded are uploaded by users, in some other implementations the video clips to be transcoded may be obtained in some other fashion, or may already be stored in media store 220 (e.g., a video library provided by a media company, etc.). It should further be noted that while in the flow diagram of Figure 4 each uploaded video clip is transcoded when it is received by server machine 215, in some other implementations transcoding of uploaded video clips might instead occur at a later time (e.g., a batch job run nightly, etc.).
- Figure 5 depicts a flow diagram of aspects of a method for determining boundary frames at which to divide video into chunks.
- the method is performed by processing logic that may comprise hardware (circuitry, dedicated logic, etc.), software (such as is run on a general purpose computer system or a dedicated machine), or a combination of both.
- the method is performed by the server machine 215 of Figure 2, while in some other implementations, one or more blocks of Figure 5 may be performed by another machine.
- block 501 is performed by controller 309. [0052] At block 501, one or more scene changes in the video are identified.
- scene change identification may comprise extraction of effects such as fade in or fade out
- scene change identification may comprise computing differences in pixel values between successive frames and comparing a function of the differences (e.g., the sum of the differences over all pixels, etc.) to a threshold
- scene change identification may comprise constructing histograms of pixel values in frames, computing differences between histograms for successive frames, and comparing a function of the differences (e.g., the sum of the differences between corresponding histogram bins, etc.) to a threshold
- scene change identification may comprise a statistical analysis of features extracting from frames, while in still other implementations scene changes may be identified in some other fashion.
- block 501 is performed by scene change identification engine 304 of transcoding manager 250.
- variable S is initialized to an empty set, and at block 503, variable chunkStart is initialized to zero.
- variable chunkEnd is set to the sum of chunkStart and the default chunk size, defaultChunkSize.
- the default chunk size may be between the minimum chunk size and the maximum chunk size, inclusive (i.e., greater than or equal to the minimum chunk size and less than or equal to the maximum chunk size).
- variable p is set to the index of the frame of the first scene change preceding chunkEnd
- variable q is set to the index of the frame of the first scene change following chunkEnd.
- Block 506 compares (q - chunkStart) to the maximum chunk size, maxChunkSize; if (q - chunkStart) is less than or equal to maxChunkSize, then execution proceeds to block 507, otherwise execution continues at block 508.
- variable chunkEnd is set to the value of variable q. After block 507 is performed, execution continues at block 510.
- Block 508 compares (p - chunkStart) to the minimum chunk size, minChunkSize; if (p - chunkStart) is greater than or equal to minChunkSize, then execution proceeds to block 509, otherwise execution continues at block 510.
- variable chunkEnd is set to the value of variable p.
- value of chunkEnd which corresponds to a chunk boundary frame, is added to set s.
- Block 511 branches based on whether variable chunkEnd equals the index of the final frame of video; if not, execution continues at block 512, otherwise execution proceeds to block 513.
- the value of variable chunkStart is set to chunkEnd + 1, and after block 512 is performed, execution continues back at block 504.
- set S which contains the indices of chunk boundary frames, is returned.
- chunk boundary frames are defined as the last frame of a chunk
- the chunk boundary frames may instead be defined as the first frame of a chunk, with appropriate changes made to the method of Figure 5.
- the determination of chunk boundary frames may be based on minimum and maximum chunk sizes, but not based on a default chunk size in addition to the minimum and maximum sizes.
- the implementation of Figure 5 may be modified to handle cases when a scene exceeds the maximum chunk size.
- the splitting of a scene at a chunk boundary may be based on image content; for example, the chunk boundary may be determined based on a measure of brightness of individual frames of the scene (e.g., splitting the scene at a frame at which a measure of brightness has a minimum rate of change, etc.), or based on a measure of motion across frames of the scene (e.g., splitting the scene at a frame at which a measure of motion has a minimum rate of change, etc.), or both, while in yet other embodiments the chunk boundary of a scene exceeding the maximum size may be determined based on some other information obtained from pixel values of frames in the scene.
- an analog of frames in an audio clip might be pulse code modulated (PCM) sound samples
- an analog of a scene change in video might be a silent time interval in an audio clip.
- PCM pulse code modulated
- Figure 6 illustrates an exemplary computer system within which a set of instructions, for causing the machine to perform any one or more of the methodologies discussed herein, may be executed.
- the machine may be connected (e.g., networked) to other machines in a LAN, an intranet, an extranet, or the Internet.
- the machine may operate in the capacity of a server machine in client-server network environment.
- the machine may be a personal computer (PC), a set-top box (STB), a server, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine.
- machine shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein.
- the exemplary computer system 600 includes a processing system (processor) 602, a main memory 604 (e.g., read-only memory (ROM), flash memory, dynamic random access memory (DRAM) such as synchronous DRAM (SDRAM)), a static memory 606 (e.g., flash memory, static random access memory (SRAM)), and a data storage device 616, which communicate with each other via a bus 608.
- processor processing system
- main memory 604 e.g., read-only memory (ROM), flash memory, dynamic random access memory (DRAM) such as synchronous DRAM (SDRAM)
- DRAM dynamic random access memory
- SDRAM synchronous DRAM
- static memory 606 e.g., flash memory, static random access memory (SRAM)
- SRAM static random access memory
- Processor 602 represents one or more general -purpose processing devices such as a microprocessor, central processing unit, or the like. More particularly, the processor 602 may be a complex instruction set computing (CISC) microprocessor, reduced instruction set computing (RISC) microprocessor, very long instruction word (VLIW) microprocessor, or a processor implementing other instruction sets or processors implementing a combination of instruction sets. The processor 602 may also be one or more special-purpose processing devices such as an application specific integrated circuit (ASIC), a field programmable gate array (FPGA), a digital signal processor (DSP), network processor, or the like. The processor 602 is configured to execute instructions 626 for performing the operations and steps discussed herein.
- ASIC application specific integrated circuit
- FPGA field programmable gate array
- DSP digital signal processor
- the computer system 600 may further include a network interface device 622.
- the computer system 600 also may include a video display unit 610 (e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)), an alphanumeric input device 612 (e.g., a keyboard), a cursor control device 614 (e.g., a mouse), and a signal generation device 620 (e.g., a speaker).
- a video display unit 610 e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)
- an alphanumeric input device 612 e.g., a keyboard
- a cursor control device 614 e.g., a mouse
- a signal generation device 620 e.g., a speaker
- the data storage device 616 may include a computer-readable medium 624 on which is stored one or more sets of instructions 626 (e.g., instructions executed by
- transcoding manager 225 embodying any one or more of the methodologies or functions described herein.
- Instructions 626 may also reside, completely or at least partially, within the main memory 604 and/or within the processor 602 during execution thereof by the computer system 600, the main memory 604 and the processor 602 also constituting computer-readable media. Instructions 626 may further be transmitted or received over a network via the network interface device 622.
- computer-readable storage medium 624 is shown in an exemplary embodiment to be a single medium, the term “computer-readable storage medium” should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions.
- the term “computer-readable storage medium” shall also be taken to include any medium that is capable of storing, encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present disclosure.
- the term “computer-readable storage medium” shall accordingly be taken to include, but not be limited to, solid-state memories, optical media, and magnetic media.
- aspects and implementations of the disclosure also relate to an apparatus for performing the operations herein.
- This apparatus may be specially constructed for the required purposes, or it may comprise a general purpose computer selectively activated or reconfigured by a computer program stored in the computer.
- a computer program may be stored in a computer readable storage medium, such as, but not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, and magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, or any type of media suitable for storing electronic instructions.
Abstract
Description
Claims
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/144,331 US20150189222A1 (en) | 2013-12-30 | 2013-12-30 | Content-adaptive chunking for distributed transcoding |
PCT/US2014/072724 WO2015103247A1 (en) | 2013-12-30 | 2014-12-30 | Content-adaptive chunking for distributed transcoding |
Publications (1)
Publication Number | Publication Date |
---|---|
EP3090569A1 true EP3090569A1 (en) | 2016-11-09 |
Family
ID=52345597
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP14825587.0A Withdrawn EP3090569A1 (en) | 2013-12-30 | 2014-12-30 | Content-adaptive chunking for distributed transcoding |
Country Status (8)
Country | Link |
---|---|
US (1) | US20150189222A1 (en) |
EP (1) | EP3090569A1 (en) |
JP (1) | JP6250822B2 (en) |
KR (2) | KR20160104035A (en) |
CN (1) | CN105874813A (en) |
AU (1) | AU2014373838B2 (en) |
CA (1) | CA2935260A1 (en) |
WO (1) | WO2015103247A1 (en) |
Families Citing this family (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105338371B (en) * | 2015-10-29 | 2019-05-17 | 四川奇迹云科技有限公司 | A kind of multi-media transcoding dispatching method and device |
EP3188016A1 (en) * | 2015-12-29 | 2017-07-05 | Harmonic Inc. | Scheduler of computer processes for optimized offline video processing |
CN105704124A (en) * | 2016-01-15 | 2016-06-22 | 北京天马网视科技有限公司 | Video value regenerating system and method based on Internet |
CN109074655B (en) * | 2016-04-22 | 2022-07-29 | 松下知识产权经营株式会社 | Motion video segmentation method, motion video segmentation device and motion video processing system |
JP6956324B2 (en) * | 2016-04-28 | 2021-11-02 | パナソニックＩｐマネジメント株式会社 | Video splitting method, video splitting device and video processing system |
US10873781B2 (en) * | 2017-06-13 | 2020-12-22 | Comcast Cable Communications, Llc | Video fragment file processing |
US11366790B2 (en) | 2017-10-30 | 2022-06-21 | AtomBeam Technologies Inc. | System and method for random-access manipulation of compacted data files |
US11232076B2 (en) | 2017-10-30 | 2022-01-25 | AtomBeam Technologies, Inc | System and methods for bandwidth-efficient cryptographic data transfer |
US10509771B2 (en) * | 2017-10-30 | 2019-12-17 | AtomBeam Technologies Inc. | System and method for data storage, transfer, synchronization, and security using recursive encoding |
US10798393B2 (en) * | 2018-07-09 | 2020-10-06 | Hulu, LLC | Two pass chunk parallel transcoding process |
WO2020264522A1 (en) * | 2019-06-27 | 2020-12-30 | Atombeam Technologies, Inc. | Data storage, transfer, synchronization, and security using recursive encoding |
US20230171418A1 (en) * | 2021-11-30 | 2023-06-01 | Comcast Cable Communications, Llc | Method and apparatus for content-driven transcoder coordination |
US20230266902A1 (en) * | 2022-02-23 | 2023-08-24 | Samsung Electronics Co., Ltd. | Video stream encoding for computational storage device |
CN115942070B (en) * | 2022-12-26 | 2023-09-12 | 北京柏睿数据技术股份有限公司 | Dynamic optimization method and system for transcoding video data file |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110305273A1 (en) * | 2010-06-11 | 2011-12-15 | Microsoft Corporation | Parallel multiple bitrate video encoding |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3969649B2 (en) * | 2002-11-06 | 2007-09-05 | 株式会社エヌ・ティ・ティ・データ | Video data processing system |
US20050111835A1 (en) * | 2003-11-26 | 2005-05-26 | Friel Joseph T. | Digital video recorder with background transcoder |
US8879857B2 (en) * | 2005-09-27 | 2014-11-04 | Qualcomm Incorporated | Redundant data encoding methods and device |
US8654848B2 (en) * | 2005-10-17 | 2014-02-18 | Qualcomm Incorporated | Method and apparatus for shot detection in video streaming |
CN102037730B (en) * | 2008-05-22 | 2013-06-12 | 爱立信电话股份有限公司 | Content adaptive video encoder and coding method |
US8270473B2 (en) * | 2009-06-12 | 2012-09-18 | Microsoft Corporation | Motion based dynamic resolution multiple bit rate video encoding |
CN102163201A (en) * | 2010-02-24 | 2011-08-24 | 腾讯科技（深圳）有限公司 | Multimedia file segmentation method, device thereof and code converter |
US20130104177A1 (en) * | 2011-10-19 | 2013-04-25 | Google Inc. | Distributed real-time video processing |
US9432704B2 (en) * | 2011-11-06 | 2016-08-30 | Akamai Technologies Inc. | Segmented parallel encoding with frame-aware, variable-size chunking |
US9071842B2 (en) * | 2012-04-19 | 2015-06-30 | Vixs Systems Inc. | Detection of video feature based on variance metric |
US9191725B2 (en) * | 2013-03-15 | 2015-11-17 | Arris Technology, Inc. | Method and apparatus for streaming video |
-
2013
- 2013-12-30 US US14/144,331 patent/US20150189222A1/en not_active Abandoned
-
2014
- 2014-12-30 CN CN201480071787.9A patent/CN105874813A/en active Pending
- 2014-12-30 EP EP14825587.0A patent/EP3090569A1/en not_active Withdrawn
- 2014-12-30 KR KR1020167020591A patent/KR20160104035A/en active IP Right Grant
- 2014-12-30 AU AU2014373838A patent/AU2014373838B2/en active Active
- 2014-12-30 WO PCT/US2014/072724 patent/WO2015103247A1/en active Application Filing
- 2014-12-30 CA CA2935260A patent/CA2935260A1/en not_active Abandoned
- 2014-12-30 KR KR1020187006647A patent/KR20180029100A/en not_active Application Discontinuation
- 2014-12-30 JP JP2016543661A patent/JP6250822B2/en active Active
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110305273A1 (en) * | 2010-06-11 | 2011-12-15 | Microsoft Corporation | Parallel multiple bitrate video encoding |
Also Published As
Publication number | Publication date |
---|---|
AU2014373838B2 (en) | 2018-01-18 |
CN105874813A (en) | 2016-08-17 |
CA2935260A1 (en) | 2015-07-09 |
WO2015103247A1 (en) | 2015-07-09 |
JP2017507533A (en) | 2017-03-16 |
JP6250822B2 (en) | 2017-12-20 |
KR20160104035A (en) | 2016-09-02 |
KR20180029100A (en) | 2018-03-19 |
AU2014373838A1 (en) | 2016-06-16 |
US20150189222A1 (en) | 2015-07-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20150189222A1 (en) | Content-adaptive chunking for distributed transcoding | |
TWI692245B (en) | Video decoding apparatus, video encoding method and apparatus, and computer-readable storage medium | |
US8891939B2 (en) | Systems and methods for video-aware screen capture and compression | |
KR102316968B1 (en) | Complexity Adaptive Single-Pass to 2-Pass Transcoding | |
EP2815574B1 (en) | Metadata assisted video decoding | |
US9609338B2 (en) | Layered video encoding and decoding | |
US20150156557A1 (en) | Display apparatus, method of displaying image thereof, and computer-readable recording medium | |
US10013614B2 (en) | Using an image matching system to improve the quality of service of a video matching system | |
WO2019164753A1 (en) | Efficient streaming video for static video content | |
US11356739B2 (en) | Video playback method, terminal apparatus, and storage medium | |
CN103313090A (en) | Method and system for off-line downloading video files | |
US8620096B2 (en) | Virtualization server for presentation virtualization and image data encoding method | |
US20130039429A1 (en) | Computer display content coding method and system | |
CN106664439B (en) | Cloud streaming server | |
US10264273B2 (en) | Computed information for metadata extraction applied to transcoding | |
US20180122340A1 (en) | Electronic apparatus, image compression method thereof, and non-transitory computer readable recording medium | |
US20150063435A1 (en) | Techniques for reference based transcoding | |
CN111263211B (en) | Method for caching video data and terminal equipment | |
US20150149578A1 (en) | Storage device and method of distributed processing of multimedia data | |
US20160064039A1 (en) | Thumbnail Generation | |
KR102247887B1 (en) | System for cloud streaming service, method of cloud streaming service using source information and apparatus for the same | |
US11190774B1 (en) | Screen content encoding mode evaluation including intra-block evaluation of multiple potential encoding modes |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20160526 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAX | Request for extension of the european patent (deleted) | ||
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20170808 |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE APPLICATION HAS BEEN WITHDRAWN |
|
18W | Application withdrawn |
Effective date: 20180829 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230519 |