US20150262424A1 - Depth and Focus Discrimination for a Head-mountable device using a Light-Field Display System - Google Patents
Depth and Focus Discrimination for a Head-mountable device using a Light-Field Display System Download PDFInfo
- Publication number
- US20150262424A1 US20150262424A1 US13/755,392 US201313755392A US2015262424A1 US 20150262424 A1 US20150262424 A1 US 20150262424A1 US 201313755392 A US201313755392 A US 201313755392A US 2015262424 A1 US2015262424 A1 US 2015262424A1
- Authority
- US
- United States
- Prior art keywords
- light
- hmd
- lightfield
- interest
- feature
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
- 230000003287 optical effect Effects 0.000 claims abstract description 21
- 238000000034 method Methods 0.000 claims description 54
- 201000009310 astigmatism Diseases 0.000 claims description 28
- 238000009877 rendering Methods 0.000 claims description 17
- 238000000926 separation method Methods 0.000 claims description 6
- 239000004973 liquid crystal related substance Substances 0.000 claims description 5
- XUIMIQQOPSSXEZ-UHFFFAOYSA-N Silicon Chemical compound [Si] XUIMIQQOPSSXEZ-UHFFFAOYSA-N 0.000 claims description 3
- 229910052710 silicon Inorganic materials 0.000 claims description 3
- 239000010703 silicon Substances 0.000 claims description 3
- 230000003190 augmentative effect Effects 0.000 abstract description 12
- 238000004891 communication Methods 0.000 description 24
- 230000006870 function Effects 0.000 description 13
- 238000010586 diagram Methods 0.000 description 10
- 238000005516 engineering process Methods 0.000 description 9
- 230000033001 locomotion Effects 0.000 description 8
- 210000000988 bone and bone Anatomy 0.000 description 6
- 238000004590 computer program Methods 0.000 description 6
- 230000008569 process Effects 0.000 description 6
- 210000003128 head Anatomy 0.000 description 5
- 239000000463 material Substances 0.000 description 5
- 239000000700 radioactive tracer Substances 0.000 description 5
- 230000004886 head movement Effects 0.000 description 4
- 230000000007 visual effect Effects 0.000 description 4
- 230000004424 eye movement Effects 0.000 description 3
- 239000011159 matrix material Substances 0.000 description 3
- 230000001413 cellular effect Effects 0.000 description 2
- 239000011248 coating agent Substances 0.000 description 2
- 238000000576 coating method Methods 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 230000007547 defect Effects 0.000 description 2
- 210000005069 ears Anatomy 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 230000004438 eyesight Effects 0.000 description 2
- 238000004519 manufacturing process Methods 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 230000005236 sound signal Effects 0.000 description 2
- 230000003466 anti-cipated effect Effects 0.000 description 1
- 208000003464 asthenopia Diseases 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000013499 data model Methods 0.000 description 1
- 230000001627 detrimental effect Effects 0.000 description 1
- 210000000959 ear middle Anatomy 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000003384 imaging method Methods 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 230000008447 perception Effects 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 230000005043 peripheral vision Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 210000001747 pupil Anatomy 0.000 description 1
- 210000001525 retina Anatomy 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000010897 surface acoustic wave method Methods 0.000 description 1
- 230000007723 transport mechanism Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B27/0172—Head mounted characterised by optical features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/0075—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00 with means for altering, e.g. increasing, the depth of field or depth of focus
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/0138—Head-up displays characterised by optical features comprising image capture systems, e.g. camera
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B2027/0178—Eyeglass type
Definitions
- Computing devices such as personal computers, laptop computers, tablet computers, cellular phones, and countless types of Internet-capable devices are increasingly prevalent in numerous aspects of modern life. Over time, the manner in which these devices are providing information to users is becoming more intelligent, more efficient, more intuitive, and/or less obtrusive.
- wearable computing The trend toward miniaturization of computing hardware, peripherals, as well as of sensors, detectors, and image and audio processors, among other technologies, has helped open up a field sometimes referred to as “wearable computing.”
- wearable displays In the area of image and visual processing and production, in particular, it has become possible to consider wearable displays that place a graphic display close enough to eye(s) of a wearer (or user) such that the displayed image appears as a normal-sized image, such as might be displayed on a traditional image display device.
- the relevant technology may be referred to as “near-eye displays.”
- Wearable computing devices with near-eye displays may also be referred to as “head-mountable devices” (HMDs), “head-mounted displays,” “head-mounted devices,” or “head-mountable devices.”
- HMDs head-mountable devices
- a head-mountable device places a graphic display or displays close to one or both eyes of a wearer.
- a computer processing system may be used to generate the images on a display.
- Such displays may occupy an entire field of view of the wearer, or only occupy part of a field of view of the wearer.
- head-mounted displays may vary in size, taking a smaller form such as a glasses-style display or a larger form such as a helmet, for example.
- wearable displays include applications in which users interact in real time with an augmented or virtual reality.
- Such applications can be mission-critical or safety-critical, such as in a public safety or aviation setting.
- the applications can also be recreational, such as interactive gaming. Many other applications are also possible.
- a wearable display system such as a head-mountable device, is provided for augmenting a contemporaneously viewed “real image” of an object in a real-world environment using a light-field display system that allows for depth and focus discrimination.
- a head-mountable device in a first embodiment, includes a light-producing display engine, a viewing location element, and a microlens array.
- the microlens array is coupled to the light-producing display engine in a manner such that light emitted from the light-producing display engine is configured to follow an optical path through the microlens array to the viewing location element.
- the HMD also includes a processor.
- the processor is configured to identify a feature of interest in a field-of-view associated with the HMD in an environment.
- the feature of interest may be associated with a depth to the HMD in the environment, and the feature of interest may be visible at the viewing location element.
- the processor is also configured to obtain lightfield data.
- the lightfield data is indicative of the environment and the feature of interest.
- the processor is additionally configured to render, based on the lightfield data, a lightfield comprising a synthetic image that is related to the feature of interest at a focal point that corresponds to the depth for display at the viewing location element.
- a method in a second embodiment, includes identifying, using at least one processor of a head-mountable device (HMD), a feature of interest in a field-of-view associated with the HMD in an environment.
- the HMD comprises a light-producing display engine, a viewing location element, and a microlens array coupled to the light-producing display engine in a manner such that light emitted from the light-producing display engine is configured to follow an optical path through the microlens array to the viewing location element, and the feature of interest is associated with a depth to the HMD in the environment and visible at the viewing location element.
- the method also includes obtaining lightfield data.
- the lightfield data is indicative of the environment and the feature of interest.
- the method additionally includes rendering, based on the lightfield data, a lightfield comprising a synthetic image that is related to the feature of interest at a focal point that corresponds to the depth for display at the viewing location element.
- FIG. 1A illustrates a wearable computing system, according to an example embodiment.
- FIG. 1B illustrates an alternate view of the wearable computing system illustrated in FIG. 1A .
- FIG. 1C illustrates another wearable computing system, according to an example embodiment.
- FIG. 1D illustrates another wearable computing system, according to an example embodiment.
- FIGS. 1E , 1 F and 1 G are simplified illustrations of the wearable computing system shown in FIG. 1D , being worn by a wearer.
- FIG. 2 is a simplified block diagram of a computing device, according to an example embodiment.
- FIG. 3A is a diagram of an implementation of light-field display system that may be used by the wearable computing systems illustrated in FIGS. 1A-1D , according to an example embodiment.
- FIG. 3B is an example of an arrangement of a light-field display system that may be used by a wearer of one of the wearable computing systems depicted in FIGS. 1A-1D , according to an example embodiment.
- FIG. 4 is a block diagram of an example processor that may be used by the light-field display system of FIG. 3A , according to an example embodiment.
- FIGS. 5A-5B are block diagrams of example methods that may be carried out by a wearable computing system, using the light-field display system of FIG. 3A , according to an example embodiment.
- FIG. 6 is a functional block diagram of a computing device that may be used in conjunction with the systems and methods described herein, according to an example embodiment.
- FIG. 7 is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device according to an example embodiment.
- Example methods and systems are described herein. It should be understood that the words “example” and “exemplary” are used herein to mean “serving as an example, instance, or illustration.” Any embodiment or feature described herein as being an “example” or “exemplary” is not necessarily to be construed as preferred or advantageous over other embodiments or features.
- augmented-reality applications superimpose augmented information in the form of synthetic images at various locations that correspond with natural components of a real scene.
- the synthetic imagery is composed on a flat plane (usually the plane of a display of a device running the augmented-reality application), which overlays a view of the real scene.
- the focal plane is fixed, the synthetic imagery may be displayed at one apparent distance and focal length from the user.
- HMDs capable of running augmented reality applications may be configured to project the synthetic images at a set focal length, which may not be desirable.
- the highlight indicator synthetic imagery
- the object to be highlighted is at another focal distance.
- the synthetic image may appear out of focus and blurry. This may lead to eyestrain of the user, and the inconsistency with the natural components may harm the verisimilitude of the augmented-reality experience, making it easy for the user to tell the difference between reality and virtual reality.
- HMDs who have asymmetric or astigmatic vision may also find it difficult to use HMDs in an augmented reality manner because the synthetic imagery and natural objects may seem out of focus or blurry due to the asymmetric or astigmatic vision, regardless of the problems mentioned above.
- an HMD may be configured to run an augmented reality application and sense an environment with various natural components.
- the HMD may be configured to render light-fields and/or stereoscopic imaging of the environment in a manner that may allow any augmented information in the form of synthetic images to appear at various depths or focal distances that correspond with the depth of the natural components in the environment, and may compensate for any visual defects including those resulting from an astigmatism.
- a HMD that includes a light-field display system.
- the light-field display system may be configured in a manner that ensures light emitted from a display engine follows an optical path through a microlens array before being viewed by a wearer of the HMD.
- This configuration allows lightfield data (a light filed is a function describing the amount of light moving in every direction through every point in space) to be produced that represents an environment of the wearer of the HMD.
- the HMD may render, into the eye of the wearer, a light field that includes information about the environment of the HMD in an augmented reality manner at distances and focal points that correspond to the actual distances and focal points of objects in the environment, and may simultaneously compensate for any visual defects.
- the HMD may focus on feature points (natural components) in the environment.
- feature points may include a computer and a scanner, for example.
- the HMD may use light-field technology to acquire images of the office as well as information indicating where the scanner and computer are in the office, for example.
- the HMD may place information about the scanner and/or computer on the HMD in an augmented reality manner at distances and focal points that correspond to the actual distances and focal points of the scanner and computer.
- an example system may be implemented in or may take the form of a wearable computer (also referred to as a wearable computing device).
- a wearable computer takes the form of or includes a head-mountable device (HMD).
- HMD head-mountable device
- An example system may also be implemented in or take the form of other devices, such as a mobile phone, among other possibilities. Further, an example system may take the form of non-transitory computer readable medium, which has program instructions stored thereon that are executable by at a processor to provide the functionality described herein. An example system may also take the form of a device such as a wearable computer or mobile phone, or a subsystem of such a device, which includes such a non-transitory computer readable medium having such program instructions stored thereon.
- An HMD may generally be any display device that is capable of being worn on the head and places a display in front of one or both eyes of the wearer.
- An HMD may take various forms such as a helmet or eyeglasses.
- references to “eyeglasses” or a “glasses-style” HMD should be understood to refer to an HMD that has a glasses-like frame so that it can be worn on the head.
- example embodiments may be implemented by or in association with an HMD with a single display or with two displays, which may be referred to as a “monocular” HMD or a “binocular” HMD, respectively.
- FIG. 1A illustrates a wearable computing system according to an example embodiment.
- the wearable computing system takes the form of a head-mountable device (HMD) 102 (which may also be referred to as a head-mounted display).
- HMD head-mountable device
- the HMD 102 includes frame elements including lens-frames 104 , 106 and a center frame support 108 , lens elements 110 , 112 , a light-field display system 136 , and extending side-arms 114 , 116 .
- the center frame support 108 and the extending side-arms 114 , 116 are configured to secure the HMD 102 to a face of a user via a nose and ears of the user, respectively.
- Each of the frame elements 104 , 106 , and 108 and the extending side-arms 114 , 116 may be formed of a solid structure of plastic and/or metal, or may be formed of a hollow structure of similar material so as to allow wiring and component interconnects to be internally routed through the HMD 102 . Other materials may be possible as well.
- each of the lens elements 110 , 112 may be formed of any material that can suitably display a projected image or graphic.
- Each of the lens elements 110 , 112 may also be sufficiently transparent to allow a user to see through the lens element. Combining these two features of the lens elements may facilitate an augmented reality or heads-up display where the projected image or graphic is superimposed over a real-world view as perceived by the user through the lens elements.
- the extending side-arms 114 , 116 may each be projections that extend away from the lens-frames 104 , 106 , respectively, and may be positioned behind ears of a user to secure the HMD 102 to the user.
- the extending side-arms 114 , 116 may further secure the HMD 102 to the user by extending around a rear portion of the head of the user. Additionally or alternatively, for example, the HMD 102 may connect to or be affixed within a head-mounted helmet structure. Other configurations for an HMD are also possible.
- the HMD 102 may also include an on-board computing system 118 , an image capture device 120 , a sensor 122 , and a finger-operable touch pad 124 .
- the on-board computing system 118 is shown to be positioned on the extending side-arm 114 of the HMD 102 ; however, the on-board computing system 118 may be provided on other parts of the HMD 102 or may be positioned remote from the HMD 102 (e.g., the on-board computing system 118 could be wire- or wirelessly-connected to the HMD 102 ).
- the on-board computing system 118 may include a processor and memory, for example.
- the on-board computing system 118 may be configured to receive and analyze data from the image capture device 120 and the finger-operable touch pad 124 (and possibly from other sensory devices, user interfaces, or both) and generate images for output by the lens elements 110 and 112 .
- the image capture device 120 may be, for example, a camera that is configured to capture still images and/or to capture video. In the illustrated configuration, image capture device 120 is positioned on the extending side-arm 114 of the HMD 102 ; however, the image capture device 120 may be provided on other parts of the HMD 102 .
- the image capture device 120 may be configured to capture images at various resolutions or at different frame rates. Many image capture devices with a small form-factor, such as the cameras used in mobile phones or webcams, for example, may be incorporated into an example of the HMD 102 .
- FIG. 1A illustrates one image capture device 120
- more image capture device may be used, and each may be configured to capture the same view, or to capture different views.
- the image capture device 120 may be forward facing to capture at least a portion of the real-world view perceived by the user. This forward facing image captured by the image capture device 120 may then be used to generate an augmented reality where computer generated images appear to interact with or overlay the real-world view perceived by the user.
- the sensor 122 is shown on the extending side-arm 116 of the HMD 102 ; however, the sensor 122 may be positioned on other parts of the HMD 102 .
- the HMD 102 may include multiple sensors.
- an HMD 102 may include sensors 102 such as one or more gyroscopes, one or more accelerometers, one or more magnetometers, one or more light sensors, one or more infrared sensors, and/or one or more microphones.
- Other sensing devices may be included in addition or in the alternative to the sensors that are specifically identified herein.
- the finger-operable touch pad 124 is shown on the extending side-arm 114 of the HMD 102 . However, the finger-operable touch pad 124 may be positioned on other parts of the HMD 102 . Also, more than one finger-operable touch pad may be present on the HMD 102 .
- the finger-operable touch pad 124 may be used by a user to input commands.
- the finger-operable touch pad 124 may sense at least one of a pressure, position and/or a movement of one or more fingers via capacitive sensing, resistance sensing, or a surface acoustic wave process, among other possibilities.
- the finger-operable touch pad 124 may be capable of sensing movement of one or more fingers simultaneously, in addition to sensing movement in a direction parallel or planar to the pad surface, in a direction normal to the pad surface, or both, and may also be capable of sensing a level of pressure applied to the touch pad surface.
- the finger-operable touch pad 124 may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger-operable touch pad 124 may be formed to have a raised, indented, or roughened surface, so as to provide tactile feedback to a user when a finger of the user reaches the edge, or other area, of the finger-operable touch pad 124 . If more than one finger-operable touch pad is present, each finger-operable touch pad may be operated independently, and may provide a different function.
- HMD 102 may be configured to receive user input in various ways, in addition or in the alternative to user input received via finger-operable touch pad 124 .
- on-board computing system 118 may implement a speech-to-text process and utilize a syntax that maps certain spoken commands to certain actions.
- HMD 102 may include one or more microphones via which speech of a wearer may be captured. Configured as such, HMD 102 may be operable to detect spoken commands and carry out various computing functions that correspond to the spoken commands.
- HMD 102 may interpret certain head-movements as user input. For example, when HMD 102 is worn, HMD 102 may use one or more gyroscopes and/or one or more accelerometers to detect head movement. The HMD 102 may then interpret certain head-movements as being user input, such as nodding, or looking up, down, left, or right. An HMD 102 could also pan or scroll through graphics in a display according to movement. Other types of actions may also be mapped to head movement.
- HMD 102 may interpret certain gestures (e.g., by a hand or hands of the wearer) as user input. For example, HMD 102 may capture hand movements by analyzing image data from image capture device 120 , and initiate actions that are defined as corresponding to certain hand movements.
- certain gestures e.g., by a hand or hands of the wearer
- HMD 102 may capture hand movements by analyzing image data from image capture device 120 , and initiate actions that are defined as corresponding to certain hand movements.
- HMD 102 may interpret eye movement as user input.
- HMD 102 may include one or more inward-facing image capture devices and/or one or more other inward-facing sensors (not shown) that may be used to track eye movements and/or determine the direction of a gaze of a wearer.
- certain eye movements may be mapped to certain actions.
- certain actions may be defined as corresponding to movement of the eye in a certain direction, a blink, and/or a wink, among other possibilities.
- HMD 102 also includes a speaker 125 for generating audio output.
- the speaker could be in the form of a bone conduction speaker, also referred to as a bone conduction transducer (BCT).
- Speaker 125 may be, for example, a vibration transducer or an electroacoustic transducer that produces sound in response to an electrical audio signal input.
- the frame of HMD 102 may be designed such that when a user wears HMD 102 , the speaker 125 contacts the wearer.
- speaker 125 may be embedded within the frame of HMD 102 and positioned such that, when the HMD 102 is worn, speaker 125 vibrates a portion of the frame that contacts the wearer.
- HMD 102 may be configured to send an audio signal to speaker 125 , so that vibration of the speaker may be directly or indirectly transferred to the bone structure of the wearer.
- the wearer can interpret the vibrations provided by BCT 125 as sounds.
- bone-conduction transducers may be implemented, depending upon the particular implementation.
- any component that is arranged to vibrate the HMD 102 may be incorporated as a vibration transducer.
- an HMD 102 may include a single speaker 125 or multiple speakers.
- the location(s) of speaker(s) on the HMD may vary, depending upon the implementation. For example, a speaker may be located proximate to a temple of a wearer (as shown), behind the ear of a wearer, proximate to the nose of the wearer, and/or at any other location where the speaker 125 can vibrate the wearer's bone structure.
- FIG. 1B illustrates an alternate view of the wearable computing device illustrated in FIG. 1A .
- the lens elements 110 , 112 may act as display elements.
- the HMD 102 may include a first projector 128 coupled to an inside surface of the extending side-arm 116 and configured to project a display 130 onto an inside surface of the lens element 112 .
- a second projector 132 may be coupled to an inside surface of the extending side-arm 114 and configured to project a display 134 onto an inside surface of the lens element 110 .
- the lens elements 110 , 112 may act as a combiner in a light projection system and may include a coating that reflects the light projected onto them from the projectors 128 , 132 . In some embodiments, a reflective coating may not be used (e.g., when the projectors 128 , 132 are scanning laser devices).
- the lens elements 110 , 112 themselves may include: a transparent or semi-transparent matrix display, such as an electroluminescent display or a liquid crystal display, one or more waveguides for delivering an image to the eyes of a user, or other optical elements capable of delivering an in focus near-to-eye image to the user.
- a corresponding display driver may be disposed within the frame elements 104 , 106 for driving such a matrix display.
- a laser or LED source and scanning system could be used to draw a raster display directly onto the retina of one or more of the eyes of the user. Other possibilities exist as well.
- the lens elements 110 , 112 may include a light-field display system 136 .
- the light-field display system 136 may be affixed to the lens elements 110 , 112 in a manner that allows the light-field display system 136 to be undetectable to a wearer of the HMD (i.e., the view of the real world of the wearer is unobstructed by the light-field display system).
- the light-field display system 136 may include optical elements that are configured to generate a lightfield and/or lightfield data including a display engine, a microlens array, and a viewing location element.
- the display engine may incorporate any of the display elements discussed above (e.g., projectors 128 , 132 ).
- the display system may be separate and include other optical elements.
- the viewing location element may be lens elements 110 , 112 , for example.
- Other elements may be included in light-field display system 136 , and light-field display system 136 may be arranged in other ways.
- the light-field display system 136 may be affixed to lens frames 104 , 106 and may have separation from lens elements 110 , 112 .
- light-field display system 136 may be affixed to center frame support 108 .
- FIG. 1C illustrates another wearable computing system according to an example embodiment, which takes the form of an HMD 152 .
- the HMD 152 may include frame elements and side-arms such as those described with respect to FIGS. 1A and 1B .
- the HMD 152 may additionally include an on-board computing system 154 and an image capture device 156 , such as those described with respect to FIGS. 1A and 1B .
- the image capture device 156 is shown mounted on a frame of the HMD 152 . However, the image capture device 156 may be mounted at other positions as well.
- the HMD 152 may include a single display 158 which may be coupled to the device.
- the display 158 may be formed on one of the lens elements of the HMD 152 , such as a lens element described with respect to FIGS. 1A and 1B , and may be configured to overlay computer-generated graphics in the view of the physical world of the user.
- the display 158 is shown to be provided in a center of a lens of the HMD 152 , however, the display 158 may be provided in other positions, such as for example towards either the upper or lower portions of the field of view of the wearer.
- the display 158 is controllable via the computing system 154 that is coupled to the display 158 via an optical waveguide 160 .
- the display 158 may comprise or be coupled to a light-field display system, although a light-field display system is not shown in FIG. 1C .
- FIG. 1D illustrates another wearable computing system according to an example embodiment, which takes the form of a monocular HMD 172 .
- the HMD 172 may include side-arms 173 , a center frame support 174 , and a bridge portion with nosepiece 175 . In the example shown in FIG. 1D , the center frame support 174 connects the side-arms 173 .
- the HMD 172 does not include lens-frames containing lens elements.
- the HMD 172 may additionally include a component housing 176 , which may include an on-board computing system (not shown), an image capture device 178 , and a button 179 for operating the image capture device 178 (and/or usable for other purposes).
- Component housing 176 may also include other electrical components and/or may be electrically connected to electrical components at other locations within or on the HMD.
- HMD 172 also includes a BCT 186 .
- the HMD 172 may include a single display 180 , which may be coupled to one of the side-arms 173 via the component housing 176 .
- the display 180 may be a see-through display, which is made of glass and/or another transparent or translucent material, such that the wearer can see their environment through the display 180 .
- the display 180 may include a light-field display system (not shown in FIG. 1D ).
- the component housing 176 may include the light sources (not shown) for the display 180 and/or optical elements (not shown) to direct light from the light sources to the display 180 .
- display 180 may include optical features that direct light that is generated by such light sources towards the eye of the wearer, when HMD 172 is being worn, such as, for example, optical features that comprise a light-field display system (not shown).
- HMD 172 may include a sliding feature 184 , which may be used to adjust the length of the side-arms 173 .
- sliding feature 184 may be used to adjust the fit of HMD 172 .
- an HMD may include other features that allow a wearer to adjust the fit of the HMD, without departing from the scope of the invention.
- FIGS. 1E to 1G are simplified illustrations of the HMD 172 shown in FIG. 1D , being worn by a wearer 190 .
- BCT 186 is arranged such that when HMD 172 is worn, BCT 186 is located behind the ear of the wearer. As such, BCT 186 is not visible from the perspective shown in FIG. 1E .
- the display 180 may be arranged such that when HMD 172 is worn, display 180 is positioned in front of or proximate to an eye of a user when the HMD 172 is worn by a user.
- display 180 may be positioned below the center frame support and above the center of the eye of the wearer, as shown in FIG. 1E .
- display 180 may be offset from the center of the eye of the wearer (e.g., so that the center of display 180 is positioned to the right and above of the center of the wearer's eye, from the perspective of the wearer).
- display 180 may be located in the periphery of the field of view of the wearer 190 , when HMD 172 is worn.
- FIG. 1F when the wearer 190 looks forward, the wearer 190 may see the display 180 with their peripheral vision.
- display 180 may be outside the central portion of the field of view of the wearer when their eye is facing forward, as it commonly is for many day-to-day activities. Such positioning can facilitate unobstructed eye-to-eye conversations with others, as well as generally providing unobstructed viewing and perception of the world within the central portion of the field of view of the wearer.
- the wearer 190 may view the display 180 by, e.g., looking up with their eyes only (possibly without moving their head). This is illustrated as shown in FIG. 1G , where the wearer has moved their eyes to look up and align their line of sight with display 180 . A wearer might also use the display by tilting their head down and aligning their eye with the display 180 .
- FIG. 2 is a simplified block diagram of a computing device 210 according to an example embodiment.
- device 210 communicates using a communication link 220 (e.g., a wired or wireless connection) to a remote device 230 .
- the device 210 may be any type of device that can receive data and display information corresponding to or associated with the data.
- the device 210 may be a heads-up display system, such as the head-mounted devices 102 , 152 , or 172 described with reference to FIGS. 1A to 1G .
- the device 210 may include a display system 212 comprising a processor 214 and a display 216 .
- the display 216 may be, for example, an optical see-through display, an optical see-around display, or a video see-through display, and may comprise components of a light-field display system.
- the processor 214 may receive data from the remote device 230 , and configure the data for display on the display 216 .
- the processor 214 may be any type of processor, such as a micro-processor or a digital signal processor, for example.
- the device 210 may further include on-board data storage, such as memory 218 coupled to the processor 214 .
- the memory 218 may store software that can be accessed and executed by the processor 214 , for example.
- the remote device 230 may be any type of computing device or transmitter including a laptop computer, a mobile telephone, or tablet computing device, etc., that is configured to transmit data to the device 210 .
- the remote device 230 and the device 210 may contain hardware to enable the communication link 220 , such as processors, transmitters, receivers, antennas, etc.
- remote device 230 may take the form of or be implemented in a computing system that is in communication with and configured to perform functions on behalf of client device, such as computing device 210 .
- client device such as computing device 210 .
- Such a remote device 230 may receive data from another computing device 210 (e.g., an HMD 102 , 152 , or 172 or a mobile phone), perform certain processing functions on behalf of the device 210 , and then send the resulting data back to device 210 .
- This functionality may be referred to as “cloud” computing.
- the communication link 220 is illustrated as a wireless connection; however, wired connections may also be used.
- the communication link 220 may be a wired serial bus such as a universal serial bus or a parallel bus.
- a wired connection may be a proprietary connection as well.
- the communication link 220 may also be a wireless connection using, e.g., Bluetooth® radio technology, communication protocols described in IEEE 802.11 (including any IEEE 802.11 revisions), Cellular technology (such as GSM, CDMA, UMTS, EV-DO, WiMAX, or LTE), or Zigbee® technology, among other possibilities.
- the remote device 230 may be accessible via the Internet and may include a computing cluster associated with a particular web service (e.g., social-networking, photo sharing, address book, etc.).
- FIG. 3A illustrates a side view of an implementation of a light-field display system 300 .
- the light-field display system 300 may include a display engine 310 , a microlens array 316 , and a viewing location element 322 .
- the light-field display system 300 may be coupled to an HMD, such as one of the HMDs discussed above in section (B), or in some examples may be considered a component of an HMD.
- the display engine 310 may include an organic light emitting diode (OLED).
- OLED organic light emitting diode
- the OLED may be a transparent or semi-transparent matrix display that allows the wearer of the HMD to view the synthetic image produced by the OLED as well as allowing the wearer of the HMD to view light and objects from the real world.
- the display engine 310 may include other light producing displays such as a liquid crystal display (LCD), a Liquid Crystal over Silicon (LCoS) display, or microelectro-mechanical systems (MEMS) projector device such as a Digital Light Processing (DLP) or PicoP projector.
- the display may incorporate or be any of the display elements discussed above with regard to FIGS. 1A-1G .
- display engine 310 is shown at a separation distance from microlens array 316 and viewing location element 322 , this is not intended to be limiting. In other arrangements display engine 310 may be contiguous to microlens array 316 , which may be contiguous to the viewing location element 322 . Other arrangements are possible as well, and the display engine 310 , microlens array 316 , and viewing location element 322 may be arranged in any suitable manner so long as the light-field display system 300 is able to accomplish the disclosed functionality.
- the display engine 310 may further include a plurality of pixels 312 that generate light (e.g., light rays 320 and 321 , which are discussed in more detail later).
- Each pixel in the plurality of pixels 312 represents a unit of the display engine, and each pixel may be activated to generate light independently.
- pixel 313 may be activated to generate light with a particular color and intensity that is different than that of pixel 314 .
- pixels 313 , 314 may be activated to generate light with the same color and intensity.
- each pixel in the plurality of pixels 312 may take any shape including round, oval, rectangular or square.
- FIG. 3A depicts a finite number of pixels (which make up the plurality of pixels) any number of pixels may be stored in the display engine, and while FIG. 3A depicts the pixels from a side view, there may be additional columns of pixels that are not visible.
- the display engine 310 may include an OLED display which may have a pixel resolution of 800 ⁇ 600 pixels. Other resolutions may be used, and may be determined based on the type of display.
- the light-field display system 300 may further include a microlens array 316 .
- the microlens array 316 may include a plurality of microlenses such as microlenses 317 , 318 . While the microlens array 316 in FIG. 3A is depicted as having five microlenses (including microlenses 317 , 318 ) in other examples, any number of microlenses may be used in the microlens array.
- the microlens array may be configured as a one-dimensional (1D) microlens array, while in other examples the microlens array may be configured as a two-dimensional (2D) microlens array.
- FIG. 3A is a side view of the display engine 310 , microlens array 316 may include additional microlenses that are not visible. For example, microlens array 316 may include additional columns of microlenses that are not visible.
- each microlens depicted in FIG. 3A takes the shape of a circle. However, in other examples, each microlens may take the form of another shape. Moreover, in FIG. 3A the microlens array 316 is arranged in a square pattern (i.e., columns and rows). However, in other embodiments the microlenses may be arranged in a variety of patterns including a hexagonal, octagonal, or other shaped grid.
- the microlens array 316 may be positioned behind the light-emitting display engine 310 and in front of viewing element 322 (e.g., between the light-emitting display engine 310 and the viewing element 322 ).
- the microlens array 316 may be configured such that one or more microlenses of the microlens array correspond to the plurality of pixels and are disposed in front of the plurality of pixels and at a separation from the plurality of pixels.
- the distance between the display engine 310 and microlens array 316 may be sufficient to allow light passing from each pixel to pass through each microlens of the microlens array 316 . For example, as illustrated in in FIG.
- light 320 from a first pixel 313 passes through microlens 317 and is visible at the viewing location element 322
- light 321 from a second pixel 314 passes through microlens 318 and is also visible at the viewing location element 322
- light 320 , 321 are shown passing through microlens 317 , 318 respectively, light 320 , 321 may also pass through the other microlenses of the microlens array 316 (not shown).
- the viewing location element 322 may be the lens elements 110 , 112 discussed above with reference to FIGS. 1A-1D , for example. In other arrangements, the viewing location element 322 may be a separate lens element, other than lens elements 110 , 112 , but may take the any form discussed with respect to lens elements 110 , 112 .
- a display engine processor may control the plurality of pixels 312 to generate light such as light 320 , 321 .
- the display engine processor may be the same or similar to processor 214 .
- the components of processor 400 described in FIG. 4 , may be part of processor 214 .
- the display engine processor may be separate to that of processor 214 .
- the display engine processor may, for example, control the color and intensity of the light displayed by each pixel.
- the display engine processor may also render particular lightfield data to be viewed at the viewing location element 322 .
- the rendered light field may be a three-dimensional (3D) or four-dimensional (4D) image or scene, for example.
- FIG. 3B illustrates an example of an arrangement of the light-field display system 300 that may be used with an HMD interacting with an eye 358 , and will be discussed in more detail later in this disclosure.
- a user (represented by eye 358 ) is operating HMD 172 , which includes light-field display system 300 .
- a user views “features of interest” wearing HMD 172 comprising a light-field display system 300 .
- Light demarcated by the bold arrows, penetrates the HMD in a manner such that lightfield data is captured that represents the environment the wearer of HMD 172 is viewing, or as depicted in FIG. 3B , features of interest 350 , 352 .
- the lightfield data may be captured, for example using a lightfield camera. In other examples, the lightfield data may be received from a remote entity, but may still represent the environment the wearer is viewing.
- light and depth data that defines the environment may be obtained in manners other than utilizing a light field camera.
- the data defining the environment may, for example, be obtained by two cameras offset to measure depth via stereopsis or using a monocular configuration that measures depth via motion parallax.
- a processor of the HMD may produce the lightfield for the wearer.
- the lightfield data may be reproduced to accurately reflect what the wearer sees (e.g., based on the gaze of the wearer of the HMD), and may be used to render a lightfield representing the environment.
- the lightfield data may be processed to incorporate synthetic images or altered to compensate for astigmatisms or irregularities in the eye of the wearer of HMD 172 or lens of HMD 172 .
- an appropriate light-field may be rendered for viewing by the user.
- the data may be used to generate lightfield data that may be used to render a lightfield representing the environment. Similar to the scenario in which lightfield data is captured using a lightfield camera, the generated lightfield data may also be processed to incorporate synthetic images or altered to compensate for astigmatism or irregularities in the eye of the wearer of HMD 172 or lens of HMD 172 . In other examples, the generated lightfield data may be processed to compensate for irregularities or detrimental qualities of any part of the optical train of the HMD 172 .
- FIG. 4 is a block diagram illustrating an implementation of a processor 400 that may be used by the light-field display system 300 .
- processor 400 may include a variety of components including a Ray Tracer 402 , Lightfield Data 404 , Pixel Renderer 406 , and a View Tracker 408 .
- the view tracker 408 may determine a certain view at the viewing location element 322 associated with the light-field display system 300 .
- the view tracker 408 may receive view/gaze information of the HMD from the sensors described above with regard to FIGS. 1A-1G .
- Other systems may be used to determine the view associated with the light-field display system.
- the ray tracer 402 may determine which pixels of the plurality of pixels 312 of the display engine 310 are visible through each microlens of the microlens array 316 within the view associated with the HMD (as determined by, for example, the view tracker 408 ).
- the ray tracer 402 may determine which pixels are visible through each individual microlens of the microlens array 312 by performing ray tracing from various points on the determined location of the eye of the wearer of the HMD through each microlens of the microlens array 316 , and determine which pixels of the plurality of pixels 312 are reached by the rays for each microlens.
- the pixels that can be reached by a ray originating from the eye (e.g., pupil) of the wearer of the HMD through a microlens of the microlens array 316 are the pixels that are visible by the eye of the wearer of the HMD at the viewing location element.
- the ray tracer 402 may determine which pixels are visible through each individual microlens of the microlens array 316 by performing ray tracing from each of the plurality of pixels through each microlens of the microlens array 316 . To do so, for each pixel of the plurality of pixels 312 (including pixels 313 and 314 ), a ray may be traced to a certain point of the eye of a wearer. The intersection of the ray with the microlens array 312 may be determined. In some examples, the ray may be traced from various locations within the pixel, and if no ray intersects the eye, then the pixel is not visible to the user.
- the pixel renderer 406 may control the output of the pixels 312 such that the appropriate light-field is displayed to a wearer of the HMD comprising the light-field display system 300 .
- the pixel renderer 406 may utilize output from the ray tracer 402 and the lightfield data that is obtained by the wearer (e.g., by viewing a real-world environment through the HMD) to determine or predict the output of the pixels 312 that will result in the lightfield data being correctly rendered to a viewer of the light-field display system 300 .
- Example methods for utilizing a HMD comprising a light-field display system 300 are discussed below.
- FIG. 5A is a block diagram of an example method for providing depth and focus discrimination using a HMD that includes a light-field display system, such as light-field display system 300 .
- Method 500 shown in FIG. 5A presents an embodiment of a method that, for example, may be performed by a device the same as or similar to that discussed with regard to FIGS. 1-3 .
- method 500 may be implemented by a user wearing HMD 172 depicted in FIG. 1D , which comprises a light-field display system (although not shown in FIG. 1D ), and will be referenced as such in discussing method 500 .
- Method 500 may include one or more operations, functions, or actions as illustrated by one or more of blocks 502 - 506 . Although the blocks are illustrated in a sequential order, these blocks may also be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation.
- each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor or computing device for implementing specific logical functions or steps in the process.
- the program code may be stored on any type of computer readable medium or memory, for example, such as a storage device including a disk or hard drive.
- the computer readable medium may include non-transitory computer readable media, for example, such as computer-readable media that stores data for short periods of time like register memory, processor cache and Random Access Memory (RAM).
- the computer readable medium may also include non-transitory media, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, or compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- the computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
- method 500 includes identifying a feature of interest in a field of view associated with HMD 172 .
- the feature of interest may comprise an object in an environment of HMD 172 .
- the feature of interest may be determined by the sensors of HMD 172 along with the view tracker 406 , for example.
- the sensors may detect the angle and direction of the eye of the wearer and determine a view associated with the direction and angle.
- the HMD 172 may transmit the viewing information to the view tracker 406 .
- a user of HMD 172 may be in a garden. While operating the HMD, the user may focus on flowers (e.g., by focusing his/her eyes on the flowers) located in the garden. The flowers may be associated with a location and a perceived depth to the user. There may be other flowers or objects in the garden that are visible by the wearer of the HMD, and in some instances the wearer may focus on many flowers. In such an instance, each of the flowers may be associated with varying depths and locations. Some flowers may have the same depth and location. After accurately positioning his/her eyes, the user may wink and cause, using the proximity sensor 136 , the HMD 102 to acquire image data indicative of the flowers in the garden.
- the image data may be captured in any manner discussed above with regard to FIGS. 1A-1G .
- FIG. 3B illustrates such a scenario.
- the user of HMD 172 perceives features of interest 350 , 352 using light-field display system 300 .
- Feature of interest 350 is at one depth to the HMD, while feature of interest 352 is at another.
- method 500 includes obtaining lightfield data.
- an image of the environment may be captured, for example by image capture device 178 that may gather light defining the environment.
- the lightfield data represents all of the light that passes through the flowers.
- the light that defines the environment may be provided to the light-field display system in the form of lightfield data from a remote device.
- method 500 includes rendering, based on the lightfield data, a lightfield comprising a synthetic image that is related to the feature of interest.
- the rendered lightfield may be a lightfield described by the lightfield data and may include the synthetic image.
- the synthetic image may correspond to the location and perceived depth of the feature of interest.
- the rendering may occur, for example using pixel renderer 406 , which may use the output of ray tracer 402 .
- the lightfield data that defines the environment may be rendered along with the synthetic image. In FIG.
- features of interest 350 , 352 have been rendered as Rendered Features of Interest 356 , 360 along with synthetic image 354 that indicates to the user that he/she is viewing a “RED LILLY.”
- the synthetic image is displayed at a depth associated with rendered feature of interest 356 .
- RED LILLY is used as the synthetic imagery, it is meant only to be an example, and other synthetic images are possible.
- the rendered feature of interest 356 is shown depicted outside of viewing location element 322 for ease of explanation, and is not meant to be limiting. In practice, the rendered feature of interest may be rendered at viewing location element 322 or directly in the eye 358 of the user.
- Rendering the lightfield synthetic image may be performed in any known rendering manner.
- Many different and specialized rendering algorithms have been developed such as scan-line rendering or ray-tracing, for example.
- Ray tracing is a method to produce realistic images by determining visible surfaces in an image at the pixel level.
- the ray tracing algorithm generates an image by tracing the path of light through pixels in an image plane and simulating the effects of its encounters with virtual objects.
- Scan-line rendering generates images on a row-by-row basis rather than a pixel-by-pixel basis. All of the polygons representing the 3D object data model are sorted, and then the image is computed using the intersection of a scan line with the polygons as the scan line is advanced down the picture.
- FIG. 5B is a block diagram of another example method 550 that utilizes an HMD capable of depth and focus discrimination using a light-field display system.
- the HMD utilizes a light-field display system to address an astigmatism that may be associated with the HMD.
- method 550 may be implemented by a user wearing HMD 172 depicted in FIG. 1D , which comprises a light-field display system 300 , and will be referenced as such in discussing method 550 .
- method 550 includes receiving astigmatism information that defines an astigmatism associated with HMD 172 .
- the astigmatism may be associated with an eye of the user of HMD 172 or with the viewing lens 180 , for example.
- the astigmatism information may be received in the form of data and can be, but need not be, data that was input by the user of HMD 172 .
- the data may, for example, comprise information that defines the astigmatism such as a prescription that may be associated with the astigmatism.
- the astigmatism information may comprise any data format capable of organizing and storing astigmatism information.
- method 550 includes identifying, a second feature of interest in a field of view associated with HMD 172 .
- the field of view and second feature of interest may be determined in the same or similar manner as that discussed above with regard to method 500 , for example.
- method 550 includes obtaining second lightfield data.
- the second lightfield data may be obtained in the same or similar fashion as that discussed above with regard to method 500 , for example.
- the method 550 includes generating, based on the second lightfield data and the astigmatism information, distorted lightfield data that compensates for or cancels out the astigmatism. This may be accomplished, for example, using the onboard computing device of HMD 172 and software, for example.
- the software may be configured to utilize algorithms and/or logic that allows the software to re-compute and/or distort the lightfield obtained by the HMD 172 .
- method 550 includes render, based on the distorted lightfield data, a second lightfield comprising a second synthetic image that is related to the second feature of interest.
- the second lightfield and second feature of interest may be rendered in a manner that compensates for the astigmatism.
- FIG. 6 illustrates a functional block diagram of an example of a computing device 600 .
- the computing device 600 can be used to perform any of the functions discussed in this disclosure, including those functions discussed above in connection with FIGS. 3A and 3B , 4 and 5 A- 5 B.
- the computing device 600 can be implemented as a portion of a head-mountable device, such as, for example, any of the HMDs discussed above in connection with FIGS. 1A-1D .
- the computing device 600 can be implemented as a portion of a small-form factor portable (or mobile) electronic device that is capable of communicating with an HMD; examples of such devices include a cell phone, a personal data assistant (PDA), a personal media player device, a wireless web-watch device, an application specific device, or a hybrid device that include any of the above functions.
- the computing device 610 can be implemented as a portion of a computer, such as, for example, a personal computer, a server, or a laptop, among others.
- the computing device 600 can include one or more processors 610 and system memory 620 .
- a memory bus 630 can be used for communicating between the processor 610 and the system memory 620 .
- the processor 610 can be of any type, including a microprocessor ( ⁇ P), a microcontroller ( ⁇ C), or a digital signal processor (DSP), among others.
- ⁇ P microprocessor
- ⁇ C microcontroller
- DSP digital signal processor
- a memory controller 615 can also be used with the processor 610 , or in some implementations, the memory controller 615 can be an internal part of the processor 610 .
- the system memory 620 can be of any type, including volatile memory (such as RAM) and non-volatile memory (such as ROM, flash memory).
- the system memory 620 can include one or more applications 622 and program data 624 .
- the application(s) 622 can include an index algorithm 623 that is arranged to provide inputs to the electronic circuits.
- the program data 624 can include content information 625 that can be directed to any number of types of data.
- the application 622 can be arranged to operate with the program data 624 on an operating system.
- the computing device 600 can have additional features or functionality, and additional interfaces to facilitate communication between the basic configuration 602 and any devices and interfaces.
- data storage devices 640 can be provided including removable storage devices 642 , non-removable storage devices 644 , or both.
- removable storage and non-removable storage devices include magnetic disk devices such as flexible disk drives and hard-disk drives (HDD), optical disk drives such as compact disk (CD) drives or digital versatile disk (DVD) drives, solid state drives (SSD), and tape drives.
- Computer storage media can include volatile and nonvolatile, non-transitory, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data.
- the system memory 620 and the storage devices 640 are examples of computer storage media.
- Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, DVDs or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store the desired information and that can be accessed by the computing device 600 .
- the computing device 600 can also include output interfaces 650 that can include a graphics processing unit 652 , which can be configured to communicate with various external devices, such as display devices 690 or speakers by way of one or more A/V ports or a communication interface 670 .
- the communication interface 670 can include a network controller 672 , which can be arranged to facilitate communication with one or more other computing devices 680 over a network communication by way of one or more communication ports 674 .
- the communication connection is one example of a communication media. Communication media can be embodied by computer-readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and includes any information delivery media.
- a modulated data signal can be a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.
- communication media can include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared (IR), and other wireless media.
- RF radio frequency
- IR infrared
- FIG. 7 is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device, arranged according to at least some embodiments presented herein.
- the example computer program product 700 is provided using a signal bearing medium 701 .
- the signal bearing medium 701 may include one or more programming instructions 702 that, when executed by one or more processors may provide functionality or portions of the functionality described above with respect to FIGS. 1-5 .
- the signal bearing medium 701 may encompass a computer-readable medium 703 , such as, but not limited to, a hard disk drive, a Compact Disc (CD), a Digital Video Disk (DVD), a digital tape, memory, etc.
- the signal bearing medium 701 may encompass a computer recordable medium 704 , such as, but not limited to, memory, read/write (R/W) CDs, R/W DVDs, etc.
- the signal bearing medium 701 may encompass a communications medium 705 , such as, but not limited to, a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).
- a communications medium 705 such as, but not limited to, a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).
- the signal bearing medium 701 may be conveyed by a wireless form of the communications medium 705 (e.g., a wireless communications medium conforming with the IEEE 802.11 standard or other transmission protocol).
- the one or more programming instructions 702 may be, for example, computer executable and/or logic implemented instructions.
- a computing device such as the computing device 100 of FIG. 1 may be configured to provide various operations, functions, or actions in response to the programming instructions 702 conveyed to the computing device 700 by one or more of the computer readable medium 703 , the computer recordable medium 704 , and/or the communications medium 705 .
Abstract
A head-mountable device (HMD) is provided for augmenting a contemporaneously viewed “real image” of an object in a real-world environment using a light-field display system that allows for depth and focus discrimination. The HMD may include a light-producing display engine, a viewing location element, and a microlens array. The microlens array may be coupled to the light-producing display engine in a manner such that light emitted from the light-producing display engine is configured to follow an optical path through the microlens array to the viewing location element. The HMD may also include a processor configured to identify a feature of interest in a field-of-view associated with the HMD in an environment, obtain lightfield data, and render, based on the lightfield data, a lightfield that includes a synthetic image related to the feature of interest using depth and focus discrimination.
Description
- Unless otherwise indicated herein, the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.
- Computing devices such as personal computers, laptop computers, tablet computers, cellular phones, and countless types of Internet-capable devices are increasingly prevalent in numerous aspects of modern life. Over time, the manner in which these devices are providing information to users is becoming more intelligent, more efficient, more intuitive, and/or less obtrusive.
- The trend toward miniaturization of computing hardware, peripherals, as well as of sensors, detectors, and image and audio processors, among other technologies, has helped open up a field sometimes referred to as “wearable computing.” In the area of image and visual processing and production, in particular, it has become possible to consider wearable displays that place a graphic display close enough to eye(s) of a wearer (or user) such that the displayed image appears as a normal-sized image, such as might be displayed on a traditional image display device. The relevant technology may be referred to as “near-eye displays.”
- Wearable computing devices with near-eye displays may also be referred to as “head-mountable devices” (HMDs), “head-mounted displays,” “head-mounted devices,” or “head-mountable devices.” A head-mountable device places a graphic display or displays close to one or both eyes of a wearer. To generate the images on a display, a computer processing system may be used. Such displays may occupy an entire field of view of the wearer, or only occupy part of a field of view of the wearer. Further, head-mounted displays may vary in size, taking a smaller form such as a glasses-style display or a larger form such as a helmet, for example.
- Emerging and anticipated uses of wearable displays include applications in which users interact in real time with an augmented or virtual reality. Such applications can be mission-critical or safety-critical, such as in a public safety or aviation setting. The applications can also be recreational, such as interactive gaming. Many other applications are also possible.
- Within examples, a wearable display system, such as a head-mountable device, is provided for augmenting a contemporaneously viewed “real image” of an object in a real-world environment using a light-field display system that allows for depth and focus discrimination.
- In a first embodiment, a head-mountable device (HMD) is provided. The HMD includes a light-producing display engine, a viewing location element, and a microlens array. The microlens array is coupled to the light-producing display engine in a manner such that light emitted from the light-producing display engine is configured to follow an optical path through the microlens array to the viewing location element. The HMD also includes a processor. The processor is configured to identify a feature of interest in a field-of-view associated with the HMD in an environment. The feature of interest may be associated with a depth to the HMD in the environment, and the feature of interest may be visible at the viewing location element. The processor is also configured to obtain lightfield data. The lightfield data is indicative of the environment and the feature of interest. The processor is additionally configured to render, based on the lightfield data, a lightfield comprising a synthetic image that is related to the feature of interest at a focal point that corresponds to the depth for display at the viewing location element.
- In a second embodiment, a method is disclosed. The method includes identifying, using at least one processor of a head-mountable device (HMD), a feature of interest in a field-of-view associated with the HMD in an environment. The HMD comprises a light-producing display engine, a viewing location element, and a microlens array coupled to the light-producing display engine in a manner such that light emitted from the light-producing display engine is configured to follow an optical path through the microlens array to the viewing location element, and the feature of interest is associated with a depth to the HMD in the environment and visible at the viewing location element. The method also includes obtaining lightfield data. The lightfield data is indicative of the environment and the feature of interest. The method additionally includes rendering, based on the lightfield data, a lightfield comprising a synthetic image that is related to the feature of interest at a focal point that corresponds to the depth for display at the viewing location element.
- These as well as other aspects, advantages, and alternatives, will become apparent to those of ordinary skill in the art by reading the following detailed description, with reference where appropriate to the accompanying figures.
-
FIG. 1A illustrates a wearable computing system, according to an example embodiment. -
FIG. 1B illustrates an alternate view of the wearable computing system illustrated inFIG. 1A . -
FIG. 1C illustrates another wearable computing system, according to an example embodiment. -
FIG. 1D illustrates another wearable computing system, according to an example embodiment. -
FIGS. 1E , 1F and 1G are simplified illustrations of the wearable computing system shown inFIG. 1D , being worn by a wearer. -
FIG. 2 is a simplified block diagram of a computing device, according to an example embodiment. -
FIG. 3A is a diagram of an implementation of light-field display system that may be used by the wearable computing systems illustrated inFIGS. 1A-1D , according to an example embodiment. -
FIG. 3B is an example of an arrangement of a light-field display system that may be used by a wearer of one of the wearable computing systems depicted inFIGS. 1A-1D , according to an example embodiment. -
FIG. 4 is a block diagram of an example processor that may be used by the light-field display system ofFIG. 3A , according to an example embodiment. -
FIGS. 5A-5B are block diagrams of example methods that may be carried out by a wearable computing system, using the light-field display system ofFIG. 3A , according to an example embodiment. -
FIG. 6 is a functional block diagram of a computing device that may be used in conjunction with the systems and methods described herein, according to an example embodiment. -
FIG. 7 is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device according to an example embodiment. - Example methods and systems are described herein. It should be understood that the words “example” and “exemplary” are used herein to mean “serving as an example, instance, or illustration.” Any embodiment or feature described herein as being an “example” or “exemplary” is not necessarily to be construed as preferred or advantageous over other embodiments or features. In the following detailed description, reference is made to the accompanying figures, which form a part thereof. In the figures, similar symbols typically identify similar components, unless context dictates otherwise. Other embodiments may be utilized, and other changes may be made, without departing from the spirit or scope of the subject matter presented herein.
- The example embodiments described herein are not meant to be limiting. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein.
- A. Overview
- To provide an augmented-reality experience, augmented-reality applications superimpose augmented information in the form of synthetic images at various locations that correspond with natural components of a real scene. Generally, the synthetic imagery is composed on a flat plane (usually the plane of a display of a device running the augmented-reality application), which overlays a view of the real scene. However, because the focal plane is fixed, the synthetic imagery may be displayed at one apparent distance and focal length from the user. Accordingly, in many augmented-reality applications there can be a clear separation between the synthetic components of the scene and the natural components (i.e., the synthetic imagery appears at one focal point, while the corresponding object from the real world appears at a different focal point), which may cause, for example, discontinuity of focus, difficulty recognizing corresponding synthetic and natural components, and/or lack of visual integration between the real and synthetic scene.
- In some examples, HMDs capable of running augmented reality applications may be configured to project the synthetic images at a set focal length, which may not be desirable. For example, the highlight indicator (synthetic imagery) may be at one focal distance from the user, while the object to be highlighted is at another focal distance. Similarly, it may be difficult to highlight multiple objects within the scene of the HMD because each highlight indicator is at the same focal distance. In other examples, when a user is viewing an object of the real world through the synthetic image, the synthetic image may appear out of focus and blurry. This may lead to eyestrain of the user, and the inconsistency with the natural components may harm the verisimilitude of the augmented-reality experience, making it easy for the user to tell the difference between reality and virtual reality.
- Similarly, users of HMDs who have asymmetric or astigmatic vision may also find it difficult to use HMDs in an augmented reality manner because the synthetic imagery and natural objects may seem out of focus or blurry due to the asymmetric or astigmatic vision, regardless of the problems mentioned above.
- Within examples herein, an HMD may be configured to run an augmented reality application and sense an environment with various natural components. The HMD may be configured to render light-fields and/or stereoscopic imaging of the environment in a manner that may allow any augmented information in the form of synthetic images to appear at various depths or focal distances that correspond with the depth of the natural components in the environment, and may compensate for any visual defects including those resulting from an astigmatism.
- To this end, disclosed is a HMD that includes a light-field display system. The light-field display system may be configured in a manner that ensures light emitted from a display engine follows an optical path through a microlens array before being viewed by a wearer of the HMD. This configuration allows lightfield data (a light filed is a function describing the amount of light moving in every direction through every point in space) to be produced that represents an environment of the wearer of the HMD. Using depth information obtained from the light-field technology, the HMD may render, into the eye of the wearer, a light field that includes information about the environment of the HMD in an augmented reality manner at distances and focal points that correspond to the actual distances and focal points of objects in the environment, and may simultaneously compensate for any visual defects.
- To illustrate, in one example, consider an HMD in an office. The HMD may focus on feature points (natural components) in the environment. Such feature points may include a computer and a scanner, for example. The HMD may use light-field technology to acquire images of the office as well as information indicating where the scanner and computer are in the office, for example. Using the depth information obtained from the lightfield data, the HMD may place information about the scanner and/or computer on the HMD in an augmented reality manner at distances and focal points that correspond to the actual distances and focal points of the scanner and computer.
- B. Example Wearable Computing Devices
- Systems and devices in which example embodiments may be implemented will now be described in greater detail. In general, an example system may be implemented in or may take the form of a wearable computer (also referred to as a wearable computing device). In an example embodiment, a wearable computer takes the form of or includes a head-mountable device (HMD).
- An example system may also be implemented in or take the form of other devices, such as a mobile phone, among other possibilities. Further, an example system may take the form of non-transitory computer readable medium, which has program instructions stored thereon that are executable by at a processor to provide the functionality described herein. An example system may also take the form of a device such as a wearable computer or mobile phone, or a subsystem of such a device, which includes such a non-transitory computer readable medium having such program instructions stored thereon.
- An HMD may generally be any display device that is capable of being worn on the head and places a display in front of one or both eyes of the wearer. An HMD may take various forms such as a helmet or eyeglasses. As such, references to “eyeglasses” or a “glasses-style” HMD should be understood to refer to an HMD that has a glasses-like frame so that it can be worn on the head. Further, example embodiments may be implemented by or in association with an HMD with a single display or with two displays, which may be referred to as a “monocular” HMD or a “binocular” HMD, respectively.
-
FIG. 1A illustrates a wearable computing system according to an example embodiment. InFIG. 1A , the wearable computing system takes the form of a head-mountable device (HMD) 102 (which may also be referred to as a head-mounted display). It should be understood, however, that example systems and devices may take the form of or be implemented within or in association with other types of devices, without departing from the scope of the invention. As illustrated inFIG. 1A , theHMD 102 includes frame elements including lens-frames center frame support 108,lens elements field display system 136, and extending side-arms center frame support 108 and the extending side-arms HMD 102 to a face of a user via a nose and ears of the user, respectively. - Each of the
frame elements arms HMD 102. Other materials may be possible as well. - One or more of each of the
lens elements lens elements - The extending side-
arms frames HMD 102 to the user. The extending side-arms HMD 102 to the user by extending around a rear portion of the head of the user. Additionally or alternatively, for example, theHMD 102 may connect to or be affixed within a head-mounted helmet structure. Other configurations for an HMD are also possible. - The
HMD 102 may also include an on-board computing system 118, animage capture device 120, asensor 122, and a finger-operable touch pad 124. The on-board computing system 118 is shown to be positioned on the extending side-arm 114 of theHMD 102; however, the on-board computing system 118 may be provided on other parts of theHMD 102 or may be positioned remote from the HMD 102 (e.g., the on-board computing system 118 could be wire- or wirelessly-connected to the HMD 102). The on-board computing system 118 may include a processor and memory, for example. The on-board computing system 118 may be configured to receive and analyze data from theimage capture device 120 and the finger-operable touch pad 124 (and possibly from other sensory devices, user interfaces, or both) and generate images for output by thelens elements - The
image capture device 120 may be, for example, a camera that is configured to capture still images and/or to capture video. In the illustrated configuration,image capture device 120 is positioned on the extending side-arm 114 of theHMD 102; however, theimage capture device 120 may be provided on other parts of theHMD 102. Theimage capture device 120 may be configured to capture images at various resolutions or at different frame rates. Many image capture devices with a small form-factor, such as the cameras used in mobile phones or webcams, for example, may be incorporated into an example of theHMD 102. - Further, although
FIG. 1A illustrates oneimage capture device 120, more image capture device may be used, and each may be configured to capture the same view, or to capture different views. For example, theimage capture device 120 may be forward facing to capture at least a portion of the real-world view perceived by the user. This forward facing image captured by theimage capture device 120 may then be used to generate an augmented reality where computer generated images appear to interact with or overlay the real-world view perceived by the user. - The
sensor 122 is shown on the extending side-arm 116 of theHMD 102; however, thesensor 122 may be positioned on other parts of theHMD 102. For illustrative purposes, only onesensor 122 is shown. However, in an example embodiment, theHMD 102 may include multiple sensors. For example, anHMD 102 may includesensors 102 such as one or more gyroscopes, one or more accelerometers, one or more magnetometers, one or more light sensors, one or more infrared sensors, and/or one or more microphones. Other sensing devices may be included in addition or in the alternative to the sensors that are specifically identified herein. - The finger-
operable touch pad 124 is shown on the extending side-arm 114 of theHMD 102. However, the finger-operable touch pad 124 may be positioned on other parts of theHMD 102. Also, more than one finger-operable touch pad may be present on theHMD 102. The finger-operable touch pad 124 may be used by a user to input commands. The finger-operable touch pad 124 may sense at least one of a pressure, position and/or a movement of one or more fingers via capacitive sensing, resistance sensing, or a surface acoustic wave process, among other possibilities. The finger-operable touch pad 124 may be capable of sensing movement of one or more fingers simultaneously, in addition to sensing movement in a direction parallel or planar to the pad surface, in a direction normal to the pad surface, or both, and may also be capable of sensing a level of pressure applied to the touch pad surface. In some embodiments, the finger-operable touch pad 124 may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger-operable touch pad 124 may be formed to have a raised, indented, or roughened surface, so as to provide tactile feedback to a user when a finger of the user reaches the edge, or other area, of the finger-operable touch pad 124. If more than one finger-operable touch pad is present, each finger-operable touch pad may be operated independently, and may provide a different function. - In a further aspect,
HMD 102 may be configured to receive user input in various ways, in addition or in the alternative to user input received via finger-operable touch pad 124. For example, on-board computing system 118 may implement a speech-to-text process and utilize a syntax that maps certain spoken commands to certain actions. In addition,HMD 102 may include one or more microphones via which speech of a wearer may be captured. Configured as such,HMD 102 may be operable to detect spoken commands and carry out various computing functions that correspond to the spoken commands. - As another example,
HMD 102 may interpret certain head-movements as user input. For example, whenHMD 102 is worn,HMD 102 may use one or more gyroscopes and/or one or more accelerometers to detect head movement. TheHMD 102 may then interpret certain head-movements as being user input, such as nodding, or looking up, down, left, or right. AnHMD 102 could also pan or scroll through graphics in a display according to movement. Other types of actions may also be mapped to head movement. - As yet another example,
HMD 102 may interpret certain gestures (e.g., by a hand or hands of the wearer) as user input. For example,HMD 102 may capture hand movements by analyzing image data fromimage capture device 120, and initiate actions that are defined as corresponding to certain hand movements. - As a further example,
HMD 102 may interpret eye movement as user input. In particular,HMD 102 may include one or more inward-facing image capture devices and/or one or more other inward-facing sensors (not shown) that may be used to track eye movements and/or determine the direction of a gaze of a wearer. As such, certain eye movements may be mapped to certain actions. For example, certain actions may be defined as corresponding to movement of the eye in a certain direction, a blink, and/or a wink, among other possibilities. -
HMD 102 also includes aspeaker 125 for generating audio output. In one example, the speaker could be in the form of a bone conduction speaker, also referred to as a bone conduction transducer (BCT).Speaker 125 may be, for example, a vibration transducer or an electroacoustic transducer that produces sound in response to an electrical audio signal input. The frame ofHMD 102 may be designed such that when a user wearsHMD 102, thespeaker 125 contacts the wearer. Alternatively,speaker 125 may be embedded within the frame ofHMD 102 and positioned such that, when theHMD 102 is worn,speaker 125 vibrates a portion of the frame that contacts the wearer. In either case,HMD 102 may be configured to send an audio signal tospeaker 125, so that vibration of the speaker may be directly or indirectly transferred to the bone structure of the wearer. When the vibrations travel through the bone structure to the bones in the middle ear of the wearer, the wearer can interpret the vibrations provided byBCT 125 as sounds. - Various types of bone-conduction transducers (BCTs) may be implemented, depending upon the particular implementation. Generally, any component that is arranged to vibrate the
HMD 102 may be incorporated as a vibration transducer. Yet further it should be understood that anHMD 102 may include asingle speaker 125 or multiple speakers. In addition, the location(s) of speaker(s) on the HMD may vary, depending upon the implementation. For example, a speaker may be located proximate to a temple of a wearer (as shown), behind the ear of a wearer, proximate to the nose of the wearer, and/or at any other location where thespeaker 125 can vibrate the wearer's bone structure. -
FIG. 1B illustrates an alternate view of the wearable computing device illustrated inFIG. 1A . As shown inFIG. 1B , thelens elements HMD 102 may include afirst projector 128 coupled to an inside surface of the extending side-arm 116 and configured to project adisplay 130 onto an inside surface of thelens element 112. Additionally or alternatively, asecond projector 132 may be coupled to an inside surface of the extending side-arm 114 and configured to project adisplay 134 onto an inside surface of thelens element 110. - The
lens elements projectors projectors - In alternative embodiments, other types of display elements may also be used. For example, the
lens elements frame elements - In further embodiments, the
lens elements field display system 136. The light-field display system 136 may be affixed to thelens elements field display system 136 to be undetectable to a wearer of the HMD (i.e., the view of the real world of the wearer is unobstructed by the light-field display system). The light-field display system 136 may include optical elements that are configured to generate a lightfield and/or lightfield data including a display engine, a microlens array, and a viewing location element. The display engine may incorporate any of the display elements discussed above (e.g.,projectors 128, 132). In other embodiments, the display system may be separate and include other optical elements. The viewing location element may belens elements field display system 136, and light-field display system 136 may be arranged in other ways. For example, the light-field display system 136 may be affixed to lens frames 104, 106 and may have separation fromlens elements field display system 136 may be affixed to centerframe support 108. -
FIG. 1C illustrates another wearable computing system according to an example embodiment, which takes the form of anHMD 152. TheHMD 152 may include frame elements and side-arms such as those described with respect toFIGS. 1A and 1B . TheHMD 152 may additionally include an on-board computing system 154 and animage capture device 156, such as those described with respect toFIGS. 1A and 1B . Theimage capture device 156 is shown mounted on a frame of theHMD 152. However, theimage capture device 156 may be mounted at other positions as well. - As shown in
FIG. 1C , theHMD 152 may include asingle display 158 which may be coupled to the device. Thedisplay 158 may be formed on one of the lens elements of theHMD 152, such as a lens element described with respect toFIGS. 1A and 1B , and may be configured to overlay computer-generated graphics in the view of the physical world of the user. Thedisplay 158 is shown to be provided in a center of a lens of theHMD 152, however, thedisplay 158 may be provided in other positions, such as for example towards either the upper or lower portions of the field of view of the wearer. Thedisplay 158 is controllable via thecomputing system 154 that is coupled to thedisplay 158 via anoptical waveguide 160. Thedisplay 158 may comprise or be coupled to a light-field display system, although a light-field display system is not shown inFIG. 1C . -
FIG. 1D illustrates another wearable computing system according to an example embodiment, which takes the form of amonocular HMD 172. TheHMD 172 may include side-arms 173, acenter frame support 174, and a bridge portion withnosepiece 175. In the example shown inFIG. 1D , thecenter frame support 174 connects the side-arms 173. TheHMD 172 does not include lens-frames containing lens elements. TheHMD 172 may additionally include acomponent housing 176, which may include an on-board computing system (not shown), animage capture device 178, and abutton 179 for operating the image capture device 178 (and/or usable for other purposes).Component housing 176 may also include other electrical components and/or may be electrically connected to electrical components at other locations within or on the HMD.HMD 172 also includes aBCT 186. - The
HMD 172 may include asingle display 180, which may be coupled to one of the side-arms 173 via thecomponent housing 176. In an example embodiment, thedisplay 180 may be a see-through display, which is made of glass and/or another transparent or translucent material, such that the wearer can see their environment through thedisplay 180. Thedisplay 180 may include a light-field display system (not shown inFIG. 1D ). Further, thecomponent housing 176 may include the light sources (not shown) for thedisplay 180 and/or optical elements (not shown) to direct light from the light sources to thedisplay 180. As such,display 180 may include optical features that direct light that is generated by such light sources towards the eye of the wearer, whenHMD 172 is being worn, such as, for example, optical features that comprise a light-field display system (not shown). - In a further aspect,
HMD 172 may include a slidingfeature 184, which may be used to adjust the length of the side-arms 173. Thus, slidingfeature 184 may be used to adjust the fit ofHMD 172. Further, an HMD may include other features that allow a wearer to adjust the fit of the HMD, without departing from the scope of the invention. -
FIGS. 1E to 1G are simplified illustrations of theHMD 172 shown inFIG. 1D , being worn by awearer 190. As shown inFIG. 1F , whenHMD 172 is worn,BCT 186 is arranged such that whenHMD 172 is worn,BCT 186 is located behind the ear of the wearer. As such,BCT 186 is not visible from the perspective shown inFIG. 1E . - In the illustrated example, the
display 180 may be arranged such that whenHMD 172 is worn,display 180 is positioned in front of or proximate to an eye of a user when theHMD 172 is worn by a user. For example,display 180 may be positioned below the center frame support and above the center of the eye of the wearer, as shown inFIG. 1E . Further, in the illustrated configuration,display 180 may be offset from the center of the eye of the wearer (e.g., so that the center ofdisplay 180 is positioned to the right and above of the center of the wearer's eye, from the perspective of the wearer). - Configured as shown in
FIGS. 1E to 1G ,display 180 may be located in the periphery of the field of view of thewearer 190, whenHMD 172 is worn. Thus, as shown byFIG. 1F , when thewearer 190 looks forward, thewearer 190 may see thedisplay 180 with their peripheral vision. As a result,display 180 may be outside the central portion of the field of view of the wearer when their eye is facing forward, as it commonly is for many day-to-day activities. Such positioning can facilitate unobstructed eye-to-eye conversations with others, as well as generally providing unobstructed viewing and perception of the world within the central portion of the field of view of the wearer. Further, when thedisplay 180 is located as shown, thewearer 190 may view thedisplay 180 by, e.g., looking up with their eyes only (possibly without moving their head). This is illustrated as shown inFIG. 1G , where the wearer has moved their eyes to look up and align their line of sight withdisplay 180. A wearer might also use the display by tilting their head down and aligning their eye with thedisplay 180. -
FIG. 2 is a simplified block diagram of acomputing device 210 according to an example embodiment. In an example embodiment,device 210 communicates using a communication link 220 (e.g., a wired or wireless connection) to aremote device 230. Thedevice 210 may be any type of device that can receive data and display information corresponding to or associated with the data. For example, thedevice 210 may be a heads-up display system, such as the head-mounteddevices FIGS. 1A to 1G . - Thus, the
device 210 may include adisplay system 212 comprising aprocessor 214 and adisplay 216. Thedisplay 216 may be, for example, an optical see-through display, an optical see-around display, or a video see-through display, and may comprise components of a light-field display system. Theprocessor 214 may receive data from theremote device 230, and configure the data for display on thedisplay 216. Theprocessor 214 may be any type of processor, such as a micro-processor or a digital signal processor, for example. - The
device 210 may further include on-board data storage, such asmemory 218 coupled to theprocessor 214. Thememory 218 may store software that can be accessed and executed by theprocessor 214, for example. - The
remote device 230 may be any type of computing device or transmitter including a laptop computer, a mobile telephone, or tablet computing device, etc., that is configured to transmit data to thedevice 210. Theremote device 230 and thedevice 210 may contain hardware to enable thecommunication link 220, such as processors, transmitters, receivers, antennas, etc. - Further,
remote device 230 may take the form of or be implemented in a computing system that is in communication with and configured to perform functions on behalf of client device, such ascomputing device 210. Such aremote device 230 may receive data from another computing device 210 (e.g., anHMD device 210, and then send the resulting data back todevice 210. This functionality may be referred to as “cloud” computing. - In
FIG. 2 , thecommunication link 220 is illustrated as a wireless connection; however, wired connections may also be used. For example, thecommunication link 220 may be a wired serial bus such as a universal serial bus or a parallel bus. A wired connection may be a proprietary connection as well. Thecommunication link 220 may also be a wireless connection using, e.g., Bluetooth® radio technology, communication protocols described in IEEE 802.11 (including any IEEE 802.11 revisions), Cellular technology (such as GSM, CDMA, UMTS, EV-DO, WiMAX, or LTE), or Zigbee® technology, among other possibilities. Theremote device 230 may be accessible via the Internet and may include a computing cluster associated with a particular web service (e.g., social-networking, photo sharing, address book, etc.). -
FIG. 3A illustrates a side view of an implementation of a light-field display system 300. The light-field display system 300 may include adisplay engine 310, amicrolens array 316, and aviewing location element 322. The light-field display system 300 may be coupled to an HMD, such as one of the HMDs discussed above in section (B), or in some examples may be considered a component of an HMD. - In one example, the
display engine 310 may include an organic light emitting diode (OLED). The OLED may be a transparent or semi-transparent matrix display that allows the wearer of the HMD to view the synthetic image produced by the OLED as well as allowing the wearer of the HMD to view light and objects from the real world. In other examples, thedisplay engine 310 may include other light producing displays such as a liquid crystal display (LCD), a Liquid Crystal over Silicon (LCoS) display, or microelectro-mechanical systems (MEMS) projector device such as a Digital Light Processing (DLP) or PicoP projector. In further examples, the display may incorporate or be any of the display elements discussed above with regard toFIGS. 1A-1G . - Note that while the
display engine 310 is shown at a separation distance frommicrolens array 316 andviewing location element 322, this is not intended to be limiting. In other arrangements displayengine 310 may be contiguous tomicrolens array 316, which may be contiguous to theviewing location element 322. Other arrangements are possible as well, and thedisplay engine 310,microlens array 316, andviewing location element 322 may be arranged in any suitable manner so long as the light-field display system 300 is able to accomplish the disclosed functionality. - The
display engine 310 may further include a plurality ofpixels 312 that generate light (e.g.,light rays pixels 312 represents a unit of the display engine, and each pixel may be activated to generate light independently. For example,pixel 313 may be activated to generate light with a particular color and intensity that is different than that ofpixel 314. In other examples,pixels - Although the
pixels FIG. 3A , in other examples, each pixel in the plurality ofpixels 312 may take any shape including round, oval, rectangular or square. Moreover, althoughFIG. 3A depicts a finite number of pixels (which make up the plurality of pixels) any number of pixels may be stored in the display engine, and whileFIG. 3A depicts the pixels from a side view, there may be additional columns of pixels that are not visible. For example, thedisplay engine 310 may include an OLED display which may have a pixel resolution of 800×600 pixels. Other resolutions may be used, and may be determined based on the type of display. - The light-
field display system 300 may further include amicrolens array 316. Themicrolens array 316 may include a plurality of microlenses such asmicrolenses microlens array 316 inFIG. 3A is depicted as having five microlenses (includingmicrolenses 317, 318) in other examples, any number of microlenses may be used in the microlens array. In some examples, the microlens array may be configured as a one-dimensional (1D) microlens array, while in other examples the microlens array may be configured as a two-dimensional (2D) microlens array. Moreover, becauseFIG. 3A is a side view of thedisplay engine 310,microlens array 316 may include additional microlenses that are not visible. For example,microlens array 316 may include additional columns of microlenses that are not visible. - Each microlens depicted in
FIG. 3A takes the shape of a circle. However, in other examples, each microlens may take the form of another shape. Moreover, inFIG. 3A themicrolens array 316 is arranged in a square pattern (i.e., columns and rows). However, in other embodiments the microlenses may be arranged in a variety of patterns including a hexagonal, octagonal, or other shaped grid. - The
microlens array 316 may be positioned behind the light-emittingdisplay engine 310 and in front of viewing element 322 (e.g., between the light-emittingdisplay engine 310 and the viewing element 322). In some examples, themicrolens array 316 may be configured such that one or more microlenses of the microlens array correspond to the plurality of pixels and are disposed in front of the plurality of pixels and at a separation from the plurality of pixels. The distance between thedisplay engine 310 andmicrolens array 316 may be sufficient to allow light passing from each pixel to pass through each microlens of themicrolens array 316. For example, as illustrated in inFIG. 3A , light 320 from afirst pixel 313 passes throughmicrolens 317 and is visible at theviewing location element 322, and light 321 from asecond pixel 314 passes throughmicrolens 318 and is also visible at theviewing location element 322. Whilelight microlens - The
viewing location element 322 may be thelens elements FIGS. 1A-1D , for example. In other arrangements, theviewing location element 322 may be a separate lens element, other thanlens elements lens elements - A display engine processor (not shown) may control the plurality of
pixels 312 to generate light such aslight processor 214. In other examples, the components ofprocessor 400, described inFIG. 4 , may be part ofprocessor 214. In further examples, the display engine processor may be separate to that ofprocessor 214. The display engine processor may, for example, control the color and intensity of the light displayed by each pixel. The display engine processor may also render particular lightfield data to be viewed at theviewing location element 322. The rendered light field may be a three-dimensional (3D) or four-dimensional (4D) image or scene, for example. -
FIG. 3B illustrates an example of an arrangement of the light-field display system 300 that may be used with an HMD interacting with aneye 358, and will be discussed in more detail later in this disclosure. Briefly, inFIG. 3B , a user (represented by eye 358) is operatingHMD 172, which includes light-field display system 300. As depicted, a user views “features of interest” wearingHMD 172 comprising a light-field display system 300. Light, demarcated by the bold arrows, penetrates the HMD in a manner such that lightfield data is captured that represents the environment the wearer ofHMD 172 is viewing, or as depicted inFIG. 3B , features ofinterest - Note, light and depth data that defines the environment may be obtained in manners other than utilizing a light field camera. In other examples, the data defining the environment may, for example, be obtained by two cameras offset to measure depth via stereopsis or using a monocular configuration that measures depth via motion parallax.
- Upon capturing the lightfield data, a processor of the HMD may produce the lightfield for the wearer. To do so, the lightfield data may be reproduced to accurately reflect what the wearer sees (e.g., based on the gaze of the wearer of the HMD), and may be used to render a lightfield representing the environment. In other examples the lightfield data may be processed to incorporate synthetic images or altered to compensate for astigmatisms or irregularities in the eye of the wearer of
HMD 172 or lens ofHMD 172. Once the lightfield data has been produced, an appropriate light-field may be rendered for viewing by the user. - When the data defining the environment is obtained utilizing methods other than a lightfield camera, the data may be used to generate lightfield data that may be used to render a lightfield representing the environment. Similar to the scenario in which lightfield data is captured using a lightfield camera, the generated lightfield data may also be processed to incorporate synthetic images or altered to compensate for astigmatism or irregularities in the eye of the wearer of
HMD 172 or lens ofHMD 172. In other examples, the generated lightfield data may be processed to compensate for irregularities or detrimental qualities of any part of the optical train of theHMD 172. -
FIG. 4 is a block diagram illustrating an implementation of aprocessor 400 that may be used by the light-field display system 300. As shown,processor 400 may include a variety of components including aRay Tracer 402,Lightfield Data 404,Pixel Renderer 406, and aView Tracker 408. Theview tracker 408 may determine a certain view at theviewing location element 322 associated with the light-field display system 300. For example, theview tracker 408 may receive view/gaze information of the HMD from the sensors described above with regard toFIGS. 1A-1G . Other systems may be used to determine the view associated with the light-field display system. - The
ray tracer 402 may determine which pixels of the plurality ofpixels 312 of thedisplay engine 310 are visible through each microlens of themicrolens array 316 within the view associated with the HMD (as determined by, for example, the view tracker 408). - In some examples, the
ray tracer 402 may determine which pixels are visible through each individual microlens of themicrolens array 312 by performing ray tracing from various points on the determined location of the eye of the wearer of the HMD through each microlens of themicrolens array 316, and determine which pixels of the plurality ofpixels 312 are reached by the rays for each microlens. The pixels that can be reached by a ray originating from the eye (e.g., pupil) of the wearer of the HMD through a microlens of themicrolens array 316 are the pixels that are visible by the eye of the wearer of the HMD at the viewing location element. - In other examples, the
ray tracer 402 may determine which pixels are visible through each individual microlens of themicrolens array 316 by performing ray tracing from each of the plurality of pixels through each microlens of themicrolens array 316. To do so, for each pixel of the plurality of pixels 312 (includingpixels 313 and 314), a ray may be traced to a certain point of the eye of a wearer. The intersection of the ray with themicrolens array 312 may be determined. In some examples, the ray may be traced from various locations within the pixel, and if no ray intersects the eye, then the pixel is not visible to the user. - The
pixel renderer 406 may control the output of thepixels 312 such that the appropriate light-field is displayed to a wearer of the HMD comprising the light-field display system 300. In other words, thepixel renderer 406 may utilize output from theray tracer 402 and the lightfield data that is obtained by the wearer (e.g., by viewing a real-world environment through the HMD) to determine or predict the output of thepixels 312 that will result in the lightfield data being correctly rendered to a viewer of the light-field display system 300. - Example methods for utilizing a HMD comprising a light-
field display system 300 are discussed below. - C. Example Methods
-
FIG. 5A is a block diagram of an example method for providing depth and focus discrimination using a HMD that includes a light-field display system, such as light-field display system 300.Method 500 shown inFIG. 5A presents an embodiment of a method that, for example, may be performed by a device the same as or similar to that discussed with regard toFIGS. 1-3 . For example,method 500 may be implemented by auser wearing HMD 172 depicted inFIG. 1D , which comprises a light-field display system (although not shown inFIG. 1D ), and will be referenced as such in discussingmethod 500.Method 500 may include one or more operations, functions, or actions as illustrated by one or more of blocks 502-506. Although the blocks are illustrated in a sequential order, these blocks may also be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation. - In addition, for the
method 500 and other processes and methods disclosed herein, the flowchart shows functionality and operation of one possible implementation of present embodiments. In this regard, each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor or computing device for implementing specific logical functions or steps in the process. The program code may be stored on any type of computer readable medium or memory, for example, such as a storage device including a disk or hard drive. The computer readable medium may include non-transitory computer readable media, for example, such as computer-readable media that stores data for short periods of time like register memory, processor cache and Random Access Memory (RAM). The computer readable medium may also include non-transitory media, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, or compact-disc read only memory (CD-ROM), for example. The computer readable media may also be any other volatile or non-volatile storage systems. The computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device. - Initially, at
block 502,method 500 includes identifying a feature of interest in a field of view associated withHMD 172. The feature of interest may comprise an object in an environment ofHMD 172. The feature of interest may be determined by the sensors ofHMD 172 along with theview tracker 406, for example. The sensors may detect the angle and direction of the eye of the wearer and determine a view associated with the direction and angle. TheHMD 172 may transmit the viewing information to theview tracker 406. - For example, a user of
HMD 172 may be in a garden. While operating the HMD, the user may focus on flowers (e.g., by focusing his/her eyes on the flowers) located in the garden. The flowers may be associated with a location and a perceived depth to the user. There may be other flowers or objects in the garden that are visible by the wearer of the HMD, and in some instances the wearer may focus on many flowers. In such an instance, each of the flowers may be associated with varying depths and locations. Some flowers may have the same depth and location. After accurately positioning his/her eyes, the user may wink and cause, using theproximity sensor 136, theHMD 102 to acquire image data indicative of the flowers in the garden. The image data may be captured in any manner discussed above with regard toFIGS. 1A-1G .FIG. 3B illustrates such a scenario. InFIG. 3B the user ofHMD 172 perceives features ofinterest field display system 300. Feature ofinterest 350 is at one depth to the HMD, while feature ofinterest 352 is at another. - Once the feature of interest has been determined, at
block 504,method 500 includes obtaining lightfield data. To do so, an image of the environment may be captured, for example byimage capture device 178 that may gather light defining the environment. With respect toFIG. 3B , the lightfield data represents all of the light that passes through the flowers. In other examples, the light that defines the environment may be provided to the light-field display system in the form of lightfield data from a remote device. - Once the lightfield data has been obtained, at
step 506,method 500 includes rendering, based on the lightfield data, a lightfield comprising a synthetic image that is related to the feature of interest. The rendered lightfield may be a lightfield described by the lightfield data and may include the synthetic image. The synthetic image may correspond to the location and perceived depth of the feature of interest. The rendering, may occur, for example usingpixel renderer 406, which may use the output ofray tracer 402. In practice, the lightfield data that defines the environment may be rendered along with the synthetic image. InFIG. 3B , features ofinterest Interest synthetic image 354 that indicates to the user that he/she is viewing a “RED LILLY.” As shown, the synthetic image is displayed at a depth associated with rendered feature ofinterest 356. Thus, while the wearer ofHMD 172 can view both rendered feature ofinterest 356 and rendered feature ofinterest 360, he/she can easily determine which rendered featured of interest is associated with thesynthetic imagery 354. - Note that while “RED LILLY” is used as the synthetic imagery, it is meant only to be an example, and other synthetic images are possible. Further, in
FIG. 3B , the rendered feature ofinterest 356 is shown depicted outside ofviewing location element 322 for ease of explanation, and is not meant to be limiting. In practice, the rendered feature of interest may be rendered atviewing location element 322 or directly in theeye 358 of the user. - Rendering the lightfield synthetic image may be performed in any known rendering manner. Many different and specialized rendering algorithms have been developed such as scan-line rendering or ray-tracing, for example. Ray tracing is a method to produce realistic images by determining visible surfaces in an image at the pixel level. The ray tracing algorithm generates an image by tracing the path of light through pixels in an image plane and simulating the effects of its encounters with virtual objects. Scan-line rendering generates images on a row-by-row basis rather than a pixel-by-pixel basis. All of the polygons representing the 3D object data model are sorted, and then the image is computed using the intersection of a scan line with the polygons as the scan line is advanced down the picture.
-
FIG. 5B is a block diagram of anotherexample method 550 that utilizes an HMD capable of depth and focus discrimination using a light-field display system. Inmethod 550, the HMD utilizes a light-field display system to address an astigmatism that may be associated with the HMD. As noted in reference tomethod 500,method 550 may be implemented by auser wearing HMD 172 depicted inFIG. 1D , which comprises a light-field display system 300, and will be referenced as such in discussingmethod 550. - Initially, at block, 552,
method 550 includes receiving astigmatism information that defines an astigmatism associated withHMD 172. The astigmatism may be associated with an eye of the user ofHMD 172 or with theviewing lens 180, for example. The astigmatism information may be received in the form of data and can be, but need not be, data that was input by the user ofHMD 172. The data may, for example, comprise information that defines the astigmatism such as a prescription that may be associated with the astigmatism. The astigmatism information may comprise any data format capable of organizing and storing astigmatism information. - Once the astigmatism information has been received, at
block 554,method 550 includes identifying, a second feature of interest in a field of view associated withHMD 172. The field of view and second feature of interest may be determined in the same or similar manner as that discussed above with regard tomethod 500, for example. - At block, 556,
method 550 includes obtaining second lightfield data. The second lightfield data may be obtained in the same or similar fashion as that discussed above with regard tomethod 500, for example. - At
block 558, themethod 550 includes generating, based on the second lightfield data and the astigmatism information, distorted lightfield data that compensates for or cancels out the astigmatism. This may be accomplished, for example, using the onboard computing device ofHMD 172 and software, for example. The software may be configured to utilize algorithms and/or logic that allows the software to re-compute and/or distort the lightfield obtained by theHMD 172. - At
block 560,method 550, includes render, based on the distorted lightfield data, a second lightfield comprising a second synthetic image that is related to the second feature of interest. Using the rendering techniques described above in regard tomethod 500, the second lightfield and second feature of interest may be rendered in a manner that compensates for the astigmatism. - D. Computing Device and Media
-
FIG. 6 illustrates a functional block diagram of an example of acomputing device 600. Thecomputing device 600 can be used to perform any of the functions discussed in this disclosure, including those functions discussed above in connection withFIGS. 3A and 3B , 4 and 5A-5B. In an implementation, thecomputing device 600 can be implemented as a portion of a head-mountable device, such as, for example, any of the HMDs discussed above in connection withFIGS. 1A-1D . In another implementation, thecomputing device 600 can be implemented as a portion of a small-form factor portable (or mobile) electronic device that is capable of communicating with an HMD; examples of such devices include a cell phone, a personal data assistant (PDA), a personal media player device, a wireless web-watch device, an application specific device, or a hybrid device that include any of the above functions. In another implementation, thecomputing device 610 can be implemented as a portion of a computer, such as, for example, a personal computer, a server, or a laptop, among others. - In a basic configuration 602, the
computing device 600 can include one ormore processors 610 andsystem memory 620. A memory bus 630 can be used for communicating between theprocessor 610 and thesystem memory 620. Depending on the desired configuration, theprocessor 610 can be of any type, including a microprocessor (μP), a microcontroller (μC), or a digital signal processor (DSP), among others. Amemory controller 615 can also be used with theprocessor 610, or in some implementations, thememory controller 615 can be an internal part of theprocessor 610. - Depending on the desired configuration, the
system memory 620 can be of any type, including volatile memory (such as RAM) and non-volatile memory (such as ROM, flash memory). Thesystem memory 620 can include one ormore applications 622 andprogram data 624. The application(s) 622 can include anindex algorithm 623 that is arranged to provide inputs to the electronic circuits. Theprogram data 624 can includecontent information 625 that can be directed to any number of types of data. Theapplication 622 can be arranged to operate with theprogram data 624 on an operating system. - The
computing device 600 can have additional features or functionality, and additional interfaces to facilitate communication between the basic configuration 602 and any devices and interfaces. For example,data storage devices 640 can be provided includingremovable storage devices 642,non-removable storage devices 644, or both. Examples of removable storage and non-removable storage devices include magnetic disk devices such as flexible disk drives and hard-disk drives (HDD), optical disk drives such as compact disk (CD) drives or digital versatile disk (DVD) drives, solid state drives (SSD), and tape drives. Computer storage media can include volatile and nonvolatile, non-transitory, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. - The
system memory 620 and thestorage devices 640 are examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, DVDs or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store the desired information and that can be accessed by thecomputing device 600. - The
computing device 600 can also includeoutput interfaces 650 that can include agraphics processing unit 652, which can be configured to communicate with various external devices, such asdisplay devices 690 or speakers by way of one or more A/V ports or acommunication interface 670. Thecommunication interface 670 can include anetwork controller 672, which can be arranged to facilitate communication with one or moreother computing devices 680 over a network communication by way of one ormore communication ports 674. The communication connection is one example of a communication media. Communication media can be embodied by computer-readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and includes any information delivery media. A modulated data signal can be a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media can include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared (IR), and other wireless media. - In some embodiments, the disclosed methods may be implemented as computer program instructions encoded on a non-transitory computer-readable storage media in a machine-readable format, or on other non-transitory media or articles of manufacture.
FIG. 7 is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device, arranged according to at least some embodiments presented herein. - In one embodiment, the example
computer program product 700 is provided using a signal bearing medium 701. The signal bearing medium 701 may include one ormore programming instructions 702 that, when executed by one or more processors may provide functionality or portions of the functionality described above with respect toFIGS. 1-5 . In some examples, the signal bearing medium 701 may encompass a computer-readable medium 703, such as, but not limited to, a hard disk drive, a Compact Disc (CD), a Digital Video Disk (DVD), a digital tape, memory, etc. In some implementations, the signal bearing medium 701 may encompass acomputer recordable medium 704, such as, but not limited to, memory, read/write (R/W) CDs, R/W DVDs, etc. In some implementations, the signal bearing medium 701 may encompass acommunications medium 705, such as, but not limited to, a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.). Thus, for example, the signal bearing medium 701 may be conveyed by a wireless form of the communications medium 705 (e.g., a wireless communications medium conforming with the IEEE 802.11 standard or other transmission protocol). - The one or
more programming instructions 702 may be, for example, computer executable and/or logic implemented instructions. In some examples, a computing device such as the computing device 100 ofFIG. 1 may be configured to provide various operations, functions, or actions in response to theprogramming instructions 702 conveyed to thecomputing device 700 by one or more of the computerreadable medium 703, thecomputer recordable medium 704, and/or thecommunications medium 705. - It should be understood that arrangements described herein are for purposes of example only. As such, those skilled in the art will appreciate that other arrangements and other elements (e.g. machines, interfaces, functions, orders, and groupings of functions, etc.) can be used instead, and some elements may be omitted altogether according to the desired results. Further, many of the elements that are described are functional entities that may be implemented as discrete or distributed components or in conjunction with other components, in any suitable combination and location.
- While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting, with the true scope being indicated by the following claims.
Claims (20)
1. A head-mountable device (HMD) comprising:
a light-producing display engine;
a viewing element;
a microlens array coupled to the light-producing display engine in a manner such that light emitted from one or more pixels of the light-producing display engine is configured to follow an optical path through the microlens array to the viewing element;
at least one processor coupled to the light-producing display engine; and
a storage medium that provides instructions that, when executed by the at least one processor, will cause the HMD to perform operations including:
identifying a first feature of interest in a field-of-view associated with the HMD in an environment, wherein the first feature of interest is associated with a first location and perceived depth to the HMD in the environment, and wherein the first feature of interest is visible at the viewing element;
obtaining lightfield data indicative of the environment and the first feature of interest;
rendering, based on the lightfield data, a first lightfield comprising a first synthetic image that is related to the first feature of interest at a first focal point that corresponds to the first location and perceived depth for display at the viewing element;
identifying a second feature of interest in the field-of-view associated with the HMD in the environment, wherein the second feature of interest is associated with a second location and a second perceived depth to the HMD in the environment, and wherein the second feature of interest is visible at the viewing element;
obtaining lightfield data indicative of the environment and the second feature of interest; and
rendering, based on the lightfield data, simultaneous to the first lightfield, a second lightfield comprising a second synthetic image that is related to the second feature of interest at a second focal point that corresponds to the second location and perceived depth for display at the viewing element,
wherein rendering the first and second lightfields further comprises emitting light from the light-producing display engine along the optical path passing through the microlens array to the viewing element.
2. The head-mountable device of claim 1 , wherein the microlens array includes a one-dimensional (1D) microlens array.
3. The head-mountable device of claim 1 , wherein the microlens array includes a two-dimensional (2D) microlens array.
4. The head-mountable device of claim 1 ,
wherein the light-producing display engine comprises a plurality of pixels, and
wherein the microlens array includes one or more microlenses that correspond to the plurality of pixels and are disposed in front of the plurality of pixels and at a separation from the plurality of pixels.
5. The head-mountable device of claim 1 , wherein the light-producing display engine comprises an Organic Light-Emitting Diode (OLED) display.
6. The head-mountable device of claim 1 , wherein the light-producing display engine comprises a Light-Emitting Diode (LED) and a Liquid Crystal over Silicon (LCoS) display.
7. The head-mountable device of claim 1 , wherein the light-producing display engine comprises a Digital Light Processing (DLP) projector.
8. The head-mountable device of claim 1 , wherein the storage medium provides further instructions that, when executed by the processor, will cause the HMD to perform further operations, comprising:
receiving astigmatism information that defines an astigmatism associated with the HMD;
generating, based on the second lightfield data and the astigmatism information, distorted lightfield data that compensates for the astigmatism; and
rendering, based on the distorted lightfield data, simultaneous to the first lightfield, the second lightfield comprising the second synthetic image that is related to the second feature of interest at the second focal point that corresponds to the second depth for display at the viewing element.
9. The head-mountable device of claim 8 , wherein the astigmatism is associated with a user of the device.
10. The head-mountable device of claim 8 ,
wherein the viewing element comprises a viewing lens, and
wherein the astigmatism is associated with the viewing lens.
11. A method comprising:
identifying, using at least one processor of a head-mountable device (HMD), a first feature of interest in a field-of-view view associated with the HMD in an environment, wherein:
the HMD comprises a light-producing display engine, a viewing element, and a microlens array coupled to the light-producing display engine in a manner such that light emitted from the light-producing display engine follows an optical path through the microlens array to the viewing element; and
the first feature of interest is associated with a first location and perceived depth to the HMD in the environment and visible at the viewing element;
obtaining lightfield data indicative of the environment and the first feature of interest;
rendering, based on the lightfield data, a first lightfield comprising a first synthetic image that is related to the first feature of interest at a first focal point that corresponds to the first location and perceived depth for display at the viewing element;
identifying, using at least one processor of the head-mountable device (HMD), a second feature of interest in a field-of-view view associated with the HMD in the environment, wherein:
the second feature of interest is associated with a second location and perceived depth to the HMD in the environment and visible at the viewing element;
obtaining second lightfield data indicative of the environment and the second feature of interest; and
rendering, based on the second lightfield data, simultaneous to the first lightfield, a second lightfield comprising a second synthetic image that is related to the second feature of interest at a second focal point that corresponds to the second location and perceived depth for display at the viewing element,
wherein rendering the first and second lightfields further comprises emitting light from the light-producing display engine along the optical path passing through the microlens array to the viewing element.
12. The method of claim 11 , wherein the microlens array includes a one-dimensional (1D) microlens array.
13. The method of claim 11 , wherein the microlens array includes a two-dimensional (2D) microlens array.
14. The method of claim 11 ,
wherein the light-producing display engine comprises a plurality of pixels, and
wherein the microlens array includes one or more microlenses that correspond to the plurality of pixels and are disposed in front of the plurality of pixels and at a separation from the plurality of pixels.
15. The method of claim 11 , wherein the light-producing display engine comprises an Organic Light-Emitting Diode (OLED) display.
16. The method of claim 11 , wherein the light-producing display engine comprises a Light-Emitting Diode (LED) and a Liquid Crystal over Silicon (LCoS) display.
17. The method of claim 11 , wherein the light-producing display engine comprises a Digital Light Processing (DLP) projector
18. The method of claim 11 , further comprising:
receiving astigmatism information that defines an astigmatism associated with the HMD;
generating, based on the second lightfield data and the astigmatism information, distorted lightfield data that compensates for the astigmatism; and
rendering, based on the distorted lightfield data, simultaneous to the first lightfield, the second lightfield comprising the second synthetic image that is related to the second feature of interest at the second focal point that corresponds to the second depth for display at the viewing element.
19. The method of claim 18 , wherein the astigmatism is associated with a user of the device.
20. The method of claim 18 ,
wherein the viewing element comprises a viewing lens, and
wherein the astigmatism is associated with the viewing lens.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/755,392 US20150262424A1 (en) | 2013-01-31 | 2013-01-31 | Depth and Focus Discrimination for a Head-mountable device using a Light-Field Display System |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/755,392 US20150262424A1 (en) | 2013-01-31 | 2013-01-31 | Depth and Focus Discrimination for a Head-mountable device using a Light-Field Display System |
Publications (1)
Publication Number | Publication Date |
---|---|
US20150262424A1 true US20150262424A1 (en) | 2015-09-17 |
Family
ID=54069422
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/755,392 Abandoned US20150262424A1 (en) | 2013-01-31 | 2013-01-31 | Depth and Focus Discrimination for a Head-mountable device using a Light-Field Display System |
Country Status (1)
Country | Link |
---|---|
US (1) | US20150262424A1 (en) |
Cited By (75)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170000342A1 (en) | 2015-03-16 | 2017-01-05 | Magic Leap, Inc. | Methods and systems for detecting health conditions by imaging portions of the eye, including the fundus |
US20170039905A1 (en) * | 2015-08-03 | 2017-02-09 | Oculus Vr, Llc | Optical System for Retinal Projection from Near-Ocular Display |
US20170102545A1 (en) * | 2014-03-05 | 2017-04-13 | The Arizona Board Of Regents On Behalf Of The University Of Arizona | Wearable 3d augmented reality display with variable focus and/or object recognition |
WO2017064063A1 (en) * | 2015-10-13 | 2017-04-20 | Carl Zeiss Vision International Gmbh | Apparatus and method for augmented reality presentation |
CN106773064A (en) * | 2017-01-22 | 2017-05-31 | 网易（杭州）网络有限公司 | The display control method of image frame, device and head-mounted display apparatus |
US20170269353A1 (en) * | 2016-03-15 | 2017-09-21 | Deepsee Inc. | 3d display apparatus, method, and applications |
WO2017184694A1 (en) * | 2016-04-21 | 2017-10-26 | Magic Leap, Inc. | Visual aura around field of view |
GB2550134A (en) * | 2016-05-09 | 2017-11-15 | Euro Electronics (Uk) Ltd | Method and apparatus for eye-tracking light field display |
DE102016112326A1 (en) * | 2016-07-06 | 2018-01-11 | Dr. Ing. H.C. F. Porsche Aktiengesellschaft | Method and system for operating 3D glasses with iris properties |
WO2018022521A1 (en) | 2016-07-25 | 2018-02-01 | Magic Leap, Inc. | Light field processor system |
EP3327486A1 (en) * | 2016-11-25 | 2018-05-30 | Coretronic Corporation | Near-eye display device |
US10025095B2 (en) * | 2014-12-26 | 2018-07-17 | Panasonic Intellectual Property Management Co., Ltd. | Head-up display and mobile body equipped with head-up display |
US20180275402A1 (en) * | 2015-01-12 | 2018-09-27 | Digilens, Inc. | Holographic waveguide light field displays |
CN108810519A (en) * | 2018-03-26 | 2018-11-13 | 成都理想境界科技有限公司 | A kind of 3D rendering display equipment |
WO2018217252A1 (en) * | 2017-05-26 | 2018-11-29 | Google Llc | Near-eye display with extended accommodation range adjustment |
CN109302599A (en) * | 2018-11-26 | 2019-02-01 | 体验科技股份有限公司 | A kind of method of three-dimensional imaging |
US10203566B2 (en) | 2015-12-21 | 2019-02-12 | Facebook Technologies, Llc | Enhanced spatial resolution using a segmented electrode array |
US10247858B2 (en) | 2015-10-25 | 2019-04-02 | Facebook Technologies, Llc | Liquid crystal half-wave plate lens |
US10247941B2 (en) * | 2015-01-19 | 2019-04-02 | Magna Electronics Inc. | Vehicle vision system with light field monitor |
US10297180B2 (en) | 2015-08-03 | 2019-05-21 | Facebook Technologies, Llc | Compensation of chromatic dispersion in a tunable beam steering device for improved display |
US10338451B2 (en) | 2015-08-03 | 2019-07-02 | Facebook Technologies, Llc | Devices and methods for removing zeroth order leakage in beam steering devices |
US20190227311A1 (en) * | 2018-01-22 | 2019-07-25 | Symbol Technologies, Llc | Systems and methods for task-based adjustable focal distance for heads-up displays |
US10416454B2 (en) | 2015-10-25 | 2019-09-17 | Facebook Technologies, Llc | Combination prism array for focusing light |
US10437064B2 (en) | 2015-01-12 | 2019-10-08 | Digilens Inc. | Environmentally isolated waveguide display |
CN110325897A (en) * | 2017-05-26 | 2019-10-11 | 谷歌有限责任公司 | The near-eye display of adjustable range adjustment with extension |
US10459305B2 (en) | 2015-08-03 | 2019-10-29 | Facebook Technologies, Llc | Time-domain adjustment of phase retardation in a liquid crystal grating for a color display |
US10459231B2 (en) | 2016-04-08 | 2019-10-29 | Magic Leap, Inc. | Augmented reality systems and methods with variable focus lens elements |
WO2019210254A1 (en) | 2018-04-27 | 2019-10-31 | Limbak 4Pi S.L. | Human vision-adapted light field displays |
US20190353894A1 (en) * | 2013-01-24 | 2019-11-21 | Yuchen Zhou | Method of utilizing defocus in virtual reality and augmented reality |
US10552676B2 (en) | 2015-08-03 | 2020-02-04 | Facebook Technologies, Llc | Methods and devices for eye tracking based on depth sensing |
CN110892727A (en) * | 2017-07-13 | 2020-03-17 | 三星电子株式会社 | Method and apparatus for transmitting data in network system |
US10712571B2 (en) | 2013-05-20 | 2020-07-14 | Digilens Inc. | Holograghic waveguide eye tracker |
US10718942B2 (en) | 2018-10-23 | 2020-07-21 | Microsoft Technology Licensing, Llc | Eye tracking systems and methods for near-eye-display (NED) devices |
US10776995B2 (en) | 2017-10-17 | 2020-09-15 | Nvidia Corporation | Light fields as better backgrounds in rendering |
US10812784B2 (en) | 2017-08-09 | 2020-10-20 | Coretronic Corporation | Light field display apparatus and method for calibrating display image thereof |
US10838490B2 (en) | 2018-10-23 | 2020-11-17 | Microsoft Technology Licensing, Llc | Translating combinations of user gaze direction and predetermined facial gestures into user input instructions for near-eye-display (NED) devices |
US10852823B2 (en) | 2018-10-23 | 2020-12-01 | Microsoft Technology Licensing, Llc | User-specific eye tracking calibration for near-eye-display (NED) devices |
US10855979B2 (en) | 2018-10-23 | 2020-12-01 | Microsoft Technology Licensing, Llc | Interpreting eye gaze direction as user input to near-eye-display (NED) devices for enabling hands free positioning of virtual items |
US10867538B1 (en) * | 2019-03-05 | 2020-12-15 | Facebook Technologies, Llc | Systems and methods for transferring an image to an array of emissive sub pixels |
WO2021038468A1 (en) * | 2019-08-26 | 2021-03-04 | Evolution Optiks Limited | Light field dislay, adjusted pixel rendering method therefor, and adjusted vision perception system and method using same addressing astigmatism or similar conditions |
WO2021038421A1 (en) * | 2019-08-26 | 2021-03-04 | Evolution Optiks Limited | Light field vision testing device, adjusted pixel rendering method therefor, and vision testing system and method using same |
US10962855B2 (en) | 2017-02-23 | 2021-03-30 | Magic Leap, Inc. | Display system with variable power reflector |
US10983340B2 (en) | 2016-02-04 | 2021-04-20 | Digilens Inc. | Holographic waveguide optical tracker |
US10989968B2 (en) | 2017-09-07 | 2021-04-27 | Coretronic Corporation | Optical element and display device |
US10996746B2 (en) | 2018-10-23 | 2021-05-04 | Microsoft Technology Licensing, Llc | Real-time computational solutions to a three-dimensional eye tracking framework |
US11009699B2 (en) | 2012-05-11 | 2021-05-18 | Digilens Inc. | Apparatus for eye tracking |
WO2021138607A1 (en) * | 2020-01-03 | 2021-07-08 | Digilens Inc. | Modular waveguide displays and related applications |
US11194162B2 (en) | 2017-01-05 | 2021-12-07 | Digilens Inc. | Wearable heads up displays |
US11262901B2 (en) | 2015-08-25 | 2022-03-01 | Evolution Optiks Limited | Electronic device, method and computer-readable medium for a user having reduced visual acuity |
US11281013B2 (en) | 2015-10-05 | 2022-03-22 | Digilens Inc. | Apparatus for providing waveguide displays with two-dimensional pupil expansion |
US20220092308A1 (en) * | 2013-10-11 | 2022-03-24 | Interdigital Patent Holdings, Inc. | Gaze-driven augmented reality |
US11287883B2 (en) | 2018-10-22 | 2022-03-29 | Evolution Optiks Limited | Light field device, pixel rendering method therefor, and adjusted vision perception system and method using same |
WO2022084076A1 (en) | 2020-10-20 | 2022-04-28 | Rabbit-Eyes B.V. | Vision correction display device, eye-tracking system and method to compensate for visual impairments |
US11327563B2 (en) | 2018-10-22 | 2022-05-10 | Evolution Optiks Limited | Light field vision-based testing device, adjusted pixel rendering method therefor, and online vision-based testing management system and method using same |
US20220236584A1 (en) * | 2021-01-22 | 2022-07-28 | National Taiwan University | Device of Generating 3D Light-Field Image |
US11442222B2 (en) | 2019-08-29 | 2022-09-13 | Digilens Inc. | Evacuated gratings and methods of manufacturing |
US11442151B2 (en) | 2015-01-20 | 2022-09-13 | Digilens Inc. | Holographic waveguide LIDAR |
US11448937B2 (en) | 2012-11-16 | 2022-09-20 | Digilens Inc. | Transparent waveguide display for tiling a display having plural optical powers using overlapping and offset FOV tiles |
US11454815B2 (en) | 2017-06-01 | 2022-09-27 | NewSight Reality, Inc. | Transparent optical module using pixel patches and associated lenslets |
US11487361B1 (en) | 2019-11-01 | 2022-11-01 | Evolution Optiks Limited | Light field device and vision testing system using same |
US11500460B2 (en) | 2018-10-22 | 2022-11-15 | Evolution Optiks Limited | Light field device, optical aberration compensation or simulation rendering |
US11500461B2 (en) | 2019-11-01 | 2022-11-15 | Evolution Optiks Limited | Light field vision-based testing device, system and method |
US11543594B2 (en) | 2019-02-15 | 2023-01-03 | Digilens Inc. | Methods and apparatuses for providing a holographic waveguide display using integrated gratings |
US11635617B2 (en) | 2019-04-23 | 2023-04-25 | Evolution Optiks Limited | Digital display device comprising a complementary light field display or display portion, and vision correction system and method using same |
US11650354B2 (en) * | 2018-01-14 | 2023-05-16 | Light Field Lab, Inc. | Systems and methods for rendering data from a 3D environment |
US11703645B2 (en) | 2015-02-12 | 2023-07-18 | Digilens Inc. | Waveguide grating device |
US11709373B2 (en) | 2014-08-08 | 2023-07-25 | Digilens Inc. | Waveguide laser illuminator incorporating a despeckler |
US11726332B2 (en) | 2009-04-27 | 2023-08-15 | Digilens Inc. | Diffractive projection apparatus |
US11726323B2 (en) | 2014-09-19 | 2023-08-15 | Digilens Inc. | Method and apparatus for generating input images for holographic waveguide displays |
US11747568B2 (en) | 2019-06-07 | 2023-09-05 | Digilens Inc. | Waveguides incorporating transmissive and reflective gratings and related methods of manufacturing |
US11789531B2 (en) | 2019-01-28 | 2023-10-17 | Evolution Optiks Limited | Light field vision-based testing device, system and method |
US11823598B2 (en) | 2019-11-01 | 2023-11-21 | Evolution Optiks Limited | Light field device, variable perception pixel rendering method therefor, and variable perception system and method using same |
EP4052248A4 (en) * | 2019-11-01 | 2023-12-13 | Evolution Optiks Limited | Light field device, multi-depth pixel rendering method therefor, and multi-depth vision perception system and method using same |
US11874493B2 (en) | 2016-07-15 | 2024-01-16 | Light Field Lab, Inc. | System and methods of universal parameterization of holographic sensory data generation, manipulation and transport |
US11902498B2 (en) | 2019-08-26 | 2024-02-13 | Evolution Optiks Limited | Binocular light field display, adjusted pixel rendering method therefor, and vision correction system and method using same |
Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090128669A1 (en) * | 2006-02-07 | 2009-05-21 | Yi-Ren Ng | Correction of optical aberrations |
US20100265385A1 (en) * | 2009-04-18 | 2010-10-21 | Knight Timothy J | Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same |
US20100271467A1 (en) * | 2009-04-28 | 2010-10-28 | Microsoft Corporation | Light-field display |
US20110018903A1 (en) * | 2004-08-03 | 2011-01-27 | Silverbrook Research Pty Ltd | Augmented reality device for presenting virtual imagery registered to a viewed surface |
US20110214082A1 (en) * | 2010-02-28 | 2011-09-01 | Osterhout Group, Inc. | Projection triggering through an external marker in an augmented reality eyepiece |
US20110213664A1 (en) * | 2010-02-28 | 2011-09-01 | Osterhout Group, Inc. | Local advertising content on an interactive head-mounted eyepiece |
US20120019711A1 (en) * | 2004-10-01 | 2012-01-26 | The Board Of Trustees Of The Leland Stanford Junior Univeristy | Imaging arrangements and methods therefor |
US20120127302A1 (en) * | 2010-11-18 | 2012-05-24 | Canon Kabushiki Kaisha | Mixed reality display |
US20120127203A1 (en) * | 2010-11-18 | 2012-05-24 | Canon Kabushiki Kaisha | Mixed reality display |
US20130286053A1 (en) * | 2012-04-25 | 2013-10-31 | Rod G. Fleck | Direct view augmented reality eyeglass-type display |
US20140168034A1 (en) * | 2012-07-02 | 2014-06-19 | Nvidia Corporation | Near-eye parallax barrier displays |
-
2013
- 2013-01-31 US US13/755,392 patent/US20150262424A1/en not_active Abandoned
Patent Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110018903A1 (en) * | 2004-08-03 | 2011-01-27 | Silverbrook Research Pty Ltd | Augmented reality device for presenting virtual imagery registered to a viewed surface |
US20120019711A1 (en) * | 2004-10-01 | 2012-01-26 | The Board Of Trustees Of The Leland Stanford Junior Univeristy | Imaging arrangements and methods therefor |
US20090128669A1 (en) * | 2006-02-07 | 2009-05-21 | Yi-Ren Ng | Correction of optical aberrations |
US20100265385A1 (en) * | 2009-04-18 | 2010-10-21 | Knight Timothy J | Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same |
US20100271467A1 (en) * | 2009-04-28 | 2010-10-28 | Microsoft Corporation | Light-field display |
US20110214082A1 (en) * | 2010-02-28 | 2011-09-01 | Osterhout Group, Inc. | Projection triggering through an external marker in an augmented reality eyepiece |
US20110213664A1 (en) * | 2010-02-28 | 2011-09-01 | Osterhout Group, Inc. | Local advertising content on an interactive head-mounted eyepiece |
US20120127302A1 (en) * | 2010-11-18 | 2012-05-24 | Canon Kabushiki Kaisha | Mixed reality display |
US20120127203A1 (en) * | 2010-11-18 | 2012-05-24 | Canon Kabushiki Kaisha | Mixed reality display |
US20130286053A1 (en) * | 2012-04-25 | 2013-10-31 | Rod G. Fleck | Direct view augmented reality eyeglass-type display |
US20140168034A1 (en) * | 2012-07-02 | 2014-06-19 | Nvidia Corporation | Near-eye parallax barrier displays |
Non-Patent Citations (2)
Title |
---|
Marc Levoy and Pat Hanrahan, Light Field Rendering, 1996, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, ACM, pages 31-42. * |
Ren Ng, Digital Light Field Photography, 2006, Doctoral dissertation, Stanford University, pages 1-187. * |
Cited By (187)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11726332B2 (en) | 2009-04-27 | 2023-08-15 | Digilens Inc. | Diffractive projection apparatus |
US11009699B2 (en) | 2012-05-11 | 2021-05-18 | Digilens Inc. | Apparatus for eye tracking |
US11448937B2 (en) | 2012-11-16 | 2022-09-20 | Digilens Inc. | Transparent waveguide display for tiling a display having plural optical powers using overlapping and offset FOV tiles |
US20230114549A1 (en) * | 2012-11-16 | 2023-04-13 | Rockwell Collins, Inc. | Transparent waveguide display |
US11815781B2 (en) * | 2012-11-16 | 2023-11-14 | Rockwell Collins, Inc. | Transparent waveguide display |
US11006102B2 (en) * | 2013-01-24 | 2021-05-11 | Yuchen Zhou | Method of utilizing defocus in virtual reality and augmented reality |
US20190353894A1 (en) * | 2013-01-24 | 2019-11-21 | Yuchen Zhou | Method of utilizing defocus in virtual reality and augmented reality |
US11662590B2 (en) | 2013-05-20 | 2023-05-30 | Digilens Inc. | Holographic waveguide eye tracker |
US10712571B2 (en) | 2013-05-20 | 2020-07-14 | Digilens Inc. | Holograghic waveguide eye tracker |
US20220092308A1 (en) * | 2013-10-11 | 2022-03-24 | Interdigital Patent Holdings, Inc. | Gaze-driven augmented reality |
US11350079B2 (en) * | 2014-03-05 | 2022-05-31 | Arizona Board Of Regents On Behalf Of The University Of Arizona | Wearable 3D augmented reality display |
US20170102545A1 (en) * | 2014-03-05 | 2017-04-13 | The Arizona Board Of Regents On Behalf Of The University Of Arizona | Wearable 3d augmented reality display with variable focus and/or object recognition |
US10469833B2 (en) * | 2014-03-05 | 2019-11-05 | The Arizona Board Of Regents On Behalf Of The University Of Arizona | Wearable 3D augmented reality display with variable focus and/or object recognition |
US11709373B2 (en) | 2014-08-08 | 2023-07-25 | Digilens Inc. | Waveguide laser illuminator incorporating a despeckler |
US11726323B2 (en) | 2014-09-19 | 2023-08-15 | Digilens Inc. | Method and apparatus for generating input images for holographic waveguide displays |
US10025095B2 (en) * | 2014-12-26 | 2018-07-17 | Panasonic Intellectual Property Management Co., Ltd. | Head-up display and mobile body equipped with head-up display |
US20180275402A1 (en) * | 2015-01-12 | 2018-09-27 | Digilens, Inc. | Holographic waveguide light field displays |
US11726329B2 (en) | 2015-01-12 | 2023-08-15 | Digilens Inc. | Environmentally isolated waveguide display |
US10437064B2 (en) | 2015-01-12 | 2019-10-08 | Digilens Inc. | Environmentally isolated waveguide display |
US11480788B2 (en) * | 2015-01-12 | 2022-10-25 | Digilens Inc. | Light field displays incorporating holographic waveguides |
US11740472B2 (en) | 2015-01-12 | 2023-08-29 | Digilens Inc. | Environmentally isolated waveguide display |
US10247941B2 (en) * | 2015-01-19 | 2019-04-02 | Magna Electronics Inc. | Vehicle vision system with light field monitor |
US11442151B2 (en) | 2015-01-20 | 2022-09-13 | Digilens Inc. | Holographic waveguide LIDAR |
US11703645B2 (en) | 2015-02-12 | 2023-07-18 | Digilens Inc. | Waveguide grating device |
US10788675B2 (en) | 2015-03-16 | 2020-09-29 | Magic Leap, Inc. | Methods and systems for diagnosing and treating eyes using light therapy |
US10371949B2 (en) | 2015-03-16 | 2019-08-06 | Magic Leap, Inc. | Methods and systems for performing confocal microscopy |
US10983351B2 (en) | 2015-03-16 | 2021-04-20 | Magic Leap, Inc. | Augmented and virtual reality display systems and methods for diagnosing health conditions based on visual fields |
US11156835B2 (en) | 2015-03-16 | 2021-10-26 | Magic Leap, Inc. | Methods and systems for diagnosing and treating health ailments |
US11256096B2 (en) | 2015-03-16 | 2022-02-22 | Magic Leap, Inc. | Methods and systems for diagnosing and treating presbyopia |
US11747627B2 (en) | 2015-03-16 | 2023-09-05 | Magic Leap, Inc. | Augmented and virtual reality display systems and methods for diagnosing health conditions based on visual fields |
US20170000342A1 (en) | 2015-03-16 | 2017-01-05 | Magic Leap, Inc. | Methods and systems for detecting health conditions by imaging portions of the eye, including the fundus |
US20170007843A1 (en) | 2015-03-16 | 2017-01-12 | Magic Leap, Inc. | Methods and systems for diagnosing and treating eyes using laser therapy |
US10775628B2 (en) | 2015-03-16 | 2020-09-15 | Magic Leap, Inc. | Methods and systems for diagnosing and treating presbyopia |
US11474359B2 (en) | 2015-03-16 | 2022-10-18 | Magic Leap, Inc. | Augmented and virtual reality display systems and methods for diagnosing health conditions based on visual fields |
US10345591B2 (en) | 2015-03-16 | 2019-07-09 | Magic Leap, Inc. | Methods and systems for performing retinoscopy |
US10345593B2 (en) | 2015-03-16 | 2019-07-09 | Magic Leap, Inc. | Methods and systems for providing augmented reality content for treating color blindness |
US10345590B2 (en) | 2015-03-16 | 2019-07-09 | Magic Leap, Inc. | Augmented and virtual reality display systems and methods for determining optical prescriptions |
US10345592B2 (en) | 2015-03-16 | 2019-07-09 | Magic Leap, Inc. | Augmented and virtual reality display systems and methods for diagnosing a user using electrical potentials |
US20170007450A1 (en) | 2015-03-16 | 2017-01-12 | Magic Leap, Inc. | Augmented and virtual reality display systems and methods for delivery of medication to eyes |
US10564423B2 (en) | 2015-03-16 | 2020-02-18 | Magic Leap, Inc. | Augmented and virtual reality display systems and methods for delivery of medication to eyes |
US10359631B2 (en) * | 2015-03-16 | 2019-07-23 | Magic Leap, Inc. | Augmented reality display systems and methods for re-rendering the world |
US20170010469A1 (en) * | 2015-03-16 | 2017-01-12 | Magic Leap, Inc. | Augmented reality display systems and methods for re-rendering the world |
US10365488B2 (en) | 2015-03-16 | 2019-07-30 | Magic Leap, Inc. | Methods and systems for diagnosing eyes using aberrometer |
US10545341B2 (en) | 2015-03-16 | 2020-01-28 | Magic Leap, Inc. | Methods and systems for diagnosing eye conditions, including macular degeneration |
US10371945B2 (en) | 2015-03-16 | 2019-08-06 | Magic Leap, Inc. | Methods and systems for diagnosing and treating higher order refractive aberrations of an eye |
US10459229B2 (en) | 2015-03-16 | 2019-10-29 | Magic Leap, Inc. | Methods and systems for performing two-photon microscopy |
US10371948B2 (en) | 2015-03-16 | 2019-08-06 | Magic Leap, Inc. | Methods and systems for diagnosing color blindness |
US10371946B2 (en) | 2015-03-16 | 2019-08-06 | Magic Leap, Inc. | Methods and systems for diagnosing binocular vision conditions |
US10371947B2 (en) | 2015-03-16 | 2019-08-06 | Magic Leap, Inc. | Methods and systems for modifying eye convergence for diagnosing and treating conditions including strabismus and/or amblyopia |
US10379350B2 (en) | 2015-03-16 | 2019-08-13 | Magic Leap, Inc. | Methods and systems for diagnosing eyes using ultrasound |
US10379353B2 (en) | 2015-03-16 | 2019-08-13 | Magic Leap, Inc. | Augmented and virtual reality display systems and methods for diagnosing health conditions based on visual fields |
US10379354B2 (en) | 2015-03-16 | 2019-08-13 | Magic Leap, Inc. | Methods and systems for diagnosing contrast sensitivity |
US10379351B2 (en) | 2015-03-16 | 2019-08-13 | Magic Leap, Inc. | Methods and systems for diagnosing and treating eyes using light therapy |
US10386639B2 (en) | 2015-03-16 | 2019-08-20 | Magic Leap, Inc. | Methods and systems for diagnosing eye conditions such as red reflex using light reflected from the eyes |
US10386641B2 (en) | 2015-03-16 | 2019-08-20 | Magic Leap, Inc. | Methods and systems for providing augmented reality content for treatment of macular degeneration |
US10386640B2 (en) | 2015-03-16 | 2019-08-20 | Magic Leap, Inc. | Methods and systems for determining intraocular pressure |
US10539794B2 (en) | 2015-03-16 | 2020-01-21 | Magic Leap, Inc. | Methods and systems for detecting health conditions by imaging portions of the eye, including the fundus |
US10429649B2 (en) | 2015-03-16 | 2019-10-01 | Magic Leap, Inc. | Augmented and virtual reality display systems and methods for diagnosing using occluder |
US10437062B2 (en) | 2015-03-16 | 2019-10-08 | Magic Leap, Inc. | Augmented and virtual reality display platforms and methods for delivering health treatments to a user |
US10539795B2 (en) | 2015-03-16 | 2020-01-21 | Magic Leap, Inc. | Methods and systems for diagnosing and treating eyes using laser therapy |
US10527850B2 (en) | 2015-03-16 | 2020-01-07 | Magic Leap, Inc. | Augmented and virtual reality display systems and methods for determining optical prescriptions by imaging retina |
US10473934B2 (en) | 2015-03-16 | 2019-11-12 | Magic Leap, Inc. | Methods and systems for performing slit lamp examination |
US10444504B2 (en) | 2015-03-16 | 2019-10-15 | Magic Leap, Inc. | Methods and systems for performing optical coherence tomography |
US10451877B2 (en) | 2015-03-16 | 2019-10-22 | Magic Leap, Inc. | Methods and systems for diagnosing and treating presbyopia |
US10969588B2 (en) | 2015-03-16 | 2021-04-06 | Magic Leap, Inc. | Methods and systems for diagnosing contrast sensitivity |
US10466477B2 (en) | 2015-03-16 | 2019-11-05 | Magic Leap, Inc. | Methods and systems for providing wavefront corrections for treating conditions including myopia, hyperopia, and/or astigmatism |
US10451876B2 (en) | 2015-08-03 | 2019-10-22 | Facebook Technologies, Llc | Enhanced visual perception through distance-based ocular projection |
US10359629B2 (en) * | 2015-08-03 | 2019-07-23 | Facebook Technologies, Llc | Ocular projection based on pupil position |
US10042165B2 (en) * | 2015-08-03 | 2018-08-07 | Oculus Vr, Llc | Optical system for retinal projection from near-ocular display |
US10459305B2 (en) | 2015-08-03 | 2019-10-29 | Facebook Technologies, Llc | Time-domain adjustment of phase retardation in a liquid crystal grating for a color display |
US10338451B2 (en) | 2015-08-03 | 2019-07-02 | Facebook Technologies, Llc | Devices and methods for removing zeroth order leakage in beam steering devices |
US10297180B2 (en) | 2015-08-03 | 2019-05-21 | Facebook Technologies, Llc | Compensation of chromatic dispersion in a tunable beam steering device for improved display |
US10345599B2 (en) | 2015-08-03 | 2019-07-09 | Facebook Technologies, Llc | Tile array for near-ocular display |
US10437061B2 (en) * | 2015-08-03 | 2019-10-08 | Facebook Technologies, Llc | Near-ocular display based on hologram projection |
US10534173B2 (en) | 2015-08-03 | 2020-01-14 | Facebook Technologies, Llc | Display with a tunable mask for augmented reality |
US10162182B2 (en) * | 2015-08-03 | 2018-12-25 | Facebook Technologies, Llc | Enhanced pixel resolution through non-uniform ocular projection |
US10274730B2 (en) | 2015-08-03 | 2019-04-30 | Facebook Technologies, Llc | Display with an embedded eye tracker |
US20170039905A1 (en) * | 2015-08-03 | 2017-02-09 | Oculus Vr, Llc | Optical System for Retinal Projection from Near-Ocular Display |
US10552676B2 (en) | 2015-08-03 | 2020-02-04 | Facebook Technologies, Llc | Methods and devices for eye tracking based on depth sensing |
US9989765B2 (en) | 2015-08-03 | 2018-06-05 | Oculus Vr, Llc | Tile array for near-ocular display |
US11262901B2 (en) | 2015-08-25 | 2022-03-01 | Evolution Optiks Limited | Electronic device, method and computer-readable medium for a user having reduced visual acuity |
US11754842B2 (en) | 2015-10-05 | 2023-09-12 | Digilens Inc. | Apparatus for providing waveguide displays with two-dimensional pupil expansion |
US11281013B2 (en) | 2015-10-05 | 2022-03-22 | Digilens Inc. | Apparatus for providing waveguide displays with two-dimensional pupil expansion |
US10573080B2 (en) * | 2015-10-13 | 2020-02-25 | Carl Zeiss Vision International Gmbh | Apparatus and method for augmented reality presentation |
WO2017064063A1 (en) * | 2015-10-13 | 2017-04-20 | Carl Zeiss Vision International Gmbh | Apparatus and method for augmented reality presentation |
US20180225878A1 (en) * | 2015-10-13 | 2018-08-09 | Carl Zeiss Vision International Gmbh | Apparatus and method for augmented reality presentation |
US10416454B2 (en) | 2015-10-25 | 2019-09-17 | Facebook Technologies, Llc | Combination prism array for focusing light |
US10705262B2 (en) | 2015-10-25 | 2020-07-07 | Facebook Technologies, Llc | Liquid crystal half-wave plate lens |
US10247858B2 (en) | 2015-10-25 | 2019-04-02 | Facebook Technologies, Llc | Liquid crystal half-wave plate lens |
US10203566B2 (en) | 2015-12-21 | 2019-02-12 | Facebook Technologies, Llc | Enhanced spatial resolution using a segmented electrode array |
US10670928B2 (en) | 2015-12-21 | 2020-06-02 | Facebook Technologies, Llc | Wide angle beam steering for virtual reality and augmented reality |
US10670929B2 (en) | 2015-12-21 | 2020-06-02 | Facebook Technologies, Llc | Enhanced spatial resolution using a segmented electrode array |
US10983340B2 (en) | 2016-02-04 | 2021-04-20 | Digilens Inc. | Holographic waveguide optical tracker |
US10088673B2 (en) * | 2016-03-15 | 2018-10-02 | Deepsee Inc. | 3D display apparatus, method, and applications |
US20170269353A1 (en) * | 2016-03-15 | 2017-09-21 | Deepsee Inc. | 3d display apparatus, method, and applications |
US11106041B2 (en) | 2016-04-08 | 2021-08-31 | Magic Leap, Inc. | Augmented reality systems and methods with variable focus lens elements |
US11614626B2 (en) | 2016-04-08 | 2023-03-28 | Magic Leap, Inc. | Augmented reality systems and methods with variable focus lens elements |
US10459231B2 (en) | 2016-04-08 | 2019-10-29 | Magic Leap, Inc. | Augmented reality systems and methods with variable focus lens elements |
US10838484B2 (en) | 2016-04-21 | 2020-11-17 | Magic Leap, Inc. | Visual aura around field of view |
WO2017184694A1 (en) * | 2016-04-21 | 2017-10-26 | Magic Leap, Inc. | Visual aura around field of view |
AU2017252557B2 (en) * | 2016-04-21 | 2022-01-27 | Magic Leap, Inc. | Visual aura around field of view |
US11340694B2 (en) | 2016-04-21 | 2022-05-24 | Magic Leap, Inc. | Visual aura around field of view |
CN109313509A (en) * | 2016-04-21 | 2019-02-05 | 奇跃公司 | The vision ring of light around the visual field |
GB2550134A (en) * | 2016-05-09 | 2017-11-15 | Euro Electronics (Uk) Ltd | Method and apparatus for eye-tracking light field display |
DE102016112326A1 (en) * | 2016-07-06 | 2018-01-11 | Dr. Ing. H.C. F. Porsche Aktiengesellschaft | Method and system for operating 3D glasses with iris properties |
US11874493B2 (en) | 2016-07-15 | 2024-01-16 | Light Field Lab, Inc. | System and methods of universal parameterization of holographic sensory data generation, manipulation and transport |
KR102412525B1 (en) * | 2016-07-25 | 2022-06-23 | 매직 립, 인코포레이티드 | Optical Field Processor System |
KR20220090591A (en) * | 2016-07-25 | 2022-06-29 | 매직 립, 인코포레이티드 | Light field processor system |
KR20190029727A (en) * | 2016-07-25 | 2019-03-20 | 매직 립, 인코포레이티드 | Optical Field Processor System |
CN109788901A (en) * | 2016-07-25 | 2019-05-21 | 奇跃公司 | Light field processor system |
US11733542B2 (en) | 2016-07-25 | 2023-08-22 | Magic Leap, Inc. | Light field processor system |
EP3488284A4 (en) * | 2016-07-25 | 2020-03-18 | Magic Leap, Inc. | Light field processor system |
JP7441915B2 (en) | 2016-07-25 | 2024-03-01 | マジック リープ， インコーポレイテッド | light field processor system |
JP2022176291A (en) * | 2016-07-25 | 2022-11-25 | マジック リープ， インコーポレイテッド | light field processor system |
US11048101B2 (en) | 2016-07-25 | 2021-06-29 | Magic Leap, Inc. | Light field processor system |
WO2018022521A1 (en) | 2016-07-25 | 2018-02-01 | Magic Leap, Inc. | Light field processor system |
KR102520143B1 (en) * | 2016-07-25 | 2023-04-11 | 매직 립, 인코포레이티드 | Light field processor system |
JP7229302B2 (en) | 2016-07-25 | 2023-02-27 | マジック リープ， インコーポレイテッド | light field processor system |
JP2021164665A (en) * | 2016-07-25 | 2021-10-14 | マジック リープ, インコーポレイテッドMagic Leap, Inc. | Light field processor system |
EP4235366A3 (en) * | 2016-07-25 | 2023-11-01 | Magic Leap, Inc. | Light field processor system and method |
EP3671318A1 (en) | 2016-11-25 | 2020-06-24 | Coretronic Corporation | Near-eye display device |
EP3327486A1 (en) * | 2016-11-25 | 2018-05-30 | Coretronic Corporation | Near-eye display device |
US10718944B2 (en) | 2016-11-25 | 2020-07-21 | Coretronic Corporation | Near-eye display device with phase modulation |
EP3518025A2 (en) | 2016-11-25 | 2019-07-31 | Coretronic Corporation | Near-eye display device |
US11586046B2 (en) | 2017-01-05 | 2023-02-21 | Digilens Inc. | Wearable heads up displays |
US11194162B2 (en) | 2017-01-05 | 2021-12-07 | Digilens Inc. | Wearable heads up displays |
CN106773064A (en) * | 2017-01-22 | 2017-05-31 | 网易（杭州）网络有限公司 | The display control method of image frame, device and head-mounted display apparatus |
US10901214B2 (en) | 2017-01-22 | 2021-01-26 | Netease (Hangzhou) Network Co., Ltd. | Method and device for controlling display of image and head-mounted display |
US10962855B2 (en) | 2017-02-23 | 2021-03-30 | Magic Leap, Inc. | Display system with variable power reflector |
US11300844B2 (en) | 2017-02-23 | 2022-04-12 | Magic Leap, Inc. | Display system with variable power reflector |
US11774823B2 (en) | 2017-02-23 | 2023-10-03 | Magic Leap, Inc. | Display system with variable power reflector |
WO2018217252A1 (en) * | 2017-05-26 | 2018-11-29 | Google Llc | Near-eye display with extended accommodation range adjustment |
US20180343443A1 (en) * | 2017-05-26 | 2018-11-29 | Google Llc | Near-eye display with extended accommodation range adjustment |
TWI720293B (en) * | 2017-05-26 | 2021-03-01 | 美商谷歌有限責任公司 | Near-eye display with extended accommodation range adjustment |
US10855977B2 (en) | 2017-05-26 | 2020-12-01 | Google Llc | Near-eye display with extended accommodation range adjustment |
CN110325897A (en) * | 2017-05-26 | 2019-10-11 | 谷歌有限责任公司 | The near-eye display of adjustable range adjustment with extension |
US11435576B2 (en) | 2017-05-26 | 2022-09-06 | Google Llc | Near-eye display with extended accommodation range adjustment |
US11454815B2 (en) | 2017-06-01 | 2022-09-27 | NewSight Reality, Inc. | Transparent optical module using pixel patches and associated lenslets |
CN110892727A (en) * | 2017-07-13 | 2020-03-17 | 三星电子株式会社 | Method and apparatus for transmitting data in network system |
US10812784B2 (en) | 2017-08-09 | 2020-10-20 | Coretronic Corporation | Light field display apparatus and method for calibrating display image thereof |
US10989968B2 (en) | 2017-09-07 | 2021-04-27 | Coretronic Corporation | Optical element and display device |
US10776995B2 (en) | 2017-10-17 | 2020-09-15 | Nvidia Corporation | Light fields as better backgrounds in rendering |
US11650354B2 (en) * | 2018-01-14 | 2023-05-16 | Light Field Lab, Inc. | Systems and methods for rendering data from a 3D environment |
US20190227311A1 (en) * | 2018-01-22 | 2019-07-25 | Symbol Technologies, Llc | Systems and methods for task-based adjustable focal distance for heads-up displays |
US10634913B2 (en) * | 2018-01-22 | 2020-04-28 | Symbol Technologies, Llc | Systems and methods for task-based adjustable focal distance for heads-up displays |
CN108810519A (en) * | 2018-03-26 | 2018-11-13 | 成都理想境界科技有限公司 | A kind of 3D rendering display equipment |
WO2019210254A1 (en) | 2018-04-27 | 2019-10-31 | Limbak 4Pi S.L. | Human vision-adapted light field displays |
EP3785066A4 (en) * | 2018-04-27 | 2021-07-28 | Limbak 4PI S.L. | Human vision-adapted light field displays |
CN112189161A (en) * | 2018-04-27 | 2021-01-05 | 林巴克4Pi有限公司 | Light field display adapted to human vision |
US11921302B2 (en) | 2018-04-27 | 2024-03-05 | Tesseland Llc | Human vision-adapted light field displays |
US11841988B2 (en) | 2018-10-22 | 2023-12-12 | Evolution Optiks Limited | Light field vision-based testing device, adjusted pixel rendering method therefor, and online vision-based testing management system and method using same |
US11327563B2 (en) | 2018-10-22 | 2022-05-10 | Evolution Optiks Limited | Light field vision-based testing device, adjusted pixel rendering method therefor, and online vision-based testing management system and method using same |
US11966507B2 (en) | 2018-10-22 | 2024-04-23 | Evolution Optiks Limited | Light field vision testing device, adjusted pixel rendering method therefor, and vision testing system and method using same |
US11619995B2 (en) | 2018-10-22 | 2023-04-04 | Evolution Optiks Limited | Light field vision-based testing device, adjusted pixel rendering method therefor, and online vision-based testing management system and method using same |
US11762463B2 (en) | 2018-10-22 | 2023-09-19 | Evolution Optiks Limited | Light field device, optical aberration compensation or simulation rendering method and vision testing system using same |
US11287883B2 (en) | 2018-10-22 | 2022-03-29 | Evolution Optiks Limited | Light field device, pixel rendering method therefor, and adjusted vision perception system and method using same |
US11726563B2 (en) | 2018-10-22 | 2023-08-15 | Evolution Optiks Limited | Light field device, pixel rendering method therefor, and adjusted vision perception system and method using same |
US11500460B2 (en) | 2018-10-22 | 2022-11-15 | Evolution Optiks Limited | Light field device, optical aberration compensation or simulation rendering |
US10996746B2 (en) | 2018-10-23 | 2021-05-04 | Microsoft Technology Licensing, Llc | Real-time computational solutions to a three-dimensional eye tracking framework |
US10718942B2 (en) | 2018-10-23 | 2020-07-21 | Microsoft Technology Licensing, Llc | Eye tracking systems and methods for near-eye-display (NED) devices |
US10855979B2 (en) | 2018-10-23 | 2020-12-01 | Microsoft Technology Licensing, Llc | Interpreting eye gaze direction as user input to near-eye-display (NED) devices for enabling hands free positioning of virtual items |
US10838490B2 (en) | 2018-10-23 | 2020-11-17 | Microsoft Technology Licensing, Llc | Translating combinations of user gaze direction and predetermined facial gestures into user input instructions for near-eye-display (NED) devices |
US10852823B2 (en) | 2018-10-23 | 2020-12-01 | Microsoft Technology Licensing, Llc | User-specific eye tracking calibration for near-eye-display (NED) devices |
CN109302599A (en) * | 2018-11-26 | 2019-02-01 | 体验科技股份有限公司 | A kind of method of three-dimensional imaging |
US11789531B2 (en) | 2019-01-28 | 2023-10-17 | Evolution Optiks Limited | Light field vision-based testing device, system and method |
US11543594B2 (en) | 2019-02-15 | 2023-01-03 | Digilens Inc. | Methods and apparatuses for providing a holographic waveguide display using integrated gratings |
US11176860B1 (en) * | 2019-03-05 | 2021-11-16 | Facebook Technologies, Llc | Systems and methods for transferring an image to an array of emissive subpixels |
US10867538B1 (en) * | 2019-03-05 | 2020-12-15 | Facebook Technologies, Llc | Systems and methods for transferring an image to an array of emissive sub pixels |
US11635617B2 (en) | 2019-04-23 | 2023-04-25 | Evolution Optiks Limited | Digital display device comprising a complementary light field display or display portion, and vision correction system and method using same |
US11899205B2 (en) | 2019-04-23 | 2024-02-13 | Evolution Optiks Limited | Digital display device comprising a complementary light field display or display portion, and vision correction system and method using same |
US11747568B2 (en) | 2019-06-07 | 2023-09-05 | Digilens Inc. | Waveguides incorporating transmissive and reflective gratings and related methods of manufacturing |
WO2021038468A1 (en) * | 2019-08-26 | 2021-03-04 | Evolution Optiks Limited | Light field dislay, adjusted pixel rendering method therefor, and adjusted vision perception system and method using same addressing astigmatism or similar conditions |
US11902498B2 (en) | 2019-08-26 | 2024-02-13 | Evolution Optiks Limited | Binocular light field display, adjusted pixel rendering method therefor, and vision correction system and method using same |
WO2021038421A1 (en) * | 2019-08-26 | 2021-03-04 | Evolution Optiks Limited | Light field vision testing device, adjusted pixel rendering method therefor, and vision testing system and method using same |
US11899238B2 (en) | 2019-08-29 | 2024-02-13 | Digilens Inc. | Evacuated gratings and methods of manufacturing |
US11592614B2 (en) | 2019-08-29 | 2023-02-28 | Digilens Inc. | Evacuated gratings and methods of manufacturing |
US11442222B2 (en) | 2019-08-29 | 2022-09-13 | Digilens Inc. | Evacuated gratings and methods of manufacturing |
US11823598B2 (en) | 2019-11-01 | 2023-11-21 | Evolution Optiks Limited | Light field device, variable perception pixel rendering method therefor, and variable perception system and method using same |
EP4052248A4 (en) * | 2019-11-01 | 2023-12-13 | Evolution Optiks Limited | Light field device, multi-depth pixel rendering method therefor, and multi-depth vision perception system and method using same |
US11487361B1 (en) | 2019-11-01 | 2022-11-01 | Evolution Optiks Limited | Light field device and vision testing system using same |
US11500461B2 (en) | 2019-11-01 | 2022-11-15 | Evolution Optiks Limited | Light field vision-based testing device, system and method |
WO2021138607A1 (en) * | 2020-01-03 | 2021-07-08 | Digilens Inc. | Modular waveguide displays and related applications |
US20230409109A1 (en) * | 2020-10-20 | 2023-12-21 | Rabbit Eyes B.V. | Vision correction display device, eye-tracking system and method to compensate for visual impairments |
NL2026709B1 (en) * | 2020-10-20 | 2022-06-16 | Rabbit Eyes B V | Display device |
WO2022084076A1 (en) | 2020-10-20 | 2022-04-28 | Rabbit-Eyes B.V. | Vision correction display device, eye-tracking system and method to compensate for visual impairments |
US20220236584A1 (en) * | 2021-01-22 | 2022-07-28 | National Taiwan University | Device of Generating 3D Light-Field Image |
US11947134B2 (en) * | 2021-01-22 | 2024-04-02 | National Taiwan University | Device of generating 3D light-field image |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20150262424A1 (en) | Depth and Focus Discrimination for a Head-mountable device using a Light-Field Display System | |
US10009542B2 (en) | Systems and methods for environment content sharing | |
US9274599B1 (en) | Input detection | |
US9147111B2 (en) | Display with blocking image generation | |
US9442631B1 (en) | Methods and systems for hands-free browsing in a wearable computing device | |
US10139623B2 (en) | Virtual object orientation and visualization | |
US9456284B2 (en) | Dual-element MEMS microphone for mechanical vibration noise cancellation | |
US9100732B1 (en) | Hertzian dipole headphone speaker | |
JP6337418B2 (en) | Head-mounted display device and method for controlling head-mounted display device | |
CN106489171B (en) | Stereoscopic image display | |
US9547175B2 (en) | Adaptive piezoelectric array for bone conduction receiver in wearable computers | |
WO2016063504A1 (en) | Head-mounted type display device and method for controlling same, and computer program | |
EP3714318B1 (en) | Position tracking system for head-mounted displays that includes sensor integrated circuits | |
US9541996B1 (en) | Image-recognition based game | |
US20140368537A1 (en) | Shared and private holographic objects | |
US20150169070A1 (en) | Visual Display of Interactive, Gesture-Controlled, Three-Dimensional (3D) Models for Head-Mountable Displays (HMDs) | |
JP6369005B2 (en) | Head-mounted display device and method for controlling head-mounted display device | |
KR20170041862A (en) | Head up display with eye tracking device determining user spectacles characteristics | |
US9336779B1 (en) | Dynamic image-based voice entry of unlock sequence | |
US20170090557A1 (en) | Systems and Devices for Implementing a Side-Mounted Optical Sensor | |
US9418617B1 (en) | Methods and systems for receiving input controls | |
JP2016091348A (en) | Head-mounted display device and control method for the same as well as computer program | |
US11237413B1 (en) | Multi-focal display based on polarization switches and geometric phase lenses | |
JP6683218B2 (en) | Head-mounted display device and control method for head-mounted display device | |
WO2021061551A1 (en) | Method and device for processing camera images |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:TABAKA, COREY;STRONG, JASMINE L.;SIGNING DATES FROM 20130129 TO 20130301;REEL/FRAME:029915/0552 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |