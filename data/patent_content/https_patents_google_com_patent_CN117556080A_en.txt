CN117556080A - Coordinating parallel processing of audio queries across multiple devices - Google Patents
Coordinating parallel processing of audio queries across multiple devices Download PDFInfo
- Publication number
- CN117556080A CN117556080A CN202311507585.1A CN202311507585A CN117556080A CN 117556080 A CN117556080 A CN 117556080A CN 202311507585 A CN202311507585 A CN 202311507585A CN 117556080 A CN117556080 A CN 117556080A
- Authority
- CN
- China
- Prior art keywords
- entity
- multimedia content
- data processing
- processing system
- remote data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012545 processing Methods 0.000 title claims abstract description 190
- 230000004044 response Effects 0.000 claims description 114
- 238000000034 method Methods 0.000 claims description 67
- 238000004590 computer program Methods 0.000 claims description 24
- 230000003993 interaction Effects 0.000 claims description 13
- 230000009471 action Effects 0.000 description 52
- 230000005236 sound signal Effects 0.000 description 41
- 238000004891 communication Methods 0.000 description 20
- 230000008569 process Effects 0.000 description 18
- 230000005540 biological transmission Effects 0.000 description 9
- 238000013515 script Methods 0.000 description 8
- 238000007726 management method Methods 0.000 description 6
- 238000009877 rendering Methods 0.000 description 6
- 230000003068 static effect Effects 0.000 description 6
- 238000010586 diagram Methods 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 230000000670 limiting effect Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000013528 artificial neural network Methods 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 230000033001 locomotion Effects 0.000 description 2
- 238000003058 natural language processing Methods 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000001360 synchronised effect Effects 0.000 description 2
- IRLPACMLTUPBCL-KQYNXXCUSA-N 5'-adenylyl sulfate Chemical compound C1=NC=2C(N)=NC=NC=2N1[C@@H]1O[C@H](COP(O)(=O)OS(O)(=O)=O)[C@@H](O)[C@H]1O IRLPACMLTUPBCL-KQYNXXCUSA-N 0.000 description 1
- 241001611138 Isma Species 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 238000013475 authorization Methods 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000003780 insertion Methods 0.000 description 1
- 230000037431 insertion Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/63—Querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/432—Query formulation
- G06F16/433—Query formulation using audio data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/90335—Query processing
- G06F16/90339—Query processing by using parallel associative memories or content-addressable memories
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/23—Updating
- G06F16/2379—Updates performed during online database operations; commit processing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/242—Query formulation
- G06F16/243—Natural language query formulation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/25—Integrating or interfacing systems involving database management systems
- G06F16/252—Integrating or interfacing systems involving database management systems between a Database Management System and a front-end application
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/68—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/38—Concurrent instruction execution, e.g. pipeline, look ahead
- G06F9/3885—Concurrent instruction execution, e.g. pipeline, look ahead using a plurality of independent parallel functional units
Abstract
The present disclosure relates generally to coordinating parallel processing of audio queries across multiple devices. A data processing system may receive an audio input signal detected at a display device and parse the audio input signal to identify an entity. The data processing system may send a query command to the display device to cause a multimedia content application to perform a search for the entity. The data processing system may access at least one of an address database and a multimedia content provider to identify a reference address for the entity. The data processing system may provide the reference address for the entity to cause the display device to present a content selection interface. The content selection interface may include an element for the reference address before the search for the entity performed by the multimedia content application is completed.
Description
Description of the division
The present application belongs to the divisional application of the Chinese patent application 201780092010.4 with the application date of 2017, 10, 03.
Technical Field
The present disclosure relates generally to coordinating parallel processing of audio queries across multiple devices.
Background
Packet-based or otherwise excessive network transmission of network traffic data between computing devices may prevent the computing devices from properly processing, completing operations related to, or responding to the network traffic data in a timely manner. Excessive network transmission of network traffic data can also complicate data routing or degrade the quality of the response if the responding computing device meets or exceeds its processing capabilities, which can lead to inefficient bandwidth utilization. A portion of the excessive network transmissions may include transmissions for requests that are not valid requests.
Disclosure of Invention
In accordance with one aspect of the disclosure, a system for coordinating parallel processing of an audio query-based search across multiple devices may include a data processing system. A natural language processor component executed by the data processing system may receive data packets via an interface that include audio input signals detected by a sensor of a display device. The natural language processor component can parse an audio input signal of the data packet to identify a request and an entity from the request. A search coordinator component executed by the data processing system may generate a query command including the request identified from parsing the audio signal input. The search coordinator component can send the query command to the display device via the interface such that a multimedia content application executing on the display device performs a search for the entity. A content indexer component executed by the data processing system can access at least one of an address database and a multimedia content provider to identify a reference address corresponding to the entity. A response handler component executed by the data processing system may provide the reference address for the entity to the display device via the interface, causing the display device to present a content selection interface. Before the search performed by the multimedia content application for the entity is completed, the content selection interface may include a selection element for a reference address of the entity and a placeholder element for the search performed by the multimedia content application.
In accordance with one aspect of the disclosure, a method of coordinating parallel processing of an audio query-based search across multiple devices may include: a natural language processor component executing on the data processing system receives data packets via the interface that include audio input signals detected by a sensor of the display device. The method may comprise: the natural language processor component parses an audio input signal of the data packet to identify a request and an entity from the request. The method may comprise: a search coordinator component executing on the data processing system obtains a query command including the request identified from parsing the audio signal input. The method may comprise: the search coordinator component sends the query command to the display device via the interface such that a multimedia content application executing on the display device performs a search for the entity. The method may comprise: a content indexer component executing on the data processing system accesses at least one of an address database and a multimedia content provider to identify a reference address corresponding to the entity. The method may comprise: a response handler component executing on the data processing system provides the reference address for the entity to the display device via the interface, causing the display device to present a content selection interface. Before the search performed by the multimedia content application for the entity is completed, the content selection interface may include a selection element for a reference address of the entity and a placeholder element for the search performed by the multimedia content application.
These and other aspects and embodiments are discussed in detail below. The foregoing information and the following detailed description include illustrative examples of various aspects and embodiments, and provide an overview or framework for understanding the nature and character of the claimed aspects and embodiments. The accompanying drawings provide a further understanding of the various aspects and embodiments, and are incorporated in and constitute a part of this specification.
Drawings
The drawings are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing.
In the drawings:
FIG. 1 illustrates an example system for coordinating parallel processing of audio queries across multiple devices.
Fig. 2 illustrates a signal flow process diagram of the system illustrated in fig. 1.
FIG. 3 illustrates a method for coordinating parallel processing of an audio query across multiple devices using the example system illustrated in FIG. 1.
FIG. 4 is a block diagram of an example computer system.
Detailed Description
The following is a detailed description of various concepts related to methods, apparatuses, and systems for coordinating parallel processing of audio queries across multiple devices, such as data processing systems, display devices, and client devices, and implementations of the methods, apparatuses, and systems. The various concepts introduced above and discussed in more detail below may be implemented in any of a number of ways.
The present disclosure is generally directed to a data processing system that coordinates parallel processing of audio queries across multiple devices. The systems and methods described herein may include a data processing system that may receive an audio input query, which may also be referred to herein as an audio input signal. Based on the audio input query, the data processing system may identify the request. The request may include an entity corresponding to the multimedia content. The digital assistant application may carry, obtain, respond to, or process commands extracted from the audio input query. The digital assistant application may be a program or script executing on a data processing system or a display device (e.g., a smart television, a hybrid television, or a set-top box connected to a television, etc.) and a client device (e.g., a smart phone, tablet, laptop, desktop, etc.) that interfaces with the data processing system. The digital assistant application may receive audio input queries, process requests associated with such queries using natural language processing algorithms, and then present audio responses in conversational fashion.
A multimedia content application executing on a display device may be used to play, stream, download, or otherwise access multimedia content provided by a multimedia content provider. However, searching for all possible sources of multimedia content in response to a query from a client multimedia content application may consume an excessive amount of time, resulting in latency in retrieving the results of the query. These may be several factors that affect the amount of time that the multimedia content application can retrieve the results of the query. For example, a multimedia content application may utilize a large amount of memory to initialize and then run to play multimedia content. Upon initialization, the multimedia content application may also perform account authentication with the multimedia content provider for the display device, resulting in more time being spent. Even in the event that initialization and authentication are complete, accessing the multimedia content provider from the display device to obtain a catalog of available multimedia content can result in additional time being spent. Furthermore, the display device may lack computing resources such as processing speed and memory for fast processing of the audio input signals containing entities. Under the combined action of these factors, it can be challenging for such multimedia content applications to respond in time to queries to determine whether a multimedia content provider can provide multimedia content.
By running the processing of queries in parallel on the display device and the data processing system, the present systems and methods described herein may reduce the amount of time to retrieve search results for audio-based queries. When the display device receives an audio input query, the display device may send a data packet with the audio input query to the data processing system. The data processing system may in turn process the audio input queries in the data packets using natural language processing techniques to identify the request and the entity from the request. The entity may include one or more words recognized from the audio input query. The one or more words of the entity may include information related to the multimedia content, such as title, author, publisher, release date, language, genre, length of time, and other associated keywords. Using the entities identified from the audio packets, the data processing system may generate and send a query command back to the display device to perform a search at the display device using one or more of the multimedia content applications. The query command may trigger the client device to initialize each multimedia content application, perform authentication with the multimedia content server, and then access a catalog of available multimedia content based on the entity.
In parallel with the search performed by the display device, the data processing system may perform its own search for the entity. The data processing system may access a data repository. The data repository may include a list of reference addresses (e.g., uniform resource locators) for the entities. The reference address may include a hostname corresponding to a particular multimedia content provider and a file pathname corresponding to an entity hosted on the multimedia content provider. A list of reference addresses may be maintained across multiple display devices by a data processing system using machine learning techniques and reference addresses from previous requests. The data processing system may also retrieve the results directly from the multimedia content provider by sending another query command. Upon receiving the query command, the multimedia content provider may access its own data repository to obtain a reference address corresponding to the entity.
Once the reference address for the entity is identified, the data processing system may send the search results to the display device. In response, the display device may present a display card. The display card may have a selection element for opening and playing the multimedia content identified by the reference address and a placeholder element for a search performed by the multimedia content application executing on the display device. Subsequent interactions with the placeholder element may cause the display device to replace the placeholder element with another reference address from a search performed locally by the multimedia content application.
The present approach may improve the efficiency and effectiveness of auditory data packet processing by coordinating audio-based query searches in parallel. Because the data processing system does not perform the initialization, authentication, or additional processes of the multimedia content application executing on the display device, the data processing system may retrieve results faster than the display device, thereby reducing latency. From a human-computer interaction (HCI) perspective, such parallel processing of audio-based search queries may result in a reduction in perceived latency in addition to a reduction in actual latency.
FIG. 1 illustrates an example system 100 for coordinating parallel processing of audio queries across multiple devices. The system 100 may include at least one data processing system 102, at least one multimedia content provider 106, one or more client devices 104, one or more display devices 108, and a network 110. The display device 108 may be coupled to the client device 104 via a wired or wireless connection (e.g., through the network 110, short-range communication, or pairing). The client device 104 may be part of the display device 108 or otherwise integrated into the display device. At least one data processing system 102, one or more client devices 104, and one or more display devices 108 may be communicatively coupled to one another via a network 110.
Data processing system 102 may include an interface 122. The data processing system 102 may include a digital assistant application 120. The data processing system 102 can include a Natural Language Processor (NLP) component 124 for parsing audio-based inputs. Data processing system 102 can include interface management component 128 for detecting and managing interfaces of other devices in system 100. The data processing system 102 can include an audio signal generator component 130 for generating an audio-based signal. The data processing system 102 may include a direct action Application Programming Interface (API) 126 for fulfilling requests parsed from the audio-based input. The data processing system 102 can include an audio signal generator component 130 for selecting a response to an audio-based input signal. Data processing system 102 may include data repository 140. The data processing system 102 can also include a search coordinator component 132, a content indexer component 134, a deep-chain model engine 136, and a response handler component 138.NLP component 124, interface management component 128, audio signal generator component 130, data repository 140, direct manipulation API 126, interface management component 128, search coordinator component 132, content indexer component 134, deep chain model engine 136, response handler component 138 can form digital assistant application 120.
The functionality of the data processing system 102, such as the digital assistant application 120, may be included or otherwise accessed from one or more client devices 104 and one or more display devices 108. The functionality of the data processing system 102 may correspond to the functionality of a digital assistant application 120 executing on the client device 104 or the display device 108, or interface with a digital assistant application 120 executing on the client device 104 or the display device 108. The client device 104 and the display device 108 may each include and execute separate instances of one or more components of the digital assistant application 120. Client device 104 and display device 108 may otherwise access the functionality of components of digital helper application 120 on remote data processing system 102 via network 110. For example, display device 108 may include the functionality of NLP component 124 and access the remainder of the components of digital assistant application 120 via network 110 to data processing system 102. The data processing system 102, client device 104, and display device 108 may comprise and execute separate instances of the components of the digital assistant application 120. The digital assistant applications 120 that are accessible or executable on the client device 104 and the display device 108 may each have different functionality.
Client device 104 and display device 108 may each include at least one logic device, such as a computing device having a processor to communicate with data processing system 102 via network 110. Client device 104 and display device 108 may include examples of any of the components described with respect to data processing system 102. Client device 104 may include a desktop computer, laptop computer, tablet computer, personal digital assistant, smart phone, mobile device, portable computer, client computer, virtual server, speaker-based digital assistant, or other computing device. Display device 108 may include a smart television, a hybrid television, a networked television, a set-top box connected to a television, a digital video recording, a monitor, a screen, or other computing device having display functionality. The client device 104 may be communicatively coupled with a display device 108. For example, once paired, client device 104 may act as a remote control to control various functionalities at display device 108. The client device 104 may be part of the display device 108 or integrated with the display device 108.
The components of system 100 may communicate over a network 110. The network 110 may include, for example, a point-to-point network, a broadcast network, a wide area network, a local area network, a telecommunications network, a data communication network, a computer network, an ATM (asynchronous transfer mode) network, a SONET (synchronous optical network) network, an SDH (synchronous digital hierarchy) network, an NFC (near field communication) network, a Local Area Network (LAN), a wireless network, or a wired network, and combinations thereof. Network 110 may include wireless links such as infrared channels or satellite bands. The topology of the network 110 may include a bus, star, or ring network topology. Network 110 may include a mobile telephone network that uses any one or more protocols for communication between mobile devices, including advanced mobile phone protocol (AMPS), time Division Multiple Access (TDMA), code Division Multiple Access (CDMA), global system for mobile communications (GSM), general Packet Radio Service (GPRS), or Universal Mobile Telecommunications System (UMTS). Different types of data may be sent via different protocols, or the same type of data may be sent via different protocols.
The network 110 may include a short-range communication link (e.g., ranging up to 30 meters) established between the client device 104 and the display device 108 using bluetooth, bluetooth low energy, dedicated short-range communication (DSRC), or Near Field Communication (NFC) protocols. Using such protocols, data processing system 102 can establish a communication link with one or more client devices 104 via interface 122. Data processing system 102 may establish a communication link with one or more display devices 108 via interface 122. A remote communication link may be established between the client device 104 and the display device 108 via a pairing protocol.
Client device 104 may also include sensor 152, speaker 150, interface 122, and transducer 154. The display device 108 may also include a sensor 152, a speaker 150, an interface 122, and a transducer 154. The client device 104 and the display device 108 may include at least one sensor 152, at least one transducer 154, at least one audio driver, and at least one speaker 150. The sensor 152 may include a microphone or an audio input sensor. The sensor 152 may also include at least one of a GPS sensor, a proximity sensor, an ambient light sensor, a temperature sensor, a motion sensor, an accelerometer, or a gyroscope. The sensor may comprise an occupancy or weight sensor. The transducer 154 may convert the audio input into an electronic signal. The audio driver may include scripts or programs that are executed by one or more processors of the client device 104 or the display device 108 to control the speaker 150. Speaker 150 may render audio signals by converting electrical signals into audible waves. The client device 104 and the display device 108 may each include peripheral devices such as a keyboard, pointing device, monitor (built-in or stand-alone) and headphones, among other devices.
The client device 104 and the display device 108 may each execute an instance of the multimedia content application 160. The multimedia content application 160 may be associated with a particular multimedia content provider 106. The multimedia content application 160 may include scripts or programs installed at the client device 104 or the display device 108. The multimedia content application 160 may include scripts or programs executable via another application (such as a web browser) installed at the client device 104 or the display device 108. The multimedia content application 160 may include an Application Programming Interface (API) that may allow other applications at the client device 104 or the display device 108 (e.g., the digital assistant application 120) to interface with the multimedia content application 160. The multimedia content application 160 may play, stream, download, or otherwise access the multimedia content. The multimedia content may correspond to one or more audiovisual content files in any format, such as MP3, ACC, OPUS, RTMP, RTP, MP4, FLV, webM, ASF, ISMA, HEVC, h.264, VP8, VP9, HLS, HDS, SMIL, and the like. The one or more audiovisual content files may include audio and/or visual content for playback at the client device 104 or the display device 108. Upon loading one or more audiovisual content files, the multimedia content application 160 may play or stream the multimedia content on the client device 104 or the display device 108.
The multimedia content application 160 may also perform an authentication process with the multimedia content provider 106. The authentication process may include identification of an account identifier and an access code that are retrieved at the client device 104 or the display device 108 via input or from storage. Upon receipt, the multimedia content application 160 may send the account identifier and the access code to the multimedia content provider 106. The multimedia content provider 106 may in turn compare its own stored access code for the account identifier with the access code received from the client device 104 or the display device 108. In response to determining a match between the two access codes, the multimedia content provider 106 may send a successful authentication indicator to the client device 104 or the display device 108.
In response to receiving the successful authentication indicator, the multimedia content application 160 can retrieve a catalog of available multimedia content provided by the multimedia content provider 106 for the account identifier. The multimedia content application 160 may display a catalog of available multimedia content in a graphical user interface with each element corresponding to each available multimedia content. Upon interaction with one of the elements of the graphical user interface, the multimedia content application 160 may send a request for the selected multimedia content to the multimedia content provider 106. The multimedia content provider 106 may identify and provide one or more audiovisual content files corresponding to the selected multimedia content. The multimedia content application 160, in turn, may store, play, or stream one or more audiovisual content files corresponding to the selected multimedia content at the client device 104 or the display device 108. Additional details regarding the functionality of the multimedia content provider 106 and the multimedia content application 160 in the context of the system 100 are described herein.
An application, script, program, or other component associated with data processing system 102 can be installed at client device 104 or display device 108. The application may enable the client device 104 or the display device 108 to communicate input audio signals (and other data) to the interface 122 of the data processing system 102. The application may enable the client device 104 and the display device 108 to drive components of the client device 104 and the display device 108 to render the output audio signals.
The client device 104 and the display device 108 may be associated with an end user that inputs voice queries as input audio signals into the client device 104 or the display device 108 (via the sensor 152) and receives audio output in the form of computer-generated voice that may be provided from the data processing system 102. In response to the input audio signals, the client device 104 and the display device 108 may also receive action data structures for performing predetermined functions or actions. Interface 122 may receive data messages or provide data messages to direct action API 126 of data processing system 102 and enable communication between components of system 100. The client device 104 and the display device 108 may also include user interfaces that enable a user to interact with the components of the system 100.
Data processing system 102 may include at least one server having at least one processor. For example, data processing system 102 may include multiple servers located in at least one data center or server farm. The data processing system 102 may determine a request and trigger keywords associated with the request from the audio input signal. Based on the request and the trigger keywords, data processing system 102 may generate or select response data. The response data may be audio-based or text-based. For example, the response data may include one or more audio files that, when rendered, provide audio output or sound waves. The data within the response data may also be referred to as content items. In addition to audio content, the response data may also include other content (e.g., text, video, or image content).
Data processing system 102 may include multiple logically grouped servers and facilitate distributed computing techniques. The logical group of servers may be referred to as a data center, a server farm, or a machine farm. The servers may be geographically dispersed. The data center or machine farm may be managed as a single entity or the machine farm may include multiple machine farms. The servers within each computer field may be heterogeneous—one or more of these servers or machines may operate in accordance with one or more types of operating system platforms. Data processing system 102 may include servers in a data center that are stored in one or more high-density rack systems and associated storage systems located, for example, in an enterprise data center. In this manner, data processing system 102 with a consolidated server may improve manageability, data security, physical security, and system performance of the system by locating the servers and high-performance storage systems on a localized high-performance network. Centralization of all or some of the data processing system 102 components, including servers and storage systems, and coupling them together with advanced system management tools allows for more efficient use of server resources, which saves power and processing requirements and reduces bandwidth usage. Each of the components of data processing system 102 may include at least one processing unit, server, virtual server, circuit, engine, agent, appliance, or other logic device, such as a programmable logic array configured to communicate with data repository 140 and with other computing devices.
Data processing system 102 may include data repository 140. The data repository 140 may include one or more local or distributed databases and may include a database management system. The data repository 140 may comprise a computer data storage or memory and may store one or more application identifiers 142, one or more entity metadata 144, one or more reference addresses 146, and an address model 148. Each application identifier 142 may be an alphanumeric value corresponding to a multimedia content application 160 to be executed on the client device 104 or the display device 108. Each entity metadata 144 may correspond to an entity. Each reference address 146 may identify or reference. Details of the use and functionality of the one or more application identifiers 142, the one or more entity metadata 144, the one or more reference addresses 146, and the address model 148 are provided below.
The data repository 140 may include a computer data storage or memory and may store one or more parameters, one or more policies, response data and templates, and other data. The parameters, policies, and templates may include information such as rules regarding voice-based sessions between client device 104, data processing system 102, and display device 108. The parameters, policies, and templates may also include information for another digital assistant application 120 received via interface 122 from another source (e.g., data processing system 102, client device 104, and display device 108). For example, parameters, policies, and templates stored in the data repository 140 of the digital assistant application 120 hosted on the client device 104 or the display device 108 may include parameters, policies, and templates from the data repository 140 of the digital assistant application 120 accessible via the client device 104 and the display device 108, and vice versa. In this way, the parameters, policies, and templates of the different digital assistant applications 120 may be shared and used with each other. The response data may include content items for audio output or associated metadata and input audio messages that may be part of one or more communication sessions with the client device 104 and the display device 108.
NLP component 124 may receive an input audio signal. The data processing system 102 may receive an input audio signal included in a data packet (e.g., via the transducer 154 or the sensor 152) from the client device 104 or the display device 108. The data packet may also include a device identifier associated with the client device 104 or the display device 108. The data packet may also include an application identifier of the multimedia content application 160 executable at the client device 104 or the display device 108. The functionality of NLP component 124 may be split between data processing system 102, client device 104, and display device 108. For example, the NLP component 124 executing on the client device 104 or the display device 108 may encapsulate the input audio signal detected at the sensor 152 into data packets, and may send the data packets to the data processing system 102 for further processing at the NLP component 124 executing at the data processing system 102.
The NLP component 124 may convert the input audio signal into recognized text by comparing the input audio signal against a stored set of representative audio waveforms and selecting the closest match. A representative waveform may be generated across a large set of input audio signals. Once the input audio signal is converted to recognized text, the NLP component 124 can match the text with words associated with the action or output audio signal, for example, via a learning phase.
From the input audio signal, NLP component 124 can identify at least one request. The request may indicate an intent or topic of the input audio signal. The request may indicate the type of action that is likely to be taken. For example, the NLP component 124 may parse the input audio signal to identify at least one request to play the multimedia content (e.g., "Okay, play Bat Movie form 2015 (good, playing bat movies since 2015)"). The request may include at least one word, phrase, root or partial word or derivative indicating an action to be taken. The request may also contain a trigger key, such as "Okay" or "go". NLP component 124 can detect trigger keywords in a request. In response to detection of the trigger keywords, NLP component 124 can identify from the input audio signal the intent, topic, and type of action to take.
NLP component 126 can also identify at least one entity from the request. The at least one entity may correspond to at least one word, phrase, root or partial word or derivative in the request. The at least one entity may include descriptors for the multimedia content, such as title, author, publisher, original release date, language, genre, length of time, and other associated keywords (e.g., actor names). For example, entities for the "Bat Movie I (Bat Movie I)" Movie released in 2005 may include: "Bat Movie I" as a title, "ms.director" as an author, "June 15,2005 (6 th 15 th 2005)" as an original release date, "englist" as a language, "Action" as a genre, and "140 minutes" as a length, and other information. The NLP component 126 can determine that at least one word, phrase, root or partial word or derivative in the request corresponds to at least one entity. To perform the determination, the NLP component 126 can access a semantic knowledge graph. The semantic knowledge graph may specify a set of words, phrases, root or partial words, or derivatives related to at least one entity. The semantic knowledge graph may include nodes connected to each other via edges. A node may be associated with a word, phrase, root or partial word or derivative, and may be labeled as related or unrelated to at least one entity. Each edge connecting two nodes may represent a relationship between the two. Continuing from the previous example, a node for "Bat Movie" in the semantic knowledge graph may be marked as related to an entity, and an edge to a node marked as "Bat Movie I (Bat Movie I)" may also be marked as related to an entity indicating a relationship between the two items. Using the semantic knowledge graph, NLP component 126 can determine that at least one word, phrase, root or partial word or derivative in the request corresponds to at least one entity.
The NLP component 124 can also determine whether at least one request corresponds to a query for multimedia content based on an entity. In response to identifying that at least one word, phrase, root or partial word or derivative in the request corresponds to at least one entity, NLP component 124 can determine that the at least one request corresponds to a query for multimedia content. In response to identifying that none of the words, phrases, roots, or partial words or derivatives in the request corresponds to at least one entity, NLP component 124 may determine that at least one request does not correspond to a query for multimedia content. As described below, the determination that at least one request corresponds to a query for multimedia content may trigger the functionality of the search coordinator component 132, the content indexer component 134, the deep-chain model engine 136, and the response handler component 138.
The audio signal generator component 130 can obtain information from the data repository 140, where it can be stored as part of the response data. The audio signal generator component 130 can query the data repository 140 to select or otherwise identify a response phrase or content item, for example, from the response data. The audio signal generator component 130 can generate or otherwise obtain an output signal comprising the content item. The data processing system 102 can execute the audio signal generator component 130 to generate or create an output signal corresponding to a content item or request. For example, once the request is fulfilled, the audio signal generator component 130 may generate an audio output signal that includes the phrase "The action was completed (action completed)".
Interface 122 may be a data interface or a network interface that enables components of system 100 to communicate with each other. Interface 122 of data processing system 102 may provide or transmit one or more data packets including action data structures, audio signals, or other data to client device 104 or display device 108 via network 110. For example, the data processing system 102 can provide output signals from the data repository 140 or from the audio signal generator component 130 to the client device 104. Data processing system 102 can also instruct client device 104 or display device 108 via data packet transmission to perform the functions indicated in the action data structure. An output signal may be obtained, generated, converted to one or more data packets (or other communication protocols), or transmitted from the data processing system 102 (or other computing device) to the client device 104 or the display device 108 as one or more data packets (or other communication protocols).
The direct action API 126 of the data processing system 102 may generate an action data structure based on, for example, the request. The action data structure may include data or instructions for performing specified actions to satisfy the request. In some implementations, the action data structure can be a JSON formatted data structure or an XML formatted data structure.
Depending on the action specified in the request, the direct action API 126 may execute code or dialog scripts that identify parameters needed to fulfill the request. The action data structure may be generated in response to a request. The action data structure may be included in a message sent to the client device 104 or the display device 108 or received by the client device 104 or the display device 108. The direct action API 126 may encapsulate the request into an action data structure for transmission to the display device 108. The action API 126 may access the device identifier from the response data to determine which of the client device 104 or the display device 108 is associated with the user that generated the request. Once received, the display device 108 may process the action data structure and may perform the indicated action. The direct action API 126 may also encapsulate the request into an action data structure for execution by the client device 104 or the display device 108. Once received, the client device 104 may process the action data structure using the digital assistant application 120 or one or more applications running on the client device 104.
The action data structure may include information for completing the request. For example, the action data structure may be an XML or JSON formatted data structure that includes attributes that are used when completing or otherwise fulfilling a request. The attributes may include a location of the display device 108, a location of the client device 104, an authorization level of a user associated with the client device 104, a device identifier, an interface identifier, a vehicle status, or a request status. In some implementations, the request state includes one or more attributes that should be satisfied before the action is fulfilled. For example, in the case of a request "Ok, change the song," the request status may have the attribute { requestor: [ authorized, passenger ] }, indicating that the request should be an explicitly authorized user.
The direct action API 126 may retrieve templates from the data repository 140 to determine which fields or attributes to include in the action data structure. The direct action API 126 may determine the necessary parameters and may encapsulate the information into an action data structure. The direct action API 126 may retrieve content from the data repository 140 to obtain information for attributes of the data structure.
The direct action API 126 may populate fields with data from the input audio signal. The direct action API 126 may also populate fields with data from the client device 104 or the display device 108, or from another source. The direct action API 126 may prompt the user for additional information when populating the fields. Templates may be normalized for different types of actions, responsive to messages, and performing functions within client device 104 or display device 108. The action data structure may be initially generated by a direct action API 126 executed by remote data processing system 102. Remote data processing system 102 can send the action data structure to client device 104 or display device 108, which can add fields and attributes to the action data structure.
The direct action API 126 may obtain response data (or parameters or policies) from the data repository 140, as well as data received from the client device 104 or the display device 108 with the end user's consent. The response data (or parameters or policies) may be included in the operation data structure. When the content included in the action data structure includes end user data that is used for authentication, the data may be parsed by a hash function before being stored in the data repository 140.
The data processing system 102 can include a search coordinator component 132 that interfaces with the search coordinator component 132 or otherwise communicates with the search coordinator component 132. The search coordinator component 132 can generate query commands to be sent to the client device 104 or the display device 108. Generating the query command by the search coordinator component 132 can correspond to a query for multimedia content in response to determining that the request corresponds to a query for multimedia content. The query command may include an entity identified from the response. The query command may also include instructions for the client device 104 or the display device 108 to process the query command. The instructions may instruct which multimedia content application 160 on the client device 104 or the display device 108 will perform a search for an entity. To generate the instructions, the search coordinator component 132 can access the data repository 140 to identify one or more application identifiers 142 to be included in the query command. The data repository 140 may identify one or more application identifiers 142 by a device identifier associated with the client device 104 or the display device 108. Each application identifier 142 may also be tagged by the frequency used by the client device 104 or the display device 108. The search coordinator component 132 can also insert one or more application identifiers 142 in the initial data packet into the instructions of the query command. The search coordinator component 132 can identify a subset of application identifiers 142 for the client device 104 and the display device 108 based on the frequencies used by the client device 104 and the display device 108. For example, the search coordinator component 132 may identify that the display device 108 uses the multimedia content applications 160"a" and "B" more frequently than the multimedia content applications 160"c", "D", and "E". In this example, the search coordinator component 132 can select an application identifier 142 corresponding to the multimedia content application 160 for insertion into the query command. Each application identifier 142 of the query command may indicate which multimedia content application 160 on the client device 104 or display device 108 is to perform a search for an entity. Following generation of the query command, the search coordinator 132 may send the query command to the client device 104 or the display device 108.
Receipt of the query command may cause the client device 104 or the display device 108 to perform a search for an entity to the multimedia content application 160. In response to receipt of the query command, the client device 104 or the display device 108 may parse the query command to identify the entity. The client device 104 or the display device 108 may parse the query command to also identify one or more application identifiers 142. The client device 104 or the display device 108 may identify the multimedia content application 160 corresponding to the one or more application identifiers 142 of the query command. The client device 104 or the display device 108 may initiate execution of the multimedia content application 160 identified from the one or more application identifiers 142 of the query command. To perform the search, the client device 104 or the display device 108 may provide at least one entity in the query command to each multimedia content application 160 via an application programming interface for the multimedia content application 160. Once the entities are provided, each entity multimedia content application 160 may send a query request to the associated multimedia content provider 106.
In parallel with the generation of the query command and the transmission to the client device 104 or the display device 108, the search coordinator component 132 can also generate a query request to be transmitted to the multimedia content provider 106. Generating the query request by the search coordinator component 132 may be in response to determining that the request corresponds to a query for multimedia content. The generation of the query request by the search coordinator component 132 may be in response to a call made by the content indexer component 134, as will be discussed below. The query request may include an entity identified from the response. The query request may also include instructions for the multimedia content provider 106 to process the query request. The search coordinator 132 may identify which multimedia content provider 106 is to send the query request from one or more application identifiers 142 for the client device 104 or the display device 108. Following generation of the query request, the search coordinator 132 may send the query request to the multimedia content provider 106.
Receipt of a query request from the client device 104, the display device 108, or the search coordinator component 132 may cause the multimedia content provider 106 to perform a search for an entity. Because the client device 104 or the display device 108 may have initialized the multimedia content application 160 to send the query request, the query request from the client device 104 or the display device 108 may be received subsequent to the query request from the search coordinator component 132. The multimedia content provider 106 may access the content data repository 162 in response to a query request. Content data repository 162 may include one or more entity metadata 144 and one or more reference addresses 146. Each entity metadata 144 may correspond to an entity. Each entity metadata 144 on the data repository 162 may include one or more entries for multimedia content corresponding to the entity, such as title, author, publisher, original release date, language, genre, length of time, and other associated keywords. For example, one entity metadata 144 may correspond to "Bat Movie I" and another entity metadata 144 may correspond to "Bat Movie II". Each entity metadata 144 may also be associated with a reference address 146 (e.g., a uniform resource locator). The reference address 146 may include a hostname, a file path name, and a query parameter. The hostname may correspond to the multimedia content provider 106. The file pathname may correspond to one or more audiovisual files for the multimedia content corresponding to the entity. The query parameters may include attribute-value pairs. The query parameters may be dynamically generated by the multimedia content provider 106 and may vary per client device 104, display device 108, or search coordinator component 132.
Based on the entity from the query request, the multimedia content provider 106 may identify a reference address 146 corresponding to the entity. The multimedia content provider 106 may traverse the content data repository 162. For each entity metadata 144, the multimedia content provider 106 may compare the entity to the entity metadata 144. For example, the multimedia content provider 106 may compare the entity "Bat Movie" with any of the entries for the entity metadata 144, such as a title. The multimedia content provider 106 may determine whether one or more entries of the entity metadata 144 match an entity from the query request. The multimedia content provider 106 may determine that there is no match between the entity and the entity metadata 144. In response to determining that no match exists, the multimedia content provider 106 may continue traversing the content data repository 162. The multimedia content provider 106 may determine that there are no more entity metadata 144 to traverse. In response to determining that there are no more entity metadata 144 to traverse, multimedia content provider 106 may determine that an entity is not present on content data repository 162. The multimedia content provider 106 may also send a null response. The null response may indicate that the multimedia content provider 106 does not have multimedia content corresponding to the entity.
Conversely, the multimedia content provider 106 may determine that there is a match between the entity and the entity metadata 144. In response to the determination of a match, the multimedia content provider 106 may determine that an entity exists on the content data repository 162. The multimedia content provider 106 may identify a reference address 146 corresponding to entity metadata 144 that matches the entity. The multimedia content provider 106 may generate a search response. The search response may indicate that the multimedia content provider 106 does have multimedia content corresponding to the entity. The search response may include a reference address 146 corresponding to the multimedia content for the entity.
The multimedia content provider 106 may continue traversing the content data repository 162 to identify additional matches between the entity of the query request and the entity metadata 144. In the case of additional matches, the multimedia content provider 106 may repeatedly identify the reference address 146 corresponding to the entity metadata 144 that matches the entity and may include the reference address 146 into the search response. The multimedia content provider 106 may determine that there are no more entity metadata 144 to traverse. In response to determining that there are no more entity metadata 144 to traverse, the multimedia content provider 106 may send a response to the client device 104, the display device 108, or the search coordinator component 132 that sent the query search.
The data processing system 102 can include a content indexer component 134 that interfaces with the content indexer component 134 or otherwise communicates with the content indexer component 134. The content indexer component 134 can access a data repository 140 of the data processing system 102 or multimedia content provider 106 to identify a reference address 146 corresponding to an entity. The content indexer component 134 can access the data repository 140 prior to accessing the multimedia content provider 106 to identify the reference address 146. The content indexer component 134 can traverse the data repository 140. In addition to the application identifier 142, the data repository 140 may also include one or more entity metadata 144 and one or more reference addresses 146. Entity metadata 144 and reference address 146 on data repository 140 may be maintained separately and independently from entity metadata 144 and reference address 146 on content data repository 162. Entity metadata 144 and reference address 146 on data repository 140 may be derived from previous search responses from multimedia content provider 106. The content indexer component 134 can maintain a timer to track the elapsed time for each entity metadata 144 and associated reference address 146 in the storage at the data repository 140.
The content indexer component 134 can traverse the content data repository 140. For each entity metadata 144, the content indexer component 134 can compare the entity to the entity metadata 144. For example, the content indexer component 134 can compare the entity "Bat Movie" to any of the entries for the entity metadata 144, such as the title. The content indexer component 134 can determine if one or more entries of entity metadata 144 match an entity from the query request. The content indexer component 134 can determine that there is no match between the entity and the entity metadata 144. In response to determining that there is no match, the content indexer component 134 can continue traversing the content data repository 140. The content indexer component 134 can determine that there are no more entity metadata 144 to traverse. In response to determining that there are no more entity metadata 144 to traverse, the content indexer component 134 can determine that an entity is not present on the content data repository 140. In response to determining that the entity is not present on the content data repository 140, the content indexer component 134 can invoke the search coordinator component 132 to send a query request to the multimedia content provider 106 to retrieve the reference address 146.
Conversely, the content indexer component 134 can determine that there is a match between the entity and the entity metadata 144. In response to the determination of a match, the content indexer component 134 can determine that an entity exists on the content data repository 140. The content indexer component 134 can identify a reference address 146 corresponding to entity metadata 144 that matches the entity. The content indexer component 134 can store reference addresses 146 corresponding to the entities.
The content indexer component 134 can identify the elapsed time for the identified reference address 146. The content indexer component 134 can compare the elapsed time to a preset expiration time. The preset expiration time may range from 1 hour to 1 month because the multimedia content provider 106 may periodically update or change the catalog of available multimedia content. The preset expiration time may correspond to an amount of time that the reference address 146 is valid for an entity at the multimedia content provider 106. The content indexer component 134 can determine that the elapsed time is less than the preset expiration time. In response to determining that the elapsed time is less than the preset expiration time, the content indexer component 134 can store reference addresses 146 corresponding to the entities. The content indexer component 134 can also terminate access to the multimedia content provider 106 to search for a reference address corresponding to the entity.
On the other hand, the content indexer component 134 can determine that the elapsed time is greater than or equal to the preset expiration time. In response to determining that the elapsed time is greater than or equal to the preset expiration time, the content indexer component 134 can replace the reference address 146 from the data repository 140. To update the data repository 140, the content indexer component 134 can invoke the search coordinator component 132 to send a query request to the multimedia content provider 106 to retrieve the reference address 146. The content indexer component 134 can receive a search response from the multimedia content provider 106. The search response may include a new reference address 146 for the entity. In response to receiving the reference address 146 from the multimedia content provider 106, the content indexer component 134 can save the reference address 146 onto the data repository 140 and can associate the newly saved reference address 146 with entity metadata 144 that matches the entity. The content indexer component 134 can receive the null response from the multimedia content provider 106. In response to receipt of the empty response, the content indexer component 134 can remove the reference address 146 from the data repository 140.
The content indexer component 134 can continue traversing the content data repository 140 to identify additional matches between the entity of the query request and the entity metadata 144. In the case of additional matches, the content indexer component 134 can repeatedly identify the reference address 146 corresponding to the entity metadata 144 that matches the entity and can include the reference address 146 into the search response. The content indexer component 134 can determine that there are no more entity metadata 144 to traverse. In response to determining that there are no more entity metadata 144 to traverse, the content indexer component 134 can terminate the search of the additional reference address 146 at the data repository 140. Following traversal, the content indexer component 134 can also invoke the search coordinator component 132 to send a query request to the multimedia content provider 106 to retrieve the reference address 146. The invocation of the search coordinator component 132 may be regardless of the comparison of the elapsed time with the preset expiration time described above.
Following invocation of the search coordinator component 132 to send the query request, the content indexer component 134 can receive a search response from the multimedia content provider 106. The receipt of a search response from the multimedia content provider 106 may indicate that the multimedia content provider 106 is capable of accessing the entity. The search response may include a reference address 146 corresponding to the entity. Content indexer component 134 can parse the search response to identify reference address 146. In response to identifying a match between an entity and entity metadata 144 on data repository 140, content indexer component 134 can compare reference address 146 from data repository 140 to reference address 146 from multimedia content provider 106. The content indexer component 134 can determine that the reference address 146 from the data repository 140 is different from the reference address 146 from the multimedia content provider 106. The difference in the reference address 146 may indicate that the multimedia content provider 106 has updated the reference address 146 for the entity. In response to determining that the reference addresses 146 are different, the content indexer component 134 can replace the reference addresses 146 stored at the data repository 140 with the reference addresses 146 from the multimedia content provider 106. The content indexer component 134 can determine that the reference address 146 from the data repository 140 is the same as the reference address 146 from the multimedia content provider 106. In response to determining that the reference addresses 146 are the same, the content indexer component 134 can continue to invoke the response handler component 138 to send query results to the client device 104 or the display device 108, as described below.
The content indexer component 134 can also receive a null response from the multimedia content provider 106. Receipt of a null response from the multimedia content provider 106 may indicate that the multimedia content provider 106 no longer has the entity. For example, the multimedia content provider 106 may have updated a catalog of available multimedia content to exclude the entity. In response to receipt of the empty response and in response to identification of a match between the entity and entity metadata 144 on data repository 140, content indexer component 134 can remove reference address 146 from data repository 140. As described below, the content indexer component 134 can continue to invoke the response handler component 138 to send query results to the client device 104 or the display device 108. In some cases, the content indexer component 134 may not receive a response from the multimedia content provider 106 within a predetermined amount of time following transmission of the query response, for example, due to a problem with the network 110 or downtime at the multimedia content provider 106. In such cases, the content indexer component 134 can maintain the reference address 146 at the data repository 140.
The data processing system 102 may include a deep-chain model engine 136 that interfaces with the deep-chain model engine 136 or otherwise communicates with the deep-chain model engine 136. When the content indexer component 132 receives a search response from the multimedia content provider 106, the deep-chain model engine 136 can maintain an address model 148 for the multimedia content provider 106 to generate an address structure for the reference address 146. The address model 148 may be an Artificial Neural Network (ANN), a bayesian model, a gaussian mixture model, a support vector machine, or a decision tree, etc., to represent and identify the address structure of the reference address 148 for the multimedia content provider 106. The reference address 148 received by the content indexer component 132 may be a depth chain. For example, a depth chain may have the form "www.example _mp.com/video33123&987" or "e_mp:// stream? =vid45670). The reference address 148 may include a hostname corresponding to the multimedia content provider 106, a file path name to one or more audiovisual content files for the multimedia content corresponding to the entity, and a query string. In the former example, the hostname may be "example_mp.com", the pathname may be "/video33", and the query string may be "?123&987". Even for the same one or more audiovisual content files, the subset of characters forming the file path name or query string of reference address 148 may vary based on the properties of the recipient device (e.g., client device 104, display device 108, and data processing system 102). Examples of attributes may include account identifier, device type, resolution of display, location identifier, platform application (e.g., web browser for running multimedia content application 106), operating system, and so forth.
The deep-chain model 136 may use the reference address 146 received from the multimedia content provider 106 as well as attributes of the client device 104, the display device 108, and the data processing system 102 as inputs to the address model 148. Using the input of the address model 148, the deep-chain model engine 136 may train the address model 148. In training address model 148, depth chain model 136 may maintain comparators to track which indexes of characters vary across reference addresses 148 for multimedia content provider 106.
Using the address model 148, the deep-chain model engine 136 may generate an address structure for the reference address 146 of the multimedia content provider 106. Using the address model 148, the deep-chain model engine 136 may also determine an address structure for the newly received reference address 146 of the multimedia content provider 106. The address structure may specify an index of characters that are identified as static across the reference address 146 for the multimedia content provider 106. The address structure may specify an index of characters that are identified as variable across the reference address 146 for the multimedia content provider 106. For each newly received reference address 146, the deep chain model engine 136 may remove the identified static portion based on the index of characters identified as static. For each newly received reference address 146, the deep chain model engine 136 may identify variant parts based on the index of characters identified as variable. The deep chain model engine 136 may maintain characters of the reference address 148 corresponding to the static portion. The deep chain model engine 136 may remove the characters of the reference address 148 corresponding to the variant portion. The deep-chain model engine 136 may then add a reference address 148 having a character corresponding to the removed variant portion to the data repository 140.
The data processing system 102 may include a response handler component 138, may interface with the response handler component 138, or may otherwise communicate with the response handler component 138. Responsive to identifying the reference address 148 corresponding to the entity in the request, the response handler component 138 can send or provide the reference address 148 to the client device 104 or the display device 108. The response handler component 138 may also generate instructions for rendering the content selection interface at the client device 104 or the display device 108. The content selection interface may include one or more selection elements and placeholder elements (sometimes collectively referred to as "display cards"). The one or more selection elements may correspond to a reference address 148 for an entity included in the request of the client device 104 or the display device 108. The search performed by the multimedia content application 160 may not have been completed before the response handler component 138 sends the reference address 148 to the client device 104 or the display device 108. Thus, the content selection interface may initially exclude results from searches performed by the multimedia content application 160 executing on the client device 104 or the display device 108. The response handler component 138 may provide instructions for rendering the content selection interface to the client device 104 or the display device 108 along with the reference address 148.
Subsequent to rendering the content selection interface at the client device 104 or the display device 108, interaction with the placeholder element may cause the client device 104 or the display device 108 to determine whether a search performed by the multimedia content application 160 is complete. The client device 104 or the display device 108 may determine that the search performed by the multimedia content application 160 is complete. In response to determining that the search is complete, the client device 104 or the display device 108 may modify the content selection interaction to include additional selection elements. Each additional selection element may correspond to a reference address 148 found during a search performed by the multimedia content application 160 executing on the client device 104 or the display device 108. Interaction with the selection element of the content selection element may cause the client device 104 or the display device 108 to launch the multimedia content application 160 (or another application running on the client device 104 or the display device 108) to play one or more audiovisual files corresponding to the reference address 148 of the associated selection element.
On the other hand, the client device 104 or the display device 108 may determine that the search performed by the multimedia content application 160 has not been completed. In response to determining that the search has not been completed, the client device 104 or the display device 108 may wait for a predetermined amount of time. During the predetermined amount of time, the client device 104 or the display device 108 may determine that the search performed by the multimedia content application 160 is complete. In response to determining that the search is complete, the client device 104 or the display device 108 may modify the content selection interaction to include additional selection elements. The client device 104 or the display device 108 may determine that a predetermined amount of time has elapsed and that the search has not been completed. In response to determining that the predetermined amount of time has elapsed and the search has not been completed, the client device 104 or the display device 108 may modify the content selection interface to remove the placeholder element.
Fig. 2 illustrates a signal flow process diagram 200 of the system illustrated in fig. 1. In response to detection of the audio input query, the display device 108 may send a data packet 202 with the audio input query to the data processing system 102. The data processing system 102 may in turn parse the audio input query and determine that the audio input query corresponds to a query request for multimedia content. The data processing system 102 can also identify entities in the query request (e.g., words related to the multimedia content). The data process 102 sends a query command 204 to the display device 108 to trigger the display device 108 to initialize the multimedia content application 160. Upon initializing the multimedia content application 160, the display device 108 may allocate memory to perform authentication and other processes with the multimedia content provider 106. After initialization, the multimedia content application 160 may begin searching for entities.
At the same time, data processing system 102 may search through a local cache (e.g., data repository 140) to identify a reference address 148 corresponding to the entity and may send a query request 206 to multimedia content provider 106. The query request 206 may include an entity identified from the audio input query. The multimedia content provider 106 may access its own data repository 162 to identify the reference address 148 corresponding to the entity 148. The multimedia content provider 106 may then return a search response 208 to the data processing system 102. The data processing system 102 may generate instructions for rendering the content selection interface 216 at the display device 108. The content selection interface 216 may include a selection element 218 and a placeholder element 220. The select element 218 may be associated with a reference address 148 found during a search by the data processing system 102. At time T 1 The data processing system 102 may send the instructions 212 to the display device 108. The display device 108 may then present the content selection interface 216.
At the display device 108, after initialization, the multimedia content application 160 may begin searching for entities. The multimedia content application 160 may send a query request 210 to the multimedia content provider 106. The multimedia content provider 106 may access its own data repository 162 to identify the reference address 148 corresponding to the entity 148. At time T 2 The multimedia content provider 106 may then return a search response 214 to the data processing system 102. At time T 2 Thereafter, interaction with the placeholder element 220 may cause the display device 108 to replace the placeholder element 220 with the additional selection element 218. The additional selection element 218 may be associated with a reference address 148 found during a search by the multimedia content application 160. Due to less computing resources at the display device 108 and time consumed in the initialization of the multimedia content application 160Searching or pinging the multimedia content provider 106 at the local cache may be faster than the search performed by the multimedia content application 160 on the display device 108. Thus, time T 2 Can be greater than T 1 。
FIG. 3 illustrates an example method 300 for coordinating parallel processing of audio queries across multiple devices. The method 300 may be implemented or performed by the system 100 described above in connection with fig. 1 and 2 or the system 400 described below in connection with fig. 4. The method 300 may include receiving an audio signal from a display device (block 305). The method 300 may include parsing the audio signal to identify an entity (block 310). The method 300 may include sending a query command to the display device (block 315). The method 300 may include identifying a reference address for an entity (block 320). The method 300 may include sending the reference address to the display device (block 325).
The method 300 may include receiving an audio signal from a client device (block 305). For example, an NLP component of a data processing system may receive data packets that may include audio signals. The data processing system (and NLP components) may be or otherwise be executed by a client device, a component of a display device, or may be a stand-alone device. A sensor (such as a microphone) at the client device or display device may detect the input audio signal, and the respective client device or display device may then send the input audio signal to the data processing system. For example, an application executing on a display device may detect that the user utters "Okay, play Bat Movie form 2005 (good, playing bat movie 2005)". The detected utterance may be encoded into an input audio signal and sent to a data processing system or NLP component of the vehicle.
The method 300 may include parsing the audio signal to identify an entity (block 310). For example, the NLP component may parse the input audio signal to identify a request in the input audio signal. The NLP component can identify one or more words that form the request. From one or more words, the NLP component can use the semantic knowledge graph to identify an entity. An entity may be associated with multimedia content and a semantic knowledge graph may include a set of words that are labeled as related or unrelated to the multimedia content.
The method 300 may include sending a query command to the display device (block 315). For example, a search coordinator component of a data processing system can generate a query command for a display device. The query command may include an entity identified from the request for the audio signal. The query command may also include instructions indicating which multimedia content applications are to perform a search on the display device. The search coordinator component can send a query command to the display device. Upon receipt, the display device may initialize the multimedia content application indicated in the query command to perform a search for the entity. The search coordinator component can also generate another query command for the multimedia content server. The query command for the multimedia content provider may include an entity to be searched at a content database associated with the content server. The search coordinator component can send a query command to the multimedia content provider. Upon receipt, the multimedia content provider may perform a search for the entity.
The method 300 may include identifying a reference address for an entity (block 320). For example, a content indexer component of the data processing system can identify the reference address from at least one of a local cache (e.g., a data repository) and a multimedia content server. The content indexer component can maintain a local list of reference addresses for the entities at a local cache. The content indexer component can access the local cache to traverse the list of reference addresses and identify the reference address corresponding to the entity. The content indexer component can also receive a search response from the multimedia content provider. The search response may include a reference address corresponding to the entity. The content indexer component can compare the reference addresses in the local cache to the reference addresses from the multimedia content provider to determine if the reference addresses in the local cache are up-to-date.
The method 300 may include sending the reference address to the display device (block 325). For example, the response handler component may generate instructions for rendering a content selection interface with a reference address. The content selection interface may include a selection element and a placeholder element. The selection element may correspond to a reference address found by the data processing system. The response handler component may send instructions to the display device for rendering the content selection interface. When received, the display device may present a content selection interface. Subsequent interactions with the placeholder element may cause the display device to update the content selection interface with its own search performed by the multimedia content application.
Fig. 4 is a block diagram of an example computer system 400. Computer system or computing device 400 may include or be used to implement system 100 or components thereof, such as data processing system 102, client device 104, multimedia content provider 106, and display device 108. Computing system 400 includes a bus 405 or other communication component for communicating information, and a processor 410 or processing circuit coupled to bus 405 for processing information. Computing system 400 may also include one or more processors 410 or processing circuits coupled to the bus for processing information. Computing system 400 also includes a main memory 415, such as a Random Access Memory (RAM) or other dynamic storage device, coupled to bus 405 for storing information and instructions to be executed by processor 410. Main memory 415 may be or include data repository 140. Main memory 415 also may be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 410. Computing system 400 may also include a Read Only Memory (ROM) 420 or other static storage device coupled to bus 405 for storing static information and instructions for processor 410. A storage device 425, such as a solid state device, magnetic disk, or optical disk, may be coupled to bus 405 to persistently store information and instructions. The storage device 425 may comprise or be part of the data repository 140.
The computing system 400 may be coupled via bus 405 to a display 435, such as a liquid crystal display or an active matrix display, for displaying information to a user. An input device 430, such as a keyboard including alphanumeric and other keys, may be coupled to bus 405 for communicating information and command selections to processor 410. The input device 430 may include a touch screen display 435. The input device 430 may also include a cursor control, such as a mouse, a trackball, or cursor direction keys for communicating direction information and command selections to the processor 410 and for controlling cursor movement on the display 435. For example, display 435 may be part of data processing system 102, client device 140, display device 108, or other components of fig. 1.
The processes, systems, and methods described herein may be implemented by computing system 400 in response to processor 410 executing an arrangement of instructions contained in main memory 415. Such instructions may be read into main memory 415 from another computer-readable medium, such as storage device 425. Execution of the arrangement of instructions contained in main memory 415 causes computing system 400 to perform the illustrative processes described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 415. Hardwired circuitry may be used in place of or in combination with software instructions in combination with the systems and methods described herein. The systems and methods described herein are not limited to any specific combination of hardware circuitry and software.
Although an example computing system has been described in FIG. 4, the subject matter including the operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
For situations where the system discussed herein collects personal information about a user or may utilize personal information, the user may be provided with an opportunity to control whether programs or features may collect personal information (e.g., information about the user's social network, social actions or activities, the user's preferences, or the user's location) or whether or how to receive content more relevant to the user from a content server or other data processing system. In addition, certain data may be anonymized in one or more ways before it is stored or used so that personally identifiable information is removed when parameters are generated. For example, the identity of the user may be anonymized so that personally identifiable information is not determinable for the user, or the geographic location of the user may be generalized (such as to a city, zip code, or state level) where location information is obtained so that a particular location of the user cannot be determined. Thus, the user can control how his or her information is collected and used by the content server.
The subject matter and operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs (e.g., one or more circuits of computer program instructions encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus). Alternatively or in addition, the program instructions may be encoded on a manually generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium may be or be included in a computer readable storage device, a computer readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Although the computer storage medium is not a propagated signal, the computer storage medium may be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be included in one or more separate components or multimedia (e.g., multiple CDs, disks, or other storage devices). The operations described in this specification may be implemented as operations performed by a data processing apparatus on data stored on one or more computer readable storage devices or received from other sources.
The terms "data processing system," "computing device," "component" or "data processing apparatus" encompass a variety of devices, apparatus, and machines for processing data, including by way of example a programmable processor, computer, system-on-a-chip, or multiple programmable processors, computers, system-on-a-chip, or a combination of the foregoing. The device may comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus may include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures. The components of system 100 may include or share one or more data processing apparatuses, systems, computing devices, or processors.
A computer program (also known as a program, software application, app, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. The computer program may correspond to a file in a file system. A computer program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs (e.g., components of data processing system 102, client device 104, multimedia content provider 106, and display device 108) to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, multimedia and storage devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
The subject matter described herein may be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described herein), or a combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), internetworks (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
A computing system such as system 100 or system 400 may include clients and servers. The clients and servers are generally remote from each other and typically interact through a communication network (e.g., network 110). The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, the server sends data (e.g., data packets representing the content items) to the client device (e.g., to display data to and receive user input from a user interacting with the client device). Data generated at the client device (e.g., results of user interactions) may be received at the server from the client device (e.g., received by the data processing system 102 from the client device 140 or the display device 108).
Although operations are depicted in the drawings in a particular order, such operations are not required to be performed in the particular order shown or in sequential order, and not all illustrated operations are required to be performed. The acts described herein may be performed in a different order.
The separation of various system components does not require separation in all embodiments, and the described program components can be included in a single hardware or software product. For example, NLP component 124 and direct action API 126 may be a single component, app, or program, or logic device having one or more processing circuits, or a portion of one or more servers of data processing system 102.
Having now described some illustrative embodiments, it should be apparent that the foregoing is illustrative and not limiting and has been presented by way of example. In particular, although many of the examples presented herein involve specific combinations of method acts or system elements, those acts and those elements may be combined in other ways to achieve the same objectives. Acts, elements and features discussed in connection with one embodiment in other embodiments are not intended to be excluded from a similar role.
The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of "including," "comprising," "having," "containing," "involving," "characterized by," and variations thereof herein, is meant to encompass the items listed thereafter and equivalents thereof as well as additional items and alternative embodiments consisting solely of the items listed thereafter and exclusively. In one embodiment, the systems and methods described herein consist of each combination or all of one or more of the elements, acts, or components described.
Any reference to an embodiment, element, or act of a system and method recited in the singular and proceeded with the word "a" or "an" may also encompass embodiments comprising plural of such elements, and any reference to any embodiment, element, or act of plural numbers herein may also encompass embodiments comprising only a single element. Singular or plural references are not intended to limit the presently disclosed systems or methods, their components, acts, or elements to single or multiple configurations. References to any act or element based on any information, act or element may include embodiments in which the act or element is based at least in part on any information, act or element.
Any embodiment disclosed herein may be combined with any other embodiment or example, and references to "an embodiment," "some embodiments," "one embodiment," etc., are not necessarily mutually exclusive and are intended to indicate that a particular feature, structure, or characteristic described in connection with the embodiment may be included in at least one embodiment or example. Such terms as used herein do not necessarily all refer to the same embodiment. Any embodiment may be combined with any other embodiment, either inclusive or exclusive, in any manner consistent with aspects and embodiments disclosed herein.
Reference to "or" may be construed as inclusive such that any term described using "or" may indicate any one of a single, more than one, and all of the described terms. Reference to "at least one of a 'and B' may include only 'a', only 'B', and both 'a' and 'B'. Such references, used in conjunction with "containing" or other open terms, may include additional terms.
Where technical features in the figures, detailed description, or any claim are followed by reference signs, the reference signs have been included to increase the intelligibility of the figures, detailed description, and claims. Accordingly, neither the reference signs nor their absence have any limiting effect on the scope of any claim elements.
The systems and methods described herein may be embodied in other specific forms without departing from the characteristics thereof. The foregoing embodiments are illustrative and not limiting of the systems and methods described. The scope of the systems and methods described herein is, therefore, indicated by the appended claims rather than by the foregoing description, and all changes which come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein.
Claims (38)
1. A display device, comprising:
an interface;
a transducer;
a display;
a memory storing instructions; and
one or more processors that execute the instructions to:
transmitting an audio input signal to a remote data processing system, wherein the audio input signal is detected via the transducer, and wherein, upon transmitting the audio input signal, one or more of the processors transmits the audio input signal over a network and via the interface;
receiving, via the interface and from the remote data processing system, a query command comprising an entity, wherein the entity is identified by the remote data processing system by parsing the audio input signal;
In response to receiving the query command, causing a multimedia content application executing on the display device to perform a search for the entity;
receiving, via the interface and from the remote data processing system, a reference address corresponding to the entity, wherein the reference address corresponding to the entity is identified by the remote data processing system; and
in response to receiving the reference address, a content selection interface is presented on the display and prior to completion of a search for the entity performed by the multimedia content application, the content selection interface including a selection element for the reference address of the entity.
2. The display device of claim 1, wherein the reference address is identified by the remote data processing system using an address database.
3. The display device of claim 1, wherein the reference address is identified by the remote data processing system using a multimedia content provider.
4. The display device of claim 1, wherein one or more of the processors execute the instructions to further:
displaying an additional selection element for a second reference address on the content selection interface, wherein the display of the additional selection element is after completion of the search for the entity by the multimedia content application, and wherein the second reference address is provided by the multimedia content application based on completion of the search for the entity.
5. The display device of claim 4, wherein the content selection interface includes a placeholder element prior to completion of the search for the entity performed by the multimedia content application, and wherein, upon display of the second reference address, one or more of the processors is to replace the placeholder element with the additional selection element for the second reference address.
6. The display device of claim 1, wherein the display device comprises a smart television.
7. A method, comprising:
transmitting an audio input signal to a remote data processing system, wherein the audio input signal is detected via a transducer, and wherein, upon transmitting the audio input signal, one or more processors transmit the audio input signal over a network and via an interface;
receiving, via the interface and from the remote data processing system, a query command comprising an entity, wherein the entity is identified by the remote data processing system by parsing the audio input signal;
in response to receiving the query command, causing a multimedia content application executing on a display device to perform a search for the entity;
Receiving, via the interface and from the remote data processing system, a reference address corresponding to the entity, wherein the reference address corresponding to the entity is identified by the remote data processing system; and
in response to receiving the reference address, a content selection interface is presented on a display of the display device and prior to completion of a search for the entity performed by the multimedia content application, the content selection interface including a selection element for the reference address of the entity.
8. The method of claim 7, wherein the reference address is identified by the remote data processing system using an address database.
9. The method of claim 7, wherein the reference address is identified by the remote data processing system using a multimedia content provider.
10. The method of claim 7, further comprising: displaying an additional selection element for a second reference address on the content selection interface, wherein the display of the additional selection element is after completion of the search for the entity by the multimedia content application, and wherein the second reference address is provided by the multimedia content application based on completion of the search for the entity.
11. The method of claim 10, wherein the content selection interface includes a placeholder element prior to completion of the search for the entity performed by the multimedia content application, and wherein, upon displaying the second reference address, one or more of the processors is to replace the placeholder element with the additional selection element for the second reference address.
12. The method of claim 7, wherein the display device comprises a smart television.
13. A computer program product comprising one or more non-transitory computer-readable storage media having program instructions collectively stored on the one or more computer-readable storage media, the program instructions executable to:
transmitting an audio input signal to a remote data processing system, wherein the audio input signal is detected via a transducer, and wherein, upon transmitting the audio input signal, one or more processors transmit the audio input signal over a network and via an interface;
receiving, via the interface and from the remote data processing system, a query command comprising an entity, wherein the entity is identified by the remote data processing system by parsing the audio input signal;
In response to receiving the query command, causing a multimedia content application executing on a display device to perform a search for the entity;
receiving, via the interface and from the remote data processing system, a reference address corresponding to the entity, wherein the reference address corresponding to the entity is identified by the remote data processing system; and
in response to receiving the reference address, a content selection interface is presented on a display of the display device and prior to completion of a search for the entity performed by the multimedia content application, the content selection interface including a selection element for the reference address of the entity.
14. The computer program product of claim 13, wherein the reference address is identified by the remote data processing system using an address database.
15. The computer program product of claim 13, wherein the reference address is identified by the remote data processing system using a multimedia content provider.
16. The computer program product of claim 13, the program instructions further executable to: displaying an additional selection element for a second reference address on the content selection interface, wherein the display of the additional selection element is after completion of the search for the entity by the multimedia content application, and wherein the second reference address is provided by the multimedia content application based on completion of the search for the entity.
17. The computer program product of claim 16, wherein the content selection interface includes a placeholder element prior to completion of the search for the entity performed by the multimedia content application, and wherein, upon display of the second reference address, one or more of the processors is to replace the placeholder element with the additional selection element for the second reference address.
18. The computer program product of claim 13, wherein the display device comprises a smart television.
19. A method implemented by one or more processors, the method comprising:
receiving a query command from a remote data processing system comprising an entity;
in response to receiving the query command, causing a multimedia content application executing on a display device to perform a search for the entity;
receiving search results corresponding to the entity from the remote data processing system, wherein the search results corresponding to the entity are identified by the remote data processing system; and
in response to receiving the search results from the remote data processing system, a content selection interface is presented on a display of the display device and prior to completion of a search for the entity performed by the multimedia content application, the content selection interface including selection elements for the search results from the remote data processing system and placeholder elements for search results from the multimedia content application.
20. The method of claim 19, wherein the search results from the remote data processing system include a reference address identified by the remote data processing system using an address database.
21. The method of claim 19, wherein the search results from the remote data processing system include a reference address identified by the remote data processing system using a multimedia content provider.
22. The method of claim 19, further comprising: in response to completing a search performed by the multimedia content application for the entity, replacing the placeholder element with an additional selection element on the content selection interface for search results from the multimedia content application, the search results from the multimedia content application corresponding to the entity.
23. The method of claim 22, wherein replacing the placeholder element is further responsive to interaction with the placeholder element.
24. The method of claim 22, wherein the search results from the multimedia content application include a reference address provided by the multimedia content application.
25. The method of claim 19, wherein the display device comprises a smart television.
26. A computer program product comprising one or more non-transitory computer-readable storage media having program instructions stored collectively on the one or more computer-readable storage media, the program instructions executable to:
receiving a query command from a remote data processing system comprising an entity;
in response to receiving the query command, causing a multimedia content application executing on a display device to perform a search for the entity;
receiving search results corresponding to the entity from the remote data processing system, wherein the search results corresponding to the entity are identified by the remote data processing system; and
in response to receiving the search results from the remote data processing system, a content selection interface is presented on a display of the display device and prior to completion of a search for the entity performed by the multimedia content application, the content selection interface including selection elements for the search results from the remote data processing system and placeholder elements for search results from the multimedia content application.
27. The computer program product of claim 26, wherein the search results from the remote data processing system include a reference address identified by the remote data processing system using an address database.
28. The computer program product of claim 26, wherein the search results from the remote data processing system include a reference address identified by the remote data processing system using a multimedia content provider.
29. The computer program product of claim 26, wherein the program instructions are further executable to: in response to completing a search performed by the multimedia content application for the entity, replacing the placeholder element with an additional selection element on the content selection interface for search results from the multimedia content application, the search results from the multimedia content application corresponding to the entity.
30. The computer program product of claim 29, wherein replacing the placeholder element is further responsive to interaction with the placeholder element.
31. The computer program product of claim 29, wherein the search results from the multimedia content application comprise a reference address provided by the multimedia content application.
32. The computer program product of claim 26, wherein the display device comprises a smart television.
33. A system, comprising:
a processor, a computer readable memory, one or more computer readable storage media, and program instructions collectively stored on the one or more computer readable storage media, the program instructions being executable to:
receiving a query command from a remote data processing system comprising an entity;
in response to receiving the query command, causing a multimedia content application executing on a display device to perform a search for the entity;
receiving search results corresponding to the entity from the remote data processing system, wherein the search results corresponding to the entity are identified by the remote data processing system; and
in response to receiving the search results from the remote data processing system, a content selection interface is presented on a display of the display device and prior to completion of a search for the entity performed by the multimedia content application, the content selection interface including selection elements for the search results from the remote data processing system and placeholder elements for search results from the multimedia content application.
34. The system of claim 33, wherein the search results from the remote data processing system include a reference address identified by the remote data processing system using an address database.
35. The system of claim 33, wherein the search results from the remote data processing system include a reference address identified by the remote data processing system using a multimedia content provider.
36. The system of claim 33, wherein the program instructions are further executable to: in response to completing a search performed by the multimedia content application for the entity, replacing the placeholder element with an additional selection element on the content selection interface for search results from the multimedia content application, the search results from the multimedia content application corresponding to the entity.
37. The system of claim 36, wherein replacing the placeholder element is further responsive to interaction with the placeholder element.
38. The system of claim 36, wherein the search results from the multimedia content application include a reference address provided by the multimedia content application.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202311507585.1A CN117556080A (en) | 2017-10-03 | 2017-10-03 | Coordinating parallel processing of audio queries across multiple devices |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2017/054854 WO2019070234A1 (en) | 2017-10-03 | 2017-10-03 | Coordination of parallel processing of audio queries across multiple devices |
CN201780092010.4A CN110741366B (en) | 2017-10-03 | 2017-10-03 | Coordinating parallel processing of audio queries across multiple devices |
CN202311507585.1A CN117556080A (en) | 2017-10-03 | 2017-10-03 | Coordinating parallel processing of audio queries across multiple devices |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780092010.4A Division CN110741366B (en) | 2017-10-03 | 2017-10-03 | Coordinating parallel processing of audio queries across multiple devices |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117556080A true CN117556080A (en) | 2024-02-13 |
Family
ID=60153454
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202311507585.1A Pending CN117556080A (en) | 2017-10-03 | 2017-10-03 | Coordinating parallel processing of audio queries across multiple devices |
CN201780092010.4A Active CN110741366B (en) | 2017-10-03 | 2017-10-03 | Coordinating parallel processing of audio queries across multiple devices |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780092010.4A Active CN110741366B (en) | 2017-10-03 | 2017-10-03 | Coordinating parallel processing of audio queries across multiple devices |
Country Status (4)
Country | Link |
---|---|
US (3) | US11144584B2 (en) |
EP (1) | EP3692431A1 (en) |
CN (2) | CN117556080A (en) |
WO (1) | WO2019070234A1 (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10867132B2 (en) | 2019-03-29 | 2020-12-15 | Microsoft Technology Licensing, Llc | Ontology entity type detection from tokenized utterance |
US10916237B2 (en) * | 2019-03-29 | 2021-02-09 | Microsoft Technology Licensing, Llc | Training utterance generation |
US10970278B2 (en) | 2019-03-29 | 2021-04-06 | Microsoft Technology Licensing, Llc | Querying knowledge graph with natural language input |
Family Cites Families (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7668809B1 (en) * | 2004-12-15 | 2010-02-23 | Kayak Software Corporation | Method and apparatus for dynamic information connection search engine |
US9183305B2 (en) * | 2007-06-19 | 2015-11-10 | Red Hat, Inc. | Delegated search of content in accounts linked to social overlay system |
CN103339623B (en) * | 2010-09-08 | 2018-05-25 | 纽昂斯通讯公司 | It is related to the method and apparatus of Internet search |
US10735552B2 (en) * | 2013-01-31 | 2020-08-04 | Google Llc | Secondary transmissions of packetized data |
US9247309B2 (en) * | 2013-03-14 | 2016-01-26 | Google Inc. | Methods, systems, and media for presenting mobile content corresponding to media content |
US11218434B2 (en) * | 2013-06-12 | 2022-01-04 | Google Llc | Audio data packet status determination |
US11182431B2 (en) * | 2014-10-03 | 2021-11-23 | Disney Enterprises, Inc. | Voice searching metadata through media content |
US20160171122A1 (en) * | 2014-12-10 | 2016-06-16 | Ford Global Technologies, Llc | Multimodal search response |
US10360902B2 (en) * | 2015-06-05 | 2019-07-23 | Apple Inc. | Systems and methods for providing improved search functionality on a client device |
US20170103132A1 (en) * | 2015-10-11 | 2017-04-13 | Microsoft Technology Licensing, Llc | Identifying search results from local and remote search of communications in parallel |
-
2017
- 2017-10-03 US US16/609,971 patent/US11144584B2/en active Active
- 2017-10-03 CN CN202311507585.1A patent/CN117556080A/en active Pending
- 2017-10-03 CN CN201780092010.4A patent/CN110741366B/en active Active
- 2017-10-03 EP EP17787719.8A patent/EP3692431A1/en not_active Withdrawn
- 2017-10-03 WO PCT/US2017/054854 patent/WO2019070234A1/en unknown
-
2021
- 2021-10-11 US US17/498,570 patent/US11841893B2/en active Active
-
2023
- 2023-10-04 US US18/376,712 patent/US20240028633A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US11841893B2 (en) | 2023-12-12 |
EP3692431A1 (en) | 2020-08-12 |
US20200057774A1 (en) | 2020-02-20 |
US20220027403A1 (en) | 2022-01-27 |
US11144584B2 (en) | 2021-10-12 |
CN110741366B (en) | 2023-12-01 |
WO2019070234A1 (en) | 2019-04-11 |
US20240028633A1 (en) | 2024-01-25 |
CN110741366A (en) | 2020-01-31 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108521858B (en) | Device identifier dependent handling of operations for packet-based data communication | |
US11841893B2 (en) | Coordination of parallel processing of audio queries across multiple devices | |
EP3360307B1 (en) | Authentication of packetized audio signals | |
JP7195363B2 (en) | Latency-aware display-mode dependent response generation | |
JP7439186B2 (en) | Coordinating overlapping audio queries | |
US20220027124A1 (en) | Verifying operational statuses of agents interfacing with digital assistant applications | |
EP3596599A1 (en) | Activation of remote devices in a networked system | |
US20200257853A1 (en) | Verifying operational statuses of agents interfacing with digital assistant applications | |
US11823663B2 (en) | Audio processing in a low-bandwidth networked system | |
CN110741362B (en) | Coordination of overlapping processing of audio queries | |
JP2024063034A (en) | Coordination of overlapping audio queries |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |