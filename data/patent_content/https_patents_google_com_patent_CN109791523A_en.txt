CN109791523A - Support the memory management of huge page - Google Patents
Support the memory management of huge page Download PDFInfo
- Publication number
- CN109791523A CN109791523A CN201780058759.7A CN201780058759A CN109791523A CN 109791523 A CN109791523 A CN 109791523A CN 201780058759 A CN201780058759 A CN 201780058759A CN 109791523 A CN109791523 A CN 109791523A
- Authority
- CN
- China
- Prior art keywords
- page
- main memory
- data
- size
- memory
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000015654 memory Effects 0.000 title claims abstract description 320
- 238000000034 method Methods 0.000 claims description 37
- 238000003860 storage Methods 0.000 claims description 22
- 230000005540 biological transmission Effects 0.000 claims description 14
- 230000004044 response Effects 0.000 claims description 12
- 230000000977 initiatory effect Effects 0.000 claims description 6
- 238000009826 distribution Methods 0.000 claims description 4
- 230000006798 recombination Effects 0.000 claims description 3
- 238000005215 recombination Methods 0.000 claims description 3
- 238000000151 deposition Methods 0.000 claims description 2
- 230000005055 memory storage Effects 0.000 claims 1
- 230000008569 process Effects 0.000 description 19
- 230000008901 benefit Effects 0.000 description 11
- 238000012545 processing Methods 0.000 description 8
- 239000000872 buffer Substances 0.000 description 7
- 238000004590 computer program Methods 0.000 description 4
- 238000012546 transfer Methods 0.000 description 4
- 238000001514 detection method Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000007704 transition Effects 0.000 description 2
- 241001269238 Data Species 0.000 description 1
- 241001272996 Polyphylla fullo Species 0.000 description 1
- 238000009414 blockwork Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000001816 cooling Methods 0.000 description 1
- 230000007812 deficiency Effects 0.000 description 1
- 230000003111 delayed effect Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 235000013399 edible fruits Nutrition 0.000 description 1
- 239000004744 fabric Substances 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/0223—User address space allocation, e.g. contiguous or non contiguous base addressing
- G06F12/023—Free address space management
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/04—Addressing variable-length words or parts of words
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0806—Multiuser, multiprocessor or multiprocessing cache systems
- G06F12/0815—Cache consistency protocols
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0866—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches for peripheral storage systems, e.g. disk cache
- G06F12/0868—Data transfer between cache memory and other subsystems, e.g. storage devices or host systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0866—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches for peripheral storage systems, e.g. disk cache
- G06F12/0871—Allocation or management of cache space
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0866—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches for peripheral storage systems, e.g. disk cache
- G06F12/0873—Mapping of cache memory to specific storage devices or parts thereof
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0877—Cache access modes
- G06F12/0882—Page mode
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0893—Caches characterised by their organisation or structure
- G06F12/0897—Caches characterised by their organisation or structure with two or more cache hierarchy levels
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/10—Address translation
- G06F12/1009—Address translation using page tables, e.g. page table structures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/10—Address translation
- G06F12/1027—Address translation using associative or pseudo-associative address translation means, e.g. translation look-aside buffer [TLB]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/12—Replacement control
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/10—Providing a specific technical effect
- G06F2212/1016—Performance improvement
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/10—Providing a specific technical effect
- G06F2212/1016—Performance improvement
- G06F2212/1024—Latency reduction
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/15—Use in a specific computing environment
- G06F2212/152—Virtualized environment, e.g. logically partitioned system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/30—Providing cache or TLB in specific location of a processing system
- G06F2212/304—In main memory subsystem
- G06F2212/3042—In main memory subsystem being part of a memory device, e.g. cache DRAM
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/30—Providing cache or TLB in specific location of a processing system
- G06F2212/305—Providing cache or TLB in specific location of a processing system being part of a memory device, e.g. cache DRAM
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/60—Details of cache memory
- G06F2212/604—Details relating to cache allocation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/65—Details of virtual memory and virtual address translation
- G06F2212/652—Page size control
Abstract
A kind of mthods, systems and devices, for receiving the request of the data in the first part that access includes the first data page from main memory, the first data page has first page size；It is not stored in main memory based on determining first data page to initiate page fault；A part of main memory is distributed, the sizableness of a part of main memory is in first page size；The first part of first data page is transmitted to a part of assigned main memory from secondary memory without transmitting entire first data page；First page table clause associated with the first part of the first data page is updated, to be directed toward the position of a part for the assigned main memory that the first part of the first data page is sent to.
Description
Technical field
Subject specification relates generally to memory systems.
Background technique
Various memory devices can be used to safeguard and store the data of various computers and similar system and refer to
It enables.In traditional computing system, dynamic random access memory (DRAM) technology is commonly used in the dynamic memory of operation computer,
It is run at high speed to apply.However, the DRAM as main memory is no longer quick as before in computer systems
Ground expands.As a result, DRAM storage has become the limited resources calculated in environment.
Summary of the invention
Two layers of memory can be used, such as the memory based on disk, nand flash memory, spin-torque transmit magnetic memory (STT-
MRAM), resistance random memory (ReRAM) etc..Two layers of memory can be by memory or IO bus local IP access, or passes through height
Fast network remote access.But it is transparent between memory layers using needing the placement of explicit management data or system that must provide
The automatic management of ground mobile data.In addition, there are also huge page or the big page or the super page, the use of these terms interchangeably,
These verified pages can provide significant performance boost for most of workloads, especially for service based on cloud
Using, wherein huge page is the biggish memory block of size, such as 8KB, 64KB, 256KB, 1MB, 2MB, 4MB, 16MB, 256MB,
512MB or 1GB, rather than may be the representative page of 4KB, it is specifically dependent upon processor architecture.Therefore, automatic management needs pair
The smallest new technology of the influence of performance, with overcome the deficiencies in the prior art.
One novel aspects of theme described in this specification are embodied in system and method, and system and method include:
The request of the data in the first part for including the first data page is received from main memory, which includes first page
Size, and first part has second page size, and second page size is less than first page size；Based on determining first number
It is not stored in main memory and is stored in secondary memory according to page to initiate page fault；In response to the initiation of page fault,
A part of main memory is distributed, the sizableness of a part of main memory is in first page size；By the first of the first data page
Part is transmitted to a part of assigned main memory from secondary memory without transmitting entire first data page, wherein the first data
The surplus of page continues to be stored in secondary memory；And update first page associated with the first part of the first data page
Table clause, to be directed toward the position of a part for the assigned main memory that the first part of the first data page is sent to.
In some embodiments, the surplus of the first data page is transferred to main memory from secondary memory.The first number of transmission
Surplus according to page may include: to repeatedly transmit each section of the first data page of corresponding second page size from secondary memory
To a part of assigned main memory, until entire first data page is stored in main memory；It also, is the first data
Each each section of page updates each page table entries, to be directed toward each position of each section of the first data page in main memory.
In some embodiments, it once entire first data page is stored in main memory, then will be passed from secondary memory
The each section for being sent to first data page of a part of assigned main memory is reassembled as the first data page；And it updates and first
The associated page table entries of data page are to be directed toward the position of the first data page recombinated in main memory.
The another aspect of theme described in this specification is embodied in system and method, and system and method are included in
Before the remainder of one data page is transmitted to main memory from secondary memory, the first of the first data page of requested access is indicated
Part has been sent in main memory.
Another novel aspects of theme described in this specification are embodied in system and method, the system and method packet
Include the scanning based on page table scanner to page table, it is determined whether be provided with access for each page table entries of page table
Position, access position indicate whether associated with the page table entries page is accessed in the last scan period, wherein with the
At least one page of one page-size is divided into the page of second page size, and the page of second page size is in quilt
There are the page table entries of the page for each second page size in the page table of scanning；In response to determining without being and page
The associated page table entries setting access position in face, is incremented by the counting of each page；Determining whether for each page table entries
After access position, resetting access position.
In some embodiments, if sizableness cannot be divided in a part of the main memory of first page size
Match, then a page in the minimum page used with first page size determined based on the counting of each page,
And a page in the page at least used is discharged into secondary memory, and at least used at one of release
Sizableness is distributed at the position of the page in a part of the main memory of first page size；If sizableness is in first page
A part of the main memory of size can be assigned, then be transmitted to the first part of the first data page from secondary memory assigned
A part of main memory.
Other embodiments in terms of these include corresponding system, device and computer program, these systems, device and meter
Calculation machine program is configured as executing the movement of the method encoded on computer memory device.
The specific embodiment of theme described in this specification may be implemented, to realize one or more in following advantages
It is a.For example, the use or access statistic data to the page in memory can be than being related to the current method of software and sampling technique more
It is accurate and accurate, because in subpage frame rank rather than only can determine access statistic data in page level.In addition, passing through head
The specific part of the page of first transfer request access rather than whole page data, it is possible to reduce in response to accession page request and turn
Delay caused by the page is moved, this can promote faster to execute the application or process being carrying out.Another advantage is that system can
To utilize the benefit of huge page, such as since page table rank is less and translation lookaside buffers (TLB) coverage rate is preferable, and have
There is better internal storage access performance, and paging is executed with lesser page-granular always, it is wrong that this can provide the better page
Accidentally performance, because only transmitting the delay for the page fault that the small page reduces.It is thereby achieved that the internal storage access benefit of huge page
With the demand of the paging benefit to the small page.Further, since preferably being maintained in main memory compared with the huge page of direct paging
Cold data in dsc data and secondary memory, therefore required data are only transmitted according to small page-size, and result makes winner
Memory will not be occupied by unnecessary data.
The details of one or more embodiments of the invention is elaborated in the accompanying drawings and the description below.According to specification, attached
Figure and claim, other features and advantages of the present invention will become apparent.
Detailed description of the invention
Fig. 1 is depicted including the example according to the system of the memory device of embodiment of the present disclosure.
Fig. 2 is depicted including the example according to the system of the memory device of embodiment of the present disclosure.
Fig. 3 A, which is depicted, to be used for according to embodiment of the present disclosure by the page table of virtual memory mappings to physical memory
Example.
Fig. 3 B depicts the example of a part of the memory according to embodiment of the present disclosure distribution.
Fig. 4 depicts the example flow diagram of the process for memory management according to embodiment of the present disclosure.
Identical appended drawing reference and title indicate identical element in each attached drawing.
Specific embodiment
It has been proved that huge page can provide significant performance boost for most of workloads, especially for based on cloud
Be served by.Although term " huge page " may be used herein, which is suitable for any greater than the page of minimum dimension
The page of size, the manageable small page of the page, that is, certain architectures of minimum dimension or its standard page size.For example, specific
The minimum page-size or standard page-size of framework can be 4KB, and huge page can be 2MB.In other embodiments, example
Such as, huge page can be 8KB, 64KB, 256KB, 1MB, 2MB, 4MB, 16MB, 256MB, 512MB or 1GB or bigger, or therebetween
Any size.It for example, huge page can be any integral multiple n, the i.e. n*4KB of 4KB, and in some embodiments it is possible to is mark
Any power of twice of quasi- page-size.Embodiment of the disclosure introduces a kind of new departure, and huge page can be used in the program
Access main memory (such as DRAM cache), while using traditional small page come paging to slower memory second level (sometimes referred to as
Secondary memory).Some embodiments can be modified based on the type of the interconnection for accessing slower memory.For example, can be IO
The software-based solution based on customization kernel driver is realized in interconnection.In addition, for example, can be interconnected for buffer consistency
Realize the hardware solution for managing huge page.
Correspondingly, embodiment of the disclosure provides a kind of system that the high-performance for secondary memory manages automatically, should
Secondary memory is locally available to memory or IO bus or available by network remote.Secondary memory can be based on disk,
It can be non-volatile and persistent calculator memory.Secondary memory can not directly be accessed by processor, and can be compared
Basic memory or main memory are slow.Main memory is also referred to as basic memory, background memory, internal memory or first order memory, can be with
Directly accessed by CPU.As described in more detail below, for example, the kernel driver of optimization can be provided to the fast of secondary memory
All communications of fast path and processing and memory management hardware.With cause for such as synchronous, memory management and block I/O transfer
Big cost, compared by the existing route that kernel carries out paging, which is advantageous.
It is described in more detail below these features and supplementary features.
Fig. 1 is depicted including the example according to the system 100 of the memory device of embodiment of the present disclosure.Central processing list
Member (CPU) 110 can be communicated in the form of DRAM 120 and memory management unit (MMU) 150 with main memory.System 100 may be used also
To include the secondary memory in the form of long-distance inner 130, can be accessed by network.MMU 150 can be in the management of memory
It is operated.In addition, page table walker 160 and transition detection buffer area (TLB) 165 can be MMU 150 a part or
Person is realized by MMU 150.System 100 can additionally include that DRAM 170 is used as physical memory.
MMU 150 is to can have the hardware cell quoted by the memory of MMU150, executes virtual memory address to object
Manage conversion and the processing buffer control of address.It is used as example, page table can be used in MMU 150 comprising every page of page table
The memory table (in-memory table) of entry (PTE), physical page number virtual page number being mapped in main memory.As
The transition detection buffer area 165 of the association caching of PTE can be used for avoiding accessing necessity of main memory when each maps virtual addresses
Property.When PTE forbids accessing virtual page number, such as because when being assigned to the virtual page number without Physical Random Access memory,
MMU 150 can send out the signal of page fault to CPU 110.
CPU 110 can have caching, can be a small amount of fast memory being built in processor, which can be with
Ephemeral data copy is configured to contain to reduce processing delay.TLB 165 can be the fixed size of the most recently used page
Array, CPU 110 can check TLB 165 in each internal storage access.TLB 165 can be listed and is currently allocated at DRAM
The virtual address range of physical page in 170.Thus, for example, TLB 165 may be used as the caching of MMU 150.With this side
Formula can be directly entered associated physical memory, such as DRAM 170 to the access for the virtual address listed in TLB 165.
In addition, the access to the virtual address that do not list in TLB 165, i.e. TLB miss, can trigger can be by hardware or the page
The page table lookup that error handler executes.
Fig. 2 is depicted including the example according to the system 200 of the memory device of embodiment of the present disclosure.System 200 can
To include CPU 220 and physical address space 240.MMU 230 can explain virtual address to identify corresponding physical address.Example
Such as, it can will be read at virtual address, write-in or the trial of execution memory are converted to corresponding physical address, or can give birth to
At interruption, i.e. page fault, with the access for allowing software responses to attempt.Physical memory addresses can identify in one piece of storage hardware
Particular memory unit or part, storage hardware constitute and the given associated physical memory of read or write operation.Virtually
Memory can provide one group of memory address of software control, for example, virtual address space, and can permit each process, example
Such as, process A 205 and process B 210, the virtual memory address range with their own, may include kernel spacing and user
Space.Page table can be used to explain virtual address in MMU 230, and virtual address range can be mapped to associated by page table
Storage content.Although the minimum addressing unit of processor can be byte or word, MMU 230 can be with page management memory.
Fig. 3 A is depicted according to embodiment of the present disclosure for virtual memory 310 to be mapped to physical memory 330
The example of page table 320.Page table 320 can be the data structure of the memory map listing comprising process, and can be used for
Track associated resource.For example, each process can have the one group of page table of oneself.Virtual address space, such as virtual memory
310, it can be divided into the page, the page can be the continuation address range of particular size.The page can be constructed such that page
The initial address in face is the multiple of page-size.As described above, page table 320, which can be used, in MMU 230 carrys out self-virtualizing to explain
The virtual address of the page of memory 310, and identify the corresponding physical address of the page frame in physical memory 330.In addition, the page
Table can be layering, multistage, based on hash etc., this provides advantage for huge page, is traversed with the faster page and improves layer
Secondary structure.
As described above, secondary memory or two layers of memory can be such as such as based on the memory of disk or other two layers of memories
The main memory of DRAM or basic memory are slow.According to certain embodiments, the kernel driver of customization can be managed with huge page
Two layers of memory.Kernel driver can retain physical memory in continuum, these regions are the huge pages cached in DRAM
Multiple.When application needs additional memory, kernel driver may be with the multiple allocation space of huge page, i.e., with huge page size
Multiple allocation space.Page replacement policy may be implemented in kernel driver, and when the data for replacement are selected, can
With by huge page paging to two layers of memory.The process can with request access to data in EMS memory be currently running using asynchronous hair
It is raw.
When applying when access resides in the corrupt data in two layers of memory, page fault handler can only will include institute
The single small page of the cache lines of request is transmitted to the main memory such as DRAM from two layers of memory.However, according to certain embodiments,
The state for constituting the small page of each of huge page can be tracked.Thus, for example, when kernel driver is all or predetermined in huge page
When malfunctioning in the small page of quantity, it can make a determination by with the existing PTE of the single PTE of huge page replacement and from TLB
Any relevant TLB entry is removed in 165, and any remaining small page is divided into the page, and the small page is merged or reassembled into
Huge page in DRAM.
Therefore, the advantages of huge page to the data resided in DRAM can be kept, while can also be by small in transmission
The process of error handler is completed after the page, to reduce the cost of page fault.For example, reduction money can be provided using huge page
The advantages of source expense, because making it possible the smaller page table with less entry with bigger granularity trace data.But
If always two layers of memory are written in huge page by system, it may result in whole write-in bandwidth using huge page and increase.Additionally, there are
A possibility that such: in huge page as being commonly used or the small page of most recently used " heat ", may be slit into sheets it is secondary compared with
Slow memory causes the additional fault of " heat " data.According to certain embodiments, kernel driver can be based on huge page and small
Page statistical data dynamically determines when decompose or merge huge page to alleviate these problems.For example, can periodically decompose
Huge page collects the statistical data about the small page in huge page to access position by PTE, as described in more detail below.
In addition, driver can safeguard huge page and the small page, thus when the predetermined quantity in huge page the small page " heat " or it is frequent or
When accessed recently, the small page can be transferred to huge page and be merged.On the contrary, if too many subpage frame in huge page, i.e., it is small
If the page is " cold ", then huge page can be decomposed and be handled as the small page.
It is wrong to access the data being not stored in main memory and the page occurs when making trial according to certain embodiments
It mistakes, entire huge page may not be sent in main memory, but lesser data block is sent in main memory, for example, packet
The small page for including the data requested access to can be sent to main memory from secondary memory.Therefore, it is requested using accessible
Data and continue to run.Then, the rest part of huge page can be transmitted in main memory from the background, and can be correspondingly
Update page table entries.In this way it is possible to quickly access requested data, and system still can be managed
The benefit of huge page.In other words, from secondary memory read huge page and by huge page be written main memory needed for the time be greater than read and
Time needed for the small page is written；Therefore, only read the small page of data comprising requesting access to reduce application time or
Handle the thread time being suspended or the time for waiting pending datas to be transferred to main memory from secondary memory.Therefore, because performance is extremely closed
Important, the delay time that reduction transfers data to main memory is more important than the time that data are transmitted back to secondary memory, because
The data transmitted out from main memory are usually " cold " page of data, and " cold " page of data influences operating characteristics in background transfer
Very little or none influence, on the contrary be transferred to main memory data may delayed application or handle thread execution.
As described above, the routine access working as thread or being currently running is mapped to virtual address space but is practically without load
When memory pages into main memory, it may occur however that page fault.MMU 150 or page fault handler can detecte page mistake
Accidentally, and when detecting page fault, the determination that whether there is free page in memory can be made.If there is free page
Page data then can be copied to the free page position in memory from second-level storage by face.If without free page,
The page can be for example pulled out from fifo queue, which can track all pages in memory in the queue, wherein
It reaches later, is reached earliest in front recently.If the page be it is dirty, i.e., modified, then system can will
Secondary memory is written in the page.When the page is transmitted to secondary memory from main memory, page table entries associated with the page
It is invalid to can be, and the page table cache that can execute any entry associated with the page closes (TLB shootdown),
For example, refreshing TLB entry for other processors.When the page is currently idle, page data can be from secondary
Grade memory copy is to free page position.It can be by updating PTE associated with the page to be directed toward in the main memory of the page
Position update page table to create effective PTE.Once having handled page fault, thread or the program being currently running can
To restore the data that it requests to access in main memory.
Fig. 3 B describes the example of a part of the memory 305 distributed according to presently filed embodiment.For paging demand,
It may need one page " cold " data second-level storage is written, for example, " cold " data are to be less than some threshold value access rate
The data or not visited data in special time period of rate access, and when using the raw page of trial access Data Concurrent
When the mistake of face, it may be necessary to which page of data is transmitted back to main memory.According to certain embodiments, from the perspective of processor,
The process of page disengaging main memory just looks like as system only handles huge page.In other words, huge page may when cold, i.e., without
It is often used or recently when not in use, is passed out and from main memory, and when the page needs to be transferred to main memory, Ke Yifen
Entire huge page with memory, even if only as a part of the huge page of the small page can be initially transferred.Therefore, the memory of distribution
305 can correspond to huge page, and the contiguous memory for corresponding to huge page can be distributed in physical memory 325 and virtual memory
In 315.
It, can storage allocation first for example, when page fault occurring and when needing page of data being transmitted to main memory
Huge page.Then, include the entire huge page of the data requested access to compared to transmission, but can be initially only by the subpage of data
Face or the small page are transmitted in main memory, which includes the data accessed by application request.For example, using may only need to visit
Byte or word are asked, so that system can be only by the data transmission of smaller portions to main memory, example using entire huge page is not needed
Such as, the small page, the data of smaller portions include using the data for continuing to run needs.When transmitting the small page, Ke Yixiang
It is indicated using making requested data and having been delivered in main memory or can be accessed now from main memory.
Then, because entire huge page is not communicated in main memory, the remainder of huge page can pass on backstage
It is sent in main memory.Alternatively, can determine that huge page, which is divided into the small page, to be advantageous for example based on access statistic data, thus
Paged data structure is changed into the small page of element from single huge page.If then access is not yet transferred to appointing for main memory for application
What remaining small page, then can transmit those small pages when requesting access to and page fault occurring.
It, can be in order to be made whether to transmit the remainder of huge page and merge or be divided into huge page the decision of the small page
Access statistic data is collected to identify " temperature " of the page, such as " heat " page and " cold " page in the page in huge page.Cause
This, according to certain embodiments, distribution and reserved memory in huge page block, but huge page can be resolved into the small page with compared with
Small page block works together.When a huge page is broken down into the small page, page table can use the corresponding PTE of each small page
It updates；It, can be by replacing the corresponding PTE of each small page with an entry of entire huge page come refresh page when recombinating huge page
Face table.
The access statistic data for collecting the page can be completed by determining any process or the means of page access, example
Such as, " cold " page and " heat " page are determined.For example, process may include that a huge page is periodically split into the small page, scanning should
Group the page and read access position with determine the small page of last visit time frequency or recently the small page how to be accessed, then
After access statistic data information is collected, the small page is reconfigured into back huge page.In this way, for example, accessing in huge page
Subpage frame or the sample of the small page can be used for obtaining about the statistical data for accessing the small page.
In more detail, in some embodiments, the scanning of page table scan, i.e. page table from first to last, can pass through
Hardware rather than need the software of CPU overhead to execute, need CPU overhead software execute typically result in slower internal storage access
With handle and abandon some other useful cache informations.In general, can determine the visit of page data by scanning page table
Frequency is asked, for example, can determine the page data being accessed frequently relative to other page of data and phase by the scanning page
The page data that other page of data are not often accessed.The each page mapped in page table, such as each PTE,
It can have the label that can be arranged in accession page or access position, then removed after scanning page table by CPU.
The hardware can realize by expanding page table walker 160 or MMU 150, the hardware may include one or
Multiple page table walkers, for example, it is built-in within hardware to read page table and be loaded into TLB to physical transformation for virtual automatically
In 165.Therefore, hardware can be a part of processor architecture, use the page table scan mechanism in processor.For example, hard
Whether the routine that part may be implemented scanning page table, scan PTE has been arranged at each PTE since last scan with determining
Access position.After determination is already provided with access position, access position can be removed, may then continue with operation until page table
Scanning next time.Scanning can periodically occur, for example, having predetermined amount of time between scans, or scanning can be with
It is triggered by certain external events.It can be respectively each page incremental count when determined every time provided with access position or being marked.Or
Person can be respectively each page incremental count when determining not set access position or label every time.
Over time, profile can be created from scanning, and profile can indicate the accessed frequency of each page
How rate and/or nearest each page are accessed.For example, hardware may include one or more counters for being used for each page
Or one or more Bloom filters, to safeguard the use statistical data such as the access position of setting or the counting of label, Huo Zhejie
Fruit can store in such as memory of two-stage memory, to allow to classify and filter as used the most and least page or access more
The frequent and less page.In more detail, hardware can safeguard how every page of counter is accessed with the nearest page of determination, and
Each counter can be updated when scanning corresponding PTE.Every page of counter can be provided on piece SRAM quickly to visit
It asks.Alternatively, keeping the area cost of counter higher because the size of secondary memory may be very big, it is possible to use counting cloth
Grand filter safeguards the access statistic data about page set, to save area.Alternatively, hardware can be used it is a small amount of privately owned
Counter can be stored in system dram by DRAM.
Thus, for example, being based on access statistic data, the page can be at least ranked up from using up to use, on the contrary
?.When page fault is serviced, if not having free page in main memory DRAM 170, paging process can will be used
One of least page is discharged or is write back in secondary memory, and the position using the least page can be used come will be new
The page is transmitted in main memory.
Fig. 4 depicts the example flow diagram of the process 400 for memory management according to embodiment of the present disclosure.Process
400 may include, and at 410, the request of the data in the first part that access includes the first data page is received from main memory.
First data page can have first page size, for example, huge page, and first part can have less than first page size
Second page size, such as the small page.At 420, it can be not stored in main memory simultaneously based on the first data page is determined
And it is stored in secondary memory to initiate page fault.In addition, in response to the initiation of page fault, can be distributed big at 430
A part of the small main memory for being equivalent to the first page size such as huge page.At 440, the first part of the first data page can be with
A part of assigned main memory is transmitted to from secondary memory without transmitting entire first data page.Therefore, the first data page
Surplus can be still stored in secondary memory.At 450, it can update associated with the first part of the first data page
First page table clause, to be directed toward the position of a part of assigned main memory, the first part of the first data page is passed
It is sent to the position.Then, the surplus of the first data page can be transmitted to main memory from secondary memory, for example, continuing in application
It is transmitted while operation on backstage.
It, can be by the corresponding with second page size each of the first data page in order to transmit the surplus of the first data page
Part is transmitted to a part of assigned main memory from secondary memory, until entire first data page is stored in main memory
Until.Furthermore, it is possible to the respective page table clause of each corresponding portion of the first data page be updated, to be directed toward first in main memory
The corresponding position of each section of data page.In addition, once entire first data page is stored in main memory, so that it may will be from secondary
Each section that grade memory is transmitted to first data page of a part of assigned main memory merges or is reassembled as the first data page.
According to recombination, page table entries associated with the first data page can be updated to be directed toward the first data page recombinated in main memory
Position.
In some embodiments, before the remainder of the first data page is transmitted to main memory from secondary memory,
System can indicate that the first part of the first data page of requested access has been delivered to main memory, so as to what is requested access to
Using or thread can by access main memory in requested data continue to run.
The example of process for memory management can also include the scanning based on page table scanner to page table, come true
Whether fixed be each page table entries of page table provided with access position.In such a process, access position can be indicated most
Scan in the period whether have accessed the page associated with page table entries afterwards.As described above, having the first page such as huge page
At least one of page of size can be divided into the page of the second page size such as the small page, second page size
The page just in scanned page table have for each second page size the page page table entries.Certain
In embodiment, in response to determining without accessing position for page table entries associated with page setting, every page can be incremented by
It counts.Then, after determining whether to be provided with access position for each page table entries, access position can be reset.
In some embodiments, if being unable to a part that allocated size is equivalent to the main memory of first page size,
Minimum one using in the page with first page size can be then determined based on the counting of each page, and institute is really
The fixed minimum use page can be released in secondary memory.It therefore, can be in minimum one using in the page of release
Position at, allocated size is equivalent to a part of the main memory of first page size.On the contrary, if can be with allocated size phase
When in the main memory part of first page size, then the first part of the first data page being transmitted to from secondary memory and be divided
The a part for the main memory matched.
In more detail, for example, can be based on if main memory does not have free page and cannot receive page transmission
Each page counts to determine in main memory at least using one in the page.Page fault handler or controller can manage
Page transmission is managed, and identified minimum one using in the page can be released or write back in secondary memory.In addition,
The page of data requested access to can be transmitted to minimum one 's using in the page of the release in main memory from secondary memory
At position.Alternatively, if main memory there is free page really and can receive the page transmission, page fault handler or
Controller can be with transmission of the administration page data from secondary memory to main memory.
In certain embodiments, when initiating page fault, the execution of thread or the operation of program can be stagnated, while right
Data transmission is managed with service page mistake, as described above.Then, after service page mistake, thread can be discharged
To access the page in main memory.
In some embodiments, it may be advantageous below: not only determining which page in secondary memory becomes
" heat ", the i.e. increase of access frequency, but also determine which page in main memory DRAM becomes " cold ", i.e., access frequency subtracts
It is few.It in other words, it is determined the page more frequently accessed in secondary memory that may be slower than main memory, and determine in master
The page less frequently being accessed in memory.It is illustrated above the counting based on the access position being such as set, with reference to for such as
The use of the main memory of DRAM or access statistic data determine a kind of mistake for the page less frequently being accessed in main memory
Journey.Data can be moved to time based on the access statistic data described above for main memory by system to determine from main memory
The time of grade memory and the determining time that data are moved to main memory from secondary memory.
In addition, it is as described above in greater detail, cooling can be determined by monitoring PTE or by the page of less access.
For example, when the access time of the page meeting access time threshold value, system can by make PTE associated with the page it is invalid,
It is transmitted to the associated any entry execution page table cache closing (TLB shootdown) of the page and by the page from main memory
To secondary memory, the Lai Faqi page is from main memory to the transmission of the second memory.
According to certain embodiments, interconnected using buffer consistency, DRAM cache and two layers of memory can by hardware management,
Hardware can serve as the owner of consistency memory and the user of consistency memory.In other words, DRAM is served as by hardware management
Caching, for carrying out paging with configurable granularity to obtain optimum performance.Configurable granularity for optimum performance is desirable
Certainly in the performance of application site and two layers of memory.
The address space that the kernel driver customized as described above can only be possessed with huge page mapping hardware.With this side
Formula, when system accesses the region of memory, the benefit of huge page is may be implemented in system, such as changes due to bigger TLB range
Kind performance.Hardware can safeguard cache lookup structure to check in main memory with the presence or absence of the page.In being received from host
When depositing access, the lookup structure can be inquired.If the page exists, reading or write-in can be executed directly in main memory.
If the page is not present, data can be fetched into the main memory such as DRAM from secondary memory.In some embodiments,
For performance consider, caching asynchronously can execute expulsion-for example, " cold " data can write back to from the background second-level storage with
Kept for the page free time of minimum number to service the page of entrance.In general, the process can provide caching mechanism in page level,
Allow to provide buffer consistency interconnection for the caching for paging.
In some embodiments, can be optimized with application cache.For example, being deposited when page fault and the page occurs from secondary
When reservoir is transmitted to main memory, system may determine whether for the page to be maintained in main memory, or if the page is nonvolatile
Property, it may determine whether to transmit the page as a stream and read the page for given access, or may determine whether to lead to
It crosses and determines that obtaining lower one page executes pre-acquiring to respond the currently accessed page.In some embodiments, it can be set
The label of memory how is used about application, and the determination for carrying out pre-acquiring can be made based on those labels.
Many embodiments have been described.It should be appreciated, however, that the case where not departing from spirit and scope of the present disclosure
Under, various modifications can be carried out.It is, for example, possible to use the various forms of flow shown above, are arranged again its step
Sequence, addition step or removing step.
Embodiments of the present invention described in this specification and all feature operations may be implemented Fundamental Digital Circuit,
In firmware or hardware, including structure disclosed in this specification and its equivalent structures or one of these or it is multiple
Combination.Embodiments of the present invention can be implemented as one or more computer program products, that is, on a computer-readable medium
One or more computer program instructions modules of coding, for being executed by data processing equipment or being controlled data processing equipment
Operation.Computer-readable medium can be machine readable storage device, machine readable storage substrate, memory device or these in
One or more combinations.Term " data processing equipment " includes all devices, equipment and the machine for handling data, packet
Include such as programmable processor, computer or multiple processors or computer.In addition to hardware, which can also include being
The code of the computer program creation performing environment discussed, for example, constituting code, protocol stack, the database of processor firmware
Management system, operating system or one of these or multiple combinations.
Although the disclosure includes many details, these details are not necessarily to be construed as to the present invention or can claimed model
The limitation enclosed, but as the description to the detailed feature of only certain exemplary embodiments of this invention.In the upper of different embodiments
The certain features hereinafter described in the present specification can also combine realization in single embodiment.Conversely, single real
Applying various features described in the context of mode can also be implemented separately or with any suitable in multiple embodiments
Sub-portfolio is realized.In addition, although features described above can be described as working with certain combinations and even initially so stating,
It is that can be cut off from combination in some cases from combined one or more features claimed, and required
The combination of protection can be for the variation of sub-portfolio or sub-portfolio.
Similarly, although depicting operation in the accompanying drawings with particular order, this is not construed as requiring with institute
The particular order shown implements these operations in order, or implements all operations shown, to realize desired result.At certain
In a little situations, multitasking and parallel processing be may be advantageous.In addition, various system components in above embodiment
Separation is understood not to require this separation in all embodiments, and it is to be understood that described program assembly
Usually it can integrate or be packaged into various software product in single software product with system.
Therefore, it has been described that the particular implementation of the disclosure.Other embodiments are in the scope of the following claims
It is interior.For example, the movement recorded in claim can be implemented in a different order and still realize desired result.It has retouched
Many embodiments are stated.It should be appreciated, however, that without departing from the spirit and scope of the disclosure, can carry out each
Kind modification.It is, for example, possible to use various forms of processes illustrated above, wherein the step of be reordered, be added or
It is removed.Therefore, other embodiments are in the range of following claims.
Claims (20)
1. a method of computer implementation characterized by comprising
The request of the data in the first part that access includes the first data page, the first data page tool are received from main memory
There is first page size, and the first part includes second page size, the second page size is less than described first
Page-size；
It is not stored in main memory and is stored in secondary memory based on determination first data page to initiate page fault；
In response to the initiation of the page fault, a part of main memory, the sizableness of a part of the main memory are distributed
In the size of the first page；
The first part of first data page is transmitted to the one of the assigned main memory from the secondary memory
Part is without transmitting entire first data page, wherein the surplus of first data page continues to be stored in the secondary
In depositing；And
First page table clause associated with the first part of first data page is updated, to be directed toward first number
The position of a part for the assigned main memory being sent to according to the first part of page.
2. the method as described in claim 1, which is characterized in that further include:
The surplus of first data page is transmitted to the main memory from the secondary memory.
3. method according to claim 2, which is characterized in that the surplus packet of transmission first data page
It includes:
Each section of first data page of the correspondence second page size is repeatedly transmitted from the secondary memory to quilt
A part of the main memory of distribution, until entire first data page is stored in the main memory；And
Each page table entries are updated for the described each section of each of first data page, described in being directed toward in the main memory
The each position of each section of the first data page.
4. method as claimed in claim 3, which is characterized in that further include:
Once entire first data page is stored in the main memory, then will be transmitted to from the secondary memory assigned
Described each section of first data page of a part of the main memory be reassembled as first data page；And
Page table entries associated with first data page are updated with described the first of the recombination being directed toward in the main memory
The position of data page.
5. method as claimed in claim 2 or claim 3, which is characterized in that further include:
Before the remainder of first data page is transmitted to the main memory from the secondary memory, instruction is requested
The first part of first data page of access has been sent in the main memory.
6. method described in any one of claim as in the previous, which is characterized in that further include:
Scanning based on page table scanner to page table, it is determined whether be provided with for each page table entries of the page table
Position is accessed, the access position indicates whether the page associated with the page table entries is interviewed in the last scan period
It asks, wherein at least one page with the first page size is divided into the page of the second page size, it is described
The page of second page size has the institute for each second page size in the scanned page table
State the page table entries of the page；
In response to determining that the access position is not arranged for the page table entries associated with the page, it is incremented by each institute
State the counting of the page；
After determining whether to be provided with the access position for each page table entries, the access position is reset；And
The page with the second page size is reassembled as the divided page with the first page size.
7. method as claimed in claim 6, which is characterized in that further include:
If sizableness cannot be assigned in a part of the main memory of the first page size, based on each page
It is described to count the minimum page used for determining and there is the first page size, and the page at least used is released
It is put into the secondary memory, and distributes sizableness in described the at the position of the page at least used of release
A part of the main memory of one page-size；And
If a part for being equivalent to the main memory of the first page size can be assigned, by first data page
The first part a part of the assigned main memory is transmitted to from secondary memory.
8. the method any one of before as described in claim, which is characterized in that further include:
The internal storage structure of first data page with the first page size is changed into big with the second page
Small multiple data pages, the second page size are less than the first page size.
9. a kind of system characterized by comprising
One or more processors；With
Memory, the memory include main memory and secondary memory, and the memory storage instructs, and described instruction can be grasped when executed
Make so that one or more of processors execute operation, the operation includes:
The request of the data in the first part that access includes the first data page, first data are received from the main memory
Page has first page size, and the first part includes second page size, and the second page size is less than described
First page size；
It is not stored in the main memory based on determination first data page and is stored in the secondary memory and initiates
Page fault；
In response to the initiation of the page fault, a part of main memory, the sizableness of a part of the main memory are distributed
In the size of the first page；
The first part of first data page is transmitted to a part of the assigned main memory from secondary memory
Without transmitting entire first data page, wherein the surplus of first data page continues to be stored in the secondary memory
In；And
First page table clause associated with the first part of first data page is updated, to be directed toward first number
The position of a part for the assigned main memory being sent to according to the first part of page.
10. system as claimed in claim 9, which is characterized in that the operation further include:
The surplus of first data page is transmitted to the main memory from the secondary memory.
11. system as claimed in claim 10, which is characterized in that the surplus packet of transmission first data page
It includes:
By correspond to the second page size first data page each section from the secondary memory repeatedly transmit to
A part of the assigned main memory, until entire first data page is stored in the main memory；And
A page table entries are updated for the described each section of each of first data page, described in being directed toward in the main memory
The each position of each section of the first data page.
12. system as claimed in claim 11, which is characterized in that the operation further include:
Once entire first data page is stored in the main memory, then will be transmitted to from the secondary memory assigned
Described each section of first data page of a part of the main memory be reassembled as first data page；And
Page table entries associated with first data page are updated to be directed toward first number recombinated in the main memory
According to the position of page.
13. system as described in claim 10 or 11, which is characterized in that the operation further include:
Before the remainder of first data page is transmitted to the main memory from the secondary memory, instruction is requested
The first part of first data page of access has been sent in the main memory.
14. the system as described in any one of claim 9 to 13, which is characterized in that the operation further include:
Scanning based on page table scanner to page table, it is determined whether be provided with for each page table entries of the page table
Position is accessed, the access position indicates whether the page associated with the page table entries is interviewed in the last scan period
It asks, wherein at least one page with the first page size is divided into the page of the second page size, it is described
The page of second page size has the institute for each second page size in the scanned page table
State the page table entries of the page；
In response to determining that the access position is not arranged for the page table entries associated with the page, it is incremented by each institute
State the counting of the page；
After determining whether to be provided with the access position for each page table entries, the access position is reset；And
The page with the second page size is reassembled as the divided page with the first page size.
15. system as claimed in claim 14, which is characterized in that the operation further include:
If sizableness cannot be assigned in a part of the main memory of the first page size, based on each page
It is described to count the minimum page used for determining and there is the first page size, and the page at least used is released
It is put into the secondary memory, and distributes sizableness in described at the position of the page at least used of release
A part of the main memory of first page size；And
If a part for being equivalent to the main memory of the first page size can be assigned, by first data page
The first part a part of the assigned main memory is transmitted to from secondary memory.
16. the system as described in any one of claim 9 to 15, which is characterized in that the operation further include:
The internal storage structure of first data page with the first page size is changed into big with the second page
Small multiple data pages, the second page size are less than the first page size.
17. a kind of computer readable storage devices, which is characterized in that the storage equipment storage can be by one or more processors
The instruction of execution, described instruction make one or more of processors execute operation when executed, and the operation includes:
The request of the data in the first part that access includes the first data page, the first data page tool are received from main memory
There is first page size, and the first part includes second page size, the second page size is less than described first
Page-size；
It is not stored in the main memory based on determination first data page and is stored in the secondary memory and initiates
Page fault；
In response to the initiation of the page fault, a part of main memory, the sizableness of a part of the main memory are distributed
In the size of the first page；
The first part of first data page is transmitted to the assigned main memory one from the secondary memory
Divide without transmitting entire first data page, wherein the surplus of first data page continues to be stored in the secondary memory
In；And
First page table clause associated with the first part of first data page is updated, to be directed toward first number
The position of a part for the assigned main memory being sent to according to the first part of page.
18. storage equipment as claimed in claim 17, which is characterized in that the operation further include:
The surplus of first data page is transmitted to the main memory from the secondary memory.
19. storage equipment as claimed in claim 18, which is characterized in that the residue of transmission first data page
Amount includes:
By correspond to the second page size first data page each section from the secondary memory repeatedly transmit to
A part of the assigned main memory, until entire first data page is stored in the main memory；
Each page table entries are updated for the described each section of each of first data page, described in being directed toward in the main memory
The each position of each section of the first data page；
Once entire first data page is stored in the main memory, then will be transmitted to from the secondary memory assigned
Described each section of first data page of a part of the main memory be reassembled as first data page；And
Page table entries associated with first data page are updated with described the first of the recombination being directed toward in the main memory
The position of data page.
20. storage equipment described in any one of claim as in the previous, which is characterized in that the operation further include:
Scanning based on page table scanner to page table, it is determined whether be provided with for each page table entries of the page table
Position is accessed, the access position indicates whether the page associated with the page table entries is interviewed in the last scan period
It asks, wherein at least one page with the first page size is divided into the page of the second page size, it is described
The page of second page size has the institute for each second page size in the scanned page table
State the page table entries of the page；
In response to determining that the access position is not arranged for the page table entries associated with the page, it is incremented by each institute
State the counting of the page；
After determining whether to be provided with the access position for each page table entries, the access position is reset；
The page with the second page size is reassembled as to the page of divided first page size；
If sizableness cannot be assigned in a part of the main memory of the first page size, based on each page
It is described to count the minimum page used for determining and there is the first page size, and the page at least used is released
It is put into the secondary memory, and distributes sizableness in described the at the position of the page at least used of release
A part of the main memory of one page-size；
If sizableness can be assigned in a part of the main memory of the first page size, by first data page
The first part from a part of the assigned main memory of secondary memory transmission.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202310717151.8A CN116701250A (en) | 2016-09-22 | 2017-08-25 | Memory management supporting megapages |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/273,433 | 2016-09-22 | ||
US15/273,433 US10108550B2 (en) | 2016-09-22 | 2016-09-22 | Memory management supporting huge pages |
PCT/US2017/048663 WO2018057235A1 (en) | 2016-09-22 | 2017-08-25 | Memory management supporting huge pages |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202310717151.8A Division CN116701250A (en) | 2016-09-22 | 2017-08-25 | Memory management supporting megapages |
Publications (2)
Publication Number | Publication Date |
---|---|
CN109791523A true CN109791523A (en) | 2019-05-21 |
CN109791523B CN109791523B (en) | 2023-07-14 |
Family
ID=59772830
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202310717151.8A Pending CN116701250A (en) | 2016-09-22 | 2017-08-25 | Memory management supporting megapages |
CN201780058759.7A Active CN109791523B (en) | 2016-09-22 | 2017-08-25 | Memory management supporting megapages |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202310717151.8A Pending CN116701250A (en) | 2016-09-22 | 2017-08-25 | Memory management supporting megapages |
Country Status (9)
Country | Link |
---|---|
US (2) | US10108550B2 (en) |
EP (1) | EP3516526B1 (en) |
JP (1) | JP6719027B2 (en) |
KR (1) | KR102273622B1 (en) |
CN (2) | CN116701250A (en) |
DK (1) | DK3516526T3 (en) |
IE (2) | IE20170188A1 (en) |
SG (2) | SG10201903332RA (en) |
WO (1) | WO2018057235A1 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111913893A (en) * | 2020-06-22 | 2020-11-10 | 成都菁蓉联创科技有限公司 | Mapping method and device for reserved memory, equipment and storage medium |
CN112148736A (en) * | 2020-09-23 | 2020-12-29 | 北京字节跳动网络技术有限公司 | Method, device and storage medium for caching data |
CN113641464A (en) * | 2021-10-15 | 2021-11-12 | 云宏信息科技股份有限公司 | Memory configuration method and system of XEN platform and computer readable storage medium |
WO2022062524A1 (en) * | 2020-09-22 | 2022-03-31 | 华为技术有限公司 | Memory management method and apparatus, device and storage medium |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN110427340B (en) * | 2018-04-28 | 2023-08-04 | 伊姆西Ip控股有限责任公司 | Method, apparatus and computer storage medium for file storage |
US20190354470A1 (en) * | 2018-05-16 | 2019-11-21 | Sap Se | Reduced Database Backup Memory Usage |
US10956058B2 (en) * | 2018-08-03 | 2021-03-23 | Western Digital Technologies, Inc. | Tiered storage system with tier configuration by peer storage devices |
US10949356B2 (en) * | 2019-06-14 | 2021-03-16 | Intel Corporation | Fast page fault handling process implemented on persistent memory |
US11392428B2 (en) * | 2019-07-17 | 2022-07-19 | Memverge, Inc. | Fork handling in application operations mapped to direct access persistent memory |
US20210019069A1 (en) * | 2019-10-21 | 2021-01-21 | Intel Corporation | Memory and storage pool interfaces |
JP6972202B2 (en) * | 2020-02-14 | 2021-11-24 | 株式会社日立製作所 | Computer system and memory management method |
US11829298B2 (en) * | 2020-02-28 | 2023-11-28 | Apple Inc. | On-demand memory allocation |
CN111666230B (en) * | 2020-05-27 | 2023-08-01 | 江苏华创微系统有限公司 | Method for supporting macro page in set associative TLB |
US20220382478A1 (en) * | 2021-06-01 | 2022-12-01 | Samsung Electronics Co., Ltd. | Systems, methods, and apparatus for page migration in memory systems |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090172344A1 (en) * | 2007-12-31 | 2009-07-02 | Ed Grochowski | Method, system, and apparatus for page sizing extension |
CN102473091A (en) * | 2009-07-01 | 2012-05-23 | 超威半导体公司 | Extended page size using aggregated small pages |
CN102473138A (en) * | 2009-06-29 | 2012-05-23 | 甲骨文美国公司 | Extended main memory hierarchy having flash memory for page fault handling |
Family Cites Families (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5361345A (en) | 1991-09-19 | 1994-11-01 | Hewlett-Packard Company | Critical line first paging system |
US5987561A (en) * | 1995-08-31 | 1999-11-16 | Advanced Micro Devices, Inc. | Superscalar microprocessor employing a data cache capable of performing store accesses in a single clock cycle |
US5960463A (en) * | 1996-05-16 | 1999-09-28 | Advanced Micro Devices, Inc. | Cache controller with table walk logic tightly coupled to second level access logic |
US6112285A (en) | 1997-09-23 | 2000-08-29 | Silicon Graphics, Inc. | Method, system and computer program product for virtual memory support for managing translation look aside buffers with multiple page size support |
US6804729B2 (en) | 2002-09-30 | 2004-10-12 | International Business Machines Corporation | Migrating a memory page by modifying a page migration state of a state machine associated with a DMA mapper based on a state notification from an operating system kernel |
US7447869B2 (en) * | 2005-04-07 | 2008-11-04 | Ati Technologies, Inc. | Method and apparatus for fragment processing in a virtual memory system |
US7519781B1 (en) | 2005-12-19 | 2009-04-14 | Nvidia Corporation | Physically-based page characterization data |
US8543792B1 (en) | 2006-09-19 | 2013-09-24 | Nvidia Corporation | Memory access techniques including coalesing page table entries |
US7917725B2 (en) | 2007-09-11 | 2011-03-29 | QNX Software Systems GmbH & Co., KG | Processing system implementing variable page size memory organization using a multiple page per entry translation lookaside buffer |
US8615642B2 (en) | 2009-10-14 | 2013-12-24 | International Business Machines Corporation | Automatic page promotion and demotion in multiple page size environments |
US8533382B2 (en) * | 2010-01-06 | 2013-09-10 | Vmware, Inc. | Method and system for frequent checkpointing |
US9158701B2 (en) | 2012-07-03 | 2015-10-13 | International Business Machines Corporation | Process-specific views of large frame pages with variable granularity |
US10133677B2 (en) | 2013-03-14 | 2018-11-20 | Nvidia Corporation | Opportunistic migration of memory pages in a unified virtual memory system |
US20150058520A1 (en) | 2013-08-22 | 2015-02-26 | International Business Machines Corporation | Detection of hot pages for partition migration |
US9864698B2 (en) | 2013-11-04 | 2018-01-09 | International Business Machines Corporation | Resolving cache lookup of large pages with variable granularity |
US9535831B2 (en) | 2014-01-10 | 2017-01-03 | Advanced Micro Devices, Inc. | Page migration in a 3D stacked hybrid memory |
US9501422B2 (en) | 2014-06-11 | 2016-11-22 | Vmware, Inc. | Identification of low-activity large memory pages |
CN105095099B (en) | 2015-07-21 | 2017-12-29 | 浙江大学 | A kind of big page integration method based on the change of page bitmap |
US10037173B2 (en) | 2016-08-12 | 2018-07-31 | Google Llc | Hybrid memory management |
US10152427B2 (en) | 2016-08-12 | 2018-12-11 | Google Llc | Hybrid memory management |
-
2016
- 2016-09-22 US US15/273,433 patent/US10108550B2/en active Active
-
2017
- 2017-08-25 WO PCT/US2017/048663 patent/WO2018057235A1/en unknown
- 2017-08-25 JP JP2019536814A patent/JP6719027B2/en active Active
- 2017-08-25 DK DK17761767.7T patent/DK3516526T3/en active
- 2017-08-25 EP EP17761767.7A patent/EP3516526B1/en active Active
- 2017-08-25 CN CN202310717151.8A patent/CN116701250A/en active Pending
- 2017-08-25 KR KR1020197011367A patent/KR102273622B1/en active IP Right Grant
- 2017-08-25 CN CN201780058759.7A patent/CN109791523B/en active Active
- 2017-09-18 SG SG10201903332RA patent/SG10201903332RA/en unknown
- 2017-09-18 SG SG10201707699VA patent/SG10201707699VA/en unknown
- 2017-09-20 IE IE20170188A patent/IE20170188A1/en not_active IP Right Cessation
- 2017-09-20 IE IE20180302A patent/IE87058B1/en unknown
-
2018
- 2018-08-27 US US16/113,285 patent/US10474580B2/en active Active
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090172344A1 (en) * | 2007-12-31 | 2009-07-02 | Ed Grochowski | Method, system, and apparatus for page sizing extension |
CN102473138A (en) * | 2009-06-29 | 2012-05-23 | 甲骨文美国公司 | Extended main memory hierarchy having flash memory for page fault handling |
CN102473091A (en) * | 2009-07-01 | 2012-05-23 | 超威半导体公司 | Extended page size using aggregated small pages |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111913893A (en) * | 2020-06-22 | 2020-11-10 | 成都菁蓉联创科技有限公司 | Mapping method and device for reserved memory, equipment and storage medium |
WO2022062524A1 (en) * | 2020-09-22 | 2022-03-31 | 华为技术有限公司 | Memory management method and apparatus, device and storage medium |
CN112148736A (en) * | 2020-09-23 | 2020-12-29 | 北京字节跳动网络技术有限公司 | Method, device and storage medium for caching data |
CN112148736B (en) * | 2020-09-23 | 2024-03-12 | 抖音视界有限公司 | Method, device and storage medium for caching data |
CN113641464A (en) * | 2021-10-15 | 2021-11-12 | 云宏信息科技股份有限公司 | Memory configuration method and system of XEN platform and computer readable storage medium |
Also Published As
Publication number | Publication date |
---|---|
IE20180302A1 (en) | 2018-10-31 |
US10108550B2 (en) | 2018-10-23 |
EP3516526B1 (en) | 2020-10-14 |
JP2019532450A (en) | 2019-11-07 |
KR20190052106A (en) | 2019-05-15 |
CN116701250A (en) | 2023-09-05 |
US10474580B2 (en) | 2019-11-12 |
WO2018057235A1 (en) | 2018-03-29 |
US20180365157A1 (en) | 2018-12-20 |
US20180081816A1 (en) | 2018-03-22 |
SG10201707699VA (en) | 2018-04-27 |
JP6719027B2 (en) | 2020-07-08 |
IE87058B1 (en) | 2019-10-16 |
DK3516526T3 (en) | 2020-11-30 |
EP3516526A1 (en) | 2019-07-31 |
SG10201903332RA (en) | 2019-05-30 |
CN109791523B (en) | 2023-07-14 |
IE20170188A1 (en) | 2018-04-04 |
KR102273622B1 (en) | 2021-07-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109791523A (en) | Support the memory management of huge page | |
CN110869913B (en) | Memory system for a data processing network | |
CN111177030B (en) | Hybrid memory management | |
US10705975B2 (en) | Hybrid memory management | |
US10235290B2 (en) | Hot page selection in multi-level memory hierarchies | |
US10289555B1 (en) | Memory read-ahead using learned memory access patterns | |
CN106030553B (en) | Memory network | |
US10459852B1 (en) | Memory utilization analysis for memory management systems | |
CN110362504A (en) | Management to consistency link and multi-level store | |
CN105917319A (en) | Memory unit and method | |
JP2022517318A (en) | Cache replacement based on translation lookaside buffer eviction | |
KR101893966B1 (en) | Memory management method and device, and memory controller | |
CN110869916B (en) | Method and apparatus for two-layer copy-on-write | |
WO2004061676A2 (en) | Allocating cache lines | |
CN114761934A (en) | In-process Translation Lookaside Buffer (TLB) (mTLB) for enhancing a Memory Management Unit (MMU) TLB for translating Virtual Addresses (VA) to Physical Addresses (PA) in a processor-based system | |
CN113010452B (en) | Efficient virtual memory architecture supporting QoS | |
Wang et al. | Superpage-Friendly Page Table Design for Hybrid Memory Systems | |
Jing et al. | Construction and optimization of heterogeneous memory system based on NUMA architecture | |
Kokolis | New architectures for non-volatile memory technologies | |
Riekenbrauck et al. | A Three-Tier Buffer Manager Integrating CXL Device Memory for Database Systems | |
김현익 | RapidSwap: An Efficient Hierarchical Far Memory | |
KR101480954B1 (en) | NUMA System Scheduling Apparatus and Secheduing Method Therefor |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |