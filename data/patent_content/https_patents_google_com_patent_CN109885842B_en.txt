CN109885842B - Processing text neural networks - Google Patents
Processing text neural networks Download PDFInfo
- Publication number
- CN109885842B CN109885842B CN201910134308.8A CN201910134308A CN109885842B CN 109885842 B CN109885842 B CN 109885842B CN 201910134308 A CN201910134308 A CN 201910134308A CN 109885842 B CN109885842 B CN 109885842B
- Authority
- CN
- China
- Prior art keywords
- neural network
- image
- embedding
- text segment
- digital
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5846—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using extracted text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/26—Techniques for post-processing, e.g. correcting the recognition result
- G06V30/262—Techniques for post-processing, e.g. correcting the recognition result using context analysis, e.g. lexical, syntactic or semantic context
- G06V30/274—Syntactic or semantic context, e.g. balancing
Abstract
The present disclosure relates to processing text neural networks. Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for generating a data set associating each text segment in a vocabulary of text segments with a respective digital embedding. In one aspect, a method includes: providing a search query including the text segment to an image search engine; obtaining image search results that have been categorized as being responsive to the search query by the image search engine, wherein each image search result identifies a respective image; for each image search result, processing the image identified by the image search result using a convolutional neural network, wherein the convolutional neural network has been trained to process the image to generate an image digital embedding for the image; and generating a digital embedding for the text segment from the image digital embedding for the image identified by the image search result.
Description
Technical Field
The present description relates to processing text using neural networks.
Background
Neural networks are machine learning models that use one or more layers of nonlinear cells to predict the output of a received input. In addition to the output layer, some neural networks include one or more hidden layers. The output of each hidden layer serves as an input to the next layer in the network, the next hidden layer or output layer. Each layer of the network generates an output from the received inputs based on the current values of the respective parameter sets.
Some neural networks are recurrent neural networks. A recurrent neural network is a neural network that receives an input sequence and generates an output sequence from the input sequence. In particular, the recurrent neural network can calculate the output of the current time step using some or all of the internal states of the network from the previous time step. An example of a recurrent neural network is a Long Short Term (LSTM) neural network that includes one or more LSTM memory blocks. Each LSTM memory block can include one or more cells, each cell including an input gate, a forget gate, and an output gate that allow the cell to store a previous state of the cell, e.g., for generating a current activation or other component provided to the LSTM neural network.
Disclosure of Invention
The present specification describes a system for generating or using digital embedding of text segments, the system being implemented as a computer program on one or more computers in one or more locations. Text segments can be words or phrases that contain multiple words. Numerical embedding of text segments is an ordered set of numerical digits in an embedding space having a predetermined dimension, e.g., a floating point value or a vector of quantized floating point values. In particular, the system generates and/or uses digital embedding that is based using image searching, i.e., incorporates visual features associated with text segments by utilizing image searching.
The subject matter described in this specification can be implemented in specific embodiments to realize one or more of the following advantages.
Various machine learning systems receive and operate as input the embedding of text segments. For example, a machine learning system that performs machine translation receives as input the embeddings of text in a source language to be translated and operates on these embeddings to translate the text into a source language. As another example, a machine learning system that performs natural language processing tasks such as text summaries, part-of-speech tags, entity tags, etc., receives as input the embedding of text and operates on these embeddings to perform natural language processing tasks on the text. Once generated, the embedding can be used to improve the performance of any of these machine learning systems. That is, because the embeddings are based using image searches as described herein, the performance of the system can be improved over using conventional embeddings when the embeddings or embeddings derived from the embeddings are used by a machine learning system to represent corresponding text segments.
In addition, the system is able to efficiently generate an embedding for a vocabulary comprising a large number of text segments without consuming excessive computing resources. In particular, because the system utilizes a pre-trained convolutional neural network and an existing image search engine, the system is able to efficiently generate a large number of vision-based (visually grounded) embeddings.
The present specification also describes techniques for combining the embedding with embedding generated using other sources that utilize a gated neural network. By generating the combined embeddings in this way, the weights assigned to the image search based embeddings can be determined in a task-specific manner during training of the gated neural network. This allows for further improvement in the performance of machine learning systems employing gated neural networks. That is, the performance of a system incorporating a gate system rather than embedding network inputs using conventional methods will increase.
In addition, as described in this specification, by replacing the conventional output layer of the neural network system with a layer that searches for similarity between the internal representation generated by the neural network, i.e., the data that would otherwise be input to the conventional output layer, and the digital embedding of the text segment, the embedding can be used to improve the quality of the output generated by the neural network system. By selecting the output in such a way that the text segments are selected to be output according to the similarity measure, the system is able to generate a higher quality output sequence without unduly increasing the computational resources required to run the neural network.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example text embedding system.
FIG. 2 is a flow chart of an example process for generating vision-based embedding of text segments.
FIG. 3 is a flow chart of an example process for generating a final embedding of a text segment.
FIG. 4 is a flow chart of an example process for selecting a text segment of an output step size.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
Fig. 1 illustrates an example text embedding system 100. Text-embedding system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented.
The text embedding system 100 generates digital embedding of text segments. In particular, the text embedding system 100 receives the vocabulary of text segments 106 and generates a data set 110, the data set 110 including a respective digital embedding for each text segment in the vocabulary 106.
The text segments in the vocabulary 106 can include words, multi-word phrases, or both. That is, in some embodiments, the vocabulary is a vocabulary of words. In some other implementations, the vocabulary is a vocabulary of multi-word phrases. In other embodiments, the vocabulary includes both words and multi-word phrases.
The digital embedding of text segments is an ordered set of values, such as floating point vectors or quantized floating point values, in an embedding space having a predetermined dimension.
In particular, the system 100 generates digital embedding based on visual features associated with text segments using image searching, i.e., by utilizing the image search engine 114. Because the embedding is generated using image searching, the embedding generated by the system 100 will be referred to as "vision-based" embedding in this specification.
The system 100 is capable of interacting with an image search engine 114 over a data communications network 112, such as a Local Area Network (LAN) or Wide Area Network (WAN), such as the internet, or a combination of networks.
The image search engine 114 is configured to search a collection of images. Typically, the images in the collection are images found on web pages on the Internet. In particular, the image search engine 114 can be any image search engine on the Internet that receives a search query and, in response, provides image search results that the image search engine has classified in response to the search query and each identifies a corresponding image. Typically, image search engines rank image search results based on responsiveness, i.e., from most responsive to least responsive.
Thus, the system 100 is able to submit a search query including text segments 108 to an image search engine 114 over the network 112 and, in response to the search query, receive image search results 128 that the image search engine 114 has classified in response to the search query. .
The system 100 also includes a convolutional neural network 120. Convolutional neural network 120 is a neural network that includes multiple convolutional neural network layers and has been trained to generate an embedding of an image (referred to as "image embedding" in this specification). In particular, convolutional neural network 120 has been trained on a particular target to generate an image embedding that reflects certain properties of the image that is input into convolutional neural network 120. As a particular example, convolutional neural network 120 can have been trained on semantic ordering targets to generate image-number embeddings that preserve semantic similarity between input images. The semantic ordering objective may be in the form of hinge loss that operates on triplets of training images and is based on semantic similarity between images in the triplets. An example architecture for convolutional neural networks, and example techniques for training convolutional neural networks to preserve semantic similarity between images, are described in "Learning engine-grained Image Similarity with Deep Ranking" published in CVPR in Jiang Wang, yang Song, thomas Leung, chuck Rosenberg, jingbin Wang, james Philin, bo Chen, and Yeng Wu, 2014.
The generation of the data set 110 using the image search engine 114 and the convolutional neural network 120 is described in more detail below with reference to FIG. 2.
Once the system 100 has generated the data set 110, the system 100 can use the generated data set 110 for any of a variety of purposes to improve the operation of any of a variety of machine learning systems.
For example, the system can provide the representation embedded as the corresponding text segment as input to the machine learning system. Since the manner in which vision-based embedding has been generated, using vision-based embedding instead of embedding generated using other techniques can improve the performance of machine learning systems in various machine learning tasks.
As another example, the system can combine the embedding in the data set 110 with the embedding from a different data set (alternative data set 118) that has been generated using a different technique to provide task-specific embedding in response to the received request. The alternative dataset 118 may have been generated using any conventional technique for generating text embeddings. An example of an alternative dataset 118 is the generated GloVe embedding as described in "GloVe: global Vectors for Word Representation" published in EMNLP under Jeffrey Pennington, richard society and Christopher Manning, 2014. Another example of an alternative dataset 118 is word2vec embedding generated as described in "Distributed Representations of Words and Phrases and their Compositionality" by Tomas Mikolov et al.
In other words, the system can receive a request to embed text segments in the vocabulary for certain machine learning tasks, i.e., as input to a task-specific machine learning model configured to perform the machine learning tasks. The system can combine the vision-based embedding of text segments with the embedding of text segments generated using other techniques in a task-specific manner to generate a final embedding, and then provide the final embedding in response to a request.
The task-specific way of combining embedding is described in more detail below with reference to fig. 3.
As another example, the system can use the generated vision-based embedding to improve the operation of a task-specific neural network that generates an output that includes text segments in a vocabulary. In particular, the system is able to replace the traditional output layer of a task-specific neural network by using vision-based embedding to select an operation of the output generated by the task-specific neural network.
The use of vision-based embedding to select an output is described in more detail below with reference to fig. 4.
FIG. 2 is a flow chart of an example process for generating an embedding of text segments. For convenience, process 200 will be described as being performed by a system of one or more computers located at one or more locations. For example, an embedded system, such as embedded system 100 of fig. 1, suitably programmed in accordance with the present description is capable of performing process 200.
The system can perform process 200 on each text segment in the vocabulary of text segments to generate a dataset that associates each text segment in the vocabulary with a corresponding vision-based numeric embedding.
The system provides a search query including text segments to an image search engine (step 202).
The system obtains image search results for a search query from an image search engine (step 204). The image search results each identify a respective image and have been categorized by the image search engine in response to a search query. The image search results are also ranked by the image search engine from most responsive to least responsive, i.e., the first search result in the order is most responsive, the fifth search result in the order is the fifth most responsive, and so on.
In some cases, the system receives from the image search engine an initial set of image search results ordered by the image search engine from most responsive to least responsive and selects a subset of the initial set of image search results most responsive to the search query according to an order used in generating the text segment embedding. For example, the system can select the first five, ten, or twenty most responsive search results for generating an embedding of the text segment.
For each image search result, the system processes the image identified by the image search result using a convolutional neural network (step 206).
As described above, convolutional neural networks have been trained to process input images to generate image digital embeddings of the images.
The system generates a digital embedding for the text segment from the image digital embedding for the image identified by the image search result (step 208). In particular, the system combines image digital embedding for images identified by the search results to generate digital embedding for text segments.
As a specific example, the system can concatenate image number embeddings according to the ordering of the respective image search results, i.e., such that the embedding of text segments is a concatenation of image number embeddings of the search results. In other words, the system concatenates the image number embedding with the image embedding of the first search result in order, the image number embedding of the fifth search result in order, and so forth.
By performing process 200 on all text segments in the vocabulary, the system generates an embedding of the vision-based text segments. In particular, since the embedding of a given text segment is derived from the embedding of an image associated with the text segment, the embedding effectively represents the semantics of the text segment.
As described above, in some cases, the system also maintains a different set of embeddings for text segments in the vocabulary, i.e., embeddings generated using different techniques. In these cases, the system can receive a request for combined (or "final") digital embedding of text segments in the vocabulary for performing a particular machine learning task, and can use a gated neural network to combine two different types of embedding of text segments.
More specifically, because the gated neural network has been trained in a task-specific manner, i.e., training data specific to a particular machine learning task, the system is able to combine embeddings in a manner optimized for the particular machine learning task, thus resulting in improved performance for the particular machine learning task relative to using only one embedment for the task or using a fixed, non-task-related combination of both embeddings. The embedding that is maintained by the system but generated using other techniques will be referred to as a "second embedding" in this specification.
FIG. 3 is a flow diagram of an example process 300 for generating a final embedding of a text segment. For convenience, process 300 will be described as being performed by a system of one or more computers located at one or more locations. For example, an embedded system, such as text embedded system 100 of FIG. 1, suitably programmed in accordance with the present description is capable of performing process 300.
The system receives a request for final embedding of a first text segment in a text segment vocabulary (step 302).
The system processes the vision-based embedding and the second embedding of the first text segment using a gated neural network to generate weight values (step 304).
Gated neural networks have been trained to generate weight values, which result in embeddings with high performance on specific machine learning tasks. In particular, the gate network has been trained in conjunction with a task-specific neural network that is training data for a specific task to adjust the values of the parameters to cause the gate network to generate weight values that are more useful for the specific task.
As a specific example, the gated neural network can be a neural network with one or more hidden layers, i.e., one or more fully connected layers, followed by an S-shaped output layer that generates weight values.
The system generates a final embedding of the first text segment using the vision-based embedding, the second embedding, and the weight value (step 306).
In particular, the system processes a vision-based embedding of a first text segment using a first encoder neural network to generate a first encoded digital embedding of the vision-based embedding, processes a second digital embedding of a second text segment using a second encoder neural network to generate a second encoded digital embedding of the second digital embedding, and combines the first and second encoded digital embeddings according to weight values to generate a final digital embedding of the first text segment.
As with the gated neural network, the first and second encoder neural networks can also be trained to generate high performance embeddings for specific tasks, i.e., can also be trained in conjunction with the task-specific neural network.
As a specific example, each encoder neural network can be a neural network with one or more hidden layers, i.e., one or more fully connected layers, followed by an arctangent output layer that generates the encoded digital embedding. The first encoder neural network is used only for vision-based embedding, while the second encoder neural network is used only for second digital embedding.
This approach generates the final embedding independent of the context (i.e., context) in which the word appears, i.e., the context in which the first text segment appears is not considered for the purposes of the request. However, in some cases it may be beneficial to use context gates that are able to learn the sentences in which the words appear to decide how to weight the vision-based and second embeddings.
For the purpose of real-world scene gating, the gating neural network is modified to consider the context of the first text segment. In particular, the gated neural network includes one or more recurrent neural network layers, such as a bi-directional LSTM layer, followed by one or more feedforward neural network layers.
To utilize the context, one or more context text segments are specified for the first text segment, and the system processes the first text segment and the visual-based embedding of the context text segment using a recurrent neural network layer to generate a contextualized (contextualized) visual-based embedding, and processes the first text segment and the second digital embedding of the context text segment using the recurrent neural network layer to generate a contextualized second digital embedding. The system then processes the contextualized vision-based embedding and the contextualized second digital embedding using the feedforward neural network layer to generate the weight values described above.
The system provides a final embedding of the first text segment in response to the request (step 308). For example, the system can provide the final embedding of the text segment as input to a task-specific neural network that operates on the embedding to perform a particular machine learning task.
In some cases, the system can use vision-based embedding to improve the accuracy of a neural network that maps network input to a target sequence including text segments of a target natural language at each of one or more output steps. In particular, the system can use vision-based embedding instead of the traditional output layer of the neural network to select text segments at each output step.
For example, the network input may be a sequence of text segments from a natural language different from the target language and the target sequence may be a translation of the input sequence into the target natural language.
As another example, the network input may be an image and the target sequence may be an illustration of the image in the target natural language.
As another example, the network input may be a sequence of text segments of a target natural language and the target sequence may be a digest of the input sequence or an answer to a question posed by the input sequence.
FIG. 4 is a flow chart of an example process 400 for selecting text segments for output time steps. For convenience, process 400 will be described as being performed by a system of one or more computers located in one or more locations. For example, an embedded system, such as text embedded system 100 of fig. 1, suitably programmed in accordance with the present description may perform process 400.
The system uses the task-specific neural network to generate an initial representation of the text segment at the output time step from at least the network input (step 402). In particular, the initial representation may be an input that would be provided to a conventional output layer of the task-specific neural network, such as a softmax output layer. For example, the task-specific neural network may be a sequence-to-sequence neural network that includes an encoder neural network and a decoder neural network. The initial representation may be a hidden state of the decoder neural network at the output time step, e.g., a hidden state that would be used by the softmax output layer of the neural network to generate a probability distribution of text segments throughout the vocabulary. In this example, the system generates the initial representation by using a decoder neural network to process the embedding of the most recently generated text segment, i.e., the text segment at an immediately preceding time step. In some cases, the decoder neural network includes an attention mechanism that focuses on the encoded representation of the network input generated by the encoder.
The system determines, for each text segment in the vocabulary of text segments of the target natural language, a similarity measure between the initial representation of the text segment and the digital embedding of the text segment at the output time step (step 404). For example, the similarity measure may be a cosine similarity between the initial representation and the digital embedding. As another example, the similarity measure may be a cosine similarity between (i) the initial representation and (ii) the sum of the digital embedding and the learning weight vector for the text segment. As yet another example, the similarity measure may be a learned bias for the text segment plus a cosine similarity between (i) the initial representation and (ii) the sum of the number embedding and the learned weight vector for the text segment.
The system selects the text segment with the most similar embedding to the initial representation according to the similarity measure as the text segment at the output time step (step 406). By selecting output text segments based on similarity to vision-based embedding, the system can more accurately generate output sequences and, thus, improve performance of machine learning tasks.
The term "configured" is used in this specification in connection with systems and computer program components. For a system of one or more computers, to be configured to perform a particular operation or action means that the system has installed thereon software, firmware, hardware, or a combination thereof that, in operation, causes the system to perform the operation or action. To be configured to perform a particular operation or action with respect to one or more computer programs means that the one or more programs include instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operation or action.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangible embodied in computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions may be encoded on a manually generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus may include, in addition to hardware, code that creates an execution environment for a computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may also be referred to or described as a program, software application, app, module, software module, script, or code, may be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of data: the data need not be structured in any particular way, or at all, and it may be stored on a storage device in one or more locations. Thus, for example, an index database may include multiple collections of data, each of which may be organized and accessed differently.
Similarly, the term "engine" is used broadly throughout this specification to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or through, special purpose logic circuitry, e.g., an FPGA or ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
A computer suitable for executing a computer program may be based on a general purpose microprocessor or a special purpose microprocessor or both, or any other kind of central processing unit. Typically, the central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or carrying out the instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Furthermore, the computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, etc.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from devices used by the user; for example, by sending a web page to a web browser on a user's device in response to a request received from the web browser. In addition, the computer may interact with the user by sending text messages or other forms of messages to a personal device, such as a smart phone, that is running a messaging application, and receiving response messages as feedback from the user.
The data processing apparatus for implementing the machine learning model may also include, for example, a dedicated hardware accelerator unit for handling public and computationally intensive portions of the machine learning training or production (i.e., reasoning) workload.
The machine learning model can be implemented and deployed using a machine learning framework such as a TensorFlow framework, microsoft cognitive toolkit framework, apache Single framework, or Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include Local Area Networks (LANs) and Wide Area Networks (WANs), such as the internet.
The computing system may include clients and servers. The client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data, such as HTML pages, to the user device, e.g., for the purpose of displaying data to and receiving user input from a user interacting with the device as a client. Data generated at the user device, e.g., results of a user interaction, may be received at the server from the device.
While this specification contains many specifics, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings and described in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (21)
1. A method of generating a data set that associates each text segment in a vocabulary of text segments with a respective digital embedding, the method comprising, for each text segment in the vocabulary of text segments:
providing a search query including the text segment to an image search engine;
obtaining a plurality of image search results that have been categorized by the image search engine as being responsive to the search query, wherein each image search result identifies a respective image;
for each image search result, processing the image identified by the image search result using a convolutional neural network, wherein the convolutional neural network has been trained to process the image to generate an image digital embedding for the image; and
a digital embedding for the text segment is generated from a corresponding image digital embedding for each image identified by the plurality of image search results.
2. The method of claim 1, wherein the plurality of image search results are ranked from most responsive to least responsive by the image search engine, and wherein the method further comprises:
concatenating respective image number embeddings for respective images identified by the plurality of image search results according to the ordering of the plurality of image search results.
3. The method of claim 1, wherein the convolutional neural network has been trained on semantic ranking targets to generate image numerical embeddings that preserve semantic similarity between input images.
4. The method of claim 1, wherein obtaining the plurality of image search results comprises:
obtaining, from the image search engine, an initial set of image search results ranked by the image search engine from most responsive to least responsive; and
a subset of the initial set of image search results that are most responsive to the search query is selected according to the ranking.
5. The method of any of claims 1-4, wherein the text segment in the vocabulary comprises a word.
6. The method of any of claims 1-4, wherein the text segment in the vocabulary comprises one or more multi-word phrases.
7. A method for generating digital embedding of text segments, comprising:
maintaining data specifying a respective first digital embedding and a respective different second digital embedding for each text segment in a vocabulary of text segments;
receiving a request for final digital embedding of a first text segment in the vocabulary;
processing the first digital embedding and the second digital embedding of the first text segment using a gated neural network, wherein the gated neural network is configured to process the first digital embedding and the second digital embedding to generate weight values;
processing the first digital embedding of the first text segment using a first encoder neural network to generate a first encoded digital embedding of the first digital embedding;
processing the second digital embedding of the first text segment using a second encoder neural network to generate a second encoded digital embedding of the second digital embedding; and
the first and second encoded digital embeddings are combined in accordance with the weight values to generate the final digital embeddings of the first text segment.
8. The method of claim 7, wherein the first digital embedding for each text segment in the vocabulary has been generated using the method of any one of claims 1 to 6.
9. The method of claim 7, wherein combining the first and second encoded digital embeddings according to the weight values to generate the final digital embeddings of the first text segment comprises:
for each dimension of the first and second code number embeddings, a weighted sum of the value of the first code number embeddings over the dimension and the value of the second code number embeddings over the dimension is determined from the weight values.
10. The method of any of claims 7 to 9, wherein the gated neural network is a deep feed forward neural network.
11. The method according to any one of claim 7 to 9,
wherein the request specifies one or more context text segments for the first text segment,
wherein the gated neural network comprises one or more recurrent neural network layers followed by one or more feedforward neural network layers, and
wherein processing the first digital embedding and the second digital embedding of the first text segment using the gated neural network comprises:
processing the first digital embedding of the first text segment and the one or more contextual text segments using the one or more recurrent neural network layers to generate a contextually first digital embedding;
processing the second digital embedding of the first text segment and the one or more contextual text segments using the one or more recurrent neural network layers to generate a contextually second digital embedding; and
the one or more feedforward neural network layers are used to process the contextualized first digital embedding and the contextualized second digital embedding to generate the weight values.
12. The method of claim 11, wherein the one or more recurrent neural network layers are bi-directional LSTM layers.
13. The method of any of claims 7 to 9, further comprising:
the final digital embedding is provided as input to a task specific neural network.
14. A method of mapping a network input to a target sequence, the target sequence comprising a target text segment of a target natural language at each of one or more output time steps, the method comprising, for each output time step:
generating an initial representation of the target text segment at the output time step from at least the network input using a task-specific neural network;
determining, for each candidate text segment in a vocabulary of candidate text segments for the target natural language, a similarity measure between the initial representation of the target text segment and a digital embedding of the candidate text segment at the output time step; and
selecting as the target text segment at the output time step a candidate text segment most similar to the initial representation according to the similarity measure, wherein the numerical embedding of candidate text segments in the vocabulary of candidate text segments has been generated by, for each candidate text segment in the vocabulary of candidate text segments:
providing a search query including the candidate text segment to an image search engine;
obtaining image search results that have been categorized by the image search engine as responsive to the search query, wherein each image search result identifies a respective image;
processing, for each image search result, the image identified by the image search result using a convolutional neural network, wherein the convolutional neural network has been trained to process the image to generate an image digital embedding of the image; and
a digital embedding of the candidate text segment is generated from the image digital embedding of the image identified by the image search result.
15. The method according to claim 14,
wherein the network input is an input sequence comprising an input at each of a plurality of input time steps,
wherein the task-specific neural network is a sequence-to-sequence neural network comprising an encoder neural network and a decoder neural network, and
wherein the initial representation of the target text segment at the output time step is a hidden state of the decoder neural network at the output time step.
16. The method of claim 15, wherein the sequence-to-sequence neural network includes an attention mechanism.
17. The method of any of claims 15 to 16, wherein the network input is a sequence of text segments from a natural language.
18. The method of claim 17, wherein the source natural language is different from the target natural language, and wherein the target sequence is a translation of the input sequence to the target natural language.
19. The method of claim 17, wherein the task-specific neural network processes the sequence of text segments of the natural language using the method of any one of claims 7 to 13 to generate a respective final digital embedding for each text segment.
20. A system for processing text segments, the system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the operations of the respective method of any one of claims 1-19.
21. A non-transitory computer-readable storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the respective method of any one of claims 1-19.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862634164P | 2018-02-22 | 2018-02-22 | |
US62/634,164 | 2018-02-22 | ||
US201862666650P | 2018-05-03 | 2018-05-03 | |
US62/666,650 | 2018-05-03 |
Publications (2)
Publication Number | Publication Date |
---|---|
CN109885842A CN109885842A (en) | 2019-06-14 |
CN109885842B true CN109885842B (en) | 2023-06-20 |
Family
ID=66928853
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201910134308.8A Active CN109885842B (en) | 2018-02-22 | 2019-02-22 | Processing text neural networks |
Country Status (2)
Country | Link |
---|---|
US (1) | US11003856B2 (en) |
CN (1) | CN109885842B (en) |
Families Citing this family (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109564575B (en) * | 2016-07-14 | 2023-09-05 | 谷歌有限责任公司 | Classifying images using machine learning models |
CN110023963B (en) * | 2016-10-26 | 2023-05-30 | 渊慧科技有限公司 | Processing text sequences using neural networks |
US11003856B2 (en) * | 2018-02-22 | 2021-05-11 | Google Llc | Processing text using neural networks |
US20200272695A1 (en) * | 2019-02-25 | 2020-08-27 | Disney Enterprises, Inc. | Techniques for performing contextual phrase grounding |
CN110443863B (en) * | 2019-07-23 | 2023-04-07 | 中国科学院深圳先进技术研究院 | Method for generating image by text, electronic equipment and storage medium |
CN110598671B (en) * | 2019-09-23 | 2022-09-27 | 腾讯科技（深圳）有限公司 | Text-based avatar behavior control method, apparatus, and medium |
US20220261856A1 (en) * | 2019-10-16 | 2022-08-18 | Limited Liability Company "Sarafan Tekhnologii" | Method for generating search results in an advertising widget |
US11275934B2 (en) * | 2019-11-20 | 2022-03-15 | Sap Se | Positional embeddings for document processing |
CN111063000B (en) * | 2019-12-15 | 2023-12-26 | 中国科学院深圳先进技术研究院 | Magnetic resonance rapid imaging method and device based on neural network structure search |
CN111191002B (en) * | 2019-12-26 | 2023-05-23 | 武汉大学 | Neural code searching method and device based on hierarchical embedding |
CN113270091B (en) * | 2020-02-14 | 2024-04-16 | 声音猎手公司 | Audio processing system and method |
CN111507406A (en) * | 2020-04-17 | 2020-08-07 | 上海眼控科技股份有限公司 | Method and equipment for optimizing neural network text recognition model |
WO2021234577A1 (en) * | 2020-05-21 | 2021-11-25 | Element Ai Inc. | Method of and system for training machine learning algorithm for object classification |
US11704558B2 (en) | 2020-05-21 | 2023-07-18 | Servicenow Canada Inc. | Method of and system for training machine learning algorithm for object classification |
CN111737954B (en) * | 2020-06-12 | 2023-07-28 | 百度在线网络技术（北京）有限公司 | Text similarity determination method, device, equipment and medium |
US20210406262A1 (en) * | 2020-06-30 | 2021-12-30 | Lyft, Inc. | Systems and methods for encoding and searching scenario information |
CN111985491A (en) * | 2020-09-03 | 2020-11-24 | 深圳壹账通智能科技有限公司 | Similar information merging method, device, equipment and medium based on deep learning |
TWI778442B (en) * | 2020-11-03 | 2022-09-21 | 財團法人資訊工業策進會 | Device and method for detecting purpose of article |
US11817081B2 (en) * | 2021-03-31 | 2023-11-14 | Nippon Telegraph And Telephone Corporation | Learning device, learning method, learning program, retrieval device, retrieval method, and retrieval program |
CN113342343B (en) * | 2021-04-20 | 2022-05-27 | 山东师范大学 | Code abstract generation method and system based on multi-hop inference mechanism |
CN112988976A (en) * | 2021-04-21 | 2021-06-18 | 百度在线网络技术（北京）有限公司 | Search method, search apparatus, electronic device, storage medium, and program product |
US20230169110A1 (en) * | 2021-11-30 | 2023-06-01 | Microsoft Technology Licensing, Llc | Method and system of content retrieval for visual data |
US20230186331A1 (en) * | 2021-12-13 | 2023-06-15 | International Business Machines Corporation | Generalized demand estimation for automated forecasting systems |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101571875A (en) * | 2009-05-05 | 2009-11-04 | 程治永 | Realization method of image searching system based on image recognition |
CN105512220A (en) * | 2015-11-30 | 2016-04-20 | 小米科技有限责任公司 | Image page output method and device |
CN105940395A (en) * | 2014-01-31 | 2016-09-14 | 谷歌公司 | Generating vector representations of documents |
CN106255968A (en) * | 2014-05-16 | 2016-12-21 | 微软技术许可有限责任公司 | Natural language picture search |
CN107194863A (en) * | 2017-04-07 | 2017-09-22 | 广东精点数据科技股份有限公司 | Image digital watermark embedded system and method |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB2457267B (en) * | 2008-02-07 | 2010-04-07 | Yves Dassas | A method and system of indexing numerical data |
US20140108103A1 (en) * | 2012-10-17 | 2014-04-17 | Gengo, Inc. | Systems and methods to control work progress for content transformation based on natural language processing and/or machine learning |
US10645548B2 (en) * | 2016-06-19 | 2020-05-05 | Data.World, Inc. | Computerized tool implementation of layered data files to discover, form, or analyze dataset interrelations of networked collaborative datasets |
US10747774B2 (en) * | 2016-06-19 | 2020-08-18 | Data.World, Inc. | Interactive interfaces to present data arrangement overviews and summarized dataset attributes for collaborative datasets |
GB201620232D0 (en) * | 2016-11-29 | 2017-01-11 | Microsoft Technology Licensing Llc | Data input system with online learning |
US11003856B2 (en) * | 2018-02-22 | 2021-05-11 | Google Llc | Processing text using neural networks |
US20200250538A1 (en) * | 2019-02-01 | 2020-08-06 | Google Llc | Training image and text embedding models |
-
2019
- 2019-02-22 US US16/283,632 patent/US11003856B2/en active Active
- 2019-02-22 CN CN201910134308.8A patent/CN109885842B/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101571875A (en) * | 2009-05-05 | 2009-11-04 | 程治永 | Realization method of image searching system based on image recognition |
CN105940395A (en) * | 2014-01-31 | 2016-09-14 | 谷歌公司 | Generating vector representations of documents |
CN106255968A (en) * | 2014-05-16 | 2016-12-21 | 微软技术许可有限责任公司 | Natural language picture search |
CN105512220A (en) * | 2015-11-30 | 2016-04-20 | 小米科技有限责任公司 | Image page output method and device |
CN107194863A (en) * | 2017-04-07 | 2017-09-22 | 广东精点数据科技股份有限公司 | Image digital watermark embedded system and method |
Non-Patent Citations (2)
Title |
---|
Distributed Representations ofWords and Phrases and their Compositionality;Tomas Mikolov et al;《arXiv:1310.4546v1》;20131016;第1-9页 * |
基于文本的数字水印嵌入技术的研究;陈翔和蒋外文;《基于文本的数字水印嵌入技术的研究》;20071231;第76-77页 * |
Also Published As
Publication number | Publication date |
---|---|
CN109885842A (en) | 2019-06-14 |
US20190258713A1 (en) | 2019-08-22 |
US11003856B2 (en) | 2021-05-11 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109885842B (en) | Processing text neural networks | |
US11669744B2 (en) | Regularized neural network architecture search | |
JP6892389B2 (en) | Selection of representative video frames for video | |
US11113479B2 (en) | Utilizing a gated self-attention memory network model for predicting a candidate answer match to a query | |
US20220198145A1 (en) | Generating author vectors | |
US20190164084A1 (en) | Method of and system for generating prediction quality parameter for a prediction model executed in a machine learning algorithm | |
US20220027398A1 (en) | Cluster based search and recommendation method to rapidly on-board commands in personal assistants | |
CN110402445B (en) | Method and system for browsing sequence data using recurrent neural network | |
CN110678882B (en) | Method and system for selecting answer spans from electronic documents using machine learning | |
CN111652378B (en) | Learning to select vocabulary for category features | |
US9852177B1 (en) | System and method for generating automated response to an input query received from a user in a human-machine interaction environment | |
US11915129B2 (en) | Method and system for table retrieval using multimodal deep co-learning with helper query-dependent and query-independent relevance labels | |
US8122002B2 (en) | Information processing device, information processing method, and program | |
CN110737756B (en) | Method, apparatus, device and medium for determining answer to user input data | |
US20220383119A1 (en) | Granular neural network architecture search over low-level primitives | |
US20230029590A1 (en) | Evaluating output sequences using an auto-regressive language model neural network | |
CN116662538A (en) | Text abstract generation method, device, equipment and medium based on multitask learning | |
US11822893B2 (en) | Machine learning models for detecting topic divergent digital videos | |
KR20220107737A (en) | Method, system, and computer readable record medium for generating reformulated query | |
US20230351190A1 (en) | Deterministic training of machine learning models | |
WO2023175089A1 (en) | Generating output sequences with inline evidence using language model neural networks | |
KR20230162721A (en) | Integrating decision trees into neural networks | |
KR20240048799A (en) | Document recommendation method, computer device, and computer program to enhance embedding of title words using body words | |
CN111563159A (en) | Text sorting method and device | |
CN116680381A (en) | Document retrieval method, device, electronic equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |