US8977627B1 - Filter based object detection using hash functions - Google Patents
Filter based object detection using hash functions Download PDFInfo
- Publication number
- US8977627B1 US8977627B1 US13/286,963 US201113286963A US8977627B1 US 8977627 B1 US8977627 B1 US 8977627B1 US 201113286963 A US201113286963 A US 201113286963A US 8977627 B1 US8977627 B1 US 8977627B1
- Authority
- US
- United States
- Prior art keywords
- hash
- image
- windows
- hash values
- rank
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 230000006870 function Effects 0.000 title claims abstract description 62
- 238000001514 detection method Methods 0.000 title abstract description 22
- 239000013598 vector Substances 0.000 claims description 61
- 238000000034 method Methods 0.000 claims description 49
- 230000004807 localization Effects 0.000 claims description 8
- 238000011524 similarity measure Methods 0.000 claims description 8
- 238000007667 floating Methods 0.000 claims description 6
- 230000000875 corresponding effect Effects 0.000 claims 3
- 230000002596 correlated effect Effects 0.000 claims 2
- 238000003860 storage Methods 0.000 description 17
- 230000008569 process Effects 0.000 description 8
- 238000012545 processing Methods 0.000 description 7
- 238000010586 diagram Methods 0.000 description 6
- 238000004891 communication Methods 0.000 description 5
- 230000008901 benefit Effects 0.000 description 4
- 239000006227 byproduct Substances 0.000 description 3
- 238000009826 distribution Methods 0.000 description 3
- 238000013461 design Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000005070 sampling Methods 0.000 description 2
- 238000012549 training Methods 0.000 description 2
- 238000011426 transformation method Methods 0.000 description 2
- 241000132023 Bellis perennis Species 0.000 description 1
- 235000005633 Chrysanthemum balsamita Nutrition 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001186 cumulative effect Effects 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 239000000284 extract Substances 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000009022 nonlinear effect Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 230000035945 sensitivity Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
- 238000009827 uniform distribution Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/213—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/213—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods
- G06F18/2137—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods based on criteria of topology preservation, e.g. multidimensional scaling or self-organising maps
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/7715—Feature extraction, e.g. by transforming the feature space, e.g. multi-dimensional scaling [MDS]; Mappings, e.g. subspace methods
Definitions
- This disclosure generally relates to systems and methods that facilitate object detection and recognition in digital images.
- a technique that has been employed for object detection in images consist of analyzing pieces of an image by running object filters across an image in a sliding window fashion, computing the byproduct of the object filters with the underlying image at every location in the image, and using the largest value, or value set, across a particular threshold for object detection.
- images to be analyzed for object detection can be of poor quality, and are rarely captured at a uniform size, scale, or viewpoint.
- computer recognition systems often have to learn object filters for different viewpoints, and convolve the object filters on an image pyramid during object detection.
- object filters can be used for object detection or localization. It can be readily appreciated that computing the byproduct of up to several thousand object filters for multiple viewpoints at every location on an underlying image can require significant time and computational resources. The amount of resources required in such computer recognition systems can be limiting. For example, such systems may have difficulty in scaling for multiple object categories.
- An image analyzer includes a hashing component that computes respective hash values for a set of object windows that are associated with an image to be scanned.
- the hashing component can employ various hash functions in connection with computing the hash values, such as for example a winner takes all (WTA) hash function.
- WTA winner takes all
- a filter selection component compares the respective hash values of the object windows against a hash table of object filters, and selects one or more object filters for recognizing or localizing at least one of an object within the image as a function of the comparison.
- an image analyzer includes a hashing component that computes respective hash values for a set of object windows associated with an image to be scanned.
- the hashing component can include a winner takes all (WTA) hash component that transforms an input feature space into binary codes such that Hamming distance in a resulting space correlates with rank similarity measures, and a ranking component that biases rank embeddings to be sensitive to elements at a head of a rank list.
- WTA winner takes all
- a filter selection component compares the respective hash values of the object windows against a hash table of object filters, and selects one or more object filters for recognizing or localizing at least one of an object within the image as a function of the comparison.
- FIG. 1 illustrates an example system for filter based object detection using hash functions in accordance with various aspects described herein;
- FIG. 2 illustrates an example system for filter based object detection using hash functions in accordance with various aspects described herein;
- FIG. 3 illustrates an example winner takes all (WTA) hashing in accordance with various aspects described herein;
- FIG. 4A is an example plot illustrating an example rate of sampling of a predetermined index from a sorted list in accordance with various aspects described herein;
- FIG. 4B is an example plot illustrating an example contribution to similarity for different biasing factors in accordance with various aspects described herein;
- FIG. 5 illustrates an example hashing component in accordance with various aspects described herein;
- FIG. 6 illustrates an example system for filter based object detection using hash functions in accordance with various aspects described herein;
- FIGS. 7-9 are example flow diagrams of respective methods filter based object detection using hash function in accordance with various aspects described herein;
- FIG. 10 is a block diagram representing an exemplary non-limiting networked environment in which the various embodiments can be implemented.
- FIG. 11 is a block diagram representing an exemplary non-limiting computing system or operating environment in which the various embodiments may be implemented.
- One non-limiting implementation of the innovation provides efficient and accurate object or feature recognition or localization in images.
- the implementation reduces computational cost by obtaining image windows for particular features that would be convolved with object filters from images, computing hash values for the object windows, and using the hash values to directly lookup object filters stored in a hash table.
- a hashing component computes respective hash values for a set of object windows that are associated with an image to be scanned, and a filter selection component compares the respective hash values of the object windows against a hash table of object filters, and as a function of the comparison selects at least one of the object filters for recognizing or localizing at least one of an object or feature within the image.
- system 100 can include a memory that stores computer executable components and a processor that executes computer executable components stored in the memory, examples of which can be found with reference to FIG. 11 .
- System 100 includes an image analyzer 102 .
- the image analyzer 102 recognizes or localizes features or objects included in an image 104 (e.g., digital image, picture, etc.).
- the image analyzer 102 includes a hashing component 106 , a filter selection component 108 , and a hash table 110 .
- the hashing component 106 obtains, acquires, or otherwise receives a set of object windows (W) 112 associated with the image 104 .
- the image analyzer 102 can determine the set of object windows 112 as a function of the image 104 .
- the hashing component 106 determines, calculates or otherwise computes a set of respective hash values for the set of object windows 112 . For instance, if the set of object windows includes a first object window, and a second object window, then the hashing component 106 computes a first hash value for the first object window, and a second hash value for the second object window.
- the hashing component 106 can employ various hash functions to calculate the hash values, including, but not limited to, winner takes all (WTA) hashing (discussed in greater detail in connection with FIGS. 2-4 ), a locality sensitive hashing function, hashing based on random projections, hashing with concomitant statistics, or Similarity Preserving algorithm for Entropy-based Coding (SPEC) hashing (discussed in greater detail in connection with FIG. 5 ).
- WTA winner takes all
- SPEC Similarity Preserving algorithm for Entropy-based Coding
- the filter selection component 108 evaluates, assesses, or otherwise compares the set of respective hash values for the set of object windows against a hash table 110 for object filters.
- the hash table 110 can map virtually any quantity of object filters.
- the filter selection component 108 picks, chooses, or otherwise selects at least one of the object filters as a function of the comparison. As an example, if the filter selection component 108 determines that the first hash value in the previous example corresponds to an N th object filter in the hash table 110 (N is an integer), then the filter selection component 108 selects the N th object filter for recognizing or localizing the object or feature associated with the first object window in the image 104 .
- the hash table 110 is illustrated as being included in the image analyzer 102 , such implementation is not so limited.
- the hash table 110 can be maintained in a disparate location, wherein the image analyzer 102 accesses the hash table 110 via a network connection.
- FIG. 2 illustrates an example system 200 for filter based object detection using hash functions in accordance with various aspects described in this disclosure.
- the system 200 includes an image analyzer 102 that recognizes or localizes features or objects included in an image 104 .
- the image analyzer 102 includes the hashing component 106 , the filter selection component 108 , and the hash table 110 .
- the hashing component 106 receives a set of object windows (W) 112 associated with the image 104 , and computes a set of respective hash values for the set of object windows 112 .
- the filter selection component 108 compares the set of respective hash values for the set of object windows against the set of hash values corresponding to a set of object filters maintained in the hash table 110 .
- the image analyzer 102 in FIG. 2 also includes a hash table component 202 that saves, maintains, or otherwise stores the object windows in the hash table 110 .
- the hashing component 106 in FIG. 2 includes a winner takes all (WTA) hash component 204 that facilitates computation of the set of respective hash values for the set of object windows.
- the WTA hash component 204 transforms an input feature space (e.g., input feature vectors) into binary codes (e.g., hash values) such that a Hamming distance, or Hamming similarity, in a resulting space (e.g., binary codes or hash values) correlates with rank similarity measures.
- the WTA hash component 204 (or hashing component 106 ) generates a hash vector by combining a set of hash values corresponding respectively to different input feature vectors.
- the input feature space can include a set of input feature vectors determined as a function of an object window in the set of object windows 112 .
- the WTA hash component 204 can permutate the input vectors, analyze a first subset of the elements in the respective permutated input vectors, and generate a hash vector by coding an index of a maximum element in the respective first subset of elements for the permutated input vectors (e.g., a K-sized subset of feature dimensions, where K is an integer).
- the hashing component 106 in FIG. 2 includes a ranking component 206 that induces ranking metrics on descriptors for similarity search.
- the ranking component 206 employs a set of hashed descriptors computed as a function of known descriptors.
- the ranking component 206 can employ a set of WTA descriptors with respective WTA hash codes computed respectively from a set of raw image pixels descriptors, or a set of Scale-invariant feature transform (SIFT) and Daisy descriptors, each with the hamming distance, in order to determine and/or rank the similarity of a set of disparate images to the image 104 , or a segment of the image 104 .
- SIFT Scale-invariant feature transform
- the ranking component 206 biases rank embeddings for sensitivity to elements (e.g., indices) at the head of a rank list.
- the elements of the hash vectors correlate to rank embeddings (e.g., ordinal embeddings) of the input feature space.
- the ranking component 206 can bias the rank embeddings by altering the size of the subset of feature dimensions (e.g., K). The greater the size of the subset of feature dimensions, the greater the bias is toward the elements at the head of the rank list (discussed in greater detail with reference to FIG. 4 ). It is to be appreciated that in the context of ranking, agreement among high ranking coefficients in a list can be more important than coefficients further down the list.
- system 200 can include a memory that stores computer executable components and a processor that executes computer executable components stored in the memory, examples of which can be found with reference to FIG. 11 .
- the WTA hashing as employed by the WTA hash component 204 is based in part on comparisons of elements, and can be efficiently implemented without computation of floating point units.
- the processor and/or the memory can be configured as special case hardware that does not compute floating point units, or fixed-point hardware (e.g., fixed-point processor, etc.).
- WTA hash component 204 transforms an input feature space into binary codes such that a Hamming distance in a resulting space correlates with rank similarity measures. Rank similarity measures provide stability to perturbations in numeric values, and provide good indications of inherent similarity, or agreement, between items or vectors being considered.
- a non-limiting example WTA hashing algorithm, as employed by the WTA hash component 204 is detailed by Algorithm 1, shown below:
- Algorithm 1 provides a feature space transformation having a resulting space that is not sensitive to the absolute values of the feature dimensions, but rather on the implicit ordering defined by those values. In effect, the similarity between two points is defined by the degree to which their feature dimension rankings agree.
- a pair-wise order measure can be defined according to Equation 1 below:
- x i and y i are the i th feature dimensions in X, Y R n and T is a threshold function
- Equation 1 measures the number of pairs of feature dimensions in X and Y that agree in ordering.
- Equation 2 groups pair-wise agreement terms by one of the indices in the pair.
- R i (X, Y) measures the ranking agreement for index i with indices that rank below i.
- Indices of elements in X that are ranked below index I are denoted with L (X, i), in Equation 4.
- the rank agreement at index i is the cardinality of the intersection of the corresponding L sets from X and Y.
- the term R 0 (X, Y) will measure the size of the intersection for the set of indices smaller than index 0.
- Algorithm 1 outlines a transformation method where the permutations in the algorithm are generated randomly and stored for use by all data points.
- the transformation method depends on coding multiple partial orderings of a data point as a way to lower bound ranking agreement in case of a match.
- K dimensions are selected from the sample, and the dimension with the highest value in the subset of size K is coded. For example, K can be selected at random and consistent across all samples which gives rise to the notion of permutations.
- the first input vector 306 a and the second input vector 306 b are unrelated and result in different output codes (e.g., 1 and 2, respectively).
- the input third vector 306 c is a scaled and offset version of the first input vector 306 a (in this case, times 2, then plus 2), and results in the same output code as 306 a (e.g., 1).
- the fourth input vector 306 d is a version of the first input vector 306 a , wherein each element has been perturbed by 1 (in this case, +1, ⁇ 1, +1, +1, +1, ⁇ 1).
- the elements of the fourth input vector 306 d are ranked differently from the first input vector, but the index of the maximum element of the first K (e.g., 4) elements in the fourth input vector 306 d (e.g., 1) is the same as the index of the maximum element of the first K (e.g., 4) elements in the first input vector 306 a (e.g., 1).
- the vectors 306 a and 306 c satisfy three inequalities, namely X′(1)>X′(0), X′(1)>X′(2), and X′(1)>X′(3).
- these three terms e.g., inequalities
- K the choice of K leads to different emphasis on pair-wise agreements for indices at the head of the list.
- K the number of permutations that relate the max element to all the others would be captured.
- FIG. 4A is a plot illustrating an example rate of sampling of a predetermined index from a sorted list
- FIG. 4B is a graph illustrating a contribution to similarity for a predetermined value of R i (X, Y) for different values of K.
- a window size K influences the distribution of which indices from an original vector are included in the code.
- K enables tuning to give higher weight to top elements in the vectors compared to other elements in the vectors.
- the final similarity function induced in the limit can be expressed as:
- i denotes indices into vectors X and Y
- ⁇ (i) denotes indices into permuted vectors X′ and Y′
- index i is chosen as the hash code for a given random permutation it implies, X(i)>X(j) ⁇ j0 ⁇ ( j ) ⁇ K, J ⁇ i, which means that all other indices in the permutation are members of the set L(X, i).
- i is chosen as the hash code for both X and Y (e.g., for a fixed random permutation ⁇ )
- the indices 0 ⁇ ( j ) ⁇ K belong to both L(X, i) and L(Y, i).
- the total number of ways codes for a permutation ⁇ on vectors X and Y can collide on any given code is given by:
- K leading indices of a permutation ⁇ can be drawn from n dimensions in ( K n ) ways. Therefore, the probability of collision of codes given by a random permutation ⁇ on vectors X and Y can be expressed as:
- LSH locality sensitive hashing
- the hashing component 106 computes a set of respective hash values for the set of object windows 112 .
- the hashing component 106 can include a classifier component 502 that learns hash values optimized for recognition or localization of a specific object or a specific feature.
- the classifier component 502 can be a trained classifier that is trained via a set of training data 506 stored in one or more data stores, e.g., a data storage 504 .
- the classifier component can employ a Similarity Preserving algorithm for Entropy-based Coding (SPEC) hashing to learn a hash function with binary code such that objects with high similarity have small Hamming distance.
- SPEC Entropy-based Coding
- the classifier component 502 can be incrementally trained (e.g., one bit at a time) via the set of training data 506 , and as bits are added to the hash code the Hamming distance between dissimilar objects increases, enabling fast and efficient nearest neighbor retrieval.
- SPEC Entropy-based Coding
- FIG. 6 illustrates an example system 600 for filter based object detection using hash functions in accordance with various aspects described in this disclosure.
- the system 600 includes an image analyzer 102 that recognizes or localizes features or objects included in an image 104 .
- the image analyzer 102 includes a hashing component 106 , a filter selection component 108 , and a generator component 602 .
- the hashing component 106 receives a set of object windows (W) 112 associated with the image 104 , and computes a set of respective hash values for the set of object windows 112 .
- W object windows
- the generator component 602 constructs, provides, or otherwise generates a set of hash tables 604 (e.g., hash table 1-hash table N) for a set of object filters.
- the quantity of hash tables in the set of hash tables is determined as a function of the length of the hash values extracted by the hashing component 106 , and hash values corresponding to the object filters are partitioned, split, or otherwise divided among the hash tables. For example, if the hashing component 106 extracts 100 bits, then the generator component 602 can construct 25 hash tables for four bits each.
- the filter selection component 108 in FIG. 6 compares the set of respective hash values against the set of hash tables 604 .
- the filter selection component 108 can compare the set of respective hash values against each of the 25 hash tables (e.g., set of hash tables 604 ).
- the filter selection component 108 in FIG. 6 includes a counter component 606 that tracks, logs, or otherwise records a quantity of matches (e.g., hits, etc.) determined between the hash tables and the set of respective hash values.
- a match can be determined based on a hash value in the set of respective hash values satisfying a predetermined criterion, such as a threshold of bit matches.
- a predetermined criterion such as a threshold of bit matches.
- the set of hash tables 604 and the counter component 606 enable a determination of accuracy or closeness of an input pass to the object filters, rather than strictly matching the hash values corresponding to the object window with the object filters. For example, a quantity of matches recorded by the counter component 606 can indicate a close match with one or more object filters maintained in the set of hash tables 604 . It is to be further appreciated that although the generator component 602 and the set hash tables 604 are illustrated as being included in the image analyzer 102 , such implementation is not so limited. For instance, the set of hash tables 604 can be maintained in a disparate location, wherein the image analyzer 102 accesses the set of hash tables 604 via a network connection.
- FIGS. 7-9 illustrate various methodologies in accordance with the disclosed subject matter. While, for purposes of simplicity of explanation, the methodologies are shown and described as a series of acts, the disclosed subject matter is not limited by the order of acts, as some acts may occur in different orders and/or concurrently with other acts from that shown and described herein. For example, those skilled in the art will understand and appreciate that a methodology can alternatively be represented as a series of interrelated states or events, such as in a state diagram. Moreover, not all illustrated acts may be required to implement a methodology in accordance with the disclosed subject matter. Additionally, it is to be appreciated that the methodologies disclosed in this disclosure are capable of being stored on an article of manufacture to facilitate transporting and transferring such methodologies to computers or other computing devices.
- a set of hash values are calculated for a set of object windows, respectively (e.g., by hashing component 106 ).
- the object windows e.g., object windows 112
- an image e.g., image 104
- hash functions can be used to calculate the hash values, including, but not limited to, winner takes all (WTA) hashing (e.g., using WTA hash component 204 ), locality sensitive hashing function, hashing based on random projections, hashing with concomitant statistics, or Similarity Preserving algorithm for Entropy-based Coding (SPEC) hashing (e.g., using classifier component 502 ).
- WTA winner takes all
- SPEC Similarity Preserving algorithm for Entropy-based Coding
- the hash values are used to map, locate, or otherwise lookup one or more object filters in a hash table (e.g., using filter selection component 108 ).
- one or more object filters are selected for object or feature recognition or localization in the image based on a match with the hash values (e.g., filter selection component 108 ). For example, if the hash values maps to an N th object filter in the hash table, then the N th object filter is selected for object recognition or localization in the image.
- an input vector is permutated (e.g., using WTA hash component 204 ).
- the input vector can correspond to an object window, or section of an object window for an image to be scanned.
- an input vector, X can be permutated to obtain a permutated input vector, X′.
- rank embeddings are biased by altering a size (e.g., K) of the subset of elements to be analyzed for hash value generation (e.g., using ranking component 206 ). As discussed, the greater the size (e.g., K) of the subset of feature dimensions, the greater the bias is toward the elements at the head of the rank list.
- the subset of elements in the permutated input vector (e.g., X′) are analyzed (e.g., using WTA hash component 204 ), wherein the size (e.g., K) of the subset of elements was determined at reference numeral 804 .
- a maximum element in the subset of elements is identified (e.g., using WTA hash component 204 ). For example, if the subset of elements contains (5, 12, 2, 3), then 12 is identified as the maximum element in the subset of elements, and the index of 12 in the subset of elements is 1, therefore, the hash value is 1.
- a hash vector is generated by combining the hash value of the input vector with a set of hash values for different input vectors (e.g., using WTA hash component 204 ).
- the hash values or hash vector can be used to lookup one or more object filters in an object table for object or feature recognition or localization.
- the WTA hashing of methodology 800 is based on comparisons of elements, and therefore can be efficiently implemented without computing floating point units.
- the methodology 800 can be implemented by employing special case hardware that does not compute floating point units, or fixed-point hardware (e.g., fixed point processor, etc.).
- FIG. 9 illustrates an example methodology 900 for filter based object detection using hash functions in accordance with various aspects described in this disclosure.
- a set of hash tables are generated for a set of object filters (e.g., using generator component 602 ).
- a quantity of hash tables in the set of hash tables can be determined as a function of the length of hash values extracted from object windows corresponding to an image (e.g., using the hashing component 106 ), and hash values corresponding to the object filters divided among the set of hash tables. For example, if the hash values extracted contain 100 bits, then 25 hash tables for four bits each can be generated.
- a set of hash values are computed for a set of object windows, respectively, that correspond to an image (e.g., using hashing component 106 ).
- the hash values can be computed by using various hash functions, including, but not limited to, winner takes all (WTA) hashing, locality sensitive hashing function, hashing based on random projections, hashing with concomitant statistics, or Similarity Preserving algorithm for Entropy-based Coding (SPEC) hashing (e.g., using classifier component 502 ).
- WTA winner takes all
- SPEC Similarity Preserving algorithm for Entropy-based Coding
- the set of respective hash values are compared against the set of hash tables (e.g., using filter selection component 108 ). Continuing with the previous example, the set of respective hash values can be compared against each of the 25 hash tables.
- a quantity e.g., number
- a hash value in the set of respective hash values contains four bits that match the bits contained in a hash table for an object filter, then a match is recorded for the hash value.
- an object filter is selected as function of the number of matches recorded.
- the quantity of matches can indicate a close match with one or more object filters maintained in the set of hash tables, and at least one object filter can be selected based on a predetermined criterion, such as satisfying a quantity of matches threshold.
- the various embodiments described herein can be implemented in connection with any computer or other client or server device, which can be deployed as part of a computer network or in a distributed computing environment, and can be connected to any kind of data store where media may be found.
- the various embodiments described herein can be implemented in any computer system or environment having any number of memory or storage units, and any number of applications and processes occurring across any number of storage units. This includes, but is not limited to, an environment with server computers and client computers deployed in a network environment or a distributed computing environment, having remote or local storage.
- Distributed computing provides sharing of computer resources and services by communicative exchange among computing devices and systems. These resources and services include the exchange of information, cache storage and disk storage for objects, such as files. These resources and services also include the sharing of processing power across multiple processing units for load balancing, expansion of resources, specialization of processing, and the like. Distributed computing takes advantage of network connectivity, allowing clients to leverage their collective power to benefit the entire enterprise. In this regard, a variety of devices may have applications, objects or resources that may participate in the smooth streaming mechanisms as described for various embodiments of this disclosure.
- FIG. 10 provides a schematic diagram of an exemplary networked or distributed computing environment.
- the distributed computing environment comprises computing objects 1010 , 1012 , etc. and computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc., which may include programs, methods, data stores, programmable logic, etc., as represented by applications 1030 , 1032 , 1034 , 1036 , 1038 .
- computing objects 1010 , 1012 , etc. and computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc. may comprise different devices, such as personal data assistants (PDAs), audio/video devices, mobile phones, MP3 players, personal computers, tablets, laptops, etc.
- PDAs personal data assistants
- Each computing object 1010 , 1012 , etc. and computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc. can communicate with one or more other computing objects 1010 , 1012 , etc. and computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc. by way of the communications network 1040 , either directly or indirectly.
- network 1040 may comprise other computing objects and computing devices that provide services to the system of FIG. 10 , and/or may represent multiple interconnected networks, which are not shown.
- computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc. can also contain an application, such as applications 1030 , 1032 , 1034 , 1036 , 1038 , that might make use of an API, or other object, software, firmware and/or hardware, suitable for communication with or implementation of the smooth streaming provided in accordance with various embodiments of this disclosure.
- an application such as applications 1030 , 1032 , 1034 , 1036 , 1038 , that might make use of an API, or other object, software, firmware and/or hardware, suitable for communication with or implementation of the smooth streaming provided in accordance with various embodiments of this disclosure.
- computing systems can be connected together by wired or wireless systems, by local networks or widely distributed networks.
- networks are coupled to the Internet, which provides an infrastructure for widely distributed computing and encompasses many different networks, though any network infrastructure can be used for exemplary communications made incident to the systems as described in various embodiments.
- client is a member of a class or group that uses the services of another class or group to which it is not related.
- a client can be a process, e.g., roughly a set of instructions or tasks, that requests a service provided by another program or process.
- the client may be or use a process that utilizes the requested service without having to “know” any working details about the other program or the service itself.
- a client is usually a computer that accesses shared network resources provided by another computer, e.g., a server.
- a server e.g., a server.
- computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc. can be thought of as clients and computing objects 1010 , 1012 , etc. can be thought of as servers where computing objects 1010 , 1012 , etc.
- client computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc. provide data services, such as receiving data from client computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc., storing of data, processing of data, transmitting data to client computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc., although any computer can be considered a client, a server, or both, depending on the circumstances.
- a server is typically a remote computer system accessible over a remote or local network, such as the Internet or wireless network infrastructures.
- the client process may be active in a first computer system, and the server process may be active in a second computer system, communicating with one another over a communications medium, thus providing distributed functionality and allowing multiple clients to take advantage of the information-gathering capabilities of the server.
- the computing objects 1010 , 1012 , etc. can be Web servers with which the client computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc. communicate via any of a number of known protocols, such as the hypertext transfer protocol (HTTP).
- HTTP hypertext transfer protocol
- Objects 1010 , 1012 , etc. may also serve as client computing objects or devices 1020 , 1022 , 1024 , 1026 , 1028 , etc., as may be characteristic of a distributed computing environment.
- the techniques described herein can be applied to any device suitable for implementing various embodiments described herein.
- Handheld, portable and other computing devices and computing objects of all kinds are contemplated for use in connection with the various embodiments, e.g., anywhere that a device may wish to read or write transactions from or to a data store.
- the below general purpose remote computer described below in FIG. 11 is but one example of a computing device.
- embodiments can partly be implemented via an operating system, for use by a developer of services for a device or object, and/or included within application software that operates to perform one or more functional aspects of the various embodiments described herein.
- Software may be described in the general context of computer executable instructions, such as program modules, being executed by one or more computers, such as client workstations, servers or other devices.
- computers such as client workstations, servers or other devices.
- client workstations such as client workstations, servers or other devices.
- FIG. 11 thus illustrates an example of a suitable computing system environment 1100 in which one or aspects of the embodiments described herein can be implemented, although as made clear above, the computing system environment 1100 is only one example of a suitable computing environment and is not intended to suggest any limitation as to scope of use or functionality. Neither is the computing environment 1100 be interpreted as having any dependency or requirement relating to any one or combination of components illustrated in the exemplary operating environment 1100 .
- an exemplary remote device for implementing one or more embodiments includes a general purpose computing device in the form of a computer 1110 .
- Components of computer 1110 may include, but are not limited to, a processing unit 1120 , a system memory 1130 , and a system bus 1122 that couples various system components including the system memory to the processing unit 1120 .
- Computer 1110 includes a variety of computer readable media and can be any available media that can be accessed by computer 1110 .
- the system memory 1130 may include computer storage media in the form of volatile and/or nonvolatile memory such as read only memory (ROM) and/or random access memory (RAM).
- ROM read only memory
- RAM random access memory
- memory 1130 may also include an operating system, application programs, other program modules, and program data.
- a user can enter commands and information into the computer 1110 through input devices 1140 .
- a monitor or other type of display device is also connected to the system bus 1122 via an interface, such as output interface 1150 .
- computers can also include other peripheral output devices such as speakers and a printer, which may be connected through output interface 1150 .
- the computer 1110 may operate in a networked or distributed environment using logical connections to one or more other remote computers, such as remote computer 1170 .
- the remote computer 1170 may be a personal computer, a server, a router, a network PC, a peer device or other common network node, or any other remote media consumption or transmission device, and may include any or all of the elements described above relative to the computer 1110 .
- the logical connections depicted in FIG. 11 include a network 1172 , such local area network (LAN) or a wide area network (WAN), but may also include other networks/buses.
- LAN local area network
- WAN wide area network
- Such networking environments are commonplace in homes, offices, enterprise-wide computer networks, intranets and the Internet.
- exemplary is used herein to mean serving as an example, instance, or illustration. For the avoidance of doubt, this matter disclosed herein is not limited by such examples.
- any aspect or design described herein as “exemplary” is not necessarily to be construed as preferred or advantageous over other aspects or designs, nor is it meant to preclude equivalent exemplary structures and techniques known to those of ordinary skill in the art.
- the terms “includes,” “has,” “contains,” and other similar words are used in either the detailed description or the claims, for the avoidance of doubt, such terms are intended to be inclusive in a manner similar to the term “comprising” as an open transition word without precluding any additional or other elements.
- Computer-readable storage media can be any available storage media that can be accessed by the computer, is typically of a non-transitory nature, and can include both volatile and nonvolatile media, removable and non-removable media.
- Computer-readable storage media can be implemented in connection with any method or technology for storage of information such as computer-readable instructions, program modules, structured data, or unstructured data.
- Computer-readable storage media can include, but are not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disk (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or other tangible and/or non-transitory media which can be used to store desired information.
- Computer-readable storage media can be accessed by one or more local or remote computing devices, e.g., via access requests, queries or other data retrieval protocols, for a variety of operations with respect to the information stored by the medium.
- a component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program, and/or a computer.
- a component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program, and/or a computer.
- an application running on computer and the computer can be a component.
- One or more components may reside within a process and/or thread of execution and a component may be localized on one computer and/or distributed between two or more computers.
- a “device” can come in the form of specially designed hardware; generalized hardware made specialized by the execution of software thereon that enables the hardware to perform specific function (e.g., coding and/or decoding); software stored on a computer readable medium; or a combination thereof.
Abstract
Description
-
- Input: A set of m Permutations Θ, window size K, input vector X.
- Output: Sparse vector of codes CX.
- 1. For each permutation Θi in Θ.
- (a) Permutate elements of X according to Θi to get X′.
- (b) Initialize ith sparse code cxi to 0.
- (c) Set cxi to the index of maximum value in X′(1 . . . K)
- i. For j=0 to K−1
- A. If X′(j)>X′(cxi) then cxi=j.
- 2. CX=[cx0, cx1, . . . , cxm−1], C contains m codes, each taking a value between 0 and K−1.
- (Algorithm 1)
- 1. For each permutation Θi in Θ.
where
R i(X,Y)=|L(X,i)∩L(Y,i)| (Equation 3)
L(X,i)={j|X(i)>X(j)} (Equation 4)
where j is an index that represents the positions in a sorted vector of dimensions. In order to derive Pk (Rank(Θ(ci))=j) for 0≦j≦n−K the difference of the cumulative distribution is determined and simplified, resulting in
where Ri(X, Y) is given by
The total number of ways codes for a permutation Θ on vectors X and Y can collide on any given code is given by:
K leading indices of a permutation Θ can be drawn from n dimensions in (K n) ways.
Therefore, the probability of collision of codes given by a random permutation Θ on vectors X and Y can be expressed as:
Claims (36)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/286,963 US8977627B1 (en) | 2011-11-01 | 2011-11-01 | Filter based object detection using hash functions |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/286,963 US8977627B1 (en) | 2011-11-01 | 2011-11-01 | Filter based object detection using hash functions |
Publications (1)
Publication Number | Publication Date |
---|---|
US8977627B1 true US8977627B1 (en) | 2015-03-10 |
Family
ID=52597920
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/286,963 Active 2033-04-15 US8977627B1 (en) | 2011-11-01 | 2011-11-01 | Filter based object detection using hash functions |
Country Status (1)
Country | Link |
---|---|
US (1) | US8977627B1 (en) |
Cited By (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9754355B2 (en) * | 2015-01-09 | 2017-09-05 | Snap Inc. | Object recognition based photo filters |
CN109416689A (en) * | 2018-01-16 | 2019-03-01 | 深圳力维智联技术有限公司 | Similar to search method and apparatus, the storage medium of magnanimity characteristic vector data |
US10326585B2 (en) | 2016-06-17 | 2019-06-18 | Hewlett Packard Enterprise Development Lp | Hash value generation through projection vector split |
US20190303750A1 (en) * | 2019-06-17 | 2019-10-03 | Intel Corporation | Reconfigurable memory compression techniques for deep neural networks |
US10521878B1 (en) * | 2017-09-27 | 2019-12-31 | United Services Automobile Association (Usaa) | Machine-learning for enhanced machine reading of non-ideal capture conditions |
CN112988815A (en) * | 2021-03-16 | 2021-06-18 | 重庆工商大学 | Method and system for online anomaly detection of large-scale high-dimensional high-speed stream data |
WO2023282927A1 (en) * | 2021-07-08 | 2023-01-12 | UiPath, Inc. | Image pattern matching to robotic process automations |
US20230090262A1 (en) * | 2020-06-24 | 2023-03-23 | Gsi Technology Inc. | Neural hashing for similarity search |
US11803860B2 (en) | 2013-09-09 | 2023-10-31 | UnitedLex Corp. | Email mappings |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7257586B2 (en) * | 2001-09-12 | 2007-08-14 | Science Applications International Corporation | Data ranking with a Lorentzian fuzzy score |
US20080288728A1 (en) * | 2007-05-18 | 2008-11-20 | Farooqui Aamir A | multicore wireless and media signal processor (msp) |
US7761466B1 (en) * | 2007-07-30 | 2010-07-20 | Hewlett-Packard Development Company, L.P. | Hash-based image identification |
US8055078B2 (en) * | 2008-02-28 | 2011-11-08 | Yahoo! Inc. | Filter for blocking image-based spam |
US8224849B2 (en) * | 2007-04-18 | 2012-07-17 | Microsoft Corporation | Object similarity search in high-dimensional vector spaces |
-
2011
- 2011-11-01 US US13/286,963 patent/US8977627B1/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7257586B2 (en) * | 2001-09-12 | 2007-08-14 | Science Applications International Corporation | Data ranking with a Lorentzian fuzzy score |
US8224849B2 (en) * | 2007-04-18 | 2012-07-17 | Microsoft Corporation | Object similarity search in high-dimensional vector spaces |
US20080288728A1 (en) * | 2007-05-18 | 2008-11-20 | Farooqui Aamir A | multicore wireless and media signal processor (msp) |
US7761466B1 (en) * | 2007-07-30 | 2010-07-20 | Hewlett-Packard Development Company, L.P. | Hash-based image identification |
US8055078B2 (en) * | 2008-02-28 | 2011-11-08 | Yahoo! Inc. | Filter for blocking image-based spam |
Non-Patent Citations (34)
Title |
---|
A. Broder, et al. Min-wise independent permutations. In STOC 1998. |
A. Broder. On the resemblance and containment of documents. In Sequences 1997. |
A. Maturi, et al. A New Weighted Rank Correlation. In J. Mathematics and Statistics, Oct. 1, 2008. * |
B. Wang, et al. Large-scale duplicate detection for web image search. In ICME 2006. |
D. Bhat, et al. Ordinal Measures for Visual Correspondence. In CVPR'96. 1996 IEEE. |
D. Lowe. Object recognition from local scale-invariant features. Proceedings of the International Conference on Computer Vision, Corfu (Sep. 1999). In IJCV 1999. |
D. Nister, et al. Scalable recognition with a vocabulary tree. In CVPR 2006. |
D. Tschopp, et al. Approximate nearest neighbor search through comparisons. In ArXiv preprint Sep. 11, 2009. |
E. Tola, et al. A fast local descriptor for dense matching. CVPR 2008. |
E. Tola, et al. Daisy: an Efficient Dense Descriptor Applied to Wide Baseline Stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, No. 5, May 2010, 815-830. In PAMI 2010. |
G. Shieh. A weighted Kendall's tau statistic. In Statistics & Probability Letters 39 (1998) 17-24. |
J. Friedman. An Overview of Predictive Learning and Function Approximation. Technical Report 112, Sep. 1994. Laboratory for Computational Statistics. Department of Statistics, Stanford University . In From Statistics to Neural Networks 1994. |
J. Pinto Da Costa, et al. A Weighted Rank Measure of Correlation. In Australian New Zealand Journal of Statistics 47(4), 2005, 515-529. |
J. Zhang, et al. Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study. International Journal of Computer Vision 2007. |
K. Eshghi, et al. Locality sensitive hash functions based on concomitant rank order statistics. In 14th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD'08), Aug. 2008. Posting Date Jul. 6, 2008. |
K. Yu, T. Zhang, Y. Gong. Nonlinear Learning using Local Coordinate Coding. In NIPS 2009. |
Lu, Jian, "Video Fingerprinting and Applications: a review," Media Forensics & Security Conference, Vobile, Inc., San Jose, CA, http://www.slideshare.net/jianlu/videofingerprintingspiemfs09d, Last accessed May 30, 2012. |
Lu, Jian, "Video fingerprinting for copy identification: from research to industry applications," Proceedings of SPIE13 Media Forensics and Security XI, vol. 7254, Jan. 2009, http://idm.pku.edu.cn/jiaoxue-MMF/2009/VideoFingerprinting-SPIE-MFS09.pdf, Last accessed May 30, 2012. |
M. Ozuysal, et al. Fast Keypoint Recognition in Ten Lines of Code. In CVPR 2007. |
M. Ozuysal, et al. Feature harvesting for tracking-by-detection. In ECCV 2006. |
Media Hedge, "Digital Fingerprinting," White Paper, Civolution and Gracenote, 2010, http://www.civolution.com/fileadmin/bestanden/white%20papers/Fingerprinting%20-%20by%20Civolution%20and%20Gracenote%20-%202010.pdf, Last accessed May 30, 2012. |
Milano, Dominic, "Content Control: Digital Watermarking and Fingerprinting," White Paper, Rhozet, a business unit of Harmonic Inc., http://www.rhozet.com/whitepapers/Fingerprinting-Watermarking.pdf, Last accessed May 30, 2012. |
O. Pele, et al. Robust real time pattern matching using bayesian sequential hypothesis testing. In PAMI 2008. |
P. Diaconis, et al. Spearman's footrule as a measure of disarray. Stanford, California and Murray Hill, N.J. Received Aug. 1976. Revised Apr. 1977. In J. Roy. Statistical Society 1977. |
P. Indyk, et al. Approximate nearest neighbors: towards removing the curse of dimensionality. In STOC 1998, Dallas, Texas. ACM 1998. |
R. Lin, et al. SPEC Hashing: Similarity Preserving algorithm for Entropy-based Coding. In CVPR 2010. |
R. Zabih, et al. Non-parametric local transforms for computing visual correspondence. In ECCV 1994. |
R.E. Fan, et al. Liblinear: A Library for Large Linear Classification. Journal of Machine Learning Research 9 (2008) 1871-1874. |
S. Baluja, et al. Boosting Sex Identification Performance. In IJCV 2007. |
S. Winder, et al. Picking the best Daisy. In CVPR 2009. |
V. Lepetit. Keypoint recognition using randomized trees. In PAMI 2006. |
X.J. Wang, et al. Annosearch: Image autoannotation by search. In CVPR 2006. |
Y. Lifshits, et al. Combinatorial Algorithms for Nearest Neighbors, Near-Duplicates and Small World Design. In SODA 2009. |
Y. Weiss, et al. Spectral Hashing. In NIPS 2009. |
Cited By (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11803860B2 (en) | 2013-09-09 | 2023-10-31 | UnitedLex Corp. | Email mappings |
US11978057B2 (en) | 2013-09-09 | 2024-05-07 | UnitedLex Corp. | Single instance storage of metadata and extracted text |
US11734342B2 (en) | 2015-01-09 | 2023-08-22 | Snap Inc. | Object recognition based image overlays |
US11301960B2 (en) | 2015-01-09 | 2022-04-12 | Snap Inc. | Object recognition based image filters |
US10380720B1 (en) | 2015-01-09 | 2019-08-13 | Snap Inc. | Location-based image filters |
US9978125B1 (en) | 2015-01-09 | 2018-05-22 | Snap Inc. | Generating and distributing image filters |
US10157449B1 (en) | 2015-01-09 | 2018-12-18 | Snap Inc. | Geo-location-based image filters |
US9754355B2 (en) * | 2015-01-09 | 2017-09-05 | Snap Inc. | Object recognition based photo filters |
US10326585B2 (en) | 2016-06-17 | 2019-06-18 | Hewlett Packard Enterprise Development Lp | Hash value generation through projection vector split |
US10521878B1 (en) * | 2017-09-27 | 2019-12-31 | United Services Automobile Association (Usaa) | Machine-learning for enhanced machine reading of non-ideal capture conditions |
US11514548B1 (en) | 2017-09-27 | 2022-11-29 | United Services Automobile Association (Usaa) | Machine-learning for enhanced machine reading of non-ideal capture conditions |
US11710210B1 (en) | 2017-09-27 | 2023-07-25 | United Services Automobile Association (Usaa) | Machine-learning for enhanced machine reading of non-ideal capture conditions |
CN109416689A (en) * | 2018-01-16 | 2019-03-01 | 深圳力维智联技术有限公司 | Similar to search method and apparatus, the storage medium of magnanimity characteristic vector data |
US11625584B2 (en) * | 2019-06-17 | 2023-04-11 | Intel Corporation | Reconfigurable memory compression techniques for deep neural networks |
US20190303750A1 (en) * | 2019-06-17 | 2019-10-03 | Intel Corporation | Reconfigurable memory compression techniques for deep neural networks |
US20230090262A1 (en) * | 2020-06-24 | 2023-03-23 | Gsi Technology Inc. | Neural hashing for similarity search |
US11763136B2 (en) * | 2020-06-24 | 2023-09-19 | Gsi Technology Inc. | Neural hashing for similarity search |
CN112988815A (en) * | 2021-03-16 | 2021-06-18 | 重庆工商大学 | Method and system for online anomaly detection of large-scale high-dimensional high-speed stream data |
CN112988815B (en) * | 2021-03-16 | 2023-09-05 | 重庆工商大学 | Method and system for online anomaly detection of large-scale high-dimensional high-speed stream data |
WO2023282927A1 (en) * | 2021-07-08 | 2023-01-12 | UiPath, Inc. | Image pattern matching to robotic process automations |
EP4136598A4 (en) * | 2021-07-08 | 2023-09-20 | UiPath, Inc. | Image pattern matching to robotic process automations |
US11809883B2 (en) | 2021-07-08 | 2023-11-07 | UiPath, Inc. | Image pattern matching to robotic process automations |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8977627B1 (en) | Filter based object detection using hash functions | |
US9684715B1 (en) | Audio identification using ordinal transformation | |
Arandjelović et al. | DisLocation: Scalable descriptor distinctiveness for location recognition | |
AU2011326269B2 (en) | Vector transformation for indexing, similarity search and classification | |
US10120934B2 (en) | Hold back and real time ranking of results in a streaming matching system | |
US8676725B1 (en) | Method and system for entropy-based semantic hashing | |
US9704100B2 (en) | Authentication method, authentication device, and recording medium | |
US9208154B1 (en) | IDF weighting of LSH bands for live reference ingestion | |
Gao et al. | Processing k-skyband, constrained skyline, and group-by skyline queries on incomplete data | |
Okkalioglu et al. | A survey: deriving private information from perturbed data | |
US20230275744A1 (en) | Privacy-preserving fast approximate k-means clustering with hamming vectors | |
Navarro et al. | An empirical evaluation of intrinsic dimension estimators | |
US9143784B2 (en) | Transformation invariant media matching | |
Venkat | The curse of dimensionality: Inside out | |
Bhowmik et al. | Letor methods for unsupervised rank aggregation | |
US20130204861A1 (en) | Method and apparatus for facilitating finding a nearest neighbor in a database | |
US9081817B2 (en) | Active learning of record matching packages | |
Chelvan et al. | A comparative analysis of feature selection stability measures | |
Gao et al. | Durable top-k instant-stamped temporal records with user-specified scoring functions | |
Bustos et al. | An empirical evaluation of intrinsic dimension estimators | |
Fang et al. | Multi-dimensional Time Series Approximation Using Local Features at Thinned-out Keypoints. | |
Borges et al. | Spatial-time motifs discovery | |
Khurana et al. | FAQ: A framework for fast approximate query processing on temporal data | |
Sharma et al. | Enhancing the accuracy of movie recommendation system based on probabilistic data structure and graph database | |
Fuad | When optimization is just an illusion |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:VIJAYANARASIMHAN, SUDHEENDRA;YAGNIK, JAY;SIGNING DATES FROM 20111031 TO 20111101;REEL/FRAME:027163/0372 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044334/0466Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |