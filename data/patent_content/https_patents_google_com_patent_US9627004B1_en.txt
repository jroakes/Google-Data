US9627004B1 - Video frame annotation - Google Patents
Video frame annotation Download PDFInfo
- Publication number
- US9627004B1 US9627004B1 US14/883,461 US201514883461A US9627004B1 US 9627004 B1 US9627004 B1 US 9627004B1 US 201514883461 A US201514883461 A US 201514883461A US 9627004 B1 US9627004 B1 US 9627004B1
- Authority
- US
- United States
- Prior art keywords
- video
- existence
- entity
- probability
- metadata
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7847—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using low-level visual features of the video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/7867—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using information manually generated, e.g. tags, keywords, comments, title and artist information, manually generated time, location and usage information, user ratings
-
- G06F17/241—
-
- G06F17/30799—
-
- G06F17/3082—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
- G06F18/2155—Generating training patterns; Bootstrap methods, e.g. bagging or boosting characterised by the incorporation of unlabelled data, e.g. multiple instance learning [MIL], semi-supervised techniques using expectation-maximisation [EM] or naïve labelling
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2415—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on parametric or probabilistic models, e.g. based on likelihood ratio or false acceptance rate versus a false rejection rate
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/166—Editing, e.g. inserting or deleting
- G06F40/169—Annotation, e.g. comment data or footnotes
-
- G06K9/00744—
-
- G06K9/4642—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/764—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using classification, e.g. of video objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/774—Generating sets of training patterns; Bootstrap methods, e.g. bagging or boosting
- G06V10/7753—Incorporation of unlabelled data, e.g. multiple instance learning [MIL]
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/102—Programmed access in sequence to addressed parts of tracks of operating record carriers
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/19—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier
- G11B27/28—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/23418—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/235—Processing of additional data, e.g. scrambling of additional data or processing content descriptors
- H04N21/2353—Processing of additional data, e.g. scrambling of additional data or processing content descriptors specifically adapted to content descriptors, e.g. coding, compressing or processing of metadata
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/25—Management operations performed by the server for facilitating the content distribution or administrating data related to end-users or client devices, e.g. end-user or client device authentication, learning user preferences for recommending movies
- H04N21/251—Learning process for intelligent management, e.g. learning user preferences for recommending movies
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/27—Server based end-user applications
- H04N21/274—Storing end-user multimedia data in response to end-user request, e.g. network recorder
- H04N21/2743—Video hosting of uploaded data from client
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/27—Server based end-user applications
- H04N21/278—Content descriptor database or directory service for end-user access
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/47—End-user applications
- H04N21/482—End-user interface for program selection
- H04N21/4828—End-user interface for program selection for searching program descriptors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/83—Generation or processing of protective or descriptive data associated with content; Content structuring
- H04N21/84—Generation or processing of descriptive data, e.g. content descriptors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/83—Generation or processing of protective or descriptive data associated with content; Content structuring
- H04N21/845—Structuring of content, e.g. decomposing content into time segments
- H04N21/8456—Structuring of content, e.g. decomposing content into time segments by decomposing the content in the time domain, e.g. in time segments
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V2201/00—Indexing scheme relating to image or video recognition or understanding
- G06V2201/10—Recognition assisted with metadata
Definitions
- the present application is related to video processing, and more particularly, to labelling video frames using metadata.
- Media hosting services can host millions of media content items (also referred to as “media items”, or simply “items”), such as music, movies, e-books, news articles, user generated content, and the like.
- media content items also referred to as “media items”, or simply “items”
- users of the media hosting service can browse or search media content items by providing keywords or search terms to search the information describing the media content items such as titles, summaries of the media content items, objects of interest, etc.
- many media content items may be returned responsive to user searches. Consequently, it can be difficult for the users to assess which of the hundreds or thousands of media content items in the set of search results are of the greatest relevance to them.
- Additional information may be provided to users to help the users to assess the relevance of the search results.
- metadata associated with the video content may allow users to assess the relevance of the video content items such as images from the video content items, authors of the video content items, length of the video content items, or information indicating the popularity of the video content items, but such metadata may be lacking in many cases (e.g., if a video submitter did not provide it), or inaccurate (e.g., in the case of intentionally misleading “spam” metadata).
- metadata may be lacking in many cases (e.g., if a video submitter did not provide it), or inaccurate (e.g., in the case of intentionally misleading “spam” metadata).
- a computer-implemented method selects an entity from a plurality of entities identifying characteristics of a video item, where the video item has associated metadata.
- the computer-implemented method receives probabilities of existence of the entity in video frames of the video item, and selects a video frame determined to comprise the entity responsive to determining the video frame having a probability of existence of the entity greater than zero.
- the computer-implemented method determines a scaling factor for the probability of existence of the entity using the metadata of the video item, and determines an adjusted probability of existence of the entity by using the scaling factor to adjust the probability of existence of the entity.
- the computer-implemented method labels the video frame with the adjusted probability of existence.
- a non-transitory computer-readable medium comprising computer program instructions executable by a processor.
- the computer program instructions comprise instructions for selecting an entity from a plurality of entities identifying characteristics of a video item, where the video item has associated metadata.
- the computer program instructions comprise instructions for receiving probabilities of existence of the entity in video frames of the video item, and instructions for selecting a video frame determined to comprise the entity responsive to determining the video frame having a probability of existence of the entity greater than zero.
- the computer program instructions comprise instructions for determining a scaling factor for the probability of existence of the entity using the metadata of the video item, and instructions for determining an adjusted probability of existence of the entity by using the scaling factor to adjust the probability of existence of the entity.
- the computer program instructions comprise instructions for labeling the video frame with the adjusted probability of existence.
- a system comprises a processor for executing computer program instructions and a non-transitory computer-readable storage medium comprising computer program instructions executable by the processor.
- the computer program instructions comprise instructions for selecting an entity from a plurality of entities identifying characteristics of a video item, where the video item has associated metadata.
- the computer program instructions comprise instructions for receiving probabilities of existence of the entity in video frames of the video item, and instructions for selecting a video frame determined to comprise the entity responsive to determining the video frame having a probability of existence of the entity greater than zero.
- the computer program instructions comprise instructions for determining a scaling factor for the probability of existence of the entity using the metadata of the video item, and instructions for determining an adjusted probability of existence of the entity by using the scaling factor to adjust the probability of existence of the entity.
- the computer program instructions comprise instructions for labeling the video frame with the adjusted probability of existence.
- FIG. 1 is a high-level block diagram of a video hosting system server providing video annotation according to one embodiment.
- FIG. 2 is a block diagram of a video annotation engine according to one embodiment.
- FIG. 3 is a flowchart illustrating a process for annotating videos with probabilities of existence of entities according to one embodiment.
- FIG. 1 is a high-level block diagram of a system providing annotation of videos with probabilities of existence of entities at each video frame according to one embodiment.
- FIG. 1 illustrates a video hosting system 102 and a user 120 connected by a network 124 .
- the user 120 represents an entity that can access videos contained within the video hosting system 102 .
- a user 120 can access a video from the video hosting system 102 by browsing a catalog of videos, conducting searches using keywords, reviewing play lists from other users or the system administrator (e.g., collections of videos forming channels), or viewing videos associated with particular user groups (e.g., communities).
- the video hosting system 102 is adapted to receive videos for storage in order to enable the sharing of the videos with other users.
- the user 120 uses a computer system to communicate with the video hosting system 102 over the network 124 .
- the computer system is a personal computer executing a web browser 122 such as MICROSOFT INTERNET EXPLORER or GOOGLE CHROME that allows the user to view web pages and videos provided by the video hosting system 102 .
- the web browser 122 includes a video player (e.g., FlashTM from Adobe Systems, Inc.).
- the user 120 may utilize a network-capable device other than a computer system, such as a smart phone, a tablet, a car, a television “set-top box,” etc.
- FIG. 1 illustrates only a single user, it should be understood that many users (e.g., millions) can communicate with the website at any time.
- the single user 120 is illustrated in order to simplify and clarify the present description.
- the network 124 represents the communication pathways between the user and the video hosting system 102 .
- the network 124 is the Internet, but may also be any network, including but not limited to a LAN, a MAN, a WAN, a mobile, wired or wireless network, a cloud computing network, a private network, or a virtual private network, and any combination thereof.
- all or some of links can be encrypted using conventional encryption technologies such as the secure sockets layer (SSL), Secure HTTP and/or virtual private networks (VPNs).
- the entities can use custom and/or dedicated data communications technologies instead of, or in addition to, the ones described above.
- the video hosting system 102 represents any system that allows users to access video content via searching and/or browsing interfaces.
- the sources of videos can be from user uploads of videos, searches or crawls of other websites or databases of videos, or the like, or any combination thereof.
- a video hosting system 102 can be configured to allow for user uploads of content; in another embodiment a video hosting system 102 can be configured to only obtain videos from other sources by crawling such sources or searching such sources in real time.
- a suitable video hosting system 102 for implementation of the system is the YOUTUBETM website; other video hosting websites are known as well, and can be adapted to operate according to the teaching disclosed herein.
- website represents any computer system adapted to serve content using any internetworking protocols, and is not intended to be limited to content uploaded or downloaded via the Internet or the HTTP protocol.
- functions described in one embodiment as being performed on the server side can also be performed on the client side in other embodiments if appropriate.
- the functionality attributed to a particular component can be performed by different or multiple components operating together.
- the video hosting system 102 comprises a front end server 104 , an ingest server 106 , a video search server 108 , a video annotation engine 110 , a video access server 112 , a video data store 114 , a feature data store 116 , and an entity data store 118 .
- Many conventional features, such as firewalls, load balancers, application servers, failover servers, site management tools and so forth are not shown so as not to obscure the features of the system.
- the front end server 104 handles all communication with the user via the network 124 .
- the front end server receives requests from users and communicates with the other servers of the video hosting system 102 in order to process the requests.
- the front end server 104 is further configured to monitor user interactions with the video hosting system 102 . For example, if a user clicks on a web page, views a video, makes a purchase, opens a document, fills a web-based form, the front end server 104 monitors these interactions.
- the front end server 104 may be further configured to transmit and present the requested video and related video links to the user on a webpage.
- the requested video is streamed by the front end server 104 to the user.
- One or more related video links appear on the webpage where the requested video is playing, such that the related video link can be selected by a user 120 in order to view the related videos.
- Any content received via the network 124 from a user for posting to the video hosting system 102 is passed on to the ingest server 106 for processing.
- the processing of the video file includes assigning an identification number to the newly received video file.
- Other steps of processing the video file may include formatting (e.g., transcoding), compressing, metadata tagging, content analysis, and/or other data processing methods.
- the user transmits a form along with the video file transmitted to the video hosting system 102 .
- the user may include in the form information that describes the video (e.g., title, description, and tag information).
- the form information may also include an indication of the media type, which for uploaded videos would always be the “video” type.
- the ingest server 106 stores the processed video file in a video data store 114 and stores the information included in the form as metadata of the video file.
- the video data store 114 is the storage system where the video files transmitted to the video hosting system 102 are stored.
- a video may be accompanied by icons or thumbnail views, associated metadata, such as title, author, tags, description, comments, and rating.
- the ingest server 106 may generate features used to characterize the media content items stored in the video data store 114 .
- Example features include lego centrality, lego relevance, topicality, and cowatch.
- Lego centrality is the video level centrality score of an entity.
- Lego relevance is the video level relevance score of an entity. Lego centrality and relevance may be obtained from the title and description of a video.
- Topicality is a score summarizing annotations of a webpage hosting the video.
- Cowatch are centrality, relevance, and topicality scores of other videos co-watched with the video.
- Features may be stored with media content items as metadata, for example in the video data store 114 .
- the ingest server 106 may further store the features in the feature data store 116 .
- An index may be maintained in the video data store 114 that associates each video file stored in the video data store with the features stored in the feature data store 116 that are associated with the video file.
- the ingest server 106 For each video frame of a video file, the ingest server 106 generates features that characterize each video frame of the video file.
- the ingest server 106 may identify entities associated with the video files stored in the video data store 114 and store the entities in the entity data store 118 .
- Entities are text descriptors that identify characteristics of media content items, for example, actual things, ideas, or concepts that have meanings. For example, “pugs”, “Machu Picchu”, “philosophy”, and “sleepy” are all examples of entities. Entities may be stored with media content items as metadata, for example in the video data store 114 .
- the ingest server 106 identifies entities associated with each video frame of the video file. Each video frame of each video file may have one or more entities associated with it.
- the entities stored as metadata might include “dog”, “food”, “San Francisco”, “pug”, “croissant”, “hungry”, and “Golden Gate Park”, among others.
- Entities may be associated with media content items when the media content items are ingested by the ingest server 106 .
- the set of potential entities that can be associated with media content items may be derived from tags or other descriptive information provided by a user.
- entities may be derived from textual descriptions and metadata accompanying the videos, as well as closed captioning text present in the video, or by converting audio to text and then extracting entities from the text.
- textual documents e.g., web pages, emails, etc.
- entities may be determined using term frequency analysis, semantic analysis, natural language processing, or other methods.
- An index is maintained in the video data store 114 that associates each entity with the set of content items to which the entity is identified. In addition, for each content item, there is maintained a list of the entities associated with the content item.
- the ingest server 106 may determine probabilities of existence x e,v (t) of entities e at time t for the video v using machine learning techniques. A likelihood of an entity existing on a frame is determined. In some embodiments, the ingest server 106 determines the probabilities of existence x e,v (t) by using the features associated with the video file stored in the feature data store 116 and the entity data store 118 . For example, the ingest server 106 determines the probability of existence x e,v (t) of an entity e being affected by one or more features associated with a video frame (i.e., at time t for the video). The probability distribution for the entity e defined over a space of possible features F is determined. The probability of existence x e,v (t) may be based on a fusion score of various features.
- the video search server 108 processes any search query received by the front end server 104 from a user.
- the search query transmitted by the user to the front end server 104 includes search criteria, such as keywords that may identify videos the user is interested in viewing.
- the search query might be the textual string “machu picchu”.
- the video search server 108 may use the search criteria, for example, to query the metadata of and/or entities associated with all video files stored in the video data store 114 or to query the entity data store 118 .
- the search results are the videos including entities that match the search query.
- the search results for the search query “machu picchu” include video items that are annotated with the entity “machu picchu” that match the search query.
- the search results from the query are transmitted to the front end server 104 , so that the search results can be presented to the user.
- the video access server 112 receives from the front end server requests from users that wish to view (or play back) a specific video. From the user perspective, a user may submit a request for a video by browsing the different categories of the video hosting system 102 or by clicking on a link to a video from a search results webpage.
- the request transmitted by the user can include the identification number of the video the user wishes to view (which can be included automatically once the user clicks on the link for the video).
- the video access server 112 uses the identification number to search and locate where the video is stored in the video data store 114 .
- the video access server 112 provides the video requested to the front end server 104 .
- the video annotation engine 110 annotates video files stored in the video data store 114 with entities.
- the video annotation engine 110 may annotate each video file using probabilities of existence of the entities.
- the video annotation engine 110 labels each video frame of the video file with entities (if any) that exist in that frame, as well as the associated probabilities that those entities exist in the frame. To improve accuracy, the video annotation engine 110 adjusts the probabilities of existence of the entities. When probabilities of existence of entities are determined using frame-level features, some entities determined to exist in video frames by the ingest server 106 may in fact not exist in those frames.
- the video annotation engine 110 removes those entities from video frames prior to labeling video frames by (for example) using video-level metadata to support analysis of entities at the frame level. As such, the video frames are annotated more accurately because the likelihoods of entities existing on video frames are boosted due to the removal of likely-spurious entity determinations arrived at by frame-level analysis.
- Video-level metadata is metadata of video items, such as title, description, comments, and the like.
- the video annotation engine 110 may determine adjusted probabilities of existence y e,v (t) for entities that are determined to exist in video frames by using metadata of videos.
- the probabilities of existence are adjusted only for those entities of which the probabilities of existence x e,v (t) are greater than 0.
- the adjusted probabilities of existence y e,v (t) are determined based on probabilities of existence x e,v (t) of entities in video frames as well as metadata associated with video frames.
- the video annotation engine 110 averages the probabilities of existence of entities for multiple frames and labels these frames with the averaged probabilities of existence. For example, a shot-level probability of existence is determined by averaging the probabilities of existence of one entity for video frames in a video shot (i.e., a segment of video). The video frames of that video shot are each labeled with this shot-level probability of existence of the entity.
- the video hosting system 102 can take a number of different actions. For example, the video hosting system 102 can identify video frames within video content items that are most relevant to search queries containing one or more keywords, e.g., showing those frames in search results as representations of the video content items. The video hosting system 102 can further rank a set of video content items retrieved responsive to a search query according to the probabilities of existence of the entities labeled for video frames of each video content items. The video hosting system 102 can further use the labeled entities and associated probabilities of existence to identify video frames within video content items that are relevant to information describing the video content items, such as the title of the video.
- Identifying probabilities of existence of entities allows a provider of the videos such as a media host (e.g., the video hosting system 102 ) to provide video content items that are most relevant to a user's search query based on the identified entities and the associated probabilities of existence.
- This identification further allows the media host to display information for a set of videos retrieved responsive to a search query in association with entities that are most relevant to the search query and the video frames that are most relevant to the search query, thus providing the users with additional information which they can use to assess the relevance of the videos to their search and identify videos they wish to view. For example, the media host displays thumbnails of most relevant segments of the videos.
- the data processing operations of the video annotation engine 110 inherently require a programmed computer system for their practical implementation.
- the content received and shared by the video hosting system 102 is generally referred to as videos, video files, or video items, as appropriate for the video-specific embodiments described herein, but it should be understood that the video hosting system 102 can receive and share content of any media type.
- This content may be referred to as media content items, media items, or items.
- the operations of the video annotation engine 110 described herein for annotating video files can be applied to any type of media content item, not only videos; other suitable type of content items include audio files (e.g. music, podcasts, audio books, and the like), documents, multimedia presentations, digital purchases of goods and services, and so forth.
- FIG. 2 is a high-level block diagram illustrating a detailed view of the video annotation engine 110 , according to one embodiment.
- the video annotation engine 110 includes several modules. Those of skill in the art will recognize that other embodiments can have different modules than the ones described here, and that the functionalities can be distributed among the modules in a different manner.
- the functions ascribed to the video annotation engine 110 can be performed by multiple engines.
- the video annotation engine 110 includes an influence data store 202 , a machine learning engine 204 , and an entity probability adjustment module 206 .
- the machine learning engine 204 measures the influence of metadata on the probability of existence of an entity (e.g., “machu picchu”) and stores such influence in the influence data store 202 .
- the influence may be a scaling factor that adjusts the probability of existence of an entity.
- the machine learning engine 204 may use a semi-supervised learning approach to determine the influence.
- the machine learning engine 204 may train a classifier (e.g., a weight vector matrix ⁇ ) using training data features such as the logarithm of probabilities of existence of entities lgx e,v (t) at time t for the video v and metadata such as various retention statistics R v (t) at time t for the video v such as an average audience retention rate at a time point, video level features V v,e (e.g., lego centrality, relevance, topicality, or cowatch) of the entity e for the video v as a whole, and frame level features F v,e (t) of the entity e at time t for the video v.
- a classifier e.g., a weight vector matrix ⁇
- the frame level features may include various image classifiers identified for a frame, for example, by one or more machine-learned models.
- the frame level features may include a fusion score of the individual classifiers.
- the scaling factor (denoted e ( ⁇ m) ) measures the influence of metadata on existence of an entity and may be based on a linear fusion model.
- the machine learning engine 204 determines a weight vector matrix ⁇ for metadata and the probability of existence of entities x e,v (t).
- the weight vector matrix ⁇ includes a weight vector for each type of metadata.
- the weight vector matrix ⁇ includes a weight vector ⁇ R for various retention statistics R v (t) at time t for the video v, a weight vector ⁇ 1 for video level feature V v,e of the entity e for the video v, and a weight vector ⁇ 2 for frame level feature F v,e (t) of the entity e at time t for the video v.
- the entity probability adjustment module 206 determines, for each entity e that is identified to exist on a frame (i.e., the probability of existence x e,v (t) is greater than 0), an adjusted probability of existence y e,v (t) by adjusting the probability of existence x e,v (t) using metadata.
- the entity probability adjustment module 206 determines a scaling factor e ( ⁇ M) based on the metadata and applies the scaling factor to the probability of existence x e,v (t).
- the scaling factor e ( ⁇ M) takes into account all selected metadata that have an influence on the probability of existence of an entity on a video frame.
- the weight vector matrix ⁇ determined by the machine learning engine 204 and stored in the influence data store 202 is applied to metadata to determine the scaling factor e ( ⁇ M) .
- the weight vector matrix ⁇ includes a retention statistics weight vector ⁇ R , a video level feature weight vector ⁇ 1 , and a frame level feature weight vector ⁇ 2 .
- the entity probability adjustment module 206 adjusts the probabilities of entities based on the importance of the entity in a video.
- the entity probability adjustment module 206 determines a scaling factor which is a ratio of the video-level centrality of the entity to the maximum probability of existence of the entity at the video and boosts the probability of existence of the entity at a frame using this factor.
- the entity probability adjustment module 206 may scale up the probability of existence of an entity if the entity is an important entity in a video, but does not scale down merely because the entity is not an important entity in the video. The more important an entity to a video is, the greater the amount that the entity probability adjustment module 206 scales up the probability of existence.
- the entity probability adjustment module 206 determines the probability of existence y e,v (t) according to
- Equation ⁇ ⁇ ( 3 ) y e , v ⁇ ( t ) max ⁇ ( x e , v ⁇ ( t ) * c ⁇ ( e , v ) x m ⁇ ⁇ a ⁇ ⁇ x ⁇ ( e ) , x e , v ⁇ ( t ) ) , ( 3 )
- x max (e) is the maximum probability of existence of an entity e in a video v
- c(e, v) is the video-level centrality of the entity e.
- the video-level centrality of the entity is used to boost the probability of existence.
- the entity probability adjustment module 206 determines an interim adjusted probability of existence
- the entity probability adjustment module 206 determines the adjusted probability of existence y e,v (t) as the greater between the interim probability of existence
- FIG. 3 is a flowchart illustrating a process for annotating videos with probabilities of existence of entities, according to one embodiment.
- the method provides 302 training data comprising metadata associated with video items and probabilities of existence that are detected features that are correlated with the entity for training a machine learning model. Entities that are determined to exist in a video frame have probabilities of existence greater than zero. Metadata can be used to remove entities that do not exist in a video frame.
- the machine learning model trains 304 a classifier for metadata using the training data.
- the classifier is a weight vector matrix comprising weight vectors for metadata determined to have an influence on the probability of existence of the entity.
- the machine learning model evaluates the influence of each type of metadata on the probabilities of existence of entities as a weight vector and combines the influence of all selected types of metadata that affect the probabilities of existence.
- the method receives 306 videos, metadata associated with the videos, and probabilities of existence of entities identified in frames of the videos.
- the method determines 308 scaling factors using the received video items, metadata, and probabilities of existence and applies scaling factors to adjust the probabilities of existence.
- the method further uses the determined classifier to determine the scaling factors and multiplies the probabilities of existence by the scaling factors to determine adjusted probabilities of existence.
- the method determines scaling factors for an entity as a ratio between the centrality of the entity and the maximum probability of existence.
- the method determines an adjusted probability of existence as the greater of the probability of existence and the probability of existence multiplied by the scaling factor.
- the method 310 labels the video frame with the entity and the adjusted probability of existence of the entity.
- the method may label each video frame of a video item with entities and the adjusted probability for each entity.
- Each video frame of a video item may be associated with the entities and the adjusted probabilities for the entities.
- the method may further receive a search query from a user and identify and provide video files or video frames that satisfy the search query.
- the method may identify the video files or video frames by querying the labels of entities and the associated probabilities of existence of the entities on video frames.
- the method can identify video frames within video content items that are most relevant to search queries containing one or more keywords, e.g., showing those frames in search results as representations of the video content items.
- the method can rank a set of video content items retrieved responsive to a search query according to the probabilities of existence of the entities labeled for video frames of each video content items.
- the method can use the labeled entities and associated probabilities of existence to identify video frames within video content items that are relevant to information describing the video content items, such as the title of the video.
- the present invention also relates to an apparatus for performing the operations herein.
- the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability.
- the present invention is well-suited to a wide variety of computer network systems over numerous topologies.
- the configuration and management of large networks comprise storage devices and computers that are communicatively coupled to dissimilar computers and storage devices over a network, such as the Internet.
Abstract
Description
y e,v(t)=x e,v(t)*e (ΛM) (1)
y e,v(t)=x e,v(t)*e (Λ
where Rv(t) are various retention statistics at time t for the video v, Vv,e are video level features (e.g., lego centrality, relevance, topicality, or cowatch) of the entity e for the video v, and Fv,e(t) are frame level features of the entity e at time t for the video v.
where xmax(e) is the maximum probability of existence of an entity e in a video v, and c(e, v) is the video-level centrality of the entity e. The video-level centrality of the entity is used to boost the probability of existence. The entity
by multiplying the probability of existence xe,v(t) and the scaling factor
The entity
and the probability of existence xe,v(t).
Claims (21)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/883,461 US9627004B1 (en) | 2015-10-14 | 2015-10-14 | Video frame annotation |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/883,461 US9627004B1 (en) | 2015-10-14 | 2015-10-14 | Video frame annotation |
Publications (1)
Publication Number | Publication Date |
---|---|
US9627004B1 true US9627004B1 (en) | 2017-04-18 |
Family
ID=58765569
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/883,461 Active 2035-10-21 US9627004B1 (en) | 2015-10-14 | 2015-10-14 | Video frame annotation |
Country Status (1)
Country | Link |
---|---|
US (1) | US9627004B1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20190037270A1 (en) * | 2017-07-31 | 2019-01-31 | Zhilabs S.L. | Determination of qoe in encrypted video streams using supervised learning |
US10866646B2 (en) | 2015-04-20 | 2020-12-15 | Tiltsta Pty Ltd | Interactive media system and method |
US20210216781A1 (en) * | 2018-05-29 | 2021-07-15 | Samsung Electronics Co., Ltd. | Electronic device and control method therefor |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8082235B1 (en) * | 2009-04-09 | 2011-12-20 | Google Inc. | Self healing system for inaccurate metadata |
US8789093B2 (en) * | 2010-05-25 | 2014-07-22 | At&T Intellectual Property I, Lp | System and method for managing a surveillance system |
-
2015
- 2015-10-14 US US14/883,461 patent/US9627004B1/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8082235B1 (en) * | 2009-04-09 | 2011-12-20 | Google Inc. | Self healing system for inaccurate metadata |
US8789093B2 (en) * | 2010-05-25 | 2014-07-22 | At&T Intellectual Property I, Lp | System and method for managing a surveillance system |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10866646B2 (en) | 2015-04-20 | 2020-12-15 | Tiltsta Pty Ltd | Interactive media system and method |
US20190037270A1 (en) * | 2017-07-31 | 2019-01-31 | Zhilabs S.L. | Determination of qoe in encrypted video streams using supervised learning |
US11234048B2 (en) * | 2017-07-31 | 2022-01-25 | Zhilabs S.L. | Determination of QOE in encrypted video streams using supervised learning |
US20210216781A1 (en) * | 2018-05-29 | 2021-07-15 | Samsung Electronics Co., Ltd. | Electronic device and control method therefor |
US11908192B2 (en) * | 2018-05-29 | 2024-02-20 | Samsung Electronics Co., Ltd. | Electronic device and control method therefor |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11200423B2 (en) | Feature-based video annotation | |
US11194859B2 (en) | Video competition discovery and recommendation | |
US8484203B1 (en) | Cross media type recommendations for media items based on identified entities | |
US9984310B2 (en) | Systems and methods for identifying semantically and visually related content | |
US8924372B2 (en) | Dynamic image display area and image display within web search results | |
US8429176B2 (en) | Extending media annotations using collective knowledge | |
US10318581B2 (en) | Video metadata association recommendation | |
US8806000B1 (en) | Identifying viral videos | |
US20220237247A1 (en) | Selecting content objects for recommendation based on content object collections | |
US9092490B2 (en) | Rich results relevant to user search queries for books | |
US9627004B1 (en) | Video frame annotation | |
US9128993B2 (en) | Presenting secondary music search result links | |
US11983218B2 (en) | Video competition discovery and recommendation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:VARADARAJAN, BALAKRISHNAN;SHETTY, SANKETH;NATSEV, APOSTOL;AND OTHERS;SIGNING DATES FROM 20160119 TO 20160120;REEL/FRAME:037547/0329 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044097/0658Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |