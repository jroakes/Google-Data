US20060212648A1 - Method and system for emulating content-addressable memory primitives - Google Patents
Method and system for emulating content-addressable memory primitives Download PDFInfo
- Publication number
- US20060212648A1 US20060212648A1 US11/083,209 US8320905A US2006212648A1 US 20060212648 A1 US20060212648 A1 US 20060212648A1 US 8320905 A US8320905 A US 8320905A US 2006212648 A1 US2006212648 A1 US 2006212648A1
- Authority
- US
- United States
- Prior art keywords
- memory element
- record
- match
- random
- reference data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11C—STATIC STORES
- G11C15/00—Digital stores in which information comprising one or more characteristic parts is written into the store and in which information is read-out by searching for one or more of these characteristic parts, i.e. associative or content-addressed stores
Definitions
- Embodiments of the present invention relate generally to data processing system memory resources and more particularly to a method and system for emulating content-addressable memory primitives.
- data and instructions are stored within memory storage elements arranged in a hierarchical structure.
- memory storage elements arranged in a hierarchical structure.
- smaller, faster memory elements are located closer (in terms of physical structure) and more tightly coupled (communicatively) to processing elements (e.g., processors or processor cores) and store a subset of data and/or instructions stored in larger, slower memory elements (e.g., fixed or removable magnetic or optical disks, tape storage, or the like) elsewhere within or coupled to the data processing system.
- processing elements e.g., processors or processor cores
- larger, slower memory elements e.g., fixed or removable magnetic or optical disks, tape storage, or the like
- main memory random-access memory.
- RAM random-access memory
- data values are stored in an array of addressed memory locations.
- an address e.g., a data processing system memory address
- data stored at the applied address is accessed and presented by the RAM.
- an address-based data searching method is performed in which data values are sequentially read out from the RAM and compared with the searched-for data value. Specifically, a series of addresses are sequentially transmitted to an address port of the RAM, thereby causing data values to be read out from the memory locations addressed. A separate comparator element is then used to compare each of the output data values with the searched-for data value, generating a signal when a match occurs.
- address-based search operations are very time consuming as only a single data value is typically processed each clock cycle.
- a data value may be searched by content, rather than address.
- a conventional CAM data values are stored such that each data value is assigned to a row or column of an array of storage cells.
- a content-based data match operation is performed in which a searched-for data value is simultaneously compared with all rows/columns containing the pre-loaded data values.
- a “match” signal is generated by the CAM element, along with an address indicating the storage location (i.e., row or column) of the matching pre-loaded data value.
- FIG. 1 illustrates a data processing system processor cache as one such exemplary application.
- the depicted data processing system processor cache 100 includes a cache memory element 102 coupled to a cache tag lookup element 104 as shown.
- data processing system processor cache 100 will be described herein as a 32 kilobyte cache having 1024 lines of 32 bytes each and organized as a 64-way set associative cache including 16 sets.
- Data processing system processor cache 100 is addressed using a 32 bit address with bits 0 through 4 (represented as bits 4 : 0 ) identifying a specific byte within a cache line, bits 5 through 8 (represented as bits 8 : 5 ) identifying a specific set of the 16 possible sets, and remaining bits (represented as bits 31 : 9 ) making up a “cache tag” which identifies a block-frame address in memory where the cache line resides.
- a cache tag is used to verify that the cache line addressed in fact stores the requested data.
- Cache tag lookup element 104 includes a number of CAM elements 106 A, 106 B, . . . 106 N coupled to a multiplexer 108 and to a match indication signal generation element, (e.g., OR gate 110 ).
- a match indication signal generation element e.g., OR gate 110
- 16 CAM elements 106 are employed.
- a cache tag e.g., bits 31 : 9 of a data processing system memory address
- Each of CAM elements 106 A- 106 N in the illustrated processor cache is a 64 ⁇ 23 CAM including 64 23-bit registers (not illustrated) coupled to 64 comparators which are in turn coupled to encoding logic (not illustrated).
- the variable “N” is intended to indicate some positive integer value and need not indicate the same value consistently.
- each of CAM elements 106 is used to generate a 6-bit address corresponding to a matching CAM element record.
- Each CAM element 106 is additionally coupled to OR gate 110 to generate a match indication signal indicating whether or not a matching record was identified.
- Each 6-bit CAM element address generated is then applied, along with a cache set index (e.g., bits 8 : 5 of the received data processing system memory address), to multiplexer 108 .
- Multiplexer 108 outputs a selected input 6-bit address specified by the received cache set index.
- the output of multiplexer 108 is then combined/concatenated with the cache set index to form a 10-bit cache memory element address as shown.
- the generated 10-bit cache memory element address is then used to address or identify a 256-bit line or “block” within cache memory element 102 .
- FIG. 2 illustrates a conventional RAM-based emulated CAM.
- the “virtual” or emulated CAM element 200 depicted in FIG. 2 may be substituted for one of the individual CAM elements 106 A- 106 N illustrated in FIG. 1 and its functionality, as applied for performing a portion of a cache tag lookup operation, will be described with respect to data processing system processor cache 100 of that figure. As represented in FIG.
- emulated CAM 200 includes RAM elements 202 A, 202 B, and 202 C coupled with combinatorial (e.g., AND gate 204 and OR gate 206 ) and encoding (e.g., encoder 208 ) logic as shown.
- combinatorial e.g., AND gate 204 and OR gate 206
- encoding e.g., encoder 208
- Each RAM element 202 may be viewed as a 2-dimensional array of bits including rows corresponding to each of the 64 ways a cache memory element being accessed. Each row stores match reference data for a portion of a cache tag associated with the row's way and represented as a vector of bits. Accordingly, a 7-bit cache tag portion is represented using a 2 7 -bit vector “one-hot” encoded to indicate, using a single bit value, which of the 128 possible cache tag portion permutations is stored within that row/way. Similarly, an 8-bit cache tag portion is represented using a 2 8 or 256-bit vector.
- emulated CAM 200 and associated RAM elements 202 , are utilized to perform a “split” lookup function in which separate portions (e.g., a 7-bit portion and 2 8-bit portions) of a cache tag are each used to address a corresponding one of RAM elements 202 .
- a match of a complete cache tag is indicated if each portion of the cache tag matches in the same way of each of RAM elements 202 and consequently of the cache.
- illustration of a write port (and a corresponding description of a write operation) has been omitted from emulated CAM 200 of FIG. 2 .
- Each cache tag portion identifies or addresses a single bit position which may be viewed a column within an accessed RAM element including a bit from each way.
- All 64 bits of that bit position or column are then output to determine whether the provided or input partial cache tag value matched reference partial cache tag data within the corresponding portion of the emulated CAM (indicated, for example, by a single bit having a logical “1” value).
- the 64-bit outputs of each of RAM elements 202 A- 202 C are then logically combined or “joined” via a bitwise AND operation using AND gate 204 .
- the combined 64-bit output is then used to generate a 6-bit match address corresponding to a matching location within emulated CAM element 200 (e.g., using encoder 208 ) and to generate a match indication signal indicating whether or not a matching record was identified (e.g., using OR gate 206 ) as shown. If none of the bits of the bitwise-coalesced RAM element output is set to a logical “1” value, a determination may be made that the complete 23-bit input cache tag failed to match an emulated CAM element entry for a single way of an associated cache memory element.
- the 6-bit match address and match indication signal may be applied, along with a CAM selection (e.g., cache set) index, to a multiplexer (e.g., multiplexer 108 of FIG. 1 ) and used to address a cache memory element as previously described herein.
- a CAM selection e.g., cache set
- multiplexer e.g., multiplexer 108 of FIG. 1
- RAM element-based CAM emulation may be utilized in some circumstances where traditional CAMs may not, providing greater flexibility and cost-effectiveness
- one significant problem associated with such CAM emulation techniques is the quantity of RAM memory required for implementation. This problem, although more prominent where emulation of a CAM element is embodied in a single RAM, is evident even where emulation is distributed across multiple RAM elements as depicted in FIG. 2 . For example, for a 32 kilobyte cache as described herein, at least 640 kilobits of RAM storage (approximately 40 kilobits of RAM storage for each of a total of 16 emulated CAM elements) is required.
- Additional RAM storage may also be required as a buffer for match reference data used to update or modify match reference data within the emulated CAM's RAM elements.
- logic used in conventional CAM-based implementations e.g., a multiplexer and an additional OR gate used to generate a global match indication signal
- this quantity of memory and logic is unacceptable and consequently a cache may be omitted or implemented in a less-than-optimal way.
- a method and system are provided for emulating content-addressable memory (CAM) primitives (e.g., a read operation).
- a method is provided for emulating a read operation on a plurality of CAM elements using a read input including match input data and a CAM element selection index.
- match reference data is distributed among a plurality of random-access memory (RAM) elements by storing match reference data corresponding to each of the plurality of CAM elements within a first RAM element of the plurality.
- RAM random-access memory
- a first record is identified within the first RAM element using a first portion of the match input data and the CAM element selection index.
- a read operation result is then generated using the first record.
- FIG. 1 illustrates a data processing system processor cache including a conventional content-addressable memory-based cache tag lookup element
- FIG. 2 illustrates a conventional RAM-based emulated CAM element
- FIG. 3 illustrates a RAM-based emulated CAM element according to an embodiment of the present invention.
- FIG. 4 illustrates a flow diagram of a process for emulating a read operation on a plurality of content-addressable memory elements according to an embodiment of the present invention.
- references within the present description to “embodiments”, “one embodiment” or “an embodiment” are intended to indicate that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention.
- the appearance of the phrase “in one embodiment” in various places within the specification are not necessarily all referring to the same embodiment, nor are separate or alternative embodiments mutually exclusive.
- various features are described which may be exhibited by some embodiments and not by others.
- various requirements may be described which are requirements for some embodiments but not other embodiments.
- Embodiments of the present invention provide a method and system for emulating content-addressable memory (CAM) primitives (e.g., a read operation) within a data processing system.
- a method is provided for emulating a read operation on a plurality of content-addressable memory elements utilizing a read input (e.g., a data processing system memory address), where the read input includes match input data (e.g., a cache tag) and a content-addressable memory element selection index (e.g., a cache set index).
- a read input e.g., a data processing system memory address
- match input data e.g., a cache tag
- a content-addressable memory element selection index e.g., a cache set index
- match reference data corresponding to each of the emulated CAM elements is distributed among a number of RAM elements.
- the match reference data is stored as portions of “one-hot” encoded match reference data within at least a first and second RAM element of the RAM elements used for CAM emulation. Records within the first and second RAM elements are then identified using first and second portions of the read input, respectively, along with the CAM element selection index. The identified records are then used to generate an emulated CAM read operation result (e.g., a match indication signal and/or match address).
- FIG. 3 illustrates a RAM-based emulated CAM element according to an embodiment of the present invention.
- the “virtual” or emulated CAM element 300 depicted in FIG. 3 may be used in place of a plurality of traditional CAM elements rather than a single CAM element of a group.
- the functionality of emulated CAM element 300 as applied for performing a portion of a cache tag lookup operation, will now be described with respect to data processing system processor cache 100 as illustrated in, and described with respect to, FIG. 1 .
- Emulated CAM 300 of the illustrated embodiment includes RAM elements 302 A- 302 F coupled with combinatorial (e.g., AND gate 304 and OR gate 306 ) and encoding (e.g., encoder 308 ) logic as shown.
- combinatorial e.g., AND gate 304 and OR gate 306
- encoding e.g., encoder 308
- Each RAM element 302 may be viewed as a 2-dimensional array of bits including rows corresponding to each of the 64 ways of a cache memory element being accessed. Unlike a conventional RAM-based emulated CAM element however, each RAM element row stores match reference data (e.g., a “one-hot” encoded bit vector) for a portion of a cache tag associated with the row's associated way as well as for each CAM of the plurality of CAMs being emulated. In other words, each row of RAM elements 302 stores match reference data corresponding to a portion of a cache tag and to all “sets” of a set-associative cache upon which emulated CAM element 300 is used to perform a cache tag lookup operation.
- match reference data e.g., a “one-hot” encoded bit vector
- emulated CAM 300 and associated RAM elements 302 are utilized to perform a “split” lookup function in which separate portions (e.g., a 3-bit portion and 5 4-bit portions) of a cache tag along with a cache set index are used to address each of RAM elements 302 .
- a match of a complete cache tag is indicated if each portion of the cache tag matches in the same way and set of the cache as determined using the match reference data distributed among RAM elements 302 .
- emulated CAM element 300 For purposes of clarity, illustration of a write port has been omitted from emulated CAM element 300 of FIG. 3 .
- a separate RAM element is used to store prior match reference data (e.g., in encoded form) which is used to identify specific bits to be cleared or “reset” prior to an update or write operation to the match reference data stored within RAM elements 302 .
- each cache tag portion is used, along with associated cache set index data, to identify or address a single bit position. Each bit position may be viewed as a column within an accessed RAM element including a bit from each way of an associated cache.
- All 64 bits of that bit position or column are then used to determine whether the provided or input partial cache tag value matched reference partial cache tag data within a specified set of a corresponding RAM element 302 of emulated CAM 300 (indicated, for example, by a single bit having a logical “1” value).
- the 64-bit outputs of each of RAM elements 302 A- 302 F are then logically combined or “joined” via a bitwise AND operation using AND gate 304 .
- the combined 64-bit output may then be used to generate a 6-bit match address corresponding to a matching emulated CAM element record (e.g., using encoder 308 ) and a match indication signal indicating whether or not a matching record was identified (e.g., using OR gate 306 ). If none of the bits of the bitwise-coalesced RAM element output is set to a logical “1” value, a determination may be made that the complete 23-bit input cache tag failed to match an emulated CAM element entry for a single way and set of an associated cache memory element.
- emulated CAM element 300 may be implemented using conventional RAM elements coupled with data processing system software or combinatorial logic. Such combinatorial logic may be implemented using discrete elements or a programmable logic device (PLD). In another embodiment, emulated CAM element 300 may be implemented using a PLD.
- emulated CAM 300 may comprise one or more field programmable gate array (FPGA) elements such as a VirtexTM-II FPGA device, provided by Xilinx Corporation of San Jose, Calif.
- FPGA field programmable gate array
- VirtexTM-II FPGA device provided by Xilinx Corporation of San Jose, Calif.
- individual RAM elements are depicted with respect to the described embodiments, a single RAM element partitioned into multiple storage areas may be similarly employed in alternative embodiments of the invention.
- FIG. 4 illustrates a flow diagram of a process for emulating a read operation on a plurality of content-addressable memory elements according to an embodiment of the present invention.
- the flow diagram depicted in FIG. 4 indicates a particular order of operation and a specific granularity of process operations, in alternative embodiments the illustrated order may be varied (e.g., process operations may be performed in another order or performed substantially in parallel with one another) and one or more of the process operations may be coalesced or fragmented. Similarly, additional process operations may be added or eliminated where necessary in alternative embodiments of the present invention.
- RAM elements used to emulate a plurality of CAM elements are first initialized with match reference data (process block 402 ).
- a read input e.g., a data processing system memory address or portion thereof
- such input data may be provided to a cache controller element or directly applied to an input or read port of an emulated CAM element (e.g., emulated CAM 300 of FIG. 3 ).
- the received read input is parsed to identify match input data (e.g., cache tag) portions and a CAM element selection (e.g., cache set) index (process block 406 ).
- each of a number of associated RAM elements is addressed using the CAM selection index along with a corresponding portion of the identified match input data (process block 408 ).
- the described addressing operation is performed by generating a RAM element address via a concatenation of a cache set index with a predetermined number of cache tag bits and applying the generated RAM element address to an associated RAM element read port.
- a RAM element record is identified containing match reference data for analysis to determine whether an applied portion of match input data “matches” (i.e., is consistent with) match reference data within that RAM element.
- an identified record may be viewed as a column of bits within a 2-dimensional array including rows corresponding to each way of a set associative cache and columns corresponding to a combination of a cache set index and a portion of a cache tag.
- An identified record is then output by each of the described RAM elements.
- the RAM element outputs e.g., “one-hot” encoded bit vectors
- process block 410 is next combined according to the illustrated process embodiment of FIG. 4 , for example, by performing a bit-wise logical AND operation.
- a match indication signal is generated by performing a logical addition or “OR” operation on the combined RAM element output.
- a match address is formed via encoding, for example, by converting the previously “one-hot” encoded data to a conventional binary representation.
- match reference data within the RAM elements comprising an associated emulated CAM may be updated (process block 414 ) if appropriate, before another read input is received for processing as shown.
- a match reference data update is performed by first erasing or “clearing” an existing “one-hot” encoded bit (identifying the storage location of the data being read) and subsequently writing or “setting” another bit within the RAM element as appropriate.
- data identifying previously-stored match reference data is stored within a separate RAM element and used to identify which bit position is to be cleared at the outset of a match reference data update operation.
- embodiments of the present invention are applicable to a number of CAM element-based search intensive functions.
- embodiments of the present invention may be used in conjunction with CAM-based data processing system processor translation lookaside buffers (TLBs), data compression systems, database accelerators, neural networks, and/or communications network elements (e.g., to perform network/Internet Protocol (IP) address translation).
- TLBs data processing system processor translation lookaside buffers
- IP Internet Protocol
- embodiments of the present invention may be used in conjunction with CAM-based data processing system processor translation lookaside buffers (TLBs), data compression systems, database accelerators, neural networks, and/or communications network elements (e.g., to perform network/Internet Protocol (IP) address translation).
- IP Internet Protocol
- a pool of available RAM elements may be selectively activated as needed to perform a particular CAM function with the sizes of the portions of match input data applied to each RAM element being adjusted accordingly on a dynamic basis.
- predetermined “header” data may be used to dynamically specify the size and locations of a CAM selection index and/or match input data.
Abstract
Description
- 1. Technical Field
- Embodiments of the present invention relate generally to data processing system memory resources and more particularly to a method and system for emulating content-addressable memory primitives.
- 2. Description of the Related Art
- In a conventional data processing system, data and instructions are stored within memory storage elements arranged in a hierarchical structure. In a typical hierarchical memory or storage structure, smaller, faster memory elements are located closer (in terms of physical structure) and more tightly coupled (communicatively) to processing elements (e.g., processors or processor cores) and store a subset of data and/or instructions stored in larger, slower memory elements (e.g., fixed or removable magnetic or optical disks, tape storage, or the like) elsewhere within or coupled to the data processing system. One type of memory element used frequently in data processing systems for so-called “main” or system memory is random-access memory.
- In a conventional random-access memory or “RAM” element, data values are stored in an array of addressed memory locations. To perform a read operation on a RAM element, an address (e.g., a data processing system memory address) is applied to the RAM element, causing data stored at the applied address to be accessed and presented by the RAM.
- In order to determine whether a particular data value is stored within a RAM element, an address-based data searching method is performed in which data values are sequentially read out from the RAM and compared with the searched-for data value. Specifically, a series of addresses are sequentially transmitted to an address port of the RAM, thereby causing data values to be read out from the memory locations addressed. A separate comparator element is then used to compare each of the output data values with the searched-for data value, generating a signal when a match occurs. When a large number of data values is to be searched or compared, such address-based search operations are very time consuming as only a single data value is typically processed each clock cycle.
- Another type of memory element used in data processing systems to perform data search or comparison operations is content-addressable memory. In a content-addressable memory (CAM) element, a data value may be searched by content, rather than address. In a conventional CAM, data values are stored such that each data value is assigned to a row or column of an array of storage cells. To determine whether a particular data value is stored in a CAM element, a content-based data match operation is performed in which a searched-for data value is simultaneously compared with all rows/columns containing the pre-loaded data values. When one or more of the pre-loaded data values matches the searched-for data value, a “match” signal is generated by the CAM element, along with an address indicating the storage location (i.e., row or column) of the matching pre-loaded data value.
- By simultaneously comparing searched-for data with several pre-loaded data values, a CAM element is able to perform comparison or matching operations involving several pre-loaded data values in a single clock cycle. Consequently, CAM elements significantly reduce the time needed to identify a specific data value within a large amount of data as compared with conventional RAM elements and are used frequently for search or pattern-matching-intensive applications.
FIG. 1 illustrates a data processing system processor cache as one such exemplary application. The depicted data processingsystem processor cache 100 includes acache memory element 102 coupled to a cachetag lookup element 104 as shown. For purposes of illustration, data processingsystem processor cache 100 will be described herein as a 32 kilobyte cache having 1024 lines of 32 bytes each and organized as a 64-way set associative cache including 16 sets. - Data processing
system processor cache 100, as depicted inFIG. 1 , is addressed using a 32 bit address with bits 0 through 4 (represented as bits 4:0) identifying a specific byte within a cache line,bits 5 through 8 (represented as bits 8:5) identifying a specific set of the 16 possible sets, and remaining bits (represented as bits 31:9) making up a “cache tag” which identifies a block-frame address in memory where the cache line resides. In a conventional cache memory, a cache tag is used to verify that the cache line addressed in fact stores the requested data. - Cache
tag lookup element 104 includes a number ofCAM elements multiplexer 108 and to a match indication signal generation element, (e.g., OR gate 110). In the illustrated data processingsystem processor cache 100, 16 CAM elements 106 (one for each cache “set”) are employed. In operation, a cache tag (e.g., bits 31:9 of a data processing system memory address) is applied simultaneously to each ofCAM elements 106A-106N. Each ofCAM elements 106A-106N in the illustrated processor cache is a 64×23 CAM including 64 23-bit registers (not illustrated) coupled to 64 comparators which are in turn coupled to encoding logic (not illustrated). Within the present description, the variable “N” is intended to indicate some positive integer value and need not indicate the same value consistently. - The encoding logic (not illustrated) of each of CAM elements 106 is used to generate a 6-bit address corresponding to a matching CAM element record. Each CAM element 106 is additionally coupled to
OR gate 110 to generate a match indication signal indicating whether or not a matching record was identified. Each 6-bit CAM element address generated is then applied, along with a cache set index (e.g., bits 8:5 of the received data processing system memory address), to multiplexer 108.Multiplexer 108 outputs a selected input 6-bit address specified by the received cache set index. The output ofmultiplexer 108 is then combined/concatenated with the cache set index to form a 10-bit cache memory element address as shown. The generated 10-bit cache memory element address is then used to address or identify a 256-bit line or “block” withincache memory element 102. - While CAM elements are well-suited for performing comparison operations such as those required by cache
tag lookup element 104, CAMs may not be implemented in some cases or may be prohibitively expensive in some cases where they may otherwise be used. One technique for providing basic CAM functionality or “primitives” is to emulate the operation of a CAM element using one or more RAM elements.FIG. 2 illustrates a conventional RAM-based emulated CAM. The “virtual” or emulatedCAM element 200 depicted inFIG. 2 may be substituted for one of theindividual CAM elements 106A-106N illustrated inFIG. 1 and its functionality, as applied for performing a portion of a cache tag lookup operation, will be described with respect to data processingsystem processor cache 100 of that figure. As represented inFIG. 2 , emulatedCAM 200 includesRAM elements gate 204 and OR gate 206) and encoding (e.g., encoder 208) logic as shown. - Each RAM element 202 may be viewed as a 2-dimensional array of bits including rows corresponding to each of the 64 ways a cache memory element being accessed. Each row stores match reference data for a portion of a cache tag associated with the row's way and represented as a vector of bits. Accordingly, a 7-bit cache tag portion is represented using a 27-bit vector “one-hot” encoded to indicate, using a single bit value, which of the 128 possible cache tag portion permutations is stored within that row/way. Similarly, an 8-bit cache tag portion is represented using a 28 or 256-bit vector.
- In operation, emulated
CAM 200, and associated RAM elements 202, are utilized to perform a “split” lookup function in which separate portions (e.g., a 7-bit portion and 2 8-bit portions) of a cache tag are each used to address a corresponding one of RAM elements 202. A match of a complete cache tag is indicated if each portion of the cache tag matches in the same way of each of RAM elements 202 and consequently of the cache. For purposes of clarity, illustration of a write port (and a corresponding description of a write operation) has been omitted from emulatedCAM 200 ofFIG. 2 . Each cache tag portion identifies or addresses a single bit position which may be viewed a column within an accessed RAM element including a bit from each way. All 64 bits of that bit position or column are then output to determine whether the provided or input partial cache tag value matched reference partial cache tag data within the corresponding portion of the emulated CAM (indicated, for example, by a single bit having a logical “1” value). - The 64-bit outputs of each of
RAM elements 202A-202C are then logically combined or “joined” via a bitwise AND operation usingAND gate 204. The combined 64-bit output is then used to generate a 6-bit match address corresponding to a matching location within emulated CAM element 200 (e.g., using encoder 208) and to generate a match indication signal indicating whether or not a matching record was identified (e.g., using OR gate 206) as shown. If none of the bits of the bitwise-coalesced RAM element output is set to a logical “1” value, a determination may be made that the complete 23-bit input cache tag failed to match an emulated CAM element entry for a single way of an associated cache memory element. Once the 6-bit match address and match indication signal have been generated, they may be applied, along with a CAM selection (e.g., cache set) index, to a multiplexer (e.g.,multiplexer 108 ofFIG. 1 ) and used to address a cache memory element as previously described herein. - While RAM element-based CAM emulation may be utilized in some circumstances where traditional CAMs may not, providing greater flexibility and cost-effectiveness, one significant problem associated with such CAM emulation techniques is the quantity of RAM memory required for implementation. This problem, although more prominent where emulation of a CAM element is embodied in a single RAM, is evident even where emulation is distributed across multiple RAM elements as depicted in
FIG. 2 . For example, for a 32 kilobyte cache as described herein, at least 640 kilobits of RAM storage (approximately 40 kilobits of RAM storage for each of a total of 16 emulated CAM elements) is required. - Additional RAM storage may also be required as a buffer for match reference data used to update or modify match reference data within the emulated CAM's RAM elements. Moreover, logic used in conventional CAM-based implementations (e.g., a multiplexer and an additional OR gate used to generate a global match indication signal) is not eliminated in such CAM-emulation systems. In some circumstances this quantity of memory and logic is unacceptable and consequently a cache may be omitted or implemented in a less-than-optimal way.
- A method and system are provided for emulating content-addressable memory (CAM) primitives (e.g., a read operation). According to one embodiment, a method is provided for emulating a read operation on a plurality of CAM elements using a read input including match input data and a CAM element selection index. In the described method, match reference data is distributed among a plurality of random-access memory (RAM) elements by storing match reference data corresponding to each of the plurality of CAM elements within a first RAM element of the plurality. Thereafter, a first record is identified within the first RAM element using a first portion of the match input data and the CAM element selection index. A read operation result is then generated using the first record.
- The foregoing is a summary and thus contains, by necessity, simplifications, generalizations and omissions of detail; consequently, those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. As will also be apparent to one of skill in the art, the operations disclosed herein may be implemented in a number of ways including implementation in hardware using a variety of techniques. For example, implementations of the present invention may be provided using application-specific integrated circuits (ASICs) or other special-purpose electronic circuits as well as programmable logic devices (PLDs) such as field-programmable gate arrays (FPGAs), programmable logic arrays (PLAs) or the like. Such changes and modifications may be made without departing from this invention and its broader aspects. Other aspects, inventive features, and advantages of the present invention, as defined solely by the claims, will become apparent in the non-limiting detailed description set forth below.
- The present invention may be better understood, and its numerous features and advantages made apparent to those skilled in the art by referencing the accompanying drawings in which:
-
FIG. 1 illustrates a data processing system processor cache including a conventional content-addressable memory-based cache tag lookup element; -
FIG. 2 illustrates a conventional RAM-based emulated CAM element; -
FIG. 3 illustrates a RAM-based emulated CAM element according to an embodiment of the present invention; and -
FIG. 4 illustrates a flow diagram of a process for emulating a read operation on a plurality of content-addressable memory elements according to an embodiment of the present invention. - The use of the same reference symbols in different drawings is intended to indicate similar or identical items.
- The following description and its accompanying figures sets forth embodiments for carrying out one or more method, systems, and/or devices of the present invention. This description is intended to be illustrative rather than restrictive and should not be taken to be limiting. More specifically, in the following detailed description, numerous specific details such as specific method orders, structures, elements, and connections have been set forth. It is to be understood however that these and other specific details need not be utilized to practice embodiments of the present invention. In other circumstances, well-known structures, elements, or connections have been omitted, or have not been described in particular detail in order to avoid unnecessarily obscuring the described invention embodiments.
- References within the present description to “embodiments”, “one embodiment” or “an embodiment” are intended to indicate that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. The appearance of the phrase “in one embodiment” in various places within the specification are not necessarily all referring to the same embodiment, nor are separate or alternative embodiments mutually exclusive. Moreover, various features are described which may be exhibited by some embodiments and not by others. Similarly, various requirements may be described which are requirements for some embodiments but not other embodiments.
- Embodiments of the present invention provide a method and system for emulating content-addressable memory (CAM) primitives (e.g., a read operation) within a data processing system. According to one embodiment, a method is provided for emulating a read operation on a plurality of content-addressable memory elements utilizing a read input (e.g., a data processing system memory address), where the read input includes match input data (e.g., a cache tag) and a content-addressable memory element selection index (e.g., a cache set index).
- In the described method embodiment, match reference data corresponding to each of the emulated CAM elements is distributed among a number of RAM elements. The match reference data is stored as portions of “one-hot” encoded match reference data within at least a first and second RAM element of the RAM elements used for CAM emulation. Records within the first and second RAM elements are then identified using first and second portions of the read input, respectively, along with the CAM element selection index. The identified records are then used to generate an emulated CAM read operation result (e.g., a match indication signal and/or match address).
-
FIG. 3 illustrates a RAM-based emulated CAM element according to an embodiment of the present invention. As distinguished from conventional RAM-based CAM emulation systems, the “virtual” or emulated CAM element 300 depicted inFIG. 3 may be used in place of a plurality of traditional CAM elements rather than a single CAM element of a group. The functionality of emulated CAM element 300, as applied for performing a portion of a cache tag lookup operation, will now be described with respect to data processingsystem processor cache 100 as illustrated in, and described with respect to,FIG. 1 . Emulated CAM 300 of the illustrated embodiment includesRAM elements 302A-302F coupled with combinatorial (e.g., ANDgate 304 and OR gate 306) and encoding (e.g., encoder 308) logic as shown. - Each RAM element 302 may be viewed as a 2-dimensional array of bits including rows corresponding to each of the 64 ways of a cache memory element being accessed. Unlike a conventional RAM-based emulated CAM element however, each RAM element row stores match reference data (e.g., a “one-hot” encoded bit vector) for a portion of a cache tag associated with the row's associated way as well as for each CAM of the plurality of CAMs being emulated. In other words, each row of RAM elements 302 stores match reference data corresponding to a portion of a cache tag and to all “sets” of a set-associative cache upon which emulated CAM element 300 is used to perform a cache tag lookup operation.
- By exploiting the set associativity of a cache and storing match reference data corresponding to each cache “set” within each RAM element used, the size of each RAM element 302 is increased but the total number of RAM elements 302 used is decreased as compared to conventional multiple-CAM element systems. Accordingly, a smaller number of bits of total storage capacity is required. In operation, emulated CAM 300 and associated RAM elements 302 are utilized to perform a “split” lookup function in which separate portions (e.g., a 3-bit portion and 5 4-bit portions) of a cache tag along with a cache set index are used to address each of RAM elements 302. A match of a complete cache tag is indicated if each portion of the cache tag matches in the same way and set of the cache as determined using the match reference data distributed among RAM elements 302.
- For purposes of clarity, illustration of a write port has been omitted from emulated CAM element 300 of
FIG. 3 . In one embodiment of the present invention however, a separate RAM element is used to store prior match reference data (e.g., in encoded form) which is used to identify specific bits to be cleared or “reset” prior to an update or write operation to the match reference data stored within RAM elements 302. In emulated CAM element 300, each cache tag portion is used, along with associated cache set index data, to identify or address a single bit position. Each bit position may be viewed as a column within an accessed RAM element including a bit from each way of an associated cache. All 64 bits of that bit position or column are then used to determine whether the provided or input partial cache tag value matched reference partial cache tag data within a specified set of a corresponding RAM element 302 of emulated CAM 300 (indicated, for example, by a single bit having a logical “1” value). - The 64-bit outputs of each of
RAM elements 302A-302F are then logically combined or “joined” via a bitwise AND operation using ANDgate 304. The combined 64-bit output may then be used to generate a 6-bit match address corresponding to a matching emulated CAM element record (e.g., using encoder 308) and a match indication signal indicating whether or not a matching record was identified (e.g., using OR gate 306). If none of the bits of the bitwise-coalesced RAM element output is set to a logical “1” value, a determination may be made that the complete 23-bit input cache tag failed to match an emulated CAM element entry for a single way and set of an associated cache memory element. - According to one embodiment of the present invention, emulated CAM element 300 may be implemented using conventional RAM elements coupled with data processing system software or combinatorial logic. Such combinatorial logic may be implemented using discrete elements or a programmable logic device (PLD). In another embodiment, emulated CAM element 300 may be implemented using a PLD. For example, emulated CAM 300 may comprise one or more field programmable gate array (FPGA) elements such as a Virtex™-II FPGA device, provided by Xilinx Corporation of San Jose, Calif. Moreover, although individual RAM elements are depicted with respect to the described embodiments, a single RAM element partitioned into multiple storage areas may be similarly employed in alternative embodiments of the invention.
-
FIG. 4 illustrates a flow diagram of a process for emulating a read operation on a plurality of content-addressable memory elements according to an embodiment of the present invention. Although the flow diagram depicted inFIG. 4 indicates a particular order of operation and a specific granularity of process operations, in alternative embodiments the illustrated order may be varied (e.g., process operations may be performed in another order or performed substantially in parallel with one another) and one or more of the process operations may be coalesced or fragmented. Similarly, additional process operations may be added or eliminated where necessary in alternative embodiments of the present invention. - In the illustrated process embodiment, RAM elements used to emulate a plurality of CAM elements are first initialized with match reference data (process block 402). Once initialization is complete, a read input (e.g., a data processing system memory address or portion thereof) is received (process block 404). According to alternative embodiments of the present invention, such input data may be provided to a cache controller element or directly applied to an input or read port of an emulated CAM element (e.g., emulated CAM 300 of
FIG. 3 ). Thereafter, the received read input is parsed to identify match input data (e.g., cache tag) portions and a CAM element selection (e.g., cache set) index (process block 406). - Once the received read input has been parsed, each of a number of associated RAM elements is addressed using the CAM selection index along with a corresponding portion of the identified match input data (process block 408). According to one embodiment, the described addressing operation is performed by generating a RAM element address via a concatenation of a cache set index with a predetermined number of cache tag bits and applying the generated RAM element address to an associated RAM element read port. As each RAM element is addressed, a RAM element record is identified containing match reference data for analysis to determine whether an applied portion of match input data “matches” (i.e., is consistent with) match reference data within that RAM element.
- According to one embodiment of the present invention, an identified record may be viewed as a column of bits within a 2-dimensional array including rows corresponding to each way of a set associative cache and columns corresponding to a combination of a cache set index and a portion of a cache tag. An identified record is then output by each of the described RAM elements. The RAM element outputs (e.g., “one-hot” encoded bit vectors) are next combined (process block 410) according to the illustrated process embodiment of
FIG. 4 , for example, by performing a bit-wise logical AND operation. - Thereafter, the combined RAM element outputs are utilized to generate a match indication signal and, if a match is determined to have occurred for all portions of the received match input data, a match address specifying a location at which the matching data is stored within a CAM element being emulated (process block 412). In one embodiment, a match indication signal is generated by performing a logical addition or “OR” operation on the combined RAM element output. Similarly, in the described embodiment, a match address is formed via encoding, for example, by converting the previously “one-hot” encoded data to a conventional binary representation.
- Once the match address and match indication signal have been generated for a received read input, match reference data within the RAM elements comprising an associated emulated CAM may be updated (process block 414) if appropriate, before another read input is received for processing as shown. In one embodiment of the present invention, a match reference data update is performed by first erasing or “clearing” an existing “one-hot” encoded bit (identifying the storage location of the data being read) and subsequently writing or “setting” another bit within the RAM element as appropriate. In another embodiment, data identifying previously-stored match reference data is stored within a separate RAM element and used to identify which bit position is to be cleared at the outset of a match reference data update operation.
- While described primarily herein with respect to operation with a data processing system processor cache (e.g., a cache tag lookup operation) embodiments of the present invention are applicable to a number of CAM element-based search intensive functions. For example, embodiments of the present invention may be used in conjunction with CAM-based data processing system processor translation lookaside buffers (TLBs), data compression systems, database accelerators, neural networks, and/or communications network elements (e.g., to perform network/Internet Protocol (IP) address translation). Similarly, while fixed or predetermined read input segments or portions have been illustrated and described herein, such elements (e.g., match input data portions and/or CAM element selection indices) may be dynamically determined or identified. For example, in one embodiment, a pool of available RAM elements may be selectively activated as needed to perform a particular CAM function with the sizes of the portions of match input data applied to each RAM element being adjusted accordingly on a dynamic basis. In another exemplary embodiment, predetermined “header” data may be used to dynamically specify the size and locations of a CAM selection index and/or match input data.
- While particular embodiments of the present invention have been shown and described, it will be obvious to those skilled in the art that, based upon the teachings herein, changes and modifications may be made without departing from this invention and its broader aspects and, therefore, the appended claims are to encompass within their scope all such changes and modifications as are within the true spirit and scope of this invention.
- Consequently, the invention is intended to be limited only by the scope of the appended claims, giving full cognizance to equivalents in all respects.
Claims (15)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US11/083,209 US7380053B2 (en) | 2005-03-17 | 2005-03-17 | Method and system for emulating content-addressable memory primitives |
US12/038,009 US20080276039A1 (en) | 2005-03-17 | 2008-02-27 | Method and System for Emulating Content-Addressable Memory Primitives |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US11/083,209 US7380053B2 (en) | 2005-03-17 | 2005-03-17 | Method and system for emulating content-addressable memory primitives |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/038,009 Continuation US20080276039A1 (en) | 2005-03-17 | 2008-02-27 | Method and System for Emulating Content-Addressable Memory Primitives |
Publications (2)
Publication Number | Publication Date |
---|---|
US20060212648A1 true US20060212648A1 (en) | 2006-09-21 |
US7380053B2 US7380053B2 (en) | 2008-05-27 |
Family
ID=37011713
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US11/083,209 Expired - Fee Related US7380053B2 (en) | 2005-03-17 | 2005-03-17 | Method and system for emulating content-addressable memory primitives |
US12/038,009 Abandoned US20080276039A1 (en) | 2005-03-17 | 2008-02-27 | Method and System for Emulating Content-Addressable Memory Primitives |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/038,009 Abandoned US20080276039A1 (en) | 2005-03-17 | 2008-02-27 | Method and System for Emulating Content-Addressable Memory Primitives |
Country Status (1)
Country | Link |
---|---|
US (2) | US7380053B2 (en) |
Cited By (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090198910A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Ravi K | Data processing system, processor and method that support a touch of a partial cache line of data |
US20090198911A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Lakshminarayana B | Data processing system, processor and method for claiming coherency ownership of a partial cache line of data |
US20090198912A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Lakshminarayana B | Data processing system, processor and method for implementing cache management for partial cache line operations |
US20090198865A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Ravi K | Data processing system, processor and method that perform a partial cache line storage-modifying operation based upon a hint |
US20090198903A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Ravi K | Data processing system, processor and method that vary an amount of data retrieved from memory based upon a hint |
US20090198914A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Lakshminarayana B | Data processing system, processor and method in which an interconnect operation indicates acceptability of partial data delivery |
US20100100672A1 (en) * | 2007-07-03 | 2010-04-22 | Fujitsu Limited | Relay apparatus and data control method |
US20100268885A1 (en) * | 2009-04-16 | 2010-10-21 | International Business Machines Corporation | Specifying an access hint for prefetching limited use data in a cache hierarchy |
US20100268884A1 (en) * | 2009-04-15 | 2010-10-21 | International Business Machines Corporation | Updating Partial Cache Lines in a Data Processing System |
US20100268886A1 (en) * | 2009-04-16 | 2010-10-21 | International Buisness Machines Corporation | Specifying an access hint for prefetching partial cache block data in a cache hierarchy |
US20100293339A1 (en) * | 2008-02-01 | 2010-11-18 | Arimilli Ravi K | Data processing system, processor and method for varying a data prefetch size based upon data usage |
US20110161595A1 (en) * | 2009-12-26 | 2011-06-30 | Zhen Fang | Cache memory power reduction techniques |
US8250307B2 (en) | 2008-02-01 | 2012-08-21 | International Business Machines Corporation | Sourcing differing amounts of prefetch data in response to data prefetch requests |
WO2013120516A1 (en) * | 2012-02-15 | 2013-08-22 | Siemens Aktiengesellschaft | Field-programmable logic gate arrangement |
US20140278329A1 (en) * | 2013-03-15 | 2014-09-18 | Mentor Graphics Corporation | Modeling Content-Addressable Memory For Emulation |
US20150012695A1 (en) * | 2013-07-08 | 2015-01-08 | Hewlett-Packard Development Company, L.P. | Apparatus and method for multi-mode storage |
CN109542799A (en) * | 2018-11-05 | 2019-03-29 | 西安智多晶微电子有限公司 | Block storage joining method, splicing module, storage device and field programmable gate array |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP5578566B2 (en) * | 2010-12-08 | 2014-08-27 | 株式会社ワコム | Indicator detection apparatus and indicator detection method |
US9916086B2 (en) | 2013-08-31 | 2018-03-13 | Empire Technology Development Llc | Content-addressable memory device |
KR101787877B1 (en) | 2016-03-15 | 2017-11-15 | 한양대학교 에리카산학협력단 | Method and apparatus and for emulating match lines and comparison cells architecture in CAM using RAM hardware |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6344989B1 (en) * | 1998-06-26 | 2002-02-05 | Altera Corporation | Programmable logic devices with improved content addressable memory capabilities |
US6351143B1 (en) * | 2000-05-01 | 2002-02-26 | Xilinx, Inc. | Content-addressable memory implemented using programmable logic |
US6353332B1 (en) * | 2000-02-07 | 2002-03-05 | Xilinx, Inc. | Methods for implementing CAM functions using dual-port RAM |
US6754766B1 (en) * | 2002-02-14 | 2004-06-22 | Altera Corporation | Emulation of content-addressable memories |
US6781857B1 (en) * | 2002-02-27 | 2004-08-24 | Integrated Device Technology, Inc. | Content addressable memory (CAM) devices that utilize multi-port CAM cells and control logic to support multiple overlapping search cycles that are asynchronously timed relative to each other |
-
2005
- 2005-03-17 US US11/083,209 patent/US7380053B2/en not_active Expired - Fee Related
-
2008
- 2008-02-27 US US12/038,009 patent/US20080276039A1/en not_active Abandoned
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6344989B1 (en) * | 1998-06-26 | 2002-02-05 | Altera Corporation | Programmable logic devices with improved content addressable memory capabilities |
US6353332B1 (en) * | 2000-02-07 | 2002-03-05 | Xilinx, Inc. | Methods for implementing CAM functions using dual-port RAM |
US6351143B1 (en) * | 2000-05-01 | 2002-02-26 | Xilinx, Inc. | Content-addressable memory implemented using programmable logic |
US6754766B1 (en) * | 2002-02-14 | 2004-06-22 | Altera Corporation | Emulation of content-addressable memories |
US6781857B1 (en) * | 2002-02-27 | 2004-08-24 | Integrated Device Technology, Inc. | Content addressable memory (CAM) devices that utilize multi-port CAM cells and control logic to support multiple overlapping search cycles that are asynchronously timed relative to each other |
Cited By (29)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100100672A1 (en) * | 2007-07-03 | 2010-04-22 | Fujitsu Limited | Relay apparatus and data control method |
US8244973B2 (en) * | 2007-07-03 | 2012-08-14 | Fujitsu Limited | Relay apparatus and data control method |
US8266381B2 (en) | 2008-02-01 | 2012-09-11 | International Business Machines Corporation | Varying an amount of data retrieved from memory based upon an instruction hint |
US20090198865A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Ravi K | Data processing system, processor and method that perform a partial cache line storage-modifying operation based upon a hint |
US8117401B2 (en) | 2008-02-01 | 2012-02-14 | International Business Machines Corporation | Interconnect operation indicating acceptability of partial data delivery |
US20090198914A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Lakshminarayana B | Data processing system, processor and method in which an interconnect operation indicates acceptability of partial data delivery |
US20090198912A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Lakshminarayana B | Data processing system, processor and method for implementing cache management for partial cache line operations |
US8140771B2 (en) | 2008-02-01 | 2012-03-20 | International Business Machines Corporation | Partial cache line storage-modifying operation based upon a hint |
US8250307B2 (en) | 2008-02-01 | 2012-08-21 | International Business Machines Corporation | Sourcing differing amounts of prefetch data in response to data prefetch requests |
US20090198911A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Lakshminarayana B | Data processing system, processor and method for claiming coherency ownership of a partial cache line of data |
US20100293339A1 (en) * | 2008-02-01 | 2010-11-18 | Arimilli Ravi K | Data processing system, processor and method for varying a data prefetch size based upon data usage |
US20090198910A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Ravi K | Data processing system, processor and method that support a touch of a partial cache line of data |
US8108619B2 (en) | 2008-02-01 | 2012-01-31 | International Business Machines Corporation | Cache management for partial cache line operations |
US8595443B2 (en) | 2008-02-01 | 2013-11-26 | International Business Machines Corporation | Varying a data prefetch size based upon data usage |
US20090198903A1 (en) * | 2008-02-01 | 2009-08-06 | Arimilli Ravi K | Data processing system, processor and method that vary an amount of data retrieved from memory based upon a hint |
US8255635B2 (en) | 2008-02-01 | 2012-08-28 | International Business Machines Corporation | Claiming coherency ownership of a partial cache line of data |
US8117390B2 (en) | 2009-04-15 | 2012-02-14 | International Business Machines Corporation | Updating partial cache lines in a data processing system |
US20100268884A1 (en) * | 2009-04-15 | 2010-10-21 | International Business Machines Corporation | Updating Partial Cache Lines in a Data Processing System |
US20100268886A1 (en) * | 2009-04-16 | 2010-10-21 | International Buisness Machines Corporation | Specifying an access hint for prefetching partial cache block data in a cache hierarchy |
US8176254B2 (en) | 2009-04-16 | 2012-05-08 | International Business Machines Corporation | Specifying an access hint for prefetching limited use data in a cache hierarchy |
US8140759B2 (en) | 2009-04-16 | 2012-03-20 | International Business Machines Corporation | Specifying an access hint for prefetching partial cache block data in a cache hierarchy |
US20100268885A1 (en) * | 2009-04-16 | 2010-10-21 | International Business Machines Corporation | Specifying an access hint for prefetching limited use data in a cache hierarchy |
US20110161595A1 (en) * | 2009-12-26 | 2011-06-30 | Zhen Fang | Cache memory power reduction techniques |
US8631207B2 (en) * | 2009-12-26 | 2014-01-14 | Intel Corporation | Cache memory power reduction techniques |
WO2013120516A1 (en) * | 2012-02-15 | 2013-08-22 | Siemens Aktiengesellschaft | Field-programmable logic gate arrangement |
US20140278329A1 (en) * | 2013-03-15 | 2014-09-18 | Mentor Graphics Corporation | Modeling Content-Addressable Memory For Emulation |
US20150012695A1 (en) * | 2013-07-08 | 2015-01-08 | Hewlett-Packard Development Company, L.P. | Apparatus and method for multi-mode storage |
US9165088B2 (en) * | 2013-07-08 | 2015-10-20 | Hewlett-Packard Development Company, L.P. | Apparatus and method for multi-mode storage |
CN109542799A (en) * | 2018-11-05 | 2019-03-29 | 西安智多晶微电子有限公司 | Block storage joining method, splicing module, storage device and field programmable gate array |
Also Published As
Publication number | Publication date |
---|---|
US20080276039A1 (en) | 2008-11-06 |
US7380053B2 (en) | 2008-05-27 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US7380053B2 (en) | Method and system for emulating content-addressable memory primitives | |
JP3169155B2 (en) | Circuit for caching information | |
US9563658B2 (en) | Hardware implementation of the aggregation/group by operation: hash-table method | |
US9406381B2 (en) | TCAM search unit including a distributor TCAM and DRAM and a method for dividing a database of TCAM rules | |
US7213101B1 (en) | Classless interdomain routing using binary content addressable memory having mask bits and mask valid bits | |
EP0694845B1 (en) | Low-latency memory indexing method and structure | |
US8244691B1 (en) | Dictionary architecture and methodology for revision-tolerant data de-duplication | |
US7042748B2 (en) | Content addressable memory with cascaded array | |
US7986696B1 (en) | Method and apparatus for longest prefix matching | |
US7194574B2 (en) | Searching small entities in a wide CAM | |
JPH11203199A (en) | Cache memory | |
US4821171A (en) | System of selective purging of address translation in computer memories | |
US10191839B2 (en) | Search device includes associative memory, search data generating unit for generating search information based on hit information and a search key generating unit generating search keys based on search information and the search data | |
US7814267B1 (en) | Processor with compare operations based on any of multiple compare data segments | |
US8880845B2 (en) | Memory system and operating method thereof | |
US6865590B2 (en) | Three input variable subfield comparation for fast matching | |
US10032516B2 (en) | Duo content addressable memory (CAM) using a single CAM | |
US20040143706A1 (en) | Implementation of an LRU and MRU algorithm in a partitioned cache | |
US11100267B1 (en) | Multi dimensional memory compression using bytewide write enable | |
US5721863A (en) | Method and structure for accessing semi-associative cache memory using multiple memories to store different components of the address | |
US8117384B2 (en) | Searching a content addressable memory with modifiable comparands | |
US7577784B1 (en) | Full-ternary content addressable memory (CAM) configurable for pseudo-ternary operation | |
US20050008151A1 (en) | Processor device and method for data protection by means of data block scrambling | |
US10572440B2 (en) | High operation frequency, area efficient and cost effective content addressable memory architecture | |
JPH0614325B2 (en) | Replacement method |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: INTERNATIONAL BUSINESS MACHINES CORPORATION, NEW YFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:COX, CHARLES EDWIN;REAVES, JIMMY LEE;REEL/FRAME:016015/0183Effective date: 20050308 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:INTERNATIONAL BUSINESS MACHINES CORPORATION;REEL/FRAME:026894/0001Effective date: 20110817 |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0610Effective date: 20170929 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Expired due to failure to pay maintenance fee |
Effective date: 20200527 |