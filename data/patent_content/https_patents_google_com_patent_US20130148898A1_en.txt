US20130148898A1 - Clustering objects detected in video - Google Patents
Clustering objects detected in video Download PDFInfo
- Publication number
- US20130148898A1 US20130148898A1 US13/706,371 US201213706371A US2013148898A1 US 20130148898 A1 US20130148898 A1 US 20130148898A1 US 201213706371 A US201213706371 A US 201213706371A US 2013148898 A1 US2013148898 A1 US 2013148898A1
- Authority
- US
- United States
- Prior art keywords
- images
- facial
- frame
- frames
- facial images
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
- 230000001815 facial effect Effects 0.000 claims abstract description 373
- 238000000034 method Methods 0.000 claims abstract description 84
- 230000008569 process Effects 0.000 claims abstract description 33
- 238000012545 processing Methods 0.000 claims description 50
- 210000000887 face Anatomy 0.000 claims description 36
- 238000000605 extraction Methods 0.000 claims description 31
- 230000008859 change Effects 0.000 claims description 6
- 238000002372 labelling Methods 0.000 claims description 3
- 210000005069 ears Anatomy 0.000 claims description 2
- 238000005070 sampling Methods 0.000 claims 1
- 238000001914 filtration Methods 0.000 abstract 1
- 239000000872 buffer Substances 0.000 description 20
- 238000010586 diagram Methods 0.000 description 18
- 230000015654 memory Effects 0.000 description 11
- 238000004364 calculation method Methods 0.000 description 7
- 238000001514 detection method Methods 0.000 description 7
- 238000000638 solvent extraction Methods 0.000 description 7
- 238000004422 calculation algorithm Methods 0.000 description 6
- 238000012015 optical character recognition Methods 0.000 description 6
- 230000003139 buffering effect Effects 0.000 description 4
- 241001465754 Metazoa Species 0.000 description 3
- 238000004458 analytical method Methods 0.000 description 3
- 230000005291 magnetic effect Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000010606 normalization Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000013441 quality evaluation Methods 0.000 description 3
- 230000009471 action Effects 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000033001 locomotion Effects 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- PXFBZOLANLWPMH-UHFFFAOYSA-N 16-Epiaffinine Natural products C1C(C2=CC=CC=C2N2)=C2C(=O)CC2C(=CC)CN(C)C1C2CO PXFBZOLANLWPMH-UHFFFAOYSA-N 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000015572 biosynthetic process Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 230000008676 import Effects 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 210000001747 pupil Anatomy 0.000 description 1
- 230000003252 repetitive effect Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000000126 substance Substances 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/762—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using clustering, e.g. of similar faces in social networks
-
- G06K9/62—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/172—Classification, e.g. identification
- G06V40/173—Classification, e.g. identification face re-identification, e.g. recognising unknown faces across different face tracks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7837—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content
- G06F16/784—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content the detected or recognised objects being people
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/764—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using classification, e.g. of video objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
- G06V40/167—Detection; Localisation; Normalisation using comparisons between temporally consecutive images
Definitions
- the disclosure relates generally to the field of video processing and more specifically to detecting, tracking and clustering objects appearing in video.
- Object and facial recognition techniques may be used by media content providers in order to properly detect and identify faces and objects.
- FIG. 1 is a block diagram illustrating a system environment for object detection and recognition and database population of objects for video indexing, in accordance with an embodiment.
- FIG. 2 is a block diagram showing various components of a media processing system, in accordance with an embodiment.
- FIG. 3A is a block diagram of an environment within which a facial image clustering module is implemented, in accordance with an embodiment.
- FIG. 3B is a block diagram showing data flow and correlations between various components of a media processing system for implementing clustering, in accordance with an embodiment.
- FIG. 4 is a block diagram showing various components of a facial image extraction module, in accordance with an embodiment.
- FIG. 5 is a flow chart of a method for video processing involving facial image extraction and initial clustering, in accordance with an embodiment.
- FIG. 6 illustrates a clusterizer track, in accordance with an embodiment.
- FIG. 7 illustrates an example of merging clusters, in accordance with an embodiment.
- FIG. 8 is a block diagram showing various components of a facial image clustering module, in accordance with an embodiment.
- FIG. 9A is flow diagram illustrating a method for frame buffering, in accordance with an embodiment.
- FIG. 9B is flow diagram illustrating a method for clusterized track processing, in accordance with an embodiment.
- FIG. 9C is flow diagram illustrating a method for face quality evaluation, face collapsing, and cluster merging, in accordance with an embodiment.
- FIG. 9D is flow diagram illustrating a method for facial image identity suggestion, in accordance with an embodiment.
- FIG. 10 is a diagram representation of a computing device capable of performing the clustering of objects in media content.
- a system is configured for recognition and identification of objects in videos.
- the system is configured to accomplish this through clustering and identifying objects in videos.
- the type of objects may include cars, persons, animals, plants, etc., with identifiable features.
- Clusters can also be broken down further into more specific clusters that may identify different people, brands of cars, types of animals, etc.
- each cluster contains images of a certain type of object, based on some common property within the cluster.
- Objects may be unique compared to other objects within an initial cluster, and thus can be furthered categorized or clustered according to their differences. For example, a specific person is unique compared to other people.
- While video objects containing any person may be clustered under a “people” identifier label, images containing a specific person may be identified by distinguishable features (e.g., face, shape, color, height, etc.) and clustered under a more specific identifier. However, there may be more than one cluster created per one person because a threshold level or other settings determine the creation of another cluster associated with the same individual. In an embodiment, further calculations may be performed to determine if facial images from the two clusters belong to the same person. Depending on the results, the clusters may be merged or kept separate.
- Comparisons may be triaged such that less computationally expensive comparisons are performed and determinations (e.g., according to the degree of similarity between images) are made prior to performing more accurate or additional comparisons.
- these initial comparisons may be used to determine whether or not two images are of the same person.
- a set of images determined to likely be of the same person may form an initial cluster.
- further calculations may be used to determine an initial image to represent the clustered images or determine an identity of the cluster (e.g., the identity of the person).
- images from two clusters are determined to be of the same person, then these two clusters may be merged to form a single cluster for the person.
- the cluster data pertaining to the images may be stored in one or more databases and utilized to index objects and the videos in which they appear.
- the stored data may include, among other things, the name of the person associated with the facial images, the times or locations of appearances of the person in the video based on the determination of their facial image being present.
- inanimate objects may also be considered for identification and clustering.
- data stored for inanimate objects may include different types of cars. These cars may be clustered and identified through their different features such as headlights, rear, badge, etc., and associated with a specific model and/or brand.
- the data stored to the database may be utilized to search video clips for specific objects by keywords (e.g., a specific person's name, brand or model of a car, etc.).
- the data stored in clusters provide users with a better video viewing experience. For example, clustering objects allows users searching for a specific person in videos to determine the video clips along with the times and locations in the clips where the searched person appears, and also to navigate through the videos by appearances of the object.
- FIG. 1 a block diagram illustrates a system environment for object detection and recognition and database population of objects for video indexing, in accordance with an example embodiment.
- the environment 100 may comprise a digital media processing facility 110 , content provider 120 , user system 130 , and a network 105 .
- Network 105 may comprise any combination of local area and/or wide area networks, mobile, or terrestrial communication systems that allows for the transmission of data between digital media processing facility 110 , user system 130 and/or content provider 120 .
- the content provider 120 may comprise a store of published content 125 . While only one content provider 120 is shown, there may be multiple content providers 120 , each transmitting their own published content 125 over network 105 .
- Published content 125 may include digital media content, such as digital videos or video clips, that content provider 120 owns or has rights to.
- the published content 125 may include user content 135 uploaded to the content provider 120 (e.g., via a video sharing service).
- the content provider may be a news service agency that provides news reports to digital media broadcasters (not shown) or otherwise provides access to the news reports (e.g., via a website or streaming service).
- the news reports which may be in the form of videos or video clips, are the published contents 125 that are being distributed to other individuals or entities.
- the user system 130 may comprise of a store of user content 135 .
- There may be one or more user system 130 connected to network 105 in system environment 100 .
- a user system 130 may be a general purpose computer, a television set (TV), a personal digital assistant (PDA), a mobile telephone, a wireless device, or any other device capable of visual presentation of data acquired, stored, or transmitted in various forms.
- Each user system 130 may store its own user content 135 , which include media content stored on the user system 130 . For example, any pictures, movies, documents, and so forth stored on a user's hard drive may be considered as user content 135 .
- digital content stored in the “cloud” or a remote location may also be considered as user content 135 .
- Digital media processing facility 110 may further comprise a digital media processor 112 and a digital media search processor 114 .
- the digital media processing facility may represent fixed, mobile, or transportable structures, including any associated equipment, wiring, cabling, networks, and utilities, that provide housing to devices that have computing capability.
- Digital media from sources such as published content 125 from content provider 120 or user content 135 from user system 130 , may be sent over network 105 to digital media processing facility 110 for processing.
- the digital media processing facility may process received media content 125 , 135 to detect, identify, cluster and index recognizable objects or individuals in the media content. Additionally, the digital media processing facility 110 may enable searching of the indexed objects or individuals in the media content.
- Digital media search processor 114 may be any computing device (e.g., computer, laptop, mobile device, tablet and so forth) that is capable of performing a search through a store of digital contents. This may include searches through content available on network 105 for specific digital content or it may involve searches through content or indexes already present in digital media processing facility 110 . For example, digital media processing facility 110 may receive a request to search for instances when a specific individual appears in some set of digital media content (e.g., videos). Digital media search processor 114 runs the search through content and indexes available to it before returning a list of results.
- Digital media search processor 114 runs the search through content and indexes available to it before returning a list of results.
- the digital media processor 112 may be, but is not limited to, a general purpose processor for user in a personal or server computer, laptop, mobile device, tablet, or some other type of processor capable of receiving, processing, and distributing digital media content.
- the digital media processor 112 is capable of running processes on a digital media content store to detect, identify, cluster, and index objects that appear in the content store. This is only an example of what digital media processor 112 is capable of and other embodiments of digital media processor 112 may include more or less capabilities.
- facial images may be used to refer to the facial fronts of both animate and inanimate objects.
- digital media as videos and video clips, it will be readily understood by one skilled in the art that other embodiments of digital media, such as sequences of images, singular images, and other visual displays, may also be considered.
- FIG. 2 is a block diagram showing various components of a media processing system, in accordance with an embodiment.
- the digital media processor 112 comprises a buffered frame sequence processor 202 , facial image extraction module 204 , facial image clustering module 206 , suggestion engine 208 , cluster cache 210 , cluster database 216 , index database 218 , and pattern database 220 .
- Other embodiments of digital media processor 112 may include different or less modules.
- the digital media processor 112 processes media content received at the digital media processing facility 110 .
- the media content may comprise moving images, such as video content.
- the video content may include a number of frames, which, in turn, are processed by digital media processor 112 .
- the number of frames for a given length of video depends on the samples per seconds that the original recording was produced and the duration of time of the recording. For example, a video clip recorded at 30 frames per second and is 1 minute long will contain 1800 frames.
- digital media processor 112 may immediately start processing the 1800 frames.
- digital media processor 112 may store a given number of frames into a buffered frame sequence processor 202 .
- the buffered frame sequence processor 202 may be configured to process media content received from a content provider 120 or user system 130 .
- buffered frame sequence processor 202 may receive a video or a video feed from content provider 120 and partition the video or segments of video received in the video feed into video clips of certain time durations or into video clips having a certain number of frames. These video clips or frames are stored in the buffer before it is sent to other modules.
- facial image extraction module 204 may receive processed digital content (i.e., video frames) from buffered frame sequence processor 202 and detect facial images or other types of objects present in the video frames. Detecting facial images within the video indicates the appearance of people in the video, with further processing possibly performed to determine the identity of the individual. However, some frames in the video may contain more than one facial image or no facial image at all.
- the facial image extraction module 204 may be configured to extract all facial images appearing in a single frame. Conversely, if a frame does not contain any facial images, the frame may be removed from the buffer and not considered during further extraction and identification processes. In some embodiments, frames proximate to other frames identified with specific facial images may still be associated with individual that had shown up in the facial image frames.
- the facial image extraction module 204 may also be configured to perform other procedures within digital media processor 112 .
- the facial image extraction module 204 may also be configured to extract textual content of the video frames and save the textual content. Consequently, the textual content may be processed to extract text that suggests the identity of the person or object appearing in the media content.
- the textual content may be used to identify the type of video that the video frames had originated from and also other people appearing in the same frame.
- a clip with President Obama appearing on a news report may have frames labeled as “news” as well as “President Obama.” If President Obama appears on other shows such as THE TONIGHT SHOW with Jay Leno, those video frames may be labeled as “comedy show,” “President Obama,” and “Jay Leno.” If the facial image extraction module 204 is unable to identify the individual, it may prompt a user or operator to identify the person or object in the image.
- the facial image extraction module 206 may normalize the extracted facial images. Normalizing extracted facial images may include digitally modifying images to correct faces for factors that may include, but is not limited to, orientation, position, scale, light intensity, and color contrast. Normalizing the extracted facial images allows the facial image clustering module 206 to more effectively compare faces from an extracted image to faces in other extracted images or templates and, in turn, cluster the images (e.g., all images the same individual). Facial image comparisons allow facial image clustering module 206 to accurately cluster facial images of the same person together and to merge different clusters together if they contain facial images of the same individual. Additionally, the facial image clustering module 206 may identify the frame containing the facial image and optionally cluster the frame.
- the suggestion engine 208 may be configured to label the normalized facial images with suggested identities of a person associated with the facial images in the cluster (e.g., the facial images in the cluster are of the person). To label the clusters, the suggestion engine 208 may compare the normalized facial images to reference facial images, and based on the comparisons, may suggest one or more persons' identities for the cluster. Furthermore, suggestion engine 208 may use the textual context extracted by facial image clustering module 206 to determine identities for the faces present in each cluster.
- cluster cache 210 may be used by digital media processor 112 to temporarily store the clusters created by the facial image clustering module 206 until the clusters are labeled by the suggestion engine 208 .
- Each cluster may be assigned a confidence level that is based in part on how well digital media processor's 112 determines a probable person's identity matches the facial images in the cluster. These confidence levels may be assigned by comparing normalized facial images in the cluster with clusters present in patterns database 220 .
- the identification of facial images is based on a distance calculation from a normalized input facial image to reference images in the patterns database 220 .
- distance calculations comprise of discrete cosine transforms. Other embodiments may use various other methods of calculating distances or variance between two images.
- the clusters in the cluster cache 210 may be saved to cluster database 216 along with labels, face sizes, and corresponding video frames after the facial images in the clusters are identified.
- Cluster cache 210 may also be used to store representative facial images and corresponding information of people that appear often in video processed by digital media processor 112 .
- the cluster cache 210 may include clusters of individuals frequently identified (e.g., Bill O'Reilly on FOX) or recently identified (e.g., Bill O'Reilly's guest) in the video.
- Cluster cache information may also be used for automatic decision making as to which person the facial images of a cluster belongs to.
- the cluster cache 210 may restrict comparisons to only the clusters representing Bill O'Reilly and his guest until another individual is identified in the video (e.g., comparisons do not identify the individual as either Bill O'Reilly or the guest). This allows the suggestion engine to more quickly identify individuals that appear repeatedly in a video.
- the cluster database 216 may be a database configured to store clusters of facial images and associated metadata extracted from received video. Once the clusters have been named in the cluster cache 210 , they may be stored in a cluster database 216 .
- the metadata associated with the facial images in the clusters may be updated when previously unknown facial images in the cluster are identified.
- the cluster metadata may also be updated manually by comparing the cluster images to known reference facial images.
- the index database 218 in an embodiment, may be a database populated with the indexed records of the identified facial images, each facial image's position in the video frame(s) in which it appears, and the number of times (e.g., frames, or collection of frames) or duration the facial image appears in the video.
- the index database 218 may provide searching capabilities to users that enable searching the videos for the appearance of an individual associated with a facial image identified in the index database.
- pattern database 220 may be a database populated with reference or representative facial images of clusters that have been identified. Using the pattern database 220 , facial image clustering module 206 can quickly search through all of the clusters available in the cluster database 216 . If a facial image or a new cluster closely matches a representative facial image present in the index database 218 , digital media processor 112 may merge the new cluster with the cluster referenced by the representative facial image.
- FIG. 3A is a block diagram of an environment within which a facial clustering module is implemented, in accordance with an embodiment.
- the components shown include buffered frame sequence processor 202 , facial image clustering module 206 , cluster database 216 , and example clusters 1 through N.
- Other embodiments of the facial clustering module environment 300 may include more or less components than shown in FIG. 3A .
- the environment 300 illustrates how the buffered frame sequence processor 202 contains video clips of varying lengths that include a number of video frames 305 prior to processing.
- facial image extraction module 204 identifies six frames 305 , each including at least one facials image.
- the facial images may be extracted from the frames by the facial image extraction module 204 .
- the clustering module 206 process the facial images in each of these groups of video frames 305 from the video clips to determine a corresponding cluster(s) assignment for each facial image (and/or frame containing the facial image therein). For facial images that belong to identified people, the facial image is grouped with the same cluster in cluster database 216 . For facial images that belong to unidentified people, facial image clustering module 206 may create a new cluster (e.g., cluster N).
- facial image clustering module 206 may duplicate the frame and cluster each frame with a different cluster. For example, if President Obama and Governor Romney appear in a set of video frames together, facial image clustering module 206 may group that set video frames under two different clusters. One cluster may have frames with President Obama's facial image while the other cluster may have frames with Governor Romney's facial image.
- FIG. 3B is a block diagram showing data flow and correlations between various components of a media processing system for implementing clustering, in accordance with an embodiment.
- Media data 250 may be digital content that has been initially processed by the buffered frame sequence processor 202 and has been split into groups of frames.
- the facial image extraction module 204 may receive the media data 250 (e.g., a video stream containing frames) directly. These frames are passed into a facial image extraction module 204 that filters the frames and determines which frames contain facial images. The facial images appearing in these frames may also be normalized before being passed into a facial image clustering module 206 that clusters a given facial image with other facial images that the module identifies as a close match.
- the facial image clustering module 206 also receives data from pattern database 220 and begins to compare facial images in the formed clusters with template facial images from pattern database 220 .
- suggestion engine 208 may label the new clusters based on information associated with template facial images from the pattern database 220 (e.g., in the case of a recognition), contextual data extracted from the video feed by facial image extraction module 204 , or an operator input.
- the clusters may be stored temporarily in cluster cache 210 throughout processing of the received media data 250 as the individuals identified therein may appear frequently.
- the facial images in the clusters are stored in cluster database 216 while indexing information (e.g., time intervals that certain faces appear in a video, specific videos that certain faces appear in, and so forth) are stored in index database 218 .
- Commonly appearing facial images or representative facial images of each cluster is also forwarded from the cluster database 216 and stored in pattern database 220 as a reference for use when facial image clustering module 206 is processing new video frames.
- the information in index database 218 can be searched for by digital media search processor 114 .
- FIG. 4 is a block diagram showing various components of a facial image extraction module 204 , in accordance with an embodiment.
- the facial image extraction module 204 includes partitioning module 402 , detecting module 404 , discovering module 406 , extrapolating module 408 , limiting module 410 , evaluating module 412 , and normalizing module 414 .
- Other embodiments of facial image extraction module 204 may contain more or less modules than what is illustrated in FIG. 4 .
- Partitioning module 402 processes buffered facial image frames from buffered frame sequence processor 202 by separating the frames out into smaller sized groups. For example, if a video containing 1000 frames is inputted into the buffer frame sequence processor 202 , the processor may separate the frames into 10 groups of 100 frames for buffering purposes until the frame sets can be processed by other modules. Partitioning module 402 may separate each group of 100 frames further into groups of 10 or 15 frames each. Furthermore, partitioning module 402 may also separate frames by other factors, such as change of source, change of video resolution, scene change, logical breaks in programming and so forth. By identifying logical breaks between sets of frames, partitioning module 402 prepares the frame sets for detection module 404 to more efficiently detect facial images in sets of frames. Separating the frames allow more processing to be done in parallel as well as to reduce the workload for each set of frames to be processed by later modules.
- Partitioned frame sets may then be transferred to detecting module 404 for further processing.
- Detecting module 404 may analyze the frames in each set to determine whether a facial image is present in each frame.
- detecting module 404 may sample frames in a set in order to avoid analyzing each frame individually. For example, detecting module 404 may quickly process the first frame in a set partitioned by scene changes to determine whether a face appears in the scene.
- detecting module 404 may analyze the first and last frames of a set of frames (e.g., between scene changes) for facial images. These frames are thus temporally proximate to each other. Frames that are temporally proximate are within a predetermined number of frames from each other. Analysis of intermediate frames may be performed only in areas close to where facial images are found in the first and last frames to identify facial images. The set of facial images identified are spanned facial images.
- extrapolating module 408 may be used to extrapolate facial locations across multiple frames positioned between frames containing a detected facial image without directly processing each frame. Extrapolating provides an approximation of facial image positions in the intermediary frames and thus regions likely to contain the same facial image. Regions unlikely to contain a facial image may be omitted from scans, thus reducing the computation load on the processor.
- Limiting module 410 may be used in an embodiment to reduce the total necessary area that needs to be scanned for facial images. Limiting module 410 may crop the video frame or otherwise limit detection of facial images to the region identified by the extrapolating module 408 . For example, President Obama's face may appear centered in a news video clip. Once extrapolating module 408 has identified a rectangular region near the center of the video frame containing President Obama's face, limiting module 410 may restrict detecting module 404 from searching outside of the identified rectangular region for facial images. In other embodiments, limiting module 410 may still allow detecting module 404 to search outside of the identified region for facial images if detecting module 404 is unable to find facial images on a first scan.
- Detecting module 404 may detect facial images using various methods.
- detecting module 404 may detect eyes that appear in frames. Eyes may both indicate whether a facial image appears in each frame as well as the facial image position according to eye pupil centers.
- Evaluating module 412 may be used to determine the quality of the possible facial images, in accordance with an embodiment. For example, evaluating module 412 may scan each facial image and determine if the distance between the eyes of a facial image appearing in the frame is greater than a predetermined threshold distance. A distance between eyes that is below a certain threshold makes identifying the face unlikely. Thus, frames or regions including faces having a distance between eyes of less than a threshold number may be omitted from further processing. Evaluating module 412 may also scan for certain qualities in a frame that may make later facial normalization processes difficult, such as extremes in brightness levels, odd facial positioning, unreasonable color differences and so forth. These qualities may cause the frame to also be omitted from further processing.
- normalizing module 414 modifies the facial images so that they are oriented in a similar position to aid in facial image comparisons with template images and with other facial images. Normalization may involve using eye position, as well as other facial features such as nose and mouth, to determine how to properly shift regions of a facial image to orient the facial image in a desired position. For example, normalizing module 414 may detect that a person in an image is facing upwards. By using the relative positioning of several facial features, normalizing module 414 can digitally shift the face and extrapolate a forward positioned face. In other embodiments, normalizing module 414 may shift the face so that it is facing the side or in another position.
- discovering module 406 may also be analyzing the video frames containing detected facial images for the presence of textual content.
- the textual content may be helpful in identifying the person associated with the detected facial images.
- frames including textual content are queued for processing by an optical character recognition (OCR) processor to convert the textual content into digital text.
- OCR optical character recognition
- textual content may be present in video feeds as part of subtitles or captions.
- Detecting module 404 scanning through video frames may detect facial images that appear in certain frames. Discovering module 404 may then queue those frames for additional processing through an OCR processor (not shown).
- the OCR processor may detect the subtitles on each frame and scan them to produce keywords that may contain the identity of the people appearing in the images.
- FIG. 5 is a flow chart of a method for video processing involving facial image extraction and initial cluster, in accordance with an embodiment.
- the facial image clustering module 206 may use the extracted facial image output to generate image clusters.
- Digital media such as video
- the digital media processor 112 receives 502 the sequence of buffered frames, which may be further partitioned by partitioning module 402 , and uses detecting module 404 to detect 504 facial images in the first and last frames of each set of buffered frames.
- the facial images in the first and last frames may be temporally proximate.
- Facial image extraction module 204 is thus able to determine sets of frames that may have facial images appear.
- Frame sets that have facial images appear in either the first or last frames, or both the first and last frames may be furthered processed by an extrapolating module 408 .
- the extrapolating module 408 extrapolates 506 facial images to determine approximate locations in all frames where facial images are likely to appear.
- Detecting module 404 may scan the approximate facial image regions to locate 508 facial images. Frames with facial images may also be queued 510 for an OCR by discovering module 406 . Textual data extracted by discovering module 406 and an OCR may provide the identity of faces that appear in those frames. Detecting module 404 , in coordination with limiting module 410 and evaluating module 412 , may detect 512 certain facial features (e.g., eyes, nose, mouth, ears, and so forth) as facial “landmarks.” Because facial images should be of a certain size and quality before facial recognition can be carried out with reasonable computing resources, each facial image is analyzed by evaluating module 412 . Determining thresholds may differ between different embodiments, but in an embodiment, eyes that are well-detected and have sufficient distance between eyes may be preserved 514 for further processing. Frames that do not meet the thresholds may be omitted.
- each extracted facial image should be normalized.
- normalizing module 414 processes each facial image so that the face is normalized 516 in a horizontal orientation, normalized 518 for lighting intensity, and normalized 520 for scaling (e.g., through normalizing the number of pixels between the eyes).
- a different combination of normalizing procedures using steps both listed and not listed in this embodiment may be used to normalize facial images for clustering.
- Images determined as valid, or as providing sufficient information for a facial image to be identified, by evaluating module 412 may then be preserved 524 for clustering purposes.
- Other embodiments may determine video frame validity to preserve 524 for clustering through other means, such as identifying frames proximate to other frames that contain identifiable facial images or containing contextual information relevant to other frames that have identifiable facial images.
- Facial image clustering involves taking facial images of people appearing in different frames of a video and grouping them into a “cluster.”
- Each cluster contains facial images of individuals that have same common trait.
- a cluster may contain facial images of the same person, or it may contain facial images of people that have specific facial features in common.
- digital media processor 112 is able to more quickly and effectively identify individuals that appear in videos. Grouping like facial images together also reduces the computing resources that have to be devoted to comparing, matching, and identifying facial images by reducing the need to perform intensive computations on every facial image in every video frame.
- facial image clustering occurs as facial image clustering module 206 is sorting through the sets of video frames from a facial image extraction module.
- An initial method of separating and partitioning the sets of video frames is by analyzing the frames for changes in scenes in facial image extraction module 204 . Once facial images are extracted from these frames by the facial image extraction module 204 , facial image clustering module 206 can perform additional analysis on the sets of facial images to cluster images. Facial image movements may also be identified and tracked throughout the scene. Face detection and tracking may include labeling each face with a unique track identifier. By tracking a facial image as it moves around the field of view within a set of frames, facial image clustering module 206 may determine that the facial images appearing in the different frames belong to the same person and may cluster the frames together.
- FIG. 6 illustrates a clusterizer track, in accordance with an embodiment.
- facial image clustering module 206 may determine a clusterizer track 600 .
- a clusterizer track 600 shows the path that a facial image moves in through a time period spanned by the video frames. For example, a face appears in the 10 th frame of a video clip. On the 11 th frame, the face may have moved slightly upwards and rightwards. On the 12 th frame, the face may have moved slightly farther in the same direction. If the distances between the facial images in each of the frames do not exceed a certain threshold, then facial image clustering module 206 may determine that the individual facial images belong to the same individual and may group them into the same cluster. However, if the distances between facial images exceed the threshold, then facial image clustering module 206 may cluster the images into separate clusters.
- FIG. 7 illustrates an example of merging clusters, in accordance with an embodiment.
- each cluster includes one or more “key face” or representative facial image that best represent the facial images in the cluster.
- Key faces from one cluster may be compared with key faces from other clusters to determine distances between the clusters.
- an unknown key face from the new cluster may be compared with key faces from other clusters.
- a key face #n is associated with cluster M.
- Key face #n is compared to key face # 1 , key face # 2 , key face # 3 , key face # 1 , key face #m, key face #p, key face #r, and any other key faces that exist.
- Facial image clustering module 206 compares each distance to a threshold value and determines whether two clusters should be merged, should be kept separate, or more calculations should be performed to generate a more certain result. Clusters that are merged may have facial images of the same person while clusters that are separate may have facial images of different people.
- Each key face adds significant additional information to the cluster for digital media processor 112 to have available for identifying unknown facial images. By identifying multiple images as key faces, digital media processor 112 increases the probability that an unknown image or cluster may be identified and associated with an individual.
- Each key face may also be associated with a set of sub-facial images that form a spanned face. Facial images that form the spanned face are additional images that may not add significant information to an existing key face, such as repetitive facial images or duplicate frames.
- facial image clustering module 206 performs the computations related to clustering. As video frames from a buffer are sent into a facial image clustering module 206 , each frame (or the facial image identified in the frame) is analyzed and grouped into a cluster containing facial images of the same person. Facial image clustering module 206 also compares these clusters with previously created clusters and merges clusters as necessary. In an embodiment, clusters may be identified according to the person that each contains. In other embodiments, clusters may be identified by some other common traits, which may include facial geometry, eye color, nose structure, hair style, skin color and so forth. Clusters formed by facial image clustering module 206 are stored in cluster database 216 , with indexing information stored in index database 218 .
- FIG. 8 is a block diagram showing various components of a facial image clustering module 206 , in accordance with an embodiment.
- the facial image clustering module 206 includes a receiving module 802 , clusterizer track module 804 , quality estimation module 806 , collapsing module 808 , merging module 810 , comparing module 812 , client module 814 , assigning module 816 , associating module 818 , and populating module 820 .
- Other embodiments of a facial image clustering module 206 may include more or less modules than is represented in FIG. 8 .
- Images processed by a facial image extraction module 204 are received by facial image clustering module 206 using receiving module 802 .
- Receiving module 802 prepares facial image frames by temporarily storing a certain number of frames before releasing the frames to a clusterizer track module 804 , which will identify clusterizer tracks 600 .
- a clusterizer track module 804 receives sets of facial images in buffers from a receiving module 802 , in accordance with an embodiment.
- the clusterizer track module 804 selects a representative facial image frame in each buffered set and facial images from frames surrounding it.
- Clusterizer track module 804 then calculates the distances between the representative facial image and the facial image in other proximate frames. If the distances between the facial images in the frames fall within a specified threshold, then clusterizer track module 804 may determine that a clusterizer track 600 exists.
- a clusterizer track 600 outlines the path or region that clusterizer track module 804 may expect to find facial images in a series of video frames.
- Clusterizer track module 804 may form clusters from facial images along the same clusterizer track 600 . The formation of clusterizer tracks 600 was illustrated earlier in FIG. 6 .
- facial images are analyzed for quality by a quality estimation module 806 .
- Facial images from clusterizer track module 804 may be referred to as “crude faces” as they may consist of facial images of varying quality.
- Quality estimation module 806 performs various procedures, which may include a Fast-Fourier-Transformation (FFT), to determine values for image quality.
- FFT Fast-Fourier-Transformation
- HP high-pass
- LP low-pass
- a higher HP-LP ratio indicates that an image contains more sharp edges and is thus not blurred.
- Each “crude face” is compared against a benchmark quality value to determine whether the image is stored or removed.
- Collapsing module 808 receives sets of “quality images” processed by a quality estimation module 806 and determines a key face among the set, in accordance with an embodiment.
- the key face is thus a representative face for the cluster, allowing collapsing module 808 to “collapse” or reduce the amount of data considered as critical to the cluster.
- only the key face is stored and the rest of the faces are considered as spanned face.
- digital media processor 112 can reduce the number of comparisons and thus the computing resources necessary to identify facial images in a video.
- Clusters that contain facial images that are similar may be considered for merging.
- merging module 810 compares key faces between the newly formed clusters. If the distances between the key faces fall within a certain threshold, then merging module 810 may combine the clusters containing the compared key faces.
- merging is based on a relatively slow, but accurate, face comparison between the key face of two or more clusters. For example, merging clusters consolidates facial images of the same person so that subsequent facial image identification and comparisons can be performed with few prior clusters needing to be compared. The process merging clusters was illustrated earlier in FIG. 7 .
- comparing module 812 compares the facial images in the cluster to reference facial images from pattern database 220 . To minimize computing time, a fast and rough comparison may be performed by comparing module 812 to identify a set of likely reference facial images and exclude unlikely reference facial images before performing a slower, fine-pass comparison. In an embodiment, comparing module 812 automatically performs the comparisons based on distances between a cluster key face and a reference facial image from a pattern database 220 and determines acceptable suggestions as to the identity of the facial images in a cluster.
- facial image clustering module 206 may, in an embodiment, use client module 814 to prompt a user or operator for a suggestion. For example, an operator may be provided with an unknown key face along with other extracted contextual information about the key face and asked to identify the person. After the operator visually identifies the facial image, client module 814 can update pattern database 220 so that the operator is not likely to be prompted in the future for manual identifications of facial images belonging to the same person.
- assigning module 816 may attach identifying metadata or other information to the cluster.
- Associating module 818 may also reference index information stored in index database 218 and associate new cluster data identifiers with the index information stored in index data base 218 .
- associating module 818 may store metadata relating to, but not limited to, a person's identity, location in the video stream, time of appearance, and spatial location in the frames.
- the processed cluster data may then be saved to cluster database 216 by populating module 820 .
- FIGS. 9A , 9 B, 9 C, and 9 D illustrate flow diagrams that show a method for clustering facial images, in accordance with an example embodiment.
- the method may be performed by processing logic that may comprise hardware (e.g., dedicated logic, programmable logic, microcode, etc.), computer program code or modules executed by one or more processors to perform the steps illustrated herein (for example, on a general purpose computer system or computer server system illustrated in FIG. 10 ), or a combination of both.
- the processing logic resides at the digital media processor 112 .
- the method may be performed by the various modules of a facial image clustering module 206 . To more clearly illustrate the method for clustering facial images, FIGS. 9A , 9 B, 9 C, and 9 D each describe different components.
- the method for clusterizing images commences with frame buffering 900 A.
- frame buffering 900 A video frames are received 902 and checked for validity 904 .
- Valid frames are pushed 906 into a frame buffer for temporary storage.
- the purpose of the buffer is to collect some quantity of frames to process quickly.
- the process of receiving and checking the frames is repeated until the frame buffer becomes full 908 of the last frame of the video is received.
- the facial image clustering module 206 proceeds onto the clusterized track processing 900 B process, which is illustrated in FIG. 9B .
- clusterized track processing 900 B process shown in FIG. 9B may be performed by clusterizer track module 804 .
- Each facial image from a buffer is analyzed to determine if a clusterizer track exists and if the facial image can be related to an existing reference facial image.
- facial image clustering module 206 may decide whether an incoming facial image is inserted into a crude face buffer, incremented into a presence rate, or discarded.
- a crude face buffer contains unidentified facial images to be further optimized and analyzed at a later point in the process.
- each frame in a video buffer contains facial images that are assigned a unique track identifier, which is used to find 914 a clusterizer track.
- an incoming facial image (unclustered facial image) is used to establish 918 a new clusterizer track.
- the unclustered facial image is then added 920 to a crude face buffer before the process repeats again with the next frame in the video feed.
- the clusterizer track module 804 calculates 922 the distance between the unclustered facial image and a reference face. This process may be performed using an algorithm or an object used to evaluate the similarity of objects. In an embodiment, the distance between the unclustered facial image and a representative facial image is represented by a coefficient of similarity. A higher coefficient value may indicate a greater likelihood that both faces belong to the same cluster. In other embodiments, a discrete-cosine-transformation (DCT) for feature extraction and L1-norm for distance (similarity) calculation, or motion field and affine transformation may be used.
- DCT discrete-cosine-transformation
- L1-norm for distance (similarity) calculation
- motion field and affine transformation may be used.
- the clusterizer track module 804 should perform comparisons and calculations quickly and with an adequate degree of accuracy so that the facial image verifications can proceed smoothly.
- the unclustered facial image and the reference image are found to be sufficiently similar (e.g., below threshold 1 ), then the unclustered facial image may be matched to the reference facial image.
- the cluster presence rate is thus incremented 930 .
- a cluster presence rate indicates the amount of frames where the object in a cluster has appeared and subsequently been clustered.
- the unclustered facial image can then be dropped in part because the unclustered face is too similar to the reference facial image to provide additional recognition information.
- the unclustered facial image is inserted 932 into a crude face buffer for later analysis.
- facial image clustering module 206 may compare the unclustered facial image with the current last facial image (e.g., the previous unclustered facial image from the video frame buffer that was compared and analyzed) and calculate 934 a distance.
- the distance is above a certain threshold (e.g., threshold 1 )
- the unclustered facial image is added 938 to the crude face buffer and replaces 940 the current last facial image.
- the unclustered facial image may be assumed to be too similar to the last facial image compared.
- the unclustered facial image thus offers no additional recognition information and may be discarded.
- each facial image in the crude faces buffer is evaluated 950 for quality. If the facial image quality is sufficient for spanning a reference face (forming a more complete model of a reference face) or may serve as a quality representative face, the face may be stored 954 .
- a Fast-Fourier-Transformation FFT
- HP and LP components indicate the sharpness of the image; thus, a facial image with the maximum HP-LP ratio may be chosen for the sharpest quality.
- Quality value indicators may be compared to initial index values set 946 as a benchmark for facial quality.
- Quality facial images are analyzed in the face collapsing 900 D process to determine whether the face can become stored as a key face for an existing or a new cluster.
- An embodiment of face collapsing 900 D is shown in FIG. 9C .
- Each cluster contains a reference to a key face and each key face contains a reference to a cluster. If an existing cluster belonging to a clusterizer track does not have a key face, then it can import a key face from the processed crude face buffer. That facial image thus becomes the representative face for the related sequence of faces in the crude face buffer. If a sequence already has a key face, then that key face and the unclustered facial image are compared to determine which one is more representative of the cluster's images.
- only the key face is stored and the rest of the facial images are considered as spanned face. Storing facial images as part of a spanned face rather than as a key face reduces the amount of information needed to be stored.
- the new key face may then be used to create 962 a new cluster.
- a facial image clustering module 206 may reduce the redundancy present in the database.
- a merging is based on relatively slow, but accurate, face comparison between the key faces of two clusters.
- An embodiment of cluster merging 900 E is shown in FIG. 9C .
- new key faces are compared to existing key faces.
- facial image clustering module 206 may determine whether to merge 970 the clusters.
- facial image clustering module 206 may begin to identify the facial images in each cluster through the process of suggestion 900 F.
- An embodiment of the suggestion 900 F process is shown in FIG. 9D .
- rough comparisons of cluster images may be compared 976 to image patterns present in pattern database 220 .
- the rough comparison can quickly identify a set of possible reference facial images and exclude unlikely reference facial images before a slower, fine-pass identification 978 takes place. From this fine comparison, only one or very few reference facial images may be identified as being associated with the same person as the facial image in the cluster.
- facial image cluster module 206 may be able to automatically identify 982 and label 984 the clusters based in part on the distance calculated between the unidentified key face and a reference facial image during the fine comparison.
- facial image clustering module 206 may make an automated choice.
- an operator may be provided with the facial image for manual identification.
- cluster database 216 may be empty and accordingly there will be no suggestions generated, or the confidence level of the available suggestions may be insufficient.
- pattern database 220 may be updated 986 , so that future related images do not require manual identification, and the cluster is labeled 984 appropriately.
- the cluster database 216 and index database 218 are updated 988 , 990 .
- New cluster images or updated cluster images are stored in cluster database 216 while new or updated references (e.g., links to key faces or associated facial images) are stored in index database 218 . If too many unlabeled clusters exist 992 after the updating process, then manual identification may be performed to identify the clusters and update 986 the pattern database 220 accordingly.
- FIG. 10 shows a diagrammatic representation of a machine in the example form of a computer system 1000 , within which a set of instructions for causing the machine to perform any one or more of the methodologies discussed herein may be executed.
- the machine operates as a stand-alone device or may be connected (e.g., networked) to other machines.
- the machine may operate in the capacity of a server or a client machine in a server-client network environment, or as a peer machine in a peer-to-peer (or distributed) network environment.
- the machine may be a personal computer, a tablet computer, a wearable computer, a personal digital assistant, a cellular or mobile telephone, a portable music player (e.g., a portable hard drive audio device such as an MP3 player), a web appliance, a gaming device, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine.
- a portable music player e.g., a portable hard drive audio device such as an MP3 player
- a web appliance e.g., a portable hard drive audio device such as an MP3 player
- gaming device e.g., a portable hard drive audio device
- network router e.g., a network router, switch or bridge
- machine shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein.
- the example computer system 1000 includes a processor 1002 (e.g., a central processing unit (CPU), a graphics processing unit (GPU) or both), a main memory 1004 , and a static memory 1006 , which communicate with each other via a bus 1020 .
- the computer system 1000 may further include a graphics display unit 1008 (e.g., a liquid crystal display (LCD), organic light emitting diode (OLED), or a cathode ray tube (CRT)).
- a graphics display unit 1008 e.g., a liquid crystal display (LCD), organic light emitting diode (OLED), or a cathode ray tube (CRT)
- the computer system 1000 also includes an alphanumeric input device 1010 (e.g., a keyboard), a cursor control device 1012 (e.g., a mouse), a drive unit 1014 , a signal generation device 1016 (e.g., a speaker), and a network interface device 1018 .
- an alphanumeric input device 1010 e.g., a keyboard
- a cursor control device 1012 e.g., a mouse
- drive unit 1014 e.g., a drive unit 1014
- a signal generation device 1016 e.g., a speaker
- a network interface device 1018 e.g., a network interface device
- the storage unit 1014 includes a machine-readable medium 1022 on which is stored one or more sets of instructions and data structures (e.g., instructions 1024 ) embodying or utilized by any one or more of the methodologies or functions described herein.
- the instructions 1024 may also reside, completely or at least partially, within the main memory 1004 and/or within the processor 1002 during execution thereof by the computer system 1000 .
- the main memory 1004 and the processor 1002 also constitute machine-readable media.
- the instructions 1024 may further be transmitted or received over a network 105 via the network interface device 1018 utilizing any one of a number of well-known transfer protocols (e.g., Hyper Text Transfer Protocol (HTTP)).
- HTTP Hyper Text Transfer Protocol
- machine-readable medium 1022 is shown in an example embodiment to be a single medium, the term “machine-readable medium” should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions.
- the term “machine readable medium” shall also be taken to include any medium that is capable of storing, encoding, or carrying a set of instructions for execution by the machine and that causes the machine to perform any one or more of the methodologies of the present application, or that is capable of storing, encoding, or carrying data structures utilized by or associated with such a set of instructions.
- machine-readable medium shall accordingly be taken to include, but not be limited to, solid-state memories, optical and magnetic media, and carrier wave signals. Such media may also include, without limitation, hard disks, floppy disks, flash memory cards, subscriber identity module (SIM) cards, digital video disks, random access memory (RAMs), read only memory (ROMs), and the like.
- SIM subscriber identity module
- RAMs random access memory
- ROMs read only memory
- Modules may constitute either software modules (e.g., code embodied on a machine-readable medium or in a transmission signal) or hardware modules.
- a hardware module is tangible unit capable of performing certain operations and may be configured or arranged in a certain manner.
- one or more computer systems e.g., a standalone, client or server computer system
- one or more hardware modules of a computer system e.g., a processor or a group of processors
- software e.g., an application or application portion
- a hardware module may be implemented mechanically or electronically.
- a hardware module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a special-purpose processor, such as a field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC)) to perform certain operations.
- a hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.
- processors e.g., processor 1002
- processors may be temporarily configured (e.g., by software) or permanently configured to perform the relevant operations.
- processors may constitute processor-implemented modules that operate to perform one or more operations or functions.
- the modules referred to herein may, in some example embodiments, comprise processor-implemented modules.
- the one or more processors may also operate to support performance of the relevant operations in a “cloud computing” environment or as a “software as a service” (SaaS). For example, at least some of the operations may be performed by a group of computers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., application program interfaces (APIs).)
- SaaS software as a service
- the performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines.
- the one or more processors or processor-implemented modules may be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm). In other example embodiments, the one or more processors or processor-implemented modules may be distributed across a number of geographic locations.
- any reference to “one embodiment” or “an embodiment” means that a particular element, feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment.
- the appearances of the phrase “in one embodiment” in various places in the specification are not necessarily all referring to the same embodiment.
- Coupled and “connected” along with their derivatives.
- some embodiments may be described using the term “coupled” to indicate that two or more elements are in direct physical or electrical contact.
- the term “coupled,” however, may also mean that two or more elements are not in direct contact with each other, but yet still co-operate or interact with each other.
- the embodiments are not limited in this context.
- the terms “comprises,” “comprising,” “includes,” “including,” “has,” “having” or any other variation thereof, are intended to cover a non-exclusive inclusion.
- a process, method, article, or apparatus that comprises a list of elements is not necessarily limited to only those elements but may include other elements not expressly listed or inherent to such process, method, article, or apparatus.
- “or” refers to an inclusive or and not to an exclusive or. For example, a condition A or B is satisfied by any one of the following: A is true (or present) and B is false (or not present), A is false (or not present) and B is true (or present), and both A and B are true (or present).
Abstract
Description
- This application claims the benefit of U.S. Provisional Application No. 61/569,168, filed Dec. 9, 2011, which is incorporated by reference herein in its entirety.
- The disclosure relates generally to the field of video processing and more specifically to detecting, tracking and clustering objects appearing in video.
- Many media content consumers enjoy being able to browse through the media content such as images and video to find individuals or objects of their interest. Object and facial recognition techniques may be used by media content providers in order to properly detect and identify faces and objects.
- However, some types of media, particularly video, have been difficult to apply recognition techniques to. Some of the difficulties relate to the computational complexity of measuring the differences between the video objects. Faces and objects in these video objects are often affected by factors such as differences in brightness, positioning and expression. An effective solution to facial and object recognition in videos would allow for a smoother browsing experience where a user may be able to search for segments in a video where a certain individual or object appears.
-
FIG. 1 is a block diagram illustrating a system environment for object detection and recognition and database population of objects for video indexing, in accordance with an embodiment. -
FIG. 2 is a block diagram showing various components of a media processing system, in accordance with an embodiment. -
FIG. 3A is a block diagram of an environment within which a facial image clustering module is implemented, in accordance with an embodiment. -
FIG. 3B is a block diagram showing data flow and correlations between various components of a media processing system for implementing clustering, in accordance with an embodiment. -
FIG. 4 is a block diagram showing various components of a facial image extraction module, in accordance with an embodiment. -
FIG. 5 is a flow chart of a method for video processing involving facial image extraction and initial clustering, in accordance with an embodiment. -
FIG. 6 illustrates a clusterizer track, in accordance with an embodiment. -
FIG. 7 illustrates an example of merging clusters, in accordance with an embodiment. -
FIG. 8 is a block diagram showing various components of a facial image clustering module, in accordance with an embodiment. -
FIG. 9A is flow diagram illustrating a method for frame buffering, in accordance with an embodiment. -
FIG. 9B is flow diagram illustrating a method for clusterized track processing, in accordance with an embodiment. -
FIG. 9C is flow diagram illustrating a method for face quality evaluation, face collapsing, and cluster merging, in accordance with an embodiment. -
FIG. 9D is flow diagram illustrating a method for facial image identity suggestion, in accordance with an embodiment. -
FIG. 10 is a diagram representation of a computing device capable of performing the clustering of objects in media content. - The figures depict various embodiments of the present invention for purposes of illustration only. One skilled in the art will readily recognize from the following discussion that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles of the invention described herein.
- The Figures (FIGS.) and the following description relate to preferred embodiments by way of illustration only. It should be noted that from the following discussion, alternative embodiments of the structures and methods disclosed herein will be readily recognized as viable alternatives that may be employed without departing from the principles of what is claimed.
- Reference will now be made in detail to several embodiments, examples of which are illustrated in the accompanying figures. It is noted that wherever practicable similar or like reference numbers may be used in the figures and may indicate similar or like functionality. The figures depict embodiments of the disclosed system (or method) for purposes of illustration only. One skilled in the art will readily recognize from the following description that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles described herein.
- In one example embodiment, a system (and method) is configured for recognition and identification of objects in videos. The system is configured to accomplish this through clustering and identifying objects in videos. The type of objects may include cars, persons, animals, plants, etc., with identifiable features. Clusters can also be broken down further into more specific clusters that may identify different people, brands of cars, types of animals, etc. In an embodiment, each cluster contains images of a certain type of object, based on some common property within the cluster. Objects may be unique compared to other objects within an initial cluster, and thus can be furthered categorized or clustered according to their differences. For example, a specific person is unique compared to other people. While video objects containing any person may be clustered under a “people” identifier label, images containing a specific person may be identified by distinguishable features (e.g., face, shape, color, height, etc.) and clustered under a more specific identifier. However, there may be more than one cluster created per one person because a threshold level or other settings determine the creation of another cluster associated with the same individual. In an embodiment, further calculations may be performed to determine if facial images from the two clusters belong to the same person. Depending on the results, the clusters may be merged or kept separate.
- Comparisons may be triaged such that less computationally expensive comparisons are performed and determinations (e.g., according to the degree of similarity between images) are made prior to performing more accurate or additional comparisons. For example, these initial comparisons may be used to determine whether or not two images are of the same person. A set of images determined to likely be of the same person may form an initial cluster. Within the initial cluster, further calculations may be used to determine an initial image to represent the clustered images or determine an identity of the cluster (e.g., the identity of the person). Furthermore, if images from two clusters are determined to be of the same person, then these two clusters may be merged to form a single cluster for the person.
- The cluster data pertaining to the images may be stored in one or more databases and utilized to index objects and the videos in which they appear. In an embodiment where the object type for identification are people and the clustered objects are facial images of a person, the stored data may include, among other things, the name of the person associated with the facial images, the times or locations of appearances of the person in the video based on the determination of their facial image being present. In other embodiments, inanimate objects may also be considered for identification and clustering. For example, data stored for inanimate objects may include different types of cars. These cars may be clustered and identified through their different features such as headlights, rear, badge, etc., and associated with a specific model and/or brand.
- The data stored to the database may be utilized to search video clips for specific objects by keywords (e.g., a specific person's name, brand or model of a car, etc.). The data stored in clusters provide users with a better video viewing experience. For example, clustering objects allows users searching for a specific person in videos to determine the video clips along with the times and locations in the clips where the searched person appears, and also to navigate through the videos by appearances of the object.
- Turning now to
FIG. 1 , a block diagram illustrates a system environment for object detection and recognition and database population of objects for video indexing, in accordance with an example embodiment. As shown, theenvironment 100 may comprise a digitalmedia processing facility 110,content provider 120,user system 130, and anetwork 105.Network 105 may comprise any combination of local area and/or wide area networks, mobile, or terrestrial communication systems that allows for the transmission of data between digitalmedia processing facility 110,user system 130 and/orcontent provider 120. - The
content provider 120 may comprise a store of publishedcontent 125. While only onecontent provider 120 is shown, there may bemultiple content providers 120, each transmitting their own publishedcontent 125 overnetwork 105. Publishedcontent 125 may include digital media content, such as digital videos or video clips, thatcontent provider 120 owns or has rights to. Alternatively, the publishedcontent 125 may include user content 135 uploaded to the content provider 120 (e.g., via a video sharing service). As an example, the content provider may be a news service agency that provides news reports to digital media broadcasters (not shown) or otherwise provides access to the news reports (e.g., via a website or streaming service). The news reports, which may be in the form of videos or video clips, are the publishedcontents 125 that are being distributed to other individuals or entities. - The
user system 130 may comprise of a store of user content 135. There may be one ormore user system 130 connected to network 105 insystem environment 100. Auser system 130 may be a general purpose computer, a television set (TV), a personal digital assistant (PDA), a mobile telephone, a wireless device, or any other device capable of visual presentation of data acquired, stored, or transmitted in various forms. Eachuser system 130 may store its own user content 135, which include media content stored on theuser system 130. For example, any pictures, movies, documents, and so forth stored on a user's hard drive may be considered as user content 135. Furthermore, digital content stored in the “cloud” or a remote location may also be considered as user content 135. - Digital
media processing facility 110 may further comprise adigital media processor 112 and a digitalmedia search processor 114. In an embodiment, the digital media processing facility may represent fixed, mobile, or transportable structures, including any associated equipment, wiring, cabling, networks, and utilities, that provide housing to devices that have computing capability. Digital media from sources, such as publishedcontent 125 fromcontent provider 120 or user content 135 fromuser system 130, may be sent overnetwork 105 to digitalmedia processing facility 110 for processing. The digital media processing facility may process receivedmedia content 125, 135 to detect, identify, cluster and index recognizable objects or individuals in the media content. Additionally, the digitalmedia processing facility 110 may enable searching of the indexed objects or individuals in the media content. - Digital
media search processor 114 may be any computing device (e.g., computer, laptop, mobile device, tablet and so forth) that is capable of performing a search through a store of digital contents. This may include searches through content available onnetwork 105 for specific digital content or it may involve searches through content or indexes already present in digitalmedia processing facility 110. For example, digitalmedia processing facility 110 may receive a request to search for instances when a specific individual appears in some set of digital media content (e.g., videos). Digitalmedia search processor 114 runs the search through content and indexes available to it before returning a list of results. - The
digital media processor 112 may be, but is not limited to, a general purpose processor for user in a personal or server computer, laptop, mobile device, tablet, or some other type of processor capable of receiving, processing, and distributing digital media content. In an embodiment, thedigital media processor 112 is capable of running processes on a digital media content store to detect, identify, cluster, and index objects that appear in the content store. This is only an example of whatdigital media processor 112 is capable of and other embodiments ofdigital media processor 112 may include more or less capabilities. - While the following description discusses various embodiments related to the identification of persons based on their facial images, it will be readily understood by one skilled in the art, as described previously, that the following examples can be applied to other animate and inanimate entities, such as a horse or a car. Thus, facial images may be used to refer to the facial fronts of both animate and inanimate objects. Furthermore, while the following description discusses digital media as videos and video clips, it will be readily understood by one skilled in the art that other embodiments of digital media, such as sequences of images, singular images, and other visual displays, may also be considered.
-
FIG. 2 is a block diagram showing various components of a media processing system, in accordance with an embodiment. In one embodiment, thedigital media processor 112 comprises a bufferedframe sequence processor 202, facialimage extraction module 204, facialimage clustering module 206,suggestion engine 208,cluster cache 210, cluster database 216,index database 218, andpattern database 220. Other embodiments ofdigital media processor 112 may include different or less modules. - The
digital media processor 112 processes media content received at the digitalmedia processing facility 110. As described previously, the media content may comprise moving images, such as video content. The video content may include a number of frames, which, in turn, are processed bydigital media processor 112. The number of frames for a given length of video depends on the samples per seconds that the original recording was produced and the duration of time of the recording. For example, a video clip recorded at 30 frames per second and is 1 minute long will contain 1800 frames. In an embodiment,digital media processor 112 may immediately start processing the 1800 frames. However, in another embodiment,digital media processor 112 may store a given number of frames into a bufferedframe sequence processor 202. - The buffered
frame sequence processor 202, in an embodiment, may be configured to process media content received from acontent provider 120 oruser system 130. For example, bufferedframe sequence processor 202 may receive a video or a video feed fromcontent provider 120 and partition the video or segments of video received in the video feed into video clips of certain time durations or into video clips having a certain number of frames. These video clips or frames are stored in the buffer before it is sent to other modules. - In an embodiment, facial
image extraction module 204 may receive processed digital content (i.e., video frames) from bufferedframe sequence processor 202 and detect facial images or other types of objects present in the video frames. Detecting facial images within the video indicates the appearance of people in the video, with further processing possibly performed to determine the identity of the individual. However, some frames in the video may contain more than one facial image or no facial image at all. The facialimage extraction module 204 may be configured to extract all facial images appearing in a single frame. Conversely, if a frame does not contain any facial images, the frame may be removed from the buffer and not considered during further extraction and identification processes. In some embodiments, frames proximate to other frames identified with specific facial images may still be associated with individual that had shown up in the facial image frames. For example, on THE DAILY SHOW with Jon Stewart, when Jon Stewart shows up on screen, talks for a few minutes, plays a video of President Obama, and then makes jokes about the video, the entire segment may be associated with Jon Stewart (despite him not appearing in all of the frames). Furthermore, the shorter segment with the video on Obama may also be associated with President Obama. - The facial
image extraction module 204 may also be configured to perform other procedures withindigital media processor 112. In an embodiment, the facialimage extraction module 204 may also be configured to extract textual content of the video frames and save the textual content. Consequently, the textual content may be processed to extract text that suggests the identity of the person or object appearing in the media content. In some embodiments, the textual content may be used to identify the type of video that the video frames had originated from and also other people appearing in the same frame. For example, a clip with President Obama appearing on a news report may have frames labeled as “news” as well as “President Obama.” If President Obama appears on other shows such as THE TONIGHT SHOW with Jay Leno, those video frames may be labeled as “comedy show,” “President Obama,” and “Jay Leno.” If the facialimage extraction module 204 is unable to identify the individual, it may prompt a user or operator to identify the person or object in the image. - In an embodiment, the facial
image extraction module 206 may normalize the extracted facial images. Normalizing extracted facial images may include digitally modifying images to correct faces for factors that may include, but is not limited to, orientation, position, scale, light intensity, and color contrast. Normalizing the extracted facial images allows the facialimage clustering module 206 to more effectively compare faces from an extracted image to faces in other extracted images or templates and, in turn, cluster the images (e.g., all images the same individual). Facial image comparisons allow facialimage clustering module 206 to accurately cluster facial images of the same person together and to merge different clusters together if they contain facial images of the same individual. Additionally, the facialimage clustering module 206 may identify the frame containing the facial image and optionally cluster the frame. - The
suggestion engine 208, in an embodiment, may be configured to label the normalized facial images with suggested identities of a person associated with the facial images in the cluster (e.g., the facial images in the cluster are of the person). To label the clusters, thesuggestion engine 208 may compare the normalized facial images to reference facial images, and based on the comparisons, may suggest one or more persons' identities for the cluster. Furthermore,suggestion engine 208 may use the textual context extracted by facialimage clustering module 206 to determine identities for the faces present in each cluster. - In an embodiment,
cluster cache 210 may be used bydigital media processor 112 to temporarily store the clusters created by the facialimage clustering module 206 until the clusters are labeled by thesuggestion engine 208. Each cluster may be assigned a confidence level that is based in part on how well digital media processor's 112 determines a probable person's identity matches the facial images in the cluster. These confidence levels may be assigned by comparing normalized facial images in the cluster with clusters present inpatterns database 220. In one embodiment, the identification of facial images is based on a distance calculation from a normalized input facial image to reference images in thepatterns database 220. In an embodiment, distance calculations comprise of discrete cosine transforms. Other embodiments may use various other methods of calculating distances or variance between two images. - The clusters in the
cluster cache 210 may be saved to cluster database 216 along with labels, face sizes, and corresponding video frames after the facial images in the clusters are identified.Cluster cache 210 may also be used to store representative facial images and corresponding information of people that appear often in video processed bydigital media processor 112. For example, if thedigital media processor 112 is processing video from a same source or television program, thecluster cache 210 may include clusters of individuals frequently identified (e.g., Bill O'Reilly on FOX) or recently identified (e.g., Bill O'Reilly's guest) in the video. Cluster cache information may also be used for automatic decision making as to which person the facial images of a cluster belongs to. Specifically, if Bill O'Reilly and his guest are the only individuals identified in a portion of a video, thecluster cache 210 may restrict comparisons to only the clusters representing Bill O'Reilly and his guest until another individual is identified in the video (e.g., comparisons do not identify the individual as either Bill O'Reilly or the guest). This allows the suggestion engine to more quickly identify individuals that appear repeatedly in a video. - The cluster database 216, in an embodiment, may be a database configured to store clusters of facial images and associated metadata extracted from received video. Once the clusters have been named in the
cluster cache 210, they may be stored in a cluster database 216. The metadata associated with the facial images in the clusters may be updated when previously unknown facial images in the cluster are identified. The cluster metadata may also be updated manually by comparing the cluster images to known reference facial images. Theindex database 218, in an embodiment, may be a database populated with the indexed records of the identified facial images, each facial image's position in the video frame(s) in which it appears, and the number of times (e.g., frames, or collection of frames) or duration the facial image appears in the video. Theindex database 218 may provide searching capabilities to users that enable searching the videos for the appearance of an individual associated with a facial image identified in the index database. Furthermore, in an embodiment,pattern database 220 may be a database populated with reference or representative facial images of clusters that have been identified. Using thepattern database 220, facialimage clustering module 206 can quickly search through all of the clusters available in the cluster database 216. If a facial image or a new cluster closely matches a representative facial image present in theindex database 218,digital media processor 112 may merge the new cluster with the cluster referenced by the representative facial image. -
FIG. 3A is a block diagram of an environment within which a facial clustering module is implemented, in accordance with an embodiment. The components shown include bufferedframe sequence processor 202, facialimage clustering module 206, cluster database 216, andexample clusters 1 through N. Other embodiments of the facialclustering module environment 300 may include more or less components than shown inFIG. 3A . Theenvironment 300 illustrates how the bufferedframe sequence processor 202 contains video clips of varying lengths that include a number of video frames 305 prior to processing. For example, facialimage extraction module 204 identifies sixframes 305, each including at least one facials image. The facial images may be extracted from the frames by the facialimage extraction module 204. In turn, theclustering module 206 process the facial images in each of these groups of video frames 305 from the video clips to determine a corresponding cluster(s) assignment for each facial image (and/or frame containing the facial image therein). For facial images that belong to identified people, the facial image is grouped with the same cluster in cluster database 216. For facial images that belong to unidentified people, facialimage clustering module 206 may create a new cluster (e.g., cluster N). - In some embodiments, multiple people may be present within video clip frames 305. In this scenario, facial
image clustering module 206 may duplicate the frame and cluster each frame with a different cluster. For example, if President Obama and Governor Romney appear in a set of video frames together, facialimage clustering module 206 may group that set video frames under two different clusters. One cluster may have frames with President Obama's facial image while the other cluster may have frames with Governor Romney's facial image. -
FIG. 3B is a block diagram showing data flow and correlations between various components of a media processing system for implementing clustering, in accordance with an embodiment.Media data 250 may be digital content that has been initially processed by the bufferedframe sequence processor 202 and has been split into groups of frames. Alternatively, the facialimage extraction module 204 may receive the media data 250 (e.g., a video stream containing frames) directly. These frames are passed into a facialimage extraction module 204 that filters the frames and determines which frames contain facial images. The facial images appearing in these frames may also be normalized before being passed into a facialimage clustering module 206 that clusters a given facial image with other facial images that the module identifies as a close match. The facialimage clustering module 206 also receives data frompattern database 220 and begins to compare facial images in the formed clusters with template facial images frompattern database 220. - For each cluster formed or merged by facial
image clustering module 206,suggestion engine 208 may label the new clusters based on information associated with template facial images from the pattern database 220 (e.g., in the case of a recognition), contextual data extracted from the video feed by facialimage extraction module 204, or an operator input. The clusters may be stored temporarily incluster cache 210 throughout processing of the receivedmedia data 250 as the individuals identified therein may appear frequently. The facial images in the clusters are stored in cluster database 216 while indexing information (e.g., time intervals that certain faces appear in a video, specific videos that certain faces appear in, and so forth) are stored inindex database 218. Commonly appearing facial images or representative facial images of each cluster is also forwarded from the cluster database 216 and stored inpattern database 220 as a reference for use when facialimage clustering module 206 is processing new video frames. The information inindex database 218 can be searched for by digitalmedia search processor 114. -
FIG. 4 is a block diagram showing various components of a facialimage extraction module 204, in accordance with an embodiment. As shown inFIG. 4 , the facialimage extraction module 204 includespartitioning module 402, detectingmodule 404, discoveringmodule 406, extrapolatingmodule 408, limitingmodule 410, evaluatingmodule 412, and normalizingmodule 414. Other embodiments of facialimage extraction module 204 may contain more or less modules than what is illustrated inFIG. 4 . -
Partitioning module 402, in an embodiment, processes buffered facial image frames from bufferedframe sequence processor 202 by separating the frames out into smaller sized groups. For example, if a video containing 1000 frames is inputted into the bufferframe sequence processor 202, the processor may separate the frames into 10 groups of 100 frames for buffering purposes until the frame sets can be processed by other modules.Partitioning module 402 may separate each group of 100 frames further into groups of 10 or 15 frames each. Furthermore,partitioning module 402 may also separate frames by other factors, such as change of source, change of video resolution, scene change, logical breaks in programming and so forth. By identifying logical breaks between sets of frames,partitioning module 402 prepares the frame sets fordetection module 404 to more efficiently detect facial images in sets of frames. Separating the frames allow more processing to be done in parallel as well as to reduce the workload for each set of frames to be processed by later modules. - Partitioned frame sets may then be transferred to detecting
module 404 for further processing. Detectingmodule 404 may analyze the frames in each set to determine whether a facial image is present in each frame. In an embodiment, detectingmodule 404 may sample frames in a set in order to avoid analyzing each frame individually. For example, detectingmodule 404 may quickly process the first frame in a set partitioned by scene changes to determine whether a face appears in the scene. In an embodiment, detectingmodule 404 may analyze the first and last frames of a set of frames (e.g., between scene changes) for facial images. These frames are thus temporally proximate to each other. Frames that are temporally proximate are within a predetermined number of frames from each other. Analysis of intermediate frames may be performed only in areas close to where facial images are found in the first and last frames to identify facial images. The set of facial images identified are spanned facial images. - Facial images detected may exist in non-contiguous frames. In this scenario, extrapolating
module 408 may be used to extrapolate facial locations across multiple frames positioned between frames containing a detected facial image without directly processing each frame. Extrapolating provides an approximation of facial image positions in the intermediary frames and thus regions likely to contain the same facial image. Regions unlikely to contain a facial image may be omitted from scans, thus reducing the computation load on the processor. - Limiting
module 410 may be used in an embodiment to reduce the total necessary area that needs to be scanned for facial images. Limitingmodule 410 may crop the video frame or otherwise limit detection of facial images to the region identified by the extrapolatingmodule 408. For example, President Obama's face may appear centered in a news video clip. Once extrapolatingmodule 408 has identified a rectangular region near the center of the video frame containing President Obama's face, limitingmodule 410 may restrict detectingmodule 404 from searching outside of the identified rectangular region for facial images. In other embodiments, limitingmodule 410 may still allow detectingmodule 404 to search outside of the identified region for facial images if detectingmodule 404 is unable to find facial images on a first scan. - Detecting
module 404 may detect facial images using various methods. In an embodiment, detectingmodule 404 may detect eyes that appear in frames. Eyes may both indicate whether a facial image appears in each frame as well as the facial image position according to eye pupil centers. Evaluatingmodule 412 may be used to determine the quality of the possible facial images, in accordance with an embodiment. For example, evaluatingmodule 412 may scan each facial image and determine if the distance between the eyes of a facial image appearing in the frame is greater than a predetermined threshold distance. A distance between eyes that is below a certain threshold makes identifying the face unlikely. Thus, frames or regions including faces having a distance between eyes of less than a threshold number may be omitted from further processing. Evaluatingmodule 412 may also scan for certain qualities in a frame that may make later facial normalization processes difficult, such as extremes in brightness levels, odd facial positioning, unreasonable color differences and so forth. These qualities may cause the frame to also be omitted from further processing. - Because facial images may not be oriented in a consistent way throughout the different frames, normalizing
module 414 modifies the facial images so that they are oriented in a similar position to aid in facial image comparisons with template images and with other facial images. Normalization may involve using eye position, as well as other facial features such as nose and mouth, to determine how to properly shift regions of a facial image to orient the facial image in a desired position. For example, normalizingmodule 414 may detect that a person in an image is facing upwards. By using the relative positioning of several facial features, normalizingmodule 414 can digitally shift the face and extrapolate a forward positioned face. In other embodiments, normalizingmodule 414 may shift the face so that it is facing the side or in another position. - In an embodiment, discovering
module 406 may also be analyzing the video frames containing detected facial images for the presence of textual content. The textual content may be helpful in identifying the person associated with the detected facial images. Accordingly, frames including textual content are queued for processing by an optical character recognition (OCR) processor to convert the textual content into digital text. For example, textual content may be present in video feeds as part of subtitles or captions. Detectingmodule 404 scanning through video frames may detect facial images that appear in certain frames. Discoveringmodule 404 may then queue those frames for additional processing through an OCR processor (not shown). The OCR processor may detect the subtitles on each frame and scan them to produce keywords that may contain the identity of the people appearing in the images. -
FIG. 5 is a flow chart of a method for video processing involving facial image extraction and initial cluster, in accordance with an embodiment. In turn, the facialimage clustering module 206 may use the extracted facial image output to generate image clusters. - Digital media, such as video, are received by a buffered
frame sequence processor 202 in adigital media processor 112, which may separate the video into buffered frames. Thedigital media processor 112 then receives 502 the sequence of buffered frames, which may be further partitioned by partitioningmodule 402, and uses detectingmodule 404 to detect 504 facial images in the first and last frames of each set of buffered frames. The facial images in the first and last frames may be temporally proximate. Facialimage extraction module 204 is thus able to determine sets of frames that may have facial images appear. Frame sets that have facial images appear in either the first or last frames, or both the first and last frames may be furthered processed by anextrapolating module 408. Theextrapolating module 408 extrapolates 506 facial images to determine approximate locations in all frames where facial images are likely to appear. - Detecting
module 404 may scan the approximate facial image regions to locate 508 facial images. Frames with facial images may also be queued 510 for an OCR by discoveringmodule 406. Textual data extracted by discoveringmodule 406 and an OCR may provide the identity of faces that appear in those frames. Detectingmodule 404, in coordination with limitingmodule 410 and evaluatingmodule 412, may detect 512 certain facial features (e.g., eyes, nose, mouth, ears, and so forth) as facial “landmarks.” Because facial images should be of a certain size and quality before facial recognition can be carried out with reasonable computing resources, each facial image is analyzed by evaluatingmodule 412. Determining thresholds may differ between different embodiments, but in an embodiment, eyes that are well-detected and have sufficient distance between eyes may be preserved 514 for further processing. Frames that do not meet the thresholds may be omitted. - To efficiently and accurately compare facial images from video frames with reference/template facial images from a
pattern database 220, each extracted facial image should be normalized. In an embodiment, normalizingmodule 414 processes each facial image so that the face is normalized 516 in a horizontal orientation, normalized 518 for lighting intensity, and normalized 520 for scaling (e.g., through normalizing the number of pixels between the eyes). In other embodiments, a different combination of normalizing procedures using steps both listed and not listed in this embodiment may be used to normalize facial images for clustering. It should be noted that even though the procedure described herein relates to detecting and normalizing a human face, a person skilled in the art will understand that similar normalization procedures may be utilized to normalize images of any other object categories including, but not limited to, cars, buildings, animals, helicopters and so forth. Furthermore, it should be noted that the detection techniques described herein may also be utilized to detect other categories of objects. Images determined as valid, or as providing sufficient information for a facial image to be identified, by evaluatingmodule 412 may then be preserved 524 for clustering purposes. Other embodiments may determine video frame validity to preserve 524 for clustering through other means, such as identifying frames proximate to other frames that contain identifiable facial images or containing contextual information relevant to other frames that have identifiable facial images. - Facial image clustering involves taking facial images of people appearing in different frames of a video and grouping them into a “cluster.” Each cluster contains facial images of individuals that have same common trait. For example, a cluster may contain facial images of the same person, or it may contain facial images of people that have specific facial features in common. By forming clusters of similar facial images,
digital media processor 112 is able to more quickly and effectively identify individuals that appear in videos. Grouping like facial images together also reduces the computing resources that have to be devoted to comparing, matching, and identifying facial images by reducing the need to perform intensive computations on every facial image in every video frame. - In an embodiment, facial image clustering occurs as facial
image clustering module 206 is sorting through the sets of video frames from a facial image extraction module. An initial method of separating and partitioning the sets of video frames is by analyzing the frames for changes in scenes in facialimage extraction module 204. Once facial images are extracted from these frames by the facialimage extraction module 204, facialimage clustering module 206 can perform additional analysis on the sets of facial images to cluster images. Facial image movements may also be identified and tracked throughout the scene. Face detection and tracking may include labeling each face with a unique track identifier. By tracking a facial image as it moves around the field of view within a set of frames, facialimage clustering module 206 may determine that the facial images appearing in the different frames belong to the same person and may cluster the frames together. -
FIG. 6 illustrates a clusterizer track, in accordance with an embodiment. As facialimage clustering module 206 identifies and tracks facial images in different frames through time, it may determine a clusterizer track 600. A clusterizer track 600 shows the path that a facial image moves in through a time period spanned by the video frames. For example, a face appears in the 10th frame of a video clip. On the 11th frame, the face may have moved slightly upwards and rightwards. On the 12th frame, the face may have moved slightly farther in the same direction. If the distances between the facial images in each of the frames do not exceed a certain threshold, then facialimage clustering module 206 may determine that the individual facial images belong to the same individual and may group them into the same cluster. However, if the distances between facial images exceed the threshold, then facialimage clustering module 206 may cluster the images into separate clusters. - As new clusters are formed, these clusters are compared with previously formed clusters.
FIG. 7 illustrates an example of merging clusters, in accordance with an embodiment. As shown, each cluster includes one or more “key face” or representative facial image that best represent the facial images in the cluster. Key faces from one cluster may be compared with key faces from other clusters to determine distances between the clusters. As new clusters are created, an unknown key face from the new cluster may be compared with key faces from other clusters. For example, a key face #n is associated with cluster M. Key face #n is compared tokey face # 1,key face # 2,key face # 3,key face # 1, key face #m, key face #p, key face #r, and any other key faces that exist. Distances between key face #n and each of the other faces are calculated. These distances are represented inFIG. 7 by distanceab, where subscript a denotes the source key face and subscript b denotes the compared key face. Facialimage clustering module 206 compares each distance to a threshold value and determines whether two clusters should be merged, should be kept separate, or more calculations should be performed to generate a more certain result. Clusters that are merged may have facial images of the same person while clusters that are separate may have facial images of different people. - Multiple key faces may be selected to represent each cluster due to various factors, which may include different orientations of the face, slight changes in the face over time, slight coloration differences and the like. Each key face adds significant additional information to the cluster for
digital media processor 112 to have available for identifying unknown facial images. By identifying multiple images as key faces,digital media processor 112 increases the probability that an unknown image or cluster may be identified and associated with an individual. Each key face may also be associated with a set of sub-facial images that form a spanned face. Facial images that form the spanned face are additional images that may not add significant information to an existing key face, such as repetitive facial images or duplicate frames. - In an embodiment of
digital media processor 112, facialimage clustering module 206 performs the computations related to clustering. As video frames from a buffer are sent into a facialimage clustering module 206, each frame (or the facial image identified in the frame) is analyzed and grouped into a cluster containing facial images of the same person. Facialimage clustering module 206 also compares these clusters with previously created clusters and merges clusters as necessary. In an embodiment, clusters may be identified according to the person that each contains. In other embodiments, clusters may be identified by some other common traits, which may include facial geometry, eye color, nose structure, hair style, skin color and so forth. Clusters formed by facialimage clustering module 206 are stored in cluster database 216, with indexing information stored inindex database 218. -
FIG. 8 is a block diagram showing various components of a facialimage clustering module 206, in accordance with an embodiment. The facialimage clustering module 206 includes a receivingmodule 802,clusterizer track module 804,quality estimation module 806, collapsingmodule 808, mergingmodule 810, comparingmodule 812,client module 814, assigningmodule 816, associatingmodule 818, and populatingmodule 820. Other embodiments of a facialimage clustering module 206 may include more or less modules than is represented inFIG. 8 . - Images processed by a facial
image extraction module 204 are received by facialimage clustering module 206 using receivingmodule 802. Receivingmodule 802 prepares facial image frames by temporarily storing a certain number of frames before releasing the frames to aclusterizer track module 804, which will identify clusterizer tracks 600. - A
clusterizer track module 804 receives sets of facial images in buffers from a receivingmodule 802, in accordance with an embodiment. Theclusterizer track module 804 selects a representative facial image frame in each buffered set and facial images from frames surrounding it.Clusterizer track module 804 then calculates the distances between the representative facial image and the facial image in other proximate frames. If the distances between the facial images in the frames fall within a specified threshold, then clusterizertrack module 804 may determine that a clusterizer track 600 exists. A clusterizer track 600 outlines the path or region that clusterizertrack module 804 may expect to find facial images in a series of video frames.Clusterizer track module 804 may form clusters from facial images along the same clusterizer track 600. The formation of clusterizer tracks 600 was illustrated earlier inFIG. 6 . - In an embodiment, facial images are analyzed for quality by a
quality estimation module 806. Facial images fromclusterizer track module 804 may be referred to as “crude faces” as they may consist of facial images of varying quality.Quality estimation module 806 performs various procedures, which may include a Fast-Fourier-Transformation (FFT), to determine values for image quality. In an embodiment using FFT, high-pass (HP) and low-pass (LP) components of an image can be calculated. A higher HP-LP ratio indicates that an image contains more sharp edges and is thus not blurred. Each “crude face” is compared against a benchmark quality value to determine whether the image is stored or removed. - Collapsing
module 808 receives sets of “quality images” processed by aquality estimation module 806 and determines a key face among the set, in accordance with an embodiment. The key face is thus a representative face for the cluster, allowing collapsingmodule 808 to “collapse” or reduce the amount of data considered as critical to the cluster. In an embodiment, only the key face is stored and the rest of the faces are considered as spanned face. By representing an entire cluster with a key face,digital media processor 112 can reduce the number of comparisons and thus the computing resources necessary to identify facial images in a video. - Clusters that contain facial images that are similar may be considered for merging. In an embodiment, merging
module 810 compares key faces between the newly formed clusters. If the distances between the key faces fall within a certain threshold, then mergingmodule 810 may combine the clusters containing the compared key faces. However, merging is based on a relatively slow, but accurate, face comparison between the key face of two or more clusters. For example, merging clusters consolidates facial images of the same person so that subsequent facial image identification and comparisons can be performed with few prior clusters needing to be compared. The process merging clusters was illustrated earlier inFIG. 7 . - Once clusters are formed and the merging of clusters is completed, comparing
module 812 in an embodiment compares the facial images in the cluster to reference facial images frompattern database 220. To minimize computing time, a fast and rough comparison may be performed by comparingmodule 812 to identify a set of likely reference facial images and exclude unlikely reference facial images before performing a slower, fine-pass comparison. In an embodiment, comparingmodule 812 automatically performs the comparisons based on distances between a cluster key face and a reference facial image from apattern database 220 and determines acceptable suggestions as to the identity of the facial images in a cluster. - In the scenario that there are no reference facial images from
pattern database 220 that adequately match the key face of a cluster, then facialimage clustering module 206 may, in an embodiment, useclient module 814 to prompt a user or operator for a suggestion. For example, an operator may be provided with an unknown key face along with other extracted contextual information about the key face and asked to identify the person. After the operator visually identifies the facial image,client module 814 can updatepattern database 220 so that the operator is not likely to be prompted in the future for manual identifications of facial images belonging to the same person. - In an embodiment, when a cluster is identified, assigning
module 816 may attach identifying metadata or other information to the cluster. Associatingmodule 818 may also reference index information stored inindex database 218 and associate new cluster data identifiers with the index information stored inindex data base 218. For example, associatingmodule 818 may store metadata relating to, but not limited to, a person's identity, location in the video stream, time of appearance, and spatial location in the frames. In an embodiment, the processed cluster data may then be saved to cluster database 216 by populatingmodule 820. -
FIGS. 9A , 9B, 9C, and 9D illustrate flow diagrams that show a method for clustering facial images, in accordance with an example embodiment. The method may be performed by processing logic that may comprise hardware (e.g., dedicated logic, programmable logic, microcode, etc.), computer program code or modules executed by one or more processors to perform the steps illustrated herein (for example, on a general purpose computer system or computer server system illustrated inFIG. 10 ), or a combination of both. In an example embodiment, the processing logic resides at thedigital media processor 112. The method may be performed by the various modules of a facialimage clustering module 206. To more clearly illustrate the method for clustering facial images,FIGS. 9A , 9B, 9C, and 9D each describe different components. - The method for clusterizing images commences with frame buffering 900A. During
frame buffering 900A, video frames are received 902 and checked forvalidity 904. Valid frames are pushed 906 into a frame buffer for temporary storage. The purpose of the buffer is to collect some quantity of frames to process quickly. The process of receiving and checking the frames is repeated until the frame buffer becomes full 908 of the last frame of the video is received. At this point, the facialimage clustering module 206 proceeds onto theclusterized track processing 900B process, which is illustrated inFIG. 9B . - The embodiment of
clusterized track processing 900B process shown inFIG. 9B may be performed byclusterizer track module 804. Each facial image from a buffer is analyzed to determine if a clusterizer track exists and if the facial image can be related to an existing reference facial image. Through identifying tracks and comparing to prior facial images, facialimage clustering module 206 may decide whether an incoming facial image is inserted into a crude face buffer, incremented into a presence rate, or discarded. A crude face buffer contains unidentified facial images to be further optimized and analyzed at a later point in the process. - In a
clusterized track processing 900B process, each frame in a video buffer contains facial images that are assigned a unique track identifier, which is used to find 914 a clusterizer track. Atoperation 916, for each facial image, if a track is not found, then an incoming facial image (unclustered facial image) is used to establish 918 a new clusterizer track. The unclustered facial image is then added 920 to a crude face buffer before the process repeats again with the next frame in the video feed. - In the scenario that a track is found at
operation 916, then the unclustered facial images are compared to a reference facial image. Theclusterizer track module 804 calculates 922 the distance between the unclustered facial image and a reference face. This process may be performed using an algorithm or an object used to evaluate the similarity of objects. In an embodiment, the distance between the unclustered facial image and a representative facial image is represented by a coefficient of similarity. A higher coefficient value may indicate a greater likelihood that both faces belong to the same cluster. In other embodiments, a discrete-cosine-transformation (DCT) for feature extraction and L1-norm for distance (similarity) calculation, or motion field and affine transformation may be used. Theclusterizer track module 804 should perform comparisons and calculations quickly and with an adequate degree of accuracy so that the facial image verifications can proceed smoothly. - At
operation 924, if the unclustered facial image and the reference image are found to be sufficiently similar (e.g., below threshold 1), then the unclustered facial image may be matched to the reference facial image. Atoperation 928, if a reference to a cluster can then be found 926 for the unclustered facial image (e.g., through association with the reference facial image or through contextual information extracted from the video feed), then the cluster presence rate is thus incremented 930. A cluster presence rate indicates the amount of frames where the object in a cluster has appeared and subsequently been clustered. In an embodiment, the unclustered facial image can then be dropped in part because the unclustered face is too similar to the reference facial image to provide additional recognition information. Atoperation 928, if no references could be found, then the unclustered facial image is inserted 932 into a crude face buffer for later analysis. - At
operation 924, if the unclustered facial image and the reference image are found to be sufficiently distinct (e.g., above threshold 1), then facialimage clustering module 206 may compare the unclustered facial image with the current last facial image (e.g., the previous unclustered facial image from the video frame buffer that was compared and analyzed) and calculate 934 a distance. Atoperation 936, if the distance is above a certain threshold (e.g., threshold 1), then the unclustered facial image is added 938 to the crude face buffer and replaces 940 the current last facial image. Atoperation 936, if the distance is below a certain threshold (e.g., threshold 1), then the unclustered facial image may be assumed to be too similar to the last facial image compared. The unclustered facial image thus offers no additional recognition information and may be discarded. - Once the
clusterized track processing 900B finishes or thecrude face buffer 942 is filled, the process continues onto aface quality evaluation 900C, which is shown inFIG. 9C . During aface quality evaluation 900C, each facial image in the crude faces buffer is evaluated 950 for quality. If the facial image quality is sufficient for spanning a reference face (forming a more complete model of a reference face) or may serve as a quality representative face, the face may be stored 954. In an embodiment, a Fast-Fourier-Transformation (FFT) may be used to determine high-pass (HP) and low-pass (LP) components of an image. The HP and LP components indicate the sharpness of the image; thus, a facial image with the maximum HP-LP ratio may be chosen for the sharpest quality. Quality value indicators may be compared to initial index values set 946 as a benchmark for facial quality. - Quality facial images are analyzed in the face collapsing 900D process to determine whether the face can become stored as a key face for an existing or a new cluster. An embodiment of face collapsing 900D is shown in
FIG. 9C . Each cluster contains a reference to a key face and each key face contains a reference to a cluster. If an existing cluster belonging to a clusterizer track does not have a key face, then it can import a key face from the processed crude face buffer. That facial image thus becomes the representative face for the related sequence of faces in the crude face buffer. If a sequence already has a key face, then that key face and the unclustered facial image are compared to determine which one is more representative of the cluster's images. In one embodiment, only the key face is stored and the rest of the facial images are considered as spanned face. Storing facial images as part of a spanned face rather than as a key face reduces the amount of information needed to be stored. The new key face may then be used to create 962 a new cluster. - In some instances, it may be necessary to merge one or more clusters. For example, new clusters may represent individuals that already have existing clusters in cluster database 216. In an embodiment of cluster merging 900E, a facial
image clustering module 206 may reduce the redundancy present in the database. A merging is based on relatively slow, but accurate, face comparison between the key faces of two clusters. An embodiment of cluster merging 900E is shown inFIG. 9C . In this embodiment, new key faces are compared to existing key faces. By comparing thecalculated distances 968 between the two faces and whether they are from thesame clusterizer track 972, facialimage clustering module 206 may determine whether to merge 970 the clusters. - Once the process of creating and merging clusters is complete, facial
image clustering module 206 may begin to identify the facial images in each cluster through the process of suggestion 900F. An embodiment of the suggestion 900F process is shown inFIG. 9D . To reduce the computational load on a processor and to hasten the comparison process, rough comparisons of cluster images may be compared 976 to image patterns present inpattern database 220. The rough comparison can quickly identify a set of possible reference facial images and exclude unlikely reference facial images before a slower, fine-pass identification 978 takes place. From this fine comparison, only one or very few reference facial images may be identified as being associated with the same person as the facial image in the cluster. - In most scenarios, facial
image cluster module 206 may be able to automatically identify 982 and label 984 the clusters based in part on the distance calculated between the unidentified key face and a reference facial image during the fine comparison. In some embodiments, there may be a list containing a predetermined number of suggestions generated for every facial image. In other embodiments, there may be more than one suggestion method utilized based on different recognition technologies. For example, there may be several different algorithms performing recognition, each calculating distances between the key face in the new cluster and the reference facial images from existing clusters. The precision with which the facial image in existing clusters is identified may depend on the size of thepattern database 220. - However, there may be some scenarios where too many likely suggestions exist for facial
image clustering module 206 to make an automated choice. In this case, an operator may be provided with the facial image for manual identification. For example, cluster database 216 may be empty and accordingly there will be no suggestions generated, or the confidence level of the available suggestions may be insufficient. Once an operator has identified the facial image,pattern database 220 may be updated 986, so that future related images do not require manual identification, and the cluster is labeled 984 appropriately. - Once the cluster is labeled with the correct identification, the cluster database 216 and
index database 218 are updated 988, 990. New cluster images or updated cluster images are stored in cluster database 216 while new or updated references (e.g., links to key faces or associated facial images) are stored inindex database 218. If too many unlabeled clusters exist 992 after the updating process, then manual identification may be performed to identify the clusters and update 986 thepattern database 220 accordingly. -
FIG. 10 shows a diagrammatic representation of a machine in the example form of acomputer system 1000, within which a set of instructions for causing the machine to perform any one or more of the methodologies discussed herein may be executed. In an example embodiment, the machine operates as a stand-alone device or may be connected (e.g., networked) to other machines. In a networked deployment, the machine may operate in the capacity of a server or a client machine in a server-client network environment, or as a peer machine in a peer-to-peer (or distributed) network environment. The machine may be a personal computer, a tablet computer, a wearable computer, a personal digital assistant, a cellular or mobile telephone, a portable music player (e.g., a portable hard drive audio device such as an MP3 player), a web appliance, a gaming device, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine. Furthermore, while only a single machine is illustrated, the term “machine” shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein. - The
example computer system 1000 includes a processor 1002 (e.g., a central processing unit (CPU), a graphics processing unit (GPU) or both), amain memory 1004, and astatic memory 1006, which communicate with each other via abus 1020. Thecomputer system 1000 may further include a graphics display unit 1008 (e.g., a liquid crystal display (LCD), organic light emitting diode (OLED), or a cathode ray tube (CRT)). Thecomputer system 1000 also includes an alphanumeric input device 1010 (e.g., a keyboard), a cursor control device 1012 (e.g., a mouse), adrive unit 1014, a signal generation device 1016 (e.g., a speaker), and anetwork interface device 1018. - The
storage unit 1014 includes a machine-readable medium 1022 on which is stored one or more sets of instructions and data structures (e.g., instructions 1024) embodying or utilized by any one or more of the methodologies or functions described herein. Theinstructions 1024 may also reside, completely or at least partially, within themain memory 1004 and/or within theprocessor 1002 during execution thereof by thecomputer system 1000. Themain memory 1004 and theprocessor 1002 also constitute machine-readable media. - The
instructions 1024 may further be transmitted or received over anetwork 105 via thenetwork interface device 1018 utilizing any one of a number of well-known transfer protocols (e.g., Hyper Text Transfer Protocol (HTTP)). - While the machine-
readable medium 1022 is shown in an example embodiment to be a single medium, the term “machine-readable medium” should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions. The term “machine readable medium” shall also be taken to include any medium that is capable of storing, encoding, or carrying a set of instructions for execution by the machine and that causes the machine to perform any one or more of the methodologies of the present application, or that is capable of storing, encoding, or carrying data structures utilized by or associated with such a set of instructions. The term “machine-readable medium” shall accordingly be taken to include, but not be limited to, solid-state memories, optical and magnetic media, and carrier wave signals. Such media may also include, without limitation, hard disks, floppy disks, flash memory cards, subscriber identity module (SIM) cards, digital video disks, random access memory (RAMs), read only memory (ROMs), and the like. - The example embodiments described herein may be implemented in an operating environment comprising software installed on a computer, in hardware, or in a combination of software and hardware. Thus, a method and system of object recognition and database population for video indexing have been described. Although embodiments have been described with reference to specific example embodiments, it will be evident that various modifications and changes may be made to these example embodiments without departing from the broader spirit and scope of the present application. Accordingly, the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.
- Throughout this specification, plural instances may implement components, operations, or structures described as a single instance. Although individual operations of one or more methods are illustrated and described as separate operations, one or more of the individual operations may be performed concurrently, and nothing requires that the operations be performed in the order illustrated. Structures and functionality presented as separate components in example configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements fall within the scope of the subject matter herein.
- Certain embodiments are described herein as including logic or a number of components, modules, or mechanisms, for example, as illustrated in
FIGS. 1 , 2, 4, 8, and 10. Modules may constitute either software modules (e.g., code embodied on a machine-readable medium or in a transmission signal) or hardware modules. A hardware module is tangible unit capable of performing certain operations and may be configured or arranged in a certain manner. In example embodiments, one or more computer systems (e.g., a standalone, client or server computer system) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) may be configured by software (e.g., an application or application portion) as a hardware module that operates to perform certain operations as described herein. - In various embodiments, a hardware module may be implemented mechanically or electronically. For example, a hardware module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a special-purpose processor, such as a field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC)) to perform certain operations. A hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.
- The various operations of example methods described herein may be performed, at least partially, by one or more processors, e.g.,
processor 1002, that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions. The modules referred to herein may, in some example embodiments, comprise processor-implemented modules. - The one or more processors may also operate to support performance of the relevant operations in a “cloud computing” environment or as a “software as a service” (SaaS). For example, at least some of the operations may be performed by a group of computers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., application program interfaces (APIs).)
- The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the one or more processors or processor-implemented modules may be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm). In other example embodiments, the one or more processors or processor-implemented modules may be distributed across a number of geographic locations.
- Some portions of this specification are presented in terms of algorithms or symbolic representations of operations on data stored as bits or binary digital signals within a machine memory (e.g., a computer memory). These algorithms or symbolic representations are examples of techniques used by those of ordinary skill in the data processing arts to convey the substance of their work to others skilled in the art. As used herein, an “algorithm” is a self-consistent sequence of operations or similar processing leading to a desired result. In this context, algorithms and operations involve physical manipulation of physical quantities. Typically, but not necessarily, such quantities may take the form of electrical, magnetic, or optical signals capable of being stored, accessed, transferred, combined, compared, or otherwise manipulated by a machine. It is convenient at times, principally for reasons of common usage, to refer to such signals using words such as “data,” “content,” “bits,” “values,” “elements,” “symbols,” “characters,” “terms,” “numbers,” “numerals,” or the like. These words, however, are merely convenient labels and are to be associated with appropriate physical quantities.
- Unless specifically stated otherwise, discussions herein using words such as “processing,” “computing,” “calculating,” “determining,” “presenting,” “displaying,” or the like may refer to actions or processes of a machine (e.g., a computer) that manipulates or transforms non-transitory data or media represented as physical or tangible (e.g., electronic, magnetic, or optical) quantities within one or more memories (e.g., volatile memory, non-volatile memory, or a combination thereof), registers, or other machine components that receive, store, transmit, or display information.
- As used herein any reference to “one embodiment” or “an embodiment” means that a particular element, feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. The appearances of the phrase “in one embodiment” in various places in the specification are not necessarily all referring to the same embodiment.
- Some embodiments may be described using the expression “coupled” and “connected” along with their derivatives. For example, some embodiments may be described using the term “coupled” to indicate that two or more elements are in direct physical or electrical contact. The term “coupled,” however, may also mean that two or more elements are not in direct contact with each other, but yet still co-operate or interact with each other. The embodiments are not limited in this context.
- As used herein, the terms “comprises,” “comprising,” “includes,” “including,” “has,” “having” or any other variation thereof, are intended to cover a non-exclusive inclusion. For example, a process, method, article, or apparatus that comprises a list of elements is not necessarily limited to only those elements but may include other elements not expressly listed or inherent to such process, method, article, or apparatus. Further, unless expressly stated to the contrary, “or” refers to an inclusive or and not to an exclusive or. For example, a condition A or B is satisfied by any one of the following: A is true (or present) and B is false (or not present), A is false (or not present) and B is true (or present), and both A and B are true (or present).
- In addition, use of the “a” or “an” are employed to describe elements and components of the embodiments herein. This is done merely for convenience and to give a general sense of the invention. This description should be read to include one or at least one and the singular also includes the plural unless it is obvious that it is meant otherwise. Upon reading this disclosure, those of skill in the art will appreciate still additional alternative structural and functional designs for a system and a process for clustering and identifying facial images in media through the disclosed principles herein. Thus, while particular embodiments and applications have been illustrated and described, it is to be understood that the disclosed embodiments are not limited to the precise construction and components disclosed herein. Various modifications, changes and variations, which will be apparent to persons having skill in the art, may be made in the arrangement, operation and details of the method and apparatus disclosed herein without departing from the scope defined in the appended claims.
Claims (20)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/706,371 US20130148898A1 (en) | 2011-12-09 | 2012-12-06 | Clustering objects detected in video |
PCT/US2012/068346 WO2013086257A1 (en) | 2011-12-09 | 2012-12-07 | Clustering objects detected in video |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201161569168P | 2011-12-09 | 2011-12-09 | |
US13/706,371 US20130148898A1 (en) | 2011-12-09 | 2012-12-06 | Clustering objects detected in video |
Publications (1)
Publication Number | Publication Date |
---|---|
US20130148898A1 true US20130148898A1 (en) | 2013-06-13 |
Family
ID=48572039
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/706,371 Abandoned US20130148898A1 (en) | 2011-12-09 | 2012-12-06 | Clustering objects detected in video |
Country Status (2)
Country | Link |
---|---|
US (1) | US20130148898A1 (en) |
WO (1) | WO2013086257A1 (en) |
Cited By (29)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130136298A1 (en) * | 2011-11-29 | 2013-05-30 | General Electric Company | System and method for tracking and recognizing people |
US20130179832A1 (en) * | 2012-01-11 | 2013-07-11 | Kikin Inc. | Method and apparatus for displaying suggestions to a user of a software application |
US20140086450A1 (en) * | 2012-09-24 | 2014-03-27 | Primax Electronics Ltd. | Facial tracking method |
US20150278977A1 (en) * | 2015-03-25 | 2015-10-01 | Digital Signal Corporation | System and Method for Detecting Potential Fraud Between a Probe Biometric and a Dataset of Biometrics |
US20160078302A1 (en) * | 2014-09-11 | 2016-03-17 | Iomniscient Pty Ltd. | Image management system |
US20160095292A1 (en) * | 2015-09-28 | 2016-04-07 | Hadi Hosseini | Animal muzzle pattern scanning device |
US20160104034A1 (en) * | 2014-10-09 | 2016-04-14 | Sensory, Incorporated | Continuous enrollment for face verification |
EP2975552A3 (en) * | 2014-06-26 | 2016-04-20 | Cisco Technology, Inc. | Entropy-reducing low pass filter for face detection |
FR3031825A1 (en) * | 2015-01-19 | 2016-07-22 | Rizze | METHOD FOR FACIAL RECOGNITION AND INDEXING OF RECOGNIZED PERSONS IN A VIDEO STREAM |
EP3082065A1 (en) * | 2015-04-15 | 2016-10-19 | Cisco Technology, Inc. | Duplicate reduction for face detection |
US20160307068A1 (en) * | 2015-04-15 | 2016-10-20 | Stmicroelectronics S.R.L. | Method of clustering digital images, corresponding system, apparatus and computer program product |
US9842392B2 (en) | 2014-12-15 | 2017-12-12 | Koninklijke Philips N.V. | Device, system and method for skin detection |
US20170357875A1 (en) * | 2016-06-08 | 2017-12-14 | International Business Machines Corporation | Detecting usage of copyrighted video content using object recognition |
US20180068189A1 (en) * | 2016-09-07 | 2018-03-08 | Verint Americas Inc. | System and Method for Searching Video |
CN109492616A (en) * | 2018-11-29 | 2019-03-19 | 成都睿码科技有限责任公司 | A kind of advertisement screen face identification method based on autonomous learning |
US10311290B1 (en) * | 2015-12-29 | 2019-06-04 | Rogue Capital LLC | System and method for generating a facial model |
US10448063B2 (en) * | 2017-02-22 | 2019-10-15 | International Business Machines Corporation | System and method for perspective switching during video access |
US10671713B2 (en) * | 2017-08-14 | 2020-06-02 | Guangdong Oppo Mobile Telecommunications Corp., Ltd. | Method for controlling unlocking and related products |
CN111428590A (en) * | 2020-03-11 | 2020-07-17 | 新华智云科技有限公司 | Video clustering segmentation method and system |
CN111444822A (en) * | 2020-03-24 | 2020-07-24 | 北京奇艺世纪科技有限公司 | Object recognition method and apparatus, storage medium, and electronic apparatus |
CN112364714A (en) * | 2020-10-23 | 2021-02-12 | 岭东核电有限公司 | Face recognition method and device, computer equipment and storage medium |
CN112446902A (en) * | 2020-11-24 | 2021-03-05 | 浙江大华技术股份有限公司 | Method and device for determining abnormality of target vehicle, storage medium, and electronic device |
WO2021050769A1 (en) * | 2019-09-13 | 2021-03-18 | Nec Laboratories America, Inc. | Spatio-temporal interactions for video understanding |
US10997395B2 (en) * | 2017-08-14 | 2021-05-04 | Amazon Technologies, Inc. | Selective identity recognition utilizing object tracking |
CN113011271A (en) * | 2021-02-23 | 2021-06-22 | 北京嘀嘀无限科技发展有限公司 | Method, apparatus, device, medium, and program product for generating and processing image |
CN113873180A (en) * | 2021-08-25 | 2021-12-31 | 广东飞达交通工程有限公司 | Method for repeatedly discovering and merging multiple video detectors in same event |
CN113965772A (en) * | 2021-10-29 | 2022-01-21 | 北京百度网讯科技有限公司 | Live video processing method and device, electronic equipment and storage medium |
US11250244B2 (en) * | 2019-03-11 | 2022-02-15 | Nec Corporation | Online face clustering |
US11784975B1 (en) * | 2021-07-06 | 2023-10-10 | Bank Of America Corporation | Image-based firewall system |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104156417B (en) * | 2014-08-01 | 2019-04-26 | 北京智谷睿拓技术服务有限公司 | Information processing method and equipment |
CN104866194B (en) * | 2015-05-21 | 2018-07-13 | 百度在线网络技术（北京）有限公司 | Image searching method and device |
WO2018131875A1 (en) | 2017-01-11 | 2018-07-19 | Samsung Electronics Co., Ltd. | Display apparatus and method for providing service thereof |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8213689B2 (en) * | 2008-07-14 | 2012-07-03 | Google Inc. | Method and system for automated annotation of persons in video content |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090290791A1 (en) * | 2008-05-20 | 2009-11-26 | Holub Alex David | Automatic tracking of people and bodies in video |
US8351661B2 (en) * | 2009-12-02 | 2013-01-08 | At&T Intellectual Property I, L.P. | System and method to assign a digital image to a face cluster |
-
2012
- 2012-12-06 US US13/706,371 patent/US20130148898A1/en not_active Abandoned
- 2012-12-07 WO PCT/US2012/068346 patent/WO2013086257A1/en active Application Filing
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8213689B2 (en) * | 2008-07-14 | 2012-07-03 | Google Inc. | Method and system for automated annotation of persons in video content |
Cited By (45)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130136298A1 (en) * | 2011-11-29 | 2013-05-30 | General Electric Company | System and method for tracking and recognizing people |
US9798923B2 (en) * | 2011-11-29 | 2017-10-24 | General Electric Company | System and method for tracking and recognizing people |
US20160140386A1 (en) * | 2011-11-29 | 2016-05-19 | General Electric Company | System and method for tracking and recognizing people |
US20130179832A1 (en) * | 2012-01-11 | 2013-07-11 | Kikin Inc. | Method and apparatus for displaying suggestions to a user of a software application |
US20140086450A1 (en) * | 2012-09-24 | 2014-03-27 | Primax Electronics Ltd. | Facial tracking method |
US9171197B2 (en) * | 2012-09-24 | 2015-10-27 | Primax Electronics Ltd. | Facial tracking method |
EP2975552A3 (en) * | 2014-06-26 | 2016-04-20 | Cisco Technology, Inc. | Entropy-reducing low pass filter for face detection |
US9864900B2 (en) | 2014-06-26 | 2018-01-09 | Cisco Technology, Inc. | Entropy-reducing low pass filter for face-detection |
US20160078302A1 (en) * | 2014-09-11 | 2016-03-17 | Iomniscient Pty Ltd. | Image management system |
US9892325B2 (en) * | 2014-09-11 | 2018-02-13 | Iomniscient Pty Ltd | Image management system |
US9430696B2 (en) * | 2014-10-09 | 2016-08-30 | Sensory, Incorporated | Continuous enrollment for face verification |
US20160104034A1 (en) * | 2014-10-09 | 2016-04-14 | Sensory, Incorporated | Continuous enrollment for face verification |
US9842392B2 (en) | 2014-12-15 | 2017-12-12 | Koninklijke Philips N.V. | Device, system and method for skin detection |
FR3031825A1 (en) * | 2015-01-19 | 2016-07-22 | Rizze | METHOD FOR FACIAL RECOGNITION AND INDEXING OF RECOGNIZED PERSONS IN A VIDEO STREAM |
US20150278977A1 (en) * | 2015-03-25 | 2015-10-01 | Digital Signal Corporation | System and Method for Detecting Potential Fraud Between a Probe Biometric and a Dataset of Biometrics |
US10489681B2 (en) * | 2015-04-15 | 2019-11-26 | Stmicroelectronics S.R.L. | Method of clustering digital images, corresponding system, apparatus and computer program product |
US9619696B2 (en) | 2015-04-15 | 2017-04-11 | Cisco Technology, Inc. | Duplicate reduction for face detection |
US20160307068A1 (en) * | 2015-04-15 | 2016-10-20 | Stmicroelectronics S.R.L. | Method of clustering digital images, corresponding system, apparatus and computer program product |
EP3082065A1 (en) * | 2015-04-15 | 2016-10-19 | Cisco Technology, Inc. | Duplicate reduction for face detection |
US9826713B2 (en) * | 2015-09-28 | 2017-11-28 | Hadi Hosseini | Animal muzzle pattern scanning device |
US20160095292A1 (en) * | 2015-09-28 | 2016-04-07 | Hadi Hosseini | Animal muzzle pattern scanning device |
US10311290B1 (en) * | 2015-12-29 | 2019-06-04 | Rogue Capital LLC | System and method for generating a facial model |
US9996769B2 (en) * | 2016-06-08 | 2018-06-12 | International Business Machines Corporation | Detecting usage of copyrighted video content using object recognition |
US20180225546A1 (en) * | 2016-06-08 | 2018-08-09 | International Business Machines Corporation | Detecting usage of copyrighted video content using object recognition |
US11301714B2 (en) * | 2016-06-08 | 2022-04-12 | International Business Machines Corporation | Detecting usage of copyrighted video content using object recognition |
US20170357875A1 (en) * | 2016-06-08 | 2017-12-14 | International Business Machines Corporation | Detecting usage of copyrighted video content using object recognition |
US10579899B2 (en) * | 2016-06-08 | 2020-03-03 | International Business Machines Corporation | Detecting usage of copyrighted video content using object recognition |
US20180068189A1 (en) * | 2016-09-07 | 2018-03-08 | Verint Americas Inc. | System and Method for Searching Video |
US11074458B2 (en) * | 2016-09-07 | 2021-07-27 | Verint Americas Inc. | System and method for searching video |
US10489659B2 (en) * | 2016-09-07 | 2019-11-26 | Verint Americas Inc. | System and method for searching video |
US10448063B2 (en) * | 2017-02-22 | 2019-10-15 | International Business Machines Corporation | System and method for perspective switching during video access |
US10674183B2 (en) | 2017-02-22 | 2020-06-02 | International Business Machines Corporation | System and method for perspective switching during video access |
US10671713B2 (en) * | 2017-08-14 | 2020-06-02 | Guangdong Oppo Mobile Telecommunications Corp., Ltd. | Method for controlling unlocking and related products |
US10997395B2 (en) * | 2017-08-14 | 2021-05-04 | Amazon Technologies, Inc. | Selective identity recognition utilizing object tracking |
CN109492616A (en) * | 2018-11-29 | 2019-03-19 | 成都睿码科技有限责任公司 | A kind of advertisement screen face identification method based on autonomous learning |
US11250244B2 (en) * | 2019-03-11 | 2022-02-15 | Nec Corporation | Online face clustering |
WO2021050769A1 (en) * | 2019-09-13 | 2021-03-18 | Nec Laboratories America, Inc. | Spatio-temporal interactions for video understanding |
CN111428590A (en) * | 2020-03-11 | 2020-07-17 | 新华智云科技有限公司 | Video clustering segmentation method and system |
CN111444822A (en) * | 2020-03-24 | 2020-07-24 | 北京奇艺世纪科技有限公司 | Object recognition method and apparatus, storage medium, and electronic apparatus |
CN112364714A (en) * | 2020-10-23 | 2021-02-12 | 岭东核电有限公司 | Face recognition method and device, computer equipment and storage medium |
CN112446902A (en) * | 2020-11-24 | 2021-03-05 | 浙江大华技术股份有限公司 | Method and device for determining abnormality of target vehicle, storage medium, and electronic device |
CN113011271A (en) * | 2021-02-23 | 2021-06-22 | 北京嘀嘀无限科技发展有限公司 | Method, apparatus, device, medium, and program product for generating and processing image |
US11784975B1 (en) * | 2021-07-06 | 2023-10-10 | Bank Of America Corporation | Image-based firewall system |
CN113873180A (en) * | 2021-08-25 | 2021-12-31 | 广东飞达交通工程有限公司 | Method for repeatedly discovering and merging multiple video detectors in same event |
CN113965772A (en) * | 2021-10-29 | 2022-01-21 | 北京百度网讯科技有限公司 | Live video processing method and device, electronic equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
WO2013086257A1 (en) | 2013-06-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20130148898A1 (en) | Clustering objects detected in video | |
US9323785B2 (en) | Method and system for mobile visual search using metadata and segmentation | |
CN112565825B (en) | Video data processing method, device, equipment and medium | |
US8457368B2 (en) | System and method of object recognition and database population for video indexing | |
US10779037B2 (en) | Method and system for identifying relevant media content | |
US10606887B2 (en) | Providing relevant video scenes in response to a video search query | |
US9176987B1 (en) | Automatic face annotation method and system | |
US10104345B2 (en) | Data-enhanced video viewing system and methods for computer vision processing | |
US8064641B2 (en) | System and method for identifying objects in video | |
US9355330B2 (en) | In-video product annotation with web information mining | |
ES2648368B1 (en) | Video recommendation based on content | |
WO2011090541A2 (en) | Methods for displaying contextually targeted content on a connected television | |
US10380267B2 (en) | System and method for tagging multimedia content elements | |
EP2639745A1 (en) | Object identification in images or image sequences | |
US10210257B2 (en) | Apparatus and method for determining user attention using a deep-content-classification (DCC) system | |
CN111209431A (en) | Video searching method, device, equipment and medium | |
US11537636B2 (en) | System and method for using multimedia content as search queries | |
US20130191368A1 (en) | System and method for using multimedia content as search queries | |
CN112052352A (en) | Video sequencing method, device, server and storage medium | |
CN112069331A (en) | Data processing method, data retrieval method, data processing device, data retrieval device, data processing equipment and storage medium | |
EP2665018A1 (en) | Object identification in images or image sequences | |
JP6496388B2 (en) | Method and system for identifying associated media content | |
CN112927025A (en) | Advertisement pushing method, device, equipment and medium based on big data analysis | |
CN114915826A (en) | Information display method and device, computer equipment and computer readable storage medium | |
GB2570601A (en) | Method and system for identifying relevant media content |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: VIEWDLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MITURA, MICHAEL JASON;MUSANTENKO, YURIY S.;KOVTUN, IVAN;AND OTHERS;SIGNING DATES FROM 20130302 TO 20130322;REEL/FRAME:030075/0669 |
|
AS | Assignment |
Owner name: MOTOROLA MOBILITY LLC, ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:VIEWDLE INC.;REEL/FRAME:034162/0280Effective date: 20141028 |
|
AS | Assignment |
Owner name: GOOGLE TECHNOLOGY HOLDINGS LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA MOBILITY LLC;REEL/FRAME:034275/0004Effective date: 20141028 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |