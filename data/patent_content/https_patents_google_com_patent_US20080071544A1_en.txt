US20080071544A1 - Integrating Voice-Enabled Local Search and Contact Lists - Google Patents
Integrating Voice-Enabled Local Search and Contact Lists Download PDFInfo
- Publication number
- US20080071544A1 US20080071544A1 US11/855,980 US85598007A US2008071544A1 US 20080071544 A1 US20080071544 A1 US 20080071544A1 US 85598007 A US85598007 A US 85598007A US 2008071544 A1 US2008071544 A1 US 2008071544A1
- Authority
- US
- United States
- Prior art keywords
- user
- voice
- search
- contact information
- entity
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/487—Arrangements for providing information services, e.g. recorded voice services or time announcements
- H04M3/493—Interactive information services, e.g. directory enquiries ; Arrangements therefor, e.g. interactive voice response [IVR] systems or voice portals
- H04M3/4938—Interactive information services, e.g. directory enquiries ; Arrangements therefor, e.g. interactive voice response [IVR] systems or voice portals comprising a voice browser which renders and interprets, e.g. VoiceXML
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/487—Arrangements for providing information services, e.g. recorded voice services or time announcements
- H04M3/493—Interactive information services, e.g. directory enquiries ; Arrangements therefor, e.g. interactive voice response [IVR] systems or voice portals
- H04M3/4931—Directory assistance systems
- H04M3/4935—Connection initiated by DAS system
Definitions
- This specification relates to networked searching.
- Such devices may store contacts and scheduling information for users, and may also provide access to the internet in manners similar to desktop computers but with more constrained displays and keyboards or keypads.
- the services can include a mechanism to automatically populate a user's contact list with voice labels corresponding to businesses that the user has reached by voice-browsing a local search service.
- a user may initially search for a business, person, or other entity by providing a verbal search term, and the system to which the user submits the request may deliver a number of results. The user may then verbally select one of the results. With the result selected, data reflecting contact information for the result may be retrieved, the data may be stored in a contacts database associated with the user, and a verbal, or voice, tag, or label, that includes all or part of the initial request may be stored and associated with the contact information.
- the system may readily recognize such a request and may immediately make contact by dialing with the saved contact information (so that follow up selection of a search result will be necessary only the first time, and such later selection may occur like normal voice dialing).
- a user may be permitted to conduct searching verbally for particular people or businesses and may readily add information about those businesses or people into their contact lists so that the businesses or people can be quickly contacted in the future.
- the user may readily associate a voice label to the particular business or person.
- users may more easily locate information in which they are interested, and very easily contact businesses or people associated with that information, both at the time of the initial search and later. Businesses may in turn benefit by having their contact information more readily provided to interested users, and may also more readily target promotional materials to such users based on their needs.
- a computer-implemented method includes receiving a voice search request from a client device, identifying an entity responsive to the voice search request and identifying contact information for the entity, and automatically adding the contact information to a contact list of a user associated with the client device.
- the voice search request may be identified as a local search request.
- the entity responsive to the voice search request can comprise a commercial business.
- the contact information can comprise a telephone number.
- the method comprises storing a voice label in association with the contact information, where the voice label can comprise all or a portion of the received voice search request.
- the method may also include subsequently receiving a voice request matching the voice label and automatically making contact with the entity associated with the voice label.
- the method may include checking for duplicate voice labels and prompting a user to enter an alternative voice label if duplicate labels are identified. Identifying an entity responsive to the voice search request can comprise providing to a user a plurality of responses and receiving from the user a selection of one response from the plurality of responses. Also, the plurality of responses can be provided audibly in series, and the selection is receiving by a user interrupting the providing of the responses.
- the method may additionally include automatically connecting the client device to the entity telephonically.
- the method may comprise presenting the contact information over a network to a user associated with the client device to permit manual editing of the contact information.
- the method can include identifying a user account of a first user who is associated with the client device and a second user who is identified as an acquaintance of the first user, and providing the content information for use by the second user.
- the method can also include receiving a voice label from the second user for the contact information and associating the voice label with the contact information in a database corresponding to the second user.
- the method can additionally comprise transmitting the contact information from a central server to a mobile computing device.
- a computer-implemented method comprises verbally submitting a search request to a central server, automatically connecting telephonically to an entity associated with the search request, and automatically receiving data representing contact information for the entity associated with the search request.
- the method may also comprise verbally selecting a search result from a plurality of aurally presented search results and connecting to the selected search result.
- a computer-implemented system in yet another implementation, includes a client session server configured to prompt a user of a remote client device for input to identify one or more entities the user desires to contact, a dialer to connect the user to a selected entity, and a data channel backend sub-system connected to the client session server and a media relay to communicate contact data and digitized audio to the remote client device.
- the system may also include a search engine to receive search queries converted from audible input to textual form and to provide one or more responsive search results to be presented audibly to the user.
- a computer-implemented system in another implementation, includes a client session server configured to prompt a user of a remote client device for input to identify one or more entities the user desires to contact, a dialer to connect the user to a selected entity, and means for providing contact information to a remote client device based on verbal selection of a contact by a user of the client device.
- the system may further comprise a search engine to receive search queries converted from audible input to textual form and to provide one or more responsive search results to be presented audibly to the user.
- FIG. 1 is an interaction diagram showing an example interaction between a user searching for a business and a voice-enabled service.
- FIG. 2 is a flow chart showing actions for providing information to a user.
- FIG. 3 is a schematic diagram of an example system for providing voice-enabled data access.
- FIG. 4 is an interaction diagram for one system for providing voice-enabled data access.
- FIG. 5 is a conceptual diagram of a system for receiving voice commands.
- FIG. 6 is an example screen shot showing a display of local data from voice-based search.
- FIG. 7 is a schematic diagram of exemplary general computer systems that may be used to carry out the techniques described here.
- Voice-dialing is a convenient way to call people or businesses without having to remember their names: users just speak the name of the person or business they want to reach, and a speech recognition service maps their request to the desired name and/or phone number.
- users are generally limited to calling entities they have explicitly inputted into the system, e.g. by recording a voiceprint for the name, importing email contacts, and/or typing new contacts through some web interface.
- DA Voice-activated Local Search and Directory Assistance
- DA systems provide a generic mechanism by which to access, by phone, any business or person in a country.
- DA systems generally require a dialog between the user and the system before the desired name or phone number can be retrieved.
- typical DA systems will first ask for a city and state, then whether the user desires a business, residential or governmental listing, and then the name of the business. Confirmation questions may be added.
- These systems have an extended coverage, but they can be too cumbersome to be used for each phone call (people don't want to spend three minutes on the phone to connect to their favorite Chinese restaurant to place a take-away order).
- Described here is a particular integration of contact lists and directory assistance.
- the described form of integration may permit a user to select DA listings to be automatically transferred to the user's contact list, based on the user's usage.
- Voice-activated contact lists may come in two flavors. One is integrated on a communication device, as frequently offered with cellular phones. In such a case, speech recognition is typically performed on the device. Voice labels are typically entered on the device, but can be downloaded from a user-specified source. These can be typed names, or voice snippets. The other flavor of voice-dialing is implemented as a network system, and typically hosted by telephone carriers, e.g., Verizon. Users can enter their contacts through a web interface at some site, and call the site's number to then speak the name of the contact they want to be connected to. In such a case, voice recognition is typically server-based. Both approaches require the users to explicitly enter (or import) label/number pairs for the contacts they want to maintain.
- the other type of related technology is directory assistance systems. This is typically hosted by telephone carriers or companies such as TelIMe or Free411 in the United States. These systems aim at making all (or almost all) phone numbers in a country available to the caller. Some of these systems are partially automated with speech recognition software, some are not. They typically rely to some degree on back-off human operators to handle difficult requests. And they typically require a few back-and-forth passages between the user and the system before the user can be connected to the desired destination (or given its number).
- FIG. 1 is an interaction diagram showing an example interaction 100 between a user searching for a business and a voice-enabled service.
- a user may generally enter into an interaction that first follows a directory assistance approach, and then provides a resulting user selection to a user's contact list.
- the contact list may be stored on a central server, or the contact information (and in certain situations, a corresponding voice label) may be transmitted in real time or near real time to the communication device (e.g., smartphone) the user is using to access the system.
- the contact information may be stored centrally by the system, and may be updated to the user's device at a later time, such as when the user logs into an account associated with the system on the internet.
- the user first accesses the system such as by stating a command such as “dialer” and then providing a command like “local search.”
- the first command indicates to the user's client device that it should access the voice-search feature of the system (e.g., on the client device), and the second command is sent to the system as an indicator of which portion of the voice features are to be accessed.
- the central system Upon receiving the “local search” command, the central system responds with “what city and state?” (box 104 ) to prompt the user to provide a city name followed by a state name. In this example, the user responds with “climax Minnesota” (box 106 ), a small town in the Northwest corner of the state.
- the central service may resolve the voice command using standard voice recognition techniques to produce data that matches the city and state name.
- the system may then prompt the user to speak the entity for which it is searching. While the entity may be a person, in this example, the system is configured to ask the user for a business name with the prompt “what business” (box 108 ).
- the user then makes his or her best guess at the business name with “Vern's” (box 110 ).
- the system listens for the response, and upon the user pausing after saying “vern's,” the system decodes the voice command into the text “verns” and searches a database of information for matches or near matches in the relevant area, in a standard manner. In this example, the search returns at least two results, with the top two results being “Vern's Tavern” and “Vern's Service.” Using a voice-generator, the system plays the results back in series, from most relevant to least relevant.
- the system states “Vern's Service.”
- the system waits slightly after reading the first entity name to give the user a chance to select that entity. In this example, the user is silent and waits.
- the system then reads the next entity—“Vern's Service” (box 114 ).
- the user quickly gives a response (which could take the form of a voice response and/or of a pressing of a key on a telephone keypad), here, in the form of saying “That's it” to confirm that the just-read result of “Vern's Service” is the “verns” that the user is seeking to contact.
- the system associated with the voice server Upon receiving user confirmation, the system associated with the voice server identifies contact information for Vern's Service, including by retrieving a telephone number, and begins connecting the user to Vern's Service for a voice conversation, through the standard telephone network or via a VOIP connection, for example.
- the voice server may simultaneously notify the user that a connection is being made, so that the user can expect to next be hearing a telephone ringing in to Vern's Service.
- the voice server may also inform the user that Vern's Service has been added to the user's contact list (box 118 ).
- a contacts management server associated with the voice server may copy contact information such as telephone and fax number, address information, and web site address from a database such as a global contacts database, into a user's personal contacts database (box 120 ).
- pointers to the particular business entity in the general database may be added to the user's contacts database.
- the sound of the user's original search request for “verns” may have initially been stored in a file such as a WMV file, and may now be accessed to attach a voice label to the entry for the entity in the user's contacts database.
- the file may be interpreted in various known manners to provide a fingerprint or grammar for the command so that subsequent contacts entries by the user by voice of “verns” will result in the dialing of the vern's service telephone number, without future need for the user to enter multiple commands and to disambiguate between vern's tavern and vern's service.
- the user may contact vern's service without having to enter a local search application and without having to identify a locale for the request.
- FIG. 2 is a flow chart showing actions for providing information to a user. These actions may be performed, for example, by a server or a system having a number of servers, including a voice server.
- the illustrated process 200 involves identifying entities such as businesses in response to a user's search request, and then automatically making contact information for a selected entity available to the user (i.e., without requiring the user to enter the information or to take multiple steps to copy the information over) such as by adding the contact information for the entity to a contacts database corresponding to the user.
- the system receives a search request.
- the request may, in certain circumstances, be preceded by a command from the user to access a search system.
- a command may be received by an application running on the user's mobile computing device or other computing device, which may cause the application to institute a session with the system.
- the search request may be in the form of a verbal statement or statements.
- the request may be received from the user over a telephone (e.g., traditional and/or VOIP) voice channel and may be interpreted at the system.
- the request may also be received as a file from the user's device.
- reception of the search request may occur by an iterative process. For example, as discussed above, the user may initially identify the type of the search (e.g., local search), may then identify a locale or other parameter for the search, and may then submit the search terms—all verbally.
- the type of the search e.g., local search
- the user may initially identify the type of the search (e.g., local search)
- the system may then transform the request into a more traditional, textual query and generate a search result or results.
- the system may turn each verbal request into text and then may append the various portions of the request in an appropriate manner and submit the textual request to a standard search engine. For example, if the user says “local search,” “Boston Massachusetts,” and “Franklins Pub”, the request may be transformed into the text “franklins pub, boston ma” for submission to a search engine.
- the system may then present the results to the user, such as by playing, via voice synthesis techniques or similar techniques, the results in order to the user over the voice channel.
- the system may wait for a response from the user. If no response is received, the system may play the next result.
- the system may identify contact information for the selected entity.
- the contact information may include a telephone number, and the system may begin connecting the user to the entity by a voice channel (box 208 ).
- the system may identify other contact information, and upon informing the user, may copy the contact information into a database associated with the user (box 208 ).
- the information may be sent via a data channel to the user's device for incorporation into a contacts database on the device.
- a grammar or other information relating to the user's original verbal request in the form of a voice label, may be sent to the user's device also, so that the device may speed dial the contact number when the statement is spoken in the future.
- a user's contact list can grow to contain all the businesses in the immediate ecosystem of the user, in a manner pronounced of different sorts of systems like the addition of autocompletion of “to” names in applications like Google's GMail.
- the weight of various entries in a user's contact list may be maintained according to how frequently they are called by the user. This way, rarely used entries fall off the list after a while. This may allows the speech recognition grammar for a user's list to stay reasonably small, thereby increasing the speech recognition accuracy of the service.
- Web-based editing of the lists may also be made available to a user so that he or she can eliminate, add, or modify entries, or add nicknames for existing entries (e.g. “Little truck child development center” to “little truck”).
- a user may be allowed to record alternative speed dial invocating phrases if they do not like their current phrases. For example, perhaps the user initially became familiar with the “Golden Bowl” restaurant via a local search that started with “Chinese Restaurants.” The user may now prefer to dial the restaurant by saying “Golden Bowl” rather than “Chinese Restaurants.” In such a situation, the contact information page may include an icon that permits a user to voice a new shorthand for the contact. Similar edits may be made when a user wishes to replace a friend's formal name with a nickname.
- a mechanism may also be put in place to prevent the same voice tag, or label, to be created twice for two different numbers (e.g. prevent the tag “starbucks” to be used for two different store locations). For example, if a “starbucks” tag is already used for a store in Mountain View, and the user calls a Starbucks store in Tahoe, the tag “starbucks in tahoe” might be used for the second store.
- the user's contact list may also be auto-populated by a variety of other services such as GoogleTalk, various Google mobile services, and by people calling the user (when Brian calls Francoise, Francoise gets Brian's name inserted in her list so she can call him back).
- additional contact information may be added to a contacts record such as by performing a reverse look-up through a data channel, such as on the internet.
- the reverse lookup may be performed automatically upon receipt of some initial piece of contact information (e.g., to locate more contact information), and the located information may be presented to the user to confirm that it is the information the user wants added to their database. For example, a lawyer looking for legal pundit Arthur Miller will reject information returned for contacting the playwright Arthur Miller. Similar instances can apply when telephone numbers or other contact information is ambiguous and thus returns inapplicable other contact information for the user.
- Users contact lists can also be centralized and can be consolidated across user-specified ani groups.
- a user can group contacts gathered from his or her cellphone with contacts collected from his or her home phone, and can invite their significant other to share their cellphone contacts with the user (and vice-versa). All or some of these contacts (e.g., as selected by the user in a check-off process) can be combined into a centralized contact list that the user can call from any phone.
- Some form of user authentication can also be implemented for privacy reasons. For example, before the user may access a dialer service, the user may be required to log into a central service, such as by a Google or similar login credentialing process.
- FIG. 3 is a schematic diagram of an example system 300 for providing voice-enabled data access.
- the illustrated system 300 is provided as one simplified example to assist in understanding the described features. Other systems are also contemplated.
- the system 300 generally includes one or more clients such as client 302 and a server system 304 .
- the client 302 may take various forms, such as a desktop computer or a portable computing device such as a personal digital assistant or smartphone.
- a mobile device particularly when the input and output is to occur by voice.
- Such a system may permit a user, for example, to locate and contact businesses when their hands and eyes are busy, and then to have the businesses added to their system so that future contacts can occur much more easily.
- the system client 302 generally, according to regular norms, includes a signaling component 306 and a data component 308 .
- the signaling and data components 306 , 308 generally use standard building blocks, with the exception of an added MM module 314 .
- the MM module may take the form of an application or applet that communicates with a search system on an MM server 334 .
- the module 314 may signal to the server 334 that a user is seeking to perform voice-enabled searching, and may instigate processes like those discussed in this document for identifying entities in response to a search request and providing contact information of the entities, and making telephonic contacts with the entities for the client 302 .
- the signaling component 306 may also include a number of standard modules that may be part of a standard internet protocol suite, including an ICE module 310 , a Jingle module 312 , an XMPP module, and a TCP module 318 .
- the ICE module 310 performs actions for the Interactive Connectivity Establishment (ICE) methodology, a methodology for network address translator (NAT) traversal for offer/answer protocols.
- the XMPP module 316 carries out the Extensible Messaging and Presence Protocol, an open, XML-like protocol directed to near-real-time extensible instant messaging and presence information.
- the Jingle module 312 executes negotiation for establishing a session between devices.
- the TCP module 318 executes the well-known Transmission Control Protocol.
- An AMR audio module 320 may encode and/or decode received audio via the Adaptive Multi-Rate technique.
- the RTP module performs the Real-Time Transport Protocol, a standardized packet format for delivering audio and video over the internet.
- the UDP module carries out the User Datagram Protocol, a protocol that permits internet-connected devices to send short messages (datagrams) to on another. In this manner, audio may be received and handled through a data channel.
- Communications between the client 302 and the server system 304 may occur through a network such as the internet 328 .
- data passing between the client 302 and the server system 304 may have network address translation performed (box 326 ) as necessary.
- a front end voice communication module 330 such as that used at talk.google.com, may receive voice communications from users and may provide voice (e.g., machine generated) communication from the system 304 .
- a media relay 332 may be responsible for data transfers other than typical voice communication. Audio received and/or sent through media relay 332 may be handled by an AMR converter 338 and an automatic speech recognizer (ASR) backend 340 .
- the AMR converter 338 may perform AMR conversion and MuLaw encoding.
- the ASR backend may pass transformed speech (such as recognized results) to the MM server 334 for handling in manners like those discussed herein.
- the MM server 334 may be a server programmed to carry out various processes like those discussed here.
- the MM server may instantiate client sessions 336 upon being contacted by an MM module 314 , where each session may track search requests, such as requests voiced by a user, may receive results from a search engine, may provide the results audibly through module 330 , and may receive selections from the results again through module 330 .
- the client sessions 336 can cause contact information to be sent to a client 302 , including a voice label in the form of AMR data or in another form.
- the contact information may also include data such as phone numbers, person or business names, addresses, and other such data in a format that it may be automatically included in a user contacts database.
- FIG. 4 is an interaction diagram for one system for providing voice-enabled data access.
- the diagram shows interactions between a client, an MM-related server, and a media proxy.
- the client initially issues a GET command which causes the MM-related server to communicate with the media proxy to set up a session in a familiar fashion.
- a subsequent GET command from the client causes the client to be directed to communicate using RTP with the media proxy.
- the media proxy then forwards information to and receives information from a module like the ASR back-end 340 described above. In this manner, convenient audio information may be transmitted over a data channel.
- FIG. 5 is a conceptual diagram of a system 500 for receiving voice commands.
- a user of a mobile device 502 is shown communicating local search vocally into their device 502 , including by an interactive process like that discussed above.
- the user is prompted for a locale and a business name, and confirms that they would like data associated with a contact to be sent to their device 502 .
- the data and metadata for an entity may be sent to a phone server 504 , and then to a short message service center (SMSC), which is a standard mechanism for SMS messaging.
- SMS short message service center
- the data can be provided to the device 502 and utilized by a component such as the MM module 314 in FIG. 3 .
- FIG. 6 is a example screen shot showing a display 600 of local data from voice-based search.
- the state of the device in this example is what may take place after a user has voiced a search term and is receiving responses from a central system.
- a speaker 608 is shown as reading off the second search result, a stylist shop known as Larry's Hair Care.
- Visual interaction may also be provided on the display 600 .
- contact information 604 is displayed as each result is played audibly.
- Such information may be provided where the audible channel and the data channel may both provide information to the user immediately (or both types of information are provided by a single channel together).
- Such information may benefit a user in that it may permit the user to more readily determine if the name of the entity being played by the system is actually the entity the user wants (e.g., the user can look at the address to make sure it is really the entity they had in mind).
- a map 606 may provide additional visual feedback, such as by showing all search results, and highlighting each result (here, result 610 is highlighted and indicated as being the second result) as it is played. Also, a number is shown next to each result, so the user may select the result by pressing the corresponding number on their telephone keypad, and be connected without having to wait for the system to read all of the results. Where a map is provided, it may also be used to assist for inputting data. In particular, if a user has a map displayed when they are providing input to a system, the system may identify the area displayed on the map (e.g., by coordinating voice and data channels) so that the user need not explicitly identify an area for a local search.
- the user has previously identified contact information for the user's parents and associated a voice label (“mom and dad at home”) with that information.
- a voice label (“mom and dad at home”) with that information.
- This interaction is similar to the interaction described above for FIG. 1 . Specifically, a user identifies a business for a local search, the system finds one result in this example, and the system automatically dials the entity from the result for the user and adds the contact information for the entity to the user's contact list (either at the central system and/or on the actual user device).
- System enters (sue's indian cuisine,Suels telno) in the user contact list.
- This example is very similar to that discussed in FIG. 1 .
- multiple search results are generated and are played to the user in series until the user indicates a selection of one result.
- This example shows subsequent dialing by a user after information about an entity has been automatically added to the user's contact list.
- the entity may be contacted immediately without the need for a search.
- the user spoke “Indian Restaurants” and the system is later reacted to “Sue's Indian Cuisine.”
- Such a result may occur, for example, by the user, in the interim, editing the voice label (which may be prompted automatically by the system whenever multiple search results are generated) or by using a voice label from a source other than the user.
- locating of particular search results may be a focus.
- Such an interaction may take the form of:
- a user interface implementation like the trivial realization above
- An automated category clustering algorithm that builds a hierarchical tree of clustered category nodes
- a mapping function that evaluates the tree and provides the clustering node priors given the current user cluster request
- a sharding strategy for setting up the speech recognition grammar-pieces that are divided by both geography and by the automated clustering nodes, so that these pieces can be appropriately weighted at run time.
- the first piece is where the user gives a system more data about how the specific business should be clustered. By asking for category information with every query, the system can fall-back to category-only searches when the specific listing request fails.
- the clustering stage allows the system to learn hierarchical and synonomous semantics to associate “italian food” with “italian restaurants”, and to learn that “fine dining’ may include “italian restaurants”.
- the mapping function allows the system to provide node weights for each element of in the hierarchical cluster given a specific category request from the user.
- the sharding mechanism allows the system to quickly assemble and bias the appropriate grammar pieces that the recognizer will search, given the associated node weights.
- One alternative is to divide the problem only by geography. In that case, the potential confusions of the recognition task are much higher, and it is more likely that the systems will have to back off to human operators in order to achieve reasonable performance.
- a touch-tone based spelling mechanism for telephone applications may be used with systems like that described above.
- users can enter letters by pressing the corresponding digit key the appropriate number of times, similar to the multi-tap functionality available on mobile devices. (For example, to enter “a”, the user presses the “2” key once, for “b” twice, etc.) However, instead of seeing the letter appear on the mobile device's screen, the user hears the letter played back over the phone's voice channel via synthesized speech or prerecorded audio.
- Functionality can include the ability to add spaces, delete characters and preview what has already been entered. Such actions may occur using standard keying systems for indicating such editing functions.
- a user may first enter a key press.
- a central server may recognize which key has been pressed in various manners (e.g., DTMF tone) and may generate a voice response corresponding to the command represented by the key press. For example, if “2” is pressed once, the system may say “A”, if “2” is pressed twice, the system may say “B”.
- the system may also complete entries or otherwise disambiguate entries in various manners (e.g., so that multi-tap entry is not required) and may provide guesses about disambiguation audibly. For example, the user may press: “2,” “2,” “5”, and the system may speak the word “ball” or another term that is determined to have a high frequency of use on mobile devices for the entered key combination.
- Multi-tap is generally a clientside mobile device feature. The caller enters characters by pressing the corresponding digit key the appropriate number of times as described above (e.g. to enter “a”, the user presses the “2” key once, for “b” twice, etc.).
- the techniques described above can be implemented in a VoiceXML telephony application for local search by phone.
- the code (both the VoiceXML and the GRXML-based grammar) may include code like that below for the following example.
- FIG. 7 shows an example of a generic computer device 700 and a generic mobile computer device 750 , which may be used with the techniques described here.
- Computing device 700 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- Computing device 750 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- Computing device 700 includes a processor 702 , memory 704 , a storage device 706 , a high-speed interface 708 connecting to memory 704 and high-speed expansion ports 710 , and a low speed interface 712 connecting to low speed bus 714 and storage device 706 .
- Each of the components 702 , 704 , 706 , 708 , 710 , and 712 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 702 can process instructions for execution within the computing device 700 , including instructions stored in the memory 704 or on the storage device 706 to display graphical information for a GUI on an external input/output device, such as display 716 coupled to high speed interface 708 .
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 700 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 704 stores information within the computing device 700 .
- the memory 704 is a volatile memory unit or units.
- the memory 704 is a non-volatile memory unit or units.
- the memory 704 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 706 is capable of providing mass storage for the computing device 700 .
- the storage device 706 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in an information carrier.
- the computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 704 , the storage device 706 , memory on processor 702 , or a propagated signal.
- the high speed controller 708 manages bandwidth-intensive operations for the computing device 700 , while the low speed controller 712 manages lower bandwidth-intensive operations.
- the high-speed controller 708 is coupled to memory 704 , display 716 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 710 , which may accept various expansion cards (not shown).
- low-speed controller 712 is coupled to storage device 706 and low-speed expansion port 714 .
- the low-speed expansion port which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 700 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 720 , or multiple times in a group of such servers. It may also be implemented as part of a rack server system 724 . In addition, it may be implemented in a personal computer such as a laptop computer 722 . Alternatively, components from computing device 700 may be combined with other components in a mobile device (not shown), such as device 750 . Each of such devices may contain one or more of computing device 700 , 750 , and an entire system may be made up of multiple computing devices 700 , 750 communicating with each other.
- Computing device 750 includes a processor 752 , memory 764 , an input/output device such as a display 754 , a communication interface 766 , and a transceiver 768 , among other components.
- the device 750 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage.
- a storage device such as a microdrive or other device, to provide additional storage.
- Each of the components 750 , 752 , 764 , 754 , 766 , and 768 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
- the processor 752 can execute instructions within the computing device 750 , including instructions stored in the memory 764 .
- the processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors.
- the processor may provide, for example, for coordination of the other components of the device 750 , such as control of user interfaces, applications run by device 750 , and wireless communication by device 750 .
- Processor 752 may communicate with a user through control interface 758 and display interface 756 coupled to a display 754 .
- the display 754 may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 756 may comprise appropriate circuitry for driving the display 754 to present graphical and other information to a user.
- the control interface 758 may receive commands from a user and convert them for submission to the processor 752 .
- an external interface 762 may be provide in communication with processor 752 , so as to enable near area communication of device 750 with other devices. External interface 762 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 764 stores information within the computing device 750 .
- the memory 764 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- Expansion memory 774 may also be provided and connected to device 750 through expansion interface 772 , which may include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- expansion memory 774 may provide extra storage space for device 750 , or may also store applications or other information for device 750 .
- expansion memory 774 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- expansion memory 774 may be provide as a security module for device 750 , and may be programmed with instructions that permit secure use of device 750 .
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory, as discussed below.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 764 , expansion memory 774 , memory on processor 752 , or a propagated signal that may be received, for example, over transceiver 768 or external interface 762 .
- Device 750 may communicate wirelessly through communication interface 766 , which may include digital signal processing circuitry where necessary. Communication interface 766 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 768 . In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module 770 may provide additional navigation- and location-related wireless data to device 750 , which may be used as appropriate by applications running on device 750 .
- GPS Global Positioning System
- Device 750 may also communicate audibly using audio codec 760 , which may receive spoken information from a user and convert it to usable digital information. Audio codec 760 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 750 . Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 750 .
- Audio codec 760 may receive spoken information from a user and convert it to usable digital information. Audio codec 760 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 750 . Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 750 .
- the computing device 750 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 780 . It may also be implemented as part of a smartphone 782 , personal digital assistant, or other similar mobile device.
- implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
A computer-implemented method includes receiving a voice search request from a client device, identifying an entity responsive to the voice search request and contact information for the entity, and automatically adding the contact information to a contact list of a user associated with the client device.
Description
- This application claims priority to U.S. Application Ser. No. 60/825,686, filed on Sep. 14, 2006, the contents of which is hereby incorporated by reference.
- This specification relates to networked searching.
- In recent years, people have demanded more and more from their computing devices. With connections to networks such as the internet, more information is available to users upon request, and users want to have access to the data and have it presented in various convenient ways.
- More and more, functionality that was previously available only on fixed, desktop computers, is being made available on mobile devices such as cellular telephones, personal digital assistants, and smartphones. Such devices may store contacts and scheduling information for users, and may also provide access to the internet in manners similar to desktop computers but with more constrained displays and keyboards or keypads.
- This document describes systems and techniques involving voice-activated services that combine local search with contact lists. The services can include a mechanism to automatically populate a user's contact list with voice labels corresponding to businesses that the user has reached by voice-browsing a local search service. For example, a user may initially search for a business, person, or other entity by providing a verbal search term, and the system to which the user submits the request may deliver a number of results. The user may then verbally select one of the results. With the result selected, data reflecting contact information for the result may be retrieved, the data may be stored in a contacts database associated with the user, and a verbal, or voice, tag, or label, that includes all or part of the initial request may be stored and associated with the contact information. In that manner, if the user subsequently speaks the words for the verbal tag, the system may readily recognize such a request and may immediately make contact by dialing with the saved contact information (so that follow up selection of a search result will be necessary only the first time, and such later selection may occur like normal voice dialing).
- The systems and techniques described here may provide one or more advantages. For example, a user may be permitted to conduct searching verbally for particular people or businesses and may readily add information about those businesses or people into their contact lists so that the businesses or people can be quickly contacted in the future. In addition, the user may readily associate a voice label to the particular business or person. In this manner, users may more easily locate information in which they are interested, and very easily contact businesses or people associated with that information, both at the time of the initial search and later. Businesses may in turn benefit by having their contact information more readily provided to interested users, and may also more readily target promotional materials to such users based on their needs.
- In one implementation, a computer-implemented method is disclosed. The method includes receiving a voice search request from a client device, identifying an entity responsive to the voice search request and identifying contact information for the entity, and automatically adding the contact information to a contact list of a user associated with the client device. The voice search request may be identified as a local search request. The entity responsive to the voice search request can comprise a commercial business. Also, the contact information can comprise a telephone number.
- In some aspects, the method comprises storing a voice label in association with the contact information, where the voice label can comprise all or a portion of the received voice search request. The method may also include subsequently receiving a voice request matching the voice label and automatically making contact with the entity associated with the voice label. In addition, the method may include checking for duplicate voice labels and prompting a user to enter an alternative voice label if duplicate labels are identified. Identifying an entity responsive to the voice search request can comprise providing to a user a plurality of responses and receiving from the user a selection of one response from the plurality of responses. Also, the plurality of responses can be provided audibly in series, and the selection is receiving by a user interrupting the providing of the responses.
- In other aspects, the method may additionally include automatically connecting the client device to the entity telephonically. In addition, the method may comprise presenting the contact information over a network to a user associated with the client device to permit manual editing of the contact information. Moreover, the method can include identifying a user account of a first user who is associated with the client device and a second user who is identified as an acquaintance of the first user, and providing the content information for use by the second user. In yet other embodiments, the method can also include receiving a voice label from the second user for the contact information and associating the voice label with the contact information in a database corresponding to the second user. And the method can additionally comprise transmitting the contact information from a central server to a mobile computing device.
- In another implementation, a computer-implemented method is disclosed that comprises verbally submitting a search request to a central server, automatically connecting telephonically to an entity associated with the search request, and automatically receiving data representing contact information for the entity associated with the search request. The method may also comprise verbally selecting a search result from a plurality of aurally presented search results and connecting to the selected search result.
- In yet another implementation, a computer-implemented system is disclosed that includes a client session server configured to prompt a user of a remote client device for input to identify one or more entities the user desires to contact, a dialer to connect the user to a selected entity, and a data channel backend sub-system connected to the client session server and a media relay to communicate contact data and digitized audio to the remote client device. The system may also include a search engine to receive search queries converted from audible input to textual form and to provide one or more responsive search results to be presented audibly to the user.
- In another implementation, a computer-implemented system is disclosed. The system includes a client session server configured to prompt a user of a remote client device for input to identify one or more entities the user desires to contact, a dialer to connect the user to a selected entity, and means for providing contact information to a remote client device based on verbal selection of a contact by a user of the client device. The system may further comprise a search engine to receive search queries converted from audible input to textual form and to provide one or more responsive search results to be presented audibly to the user.
- The details of one or more implementations of the identification and contact management systems and techniques are set forth in the accompanying drawings and the description below. Other features and advantages of the systems and techniques will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is an interaction diagram showing an example interaction between a user searching for a business and a voice-enabled service. -
FIG. 2 is a flow chart showing actions for providing information to a user. -
FIG. 3 is a schematic diagram of an example system for providing voice-enabled data access. -
FIG. 4 is an interaction diagram for one system for providing voice-enabled data access. -
FIG. 5 is a conceptual diagram of a system for receiving voice commands. -
FIG. 6 is an example screen shot showing a display of local data from voice-based search. -
FIG. 7 is a schematic diagram of exemplary general computer systems that may be used to carry out the techniques described here. - Like reference symbols in the various drawings indicate like elements.
- Voice-dialing is a convenient way to call people or businesses without having to remember their names: users just speak the name of the person or business they want to reach, and a speech recognition service maps their request to the desired name and/or phone number. With this type of service, users are generally limited to calling entities they have explicitly inputted into the system, e.g. by recording a voiceprint for the name, importing email contacts, and/or typing new contacts through some web interface. These systems provide a quick, reliable, interface to a small subset of the telephone network.
- On the other end of the spectrum, voice-activated Local Search and Directory Assistance (DA) services provide a generic mechanism by which to access, by phone, any business or person in a country. Because of their extended scope, DA systems generally require a dialog between the user and the system before the desired name or phone number can be retrieved. For example, typical DA systems will first ask for a city and state, then whether the user desires a business, residential or governmental listing, and then the name of the business. Confirmation questions may be added. These systems have an extended coverage, but they can be too cumbersome to be used for each phone call (people don't want to spend three minutes on the phone to connect to their favorite Chinese restaurant to place a take-away order).
- Described here is a particular integration of contact lists and directory assistance. The described form of integration may permit a user to select DA listings to be automatically transferred to the user's contact list, based on the user's usage.
- There are two types of related technologies: voice-activated contact lists, and directory assistance systems. Voice-activated contact lists may come in two flavors. One is integrated on a communication device, as frequently offered with cellular phones. In such a case, speech recognition is typically performed on the device. Voice labels are typically entered on the device, but can be downloaded from a user-specified source. These can be typed names, or voice snippets. The other flavor of voice-dialing is implemented as a network system, and typically hosted by telephone carriers, e.g., Verizon. Users can enter their contacts through a web interface at some site, and call the site's number to then speak the name of the contact they want to be connected to. In such a case, voice recognition is typically server-based. Both approaches require the users to explicitly enter (or import) label/number pairs for the contacts they want to maintain.
- The other type of related technology is directory assistance systems. This is typically hosted by telephone carriers or companies such as TelIMe or Free411 in the United States. These systems aim at making all (or almost all) phone numbers in a country available to the caller. Some of these systems are partially automated with speech recognition software, some are not. They typically rely to some degree on back-off human operators to handle difficult requests. And they typically require a few back-and-forth passages between the user and the system before the user can be connected to the desired destination (or given its number).
-
FIG. 1 is an interaction diagram showing anexample interaction 100 between a user searching for a business and a voice-enabled service. Using the techniques and systems described here, a user may generally enter into an interaction that first follows a directory assistance approach, and then provides a resulting user selection to a user's contact list. - Though not shown here, the contact list may be stored on a central server, or the contact information (and in certain situations, a corresponding voice label) may be transmitted in real time or near real time to the communication device (e.g., smartphone) the user is using to access the system. Alternatively, the contact information may be stored centrally by the system, and may be updated to the user's device at a later time, such as when the user logs into an account associated with the system on the internet.
- Referring to the flow shown in
FIG. 1 , atbox 102, the user first accesses the system such as by stating a command such as “dialer” and then providing a command like “local search.” The first command indicates to the user's client device that it should access the voice-search feature of the system (e.g., on the client device), and the second command is sent to the system as an indicator of which portion of the voice features are to be accessed. - Upon receiving the “local search” command, the central system responds with “what city and state?” (box 104) to prompt the user to provide a city name followed by a state name. In this example, the user responds with “climax Minnesota” (box 106), a small town in the Northwest corner of the state. The central service may resolve the voice command using standard voice recognition techniques to produce data that matches the city and state name. The system may then prompt the user to speak the entity for which it is searching. While the entity may be a person, in this example, the system is configured to ask the user for a business name with the prompt “what business” (box 108).
- The user then makes his or her best guess at the business name with “Vern's” (box 110). The system listens for the response, and upon the user pausing after saying “vern's,” the system decodes the voice command into the text “verns” and searches a database of information for matches or near matches in the relevant area, in a standard manner. In this example, the search returns at least two results, with the top two results being “Vern's Tavern” and “Vern's Service.” Using a voice-generator, the system plays the results back in series, from most relevant to least relevant.
- First, at
box 112, the system states “Vern's Service.” The system waits slightly after reading the first entity name to give the user a chance to select that entity. In this example, the user is silent and waits. The system then reads the next entity—“Vern's Service” (box 114). In this instance, the user quickly gives a response (which could take the form of a voice response and/or of a pressing of a key on a telephone keypad), here, in the form of saying “That's it” to confirm that the just-read result of “Vern's Service” is the “verns” that the user is seeking to contact. - Upon receiving user confirmation, the system associated with the voice server identifies contact information for Vern's Service, including by retrieving a telephone number, and begins connecting the user to Vern's Service for a voice conversation, through the standard telephone network or via a VOIP connection, for example. The voice server may simultaneously notify the user that a connection is being made, so that the user can expect to next be hearing a telephone ringing in to Vern's Service.
- The voice server may also inform the user that Vern's Service has been added to the user's contact list (box 118). Thus, at the same time, a contacts management server associated with the voice server may copy contact information such as telephone and fax number, address information, and web site address from a database such as a global contacts database, into a user's personal contacts database (box 120). Alternatively, pointers to the particular business entity in the general database may be added to the user's contacts database. In addition, the sound of the user's original search request for “verns” may have initially been stored in a file such as a WMV file, and may now be accessed to attach a voice label to the entry for the entity in the user's contacts database. The file may be interpreted in various known manners to provide a fingerprint or grammar for the command so that subsequent contacts entries by the user by voice of “verns” will result in the dialing of the vern's service telephone number, without future need for the user to enter multiple commands and to disambiguate between vern's tavern and vern's service. Also, in certain implementations, the user may contact vern's service without having to enter a local search application and without having to identify a locale for the request.
-
FIG. 2 is a flow chart showing actions for providing information to a user. These actions may be performed, for example, by a server or a system having a number of servers, including a voice server. In general, the illustratedprocess 200 involves identifying entities such as businesses in response to a user's search request, and then automatically making contact information for a selected entity available to the user (i.e., without requiring the user to enter the information or to take multiple steps to copy the information over) such as by adding the contact information for the entity to a contacts database corresponding to the user. - At
box 202, the system receives a search request. The request may, in certain circumstances, be preceded by a command from the user to access a search system. Such a command may be received by an application running on the user's mobile computing device or other computing device, which may cause the application to institute a session with the system. The search request may be in the form of a verbal statement or statements. For example, the request may be received from the user over a telephone (e.g., traditional and/or VOIP) voice channel and may be interpreted at the system. The request may also be received as a file from the user's device. - In certain instances, reception of the search request may occur by an iterative process. For example, as discussed above, the user may initially identify the type of the search (e.g., local search), may then identify a locale or other parameter for the search, and may then submit the search terms—all verbally.
- The system, at
box 204, may then transform the request into a more traditional, textual query and generate a search result or results. For example, the system may turn each verbal request into text and then may append the various portions of the request in an appropriate manner and submit the textual request to a standard search engine. For example, if the user says “local search,” “Boston Massachusetts,” and “Franklins Pub”, the request may be transformed into the text “franklins pub, boston ma” for submission to a search engine. - The system may then present the results to the user, such as by playing, via voice synthesis techniques or similar techniques, the results in order to the user over the voice channel. Upon playing each result, the system may wait for a response from the user. If no response is received, the system may play the next result. When a response is received, the system may identify contact information for the selected entity. The contact information may include a telephone number, and the system may begin connecting the user to the entity by a voice channel (box 208). At the same time, the system may identify other contact information, and upon informing the user, may copy the contact information into a database associated with the user (box 208). In some examples, the information may be sent via a data channel to the user's device for incorporation into a contacts database on the device. Also, a grammar or other information relating to the user's original verbal request, in the form of a voice label, may be sent to the user's device also, so that the device may speed dial the contact number when the statement is spoken in the future. In this manner, a user's contact list can grow to contain all the businesses in the immediate ecosystem of the user, in a manner reminiscent of different sorts of systems like the addition of autocompletion of “to” names in applications like Google's GMail.
- Various additional features may also be included with the techniques described here. For example, the weight of various entries in a user's contact list may be maintained according to how frequently they are called by the user. This way, rarely used entries fall off the list after a while. This may allows the speech recognition grammar for a user's list to stay reasonably small, thereby increasing the speech recognition accuracy of the service.
- Web-based editing of the lists may also be made available to a user so that he or she can eliminate, add, or modify entries, or add nicknames for existing entries (e.g. “Little truck child development center” to “little truck”). In addition, a user may be allowed to record alternative speed dial invocating phrases if they do not like their current phrases. For example, perhaps the user initially became familiar with the “Golden Bowl” restaurant via a local search that started with “Chinese Restaurants.” The user may now prefer to dial the restaurant by saying “Golden Bowl” rather than “Chinese Restaurants.” In such a situation, the contact information page may include an icon that permits a user to voice a new shorthand for the contact. Similar edits may be made when a user wishes to replace a friend's formal name with a nickname.
- A mechanism may also be put in place to prevent the same voice tag, or label, to be created twice for two different numbers (e.g. prevent the tag “starbucks” to be used for two different store locations). For example, if a “starbucks” tag is already used for a store in Mountain View, and the user calls a Starbucks store in Tahoe, the tag “starbucks in tahoe” might be used for the second store.
- The user's contact list may also be auto-populated by a variety of other services such as GoogleTalk, various Google mobile services, and by people calling the user (when Brian calls Francoise, Francoise gets Brian's name inserted in her list so she can call him back). In addition, when a telephone number is acquired, additional contact information may be added to a contacts record such as by performing a reverse look-up through a data channel, such as on the internet. The reverse lookup may be performed automatically upon receipt of some initial piece of contact information (e.g., to locate more contact information), and the located information may be presented to the user to confirm that it is the information the user wants added to their database. For example, a lawyer looking for legal pundit Arthur Miller will reject information returned for contacting the playwright Arthur Miller. Similar instances can apply when telephone numbers or other contact information is ambiguous and thus returns inapplicable other contact information for the user.
- Users contact lists can also be centralized and can be consolidated across user-specified ani groups. E.g., a user can group contacts gathered from his or her cellphone with contacts collected from his or her home phone, and can invite their significant other to share their cellphone contacts with the user (and vice-versa). All or some of these contacts (e.g., as selected by the user in a check-off process) can be combined into a centralized contact list that the user can call from any phone.
- Some form of user authentication can also be implemented for privacy reasons. For example, before the user may access a dialer service, the user may be required to log into a central service, such as by a Google or similar login credentialing process.
-
FIG. 3 is a schematic diagram of anexample system 300 for providing voice-enabled data access. The illustratedsystem 300 is provided as one simplified example to assist in understanding the described features. Other systems are also contemplated. - The
system 300 generally includes one or more clients such asclient 302 and aserver system 304. Theclient 302 may take various forms, such as a desktop computer or a portable computing device such as a personal digital assistant or smartphone. Generally the techniques discussed here may be best implemented on a mobile device, particularly when the input and output is to occur by voice. Such a system may permit a user, for example, to locate and contact businesses when their hands and eyes are busy, and then to have the businesses added to their system so that future contacts can occur much more easily. - The
system client 302 generally, according to regular norms, includes asignaling component 306 and adata component 308. In this implementation, the signaling anddata components MM module 314. The MM module may take the form of an application or applet that communicates with a search system on anMM server 334. In particular, themodule 314 may signal to theserver 334 that a user is seeking to perform voice-enabled searching, and may instigate processes like those discussed in this document for identifying entities in response to a search request and providing contact information of the entities, and making telephonic contacts with the entities for theclient 302. - The
signaling component 306 may also include a number of standard modules that may be part of a standard internet protocol suite, including anICE module 310, a Jingle module 312, an XMPP module, and aTCP module 318. TheICE module 310 performs actions for the Interactive Connectivity Establishment (ICE) methodology, a methodology for network address translator (NAT) traversal for offer/answer protocols. TheXMPP module 316 carries out the Extensible Messaging and Presence Protocol, an open, XML-like protocol directed to near-real-time extensible instant messaging and presence information. The Jingle module 312 executes negotiation for establishing a session between devices. And theTCP module 318 executes the well-known Transmission Control Protocol. - In the data component, which may handle the passing of data such as the passing of contact data to the
client 302 as discussed above, the components may generally be standard components operated in new and different manners. AnAMR audio module 320, may encode and/or decode received audio via the Adaptive Multi-Rate technique. The RTP module performs the Real-Time Transport Protocol, a standardized packet format for delivering audio and video over the internet. The UDP module carries out the User Datagram Protocol, a protocol that permits internet-connected devices to send short messages (datagrams) to on another. In this manner, audio may be received and handled through a data channel. - Communications between the
client 302 and theserver system 304 may occur through a network such as theinternet 328. In addition, data passing between theclient 302 and theserver system 304 may have network address translation performed (box 326) as necessary. - On the
server system 304, a front endvoice communication module 330 such as that used at talk.google.com, may receive voice communications from users and may provide voice (e.g., machine generated) communication from thesystem 304. In a similar manner, amedia relay 332 may be responsible for data transfers other than typical voice communication. Audio received and/or sent throughmedia relay 332 may be handled by anAMR converter 338 and an automatic speech recognizer (ASR)backend 340. TheAMR converter 338 may perform AMR conversion and MuLaw encoding. The ASR backend may pass transformed speech (such as recognized results) to theMM server 334 for handling in manners like those discussed herein. - The
MM server 334 may be a server programmed to carry out various processes like those discussed here. In particular, the MM server may instantiateclient sessions 336 upon being contacted by anMM module 314, where each session may track search requests, such as requests voiced by a user, may receive results from a search engine, may provide the results audibly throughmodule 330, and may receive selections from the results again throughmodule 330. Upon identifying a particular entity from a result, theclient sessions 336 can cause contact information to be sent to aclient 302, including a voice label in the form of AMR data or in another form. The contact information may also include data such as phone numbers, person or business names, addresses, and other such data in a format that it may be automatically included in a user contacts database. -
FIG. 4 is an interaction diagram for one system for providing voice-enabled data access. In general, the diagram shows interactions between a client, an MM-related server, and a media proxy. The client initially issues a GET command which causes the MM-related server to communicate with the media proxy to set up a session in a familiar fashion. A subsequent GET command from the client causes the client to be directed to communicate using RTP with the media proxy. The media proxy then forwards information to and receives information from a module like the ASR back-end 340 described above. In this manner, convenient audio information may be transmitted over a data channel. -
FIG. 5 is a conceptual diagram of asystem 500 for receiving voice commands. In thissystem 500, a user of amobile device 502 is shown communicating local search vocally into theirdevice 502, including by an interactive process like that discussed above. Here, the user is prompted for a locale and a business name, and confirms that they would like data associated with a contact to be sent to theirdevice 502. The data and metadata for an entity may be sent to aphone server 504, and then to a short message service center (SMSC), which is a standard mechanism for SMS messaging. In this example then, the data can be provided to thedevice 502 and utilized by a component such as theMM module 314 inFIG. 3 . -
FIG. 6 is a example screen shot showing adisplay 600 of local data from voice-based search. In particular, the state of the device in this example is what may take place after a user has voiced a search term and is receiving responses from a central system. Aspeaker 608 is shown as reading off the second search result, a stylist shop known as Larry's Hair Care. - Visual interaction may also be provided on the
display 600. In this example, contactinformation 604 is displayed as each result is played audibly. Such information may be provided where the audible channel and the data channel may both provide information to the user immediately (or both types of information are provided by a single channel together). Such information may benefit a user in that it may permit the user to more readily determine if the name of the entity being played by the system is actually the entity the user wants (e.g., the user can look at the address to make sure it is really the entity they had in mind). - A
map 606 may provide additional visual feedback, such as by showing all search results, and highlighting each result (here, result 610 is highlighted and indicated as being the second result) as it is played. Also, a number is shown next to each result, so the user may select the result by pressing the corresponding number on their telephone keypad, and be connected without having to wait for the system to read all of the results. Where a map is provided, it may also be used to assist for inputting data. In particular, if a user has a map displayed when they are providing input to a system, the system may identify the area displayed on the map (e.g., by coordinating voice and data channels) so that the user need not explicitly identify an area for a local search. - Although certain interface interactions were described above, other various interactions may also be employed as follows:
- Action: User calls GoogleOneNumber
-
- system>“dialer . . . ”
- user>Mom and dad at home
- system>“mom and dad at home, connecting” . . . ring ring
- In this interaction, the user has previously identified contact information for the user's parents and associated a voice label (“mom and dad at home”) with that information. Thus, by invoking the dialer and speaking the label, the user may be connected to their parents' home.
- Action: User calls GoogleOneNumber
-
- system>dialer . . .
- user>Local Search
- system>what city and state
- User>Mountain View California
- system>what business
- user>Sue's Indian Cuisine
- system>sue's indian cuisine
- i added—sue's indian cuisine—to your contact list connecting . . . ring ring
- Action: System enters (sue's indian cuisine,Suels telno) in the user contact list
- This interaction is similar to the interaction described above for
FIG. 1 . Specifically, a user identifies a business for a local search, the system finds one result in this example, and the system automatically dials the entity from the result for the user and adds the contact information for the entity to the user's contact list (either at the central system and/or on the actual user device). - Action: User calls GoogleOneNumber
-
- system>dialer . . .
- user>Indian restaurants
- system>i found 6 listings responding to your query
- listing 1: amber india restaurant on west el camino real
- listing 2: shiva's indian restaurant on califomia street
- listing 3: passage to india on west el camino real
- listing 4: sue's indian cuisine
- list . . .
- user>Connect me!
- system>sue's indian cuisine
- i added—sue's indian cuisine—to your contact list . . . connecting . . . ring ring
- Action: System enters (sue's indian cuisine,Suels telno) in the user contact list.
- This example is very similar to that discussed in
FIG. 1 . In particular, multiple search results are generated and are played to the user in series until the user indicates a selection of one result. - Action: User calls GoogleOneNumber
-
- system>dialer . . .
- user>Sue's Indian Cuisine
- system>sue's indian cuisine, connecting . . . ring ring
- This example shows subsequent dialing by a user after information about an entity has been automatically added to the user's contact list. In particular, when the user again speaks a term relating to the entity, the entity may be contacted immediately without the need for a search. Note that under example 2b, the user spoke “Indian Restaurants” and the system is later reacted to “Sue's Indian Cuisine.” Such a result may occur, for example, by the user, in the interim, editing the voice label (which may be prompted automatically by the system whenever multiple search results are generated) or by using a voice label from a source other than the user.
- As noted above, various mechanisms may be used to receive inputs from users and provide contact information to users. For illustration, four such alternatives are described next.
- Alternative 1: Glue together two independent services: DA and Contact Lists. Users call a single number, choose between the contact list and DA applications, but have to go through the lengthy DA dialog each time they want to order a take-away from Sue's Indian Cuisine. This until they manually add Sue's number in their contact list.
- Alternative 2: The same glue-2-services approach may offer various mechanisms to provide users with the contacts they want to add to their contact lists, e.g. sending them emails or SMS with entries to download in their list.
- Alternative 3: Editable, personalized, DA system. In such a system All DA entries are available to the user as a “flat” list of contacts (just business names, and no other dialog states such as “city and state”). This may have the disadvantage of a high ambiguity (how many Starbucks in the US?, which one do I care about?), and low recognition rate (the larger the list if contacts, the more frequently misrecognitions happen).
- Alternative 4: Same as 3 but multimodal, where a user speaks an entry, and browses a list of results to select one. Such an approach is still technically challenging with long result lists. It may also not be usable in eyes-free hands-free scenarios (e.g. while driving).
- In another example, locating of particular search results may be a focus. Such an interaction may take the form of:
-
- system: what city and state?
- caller: palo alto califonia
- system: what type of business or category?
- Caller: italian restaurants
- system: what specific business?
- caller: il fornaio
- system: search results, il fornaio on cowper street, palo alto
- caller: connect me
- There are four main design pieces for carrying on such an approach: (1) A user interface implementation, like the trivial realization above; (2) An automated category clustering algorithm that builds a hierarchical tree of clustered category nodes; (3) A mapping function that evaluates the tree and provides the clustering node priors given the current user cluster request; and (4) A sharding strategy for setting up the speech recognition grammar-pieces that are divided by both geography and by the automated clustering nodes, so that these pieces can be appropriately weighted at run time.
- The first piece is where the user gives a system more data about how the specific business should be clustered. By asking for category information with every query, the system can fall-back to category-only searches when the specific listing request fails. The clustering stage allows the system to learn hierarchical and synonomous semantics to associate “italian food” with “italian restaurants”, and to learn that “fine dining’ may include “italian restaurants”.
- The mapping function allows the system to provide node weights for each element of in the hierarchical cluster given a specific category request from the user. The sharding mechanism allows the system to quickly assemble and bias the appropriate grammar pieces that the recognizer will search, given the associated node weights. One alternative is to divide the problem only by geography. In that case, the potential confusions of the recognition task are much higher, and it is more likely that the systems will have to back off to human operators in order to achieve reasonable performance.
- Another approach more commonly used by most currently planned systems is to ask for a hard decision of yellow-pages (category) vs. white-pages (business listing) before asking for search terms. This approach limits the possibility of using both types of information to improve system performance with business listings. A degenerate case of the current proposal is an initial hard-decision category question that limits the recognition grammar to specific businesses.
- Such an approach will have worse accuracy than the interpolated clustering mechanism proposed here because it doesn't model well the semantic uncertainty of the category, both from the caller's intent and the uncertainty of a hard-decision categorization of any specific business.
- Touch-Tone Based Data Entry with Voice Feedback
- In another embodiment, a touch-tone based spelling mechanism for telephone applications may be used with systems like that described above. Using any type of touch-tone telephone (mobile or landline), users can enter letters by pressing the corresponding digit key the appropriate number of times, similar to the multi-tap functionality available on mobile devices. (For example, to enter “a”, the user presses the “2” key once, for “b” twice, etc.) However, instead of seeing the letter appear on the mobile device's screen, the user hears the letter played back over the phone's voice channel via synthesized speech or prerecorded audio.
- Functionality can include the ability to add spaces, delete characters and preview what has already been entered. Such actions may occur using standard keying systems for indicating such editing functions. Thus, in terms of data flow, a user may first enter a key press. A central server may recognize which key has been pressed in various manners (e.g., DTMF tone) and may generate a voice response corresponding to the command represented by the key press. For example, if “2” is pressed once, the system may say “A”, if “2” is pressed twice, the system may say “B”. The system may also complete entries or otherwise disambiguate entries in various manners (e.g., so that multi-tap entry is not required) and may provide guesses about disambiguation audibly. For example, the user may press: “2,” “2,” “5”, and the system may speak the word “ball” or another term that is determined to have a high frequency of use on mobile devices for the entered key combination.
- Automated, voice-driven directory assistance systems require callers to specify residential and business and listings or categories from a huge index. One major challenge for system quality is the recognition accuracy. Since speech recognition accuracy can never reach 100%, an alternative input mechanism is required. Without one, the system must rely on human intervention (e.g. live operators handling a portion of the calls). The spelling mechanism just described can work on all phones and can potentially eliminate the need for live operators.
- Other techniques may not provide as sufficient of results. For example, predictive dialing is common today for accessing names in company directories (e.g. “Enter the first few letters of the employee's last name. For the letter ‘q’ press 7 . . . ”, etc.) This technique differs from multi-tap in that it allows the caller to press a key just once for any of the corresponding letters. For example, to select “a”, “b” or “c”, the caller would press “2” once. However, predictive dialing only works for relatively small sets (like an employee directory) and is not feasible for business or residential listings; (2) Multi-tap: Multi-tap is generally a clientside mobile device feature. The caller enters characters by pressing the corresponding digit key the appropriate number of times as described above (e.g. to enter “a”, the user presses the “2” key once, for “b” twice, etc.).
- The corresponding characters are rendered graphically on the mobile device's screen. There are two drawbacks to this strategy: (a) since it is client-side, it can be hard to fold it into a server-side, telephony-based application. and (b) it does not work for traditional landline phones when it is client-side.
- The techniques described above can be implemented in a VoiceXML telephony application for local search by phone. The code (both the VoiceXML and the GRXML-based grammar) may include code like that below for the following example.
- DIALOG:
- System: Spell the business name or category on your keypad using multitap. For example, to enter “a” press the 2 key once. To enter “b” press the 2 key twice. To enter “c” press the 2 key three times. When you're finished, press zero. To insert a space,
press 1. To delete a character, press pound. - Caller: (presses “2” three times.)
- System: “C”
- Caller: Caller: (presses “2” once.)
- System: “A”
- Caller: Caller: (presses “2” twice.)
- System: “B”
- Caller: (presses “0”)
- System: “Cab”, Got it. (does search)
- VOICEXML:
-
FIG. 7 shows an example of ageneric computer device 700 and a genericmobile computer device 750, which may be used with the techniques described here.Computing device 700 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.Computing device 750 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. -
Computing device 700 includes a processor 702,memory 704, astorage device 706, a high-speed interface 708 connecting tomemory 704 and high-speed expansion ports 710, and alow speed interface 712 connecting tolow speed bus 714 andstorage device 706. Each of thecomponents computing device 700, including instructions stored in thememory 704 or on thestorage device 706 to display graphical information for a GUI on an external input/output device, such as display 716 coupled tohigh speed interface 708. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 700 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 704 stores information within thecomputing device 700. In one implementation, thememory 704 is a volatile memory unit or units. In another implementation, thememory 704 is a non-volatile memory unit or units. Thememory 704 may also be another form of computer-readable medium, such as a magnetic or optical disk. - The
storage device 706 is capable of providing mass storage for thecomputing device 700. In one implementation, thestorage device 706 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. A computer program product can be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 704, thestorage device 706, memory on processor 702, or a propagated signal. - The
high speed controller 708 manages bandwidth-intensive operations for thecomputing device 700, while thelow speed controller 712 manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only. In one implementation, the high-speed controller 708 is coupled tomemory 704, display 716 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 710, which may accept various expansion cards (not shown). In the implementation, low-speed controller 712 is coupled tostorage device 706 and low-speed expansion port 714. The low-speed expansion port, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 700 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 720, or multiple times in a group of such servers. It may also be implemented as part of a rack server system 724. In addition, it may be implemented in a personal computer such as a laptop computer 722. Alternatively, components fromcomputing device 700 may be combined with other components in a mobile device (not shown), such asdevice 750. Each of such devices may contain one or more ofcomputing device multiple computing devices -
Computing device 750 includes aprocessor 752,memory 764, an input/output device such as adisplay 754, acommunication interface 766, and atransceiver 768, among other components. Thedevice 750 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage. Each of thecomponents - The
processor 752 can execute instructions within thecomputing device 750, including instructions stored in thememory 764. The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. The processor may provide, for example, for coordination of the other components of thedevice 750, such as control of user interfaces, applications run bydevice 750, and wireless communication bydevice 750. -
Processor 752 may communicate with a user throughcontrol interface 758 anddisplay interface 756 coupled to adisplay 754. Thedisplay 754 may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology. Thedisplay interface 756 may comprise appropriate circuitry for driving thedisplay 754 to present graphical and other information to a user. Thecontrol interface 758 may receive commands from a user and convert them for submission to theprocessor 752. In addition, anexternal interface 762 may be provide in communication withprocessor 752, so as to enable near area communication ofdevice 750 with other devices.External interface 762 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used. - The
memory 764 stores information within thecomputing device 750. Thememory 764 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.Expansion memory 774 may also be provided and connected todevice 750 through expansion interface 772, which may include, for example, a SIMM (Single In Line Memory Module) card interface.Such expansion memory 774 may provide extra storage space fordevice 750, or may also store applications or other information fordevice 750. Specifically,expansion memory 774 may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example,expansion memory 774 may be provide as a security module fordevice 750, and may be programmed with instructions that permit secure use ofdevice 750. In addition, secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner. - The memory may include, for example, flash memory and/or NVRAM memory, as discussed below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the
memory 764,expansion memory 774, memory onprocessor 752, or a propagated signal that may be received, for example, overtransceiver 768 orexternal interface 762. -
Device 750 may communicate wirelessly throughcommunication interface 766, which may include digital signal processing circuitry where necessary.Communication interface 766 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 768. In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS (Global Positioning System)receiver module 770 may provide additional navigation- and location-related wireless data todevice 750, which may be used as appropriate by applications running ondevice 750. -
Device 750 may also communicate audibly usingaudio codec 760, which may receive spoken information from a user and convert it to usable digital information.Audio codec 760 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset ofdevice 750. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating ondevice 750. - The
computing device 750 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as acellular telephone 780. It may also be implemented as part of asmartphone 782, personal digital assistant, or other similar mobile device. - Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” “computer-readable medium” refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- The systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other implementations are within the scope of the following claims.
Claims (21)
1. A computer-implemented method, comprising:
receiving a voice search request from a client device;
identifying an entity responsive to the voice search request and identifying contact information for the entity; and
automatically adding the contact information to a contact list of a user associated with the client device.
2. The method of claim 1 , wherein the voice search request is identified as a local search request.
3. The method of claim 1 , wherein the entity responsive to the voice search request comprises a commercial business.
4. The method of claim 1 , wherein the contact information comprises a telephone number.
5. The method of claim 1 , further comprising storing a voice label in association with the contact information.
6. The method of claim 5 , wherein the voice label comprises all or a portion of the received voice search request.
7. The method of claim 5 , further comprising subsequently receiving a voice request matching the voice label and automatically making contact with the entity associated with the voice label.
8. The method of claim 5 , further comprising checking for duplicate voice labels and prompting a user to enter an alternative voice label if duplicate labels are identified.
9. The method of claim 1 , wherein identifying an entity responsive to the voice search request comprises providing to a user a plurality of responses and receiving from the user a selection of one response from the plurality of responses.
10. The method of claim 8 , wherein the plurality of responses is provided audibly in series, and the selection is receiving by a user interrupting the providing of the responses.
11. The method of claim 1 , further comprising automatically connecting the client device to the entity telephonically.
12. The method of claim 1 , further comprising presenting the contact information over a network to a user associated with the client device to permit manual editing of the contact information.
13. The method of claim 1 , further comprising identifying a user account of a first user who is associated with the client device and a second user who is identified as an acquaintance of the first user, and providing the content information for use by the second user.
14. The method of claim 13 , further comprising receiving a voice label from the second user for the contact information and associating the voice label with the contact information in a database corresponding to the second user.
15. The method of claim 1 , further comprising transmitting the contact information from a central server to a mobile computing device.
16. a computer-implemented method, comprising:
verbally submitting a search request to a central server;
automatically connecting telephonically to an entity associated with the search request; and
automatically receiving data representing contact information for the entity associated with the search request.
17. The method of claim 16 , further comprising verbally selecting a search result from a plurality of aurally presented search results and connecting to the selected search result.
18. A computer-implemented system, comprising:
a client session server configured to prompt a user of a remote client device for input to identify one or more entities the user desires to contact;
a dialer to connect the user to a selected entity; and
a data channel backend sub-system connected to the client session server and a media relay to communicate contact data and digitized audio to the remote client device.
19. The system of claim 18 , further comprising a search engine to receive search queries converted from audible input to textual form and to provide one or more responsive search results to be presented audibly to the user.
20. A computer-implemented system, comprising:
a client session server configured to prompt a user of a remote client device for input to identify one or more entities the user desires to contact;
a dialer to connect the user to a selected entity; and
means for providing contact information to a remote client device based on verbal selection of a contact by a user of the client device.
21. The system of claim 20 , further comprising a search engine to receive search queries converted from audible input to textual form and to provide one or more responsive search results to be presented audibly to the user.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US11/855,980 US20080071544A1 (en) | 2006-09-14 | 2007-09-14 | Integrating Voice-Enabled Local Search and Contact Lists |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US82568606P | 2006-09-14 | 2006-09-14 | |
US11/855,980 US20080071544A1 (en) | 2006-09-14 | 2007-09-14 | Integrating Voice-Enabled Local Search and Contact Lists |
Publications (1)
Publication Number | Publication Date |
---|---|
US20080071544A1 true US20080071544A1 (en) | 2008-03-20 |
Family
ID=39020782
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US11/855,980 Abandoned US20080071544A1 (en) | 2006-09-14 | 2007-09-14 | Integrating Voice-Enabled Local Search and Contact Lists |
Country Status (3)
Country | Link |
---|---|
US (1) | US20080071544A1 (en) |
EP (1) | EP2082395A2 (en) |
WO (1) | WO2008034111A2 (en) |
Cited By (204)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090012970A1 (en) * | 2007-07-02 | 2009-01-08 | Dror Daniel Ziv | Root cause analysis using interactive data categorization |
US20090271199A1 (en) * | 2008-04-24 | 2009-10-29 | International Business Machines | Records Disambiguation In A Multimodal Application Operating On A Multimodal Device |
US20100042667A1 (en) * | 2008-08-14 | 2010-02-18 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | System and method for transmitting illusory identification characteristics |
US20100039218A1 (en) * | 2008-08-14 | 2010-02-18 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | System and method for transmitting illusory and non-illusory identification characteristics |
US20100091967A1 (en) * | 2008-10-13 | 2010-04-15 | Nortel Networks Limited | Management of contact lists |
US20100161333A1 (en) * | 2008-12-23 | 2010-06-24 | Ciscotechnology, Inc | Adaptive personal name grammars |
US20100293297A1 (en) * | 2009-05-12 | 2010-11-18 | Muthu Arul Mozhi Perumal | Inter-working between network address type (anat) endpoints and interactive connectivity establishment (ice) endpoints |
US20100318595A1 (en) * | 2008-08-14 | 2010-12-16 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | System and method for conditionally transmitting one or more locum tenentes |
US20110004939A1 (en) * | 2008-08-14 | 2011-01-06 | Searete, LLC, a limited liability corporation of the State of Delaware. | Obfuscating identity of a source entity affiliated with a communiqué in accordance with conditional directive provided by a receiving entity |
US20110004940A1 (en) * | 2008-08-14 | 2011-01-06 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Obfuscating identity of a source entity affiliated with a communiqué in accordance with conditional directive provided by a receiving entity |
US20110015928A1 (en) * | 2009-07-15 | 2011-01-20 | Microsoft Corporation | Combination and federation of local and remote speech recognition |
US20110041185A1 (en) * | 2008-08-14 | 2011-02-17 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Obfuscating identity of a source entity affiliated with a communiqué directed to a receiving user and in accordance with conditional directive provided by the receiving user |
US20110041061A1 (en) * | 2008-08-14 | 2011-02-17 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Obfuscating identity of a source entity affiliated with a communiqué directed to a receiving user and in accordance with conditional directive provided by the receiving user |
US20110046962A1 (en) * | 2009-08-18 | 2011-02-24 | Askey Computer Corp. | Voice triggering control device and method thereof |
US20110083010A1 (en) * | 2008-08-14 | 2011-04-07 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally intercepting data indicating one or more aspects of a communiqué to obfuscate the one or more aspects of the communiqué |
US20110092227A1 (en) * | 2009-10-19 | 2011-04-21 | Prasenjit Phukan | Modification of dynamic contact lists |
US20110093806A1 (en) * | 2008-08-14 | 2011-04-21 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Obfuscating reception of communiqué affiliated with a source entity |
US20110107427A1 (en) * | 2008-08-14 | 2011-05-05 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Obfuscating reception of communiqué affiliated with a source entity in response to receiving information indicating reception of the communiqué |
US20110110518A1 (en) * | 2008-08-14 | 2011-05-12 | Searete Llc | Obfuscating reception of communiqué affiliated with a source entity in response to receiving information indicating reception of the communiqué |
US20110131409A1 (en) * | 2008-08-14 | 2011-06-02 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally intercepting data indicating one or more aspects of a communiqué to obfuscate the one or more aspects of the communiqué |
US20110144980A1 (en) * | 2009-12-11 | 2011-06-16 | General Motors Llc | System and method for updating information in electronic calendars |
US20110144973A1 (en) * | 2009-12-15 | 2011-06-16 | At&T Intellectual Property I, L.P. | System and method for combining geographic metadata in automatic speech recognition language and acoustic models |
US20110154020A1 (en) * | 2008-08-14 | 2011-06-23 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally releasing a communiqué determined to be affiliated with a particular source entity in response to detecting occurrence of one or more environmental aspects |
US20110166974A1 (en) * | 2008-08-14 | 2011-07-07 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally obfuscating one or more secret entities with respect to one or more billing statements related to one or more communiqués addressed to the one or more secret entities |
US20110166972A1 (en) * | 2008-08-14 | 2011-07-07 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally obfuscating one or more secret entities with respect to one or more billing statements |
US20110166973A1 (en) * | 2008-08-14 | 2011-07-07 | Searete Llc | Conditionally obfuscating one or more secret entities with respect to one or more billing statements related to one or more communiqués addressed to the one or more secret entities |
US20110173440A1 (en) * | 2008-08-14 | 2011-07-14 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally releasing a communiqué determined to be affiliated with a particular source entity in response to detecting occurrence of one or more environmental aspects |
US20110179018A1 (en) * | 2010-01-15 | 2011-07-21 | Mihal Lazaridis | Method and device for storing and accessing retail contacts |
US8005897B1 (en) * | 2008-03-21 | 2011-08-23 | Sprint Spectrum L.P. | Contact list client system and method |
US20110258223A1 (en) * | 2010-04-14 | 2011-10-20 | Electronics And Telecommunications Research Institute | Voice-based mobile search apparatus and method |
US20120036121A1 (en) * | 2010-08-06 | 2012-02-09 | Google Inc. | State-dependent Query Response |
US20120077586A1 (en) * | 2008-10-27 | 2012-03-29 | Shervin Pishevar | Apparatuses, methods and systems for an interactive proximity display tether |
US8195457B1 (en) * | 2007-01-05 | 2012-06-05 | Cousins Intellectual Properties, Llc | System and method for automatically sending text of spoken messages in voice conversations with voice over IP software |
US8224907B2 (en) | 2008-08-14 | 2012-07-17 | The Invention Science Fund I, Llc | System and method for transmitting illusory identification characteristics |
US8543407B1 (en) | 2007-10-04 | 2013-09-24 | Great Northern Research, LLC | Speech interface system and method for control and interaction with applications on a computing system |
US8660849B2 (en) | 2010-01-18 | 2014-02-25 | Apple Inc. | Prioritizing selection criteria by automated assistant |
US8677377B2 (en) | 2005-09-08 | 2014-03-18 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US20140129220A1 (en) * | 2011-03-03 | 2014-05-08 | Shilei ZHANG | Speaker and call characteristic sensitive open voice search |
CN103971679A (en) * | 2014-05-28 | 2014-08-06 | 锤子科技（北京）有限公司 | Linkman voice searching method and device and mobile terminal |
US8805690B1 (en) * | 2010-08-05 | 2014-08-12 | Google Inc. | Audio notifications |
US20140229469A1 (en) * | 2010-05-20 | 2014-08-14 | Google Inc. | Automatic dialing of search results |
US20140270258A1 (en) * | 2013-03-15 | 2014-09-18 | Pantech Co., Ltd. | Apparatus and method for executing object using voice command |
US20150066973A1 (en) * | 2013-08-29 | 2015-03-05 | Google Inc. | Modifying Search Results Based on Dismissal Action Associated With One or More of the Search Results |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
CN104680733A (en) * | 2013-11-30 | 2015-06-03 | 徐峥 | Simple household indoor object finding device based on WIFI (wireless fidelity) |
US9190062B2 (en) | 2010-02-25 | 2015-11-17 | Apple Inc. | User profiling for voice input processing |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
US9300784B2 (en) | 2013-06-13 | 2016-03-29 | Apple Inc. | System and method for emergency calls initiated by voice command |
US9330720B2 (en) | 2008-01-03 | 2016-05-03 | Apple Inc. | Methods and apparatus for altering audio output signals |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9368114B2 (en) | 2013-03-14 | 2016-06-14 | Apple Inc. | Context-sensitive handling of interruptions |
US9430463B2 (en) | 2014-05-30 | 2016-08-30 | Apple Inc. | Exemplar-based natural language processing |
US9483461B2 (en) | 2012-03-06 | 2016-11-01 | Apple Inc. | Handling speech synthesis of content for multiple languages |
US9495129B2 (en) | 2012-06-29 | 2016-11-15 | Apple Inc. | Device, method, and user interface for voice-activated navigation and browsing of a document |
US9502031B2 (en) | 2014-05-27 | 2016-11-22 | Apple Inc. | Method for supporting dynamic grammars in WFST-based ASR |
US9535906B2 (en) | 2008-07-31 | 2017-01-03 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US9576574B2 (en) | 2012-09-10 | 2017-02-21 | Apple Inc. | Context-sensitive handling of interruptions by intelligent digital assistant |
US9582608B2 (en) | 2013-06-07 | 2017-02-28 | Apple Inc. | Unified ranking with entropy-weighted information for phrase-based semantic auto-completion |
US9620104B2 (en) | 2013-06-07 | 2017-04-11 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US9620105B2 (en) | 2014-05-15 | 2017-04-11 | Apple Inc. | Analyzing audio input for efficient speech and music recognition |
US9619202B1 (en) * | 2016-07-07 | 2017-04-11 | Intelligently Interactive, Inc. | Voice command-driven database |
US9626955B2 (en) | 2008-04-05 | 2017-04-18 | Apple Inc. | Intelligent text-to-speech conversion |
US9633004B2 (en) | 2014-05-30 | 2017-04-25 | Apple Inc. | Better resolution when referencing to concepts |
US9633674B2 (en) | 2013-06-07 | 2017-04-25 | Apple Inc. | System and method for detecting errors in interactions with a voice-based digital assistant |
US9646614B2 (en) | 2000-03-16 | 2017-05-09 | Apple Inc. | Fast, language-independent method for user authentication by voice |
US9646609B2 (en) | 2014-09-30 | 2017-05-09 | Apple Inc. | Caching apparatus for serving phonetic pronunciations |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
US20170169118A1 (en) * | 2015-12-11 | 2017-06-15 | Blackberry Limited | Populating contact information |
US9697820B2 (en) | 2015-09-24 | 2017-07-04 | Apple Inc. | Unit-selection text-to-speech synthesis using concatenation-sensitive neural networks |
US9697822B1 (en) | 2013-03-15 | 2017-07-04 | Apple Inc. | System and method for updating an adaptive speech recognition model |
US9711141B2 (en) | 2014-12-09 | 2017-07-18 | Apple Inc. | Disambiguating heteronyms in speech synthesis |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US9734193B2 (en) | 2014-05-30 | 2017-08-15 | Apple Inc. | Determining domain salience ranking from ambiguous words in natural speech |
US9760559B2 (en) | 2014-05-30 | 2017-09-12 | Apple Inc. | Predictive text input |
US9785630B2 (en) | 2014-05-30 | 2017-10-10 | Apple Inc. | Text prediction using combined word N-gram and unigram language models |
US9798393B2 (en) | 2011-08-29 | 2017-10-24 | Apple Inc. | Text correction processing |
US9818400B2 (en) | 2014-09-11 | 2017-11-14 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US9842105B2 (en) | 2015-04-16 | 2017-12-12 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US9842101B2 (en) | 2014-05-30 | 2017-12-12 | Apple Inc. | Predictive conversion of language input |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US9865280B2 (en) | 2015-03-06 | 2018-01-09 | Apple Inc. | Structured dictation using intelligent automated assistants |
US20180012595A1 (en) * | 2016-07-07 | 2018-01-11 | Intelligently Interactive, Inc. | Simple affirmative response operating system |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US9886432B2 (en) | 2014-09-30 | 2018-02-06 | Apple Inc. | Parsimonious handling of word inflection via categorical stem + suffix N-gram language models |
US9899019B2 (en) | 2015-03-18 | 2018-02-20 | Apple Inc. | Systems and methods for structured stem and suffix language models |
US9911418B2 (en) | 2011-07-12 | 2018-03-06 | Google Llc | Systems and methods for speech command processing |
US9922642B2 (en) | 2013-03-15 | 2018-03-20 | Apple Inc. | Training an at least partial voice command system |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9953088B2 (en) | 2012-05-14 | 2018-04-24 | Apple Inc. | Crowd sourcing information to fulfill user requests |
US9959870B2 (en) | 2008-12-11 | 2018-05-01 | Apple Inc. | Speech recognition involving a mobile device |
US9966068B2 (en) | 2013-06-08 | 2018-05-08 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US9971774B2 (en) | 2012-09-19 | 2018-05-15 | Apple Inc. | Voice-based media searching |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US10002189B2 (en) | 2007-12-20 | 2018-06-19 | Apple Inc. | Method and apparatus for searching using an active ontology |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
US10057736B2 (en) | 2011-06-03 | 2018-08-21 | Apple Inc. | Active transport based notifications |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US10074360B2 (en) | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10079014B2 (en) | 2012-06-08 | 2018-09-18 | Apple Inc. | Name recognition system |
US10078631B2 (en) | 2014-05-30 | 2018-09-18 | Apple Inc. | Entropy-guided text prediction using combined word and character n-gram language models |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
US10089072B2 (en) | 2016-06-11 | 2018-10-02 | Apple Inc. | Intelligent device arbitration and control |
US10101822B2 (en) | 2015-06-05 | 2018-10-16 | Apple Inc. | Language input correction |
US10127220B2 (en) | 2015-06-04 | 2018-11-13 | Apple Inc. | Language identification from short strings |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10134385B2 (en) | 2012-03-02 | 2018-11-20 | Apple Inc. | Systems and methods for name pronunciation |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10185542B2 (en) | 2013-06-09 | 2019-01-22 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US10186254B2 (en) | 2015-06-07 | 2019-01-22 | Apple Inc. | Context-based endpoint detection |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
US10199051B2 (en) | 2013-02-07 | 2019-02-05 | Apple Inc. | Voice trigger for a digital assistant |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US10241752B2 (en) | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US10255907B2 (en) | 2015-06-07 | 2019-04-09 | Apple Inc. | Automatic accent detection using acoustic models |
US10255566B2 (en) | 2011-06-03 | 2019-04-09 | Apple Inc. | Generating and processing task items that represent tasks to perform |
US10269345B2 (en) | 2016-06-11 | 2019-04-23 | Apple Inc. | Intelligent task discovery |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US10283110B2 (en) | 2009-07-02 | 2019-05-07 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
US10289433B2 (en) | 2014-05-30 | 2019-05-14 | Apple Inc. | Domain specific language for encoding assistant dialog |
US10297253B2 (en) | 2016-06-11 | 2019-05-21 | Apple Inc. | Application integration with a digital assistant |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US20190180743A1 (en) * | 2017-12-13 | 2019-06-13 | Kabushiki Kaisha Toshiba | Dialog system |
US10332518B2 (en) | 2017-05-09 | 2019-06-25 | Apple Inc. | User interface for correcting recognition errors |
US10354011B2 (en) | 2016-06-09 | 2019-07-16 | Apple Inc. | Intelligent automated assistant in a home environment |
US10356243B2 (en) | 2015-06-05 | 2019-07-16 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10410637B2 (en) | 2017-05-12 | 2019-09-10 | Apple Inc. | User-specific acoustic models |
WO2019172954A1 (en) * | 2018-03-07 | 2019-09-12 | Google Llc | Interface for a distributed network system |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US10446141B2 (en) | 2014-08-28 | 2019-10-15 | Apple Inc. | Automatic speech recognition based on user feedback |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10482874B2 (en) | 2017-05-15 | 2019-11-19 | Apple Inc. | Hierarchical belief states for digital assistants |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10496753B2 (en) | 2010-01-18 | 2019-12-03 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US10521466B2 (en) | 2016-06-11 | 2019-12-31 | Apple Inc. | Data driven natural language event detection and classification |
US10540976B2 (en) | 2009-06-05 | 2020-01-21 | Apple Inc. | Contextual voice commands |
US10553209B2 (en) | 2010-01-18 | 2020-02-04 | Apple Inc. | Systems and methods for hands-free notification summaries |
US10552013B2 (en) | 2014-12-02 | 2020-02-04 | Apple Inc. | Data detection |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US10592095B2 (en) | 2014-05-23 | 2020-03-17 | Apple Inc. | Instantaneous speaking of content on touch devices |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10643611B2 (en) | 2008-10-02 | 2020-05-05 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US10659851B2 (en) | 2014-06-30 | 2020-05-19 | Apple Inc. | Real-time digital assistant knowledge updates |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10679605B2 (en) | 2010-01-18 | 2020-06-09 | Apple Inc. | Hands-free list-reading by intelligent automated assistant |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10705794B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10749914B1 (en) | 2007-07-18 | 2020-08-18 | Hammond Development International, Inc. | Method and system for enabling a communication device to remotely execute an application |
US10755703B2 (en) | 2017-05-11 | 2020-08-25 | Apple Inc. | Offline personal assistant |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10762293B2 (en) | 2010-12-22 | 2020-09-01 | Apple Inc. | Using parts-of-speech tagging and named entity recognition for spelling correction |
US10778614B2 (en) | 2018-03-08 | 2020-09-15 | Andre Arzumanyan | Intelligent apparatus and method for responding to text messages |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10789041B2 (en) | 2014-09-12 | 2020-09-29 | Apple Inc. | Dynamic thresholds for always listening speech trigger |
US10789945B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Low-latency intelligent automated assistant |
US10791216B2 (en) | 2013-08-06 | 2020-09-29 | Apple Inc. | Auto-activating smart responses based on activities from remote devices |
US10791176B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10810250B2 (en) * | 2014-02-21 | 2020-10-20 | Apple Inc. | Revisiting content history |
US10810274B2 (en) | 2017-05-15 | 2020-10-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11166127B2 (en) | 2018-03-08 | 2021-11-02 | Andre Arzumanyan | Apparatus and method for voice call initiated texting session |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US11217255B2 (en) | 2017-05-16 | 2022-01-04 | Apple Inc. | Far-field extension for digital assistant services |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US11556919B2 (en) | 2018-03-08 | 2023-01-17 | Andre Arzumanyan | Apparatus and method for payment of a texting session order from an electronic wallet |
US11587559B2 (en) | 2015-09-30 | 2023-02-21 | Apple Inc. | Intelligent device identification |
US11599332B1 (en) | 2007-10-04 | 2023-03-07 | Great Northern Research, LLC | Multiple shell multi faceted graphical user interface |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP2276227A1 (en) | 2009-07-17 | 2011-01-19 | Alcatel Lucent | Device and method for processing user communication data for quick communication with contacts |
WO2018213415A1 (en) * | 2017-05-16 | 2018-11-22 | Apple Inc. | Far-field extension for digital assistant services |
US11586415B1 (en) | 2018-03-15 | 2023-02-21 | Allstate Insurance Company | Processing system having a machine learning engine for providing an output via a digital assistant system |
Citations (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5943611A (en) * | 1995-11-02 | 1999-08-24 | Ericsson Inc. | Cellular radiotelephones including means for generating a search request data signal and receiving a telephone number from a network directory database and related methods |
US6529585B2 (en) * | 1998-08-20 | 2003-03-04 | At&T Corp. | Voice label processing apparatus and method |
US6615176B2 (en) * | 1999-07-13 | 2003-09-02 | International Business Machines Corporation | Speech enabling labeless controls in an existing graphical user interface |
US6829331B2 (en) * | 2001-01-02 | 2004-12-07 | Soundbite Communications, Inc. | Address book for a voice message delivery method and system |
US20050114131A1 (en) * | 2003-11-24 | 2005-05-26 | Kirill Stoimenov | Apparatus and method for voice-tagging lexicon |
US20050154587A1 (en) * | 2003-09-11 | 2005-07-14 | Voice Signal Technologies, Inc. | Voice enabled phone book interface for speaker dependent name recognition and phone number categorization |
US20050197110A1 (en) * | 2004-03-08 | 2005-09-08 | Lucent Technologies Inc. | Method and apparatus for enhanced directory assistance in wireless networks |
US6961414B2 (en) * | 2001-01-31 | 2005-11-01 | Comverse Ltd. | Telephone network-based method and system for automatic insertion of enhanced personal address book contact data |
US20050272415A1 (en) * | 2002-10-01 | 2005-12-08 | Mcconnell Christopher F | System and method for wireless audio communication with a computer |
US20060084414A1 (en) * | 2004-10-15 | 2006-04-20 | Alberth William P Jr | Directory assistance with location information |
US7050834B2 (en) * | 2003-12-30 | 2006-05-23 | Lear Corporation | Vehicular, hands-free telephone system |
US7085929B1 (en) * | 2000-10-11 | 2006-08-01 | Koninklijke Philips Electronics N.V. | Method and apparatus for revocation list management using a contact list having a contact count field |
US7167547B2 (en) * | 2002-03-20 | 2007-01-23 | Bellsouth Intellectual Property Corporation | Personal calendaring, schedules, and notification using directory data |
US7505568B2 (en) * | 2004-02-10 | 2009-03-17 | Call Genie Inc. | Method and system of providing personal and business information |
US7580363B2 (en) * | 2004-08-16 | 2009-08-25 | Nokia Corporation | Apparatus and method for facilitating contact selection in communication devices |
US7836147B2 (en) * | 2001-02-27 | 2010-11-16 | Verizon Data Services Llc | Method and apparatus for address book contact sharing |
US7958151B2 (en) * | 2005-08-02 | 2011-06-07 | Constad Transfer, Llc | Voice operated, matrix-connected, artificially intelligent address book system |
-
2007
- 2007-09-14 US US11/855,980 patent/US20080071544A1/en not_active Abandoned
- 2007-09-14 EP EP07842557A patent/EP2082395A2/en not_active Withdrawn
- 2007-09-14 WO PCT/US2007/078572 patent/WO2008034111A2/en active Application Filing
Patent Citations (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5943611A (en) * | 1995-11-02 | 1999-08-24 | Ericsson Inc. | Cellular radiotelephones including means for generating a search request data signal and receiving a telephone number from a network directory database and related methods |
US6529585B2 (en) * | 1998-08-20 | 2003-03-04 | At&T Corp. | Voice label processing apparatus and method |
US6615176B2 (en) * | 1999-07-13 | 2003-09-02 | International Business Machines Corporation | Speech enabling labeless controls in an existing graphical user interface |
US7085929B1 (en) * | 2000-10-11 | 2006-08-01 | Koninklijke Philips Electronics N.V. | Method and apparatus for revocation list management using a contact list having a contact count field |
US6829331B2 (en) * | 2001-01-02 | 2004-12-07 | Soundbite Communications, Inc. | Address book for a voice message delivery method and system |
US6961414B2 (en) * | 2001-01-31 | 2005-11-01 | Comverse Ltd. | Telephone network-based method and system for automatic insertion of enhanced personal address book contact data |
US7836147B2 (en) * | 2001-02-27 | 2010-11-16 | Verizon Data Services Llc | Method and apparatus for address book contact sharing |
US7167547B2 (en) * | 2002-03-20 | 2007-01-23 | Bellsouth Intellectual Property Corporation | Personal calendaring, schedules, and notification using directory data |
US20050272415A1 (en) * | 2002-10-01 | 2005-12-08 | Mcconnell Christopher F | System and method for wireless audio communication with a computer |
US20050154587A1 (en) * | 2003-09-11 | 2005-07-14 | Voice Signal Technologies, Inc. | Voice enabled phone book interface for speaker dependent name recognition and phone number categorization |
US20050114131A1 (en) * | 2003-11-24 | 2005-05-26 | Kirill Stoimenov | Apparatus and method for voice-tagging lexicon |
US7050834B2 (en) * | 2003-12-30 | 2006-05-23 | Lear Corporation | Vehicular, hands-free telephone system |
US7505568B2 (en) * | 2004-02-10 | 2009-03-17 | Call Genie Inc. | Method and system of providing personal and business information |
US20050197110A1 (en) * | 2004-03-08 | 2005-09-08 | Lucent Technologies Inc. | Method and apparatus for enhanced directory assistance in wireless networks |
US7580363B2 (en) * | 2004-08-16 | 2009-08-25 | Nokia Corporation | Apparatus and method for facilitating contact selection in communication devices |
US20060084414A1 (en) * | 2004-10-15 | 2006-04-20 | Alberth William P Jr | Directory assistance with location information |
US7958151B2 (en) * | 2005-08-02 | 2011-06-07 | Constad Transfer, Llc | Voice operated, matrix-connected, artificially intelligent address book system |
Cited By (325)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9646614B2 (en) | 2000-03-16 | 2017-05-09 | Apple Inc. | Fast, language-independent method for user authentication by voice |
US9501741B2 (en) | 2005-09-08 | 2016-11-22 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US8677377B2 (en) | 2005-09-08 | 2014-03-18 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US10318871B2 (en) | 2005-09-08 | 2019-06-11 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US9117447B2 (en) | 2006-09-08 | 2015-08-25 | Apple Inc. | Using event alert text as input to an automated assistant |
US8930191B2 (en) | 2006-09-08 | 2015-01-06 | Apple Inc. | Paraphrasing of user requests and results by automated digital assistant |
US8942986B2 (en) | 2006-09-08 | 2015-01-27 | Apple Inc. | Determining user intent based on ontologies of domains |
US8195457B1 (en) * | 2007-01-05 | 2012-06-05 | Cousins Intellectual Properties, Llc | System and method for automatically sending text of spoken messages in voice conversations with voice over IP software |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US10568032B2 (en) | 2007-04-03 | 2020-02-18 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US9015194B2 (en) * | 2007-07-02 | 2015-04-21 | Verint Systems Inc. | Root cause analysis using interactive data categorization |
US20090012970A1 (en) * | 2007-07-02 | 2009-01-08 | Dror Daniel Ziv | Root cause analysis using interactive data categorization |
US10917444B1 (en) | 2007-07-18 | 2021-02-09 | Hammond Development International, Inc. | Method and system for enabling a communication device to remotely execute an application |
US11451591B1 (en) | 2007-07-18 | 2022-09-20 | Hammond Development International, Inc. | Method and system for enabling a communication device to remotely execute an application |
US10749914B1 (en) | 2007-07-18 | 2020-08-18 | Hammond Development International, Inc. | Method and system for enabling a communication device to remotely execute an application |
US11599332B1 (en) | 2007-10-04 | 2023-03-07 | Great Northern Research, LLC | Multiple shell multi faceted graphical user interface |
US8543407B1 (en) | 2007-10-04 | 2013-09-24 | Great Northern Research, LLC | Speech interface system and method for control and interaction with applications on a computing system |
US11023513B2 (en) | 2007-12-20 | 2021-06-01 | Apple Inc. | Method and apparatus for searching using an active ontology |
US10002189B2 (en) | 2007-12-20 | 2018-06-19 | Apple Inc. | Method and apparatus for searching using an active ontology |
US10381016B2 (en) | 2008-01-03 | 2019-08-13 | Apple Inc. | Methods and apparatus for altering audio output signals |
US9330720B2 (en) | 2008-01-03 | 2016-05-03 | Apple Inc. | Methods and apparatus for altering audio output signals |
US8005897B1 (en) * | 2008-03-21 | 2011-08-23 | Sprint Spectrum L.P. | Contact list client system and method |
US9865248B2 (en) | 2008-04-05 | 2018-01-09 | Apple Inc. | Intelligent text-to-speech conversion |
US9626955B2 (en) | 2008-04-05 | 2017-04-18 | Apple Inc. | Intelligent text-to-speech conversion |
US9349367B2 (en) * | 2008-04-24 | 2016-05-24 | Nuance Communications, Inc. | Records disambiguation in a multimodal application operating on a multimodal device |
US20090271199A1 (en) * | 2008-04-24 | 2009-10-29 | International Business Machines | Records Disambiguation In A Multimodal Application Operating On A Multimodal Device |
US9535906B2 (en) | 2008-07-31 | 2017-01-03 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US10108612B2 (en) | 2008-07-31 | 2018-10-23 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US8583553B2 (en) | 2008-08-14 | 2013-11-12 | The Invention Science Fund I, Llc | Conditionally obfuscating one or more secret entities with respect to one or more billing statements related to one or more communiqués addressed to the one or more secret entities |
US20110004940A1 (en) * | 2008-08-14 | 2011-01-06 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Obfuscating identity of a source entity affiliated with a communiqué in accordance with conditional directive provided by a receiving entity |
US20100042667A1 (en) * | 2008-08-14 | 2010-02-18 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | System and method for transmitting illusory identification characteristics |
US20100039218A1 (en) * | 2008-08-14 | 2010-02-18 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | System and method for transmitting illusory and non-illusory identification characteristics |
US9641537B2 (en) | 2008-08-14 | 2017-05-02 | Invention Science Fund I, Llc | Conditionally releasing a communiqué determined to be affiliated with a particular source entity in response to detecting occurrence of one or more environmental aspects |
US20110173440A1 (en) * | 2008-08-14 | 2011-07-14 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally releasing a communiqué determined to be affiliated with a particular source entity in response to detecting occurrence of one or more environmental aspects |
US8224907B2 (en) | 2008-08-14 | 2012-07-17 | The Invention Science Fund I, Llc | System and method for transmitting illusory identification characteristics |
US20100318595A1 (en) * | 2008-08-14 | 2010-12-16 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | System and method for conditionally transmitting one or more locum tenentes |
US20110004939A1 (en) * | 2008-08-14 | 2011-01-06 | Searete, LLC, a limited liability corporation of the State of Delaware. | Obfuscating identity of a source entity affiliated with a communiqué in accordance with conditional directive provided by a receiving entity |
US9659188B2 (en) | 2008-08-14 | 2017-05-23 | Invention Science Fund I, Llc | Obfuscating identity of a source entity affiliated with a communiqué directed to a receiving user and in accordance with conditional directive provided by the receiving use |
US20110166973A1 (en) * | 2008-08-14 | 2011-07-07 | Searete Llc | Conditionally obfuscating one or more secret entities with respect to one or more billing statements related to one or more communiqués addressed to the one or more secret entities |
US20110041185A1 (en) * | 2008-08-14 | 2011-02-17 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Obfuscating identity of a source entity affiliated with a communiqué directed to a receiving user and in accordance with conditional directive provided by the receiving user |
US8626848B2 (en) | 2008-08-14 | 2014-01-07 | The Invention Science Fund I, Llc | Obfuscating identity of a source entity affiliated with a communiqué in accordance with conditional directive provided by a receiving entity |
US20110166972A1 (en) * | 2008-08-14 | 2011-07-07 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally obfuscating one or more secret entities with respect to one or more billing statements |
US20110041061A1 (en) * | 2008-08-14 | 2011-02-17 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Obfuscating identity of a source entity affiliated with a communiqué directed to a receiving user and in accordance with conditional directive provided by the receiving user |
US20110166974A1 (en) * | 2008-08-14 | 2011-07-07 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally obfuscating one or more secret entities with respect to one or more billing statements related to one or more communiqués addressed to the one or more secret entities |
US20110154020A1 (en) * | 2008-08-14 | 2011-06-23 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally releasing a communiqué determined to be affiliated with a particular source entity in response to detecting occurrence of one or more environmental aspects |
US20110083010A1 (en) * | 2008-08-14 | 2011-04-07 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally intercepting data indicating one or more aspects of a communiqué to obfuscate the one or more aspects of the communiqué |
US20110093806A1 (en) * | 2008-08-14 | 2011-04-21 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Obfuscating reception of communiqué affiliated with a source entity |
US8730836B2 (en) | 2008-08-14 | 2014-05-20 | The Invention Science Fund I, Llc | Conditionally intercepting data indicating one or more aspects of a communiqué to obfuscate the one or more aspects of the communiqué |
US20110107427A1 (en) * | 2008-08-14 | 2011-05-05 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Obfuscating reception of communiqué affiliated with a source entity in response to receiving information indicating reception of the communiqué |
US8929208B2 (en) | 2008-08-14 | 2015-01-06 | The Invention Science Fund I, Llc | Conditionally releasing a communiqué determined to be affiliated with a particular source entity in response to detecting occurrence of one or more environmental aspects |
US20110110518A1 (en) * | 2008-08-14 | 2011-05-12 | Searete Llc | Obfuscating reception of communiqué affiliated with a source entity in response to receiving information indicating reception of the communiqué |
US20110131409A1 (en) * | 2008-08-14 | 2011-06-02 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Conditionally intercepting data indicating one or more aspects of a communiqué to obfuscate the one or more aspects of the communiqué |
US8850044B2 (en) | 2008-08-14 | 2014-09-30 | The Invention Science Fund I, Llc | Obfuscating identity of a source entity affiliated with a communique in accordance with conditional directive provided by a receiving entity |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10643611B2 (en) | 2008-10-02 | 2020-05-05 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US20100091967A1 (en) * | 2008-10-13 | 2010-04-15 | Nortel Networks Limited | Management of contact lists |
US9232060B2 (en) * | 2008-10-13 | 2016-01-05 | Avaya Inc. | Management of contact lists |
US20120077586A1 (en) * | 2008-10-27 | 2012-03-29 | Shervin Pishevar | Apparatuses, methods and systems for an interactive proximity display tether |
US9959870B2 (en) | 2008-12-11 | 2018-05-01 | Apple Inc. | Speech recognition involving a mobile device |
US20100161333A1 (en) * | 2008-12-23 | 2010-06-24 | Ciscotechnology, Inc | Adaptive personal name grammars |
US8185660B2 (en) | 2009-05-12 | 2012-05-22 | Cisco Technology, Inc. | Inter-working between network address type (ANAT) endpoints and interactive connectivity establishment (ICE) endpoints |
US8312169B2 (en) | 2009-05-12 | 2012-11-13 | Cisco Technology, Inc. | Inter-working between network address type (ANAT) endpoints and interactive connectivity establishment (ICE) endpoints |
US20100293297A1 (en) * | 2009-05-12 | 2010-11-18 | Muthu Arul Mozhi Perumal | Inter-working between network address type (anat) endpoints and interactive connectivity establishment (ice) endpoints |
US11080012B2 (en) | 2009-06-05 | 2021-08-03 | Apple Inc. | Interface for a virtual digital assistant |
US10795541B2 (en) | 2009-06-05 | 2020-10-06 | Apple Inc. | Intelligent organization of tasks items |
US10475446B2 (en) | 2009-06-05 | 2019-11-12 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US10540976B2 (en) | 2009-06-05 | 2020-01-21 | Apple Inc. | Contextual voice commands |
US10283110B2 (en) | 2009-07-02 | 2019-05-07 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
US20110015928A1 (en) * | 2009-07-15 | 2011-01-20 | Microsoft Corporation | Combination and federation of local and remote speech recognition |
US8892439B2 (en) | 2009-07-15 | 2014-11-18 | Microsoft Corporation | Combination and federation of local and remote speech recognition |
US20110046962A1 (en) * | 2009-08-18 | 2011-02-24 | Askey Computer Corp. | Voice triggering control device and method thereof |
US8437779B2 (en) | 2009-10-19 | 2013-05-07 | Google Inc. | Modification of dynamic contact lists |
US20110092227A1 (en) * | 2009-10-19 | 2011-04-21 | Prasenjit Phukan | Modification of dynamic contact lists |
US20110144980A1 (en) * | 2009-12-11 | 2011-06-16 | General Motors Llc | System and method for updating information in electronic calendars |
US8868427B2 (en) * | 2009-12-11 | 2014-10-21 | General Motors Llc | System and method for updating information in electronic calendars |
US20110144973A1 (en) * | 2009-12-15 | 2011-06-16 | At&T Intellectual Property I, L.P. | System and method for combining geographic metadata in automatic speech recognition language and acoustic models |
US10121468B2 (en) | 2009-12-15 | 2018-11-06 | Nuance Communications, Inc. | System and method for combining geographic metadata in automatic speech recognition language and acoustic models |
US8892443B2 (en) | 2009-12-15 | 2014-11-18 | At&T Intellectual Property I, L.P. | System and method for combining geographic metadata in automatic speech recognition language and acoustic models |
US9373326B2 (en) | 2009-12-15 | 2016-06-21 | At&T Intellectual Property I, L.P. | System and method for combining geographic metadata in automatic speech recognition language and acoustic models |
US8533186B2 (en) | 2010-01-15 | 2013-09-10 | Blackberry Limited | Method and device for storing and accessing retail contacts |
US20110179018A1 (en) * | 2010-01-15 | 2011-07-21 | Mihal Lazaridis | Method and device for storing and accessing retail contacts |
US10679605B2 (en) | 2010-01-18 | 2020-06-09 | Apple Inc. | Hands-free list-reading by intelligent automated assistant |
US8799000B2 (en) | 2010-01-18 | 2014-08-05 | Apple Inc. | Disambiguation based on active input elicitation by intelligent automated assistant |
US8660849B2 (en) | 2010-01-18 | 2014-02-25 | Apple Inc. | Prioritizing selection criteria by automated assistant |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US8670979B2 (en) | 2010-01-18 | 2014-03-11 | Apple Inc. | Active input elicitation by intelligent automated assistant |
US8706503B2 (en) | 2010-01-18 | 2014-04-22 | Apple Inc. | Intent deduction based on previous user interactions with voice assistant |
US9548050B2 (en) | 2010-01-18 | 2017-01-17 | Apple Inc. | Intelligent automated assistant |
US8731942B2 (en) | 2010-01-18 | 2014-05-20 | Apple Inc. | Maintaining context information between user interactions with a voice assistant |
US10496753B2 (en) | 2010-01-18 | 2019-12-03 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10553209B2 (en) | 2010-01-18 | 2020-02-04 | Apple Inc. | Systems and methods for hands-free notification summaries |
US8892446B2 (en) | 2010-01-18 | 2014-11-18 | Apple Inc. | Service orchestration for intelligent automated assistant |
US8903716B2 (en) | 2010-01-18 | 2014-12-02 | Apple Inc. | Personalized vocabulary for digital assistant |
US10706841B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Task flow identification based on user intent |
US10705794B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US10049675B2 (en) | 2010-02-25 | 2018-08-14 | Apple Inc. | User profiling for voice input processing |
US9633660B2 (en) | 2010-02-25 | 2017-04-25 | Apple Inc. | User profiling for voice input processing |
US10692504B2 (en) | 2010-02-25 | 2020-06-23 | Apple Inc. | User profiling for voice input processing |
US9190062B2 (en) | 2010-02-25 | 2015-11-17 | Apple Inc. | User profiling for voice input processing |
US20110258223A1 (en) * | 2010-04-14 | 2011-10-20 | Electronics And Telecommunications Research Institute | Voice-based mobile search apparatus and method |
US11748430B2 (en) | 2010-05-20 | 2023-09-05 | Google Llc | Automatic routing using search results |
US10909199B2 (en) | 2010-05-20 | 2021-02-02 | Google Llc | Automatic routing using search results |
US20140229469A1 (en) * | 2010-05-20 | 2014-08-14 | Google Inc. | Automatic dialing of search results |
US11494453B2 (en) | 2010-05-20 | 2022-11-08 | Google Llc | Automatic dialing |
US8805690B1 (en) * | 2010-08-05 | 2014-08-12 | Google Inc. | Audio notifications |
US9349368B1 (en) | 2010-08-05 | 2016-05-24 | Google Inc. | Generating an audio notification based on detection of a triggering event |
US9807217B1 (en) | 2010-08-05 | 2017-10-31 | Google Inc. | Selective audio notifications based on connection to an accessory |
US9313317B1 (en) | 2010-08-05 | 2016-04-12 | Google Inc. | Audio notifications |
US10237386B1 (en) | 2010-08-05 | 2019-03-19 | Google Llc | Outputting audio notifications based on determination of device presence in a vehicle |
US10621253B2 (en) | 2010-08-06 | 2020-04-14 | Google Llc | State-dependent query response |
US20120036121A1 (en) * | 2010-08-06 | 2012-02-09 | Google Inc. | State-dependent Query Response |
US10496714B2 (en) * | 2010-08-06 | 2019-12-03 | Google Llc | State-dependent query response |
US10599729B2 (en) | 2010-08-06 | 2020-03-24 | Google Llc | State-dependent query response |
US10496718B2 (en) | 2010-08-06 | 2019-12-03 | Google Llc | State-dependent query response |
US11216522B2 (en) | 2010-08-06 | 2022-01-04 | Google Llc | State-dependent query response |
US10762293B2 (en) | 2010-12-22 | 2020-09-01 | Apple Inc. | Using parts-of-speech tagging and named entity recognition for spelling correction |
US20140129220A1 (en) * | 2011-03-03 | 2014-05-08 | Shilei ZHANG | Speaker and call characteristic sensitive open voice search |
US9099092B2 (en) * | 2011-03-03 | 2015-08-04 | Nuance Communications, Inc. | Speaker and call characteristic sensitive open voice search |
US10032454B2 (en) * | 2011-03-03 | 2018-07-24 | Nuance Communications, Inc. | Speaker and call characteristic sensitive open voice search |
US20150294669A1 (en) * | 2011-03-03 | 2015-10-15 | Nuance Communications, Inc. | Speaker and Call Characteristic Sensitive Open Voice Search |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
US10102359B2 (en) | 2011-03-21 | 2018-10-16 | Apple Inc. | Device access using voice authentication |
US10057736B2 (en) | 2011-06-03 | 2018-08-21 | Apple Inc. | Active transport based notifications |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US11350253B2 (en) | 2011-06-03 | 2022-05-31 | Apple Inc. | Active transport based notifications |
US10255566B2 (en) | 2011-06-03 | 2019-04-09 | Apple Inc. | Generating and processing task items that represent tasks to perform |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US9911418B2 (en) | 2011-07-12 | 2018-03-06 | Google Llc | Systems and methods for speech command processing |
US9798393B2 (en) | 2011-08-29 | 2017-10-24 | Apple Inc. | Text correction processing |
US10241752B2 (en) | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US10134385B2 (en) | 2012-03-02 | 2018-11-20 | Apple Inc. | Systems and methods for name pronunciation |
US9483461B2 (en) | 2012-03-06 | 2016-11-01 | Apple Inc. | Handling speech synthesis of content for multiple languages |
US9953088B2 (en) | 2012-05-14 | 2018-04-24 | Apple Inc. | Crowd sourcing information to fulfill user requests |
US10079014B2 (en) | 2012-06-08 | 2018-09-18 | Apple Inc. | Name recognition system |
US9495129B2 (en) | 2012-06-29 | 2016-11-15 | Apple Inc. | Device, method, and user interface for voice-activated navigation and browsing of a document |
US9576574B2 (en) | 2012-09-10 | 2017-02-21 | Apple Inc. | Context-sensitive handling of interruptions by intelligent digital assistant |
US9971774B2 (en) | 2012-09-19 | 2018-05-15 | Apple Inc. | Voice-based media searching |
US10978090B2 (en) | 2013-02-07 | 2021-04-13 | Apple Inc. | Voice trigger for a digital assistant |
US10199051B2 (en) | 2013-02-07 | 2019-02-05 | Apple Inc. | Voice trigger for a digital assistant |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US9368114B2 (en) | 2013-03-14 | 2016-06-14 | Apple Inc. | Context-sensitive handling of interruptions |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US20140270258A1 (en) * | 2013-03-15 | 2014-09-18 | Pantech Co., Ltd. | Apparatus and method for executing object using voice command |
US9697822B1 (en) | 2013-03-15 | 2017-07-04 | Apple Inc. | System and method for updating an adaptive speech recognition model |
US9922642B2 (en) | 2013-03-15 | 2018-03-20 | Apple Inc. | Training an at least partial voice command system |
US9620104B2 (en) | 2013-06-07 | 2017-04-11 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US9633674B2 (en) | 2013-06-07 | 2017-04-25 | Apple Inc. | System and method for detecting errors in interactions with a voice-based digital assistant |
US9966060B2 (en) | 2013-06-07 | 2018-05-08 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US9582608B2 (en) | 2013-06-07 | 2017-02-28 | Apple Inc. | Unified ranking with entropy-weighted information for phrase-based semantic auto-completion |
US10657961B2 (en) | 2013-06-08 | 2020-05-19 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US9966068B2 (en) | 2013-06-08 | 2018-05-08 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US10185542B2 (en) | 2013-06-09 | 2019-01-22 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US9300784B2 (en) | 2013-06-13 | 2016-03-29 | Apple Inc. | System and method for emergency calls initiated by voice command |
US10791216B2 (en) | 2013-08-06 | 2020-09-29 | Apple Inc. | Auto-activating smart responses based on activities from remote devices |
US9348945B2 (en) * | 2013-08-29 | 2016-05-24 | Google Inc. | Modifying search results based on dismissal action associated with one or more of the search results |
US20150066973A1 (en) * | 2013-08-29 | 2015-03-05 | Google Inc. | Modifying Search Results Based on Dismissal Action Associated With One or More of the Search Results |
CN104680733A (en) * | 2013-11-30 | 2015-06-03 | 徐峥 | Simple household indoor object finding device based on WIFI (wireless fidelity) |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US10810250B2 (en) * | 2014-02-21 | 2020-10-20 | Apple Inc. | Revisiting content history |
US9620105B2 (en) | 2014-05-15 | 2017-04-11 | Apple Inc. | Analyzing audio input for efficient speech and music recognition |
US10592095B2 (en) | 2014-05-23 | 2020-03-17 | Apple Inc. | Instantaneous speaking of content on touch devices |
US9502031B2 (en) | 2014-05-27 | 2016-11-22 | Apple Inc. | Method for supporting dynamic grammars in WFST-based ASR |
CN103971679A (en) * | 2014-05-28 | 2014-08-06 | 锤子科技（北京）有限公司 | Linkman voice searching method and device and mobile terminal |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US10078631B2 (en) | 2014-05-30 | 2018-09-18 | Apple Inc. | Entropy-guided text prediction using combined word and character n-gram language models |
US9842101B2 (en) | 2014-05-30 | 2017-12-12 | Apple Inc. | Predictive conversion of language input |
US10657966B2 (en) | 2014-05-30 | 2020-05-19 | Apple Inc. | Better resolution when referencing to concepts |
US10289433B2 (en) | 2014-05-30 | 2019-05-14 | Apple Inc. | Domain specific language for encoding assistant dialog |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US9633004B2 (en) | 2014-05-30 | 2017-04-25 | Apple Inc. | Better resolution when referencing to concepts |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US9430463B2 (en) | 2014-05-30 | 2016-08-30 | Apple Inc. | Exemplar-based natural language processing |
US10083690B2 (en) | 2014-05-30 | 2018-09-25 | Apple Inc. | Better resolution when referencing to concepts |
US9785630B2 (en) | 2014-05-30 | 2017-10-10 | Apple Inc. | Text prediction using combined word N-gram and unigram language models |
US11257504B2 (en) | 2014-05-30 | 2022-02-22 | Apple Inc. | Intelligent assistant for home automation |
US9760559B2 (en) | 2014-05-30 | 2017-09-12 | Apple Inc. | Predictive text input |
US9734193B2 (en) | 2014-05-30 | 2017-08-15 | Apple Inc. | Determining domain salience ranking from ambiguous words in natural speech |
US10714095B2 (en) | 2014-05-30 | 2020-07-14 | Apple Inc. | Intelligent assistant for home automation |
US10169329B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Exemplar-based natural language processing |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US10497365B2 (en) | 2014-05-30 | 2019-12-03 | Apple Inc. | Multi-command single utterance input method |
US9668024B2 (en) | 2014-06-30 | 2017-05-30 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10904611B2 (en) | 2014-06-30 | 2021-01-26 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10659851B2 (en) | 2014-06-30 | 2020-05-19 | Apple Inc. | Real-time digital assistant knowledge updates |
US10446141B2 (en) | 2014-08-28 | 2019-10-15 | Apple Inc. | Automatic speech recognition based on user feedback |
US9818400B2 (en) | 2014-09-11 | 2017-11-14 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10431204B2 (en) | 2014-09-11 | 2019-10-01 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10789041B2 (en) | 2014-09-12 | 2020-09-29 | Apple Inc. | Dynamic thresholds for always listening speech trigger |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US9646609B2 (en) | 2014-09-30 | 2017-05-09 | Apple Inc. | Caching apparatus for serving phonetic pronunciations |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10074360B2 (en) | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
US10390213B2 (en) | 2014-09-30 | 2019-08-20 | Apple Inc. | Social reminders |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US9886432B2 (en) | 2014-09-30 | 2018-02-06 | Apple Inc. | Parsimonious handling of word inflection via categorical stem + suffix N-gram language models |
US9986419B2 (en) | 2014-09-30 | 2018-05-29 | Apple Inc. | Social reminders |
US10552013B2 (en) | 2014-12-02 | 2020-02-04 | Apple Inc. | Data detection |
US11556230B2 (en) | 2014-12-02 | 2023-01-17 | Apple Inc. | Data detection |
US9711141B2 (en) | 2014-12-09 | 2017-07-18 | Apple Inc. | Disambiguating heteronyms in speech synthesis |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US9865280B2 (en) | 2015-03-06 | 2018-01-09 | Apple Inc. | Structured dictation using intelligent automated assistants |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US10311871B2 (en) | 2015-03-08 | 2019-06-04 | Apple Inc. | Competing devices responding to voice triggers |
US11087759B2 (en) | 2015-03-08 | 2021-08-10 | Apple Inc. | Virtual assistant activation |
US9899019B2 (en) | 2015-03-18 | 2018-02-20 | Apple Inc. | Systems and methods for structured stem and suffix language models |
US9842105B2 (en) | 2015-04-16 | 2017-12-12 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US10127220B2 (en) | 2015-06-04 | 2018-11-13 | Apple Inc. | Language identification from short strings |
US10356243B2 (en) | 2015-06-05 | 2019-07-16 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10101822B2 (en) | 2015-06-05 | 2018-10-16 | Apple Inc. | Language input correction |
US10186254B2 (en) | 2015-06-07 | 2019-01-22 | Apple Inc. | Context-based endpoint detection |
US10255907B2 (en) | 2015-06-07 | 2019-04-09 | Apple Inc. | Automatic accent detection using acoustic models |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US9697820B2 (en) | 2015-09-24 | 2017-07-04 | Apple Inc. | Unit-selection text-to-speech synthesis using concatenation-sensitive neural networks |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US11587559B2 (en) | 2015-09-30 | 2023-02-21 | Apple Inc. | Intelligent device identification |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10354652B2 (en) | 2015-12-02 | 2019-07-16 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US20170169118A1 (en) * | 2015-12-11 | 2017-06-15 | Blackberry Limited | Populating contact information |
US10725618B2 (en) * | 2015-12-11 | 2020-07-28 | Blackberry Limited | Populating contact information |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
US11069347B2 (en) | 2016-06-08 | 2021-07-20 | Apple Inc. | Intelligent automated assistant for media exploration |
US10354011B2 (en) | 2016-06-09 | 2019-07-16 | Apple Inc. | Intelligent automated assistant in a home environment |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US11037565B2 (en) | 2016-06-10 | 2021-06-15 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US10521466B2 (en) | 2016-06-11 | 2019-12-31 | Apple Inc. | Data driven natural language event detection and classification |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US10297253B2 (en) | 2016-06-11 | 2019-05-21 | Apple Inc. | Application integration with a digital assistant |
US11152002B2 (en) | 2016-06-11 | 2021-10-19 | Apple Inc. | Application integration with a digital assistant |
US10089072B2 (en) | 2016-06-11 | 2018-10-02 | Apple Inc. | Intelligent device arbitration and control |
US10269345B2 (en) | 2016-06-11 | 2019-04-23 | Apple Inc. | Intelligent task discovery |
US9619202B1 (en) * | 2016-07-07 | 2017-04-11 | Intelligently Interactive, Inc. | Voice command-driven database |
US20180012595A1 (en) * | 2016-07-07 | 2018-01-11 | Intelligently Interactive, Inc. | Simple affirmative response operating system |
US20180011685A1 (en) * | 2016-07-07 | 2018-01-11 | Intelligently Interactive, Inc. | Voice command-driven database |
US10115398B1 (en) * | 2016-07-07 | 2018-10-30 | Intelligently Interactive, Inc. | Simple affirmative response operating system |
US9983849B2 (en) * | 2016-07-07 | 2018-05-29 | Intelligently Interactive, Inc. | Voice command-driven database |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US10553215B2 (en) | 2016-09-23 | 2020-02-04 | Apple Inc. | Intelligent automated assistant |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US10332518B2 (en) | 2017-05-09 | 2019-06-25 | Apple Inc. | User interface for correcting recognition errors |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10847142B2 (en) | 2017-05-11 | 2020-11-24 | Apple Inc. | Maintaining privacy of personal information |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10755703B2 (en) | 2017-05-11 | 2020-08-25 | Apple Inc. | Offline personal assistant |
US10789945B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Low-latency intelligent automated assistant |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US10791176B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10410637B2 (en) | 2017-05-12 | 2019-09-10 | Apple Inc. | User-specific acoustic models |
US10482874B2 (en) | 2017-05-15 | 2019-11-19 | Apple Inc. | Hierarchical belief states for digital assistants |
US10810274B2 (en) | 2017-05-15 | 2020-10-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US11217255B2 (en) | 2017-05-16 | 2022-01-04 | Apple Inc. | Far-field extension for digital assistant services |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US11087753B2 (en) * | 2017-12-13 | 2021-08-10 | KABUSHIKl KAISHA TOSHIBA | Dialog system |
US20190180743A1 (en) * | 2017-12-13 | 2019-06-13 | Kabushiki Kaisha Toshiba | Dialog system |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
WO2019172954A1 (en) * | 2018-03-07 | 2019-09-12 | Google Llc | Interface for a distributed network system |
CN111344694A (en) * | 2018-03-07 | 2020-06-26 | 谷歌有限责任公司 | Interface for distributed network system |
US10896213B2 (en) | 2018-03-07 | 2021-01-19 | Google Llc | Interface for a distributed network system |
US10778614B2 (en) | 2018-03-08 | 2020-09-15 | Andre Arzumanyan | Intelligent apparatus and method for responding to text messages |
US11166127B2 (en) | 2018-03-08 | 2021-11-02 | Andre Arzumanyan | Apparatus and method for voice call initiated texting session |
US11310174B2 (en) | 2018-03-08 | 2022-04-19 | Andre Arzumanyan | Intelligent apparatus and method for responding to text messages |
US11875279B2 (en) | 2018-03-08 | 2024-01-16 | Andre Arzumanyan | Method for payment of an order to be forwarded to one of a plurality of client establishments |
US11556919B2 (en) | 2018-03-08 | 2023-01-17 | Andre Arzumanyan | Apparatus and method for payment of a texting session order from an electronic wallet |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US10984798B2 (en) | 2018-06-01 | 2021-04-20 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11009970B2 (en) | 2018-06-01 | 2021-05-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10504518B1 (en) | 2018-06-03 | 2019-12-10 | Apple Inc. | Accelerated task performance |
US10944859B2 (en) | 2018-06-03 | 2021-03-09 | Apple Inc. | Accelerated task performance |
Also Published As
Publication number | Publication date |
---|---|
EP2082395A2 (en) | 2009-07-29 |
WO2008034111A2 (en) | 2008-03-20 |
WO2008034111A3 (en) | 2008-07-03 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20080071544A1 (en) | Integrating Voice-Enabled Local Search and Contact Lists | |
JP7314238B2 (en) | Handling phone calls on shared voice-enabled devices | |
US11755666B2 (en) | In-conversation search | |
US8185539B1 (en) | Web site or directory search using speech recognition of letters | |
US8032383B1 (en) | Speech controlled services and devices using internet | |
US7447299B1 (en) | Voice and telephone keypad based data entry for interacting with voice information services | |
US8537980B2 (en) | Conversation support | |
EP2008193B1 (en) | Hosted voice recognition system for wireless devices | |
CN101609673B (en) | User voice processing method based on telephone bank and server | |
US8328089B2 (en) | Hands free contact database information entry at a communication device | |
US6891932B2 (en) | System and methodology for voice activated access to multiple data sources and voice repositories in a single session | |
US7689425B2 (en) | Quality of service call routing system using counselor and speech recognition engine and method thereof | |
US20070286399A1 (en) | Phone Number Extraction System For Voice Mail Messages | |
US20060143007A1 (en) | User interaction with voice information services | |
US7555533B2 (en) | System for communicating information from a server via a mobile communication device | |
CN106603792B (en) | A kind of number searching equipment | |
KR100822170B1 (en) | Database construction method and system for speech recognition ars service | |
Goldman et al. | Voice Portals—Where Theory Meets Practice | |
KR100574007B1 (en) | System and method for providing individually central office service using voice recognition, recording medium recording program for implementing the method | |
WO2008100420A1 (en) | Providing network-based access to personalized user information |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BEAUFAYS, FRANCOISE;STROPE, BRIAN;BYRNE, WILLIAM J.;REEL/FRAME:020168/0761;SIGNING DATES FROM 20071002 TO 20071126 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044142/0357Effective date: 20170929 |