JP6911155B2 - Memory of metadata associated with acquired images - Google Patents
Memory of metadata associated with acquired images Download PDFInfo
- Publication number
- JP6911155B2 JP6911155B2 JP2019563573A JP2019563573A JP6911155B2 JP 6911155 B2 JP6911155 B2 JP 6911155B2 JP 2019563573 A JP2019563573 A JP 2019563573A JP 2019563573 A JP2019563573 A JP 2019563573A JP 6911155 B2 JP6911155 B2 JP 6911155B2
- Authority
- JP
- Japan
- Prior art keywords
- user
- metadata
- images
- client device
- task request
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5846—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using extracted text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/5866—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using information manually generated, e.g. tags, keywords, comments, manually generated location and time information
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/44—Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components
- G06V10/443—Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components by matching or filtering
- G06V10/449—Biologically inspired filters, e.g. difference of Gaussians [DoG] or Gabor filters
- G06V10/451—Biologically inspired filters, e.g. difference of Gaussians [DoG] or Gabor filters with interaction between the filter responses, e.g. cortical complex cells
- G06V10/454—Integrating the filters into a hierarchical structure, e.g. convolutional neural networks [CNN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/82—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/20—Scenes; Scene-specific elements in augmented reality scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/35—Categorising the entire scene, e.g. birthday party or wedding scene
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/60—Type of objects
- G06V20/62—Text, e.g. of license plates, overlay texts or captions on TV images
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/00127—Connection or combination of a still picture apparatus with another apparatus, e.g. for storage, processing or transmission of still picture signals or of information associated with a still picture
- H04N1/00204—Connection or combination of a still picture apparatus with another apparatus, e.g. for storage, processing or transmission of still picture signals or of information associated with a still picture with a digital computer or a digital computer system, e.g. an internet server
- H04N1/00244—Connection or combination of a still picture apparatus with another apparatus, e.g. for storage, processing or transmission of still picture signals or of information associated with a still picture with a digital computer or a digital computer system, e.g. an internet server with a server, e.g. an internet server
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/00127—Connection or combination of a still picture apparatus with another apparatus, e.g. for storage, processing or transmission of still picture signals or of information associated with a still picture
- H04N1/00281—Connection or combination of a still picture apparatus with another apparatus, e.g. for storage, processing or transmission of still picture signals or of information associated with a still picture with a telecommunication apparatus, e.g. a switched network of teleprinters for the distribution of text-based information, a selective call terminal
- H04N1/00307—Connection or combination of a still picture apparatus with another apparatus, e.g. for storage, processing or transmission of still picture signals or of information associated with a still picture with a telecommunication apparatus, e.g. a switched network of teleprinters for the distribution of text-based information, a selective call terminal with a mobile telephone apparatus
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/00127—Connection or combination of a still picture apparatus with another apparatus, e.g. for storage, processing or transmission of still picture signals or of information associated with a still picture
- H04N1/00326—Connection or combination of a still picture apparatus with another apparatus, e.g. for storage, processing or transmission of still picture signals or of information associated with a still picture with a data reading, recognizing or recording apparatus, e.g. with a bar-code apparatus
- H04N1/00328—Connection or combination of a still picture apparatus with another apparatus, e.g. for storage, processing or transmission of still picture signals or of information associated with a still picture with a data reading, recognizing or recording apparatus, e.g. with a bar-code apparatus with an apparatus processing optically-read information
- H04N1/00331—Connection or combination of a still picture apparatus with another apparatus, e.g. for storage, processing or transmission of still picture signals or of information associated with a still picture with a data reading, recognizing or recording apparatus, e.g. with a bar-code apparatus with an apparatus processing optically-read information with an apparatus performing optical character recognition
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/0035—User-machine interface; Control console
- H04N1/00405—Output means
- H04N1/00408—Display of information to the user, e.g. menus
- H04N1/00411—Display of information to the user, e.g. menus the display also being used for user input, e.g. touch screen
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/0035—User-machine interface; Control console
- H04N1/00405—Output means
- H04N1/00408—Display of information to the user, e.g. menus
- H04N1/00413—Display of information to the user, e.g. menus using menus, i.e. presenting the user with a plurality of selectable options
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/0035—User-machine interface; Control console
- H04N1/00405—Output means
- H04N1/00408—Display of information to the user, e.g. menus
- H04N1/00413—Display of information to the user, e.g. menus using menus, i.e. presenting the user with a plurality of selectable options
- H04N1/00416—Multi-level menus
- H04N1/00419—Arrangements for navigating between pages or parts of the menu
- H04N1/00424—Arrangements for navigating between pages or parts of the menu using a list of graphical elements, e.g. icons or icon bar
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/21—Intermediate information storage
- H04N1/2166—Intermediate information storage for mass storage, e.g. in document filing systems
- H04N1/217—Interfaces allowing access to a single user
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/21—Intermediate information storage
- H04N1/2166—Intermediate information storage for mass storage, e.g. in document filing systems
- H04N1/2179—Interfaces allowing access to a plurality of users, e.g. connection to electronic image libraries
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/21—Intermediate information storage
- H04N1/2166—Intermediate information storage for mass storage, e.g. in document filing systems
- H04N1/2179—Interfaces allowing access to a plurality of users, e.g. connection to electronic image libraries
- H04N1/2187—Interfaces allowing access to a plurality of users, e.g. connection to electronic image libraries with image input from a plurality of different locations or from a non-central location, e.g. from one or more users
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/21—Intermediate information storage
- H04N1/2166—Intermediate information storage for mass storage, e.g. in document filing systems
- H04N1/2179—Interfaces allowing access to a plurality of users, e.g. connection to electronic image libraries
- H04N1/2191—Interfaces allowing access to a plurality of users, e.g. connection to electronic image libraries for simultaneous, independent access by a plurality of different users
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N1/32101—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N1/32106—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title separate from the image data, e.g. in a different computer file
- H04N1/32112—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title separate from the image data, e.g. in a different computer file in a separate computer file, document page or paper sheet, e.g. a fax cover sheet
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/63—Control of cameras or camera modules by using electronic viewfinders
- H04N23/631—Graphical user interfaces [GUI] specially adapted for controlling image capture or setting capture parameters
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/63—Control of cameras or camera modules by using electronic viewfinders
- H04N23/631—Graphical user interfaces [GUI] specially adapted for controlling image capture or setting capture parameters
- H04N23/632—Graphical user interfaces [GUI] specially adapted for controlling image capture or setting capture parameters for displaying or modifying preview images prior to image capturing, e.g. variety of image resolutions or capturing parameters
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/64—Computer-aided capture of images, e.g. transfer from script file into camera, check of taken image quality, advice or proposal for image composition or decision on when to take image
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/765—Interface circuits between an apparatus for recording and another apparatus
- H04N5/77—Interface circuits between an apparatus for recording and another apparatus between a recording apparatus and a television camera
- H04N5/772—Interface circuits between an apparatus for recording and another apparatus between a recording apparatus and a television camera the recording apparatus and the television camera being placed in the same enclosure
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N9/00—Details of colour television systems
- H04N9/79—Processing of colour television signals in connection with recording
- H04N9/80—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback
- H04N9/82—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback the individual colour picture signal components being recorded simultaneously only
- H04N9/8205—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback the individual colour picture signal components being recorded simultaneously only involving the multiplexing of an additional signal and the colour video signal
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2413—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on distances to training or reference patterns
- G06F18/24133—Distances to prototypes
- G06F18/24137—Distances to cluster centroïds
- G06F18/2414—Smoothing the distance, e.g. radial basis function networks [RBFN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/18—Extraction of features or characteristics of the image
- G06V30/1801—Detecting partial patterns, e.g. edges or contours, or configurations, e.g. loops, corners, strokes or intersections
- G06V30/18019—Detecting partial patterns, e.g. edges or contours, or configurations, e.g. loops, corners, strokes or intersections by matching or filtering
- G06V30/18038—Biologically-inspired filters, e.g. difference of Gaussians [DoG], Gabor filters
- G06V30/18048—Biologically-inspired filters, e.g. difference of Gaussians [DoG], Gabor filters with interaction between the responses of different filters, e.g. cortical complex cells
- G06V30/18057—Integrating the filters into a hierarchical structure, e.g. convolutional neural networks [CNN]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N2101/00—Still video cameras
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N2201/00—Indexing scheme relating to scanning, transmission or reproduction of documents or the like, and to details thereof
- H04N2201/0077—Types of the still picture apparatus
- H04N2201/0084—Digital still camera
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N2201/00—Indexing scheme relating to scanning, transmission or reproduction of documents or the like, and to details thereof
- H04N2201/0096—Portable devices
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N2201/00—Indexing scheme relating to scanning, transmission or reproduction of documents or the like, and to details thereof
- H04N2201/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N2201/3201—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N2201/3225—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title of data relating to an image, a page or a document
- H04N2201/3253—Position information, e.g. geographical position at time of capture, GPS data
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N2201/00—Indexing scheme relating to scanning, transmission or reproduction of documents or the like, and to details thereof
- H04N2201/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N2201/3201—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N2201/3261—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title of multimedia information, e.g. a sound signal
- H04N2201/3266—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title of multimedia information, e.g. a sound signal of text or character information, e.g. text accompanying an image
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N2201/00—Indexing scheme relating to scanning, transmission or reproduction of documents or the like, and to details thereof
- H04N2201/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N2201/3201—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N2201/3273—Display
Description
人間は、「自動アシスタント」と本明細書で称される(「チャットボット」、「対話型パーソナルアシスタント」、「インテリジェントパーソナルアシスタント」、「パーソナル音声アシスタント」、「会話型エージェント」等とも称される)対話型ソフトウェアアプリケーションとのヒューマンコンピュータ対話に係わることがある。例えば、人間(自動アシスタントと対話するときは「ユーザ」と称されてよい)は、一部の場合にはテキストへ変換されて次いで処理されてよい口頭自然言語入力(すなわち発話)を使用して、かつ/またはテキスト(例えば、タイプ)自然言語入力を与えることによって、コマンド、クエリおよび/または要求を与えてよい。一部の自動アシスタントは、ユーザによってそうするように要求されると、単純な事実を「覚えている」ことが可能でよい。例えば、ユーザは、自動アシスタントがユーザの名前、ユーザの配偶者の名前、ユーザの記念日等を覚えているように要求してよい。ユーザは後に自動アシスタントに、これらの事実を呼び戻すように要求してよい。しかしながら、これらの単純な事実は分離して記憶されがちであり、コンテキスト情報としては十分でない。 Humans are also referred to herein as "automatic assistants" (also referred to as "chatbots", "interactive personal assistants", "intelligent personal assistants", "personal voice assistants", "conversational agents", etc. ) May be involved in human computer dialogue with interactive software applications. For example, a human (sometimes referred to as a "user" when interacting with an automated assistant) uses oral natural language input (ie, speech) that may in some cases be converted to text and then processed. And / or text (eg, type) commands, queries and / or requests may be given by giving natural language input. Some automated assistants may be able to "remember" simple facts when requested by the user to do so. For example, the user may require the automatic assistant to remember the user's name, the user's spouse's name, the user's anniversary, and so on. The user may later request the automatic assistant to recall these facts. However, these simple facts tend to be stored separately and are not sufficient as contextual information.
クライアント装置のカメラによって取得される画像に関連したユーザ選択メタデータを記憶するための技法が本明細書に記載される。ユーザ選択メタデータは、例えば、コンテキスト情報(ジオタグ等など、画像と関連付けて既に記憶されているメタデータ以外)および/または画像が取得されるときにユーザによって提供される情報を含んでよい。一部の実装例では、ユーザは、ユーザのカメラ搭載クライアント装置上で少なくとも部分的に実行する-自動アシスタントに対する、カメラによって取得される1つまたは複数の画像に関連したユーザ選択メタデータを記憶せよとの「タスク要求」を含む入力を与えてよい。一部の実装例では、メタデータはユーザのタスク要求の内容を含んでよい。他の実装例では、メタデータは、タスク要求の内容に基づいて選択されるデータ(例えば、位置座標)を含んでよい。一部の実装例では、ユーザは後に自動アシスタントに、検索クエリなどの別のタスク要求を与えてよく、そして自動アシスタントは、検索クエリに応答するユーザ選択メタデータにタスク要求を照合しようと試みてよい。後続のタスク要求は続いて、メタデータに基づいて応じられてよい。 Techniques for storing user-selected metadata associated with images acquired by the camera of the client device are described herein. User-selected metadata may include, for example, contextual information (other than metadata already stored in association with the image, such as geotags) and / or information provided by the user when the image is retrieved. In some implementations, the user runs at least partially on the user's camera-equipped client device-store user-selected metadata associated with one or more images captured by the camera for the automated assistant. You may give an input that includes a "task request" with. In some implementations, the metadata may include the content of the user's task request. In other implementations, the metadata may include data selected based on the content of the task request (eg, position coordinates). In some implementations, the user may later give the auto-assistant another task request, such as a search query, and the auto-assistant attempts to match the task request to the user-selected metadata that responds to the search query. good. Subsequent task requests may subsequently be served based on the metadata.
簡単な例として、ユーザは、ユーザのパスポートの写真を撮るときに、口頭タスク要求「これが私のパスポートであることを覚えていて」を与えてよい。画像に関連したメタデータが次いで、例えば、画像自体と関連付けて記憶されてよい。後に、ユーザは自動アシスタントに「私に私のパスポートを見せて」または「私に私のパスポートについての情報を見せて」と頼んでよく、そしてメタデータおよび/または以前の取得画像がユーザに示されてよい。一部の実装例では、光学文字認識(「OCR」)がパスポートの取得画像に行われ、そしてメタデータとして記憶されてもよい。このように、一部のインスタンスで自動アシスタントへの要求を介してパスポート画像を呼び戻すことが可能であることに加えて、一部の実装例では、ユーザは自動アシスタントに、その失効日など、パスポートについての情報を取り出すように頼むことが可能でよい。ユーザは、類似の技法を使用して、Wi-Fi認証情報(例えば、Wi-Fiルータの底部に印刷されるサービスセット識別子または「SSID」およびパスワード)の画像を記憶させ、そして後にOCRされた認証情報を求めてよい。 As a simple example, the user may give an oral task request "remember this is my passport" when taking a picture of the user's passport. The metadata associated with the image may then be stored, for example, in association with the image itself. Later, the user may ask the automatic assistant to "show me my passport" or "show me information about my passport", and the metadata and / or previously acquired images are shown to the user. May be done. In some implementations, optical character recognition (“OCR”) may be performed on the acquired image of the passport and stored as metadata. In addition to this, in some instances it is possible to recall the passport image via a request to the auto assistant, in some implementations the user will tell the auto assistant the passport, such as its expiration date. It may be possible to ask to retrieve information about. The user uses a similar technique to store an image of the Wi-Fi credentials (eg, the service set identifier or "SSID" and password printed on the bottom of the Wi-Fi router) and later OCR. You may ask for credentials.
別の例として、ユーザは、ユーザが或る場所にカメラを向けつつ、自動アシスタントが「私がどこに駐車したかを覚えている」ように要求してよい。一部のそのような実装例では、自動アシスタントは、例えば、ユーザが取得する画像と関連するジオタグに基づいて、かつ/またはユーザの現在の位置座標(例えば、GPSセンサから決定される)に基づいて、その場所を覚えていてよい。ユーザが後に「私はどこに駐車したか?」と尋ねると、一部の実装例では、取得画像が提示されてよく、そして一部の場合には、自動アシスタントは、位置座標など、対応するメタデータの少なくとも一部を可聴的または視覚的に提示してよい。 As another example, the user may request the automatic assistant to "remember where I parked" while the user points the camera at a location. In some such implementations, the automatic assistant is based, for example, on the geotag associated with the image captured by the user and / or on the user's current position coordinates (eg, determined from a GPS sensor). You may remember the place. When the user later asks "Where did I park?", In some implementations the captured image may be presented, and in some cases the automatic assistant will provide the corresponding meta, such as position coordinates. At least part of the data may be presented audibly or visually.
各種の他のユーザ選択メタデータが記憶されてよく、そして様々な実装例で、単純な画像呼戻しに加えてまたはその代わりに様々な目的で使用されてよい。一部の実装例では、ユーザは、画像に示される物体がユーザの配偶者などの実体として識別されるように要求してよい。一部のそのような実装例では、その画像はユーザ指示実体と関連する基準画像として使用されてよい。一部の実装例では、異なる画像に同じ実体を識別するために、その画像におよび/またはユーザの他の記憶画像に画像処理が行われてよい。このように、例えば、一旦ユーザがユーザの妻にカメラを向けて「これは私の妻である」と言うと、その画像は、以前に取得されたおよび/または将来取得されるであろう、ユーザの妻の他の画像にラベル付けするために使用されてよい。 Various other user-selected metadata may be stored and used in various implementations in addition to or instead of simple image recall for various purposes. In some implementations, the user may require that the object shown in the image be identified as an entity such as the user's spouse. In some such implementations, the image may be used as a reference image associated with a user-directed entity. In some implementations, image processing may be performed on the image and / or other stored images of the user in order to identify the same entity in different images. Thus, for example, once the user points the camera at the user's wife and says "this is my wife", the image will be acquired previously and / or in the future. It may be used to label other images of the user's wife.
別の例として、飲食店を通り過ぎるユーザが飲食店の画像を取得し、そして自動アシスタントに「後で私にこの飲食店を調べることを思い出させる」ように命令できる。取得画像に画像処理(例えば、OCR)が行われて、例えば、飲食店の名前、住所等などの任意の見分けられる言葉を識別してよい。追加的または代替的に、飲食店は、画像が取得されたときのユーザの位置座標(または画像のメタデータに既に記憶されているジオタグ)などの他の信号から識別されてよい。ユーザは後に、自動アシスタントに「私があなたに話した飲食店を私に思い出させる」ように要求することによって、この画像および/またはメタデータを呼び戻せる。追加的または代替的に、自動アシスタントは、ユーザが再び飲食店に近いときに(例えば、ユーザのGPS座標に基づく)またはユーザが飲食店のお薦めを求めるときになど、将来の或る時点でユーザに合図を自動的に示して(例えば、可聴的または可視的に出力して)よい。 As another example, a user passing by a restaurant can take an image of the restaurant and instruct an automated assistant to "remind me to look at this restaurant later". Image processing (eg, OCR) may be performed on the acquired image to identify any distinguishable word such as the name, address, etc. of the restaurant. Additional or alternative, the restaurant may be identified from other signals such as the user's position coordinates (or geotags already stored in the image's metadata) when the image was acquired. The user can later recall this image and / or metadata by asking the automatic assistant to "remind me of the restaurant I spoke to you." Additional or alternative, the automatic assistant will provide the user at some point in the future, such as when the user is close to the restaurant again (eg, based on the user's GPS coordinates) or when the user asks for a restaurant recommendation. The signal may be automatically given (for example, audible or visible output).
一部の実装例では、ユーザは、製品などの実体の画像を取得してよく、そして自動アシスタントに、1つまたは複数のユーザ選択情報を画像に関連したメタデータとして記憶するように頼んでよい。例えば、1本のワインの画像を取得しつつ、ユーザは、自動アシスタントが「このワインが素晴らしい」ことを覚えておくように要求できる。ユーザが後にワインのお薦めを求めると、自動アシスタントは、ユーザが保存したかもしれない任意の他のボトルに加えて、そのボトルを薦めてよい。追加的または代替的に、ユーザが後に、例えば、いわゆる「スマート」グラスに含まれてよい拡張現実感ディスプレイを通して店舗で棚上の複数のワインを見ているとする。以前の記憶画像および対応するメタデータに基づいて、同じワインが棚上に認識されてよく、かつ、例えば、棚上のワインをハイライトまたはその他視覚的に強調すること(例えば、「あなたはこのワインが『素晴らしい』と言った」)によって、ユーザに対して視覚的にアノテーション付けされてよい。類似して、カメラ搭載スマートフォンを持つユーザが同じ棚のワインにカメラを向けてよく、そしてユーザが好んだワインがスマートフォンの電子ビューファインダ内で視覚的に強調されてよい。画像にユーザによって識別される人間の実体に類似の技法が適用されて、例えば、ユーザが後の時点で他人の名前(または関連した他の情報)を思い出すのを援助してよい。 In some implementations, the user may take an image of an entity such as a product, and ask an automated assistant to store one or more user selection information as metadata associated with the image. .. For example, while taking an image of a bottle of wine, the user can request the automatic assistant to remember that "this wine is great". If the user later asks for a wine recommendation, the automatic assistant may recommend that bottle in addition to any other bottle that the user may have stored. Additional or alternative, suppose a user later sees multiple wines on a shelf in a store through an augmented reality display that may be included, for example, in a so-called "smart" glass. Based on previous memory images and corresponding metadata, the same wine may be recognized on the shelf and, for example, highlight or otherwise visually emphasize the wine on the shelf (eg, "You are this The wine may be visually annotated to the user by "said" great ""). Similarly, a user with a camera-equipped smartphone may point the camera at a wine on the same shelf, and the wine the user prefers may be visually highlighted within the smartphone's electronic viewfinder. Techniques similar to the human entity identified by the user in the image may be applied, for example, to help the user remember another person's name (or other related information) at a later point in time.
ユーザが自動アシスタントを呼び出して、様々に取得画像に関連したユーザ選択メタデータを記憶させてよい。一部の実装例では、カメラアプリケーションと関連するグラフィカルユーザインタフェース(「GUI」)が、自動アシスタントを呼び出すためにユーザによって選択可能であるグラフィカル要素を含んでよい。画像が撮られる直前に、間に、または直後に、ユーザは、このグラフィカル要素を選択し、そしてユーザ選択メタデータを記憶させるために使用できるタスク要求を与えてよい。他の実装例では、ユーザは、写真を撮る直前に、間に、または直後に既存の呼出しフレーズ(例えば、「ヘイ、自動アシスタント」、または「覚えていて」と言うことによる)を使用して自動アシスタントを簡単に呼び出してよい。 The user may call an automatic assistant to store various user-selected metadata associated with the acquired image. In some implementations, the graphical user interface (“GUI”) associated with the camera application may include graphical elements that can be selected by the user to call the automatic assistant. Immediately before, during, or shortly after the image is taken, the user may give a task request that can be used to select this graphical element and store the user selection metadata. In another implementation, the user uses an existing calling phrase (for example, by saying "hey, auto-assistant", or "remember") just before, during, or after taking a picture. You can easily call the automatic assistant.
一部の実装例では、自動アシスタントは、様々な信号に応じてユーザにタスク要求を推奨してよい。例えば、ユーザは車(例えば、ユーザの車)にカメラを向けることができる。自動アシスタントは、画像に特定の車(またはより一般に、車両)を認識してよく、そしてユーザが自動アシスタントに駐車場を覚えていて欲しいかどうかをユーザに尋ねてよい。 In some implementations, the automated assistant may recommend task requests to the user in response to various signals. For example, the user can point the camera at a car (eg, the user's car). The auto-assistant may recognize a particular vehicle (or, more generally, the vehicle) in the image, and may ask the user if the auto-assistant wants the auto-assistant to remember the parking lot.
一部の実装例では、1つまたは複数のプロセッサによって行われる方法が提供され、ユーザによって操作される1つまたは複数のクライアント装置のうちの第1のクライアント装置で、ユーザからの自由形式入力を受け取るステップと、自由形式入力からタスク要求を認識するステップと、タスク要求が、第1のクライアント装置のカメラによって取得される1つまたは複数の画像に関連したメタデータを記憶せよとの要求を備えると判定するステップであって、メタデータがタスク要求の内容に基づいて選択される、ステップと、1つまたは複数のコンピュータ可読媒体にメタデータを記憶するステップであって、1つまたは複数のコンピュータ可読媒体が、メタデータを使用して検索可能である、ステップとを含む。 Some implementation examples provide a method performed by one or more processors, where the first client device of one or more client devices operated by the user receives free-form input from the user. It comprises a step of receiving, a step of recognizing the task request from free-form input, and a request that the task request store metadata associated with one or more images acquired by the camera of the first client device. A step in which the metadata is selected based on the content of the task request, and a step in which the metadata is stored in one or more computer-readable media, one or more computers. The readable medium includes steps, which are searchable using the metadata.
本明細書に開示される技術のこれらおよび他の実装例は、以下の特徴の1つまたは複数を任意選択で含んでよい。 These and other implementation examples of the techniques disclosed herein may optionally include one or more of the following features:
様々な実装例で、本方法は、1つまたは複数のクライアント装置のうちの第1のクライアント装置または第2のクライアント装置で、第2の自由形式入力を受け取るステップと、第2の自由形式入力から別のタスク要求を認識するステップと、カメラによって取得される1つまたは複数の画像に関連したメタデータが別のタスク要求に応答すると判定するステップと、メタデータが別のタスク要求に応答するとの判定に応じて、タスク要求を行うステップとをさらに含んでよい。 In various implementation examples, the method involves receiving a second free-form input on the first or second client device of one or more client devices, and a second free-form input. Recognizing another task request from, and determining that the metadata associated with one or more images captured by the camera responds to another task request, and when the metadata responds to another task request. It may further include a step of making a task request according to the determination of.
様々な実装例で、自由形式入力は、第1のクライアント装置の電子ビューファインダが第1のクライアント装置のカメラによって取得されるデータをストリーミングする間に受け取られてよい。様々な実装例で、本方法は、第1のクライアント装置の1つまたは複数の出力装置を介して出力として、ユーザへの提案としてのタスク要求を提供するステップであって、タスク要求が、第1のクライアント装置の1つまたは複数のセンサによって生成される1つまたは複数の信号に基づいて選択される、ステップをさらに含んでよい。様々な実装例で、1つまたは複数の信号は、カメラによって取得されるデータを含んでよい。様々な実装例で、1つまたは複数の信号は、位置座標センサからの位置座標データを含んでよい。 In various implementations, the free-form input may be received while the electronic viewfinder of the first client device is streaming the data acquired by the camera of the first client device. In various implementations, the method is a step of providing a task request as a suggestion to a user as output via one or more output devices of a first client device, wherein the task request is the first. It may further include steps that are selected based on one or more signals generated by one or more sensors in one client device. In various implementations, one or more signals may include data acquired by the camera. In various implementations, one or more signals may include position coordinate data from the position coordinate sensor.
様々な実装例で、本方法は、1つまたは複数の画像に画像処理を行うステップと、画像処理に基づいて、1つまたは複数の画像に示される物体を識別するステップと、同じ物体または同物体と1つもしくは複数の属性を共有する別の物体を示す別の記憶画像と関連付けてメタデータを記憶するステップとをさらに含んでよい。 In various implementations, the method comprises performing image processing on one or more images and identifying the object shown in one or more images based on the image processing, the same object or the same. It may further include the step of storing metadata in association with another storage image showing another object that shares one or more attributes with the object.
様々な実装例で、本方法は、1つまたは複数の画像の一部分に光学文字認識を行って、1つまたは複数の画像に示されるテキスト内容を決定するステップをさらに含んでよい。様々な実装例で、メタデータは、テキスト内容の少なくとも一部をさらに含んでよい。様々な実装例で、メタデータは、タスク要求の内容の少なくとも一部を含んでよい。様々な実装例で、メタデータは、1つまたは複数の画像の取得と同時に得られる位置座標を含んでよい。 In various implementations, the method may further include performing optical character recognition on a portion of one or more images to determine the text content shown in the one or more images. In various implementations, the metadata may further include at least a portion of the text content. In various implementations, the metadata may contain at least part of the content of the task request. In various implementations, the metadata may include position coordinates obtained at the same time as the acquisition of one or more images.
加えて、一部の実装例は、1つまたは複数のコンピューティング装置の1つまたは複数のプロセッサを含んでおり、1つまたは複数のプロセッサは、関連メモリに記憶される命令を実行するように動作可能であり、かつ命令は、上述の方法のいずれかの遂行をもたらすように構成される。一部の実装例は、1つまたは複数のプロセッサによって実行可能であって上述の方法のいずれかを行うコンピュータ命令を記憶した1つまたは複数の非一時的コンピュータ可読記憶媒体も含む。 In addition, some implementations include one or more processors in one or more computing devices so that one or more processors execute instructions stored in the associated memory. It is operational and the instructions are configured to result in the performance of any of the methods described above. Some implementations also include one or more non-temporary computer-readable storage media that store computer instructions that can be executed by one or more processors and perform any of the methods described above.
上記の概念および本明細書にさらに詳細に記載される追加の概念の全ての組合せが本明細書に開示される対象の一部であるとして企図されることが認識されるべきである。例えば、本開示の末尾に載っている特許請求される対象の全ての組合せが本明細書に開示される対象の一部であるとして企図される。 It should be recognized that all combinations of the above concepts and additional concepts described in more detail herein are intended to be part of the subject matter disclosed herein. For example, all combinations of claims at the end of this disclosure are contemplated as being part of the objects disclosed herein.
本明細書に開示される実装例はいくつかの技術的利点を提供する。取得画像に関連したユーザ選択メタデータの記憶は、取得画像の以降の取出しがより効率的であることを保証する。画像が入力クエリに基づいて識別されることが可能でないという可能性が、画像に関するメタデータを記憶する工程によって低減され、これは、成果を達成することが必要とされるクライアント装置とのユーザ対話が減少されることを意味する。これは次いで、複数の検索動作および複数のユーザ入力の処理を行うことと関連するリソースを節約する。さらには、メタデータを記憶させるプロンプトを自由形式入力に基づけ、そこからタスク要求が認識されることによって、メタデータ記憶工程の開始が効率的であり、かつユーザが制限された形式の入力を与えることを必要としないことを保証できる。結果的に、誤った入力または誤った処理の可能性が低減され、そして記憶および以降の検索取出し工程がより効率的に行え、開示される実装例をホストするシステムのリソースを最適化する。 The implementation examples disclosed herein provide some technical advantages. The storage of user-selected metadata associated with the acquired image ensures that subsequent retrieval of the acquired image is more efficient. The possibility that an image cannot be identified based on an input query is reduced by the process of storing metadata about the image, which is a user interaction with the client device that is required to achieve results. Means that is reduced. This in turn saves resources associated with performing multiple search operations and processing multiple user inputs. Furthermore, the prompt to store the metadata is based on free-form input, from which the task request is recognized, which makes it efficient to start the metadata storage process and gives the user a limited form of input. You can be assured that you don't need it. As a result, the possibility of erroneous input or processing is reduced, and the storage and subsequent retrieval and retrieval processes can be performed more efficiently, optimizing the resources of the system hosting the disclosed implementation examples.
ここで図1に着目すると、本明細書に開示される技法が実装できる環境例が例示される。環境例は、複数のクライアントコンピューティング装置1061-Nおよび自動アシスタント120を含む。自動アシスタント120がクライアントコンピューティング装置1061-Nと別個として図1に例示されるが、一部の実装例では、自動アシスタント120の全てまたは諸態様がクライアントコンピューティング装置1061-Nの1つまたは複数によって実装されてよい。例えば、クライアント装置1061が自動アシスタント120の1つまたは複数の態様の1つのインスタンスを実装してよく、そしてクライアント装置106Nも自動アシスタント120のそれらの1つまたは複数の態様の別個のインスタンスを実装してよい。自動アシスタント120の1つまたは複数の態様がクライアントコンピューティング装置1061-Nから遠隔の1つまたは複数のコンピューティング装置によって実装される実装例では、クライアントコンピューティング装置1061-Nおよび自動アシスタント120のそれらの態様は、ローカルエリアネットワーク(LAN)および/またはワイドエリアネットワーク(WAN)(例えば、インターネット)などの1つまたは複数のネットワークを介して通信してよい。
Focusing on FIG. 1 here, an example of an environment in which the technique disclosed in the present specification can be implemented is illustrated. An example environment includes multiple client computing devices 106 1-N and an
クライアント装置1061-Nとしては、例えば、デスクトップコンピューティング装置、ラップトップコンピューティング装置、タブレットコンピューティング装置、携帯電話コンピューティング装置、ユーザの車両のコンピューティング装置(例えば、車内通信システム、車内エンターテイメントシステム、車内ナビゲーションシステム)、スタンドアロン対話型スピーカ、および/またはコンピューティング装置を含むユーザのウェアラブル機器(例えば、コンピューティング装置を有するユーザの腕時計、コンピューティング装置を有するユーザの眼鏡、仮想もしくは拡張現実感コンピューティング装置)の1つまたは複数を含んでよい。追加および/または代替のクライアントコンピューティング装置が設けられてよい。一部の実装例では、本開示の選択された態様が構成される自動アシスタントは、デジタルカメラなどの他の種類の電子装置にインストールされてよい。一部の実装例では、所与のユーザが、コンピューティング装置の協調「エコシステム」を集合的に形成する複数のクライアントコンピューティング装置を活用する自動アシスタント120と通信してよい。一部のそのような実装例では、自動アシスタント120は、例えば、「応対される」ユーザによってアクセスが制御されるリソース(例えば、コンテンツ、文書等)への拡張アクセスを自動アシスタント120に与える、その所与のユーザに「応対する」と考えられてよい。しかしながら、簡潔さのために、本明細書に記載される一部の例は、単一のクライアントコンピューティング装置106を動作させるユーザに注目することになる。
Client devices 106 1-N include, for example, desktop computing devices, laptop computing devices, tablet computing devices, mobile phone computing devices, user vehicle computing devices (eg, in-vehicle communication systems, in-vehicle entertainment systems). , In-vehicle navigation system), stand-alone interactive speakers, and / or user wearable devices including computing devices (eg, user watches with computing devices, glasses for users with computing devices, virtual or augmented reality computing It may include one or more of the computing devices). Additional and / or alternative client computing devices may be provided. In some implementation examples, the automated assistants configured in the selected embodiments of the present disclosure may be installed in other types of electronic devices such as digital cameras. In some implementations, a given user may communicate with an
クライアントコンピューティング装置1061-Nの各々は、複数のメッセージ交換クライアント1071-Nの対応するもの、複数のカメラアプリケーション1091-Nの対応するもの、および音声取得/テキスト音声(「TTS: text-to-speech」)/音声テキスト(「STT: speech-to-text」)モジュール1141-Nなどの各種の異なるアプリケーションを動作させてよい。各クライアント装置106には、1つまたは複数のカメラ111(例えば、スマートフォンまたはタブレットの場合の前面および/または背面カメラ)も搭載されてよい。 Each of the client computing devices 106 1-N corresponds to multiple message exchange clients 107 1-N , corresponds to multiple camera applications 109 1-N , and voice acquisition / text voice ("TTS: text"). -to-speech ") / Speech-to-text ("STT: speech-to-text") Module 114 You may run a variety of different applications such as 1-N. Each client device 106 may also include one or more cameras 111 (eg, front and / or rear cameras in the case of smartphones or tablets).
各音声取得/TTS/STTモジュール114は、1つまたは複数の機能:例えば、マイクロホン(図示せず)を介してユーザの音声を取得すること;その取得した音声をテキストに変換すること;および/またはテキストを音声に変換することを行うように構成されてよい。例えば、一部の実装例では、クライアント装置106が計算リソース(例えば、プロセッササイクル、メモリ、バッテリ等)の点で比較的制約されることがあるので、各クライアント装置106に対してローカルである音声取得/TTS/STTモジュール114は、有限数の異なる口頭フレーズ-特に自動アシスタント120を呼び出すフレーズ-をテキストに変換するように構成されてよい。他の音声入力は、クラウドベースのTTSモジュール116および/またはクラウドベースのSTTモジュール118を含んでよい自動アシスタント120に送られてよい。
Each voice acquisition / TTS / STT module 114 has one or more functions: for example, to acquire the user's voice through a microphone (not shown); to convert the acquired voice into text; and / Alternatively, it may be configured to convert text to speech. For example, in some implementations, the client device 106 may be relatively constrained in terms of computational resources (eg, processor cycle, memory, battery, etc.), so voice that is local to each client device 106. The acquisition / TTS / STT module 114 may be configured to convert a finite number of different verbal phrases, especially those that call the automatic assistant 120-to text. Other voice inputs may be sent to an
STTモジュール118は、クラウドの実質的に無限のリソースを活かして、音声取得/TTS/STTモジュール114によって取得された音声データをテキスト(次いで自然言語プロセッサ122に提供されてよい)へ変換するように構成されてよい。TTSモジュール116は、クラウドの実質的に無限のリソースを活かして、テキストデータ(例えば、自動アシスタント120によって作成される自然言語応答)をコンピュータ生成音声出力へ変換するように構成されてよい。一部の実装例では、TTSモジュール116は、コンピュータ生成音声出力をクライアント装置106に提供して、例えば、1つまたは複数のスピーカを使用して直接出力させてよい。他の実装例では、自動アシスタント120によって生成されるテキストデータ(例えば、自然言語応答)が音声取得/TTS/STTモジュール114に提供されてよく、次いでそこでローカルにテキストデータをコンピュータ生成音声出力へ変換してよい。
The
メッセージ交換クライアント1071-Nは様々な形態であってよく、そしてその形態はクライアントコンピューティング装置1061-Nにわたって異なってよく、かつ/または複数の形態がクライアントコンピューティング装置1061-Nの単一の1つにおいて動作されてよい。一部の実装例では、メッセージ交換クライアント1071-Nの1つまたは複数が、ショートメッセージサービス(「SMS」)および/またはマルチメディアメッセージサービス(「MMS」)クライアント、オンラインチャットクライアント(例えば、インスタントメッセンジャ、インターネットリレーチャットまたは「IRC」等)、ソーシャルネットワークと関連するメッセージアプリケーション、自動アシスタント120との会話に専用のパーソナルアシスタントメッセージサービス等の形態であってよい。一部の実装例では、メッセージ交換クライアント1071-Nの1つまたは複数が、クライアントコンピューティング装置106のウェブブラウザ(図示せず)または他のアプリケーションによって描画されるウェブページまたは他のリソースを介して実装されてよい。
The message exchange client 107 1-N may be in various forms, the form of which may vary across the client computing device 106 1-N , and / or multiple forms of the client computing device 106 1-N . It may be operated in one of the ones. In some implementations, one or more of the message exchange clients 107 1-N are short message service (“SMS”) and / or multimedia message service (“MMS”) clients, online chat clients (eg, instant). It may be in the form of a messenger, Internet Relay Chat or "IRC", etc.), a message application associated with a social network, a personal assistant message service dedicated to conversations with the
カメラアプリケーション1091-Nは、ユーザがカメラ1111-Nを制御することを可能にしてよい。例えば、カメラアプリケーション1091-Nの1つまたは複数が、ユーザが対話して、例えば、ビデオ会議等を記憶させるために、1つまたは複数の画像および/または映像を取得してよいグラフィカルユーザインタフェースを提供してよい。一部の実装例では、カメラアプリケーション1091-Nは、本明細書に記載されるように自動アシスタント120と対話/インタフェースして、ユーザが、例えば、カメラ1111-Nによって取得される画像と関連付けてユーザ選択メタデータを記憶させることを可能にしてよい。他の実装例では、カメラアプリケーション1091-Nの1つまたは複数が、ユーザが、例えば、カメラ1111-Nによって取得される画像と関連付けてユーザ選択メタデータを記憶させることを可能にする、自動アシスタント120と別個の、それ自身の内蔵機能性を有してよい。様々な実装例で、この記憶されたユーザ選択メタデータは、自動アシスタント120および/またはクライアント装置1061-Nの他の部品にとってアクセス可能で、後の時点でユーザによって検索および視聴/消費されてよい。追加的または代替的に、一部の実装例では、メッセージ交換クライアント107またはクライアント装置106にインストールされる任意の他のアプリケーションが、アプリケーションがカメラ111によって取得されるデータの他にそれとともに記憶されるメタデータにアクセスし、そして本明細書に記載される技法を行うことを可能にする機能性を含んでよい。
The camera application 109 1-N may allow the user to control the camera 111 1-N. For example, one or more of the camera applications 109 1-N may acquire one or more images and / or video for the user to interact and store, for example, a video conference, etc. Graphical user interface May be provided. In some implementations, the camera application 109 1-N interacts / interfaces with the
本明細書により詳細に記載されるように、自動アシスタント120は、1つまたは複数のクライアント装置1061-Nのユーザインターフェース入出力装置を介して1人または複数のユーザとのヒューマンコンピュータダイアログセッションに係わる。一部の実装例では、自動アシスタント120は、クライアント装置1061-Nの1つの1つまたは複数のユーザインターフェース入力装置を介してユーザによって与えられるユーザインターフェース入力に応じてユーザとのヒューマンコンピュータダイアログセッションに係わってよい。それらの実装例の一部では、ユーザインターフェース入力は明示的に自動アシスタント120に宛てられる。例えば、メッセージ交換クライアント1071-Nの1つが自動アシスタント120との会話に専用のパーソナルアシスタントメッセージサービスでよく、そしてそのパーソナルアシスタントメッセージサービスを介して与えられるユーザインターフェース入力が自動アシスタント120に自動的に提供されてよい。また、例えば、ユーザインターフェース入力は、自動アシスタント120が呼び出されることになることを示す特定のユーザインターフェース入力に基づいて、メッセージ交換クライアント1071-Nの1つまたは複数において明示的に自動アシスタント120に宛てられてよい。例えば、特定のユーザインターフェース入力は、1つもしくは複数のタイプ文字(例えば、@AutomatedAssistant)、ハードウェアボタンおよび/もしくは仮想ボタン(例えば、タップ、ロングタップ)でのユーザ対話、口頭コマンド(例えば、「ヘイ、自動アシスタント」)、ならびに/または他の特定のユーザインターフェース入力でよい。一部の実装例では、自動アシスタント120は、ユーザインターフェース入力が明示的に自動アシスタント120に宛てられないときでも、そのユーザインターフェース入力に応じてダイアログセッションに係わってよい。例えば、自動アシスタント120は、ユーザインターフェース入力の内容を検査し、そして或る用語がユーザインターフェース入力に存在していることに応じて、および/または他の指示に基づいて、ダイアログセッションに係わってよい。多くの実装例では、自動アシスタント120は、ユーザがコマンド、検索等を発してよいように、対話型音声応答(「IVR」)に係わってよく
、そして自動アシスタントは、自然言語処理および/または1つもしくは複数の文法を活用して発話をテキストへ変換し、それに応じて応答してよい。
As described in more detail herein, the
クライアントコンピューティング装置1061-Nの各々および自動アシスタント120は、データおよびソフトウェアアプリケーションの記憶のための1つまたは複数のメモリ、データにアクセスし、かつアプリケーションを実行するための1つまたは複数のプロセッサ、ならびにネットワークを通じた通信を容易にする他の部品を含んでよい。クライアントコンピューティング装置1061-Nの1つまたは複数によっておよび/または自動アシスタント120によって行われる動作は複数のコンピュータシステムにわたって分散されてよい。自動アシスタント120は、例えば、ネットワークを通じて互いに結合される1つまたは複数の場所の1つまたは複数のコンピュータ上で実行するコンピュータプログラムとして実装されてよい。
Each of the client computing devices 106 1-N and the
自動アシスタント120は、図1に示されない他の部品の中で、自然言語プロセッサ122および画像メモリエンジン130を含んでよい。一部の実装例では、自動アシスタント120のエンジンおよび/またはモジュールの1つまたは複数が省略、結合、および/または自動アシスタント120と別個である部品に実装されてよい。一部の実装例では、自動アシスタント120は、自動アシスタント120とのヒューマンコンピュータダイアログセッション中にクライアント装置1061-Nの1つのユーザによって生成される様々な入力に応じて応答内容を生成する。自動アシスタント120は、ダイアログセッションの一部としてユーザへの提示のために応答内容を(例えば、ユーザのクライアント装置と別個のときは1つまたは複数のネットワークを通じて)提供する。例えば、自動アシスタント120は、クライアント装置1061-Nの1つを介して与えられる自由形式自然言語入力に応じて応答内容を生成してよい。本明細書で使用されるように、自由形式入力は、ユーザによって作成され、かつユーザによる選択のために提示される一群の選択肢に制約されない入力である。
The
本明細書で使用されるように、「ダイアログセッション」は、ユーザと自動アシスタント120(と一部の場合には、スレッドにおける他の人間の参加者)との間の1つまたは複数のメッセージの論理的に独立した交換を含んでよい。自動アシスタント120は、セッション間の時間の経過、セッション間のユーザコンテキストの変化(例えば、場所、予定された会議前/中/後、等)、ユーザと自動アシスタントとの間のダイアログ以外のユーザとクライアント装置との間の1つまたは複数の介在作用の検出(例えば、ユーザがしばらくの間アプリケーションを切り替える、ユーザがスタンドアロン音声作動型製品から出て行き、その後戻ってくる)、セッション間のクライアント装置のロック/スリープ、自動アシスタント120の1つまたは複数のインスタンスとインタフェースするために使用されるクライアント装置の変更等などの、様々な信号に基づいてユーザとの複数のダイアログセッション間を区別してよい。
As used herein, a "dialog session" is a message of one or more messages between a user and the Auto Assistant 120 (and in some cases other human participants in a thread). It may include logically independent exchanges. The
一部の実装例では、自動アシスタント120がユーザフィードバックを要請するプロンプトを提供するとき、自動アシスタント120は、プロンプトに応じて受信されることになるユーザインターフェース入力を処理するように構成されるクライアント装置(プロンプトが提供される際に介する)の1つまたは複数の部品を事前に作動させてよい。例えば、ユーザインターフェース入力がクライアント装置1061のマイクロホンを介して与えられることになる場合、自動アシスタント120は、1つまたは複数のコマンドを提供して:マイクロホンが事前に「開かれる」(それによってインタフェース要素を打ってもしくは「ホットワード」を話してマイクロホンを開く必要を阻止する)、クライアント装置1061のローカル音声テキストプロセッサが事前に作動される、クライアント装置1061とリモート音声テキストプロセッサとの間の通信セッションが事前に確立される、かつ/またはグラフィカルユーザインタフェースがクライアント装置1061に描画される(例えば、フィードバックを与えるために選択されてよい1つもしくは複数の選択可能要素を含むインタフェース)ようにしてよい。これは、部品が事前に作動されない場合よりも、ユーザインターフェース入力が迅速に提供および/または処理されることを可能にしてよい。
In some implementations, when the
自動アシスタント120の自然言語プロセッサ122は、クライアント装置1061-Nを介してユーザによって生成される自然言語入力を処理し、そして画像メモリエンジン130などの自動アシスタント120の1つまたは複数の他の部品による使用のためのアノテーション付き出力を生成してよい。例えば、自然言語プロセッサ122は、クライアント装置1061の1つまたは複数のユーザインターフェース入力装置を介してユーザによって生成される自然言語自由形式入力を処理してよい。生成されるアノテーション付き出力は、自然言語入力の1つまたは複数のアノテーションおよび任意選択で自然言語入力の用語の1つまたは複数(例えば、全て)を含む。
The natural language processor 122 of the
一部の実装例では、自然言語プロセッサ122は、自然言語入力の様々な種類の文法情報を識別およびアノテーション付けするように構成される。例えば、自然言語プロセッサ122は、用語にそれらの文法的役割をアノテーション付けするように構成される品詞タガーの一部を含んでよい。例えば、品詞タガーは各用語に、「名詞」、「動詞」、「形容詞」、「代名詞」等などのその品詞でタグ付けしてよい。また、例えば、一部の実装例では、自然言語プロセッサ122は、自然言語入力における用語間の統語関係を決定するように構成される依存パーサ(図示せず)を追加的かつ/または代替的に含んでよい。例えば、依存パーサは、どの用語が文等(例えば、解析木)の他の用語、主語および動詞を修飾するかを判定してよく-そしてそのような依存のアノテーションを作ってよい。 In some implementations, the natural language processor 122 is configured to identify and annotate various types of grammatical information for natural language input. For example, the natural language processor 122 may include part of a part-speech tagger that is configured to annotate terms with their grammatical roles. For example, the part of speech tagger may tag each term with its part of speech, such as "noun," "verb," "adjective," "pronoun," and so on. Also, for example, in some implementations, the natural language processor 122 additionally and / or replaces a dependent parser (not shown) configured to determine syntactic relationships between terms in natural language input. May include. For example, a dependent parser may determine which term modifies other terms, subjects and verbs such as sentences (eg, parse trees)-and may create annotations for such dependencies.
一部の実装例では、自然言語プロセッサ122は、人々(例えば、文学上の人物、著名人、公人等を含む)、組織、場所(実在および仮想)等の参照など、1つまたは複数の区分における実体参照にアノテーション付けするように構成される実体タガー(図示せず)を追加的かつ/または代替的に含んでよい。一部の実装例では、実体についてのデータが、知識グラフ(図示せず)になど、1つまたは複数のデータベースに記憶されてよい。一部の実装例では、知識グラフは、既知の実体(および一部の場合には、実体属性)を表すノードの他に、ノードを接続しかつ実体間の関係を表すエッジを含んでよい。例えば、「バナナ」ノードが「果物」ノードに接続されてよく(例えば、子として)、次いで「果物」ノードは「農産物」および/または「食物」ノードに接続されてよい(例えば、子として)。別の例として、「仮説のカフェ」と呼ばれる飲食店が、その住所、出される食物の種類、営業時間、連絡先情報等などの属性も含むノードによって表されてよい。「仮説のカフェ」ノードは、一部の実装例では、「飲食店」ノード、「事業」ノード、飲食店が設けられる都市および/または州を表すノード等などの1つまたは複数の他のノードにエッジ(例えば、親子関係を表す)によって接続されてよい。 In some implementations, the natural language translator 122 includes one or more references to people (including, for example, literary figures, celebrities, public officials, etc.), organizations, places (real and virtual), etc. It may additionally and / or optionally include an entity tagger (not shown) configured to annotate the entity reference in the partition. In some implementations, data about the entity may be stored in one or more databases, such as in a knowledge graph (not shown). In some implementations, the knowledge graph may include nodes that represent known entities (and, in some cases, entity attributes), as well as edges that connect the nodes and represent relationships between entities. For example, a "banana" node may be connected to a "fruit" node (eg, as a child), and then a "fruit" node may be connected to a "agricultural" and / or "food" node (eg, as a child). .. As another example, a restaurant called a "hypothetical cafe" may be represented by a node that also includes attributes such as its address, type of food served, business hours, contact information, and so on. The "hypothetical cafe" node is one or more other nodes, such as a "restaurant" node, a "business" node, a node representing the city and / or state where the restaurant is located, etc. in some implementations. May be connected to by an edge (eg, representing a parent-child relationship).
自然言語プロセッサ122の実体タガーは、高レベルの粒度(例えば、人々などの実体クラスの全ての参照の特定を可能にする)および/または低レベルの粒度(例えば、特定の人物などの特定の実体の全ての参照の特定を可能にする)で実体の参照にアノテーション付けしてよい。実体タガーは、自然言語入力の内容に依存して特定の実体を解析してよく、かつ/または任意選択で知識グラフもしくは他の実体データベースと通信して特定の実体を解析してよい。 The entity tagger of the natural language processor 122 has a high level of granularity (for example, it allows the identification of all references of an entity class such as people) and / or a low level of granularity (for example, a specific entity such as a specific person). Allows the identification of all references in) to annotate entity references. The entity tagger may analyze a specific entity depending on the content of the natural language input, and / or optionally communicate with a knowledge graph or other entity database to analyze the specific entity.
一部の実装例では、自然言語プロセッサ122は、1つまたは複数のコンテキスト指示に基づいて同じ実体の参照をグループ化または「クラスタ化」するように構成される共参照解析器(coreference resolver)(図示せず)を追加的かつ/または代替的に含んでよい。例えば、共参照解析器が活用されて、自然言語入力「私は、前回私達がそこで食事をした仮説のカフェが好きだった」において用語「そこで」を「仮説のカフェ」に解析してよい。 In some implementations, the natural language processor 122 is configured to group or "cluster" references to the same entity based on one or more contextual instructions (coreference resolver) ( (Not shown) may be included additionally and / or as an alternative. For example, a semantic analyzer may be leveraged to parse the term "there" into a "hypothetical cafe" in the natural language input "I liked the hypothetical cafe we ate there last time." ..
一部の実装例では、自然言語プロセッサ122の1つまたは複数の部品が自然言語プロセッサ122の1つまたは複数の他の部品からのアノテーションに依存してよい。例えば、一部の実装例では、固有表現タガー(named entity tagger)は、特定の実体への全ての言及にアノテーション付けする際に共参照解析器および/または依存パーサからのアノテーションに依存してよい。また、例えば、一部の実装例では、共参照解析器は、同じ実体の参照をクラスタ化する際に依存パーサからのアノテーションに依存してよい。一部の実装例では、特定の自然言語入力を処理する際に、自然言語プロセッサ122の1つまたは複数の部品が、特定の自然言語入力以外で関連した以前の入力および/または他の関連データを使用して1つまたは複数のアノテーションを決定してよい。 In some implementations, one or more components of the natural language processor 122 may rely on annotations from one or more other components of the natural language processor 122. For example, in some implementations, named entity taggers may rely on annotations from semantic parsers and / or dependent parsers in annotating all references to a particular entity. .. Also, for example, in some implementations, the semantic analyzer may rely on annotations from dependent parsers when clustering references to the same entity. In some implementations, when processing a particular natural language input, one or more components of the natural language processor 122 were associated with other than the particular natural language input, and / or other related data. May be used to determine one or more annotations.
上述したように、自動アシスタント120は、例えば、画像メモリエンジン130を通じて、ユーザがカメラ1111-Nによって取得される画像に関連したユーザ選択メタデータを(例えば、1つまたは複数のデータベースなどの検索可能なコンピュータ可読媒体に)記憶させることを可能にしてよい。一部の実装例では、ユーザ選択メタデータは、例えば、画像メモリエンジン130によって、画像メタデータインデックス124に記憶されてよい。一部の実装例では、対応する画像は画像インデックス126に記憶されてよいが、但し、これは必須ではない(例えば、一部の実装例では、一旦メタデータが抽出されると画像は破棄されてよい)。一部のそのような実装例では、メタデータインデックス124における所与のメタデータレコードおよび画像データベース126における対応する画像が、一意の識別子、ファイル名等を介してなど、様々に関連付けられてよい。しかしながら、これは、限定的であるとは意味されない。様々な実装例で、ユーザ選択メタデータおよび対応する画像が共に単一のインデックスに記憶されてよい、またはメタデータだけが記憶されてよい。その上、ユーザ選択メタデータおよび/または対応する画像は、クライアント装置1061-Nにローカルに、クライアント装置1061-Nから遠隔の1つもしくは複数のコンピューティングシステムに、またはその任意の組合せで記憶されてよい。一般的に言って、本明細書に記載される数例が取得画像と関連付けてメタデータを記憶することを伴うが、これは必須ではなく、メタデータは分離して記憶されてよく、そして関連画像は記憶されてもされなくてもよい。
As mentioned above, the
ユーザ選択メタデータは様々な形態であってよい。一部の実装例では、ユーザ選択メタデータは、自動アシスタント120にユーザによって与えられるタスク要求に含まれる内容を含んでよい。例えば、ユーザは、ユーザの配偶者の写真を撮ってよく、そしてタスク要求(口頭またはタイプ入力)「これが私の配偶者であることを覚えていて」を与えてよい。写真は画像インデックス126に記憶されてよく、そして写真がユーザの配偶者を表すことを示すメタデータが画像メタデータインデックス124に記憶されてよい。
User-selected metadata may be in various forms. In some implementation examples, the user selection metadata may include the content contained in the task request given by the user to the
一部の実装例では、ユーザ選択メタデータは、メタデータを記憶せよとのユーザの要求に応じて生成される情報を含んでよい。例えば、一部の実装例では、ユーザ選択メタデータは、画像の取得と同時に得られる位置座標を含んでよい。ユーザが公園内の特に魅力的な範囲の写真を撮って、「私の次の瞑想セッションのためにこの場所を覚えていて」と言うとする。自動アシスタント120は、例えば、同範囲の画像が取得されるのと同時(または直前もしくは直後)にユーザのクライアント装置106上の全地球測位システム(「GPS」)センサによって生成される位置座標を得てよい。この位置座標は、例えば、画像インデックス126に記憶される画像と関連付けて画像メタデータインデックス124に記憶されてよい。この特定の例では、「瞑想」、「瞑想セッション」、「次の瞑想セッション」等など、後にユーザによって検索可能でよい追加のメタデータも画像メタデータインデックス124に記憶されてよい。追加的または代替的に、一部の実装例では、同場所は、例えば、保存した場所、ドロップピン等として、クライアント装置106にインストールされる地図アプリケーションに提供されてよい。ユーザが後に地図アプリケーションを操作して、その場所またはドロップピンをクリックすると、ユーザが取得した画像が提示されてもされなくてもよい。
In some implementations, the user-selected metadata may include information generated in response to the user's request to store the metadata. For example, in some implementations, the user-selected metadata may include position coordinates obtained at the same time as the image is acquired. Suppose a user takes a picture of a particularly attractive area of the park and says, "Remember this place for my next meditation session." The
さらに他の実装例では、ユーザ選択メタデータは、光学文字認識(「OCR」)を使用して取得されるテキスト、様々な物体認識技法に基づいて、および/または画像内の視覚表示(例えば、バーコード、クイックレスポンスもしくは「QR」コード等)を読み込むことによって、例えば、下記の画像処理エンジン132によって画像に識別される物体など、画像自体に含まれる情報を含んでよい。このように、「ユーザ選択メタデータ」が、ユーザによって明示的かつ/または故意に入力される情報に限定されるのではなく、ユーザが本明細書に記載される技法を開始させて、例えば、画像と関連付けてメタデータを記憶させる結果として生成、観察、取得および/または認識される情報も含むことが理解されるべきである。
In yet another implementation, user-selected metadata is based on text obtained using Optical Character Recognition (“OCR”), various object recognition techniques, and / or visual representations within images (eg, for example). By reading a barcode, quick response, "QR" code, etc.), information contained in the image itself, such as an object identified in the image by the
画像メモリエンジン130は続いて、クライアント装置106でユーザから受け取られる自由形式入力に応じて、画像メタデータインデックス124からユーザ選択データを、および/または画像インデックス126から対応する画像を取り出すようにも構成されてよい。以上の例を続けて、同じユーザまたは異なるユーザが後に検索クエリ「私の次の瞑想セッションのための場所はどこか?」を与えるとする。クエリは、公園の魅力的な範囲の画像に関連したメタデータに合致されてよい。様々な実装例で、応答するメタデータおよび/または対応する画像は次いで、例えば、クライアント装置106のディスプレイに描画されるまたは1つもしくは複数のスピーカを使用して可聴的に再生されることによって示されてよい。一例として、瞑想するユーザに、その者が取得した公園の魅力的な範囲の画像に加えて、ユーザが再び同場所を見つけることを可能にする情報(すなわち位置座標)が提示されてよい。この情報は、例えば、クライアント装置106にインストールされる地図アプリケーションを呼び出すためにユーザが選択してよい、同場所が予めロードされた、いわゆる選択可能な「ディープリンク」を含んでよい。他のインスタンスでは、同情報は、同場所が、例えば、「X」またはドロップピンでマークされた描画地図を含んでよい。
The image memory engine 130 is also configured to retrieve user-selected data from the
一部の実装例では、上述の画像処理エンジン132は、自動アシスタント120と一体であってもなくてもよいが、画像インデックス126における画像に様々な種類の画像処理を行うように構成されてよい。一部のそのような実装例では、この画像処理は、本明細書に記載される技法の使用によって開始されてよい。例えば、ユーザは、本明細書に記載される技法を利用して、取得画像における物体がユーザの配偶者などの実体として識別されるように要求してよい。一部のそのような実装例では、その画像はユーザ指示実体と関連する基準画像として使用されてよい。一部の実装例では、異なる画像に同じ実体を識別するために、その画像におよび/または画像インデックス126に記憶される他の画像に、例えば、画像処理エンジン132によって画像処理が行われてよい。このように、例えば、一旦ユーザがユーザの妻にカメラを向けて「これは私の妻である」と言うと、その画像は、以前に取得されたおよび/または将来取得されるであろう、ユーザの妻の他の画像にラベル付けするための基準画像として使用されてよい。
In some implementations, the
追加的または代替的に、画像処理エンジン132は、ユーザ選択メタデータに関連した画像に示される物体および/またはテキストを識別するために、物体認識、OCR等などの、他の種類の画像処理を行うように構成されてよい。一部の実装例では、認識された物体の正体(例えば、特定の製品、特定の種類の食物、特定の種類の車両、特定の1本のワイン等)は画像メタデータインデックス124にユーザ選択メタデータとして記憶されてよく、そして様々に使用されてよい。以下に記載されることになるように、一部の実装例では、ユーザが将来類似の物体(例えば、1つまたは複数の属性を共有する物体)の方へクライアント装置のカメラを向けると、画像処理エンジン132は、ユーザ選択メタデータと関連付けて記憶される画像に示される物体と電子ビューファインダに現在取得されている物体を照合してよい。一部のそのような実装例では、画像メモリエンジン130は次いで、現在のカメラビューに視覚的にアノテーション付けして、物体と関連するユーザ選択メタデータを提供する。
Additional or alternative, the
例えば、ユーザはユーザがアレルギーを有する特定のナッツの画像を取得し、その画像を、例えば、ユーザのアレルギーを示すメタデータ(例えば、「私がこの種類のナッツにアレルギーがあることを覚えていて」)と関連付けて記憶できる。その後電子ビューファインダに同じ種類のナッツが示されるたびに、その示されたナッツは、例えば、画像処理エンジン132によって認識され、そして視覚的にアノテーション付けされてユーザにアレルギーを思い出させて/警告してよい。追加的または代替的に、可聴のアノテーション(例えば、自動アシスタント120によって話される)、警報、振動等などの非視覚的アノテーションが使用されてよい。類似の手法が、いわゆる「スマートグラス」または環境の拡張現実感視聴を容易にする他の技術(例えば、テレプレゼンスロボット、ビデオ会議等)とともに使用されてよい。例えば、ユーザがスマートグラスを通して同じ種類のナッツを見るたびに、スマートグラスは、例えば、同じ種類のナッツのユーザの以前の取得画像に基づいてナッツを認識し、そしてナッツを視覚的にアノテーション付けして(例えば、それを赤くハイライトする、メタデータをテキストとして表示する等)、リアルタイムにユーザにアレルギーを思い出させてよい。
For example, the user takes an image of a particular nut for which the user is allergic and uses that image, for example, metadata indicating the user's allergy (eg, "remember that I am allergic to this type of nut." Can be memorized in association with). Each time the electronic viewfinder subsequently shows the same type of nuts, the indicated nuts are, for example, recognized by the
画像処理エンジン132は、各種の目的を達成するために様々な技法を使用して画像を処理するように構成されてよい。例えば、画像に示される物体(例えば、車両、衣類、製品、場所等)を認識するために、画像処理エンジン132は、解釈木、ポーズ一貫性およびポーズクラスタ化などの特徴ベースの方法、エッジマッチング、分割統治探索、勾配マッチングおよび/またはヒストグラムなどの外観ベースの方法、コンピュータ支援設計(「CAD」)モデル比較、様々な種類の機械学習モデル(例えば、畳み込みおよび他の種類の訓練ニューラルネットワーク)を利用する方法、特徴検出等を含む技法を利用してよい。類似して、画像処理エンジン132は、マトリックスマッチング、特徴抽出等を含むがこれらに限定されない、OCRを行う各種の技法を利用してよい。
The
図2は、クライアント装置206上で動作して本明細書に記載される技法を行う自動アシスタント(図1において120、図2に図示せず)のインスタンスとユーザ(図示せず)がどのように対話できるかの一例を例示する。クライアント装置206は、タッチスクリーン240ならびに少なくとも1つのカメラ211(前面および/または背面)を含むスマートフォンまたはタブレットコンピュータの形態をとる。タッチスクリーン240に描画されるのが、カメラアプリケーション(例えば、図1において109)と関連するグラフィカルユーザインタフェースであり、カメラ211によって取得される光データを、例えば、リアルタイムに描画する電子ビューファインダを含む。グラフィカルユーザインタフェースは、ユーザ入力フィールド244、およびカメラ211の動作を制御するように動作可能でよい1つまたは複数のグラフィカル要素2461-3を含む。例えば、第1のグラフィカル要素2461は、前面および背面カメラ間を切り替えるように動作可能でよく、第2のグラフィカル要素2462は、カメラ211を使用して画像(または設定に応じて映像)を取得するように動作可能でよく、そして第3のグラフィカル要素2463は、以前に取得された写真を見るように動作可能でよい。図2に示されない他のグラフィカル要素は、カメラ設定を変更する、画像取得および映像取得モード間を切り替える、様々な効果を追加する等などの、他の作用を行うように動作可能でよい。
FIG. 2 shows how an instance and a user (not shown) of an automated assistant (120 in FIG. 1, not shown in FIG. 2) running on
ユーザ入力フィールド244は、本明細書に記載される様々な実装例に従って、クライアント装置206上で実行する自動アシスタント120(図2に図示せず)に1つまたは複数の取得画像に関連したメタデータを記憶するように命令する自由形式自然言語入力などの、様々な入力を与えるようにユーザによって操作可能でよい。ユーザは、ユーザ入力フィールド244への入力として、テキスト、音声(例えば、右側のマイクロホンアイコンをクリックすることによる)、画像等を与えてよい。様々な実装例で、ユーザ入力フィールド244を介して与えられる音声入力は、例えば、クライアント装置206においておよび/または遠隔で(例えば、1つまたは複数のクラウドベースの部品において)テキストに変換されてよい。
一例として、図2におけるカメラ211はその視界に1本のワイン248を捕えた。結果的に、1本のワイン248の描画248'が上述の電子ビューファインダの一部としてタッチスクリーン240に現れる。ユーザが1本のワイン248についての意見を文書化したいためにユーザがこの写真を撮っているとする。様々な実装例で、ユーザは、例えば、ユーザ入力フィールド244内をタップすることによってまたは「ヘイ、自動アシスタント」などの呼出しフレーズを話すことによって自動アシスタント120を呼び出してよい。一旦自動アシスタント120が呼び出されると、ユーザは、例えば、ユーザ選択メタデータと関連付けて1本のワイン248の画像(カメラ211によって取得される)を記憶せよとのタスク要求を含む自然言語入力を話してもまたはタイプしてもよい。追加的または代替的に、ユーザは、自動アシスタント120を呼び出しかつタスク要求を与える単一の自然言語入力を与えてよい。一部の実装例では、カメラアプリケーション109がクライアント装置上でアクティブなときはいつでも、自動アシスタント120は自動的に呼び出されてよい、またはそれ以外の場合には自動アシスタント120を呼び出さない異なる呼出しフレーズに応じて呼び出されてよい。例えば、一部の実装例では、カメラアプリケーション109がアクティブである(すなわち、ユーザによって対話されている、グラフィカルユーザインタフェースとして提示されている等)とき、「…ことを覚えていて」、「…ことを気に留めておいて」、「後で…ことを私に思い出させて」および類似のフレーズなどのフレーズが自動アシスタント120を呼び出してよい。
As an example,
様々な実装例で、画像を取得するおよびユーザ選択メタデータを記憶するステップは共にまたは別々に行われてよい。例えば、一部の実装例では、ユーザは、写真を撮ってよく(例えば、図2において要素2462を押すことによって)、次いで自動アシスタント120を呼び出して、例えば、最後に撮った画像と関連付けてユーザ選択メタデータを記憶させてよい。他の実装例では、ユーザは、所望の目標にカメラ211を向け(例えば、電子ビューファインダに所望の目標を捕える)、次いで自動アシスタント120を呼び出してかつ/またはタスク要求を与えてよく、これらにより画像が取得され、かつメタデータ(および一部の場合には、取得画像)を記憶させてよい。
In various implementations, the steps of acquiring images and storing user-selected metadata may be done together or separately. For example, in some implementations, the user may take a picture (eg, by pressing element 246 2 in Figure 2) and then call
図2において、ユーザがフレーズ「私がこの種類のワインが好きであることを覚えていて」を話すとする。自動アシスタント120は、口頭フレーズからタスク要求を認識し、そしてタスク要求がメタデータと関連付けてカメラ211によって取得される1つまたは複数の画像を記憶せよとの要求を含むと判定してよい。自動アシスタント120は次いで、例えば、画像インデックス126(クライアント装置206に対してローカルおよび/またはクラウドベースでよい)に、カメラ211によって取得される1つまたは複数の画像を記憶してよい。加えて、一部の実装例では、自動アシスタント120は、例えば、画像メタデータインデックス124に、メタデータを記憶画像と関連付けて記憶してよい。この例では、メタデータは、「私はこの種類のワインが好きである」などのユーザのタスク要求の内容の一部または全てを含んでよい。
In Figure 2, suppose the user speaks the phrase "remember I like this kind of wine". The
一部の実装例では、画像を取得することおよび/またはタスク要求を与えることは、例えば、自動アシスタント120および/または画像処理エンジン132によって行われてよい追加のタスクも開始させてよい。例えば、一部の実装例では、取得物から直接追加のメタデータを得ようと試みる、OCR、物体認識、バーコードスキャン等などの、サブタスクが開始されてよい。一部の実装例では、これらのサブタスクの1つまたは複数が、ユーザによって与えられたタスク要求に基づいて選択されてよい(例えば、ユーザが自分の要求において製品の名前を挙げた場合、画像処理エンジン132はOCRおよび/またはバーコードスキャンを行おうと試みてよい)。
In some implementations, retrieving images and / or giving task requests may also initiate additional tasks that may be performed, for example, by the
後に、同じユーザまたは異なるユーザが、例えば、自動アシスタント120に、記憶されたメタデータが応答する検索クエリを与えてよい。例えば、ユーザは、ユーザの自動アシスタント120にフレーズ「どのボトルのワインを私は好きであるか?」を話すことができる。記憶されたメタデータがこの検索クエリに応答するので、ユーザの自動アシスタント120は、記憶されたメタデータの一部または全てを含む応答を提供してよい(例えば、その未処理の形式で、または文として再作成されて)。追加的または代替的に、ユーザの自動アシスタント120は、例えば、タッチスクリーン240に以前の記憶画像を示してよい。もちろん、ユーザが自分が楽しむ複数本のワインの複数の画像を取得する(そして本明細書に記載される技法を使用してユーザ選択メタデータと関連付けてそれらを記憶させる)場合、そのような検索クエリは複数の結果を返してよい。一部のそのような実装例では、複数の応答する画像は個々にかつ/またはコラージュとして返されてよい。
Later, the same user or different users may, for example, give Auto Assistant 120 a search query in which the stored metadata responds. For example, the user can speak the phrase "Which bottle of wine do I like?" To the user's
図3は、図2におけるワインのボトル248の記憶画像がさらにユーザを支援するために使用できる1つの可能な応用を示す。図3において、クライアント装置306は、着用者のためのディスプレイ装置として作用するように構成される透明レンズ350を含むいわゆる「スマート」グラスの形態をとる。クライアント装置306は、着用者がレンズ350を通して見るものにほぼ対応する視界を持つ1つまたは複数のカメラ311も含んでよい。一部の実装例では、本明細書に記載される技法を使用してユーザ選択メタデータと関連付けて記憶される画像は、例えば、「拡張現実感」表示技法を使用してクライアント装置306の着用者に情報を提供するために使用されてよい。
FIG. 3 shows one possible application in which the stored image of the
例えば、図3において、クライアント装置306が、図2からのクライアント装置206と同じクライアント装置の協調エコシステムの一部であるとする。さらにクライアント装置306の着用者が複数のワインボトル348が棚に配置されている店舗を訪れているとする。着用者は、レンズ350を通して複数のワインボトル348を見ることができる。同時に、カメラ311によって取得される光データは、例えば、画像処理エンジン132によって解析されて、複数のワインのボトル348の1つまたは複数を以前の取得画像(例えば、画像インデックス126における)に照合してよい。次いで、一致する画像と関連付けて記憶されたユーザ選択メタデータが、例えば、レンズ350に描画される拡張現実感アノテーションとして、クライアント装置306の着用者に提示されてよい。図3において、例えば、1つのワインのボトルがテキスト「あなたはこれが『素晴らしい』と言った」でアノテーション付けされ、そして別の1つがテキスト「あなたはこれが『マイルドだ』と言った」でアノテーション付けされる。
For example, in FIG. 3, it is assumed that the
この例によって立証されるように、本明細書に記載される技法の様々な実装例で、ユーザ選択メタデータは、呼び戻される(例えば、検索クエリまたは何らかの他のイベントに応じて)とき、様々に示されてよい(例えば、視覚的または可聴的に出力される)。一部のインスタンスでは、それは逐語的にユーザに示されてよい。他の実装例では、メタデータは、メタデータの内容、ユーザの現在のコンテキスト等などの、様々な要因に応じて再作成されてもかつ/または言い換えられてもよい。例えば、駐車した車の画像に関連した位置座標は、「あなたの車は<場所>にある」などの言葉を使用してユーザに示されてよい。別の例として、ユーザがタスク要求「私に<メタデータ>を思い出させて」を出す場合、そのメタデータが後に呼び戻されるとき、自動アシスタント120はそれを「あなたは<メタデータ>と言った」などの言葉で始めてよい。
As evidenced by this example, in various implementations of the techniques described herein, user-selected metadata varies when recalled (eg, in response to a search query or some other event). May be shown (eg, output visually or audibly). In some instances it may be shown to the user verbatim. In other implementations, the metadata may be recreated and / or paraphrased depending on various factors such as the content of the metadata, the user's current context, and so on. For example, the position coordinates associated with an image of a parked car may be shown to the user using words such as "Your car is at <location>". As another example, if a user issues a task request "Remind me of <metadata>", when that metadata is later recalled,
様々な実装例で、図3に立証されるように物体および/または他の実体を認識する能力は常に有効にされてもかつ/または選択的に有効にされてもよい。例えば、クライアント装置306が、それが画像処理エンジン132などのクラウドベースの部品とデータを効率的に交換できるように、強固なネットワーク接続を有する場合、拡張現実感アノテーションがユーザに表示できるように、それはカメラ311を開始させてデータを取得してよい(例えば、リアルタイムに、連続的に、ネットワーク強度に基づいて予め決定および/または選択されるフレームレートで等)。他方では、クライアント装置306のネットワーク接続が弱いもしくは存在しない場合、またはクライアント装置306が比較的小さいバッテリ電力を有する場合、カメラ311は無効にされてもよく、かつ/またはカメラ311によって取得される光データは図3に図示されるように画像処理されなくてよい。一部の実装例では、ユーザは、例えば、「私がコメントしたワインのボトルをハイライトして」などのフレーズを話すことによって、カメラ311を手動で作動させて本明細書に記載される技法を実装することも可能でよい。
In various implementations, the ability to recognize objects and / or other entities may always be enabled and / or selectively enabled, as evidenced in FIG. For example, if the
図3の例が、ユーザが特定の製品を識別するのを支援することに関するが、これは限定的であるとは意味されない。様々な実装例で、着用者が他の物体および/または実体を識別するのを支援するために類似の技法が利用されてよい。例えば、一部の実装例では、ユーザはユーザがアレルギーを有する特定の種類のナッツの画像を、その旨のメタデータ(例えば、「私がこの種類のナッツにアレルギーがあることを覚えていて」)と関連付けて記憶させてよい。クライアント装置306を着用すると、その特定の種類の任意の他のナッツは以前の取得画像に照合され、そして例えば、「あなたはこのナッツにアレルギーがある」などの言葉でアノテーション付けされてよい(「私がこの種類のナッツにアレルギーがあることを覚えていて」のタスク要求からの再作成に留意されたい)。例えば、他の個人の名前または他の個人についての他の情報(例えば、「サリーの誕生日は6月4日である」、「ビルはニックスが好きである」、「この人物の名前はアルであり、そして彼の妻の名前はジェーンである」等)を思い出すために、ユーザによって類似の技法が使用できる。
The example in Figure 3 relates to helping the user identify a particular product, but this is not meant to be limited. In various implementations, similar techniques may be utilized to assist the wearer in identifying other objects and / or entities. For example, in some implementations, the user has an image of a particular type of nut that the user is allergic to, with metadata to that effect (eg, "remember that I am allergic to this type of nut". ) May be associated and stored. When wearing the
図4は、図2におけるクライアント装置206と基本的に同じ部品で構成される(したがって類似して番号付けされる)クライアント装置406を使用して、例えば、取得画像と関連付けてユーザ選択メタデータを記憶するために本明細書に記載される技法を使用することの別の一例を示す。この例では、ユーザが特定の施設452を通り過ぎており、そしてカメラ411の視界に施設452を捕えるとする。結果として、施設452の描画452'が、例えば、カメラアプリケーション(例えば、図1において109)のグラフィカルユーザインタフェースの一部を形成する電子ビューファインダ内で、タッチスクリーン440に描画される。さらに、例えば、ユーザが第2のグラフィカル要素4462を押して画像を取得する直前に、直後に、または同時に、ユーザがタイプ(例えば、ユーザ入力フィールド444を介する)または「後で私にこの場所を調べることを思い出させて」などの口頭自然言語入力を与えるとする。施設452の描画452を含む画像は、ユーザの自然言語入力に基づいて生成される(例えば、逐語的に、再作成されて、抽出キーワード等)メタデータと関連付けて記憶されてもされなくてもよい。後に、「どの場所を私は調べたかったか?」などの検索クエリを与えるとする。その検索クエリは記憶されたメタデータと合致してよい。したがって、メタデータ(および一部の場合には以前の記憶画像)は、例えば、タッチスクリーン440に描画されることによってユーザに示されてよい。
FIG. 4 uses a
上記したように、一部の実装例では、ユーザ選択メタデータは、ユーザによって明示的に入力されない情報を含んでよい。図4の例では、例えば、ユーザの自然言語入力が用語「場所」を含んだという事実が自動アシスタント120を開始させて、施設452の画像と関連する位置座標を得てよい。画像が従来のメタデータとしてジオタグと関連付けて既に記憶されている一部のインスタンスでは、ジオタグは、画像の従来のメタデータから複製されて、画像メタデータインデックス124に記憶されてよい。
As mentioned above, in some implementations, the user-selected metadata may include information that is not explicitly entered by the user. In the example of FIG. 4, for example, the fact that the user's natural language input included the term "location" may initiate the
追加的または代替的に、一部の実装例では、自動アシスタント120は、例えば、画像処理エンジン132を通じて、取得した画像(または電子ビューファインダに描画される画像データ)を処理して、画像に示される1つまたは複数の物体および/またはテキストを認識してよい。例えば、施設452は、施設の名前および/または住所を伝達するテキスト(図4に図示せず)を含む看板454を含んでよい。示されたテキストは、例えば、画像処理エンジン132によってOCRされ、そして認識されたテキストは、例えば、インデックス124にユーザ選択メタデータとして全部または一部が記憶されてよい。このように、例えば、図4において、施設452の名前および/または住所は、ユーザ選択メタデータとして、例えば、取得画像と関連付けて記憶されてよい。後に、ユーザはこの住所を探して、施設452の以前の取得画像を示すことができる。追加的または代替的に、施設452の以前の取得画像がユーザの検索クエリに応じて示されるとき、施設452の名前および/または住所も示されてよい。例えば、Wi-Fiルータの表面からWi-Fi SSIDおよびパスワードを認識および記憶するために、パスポートからパスポート番号および/または失効日を認識および記憶するために等、類似の技法が使用されてよい。
Additional or alternative, in some implementations, the
図5は、開示される技法が利用できる別のシナリオ例を示す。図5において、クライアント装置506は、ユーザ501がクライアント装置506上で少なくとも部分的に実行する自動アシスタント120との口頭ヒューマンコンピュータダイアログに係わることを可能にするスタンドアロン対話型スピーカの形態をとる。このために、クライアント装置506は、ユーザ501からの口頭入力を検出するための1つまたは複数のマイクロホン(図5に図示せず)も含んでよい。クライアント装置506は、例えば、ユーザ501からのタスク要求に応じて画像を取得するように構成される少なくとも1つのカメラ511も含む。図5に示されないが、一部の実装例では、クライアント装置506は、カメラ511とともに使用されると、本開示と関連するものなどの或る追加特徴を可能にするディスプレイ装置も含んでよい。
Figure 5 shows another example scenario in which the disclosed technique is available. In FIG. 5, the client device 506 takes the form of a stand-alone interactive speaker that allows the
この例では、ユーザ501はイベントのチケット560を購入した。図5に図示されないが、チケット560に印刷されるのが、場所、日付、時間等などのイベントの様々な詳細であることが前提とされる。本明細書に記載される技法を使用して、ユーザ501は、自動アシスタント120を開始させてイベントの詳細を思い出すことが可能でよい。例えば、ユーザ501は、例えば、カメラ511の視界にチケット560を保持しつつ、以下の口頭自然言語入力「この今度のイベントについてのこれらのチケット詳細を覚えていて下さい」を与えてよい。その瞬間に、自動アシスタント120は、カメラ511にチケット560の画像を取得させてよい。画像処理エンジン132が次いで、光学文字認識または他の類似の技法を行ってチケット560の正面からテキストを抽出してよい。このテキストは処理(例えば、構文解析等)されて、場所、時間、日付等などのイベント詳細を抽出してよい。これらのイベント詳細は次いで、例えば、取得画像と関連付けてユーザ選択メタデータとして記憶されてよい。
In this example,
ユーザ501または別のユーザが後に、例えば、「今度のイベントについて私に話して」など、イベントに宛てられる検索クエリ、または「金曜日の夜の私の予定は何か?」など、イベントに間接的に関連した検索クエリを与えることによって、取得画像および/またはユーザ選択メタデータを呼び戻してよい。(イベントが金曜日の夜であるとの前提で、自動アシスタント120は、記憶されたメタデータが検索クエリに応答すると判定してよい)。
追加的または代替的に、一部の実装例では、自動アシスタント120は、ユーザ501のカレンダーのカレンダー項目にイベント詳細を記憶してよい(そして図5に示されるように、ユーザにこの事実を通知してよい)。より一般に、様々な実装例で、ユーザ選択メタデータは、一般の画像メタデータインデックス124に加えてまたはその代わりに、ユーザのカレンダー、メモリスト、連絡先等になど、適切な場所に記憶されてよい。例えば、ユーザが他の誰かの身分証明書(例えば、運転免許証、パスポート、名札、名刺等)の画像を取得する場合、ユーザは、身分証明書から抽出されるテキストが画像メタデータインデックス124におよび/またはユーザの連絡先リストにメタデータとして記憶される(覚えられる)ように要求できる。
As an additional or alternative, in some implementations,
一部の実装例では、ユーザがタスク要求を率先して与える代わりに、タスク要求は、ユーザへの提案として1つまたは複数の出力装置を介して提供されてよい。様々な実装例で、タスク要求は、クライアント装置の1つまたは複数のセンサによって生成される1つまたは複数の信号に基づいて選択されてよい。一部の実装例では、1つまたは複数の信号は、クライアント装置のカメラによって取得されるデータを含んでよい。一部の実装例では、1つまたは複数の信号は、位置座標センサからの位置座標データを含んでよい。どんな信号が使用されても、様々な実装例で、提案されたタスク要求が、例えば、自動アシスタント120によって、ユーザへの視覚または可聴のプロンプトとして提供されてよい。
In some implementations, instead of the user proactively giving the task request, the task request may be provided via one or more output devices as a suggestion to the user. In various implementations, task requests may be selected based on one or more signals generated by one or more sensors on the client device. In some implementations, the signal may include data acquired by the camera of the client device. In some implementations, one or more signals may include position coordinate data from the position coordinate sensor. Whatever signal is used, in various implementations, the proposed task request may be provided, for example, by the
これの1つの例が図6に示されており、クライアント装置206および406に含まれたのと同じ部品の多くを含む(それ故類似して番号付けされる)タブレットまたはスマートフォンの形態のクライアント装置606を再び示す。この例では、ユーザ(図示せず)は、クライアント装置606を操作して、カメラ611を介して車両660の画像を取得した。様々な実装例で、画像処理エンジン132が画像を処理して、示された物体を車両660であるとして識別してよい。応えて、自動アシスタント120は、ユーザが開始させる提案されたタスク要求を含む自然言語出力を率先して提供してよい。例えば、図6において、自動アシスタント120(「AA」)は出力「これは駐車した車両のように見える。この画像と関連する駐車場所を覚えていて欲しい?」を提供する。一部の実装例では、ユーザは単に「はい」または「オッケー」と応答してよく(音声でまたは不図示の入力フィールドにタイプすることによって)、そして自動アシスタント120は、本明細書に記載される技法を利用して、例えば、車両660の画像と関連付けて、例えば、位置座標を含むメタデータを記憶してよい。
An example of this is shown in Figure 6, which is a client device in the form of a tablet or smartphone that contains many of the same parts contained in
追加的または代替的に、一部の実装例では(図6の例を含む)、1つまたは複数の選択可能要素662が率先して提示されてよく、その結果ユーザは、自由形式自然言語入力を与える代わりに要素を選択できる。図6において、例えば、第1の選択可能要素6621は肯定し、第2の選択可能要素6622は否定し、そして第3の選択可能要素6623は断わるが、自動アシスタント120に、示された車両がユーザの車であることを覚えているように命令する。ユーザが最後の選択可能要素6623を選択した場合、一部の実装例では、「私の車」などの内容および画像処理エンジン132によって見分けられる車両660の任意の属性が、例えば、取得画像と関連付けてユーザ選択メタデータとして記憶されてよい。上記したように、一部の実装例では、画像処理エンジン132は、同じまたは類似のメタデータを持つユーザの車を同様に示す任意の他の画像を解析およびラベル付けしてよい。
Additional or alternative, in some implementations (including the example in Figure 6), one or more
様々な実装例で、画像処理エンジン132は、取得画像および/または電子ビューファインダの内容(例えば、フレームバッファからの)を解析して、テキストおよび/または認識可能な物体を検出しようと試みてよく、そしてこの認識は、自動アシスタント120に662などの選択可能要素を率先して提供するように促してよい。例えば、画像処理エンジン132は、駐車した車両の近くの区画番号または駐車区域を検出してよい。これは、位置座標が得るのが困難である(例えば、弱いGPS信号)地下駐車場および他の構造物で特に有益であろう。一部のそのような実装例では、自動アシスタント120は、できるだけ早く(例えば、一旦ユーザが構造物を離れると)位置座標を得、そしてそれを記憶されたメタデータとして認識された区画番号と結合してよい。後に、自動アシスタント120および/または別個の地図アプリケーションは、位置座標を使用して駐車構造物の元へユーザを案内し、次いでユーザが見つけるべき区画番号を提供してよい。追加的または代替的に、たとえ車の位置座標が入手可能でなくても、自動アシスタントは2種類のデータを結合して内部マップを合成してよい。
In various implementations, the
図7は、本明細書に開示される実装例に係る方法例700を例示するフローチャートである。便宜上、フローチャートの動作は、動作を行うシステムに関して記載される。このシステムは、自動アシスタント120の1つまたは複数の部品などの、様々なコンピュータシステムの様々な部品を含んでよい。その上、方法700の動作が特定の順に図示されるが、これは限定的であるとは意味されない。1つまたは複数の動作が再順序付け、省略または追加されてよい。
FIG. 7 is a flowchart illustrating a method example 700 according to the implementation example disclosed in the present specification. For convenience, the operation of the flowchart is described with respect to the operating system. This system may include various parts of various computer systems, such as one or more parts of the
ブロック702で、システムは、ユーザによって操作される1つまたは複数のクライアント装置のうちのクライアント装置(例えば、106、206、306、406、506、606)の1つまたは複数の入力インタフェースで、ユーザからの自然言語または「自由形式」入力を受信してよい。様々な実装例で、この自然言語入力は、口頭入力および/またはタイプ入力の形態であってよい。一部の実装例では、口頭入力は、例えば、自然言語プロセッサ122に与えられる前に、テキストに変換されてよい。図6に関して上記したように、他の実装例では、入力は、提案タスク要求を表すグラフィカル要素をユーザが選択する形態をとってよい。
At
ブロック704で、システムは、入力からタスク要求を認識してよく、そしてブロック706で、システムは、タスク要求が、クライアント装置のカメラによって取得される1つまたは複数の画像に関連したメタデータを記憶せよとの要求を備えると判定してよい。一部の実装例では、「…ことを覚えていて」、「…ことを気に留めておいて」、「後で…ことを私に思い出させて」等などのキーワードまたはフレーズが、単独でまたは他の指示(例えば、カメラアプリケーション109がアクティブである)との組合せで、自動アシスタント120を開始させて、ユーザ選択メタデータと関連付けて取得画像を記憶せよとのタスク要求をユーザが与えたと認識してよい。様々な実装例で、メタデータは、タスク要求の内容に基づいて選択されてよく、かつ/またはタスク要求に応じて取得される、認識される、得られる等の他のデータ(例えば、OCRされたテキスト、位置座標、物体分類等)を含んでよい。
At block 704, the system may recognize the task request from the input, and at
ブロック708で、システムは、1つまたは複数のコンピュータ可読媒体にメタデータを記憶してよい。様々な実装例で、1つまたは複数のコンピュータ可読媒体は、例えば、ユーザが後に以前の取得画像および/または対応するユーザ選択メタデータを取り出せるようにメタデータを使用して検索可能でよい。そのような検索のためのフロー例が図8に示される。一部の実装例では、メタデータは1つまたは複数の取得画像と関連付けて記憶されてよいが、但し、これは必須ではない(例えば、一旦関連メタデータが、例えば、物体認識および/またはOCRを使用して取得されると画像は廃棄されてよい)。
At
図8は、本明細書に開示される実装例に係る別の方法例800を例示する別のフローチャートである。便宜上、フローチャートの動作は、動作を行うシステムに関して記載される。このシステムは、自動アシスタント120の1つまたは複数の部品などの、様々なコンピュータシステムの様々な部品を含んでよい。その上、方法800の動作が特定の順に図示されるが、これは限定的であるとは意味されない。1つまたは複数の動作が再順序付け、省略または追加されてよい。一部の実装例では、方法800の動作は方法700の動作の後に起こってよいが、但し、これは必須ではない。
FIG. 8 is another flowchart illustrating another method example 800 according to the implementation examples disclosed herein. For convenience, the operation of the flowchart is described with respect to the operating system. This system may include various parts of various computer systems, such as one or more parts of the
ブロック802で、システムは、クライアント装置で、自由形式入力を受け取ってよい。図7のブロック702での場合のように、ブロック802で受け取られる入力は、自動アシスタント120に宛てられてよい口頭またはタイプ自然言語入力でよい。ブロック804で、システムは、自由形式入力からタスク要求を認識してよく、そして一部の場合にはタスク要求を画像メモリエンジン130に提供してよい。一部の実装例では、このタスク要求は検索クエリ(例えば、「私の息子の写真を見つける」)の形態をとってもよいが、但し、これは必須ではない。
At
ブロック806で、システムは、例えば、画像メモリエンジン130を介して、カメラによって以前に取得された1つまたは複数の画像に関連したメタデータ(例えば、画像メタデータインデックス124に記憶される)が検索クエリに応答すると判定してよい。ブロック808で、システムは、ブロック806での判定に応じて、例えば、メタデータに基づいてタスク要求を実行してよい。例えば、一部の実装例では、自動アシスタント120は、タスク要求が受け取られた同じクライアント装置または異なるクライアント装置の1つまたは複数の出力装置を介して出力として、メタデータおよび1つまたは複数の画像を示す内容の一方または両方を提供してよい。
At
例に記載されるクライアント装置がスマートフォン、タブレット、スマートグラスおよびスタンドアロン対話型スピーカを含んだが、これは限定的であるとは意味されない。自動アシスタントまたは他の類似の機能性が他の種類のエレクトロニクスにインストールされてよい。例えば、一部の実装例では、本明細書に記載される技法はデジタルカメラに実装されてよい。例えば、デジタルカメラは、ローカルに記憶される写真を有しかつ/またはクラウドに記憶される写真へのネットワークアクセスを有してよく、そして様々なデジタルカメラ制御のユーザ選択に応じてユーザの要求により、ユーザ選択メタデータと関連付けて画像を記憶することおよび/またはユーザ選択メタデータを使用して画像を探すことが可能でよい。バックアップカメラ、ダッシュボードカメラ等など、前方、側方およびまたは後方にカメラを利用してよい、車両ナビゲーションシステムなどの他の装置に対しても同じことが言える。 The client devices described in the examples include smartphones, tablets, smart glasses and stand-alone interactive speakers, but this is not meant to be limited. Automatic assistants or other similar functionality may be installed in other types of electronics. For example, in some implementation examples, the techniques described herein may be implemented in a digital camera. For example, a digital camera may have locally stored photos and / or network access to photos stored in the cloud, and at the user's request according to various digital camera control user choices. It may be possible to store the image in association with the user-selected metadata and / or search for the image using the user-selected metadata. The same is true for other devices such as vehicle navigation systems that may utilize cameras forward, sideways and / or rearward, such as backup cameras, dashboard cameras and the like.
図9は、本明細書に記載される技法の1つまたは複数の態様を行うために任意選択で活用されてよいコンピューティング装置例910のブロック図である。一部の実装例では、クライアント装置、自動アシスタント120および/または他の部品の1つまたは複数がコンピューティング装置例910の1つまたは複数の部品から成ってよい。
FIG. 9 is a block diagram of an
コンピューティング装置910は、バスサブシステム912を介していくつかの周辺デバイスと通信する少なくとも1つのプロセッサ914を典型的に含む。これらの周辺デバイスとしては、例えば、メモリサブシステム925およびファイル記憶サブシステム926を含む、記憶サブシステム924、ユーザインターフェース出力デバイス920、ユーザインターフェース入力デバイス922ならびにネットワークインタフェースサブシステム916を含んでよい。入出力デバイスは、コンピューティング装置910とのユーザ対話を可能にする。ネットワークインタフェースサブシステム916は、外部ネットワークへのインタフェースを提供し、かつ他のコンピューティング装置における対応するインタフェースデバイスに結合される。
The
ユーザインターフェース入力デバイス922としては、キーボード、マウス、トラックボール、タッチパッドもしくはグラフィックタブレットなどのポインティングデバイス、スキャナ、ディスプレイへ組み込まれるタッチスクリーン、音声認識システムなどのオーディオ入力デバイス、マイクロホン、および/または他の種類の入力デバイスを含んでよい。一般に、用語「入力デバイス」の使用は、コンピューティング装置910へまたは通信ネットワーク上へ情報を入力する全ての可能な種類のデバイスおよび手段を含むと意図される。
User
ユーザインターフェース出力デバイス920としては、表示サブシステム、プリンタ、ファックス機器、またはオーディオ出力デバイスなどの非視覚表示を含んでよい。表示サブシステムとしては、陰極線管(CRT)、液晶ディスプレイ(LCD)などのフラットパネルデバイス、プロジェクションデバイス、または可視画像を作成するための何らかの他のメカニズム(例えば、「スマート」グラスと関連する拡張現実感ディスプレイ)を含んでよい。表示サブシステムは、オーディオ出力デバイスを介してなどの非視覚表示も提供してよい。一般に、用語「出力デバイス」の使用は、コンピューティング装置910からユーザにまたは別のマシンもしくはコンピューティング装置に情報を出力する全ての可能な種類のデバイスおよび手段を含むと意図される。
The user
記憶サブシステム924は、本明細書に記載されるモジュールの一部または全ての機能性を提供するプログラミングおよびデータ構造を記憶する。例えば、記憶サブシステム924は、図7および図8の方法の選択された態様を行う他に、図1に示される様々な部品を実装するロジックを含んでよい。
The
これらのソフトウェアモジュールは一般に、プロセッサ914によって単独でまたは他のプロセッサとの組合せで実行される。記憶サブシステム924に使用されるメモリ925としては、プログラム実行中の命令およびデータの記憶のためのメインランダムアクセスメモリ(RAM)930および固定命令が記憶されるリードオンリメモリ(ROM)932を含むいくつかのメモリを含むことができる。ファイル記憶サブシステム926は、プログラムおよびデータファイルのための永続記憶を提供でき、そしてハードディスクドライブ、フロッピーディスクドライブに加えて関連する取外し可能媒体、CD-ROMドライブ、光ドライブ、または取外し可能媒体カートリッジを含んでよい。或る実装例の機能性を実装するモジュールは、ファイル記憶サブシステム926によって記憶サブシステム924に、またはプロセッサ914によってアクセス可能な他のマシンに記憶されてよい。
These software modules are typically executed by
バスサブシステム912は、コンピューティング装置910の様々な部品およびサブシステムを意図されるように互いに通信させるためのメカニズムを提供する。バスサブシステム912が単一バスとして概略的に図示されるが、バスサブシステムの代替実装例が複数のバスを使用してよい。
The
コンピューティング装置910は、ワークステーション、サーバ、コンピューティングクラスタ、ブレードサーバ、サーバファーム、または任意の他のデータ処理システムもしくはコンピューティング装置を含む様々な種類であり得る。コンピュータおよびネットワークの絶えず変化する性質のため、図9に示されるコンピューティング装置910の説明は、単に一部の実装例を例示する目的の具体例として意図される。図9に示されるコンピューティング装置より多くのまたは少ない部品を有して、コンピューティング装置910の多くの他の構成が可能である。
The
本明細書に述べられる或る実装例がユーザについての個人情報(例えば、他の電子通信から抽出されるユーザデータ、ユーザのソーシャルネットワークについての情報、ユーザの場所、ユーザの時間、ユーザの生体情報、ならびにユーザの活動および人口統計情報、ユーザ間の関係等)を収集または使用してよい状況では、ユーザには、情報が収集されるかどうか、個人情報が記憶されるかどうか、個人情報が使用されるかどうか、ならびにユーザについて情報がどのように収集、記憶および使用されるかを制御する1つまたは複数の機会が設けられる。すなわち、本明細書に述べられるシステムおよび方法は、そうすることへの関連ユーザからの明示的な許可を受けた上でのみ、ユーザ個人情報を収集、記憶および/または使用する。 Some implementation examples described herein include personal information about a user (eg, user data extracted from other electronic communications, information about a user's social network, user location, user time, user biometric information). , And in situations where user activity and demographic information, relationships between users, etc.) may be collected or used, the user will be asked if information is collected, if personal information is stored, and if personal information is stored. There will be one or more opportunities to control whether it is used and how information about the user is collected, stored and used. That is, the systems and methods described herein collect, store and / or use user personal information only with the express permission of the user associated with doing so.
例えば、ユーザには、プログラムまたは特徴がプログラムまたは特徴に関連するその特定のユーザまたは他のユーザについてのユーザ情報を収集するかどうかの制御権が与えられる。個人情報が収集されることになる各ユーザには、そのユーザに関連する情報収集の制御を可能にして、情報が収集されるかどうかに関するおよび情報のどの部分が収集されることになるかに関する許可または認可を与える、1つまたは複数の選択肢が提示される。例えば、ユーザには、通信ネットワークを通じて1つまたは複数のそのような制御選択肢が与えられ得る。加えて、或るデータは、個人識別可能情報が削除されるように、それが記憶または使用される前に1つまたは複数の手段で処理されてよい。1つの例として、ユーザの識別情報は、個人識別可能情報が測定できないように処理されてよい。別の例として、ユーザの地理的場所は、ユーザの特定の場所が測定できないようにより大きい領域に一般化されてよい。 For example, a user is given control over whether a program or feature collects user information about that particular user or other user associated with the program or feature. For each user for whom personal information will be collected, it is possible to control the collection of information related to that user, regarding whether the information will be collected and what portion of the information will be collected. You will be presented with one or more options to grant permission or authorization. For example, a user may be given one or more such control options through a communication network. In addition, some data may be processed by one or more means before it is stored or used so that the personally identifiable information is deleted. As an example, the user's identifying information may be processed so that the personally identifiable information cannot be measured. As another example, the user's geographic location may be generalized to a larger area so that the user's specific location cannot be measured.
さらには、本開示に係る或る処理は、データおよび関連処理がネットワークまたは他の第三者装置またはサービスに共有されないように専らユーザの装置上で行われてよく、かつ追加のプライバシーおよびセキュリティのために暗号化および/またはパスワード保護されてよい。 Further, certain processing according to the present disclosure may be performed exclusively on the user's equipment so that the data and related processing are not shared with the network or other third party equipment or services, and of additional privacy and security. May be encrypted and / or password protected.
いくつかの実装例が本明細書に記載および例示されたが、本明細書に記載される機能を行ってかつ/または結果および/もしくは利点の1つもしくは複数を得るための各種の他の手段および/または構造が活用されてよく、そしてそのような変化および/または修正の各々は本明細書に記載される実装例の範囲内であると考えられる。より一般に、本明細書に記載される全てのパラメータ、寸法、材料および構成は例証的であると意味され、そして実際のパラメータ、寸法、材料および/または構成は、教示が使用される1つまたは複数の具体的な応用に応じるであろう。当業者は、本明細書に記載される具体的な実装例の多くの等価物を、ルーチン実験しか使用せずに認識し、または確認することが可能であろう。したがって、前述の実装例が単に例として提示されること、ならびに添付の請求項およびその等価物の範囲内で、実装例が具体的に記載および特許請求される以外に実施されてよいことが理解されるはずである。本開示の実装例は、本明細書に記載される各個々の特徴、システム、物品、材料、キットおよび/または方法を対象とする。加えて、2つ以上のそのような特徴、システム、物品、材料、キットおよび/または方法の任意の組合せは、そのような特徴、システム、物品、材料、キットおよび/または方法が相互に矛盾していなければ、本開示の範囲内に含まれる。 Although some implementation examples have been described and exemplified herein, various other means of performing the functions described herein and / or obtaining one or more of the results and / or benefits. And / or structures may be utilized, and each such change and / or modification is considered to be within the scope of the implementations described herein. More generally, all parameters, dimensions, materials and configurations described herein are meant to be exemplary, and the actual parameters, dimensions, materials and / or configurations are one or one in which the teaching is used. It will accommodate multiple specific applications. One of ordinary skill in the art will be able to recognize or confirm many equivalents of the specific implementations described herein using only routine experiments. Therefore, it is understood that the above-mentioned implementation examples are presented merely as examples, and that the implementation examples may be implemented in addition to the specific description and patent claims within the scope of the appended claims and their equivalents. Should be done. Implementation examples of the present disclosure cover each individual feature, system, article, material, kit and / or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits and / or methods conflict with each other such features, systems, articles, materials, kits and / or methods. If not, it is within the scope of this disclosure.
1061-N クライアントコンピューティング装置
1071-N メッセージ交換クライアント
1091-N カメラアプリケーション
1111-N カメラ
1141-N 音声取得/テキスト音声(TTS)/音声テキスト(STT)モジュール
116 TTSモジュール
118 STTモジュール
120 自動アシスタント
122 自然言語プロセッサ
124 画像メタデータインデックス
126 画像インデックス
130 画像メモリエンジン
132 画像処理エンジン
206 クライアント装置
211 カメラ
240 タッチスクリーン
244 ユーザ入力フィールド
2461-3 グラフィカル要素
248 ワイン
248' 描画
306 クライアント装置
311 カメラ
348 ワインボトル
350 透明レンズ
406 クライアント装置
411 カメラ
440 タッチスクリーン
444 ユーザ入力フィールド
4461-3 グラフィカル要素
452 特定の施設
452' 描画
454 看板
501 ユーザ
506 クライアント装置
511 カメラ
560 チケット
606 クライアント装置
611 カメラ
6461-3 グラフィカル要素
6621-3 選択可能要素
640 タッチスクリーン
660 車両
910 コンピューティング装置
912 バスサブシステム
914 プロセッサ
916 ネットワークインタフェースサブシステム
920 ユーザインターフェース出力デバイス
922 ユーザインターフェース入力デバイス
924 記憶サブシステム
925 メモリサブシステム
926 ファイル記憶サブシステム
930 RAM
932 ROM
106 1-N Client Computing Limited
107 1-N Message Exchange Client
109 1-N Camera Application
111 1-N camera
114 1-N Speech Acquisition / Text Speech (TTS) / Speech Text (STT) Module
116 TTS module
118 STT module
120 Automatic Assistant
122 natural language processor
124 Image Metadata Index
126 Image index
130 image memory engine
132 Image processing engine
206 Client device
211 camera
240 touch screen
244 User input field
246 1-3 Graphical elements
248 wine
248'drawing
306 Client device
311 camera
348 wine bottle
350 transparent lens
406 Client device
411 camera
440 touch screen
444 User input field
446 1-3 Graphical elements
452 Specific facility
452'Drawing
454 sign
501 users
506 Client device
511 camera
560 tickets
606 Client device
611 camera
646 1-3 Graphical elements
662 1-3 Selectable elements
640 touch screen
660 vehicle
910 computing equipment
912 Bus subsystem
914 processor
916 Network Interface Subsystem
920 user interface output device
922 User Interface Input Device
924 Storage subsystem
925 memory subsystem
926 File storage subsystem
930 RAM
932 ROM
Claims (19)
少なくとも部分的に前記ストリーミングに応じて自動アシスタントを呼び出すステップと、
前記カメラによって取得される前記データが前記電子ビューファインダにストリーミングされる間に前記第1のクライアント装置で、前記自動アシスタントに宛てられる前記ユーザからの自由形式入力を受け取るステップと、
前記自由形式入力からタスク要求を認識するステップと、
前記タスク要求が、前記自動アシスタントが前記カメラの1つまたは複数によって取得される1つまたは複数の画像に関連したメタデータを記憶せよとの要求を備えると判定するステップであって、前記メタデータが前記タスク要求の内容に基づいて選択される、ステップと、
前記自動アシスタントによって前記メタデータを検索することが可能であるように、1つまたは複数のコンピュータ可読記録媒体に前記メタデータを記憶するステップと
を含み、
前記1つまたは複数の画像に第1の画像処理を行うステップと、
前記第1の画像処理に基づいて、前記1つまたは複数の画像に示される物体を識別するステップと、
前記カメラの1つまたは複数によって現在取得されている1つまたは複数の画像に第2の画像処理を行うステップと、
前記第2の画像処理に基づいて、前記現在取得されている1つまたは複数の画像に示される物体を識別するステップと、
前記識別された前記1つまたは複数の画像に示される物体と、前記識別された前記現在取得されている1つまたは複数の画像に示される物体を照合するステップと、
前記識別された前記1つまたは複数の画像に示される物体が、前記識別された前記現在取得されている1つまたは複数の画像に示される物体に照合するとの判定に応じて、前記第1のクライアント装置の1つまたは複数の出力装置を介して出力として、前記メタデータを示す内容を提供するステップと
をさらに含む、方法。 A step of streaming data acquired by one or more cameras to the electronic viewfinder of the first client device of one or more client devices operated by the user.
At least in part, the step of calling an automatic assistant in response to the streaming,
A step of receiving free-form input from the user addressed to the automatic assistant at the first client device while the data acquired by the camera is streamed to the electronic viewfinder.
The step of recognizing the task request from the free-form input and
The task request is a step of determining that the automatic assistant comprises a request to store metadata associated with one or more images acquired by one or more of the cameras. Is selected based on the content of the task request,
Wherein as the automated assistant is possible to search the metadata, and a steps of storing the metadata in one or more computer-readable recording medium,
The step of performing the first image processing on the one or more images, and
A step of identifying an object shown in the one or more images based on the first image processing.
A step of performing a second image processing on one or more images currently acquired by one or more of the cameras.
A step of identifying an object shown in the currently acquired one or more images based on the second image processing.
A step of matching the identified object shown in the one or more images with the identified object shown in the currently acquired one or more images.
The first, depending on the determination that the identified object shown in the one or more images matches the identified object shown in the currently acquired one or more images. A method further comprising the step of providing content indicating the metadata as output through one or more output devices of the client device.
前記1つまたは複数のクライアント装置のうちの前記第1のクライアント装置または第2のクライアント装置で、第2の自由形式入力を受け取るステップと、
前記第2の自由形式入力から別のタスク要求を認識するステップと、
前記カメラによって取得される前記1つまたは複数の画像に関連した前記メタデータが前記別のタスク要求に応答すると判定するステップと、
前記メタデータが前記別のタスク要求に応答するとの判定に応じて、前記第1または第2のクライアント装置の1つまたは複数の出力装置を介して出力として、前記メタデータを示す内容を提供するステップと
をさらに含む、請求項1に記載の方法。 The free-form input is the first input, and the method
A step of receiving a second free-form input on the first or second client device of the one or more client devices.
The step of recognizing another task request from the second free-form input,
A step of determining that the metadata associated with the one or more images acquired by the camera responds to the other task request.
Depending on the determination that the metadata responds to the other task request, the content indicating the metadata is provided as output via one or more output devices of the first or second client device. The method of claim 1, further comprising steps and.
をさらに含む、請求項1に記載の方法。 The method of claim 1, further comprising storing the metadata in association with another storage image showing the same object as the object or another object sharing one or more attributes with the object.
ユーザによって操作される1つまたは複数のクライアント装置のうちの第1のクライアント装置の電子ビューファインダに1つまたは複数のカメラによって取得されるデータをストリーミングすることと、
少なくとも部分的に前記ストリーミングに応じて自動アシスタントを呼び出すことと、
前記カメラによって取得される前記データが前記電子ビューファインダにストリーミングされる間に前記第1のクライアント装置で、前記自動アシスタントに宛てられる前記ユーザからの自由形式入力を受け取ることと、
前記自由形式入力からタスク要求を認識することと、
前記タスク要求が、前記自動アシスタントが前記カメラの1つまたは複数によって取得される1つまたは複数の画像に関連したメタデータを記憶せよとの要求を備えると判定することであって、前記メタデータが前記タスク要求の内容に基づいて選択される、判定することと、
前記自動アシスタントによって前記メタデータを検索することが可能であるように、1つまたは複数のコンピュータ可読記録媒体に前記メタデータを記憶することと
を行わせ、
前記1つまたは複数の画像に第1の画像処理を行うことと、
前記第1の画像処理に基づいて、前記1つまたは複数の画像に示される物体を識別することと、
前記カメラの1つまたは複数によって現在取得されている1つまたは複数の画像に第2の画像処理を行うことと、
前記第2の画像処理に基づいて、前記現在取得されている1つまたは複数の画像に示される物体を識別することと、
前記識別された前記1つまたは複数の画像に示される物体と、前記識別された前記現在取得されている1つまたは複数の画像に示される物体を照合することと、
前記識別された前記1つまたは複数の画像に示される物体が、前記識別された前記現在取得されている1つまたは複数の画像に示される物体に照合するとの判定に応じて、前記第1のクライアント装置の1つまたは複数の出力装置を介して出力として、前記メタデータを示す内容を提供することと
を行う命令をさらに備える、非一時的コンピュータ可読記録媒体。 At least one non-temporary computer-readable recording medium, comprising instructions, the instructions being delivered to the one or more processors in response to execution of the instructions by one or more processors.
Streaming data captured by one or more cameras to the electronic viewfinder of the first client device of one or more client devices operated by the user.
Calling an automatic assistant in response to the streaming, at least in part,
The first client device receives free-form input from the user addressed to the automatic assistant while the data acquired by the camera is streamed to the electronic viewfinder.
Recognizing the task request from the free-form input and
The task request is to determine that the automatic assistant comprises a request to store metadata associated with one or more images acquired by one or more of the cameras, said metadata. Is selected based on the content of the task request.
Wherein so as to be able to search for the metadata using the automated assistant, to perform the <br/> and one or more computer-readable recording medium storing child the metadata,
Performing the first image processing on the one or more images,
Identifying the objects shown in the one or more images based on the first image processing.
Performing a second image processing on one or more images currently acquired by one or more of the cameras.
Identifying the object shown in the currently acquired image, based on the second image processing,
Matching the identified object shown in the one or more images with the identified object shown in the currently acquired one or more images.
The first, depending on the determination that the identified object shown in the one or more images collates with the identified object shown in the currently acquired one or more images. A non-temporary computer-readable recording medium further comprising an instruction to provide content indicating the metadata as output via one or more output devices of the client device.
ユーザによって操作される1つまたは複数のクライアント装置のうちの第1のクライアント装置の電子ビューファインダに1つまたは複数のカメラによって取得されるデータをストリーミングすることと、
少なくとも部分的に前記ストリーミングに応じて自動アシスタントを呼び出すことと、
前記カメラによって取得される前記データが前記電子ビューファインダにストリーミングされる間に前記第1のクライアント装置で、前記自動アシスタントに宛てられる前記ユーザからの自由形式入力を受信することと、
前記自由形式入力からタスク要求を認識することと、
前記タスク要求が、前記自動アシスタントが前記カメラの1つまたは複数によって取得される1つまたは複数の画像に関連したメタデータを記憶せよとの要求を備えると判定することであって、前記メタデータが前記タスク要求の内容に基づいて選択される、判定することと、
前記自動アシスタントによって前記メタデータを検索することが可能であるように、1つまたは複数のコンピュータ可読記録媒体に前記メタデータを記憶することと
を行わせ、
前記1つまたは複数の画像に第1の画像処理を行うことと、
前記第1の画像処理に基づいて、前記1つまたは複数の画像に示される物体を識別することと、
前記カメラの1つまたは複数によって現在取得されている1つまたは複数の画像に第2の画像処理を行うことと、
前記第2の画像処理に基づいて、前記現在取得されている1つまたは複数の画像に示される物体を識別することと、
前記識別された前記1つまたは複数の画像に示される物体と、前記識別された前記現在取得されている1つまたは複数の画像に示される物体を照合することと、
前記識別された前記1つまたは複数の画像に示される物体が、前記識別された前記現在取得されている1つまたは複数の画像に示される物体に照合するとの判定に応じて、前記第1のクライアント装置の1つまたは複数の出力装置を介して出力として、前記メタデータを示す内容を提供することと
を行う命令をさらに備える、システム。 A system comprising one or more processors and memory in which the memory stores an instruction and, depending on the execution of the instruction by the one or more processors, the instruction may be the one or more processors. NS,
Streaming data captured by one or more cameras to the electronic viewfinder of the first client device of one or more client devices operated by the user.
Calling an automatic assistant in response to the streaming, at least in part,
The first client device receives free-form input from the user addressed to the automatic assistant while the data acquired by the camera is streamed to the electronic viewfinder.
Recognizing the task request from the free-form input and
The task request is to determine that the automatic assistant comprises a request to store metadata associated with one or more images acquired by one or more of the cameras, said metadata. Is selected based on the content of the task request.
Wherein so as to be able to search for the metadata using the automated assistant, to perform the <br/> and one or more computer-readable recording medium storing child the metadata,
Performing the first image processing on the one or more images,
Identifying the objects shown in the one or more images based on the first image processing.
Performing a second image processing on one or more images currently acquired by one or more of the cameras.
Identifying the object shown in the currently acquired image, based on the second image processing,
Matching the identified object shown in the one or more images with the identified object shown in the currently acquired one or more images.
The first, depending on the determination that the identified object shown in the one or more images collates with the identified object shown in the currently acquired one or more images. A system further comprising an instruction to provide content indicating the metadata as output via one or more output devices of the client device.
前記1つまたは複数のクライアント装置のうちの前記第1のクライアント装置または第2のクライアント装置で、第2の自由形式入力を受信することと、
前記第2の自由形式入力から別のタスク要求を認識することと、
前記カメラによって取得される前記1つまたは複数の画像に関連した前記メタデータが前記別のタスク要求に応答すると判定することと、
前記メタデータが前記別のタスク要求に応答するとの判定に応じて、前記第1または第2のクライアント装置の1つまたは複数の出力装置を介して出力として、前記メタデータを示す内容を提供することと
を行う命令をさらに備える、請求項13に記載のシステム。 The free-form input is the first input, and the system
Receiving a second free-form input on the first or second client device of the one or more client devices.
Recognizing another task request from the second free-form input,
Determining that the metadata associated with the one or more images acquired by the camera responds to the other task request.
Depending on the determination that the metadata responds to the other task request, the content indicating the metadata is provided as output via one or more output devices of the first or second client device. 13. The system of claim 13, further comprising an instruction to do so.
を行う命令をさらに備える、請求項13に記載のシステム。 13. Further comprising an instruction to store the metadata in association with another storage image showing the same object as the object or another object sharing one or more attributes with the object. System.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2021112035A JP2021166083A (en) | 2017-05-16 | 2021-07-06 | Storage of metadata related to acquired image |
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762507108P | 2017-05-16 | 2017-05-16 | |
US62/507,108 | 2017-05-16 | ||
US15/602,661 | 2017-05-23 | ||
US15/602,661 US10469755B2 (en) | 2017-05-16 | 2017-05-23 | Storing metadata related to captured images |
PCT/US2018/032787 WO2018213322A1 (en) | 2017-05-16 | 2018-05-15 | Storing metadata related to captured images |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021112035A Division JP2021166083A (en) | 2017-05-16 | 2021-07-06 | Storage of metadata related to acquired image |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2020521226A JP2020521226A (en) | 2020-07-16 |
JP6911155B2 true JP6911155B2 (en) | 2021-07-28 |
Family
ID=64269655
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019563573A Active JP6911155B2 (en) | 2017-05-16 | 2018-05-15 | Memory of metadata associated with acquired images |
JP2021112035A Pending JP2021166083A (en) | 2017-05-16 | 2021-07-06 | Storage of metadata related to acquired image |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021112035A Pending JP2021166083A (en) | 2017-05-16 | 2021-07-06 | Storage of metadata related to acquired image |
Country Status (6)
Country | Link |
---|---|
US (2) | US10469755B2 (en) |
EP (1) | EP3513334A1 (en) |
JP (2) | JP6911155B2 (en) |
KR (3) | KR102366754B1 (en) |
CN (1) | CN110637295A (en) |
WO (1) | WO2018213322A1 (en) |
Families Citing this family (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11785161B1 (en) | 2016-06-20 | 2023-10-10 | Pipbin, Inc. | System for user accessibility of tagged curated augmented reality content |
US11876941B1 (en) | 2016-06-20 | 2024-01-16 | Pipbin, Inc. | Clickable augmented reality content manager, system, and network |
US10469755B2 (en) * | 2017-05-16 | 2019-11-05 | Google Llc | Storing metadata related to captured images |
JP6968897B2 (en) * | 2017-06-13 | 2021-11-17 | グーグル エルエルシーGoogle LLC | Establishing an audio-based network session with unregistered resources |
US20200244896A1 (en) * | 2018-08-17 | 2020-07-30 | Gregory Walker Johnson | Tablet with camera's |
KR102523982B1 (en) | 2018-08-21 | 2023-04-20 | 구글 엘엘씨 | Dynamic and/or context-specific hot words to invoke automated assistants |
KR102498811B1 (en) | 2018-08-21 | 2023-02-10 | 구글 엘엘씨 | Dynamic and/or context specific hotwords to invoke automated assistants |
CN113168526A (en) | 2018-10-09 | 2021-07-23 | 奇跃公司 | System and method for virtual and augmented reality |
US11240560B2 (en) * | 2019-05-06 | 2022-02-01 | Google Llc | Assigning priority for an automated assistant according to a dynamic user queue and/or multi-modality presence detection |
US11227007B2 (en) * | 2019-07-23 | 2022-01-18 | Obayashi Corporation | System, method, and computer-readable medium for managing image |
US10916241B1 (en) * | 2019-12-30 | 2021-02-09 | Capital One Services, Llc | Theme detection for object-recognition-based notifications |
KR102364881B1 (en) * | 2020-03-24 | 2022-02-18 | 주식회사 에스아이에이 | Training method for model that imitates expert and apparatus thereof |
JPWO2021250871A1 (en) * | 2020-06-11 | 2021-12-16 | ||
CN113793529A (en) * | 2021-08-25 | 2021-12-14 | 阿波罗智能技术(北京)有限公司 | Method, device, apparatus, storage medium and program product for assisting parking |
CN113986530A (en) * | 2021-09-30 | 2022-01-28 | 青岛歌尔声学科技有限公司 | Image processing method and device, storage medium and terminal |
US11755859B2 (en) | 2021-12-22 | 2023-09-12 | Datalogic Ip Tech S.R.L. | Apparatus and method for enabling decoding of remotely sourced and visually presented encoded data markers |
Family Cites Families (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR100754656B1 (en) * | 2005-06-20 | 2007-09-03 | 삼성전자주식회사 | Method and system for providing user with image related information and mobile communication system |
JP4175390B2 (en) * | 2006-06-09 | 2008-11-05 | ソニー株式会社 | Information processing apparatus, information processing method, and computer program |
US9318108B2 (en) * | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
JP5320913B2 (en) * | 2008-09-04 | 2013-10-23 | 株式会社ニコン | Imaging apparatus and keyword creation program |
JP5279466B2 (en) | 2008-11-28 | 2013-09-04 | キヤノン株式会社 | Information processing apparatus, control method therefor, program, and storage medium |
JP2010245607A (en) * | 2009-04-01 | 2010-10-28 | Nikon Corp | Image recording device and electronic camera |
DE102009054000A1 (en) * | 2009-11-19 | 2011-05-26 | Schoeller Holding Gmbh | Apparatus for image recording and display of objects, in particular digital binoculars, digital cameras or digital video cameras |
US9305024B2 (en) | 2011-05-31 | 2016-04-05 | Facebook, Inc. | Computer-vision-assisted location accuracy augmentation |
DE112011106063B4 (en) * | 2011-12-30 | 2021-06-24 | Intel Corporation | User interfaces for electronic devices |
US20130346068A1 (en) * | 2012-06-25 | 2013-12-26 | Apple Inc. | Voice-Based Image Tagging and Searching |
JP5869989B2 (en) * | 2012-08-24 | 2016-02-24 | 富士フイルム株式会社 | Article collation apparatus and method, and program |
US9547647B2 (en) * | 2012-09-19 | 2017-01-17 | Apple Inc. | Voice-based media searching |
KR101973649B1 (en) * | 2012-09-28 | 2019-04-29 | 엘지전자 주식회사 | Mobile Terminal and Operating Method for the Same |
KR20140075997A (en) * | 2012-12-12 | 2014-06-20 | 엘지전자 주식회사 | Mobile terminal and method for controlling of the same |
US9696874B2 (en) | 2013-05-14 | 2017-07-04 | Google Inc. | Providing media to a user based on a triggering event |
JP6273755B2 (en) * | 2013-10-11 | 2018-02-07 | 株式会社デンソー | Vehicle position management system, in-vehicle device, and program |
WO2015120019A1 (en) * | 2014-02-10 | 2015-08-13 | Google Inc. | Smart camera user interface |
US9747648B2 (en) * | 2015-01-20 | 2017-08-29 | Kuo-Chun Fang | Systems and methods for publishing data on social media websites |
KR20170034729A (en) * | 2015-09-21 | 2017-03-29 | 주식회사 웨이짱 | Portable apparatus of recognition car parking position on real time and operating method thereof |
RU2622843C2 (en) * | 2015-09-24 | 2017-06-20 | Виталий Витальевич Аверьянов | Management method of image processing device |
KR101761631B1 (en) * | 2015-12-29 | 2017-07-26 | 엘지전자 주식회사 | Mobile terminal and method for controlling the same |
US10469755B2 (en) * | 2017-05-16 | 2019-11-05 | Google Llc | Storing metadata related to captured images |
-
2017
- 2017-05-23 US US15/602,661 patent/US10469755B2/en active Active
-
2018
- 2018-05-15 KR KR1020217005424A patent/KR102366754B1/en active IP Right Grant
- 2018-05-15 WO PCT/US2018/032787 patent/WO2018213322A1/en unknown
- 2018-05-15 KR KR1020197037169A patent/KR102222421B1/en active IP Right Grant
- 2018-05-15 EP EP18733010.5A patent/EP3513334A1/en not_active Withdrawn
- 2018-05-15 JP JP2019563573A patent/JP6911155B2/en active Active
- 2018-05-15 KR KR1020227005433A patent/KR102419513B1/en active IP Right Grant
- 2018-05-15 CN CN201880032769.8A patent/CN110637295A/en active Pending
-
2019
- 2019-09-27 US US16/585,085 patent/US10893202B2/en active Active
-
2021
- 2021-07-06 JP JP2021112035A patent/JP2021166083A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR102222421B1 (en) | 2021-03-03 |
US10893202B2 (en) | 2021-01-12 |
JP2021166083A (en) | 2021-10-14 |
US10469755B2 (en) | 2019-11-05 |
CN110637295A (en) | 2019-12-31 |
EP3513334A1 (en) | 2019-07-24 |
KR20200007946A (en) | 2020-01-22 |
KR102366754B1 (en) | 2022-02-24 |
JP2020521226A (en) | 2020-07-16 |
KR20210024222A (en) | 2021-03-04 |
KR20220028147A (en) | 2022-03-08 |
US20180338109A1 (en) | 2018-11-22 |
US20200021740A1 (en) | 2020-01-16 |
KR102419513B1 (en) | 2022-07-12 |
WO2018213322A1 (en) | 2018-11-22 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6911155B2 (en) | Memory of metadata associated with acquired images | |
US10685187B2 (en) | Providing access to user-controlled resources by automated assistants | |
JP6891337B2 (en) | Resolving automated assistant requests based on images and / or other sensor data | |
JP2022079458A (en) | Automated assistant having conferencing ability | |
JP6942821B2 (en) | Obtaining response information from multiple corpora | |
CN110603545B (en) | Method, system and non-transitory computer readable medium for organizing messages | |
JP7268093B2 (en) | Selective Detection of Visual Cues for Automated Assistants | |
JP7471371B2 (en) | Selecting content to render on the assistant device's display | |
US20230244712A1 (en) | Type ahead search amelioration based on image processing | |
CN110688011A (en) | Dynamic list composition based on modalities of multimodal client devices |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20200109 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20200109 |
|
A871 | Explanation of circumstances concerning accelerated examination |
Free format text: JAPANESE INTERMEDIATE CODE: A871Effective date: 20200109 |
|
A975 | Report on accelerated examination |
Free format text: JAPANESE INTERMEDIATE CODE: A971005Effective date: 20200424 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20200608 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20200907 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20201207 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20210304 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20210607 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20210707 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 6911155Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |