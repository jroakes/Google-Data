US9984683B2 - Automatic speech recognition using multi-dimensional models - Google Patents
Automatic speech recognition using multi-dimensional models Download PDFInfo
- Publication number
- US9984683B2 US9984683B2 US15/217,457 US201615217457A US9984683B2 US 9984683 B2 US9984683 B2 US 9984683B2 US 201615217457 A US201615217457 A US 201615217457A US 9984683 B2 US9984683 B2 US 9984683B2
- Authority
- US
- United States
- Prior art keywords
- lstm
- frequency
- time
- blocks
- neural network
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
- G10L2015/025—Phonemes, fenemes or fenones being the recognition units
Definitions
- ASR Automated speech recognition systems attempt to determine the content of speech. Many ASR systems use an acoustic model to predict which sounds are present in audio data, and use a language model to predict which words were spoken.
- an ASR system uses an acoustic model that includes a recurrent neural network having a multi-dimensional memory architecture.
- an acoustic model may include a grid or other multi-dimensional arrangement that includes separate memory cells for time-domain information and frequency-domain information.
- the memory can be implemented as long short-term memory (LSTM) layers.
- LSTM long short-term memory
- certain memory cells from one dimension may share information with one or more memory cells from another dimension.
- a time-LSTM cell may share information with a frequency-LSTM cell
- a frequency-LSTM cell may share information with a time-LSTM cell.
- the memory cells in a two-dimensional LSTM architecture can be arranged with a grid-like relationships. There can be a number of frequency steps or bins, and also a number of time steps. Two LSTMs are used to model each combination of time and frequency, referred to as a “time-frequency bin.”
- the two LSTMs for the time-frequency bin can include a grid frequency LSTM (gF-LSTM) and a grid time LSTM (gT-LSTM).
- gF-LSTM grid frequency LSTM
- gT-LSTM grid time LSTM
- the frequency-focused gF-LSTM uses the state of the time-focused gT-LSTM from the previous timestep.
- the gT-LSTM uses the state of the gF-LSTM from the previous frequency step. This arrangement allows separate LSTMs to model the correlations in time and frequency, while also allowing information to be shared between the memory cells for the different dimensions.
- acoustic model with a grid-LSTM architecture can provide accuracy equal to or better than that of an acoustic model having a convolutional layer for clean audio, e.g., low or mild noise levels.
- the grid-LSTM architecture provides better accuracy than a convolutional neural network architecture for audio with moderate or high levels of noise.
- the multi-dimensional LSTM architectures discussed in this document also provide increased flexibility in modelling speech based on raw audio waveform data, including for multi-channel audio. While some implementations use predetermined audio features, such as log-mel features, other implementations use a filterbank having parameters learned jointly with the neural network parameters during training of the model. These architectures can be used in a multichannel model. For example, the input can represent the audio from multiple microphones that detect the same utterance, but from a different spatial position with respect to the speaker. Modeling based on raw audio waveforms can provide additional information that would otherwise be omitted using pre-determined features. However, when the learned filterbank is not constant-Q, a convolutional layer may degrade performance.
- a model using a grid-LSTM architecture has improved accuracy compared to standard LSTM architectures and those with convolutional layers.
- an acoustic model with a grid-LSTM architecture can be used effectively to model time-frequency patterns when the filterbank is learned, as in the case of ASR with raw audio waveform inputs or multi-channel audio processing.
- One innovative aspect of the subject matter described in this specification is embodied in methods that include the actions of: receiving, by one or more computers, audio data that describes an utterance; determining, by the one or more computers, a transcription for the utterance using an acoustic model comprising a neural network having first memory blocks for time information and second memory blocks for frequency information, the first memory blocks being different from the second memory blocks; and providing, by the one or more computers, the transcription for the utterance as output of an automated speech recognizer.
- inventions of these aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
- a system of one or more computers can be so configured by virtue of software, firmware, hardware, or a combination of them installed on the system that in operation cause the system to perform the actions.
- One or more computer programs can be so configured by virtue having instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- receiving the audio data includes receiving, over a network, audio data generated by a client device; and providing the transcription includes providing, over the network, the transcription to the client device.
- the first memory blocks and the second memory blocks are long short-term memory (LSTM) blocks.
- LSTM long short-term memory
- the neural network includes a grid-LSTM module, a linear projection layer, one or more LSTM layers, and a deep neural network (DNN).
- the grid-LSTM module includes the first memory blocks and the second memory blocks, and the grid-LSTM module provides output to the linear projection layer.
- the linear projection layer provides output to the one or more LSTM layers.
- the one or more LSTM layers provide output to the DNN.
- the DNN can provide output of the acoustic model, or output used to determine the output of the acoustic model.
- the neural network is configured to share information from each of the first memory blocks with a respective proper subset of the second memory blocks, and the neural network is configured to share information from each of the second memory blocks with a respective proper subset of the first memory blocks.
- the first memory blocks are time-LSTM blocks that each have a state
- the second memory blocks are frequency-LSTM blocks that each have a state.
- the frequency-LSTM blocks and the time-LSTM blocks can each have a corresponding frequency step in a sequence of multiple frequency steps.
- the states can be determined for each of a sequence of multiple time steps.
- the frequency-LSTM block determines its state using the state of the time-LSTM block corresponding to the same frequency step at the previous time step.
- the time-LSTM block determines its state using the state of the frequency-LSTM block corresponding to the same time step and the previous frequency step.
- the time-LSTM block determines its state based on (i) input received for a current time step, (ii) the state of the time-LSTM block at a previous time step, and (iii) a state of exactly one of the frequency-LSTM blocks.
- the frequency-LSTM block determines its state based on (i) input received for a current time step, (ii) the state of the frequency-LSTM block for a previous frequency step at the current time step, and (iii) a state of exactly one of the time-LSTM blocks.
- each of the time-LSTM blocks and the frequency-LSTM blocks has one or more weights, and the weights for the time-LSTM blocks are independent of the weights for the frequency-LSTM blocks.
- each of the time-LSTM blocks and the frequency-LSTM blocks has one or more weights, and at least some of the weights are shared between the time-LSTM blocks and the frequency-LSTM blocks.
- the neural network is configured so that the first memory blocks do not share information with the second memory blocks.
- determining, by the one or more computers, a transcription for the utterance using the acoustic model comprising the neural network includes: providing, as input to the neural network, input vectors having values describing the utterance; and receiving, as output of the neural network, one or more outputs that each indicate a likelihood that a respective phonetic unit represents a portion of the utterance.
- receiving the one or more outputs includes receiving, as output of the neural network, multiple outputs corresponding to different context-dependent states of phones, and each of the multiple outputs indicates a likelihood of occurrence for the corresponding context-dependent state.
- providing the input vectors includes providing input vectors comprising values for log-mel features.
- providing the input vectors includes providing values representing audio waveform features.
- the acoustic model includes a filterbank having parameters trained jointly with weights of the neural network.
- providing the input vectors includes providing values representing characteristics of multiple channels of audio describing the utterance.
- the speed and accuracy of speech recognition may be improved, for example, through the use of multi-dimensional architectures that include separate memory blocks for different dimensions, e.g., time and frequency. These architectures can further improve accuracy in recognizing speech in the presence of noise, through improved modeling of correlations in time and frequency. Accuracy for noisy data may also be improved by modeling localized relationships in input data without using a convolutional layer, which can decrease accuracy when noise is present.
- FIG. 1 is a diagram that illustrates an example of a system for performing speech recognition.
- FIG. 2 is a diagram that illustrates an example of a neural network acoustic model.
- FIG. 3 is a diagram that illustrates an example of a grid LSTM architecture.
- FIG. 4 is a flow diagram that illustrate an example of a process for performing speech recognition.
- FIG. 1 is a diagram that illustrates an example of a system 100 for performing speech recognition.
- the system 100 includes a client device 110 , a computing system 120 , and a network 130 .
- the computing system 120 provides information about an utterance to a neural network 150 that has been trained as an acoustic model.
- the computing system 120 uses output from the neural network 150 to determine a transcription for the utterance.
- a neural network architecture can model two-dimensional correlations in an input signal using a two-dimensional architecture such as time-frequency LSTMs, grid LSTMs, and ReNet LSTMs.
- a two-dimensional architecture such as time-frequency LSTMs, grid LSTMs, and ReNet LSTMs.
- an LSTM architecture can be used to model time-frequency patterns as the first layer in an LDNN architecture.
- grid-LDNNs offer improved accuracy over prior LDNN and CLDNN architecture on large vocabulary Voice Search tasks. Accuracy can also be improved in noisy conditions or when the learned filterbank is not constant-Q.
- the neural network 150 includes a frequency processing module 152 that can be used to process input to the neural network 150 before passing information to other layers of the neural network 150 .
- the frequency processing module 152 can be a multi-dimensional layer that includes memory blocks for multiple dimensions.
- the frequency processing module 152 can include a grid-LSTM architecture that with separate LSTM memories for frequency and time correlations. Certain memories in the grid-LSTM architecture can share information. Thus, while relationships in time and frequency can be modeled by separate blocks, patterns among time and frequency can also be more accurately modeled.
- the grid-LSTM architecture can provide improved accuracy compared to other modeling techniques, especially when significant noise is present in the audio data.
- Various different types of architectures that can be used for the frequency processing module 152 are discussed below.
- the client device 110 can be, for example, a desktop computer, laptop computer, a tablet computer, a wearable computer, a cellular phone, a smart phone, a music player, an e-book reader, a navigation system, or any other appropriate computing device.
- the functions performed by the computing system 120 can be performed by individual computer systems or can be distributed across multiple computer systems.
- the network 130 can be wired or wireless or a combination of both and can include the Internet.
- a user 102 of the client device 110 speaks, and the client device 110 records audio that includes the utterance.
- the client device 110 sends the recorded audio data 112 , e.g., audio waveform data, over the network 130 to the computing system 120 , which provides an automated speech recognition service.
- the client device 110 may record and send audio data 112 for a single channel, e.g., from a single microphone, or for multiple channels, e.g., recorded from multiple microphones.
- the computing system 120 receives the audio data 112 and begins processing audio waveform samples 122 from the audio data 112 .
- the computing system 120 may divide the audio waveform samples 122 into sets corresponding to different time periods or windows of the audio data 112 . For example, windows of 25 or 35 milliseconds of audio may be used, and in some implementations the windows may partially overlap.
- the raw audio waveform samples in the windows can be used as input to the neural network 150 .
- acoustic features 124 are extracted from the audio samples.
- a set log-mel features are extracted from the audio samples in each time window.
- the computing system 120 then provides the acoustic features 124 to the neural network 150 .
- the acoustic features 124 are provided in a series of input vectors input to the neural network 150 one at a time.
- Each input vector represents a different time step in a sequence, for example, a different time window in the series of time windows of the utterance.
- Each includes the log-mel features for a different time window of the utterance, or may include log-mel features for multiple time windows.
- the neural network 150 has been previously trained to act as an acoustic model.
- the neural network 150 indicates likelihoods that different speech units correspond to portions of the utterance based on the input vectors including the acoustic features 124 .
- the neural network 150 can be a recurrent neural network that can thus use information from previous time steps to determine a prediction for a current time step.
- the neural network 150 includes a frequency processing module 152 that is used to model patterns in the acoustic features 124 in time and frequency.
- this frequency processing module 152 can include a memory architecture that uses LSTMs to model these patterns.
- the correlations in the acoustic features in frequency and time may be modeled separately using different LSTM blocks.
- the LSTM blocks for the time and frequency may be connected so that they share information between certain blocks also.
- the output of the frequency processing module 152 is provided to a linear projection layer 154 .
- some or all of the LSTMs in the frequency processing module 150 can provide their outputs to the linear projection layer 154 for each time step.
- the linear projection layer 154 can then reduce the dimensionality of these outputs. For example, several hundred or a thousand or more outputs of the frequency processing module 152 can be projecting to a lower-dimension of 256 values.
- the output of the linear projection layer 154 is provided to one or more memory layers 156 .
- three memory layers 156 may be used, arranged in sequence so that the first memory layer passes its output to the second memory layer, which passes its output to the third memory layer.
- the one or more memory layers 156 may include LSTMs configured to model correlations in time.
- the output of the one or more memory layers 156 is provided to a deep neural network (DNN) 158 .
- the DNN 158 includes at least one hidden layer and at least one output layer.
- the neural network 150 produces neural network outputs 160 , which the computing system 120 uses to identify a transcription 114 for the audio data 112 .
- the neural network outputs 160 indicates likelihoods that the speech in a particular window, for example, in response to the input feature vector for the particular time window, represents specific phonetic units.
- the phonetic units used are phones or states of phones.
- the potential phones are referred to as s0 . . . sm.
- the phones may be any of the various phones in speech, such as an “ah” phone, an “ae” phone, a “zh” phone, and so on.
- the phones may include all of the possible phones that may occur in the audio waveform samples 122 , or fewer than all of the phones that may occur.
- Each phone can be divided into acoustic states.
- these phonetic units are context-dependent or context-independent hidden markov model (HMM) states of phones.
- HMM hidden markov model
- the neural network outputs 160 provide predictions or probabilities of acoustic states given the data included in the acoustic features 124 .
- the neural network outputs 160 provide a value, for each state of each phone, which indicates the probability that an acoustic feature vector represents the particular state of the particular phone.
- the output layer may provide 13,522 output values each corresponding to different context-dependent states.
- the computing system 120 provides different sets of acoustic feature vectors to the neural network 150 to receive predictions or probabilities of the acoustic states in different windows.
- the computing system 120 may apply a sliding window to the acoustic feature vectors to select different sets. In this manner, the computing system 120 may obtain outputs corresponding to each position of the sliding window across the acoustic feature vectors.
- the computing system 120 may provide the neural network outputs 160 to, for example, weighted finite state transducers that approximate a hidden Markov model (HMM), which may include information about a lexicon indicating the phonetic units of words, a grammar, and a language model that indicates likely sequences of words.
- HMM hidden Markov model
- the output of the HMM can be a word lattice from which the transcription 114 is derived.
- the computing system 120 then provides the transcription 114 to the client device 110 over the network 130 .
- CNNs Convolutional Neural Networks
- LSTMs Long-Short Term Memory Recurrent Neural Networks
- LVCSR large vocabulary continuous speech recognition
- an architecture having one or more CNN layers, one or more LSTM layers, and a DNN can combine the modeling benefits of all three architectures in a unified framework.
- Convolutional layers in the CLDNN model achieve local translation invariance through local filters and pooling. This requires tuning both the filter size and pooling size, which is typically done experimentally. Since pooling is done on linear shifts of the convolutional filter, it is most effective when the filterbank is constant-Q (such as log-mel) and therefore a linear shift across filters corresponds to a linear shift in pitch. Pooling is less effective when the filterbank deviates from being constant-Q, for example with raw-waveform modeling where the filterbank is learned in the network. In addition, convolution layers tend to hurt performance in very noisy conditions.
- Frequency LSTMs F-LSTM
- TF-LSTMs Time-Frequency LSTMs
- F-LSTMs and TF-LSTMs capture translation invariance through local filters and recurrent connections. Since they do not require any pooling operations, they can be more adaptable to different types of input features.
- F-LDNNs For mild speech with low noise levels, networks with an F-LSTM layer and a DNN (F-LDNNs) and CLDNNs often provide similar accuracy, which indicates that an F-LSTM layer has similar modeling capabilities to a convolutional layer. Furthermore, using an architecture with a TF-LSTM and DNN (TF-LDNNs), which give stronger modeling capabilities, a relative improvement in word error rate (WER) can be obtained over a strong CLDNN model. For moderate and high noise levels, convolution generally hurts accuracy. TF-LDNNs generally provide a relative improvement in accuracy over LDNNs also.
- TF-LDNNs For moderate and high noise levels, convolution generally hurts accuracy.
- the frequency processing module 152 includes LSTMs corresponding to multiple different dimensions. These multi-dimensional LSTM architectures can improve accuracy further by better modeling relationships between multiple dimensions.
- a grid LSTM can include LSTMs for both time and frequency dimensions. There are separate LSTMs for the different dimensions, but the architecture includes links or connections that can allow information to be shared between the memory cells for the different dimensions.
- the gate functions and output activations are computed using both time LSTM cell states and frequency LSTM cell states.
- Grid LSTMs offer the benefit of independently modeling time and frequency correlations. For recognizing speech from noisy audio data, a grid-LDNN can provide an improvement in accuracy over a TF-LDNN, for audio with low and high levels of noise.
- ReNets are another type of multi-dimensional LSTM. ReNets have completely separate time and frequency LSTMs which do not include states of both during overlap. The potential benefit of ReNets is that they are easier to parallelize since the time and frequency LSTMs move independently. However, architectures using ReNets to not always improve modeling performance, and may provide slightly lower performance than TF-LDNNs and grid-LDNNs. Nevertheless, due to the benefits of parallelization, ReNets may be appropriate for various implementations.
- Multi-dimensional LSTM architectures can also be used in implementations where the parameters of the filterbank are learned, for example, when raw audio waveform data is provided rather than values for pre-defined features (e.g., log-mel features). These architectures can be used in a multichannel model.
- the input can represents the audio from multiple microphones that simultaneously detect the same utterance, but from a different spatial position with respect to the speaker.
- a convolutional layer may degrade performance with respect to an LDNN.
- the grid-LDNN provides a relative improvement over the LDNN architecture, which demonstrates the benefit of modeling time-frequency patterns with an LSTM rather than a convolutional layer when the filterbank is learned.
- FIG. 2 is a diagram that illustrates an example of a neural network acoustic model 200 .
- the neural network 200 includes a frequency processing module 152 , a linear low-rank layer 154 , memory layers 156 a - 156 c , and a DNN 158 .
- Several variations of the frequency processing module 152 are discussed below.
- the network 200 discussed herein use a frame x t as input to the network.
- the frame x t can be considered to be a log-mel filterbank feature, although other types of features can be used.
- this frame can be optionally processed in frequency by either CNN or LSTM layers, as denoted by the frequency-processing block 252 in FIG. 2 . If no frequency processing is performed, the frame x t is passed directly to the first LSTM layer, and the architecture would be an LDNN.
- the output of the frequency processing module 252 is passed to a linear low-rank layer 254 .
- the low-rank layer can be a 256-dimensional layer.
- the output of the low-rank layer 254 is passed to one or more LSTM layers 256 a - 256 c .
- three LSTM layers 256 a - 256 c may be used.
- the one or more LSTM layers 256 a - 256 c can be arranged so that the first passes its output to the second LSTM layer, and the second LSTM passes its output to the third LSTM layer.
- each layer has 832 cells and a 512 unit projection layer.
- the output from the one or more LSTM layers 256 a - 256 c is provided to the DNN 258 which can include one or more hidden layers.
- the DNN layer has 1,024 hidden units.
- the output of the one or more hidden layers of the DNN 258 is provided to an output layer.
- the output layer can be a softmax output layer.
- the outputs of the output layer can represent values corresponding to different phonetic units.
- the respective outputs may each correspond to a different context-dependent HMM state of a phone.
- the output layer may provide 13,522 output values each corresponding to different context-dependent states.
- Each output can be indicative of a posterior probability of occurrence of the context-dependent state corresponding to the output.
- frequency processing modules 252 can be used.
- a convolutional layer may be used, with 256 feature maps.
- the filter size and pooling size are dependent on the input feature dimension for x t .
- a 21 ⁇ 1 frequency-time filter can be used for the convolutional layer.
- Non-overlapping max pooling can be used, and pooling can be performed only in frequency and with a pooling size of 9.
- the frequency processing module 252 can include LSTM blocks.
- the LSTM architecture consists of a set of recurrently connected subnetworks, referred to a memory blocks. Each memory block contains memory cells to store the temporal state of the network, as well as three multiplicative gate units to control information flow.
- the input gate controls the information passed from the input activations into the memory cells, while the output gate controls the information passed from the memory cells to the rest of the network.
- the forget gate adaptively reset's the memory of the cell.
- i j , f j , c j , and o j denote the input, forget, memory cell and output gate activations at step j.
- m j is the output of the LSTM layer.
- W are the different weight matrices, for example, W ix is the weight matrix from the input gate to the input.
- ⁇ is an element-wise dot product.
- ⁇ is the logistic sigmoid non-linearity while g and h are the cell input and output activations, which we take to be tan h.
- T-LSTM time LSTM
- the frequency LSTM uses the same equations as given above, except that it is used to model a sequential process in frequency.
- the F-LSTM is similar to the convolutional layer as both models look over a small local frequency patch and share model parameters as the filter is shifted.
- the main difference is that the F-LSTM models frequency variations through a recurrent state that is passed from one unrolled time step to another.
- the convolutional layer has a subsequent pooling step to achieve local translational invariance.
- the recurrent state in F-LSTMs can emulate local competition among features similar to max pooling.
- F-LSTMs have the added benefit that there is no need to tune the pooling parameters.
- the F-LSTM can be extended to model the sequential process of the signal in both time and frequency jointly with a time frequency LSTM (TF-LSTM). Since speech has correlations in both time and frequency, the TF-LSTMs often performs as as if not better than the F-LSTM. For each frequency step k and time step t, the TF-LSTM is given by the Equations 6-10.
- i t,k ⁇ ( W ix x t,k +W im (t) m t-1,k +W im (k) m t,k-1 +W ic c t-1,k +b i ) (6)
- f t,k ⁇ ( W fx x t,k +W fm (t) m t-1,k +W fm (k) m t,k-1 +W cf c t-1,k +b f ) (7)
- c t,k f t,k ⁇ c t-1,k +i t,k ⁇ g ( W cx x j +W cm m t-1,k +b c ) (8)
- o t,k ⁇ ( W ox x t,k +W om (t) m t-1,k +W om (k) m t,k-1 +W oc c t,k +b o
- the TF-LSTM uses the output from the previous frequency step (i.e., m t,k-1 ) and the previous time step (i.e., m t-1,k ). These previous outputs are weighted by separate weights W *m (t) and W *m (k) respectively. Notice the TF-LSTM still produces one output, m t,k , at each time-frequency (k, t) step. In FIG. 2 , this shared output is denoted by a dotted line between LSTM blocks 220 a - 220 c .
- FIG. 2 shows the state of the network 200 , including LSTM blocks 220 a - 220 c , for time step 0 in section 202 .
- FIG. 2 shows the state of the network 200 for the next time step, time step 1 , in section 204 , showing how information is passed between the LSTM blocks from one time step to another.
- the TF-LSTM still windows the input feature with a filter size of F and a stride of S, similar to the F-LSTM.
- the output out of each TF-LSTM denoted by ⁇ m t , 0, . . . , m t, L ⁇ are concatenated together and given to a linear dimensionality-reduction layer, and then the LDNN.
- a grid-LSTM has similarities with a TF-LSTM except there are separate LSTMs which move in time and frequency.
- the grid frequency LSTM (gF-LSTM) uses the state of the grid time LSTM (gT-LSTM) from the previous time step, and similarly the gT-LSTM uses the state of the gF-LSTM from the previous frequency step.
- the motivation for looking at the grid-LSTM is to explore benefits of having separate LSTMs to model the correlations in time and frequency.
- i t,k (j) ⁇ ( W ix (j) x t,k +W im (t) m t-1,k (t) +W im (k) m t,k-1 (k) +W ic (t) c t-1,k (t) +W ic (k) c t,k-1 (k) +b i (j) ) (11)
- the computation of the grid-LSTM becomes similar to that of TF-LSTMs.
- the output of the gT-LSTM denoted by ⁇ m t,0 (t) , . . . , m t,L (t) ⁇ and gF-LSTM, denoted ⁇ m t,0 (k) , . . . , m t,L (k) ⁇ are concatenated together and given to the linear dimensionality reduction layer, followed by the LDNN.
- LSTM architecture that can be used in the frequency processing module 252 is a ReNet LSTM, which is an F-LSTM unrolled in frequency and T-LSTM unrolled in time. These two LSTMs are treated completely independently, meaning the recurrent state is not shared when the F-LSTM and T-LSTMs overlap.
- the benefit of ReNet is computational efficiency, as the two LSTMs can be run in parallel. Since the LSTMs produce independent outputs, the outputs of both can be concatenated before giving them to a linear dimensionality-reduction layer and then the LDNN.
- Training for the neural network 200 can be done using a large set of training data, e.g., 2,000 hours of mild-noise training set including of 3 million English utterances.
- This data set is created by artificially corrupting clean utterances using a room simulator, adding varying degrees of noise and reverberation such that the overall SNR is between 5 dB and 30 dB.
- the noise sources can be taken from YouTube and daily life noisy environmental recordings.
- training can be done on a moderate-noise 2,000-hour data set, where clean utterances are corrupted with both reverberation and additive noise at SNRs ranging from 0 to 20 dB.
- the network is unrolled for 20 time steps for training with truncated backpropagation through time.
- the output state label is delayed by 5 frames, as we have observed that information about future frames improves the prediction of the current frame.
- Multichannel implementations can be trained using a training set with similar noise configurations to the moderate noise data set, using a 2 channel linear microphone with, e.g., a 14 cm spacing.
- the data sets can be anonymized and hand-transcribed, and are representative of search engine voice search traffic.
- the neural network 200 can be trained with the cross-entropy criterion, using asynchronous stochastic gradient descent (ASGD) optimization.
- ASGD stochastic gradient descent
- the weights for CNN and DNN layers are initialized using the Glorot-Bengio strategy, while LSTM layers are uniform randomly initialized to be between ⁇ 0.02 and 0.02.
- An exponentially decaying learning rate which starts at 0.004 and has a decay rate of 0.1 over 15 billion frames, can be used.
- the convolution layer in a CLDNN degrades performance compared to the LDNN. Since translational invariance in the CNN is captured through local filters and pooling, it is possible that as noise corrupts the signal, it becomes more difficult to make these local decisions.
- the F-LDNN shows improvements over the CLDNN and LDNN. In fact, a relative gain observed by the CLDNN over the LDNN for clean speech, is achieved by the F-LDNN in noisier conditions. Since the F-LDNN captures translational invariance through the recurrent connections, it can better model the evolutionary path of the signal in frequency compared to the CNN. By modeling correlations in time and frequency with the TF-LDNN, improvements can be achieved over LDNN, CLDNN, and F-LDNN models.
- Grid-LDNNs offer an additional relative improvement in mild and moderate noise over the TF-LDNN, with a similar computational cost since the weights of the cell state can be tied between the gT-LSTM and gF-LSTM. This shows that separate modeling of time and frequency correlations, while sharing the states, is effective to improve accuracy. In some implementations, performance with ReNet-LDNNs degrades for moderate noise conditions, showing the importance of sharing the state between T-LSTMs and F-LSTMs.
- Grid LSTMs can be used when learning a filterbank directly from a raw-waveform. Specifically, this can be effective for multichannel speech processing, where raw-waveform processing is necessary to preserve the fine time structure of the signal.
- Grid LSTMs can be used with the neural adaptive beamforming (NAB) model.
- NAB neural adaptive beamforming
- an “adaptive layer” takes a 35 ms input raw-waveform signal for each channel and uses a T-LSTM to predict filter coefficients for each channel. These filters are convolved with the raw-waveform from each channel and the outputs are summed, mimicking filter-and-sum beamforming.
- a “spectral decomposition” is performed on the single channel raw-waveform, using a time-convolution layer with max-pooling.
- This layer essentially learns a filterbank which is not constant-Q, and produces a 128-dimensional frequency feature at each frame. This feature is then given to a CLDNN/LDNN acoustic model.
- the NAB model suffers not only from the frequency convolution that sits above the spectral decomposition layer, not only because the learned filterbank is not constant-Q, but also because the filter into the spectral layer changes every frame because of the adaptive LSTM. Further improvements can be obtained replacing the CLDNN or LDNN with a grid-LDNN. Modeling time-frequency correlations with a 2D-LSTM is much more robust compared to convolution, even with a different feature representation.
- Grid-LSTMs in the LDNN architecture often offer the best performance of all 2D-LSTMs, particularly when the input is noisy or the filterbank is learned.
- the grid-LDNN shows a relative accuracy improvement over an LDNN and CLDNN for a mild noise task, moderate noise task, and multichannel task.
- FIG. 3 is a diagram that illustrates an example of a grid LSTM architecture 300 .
- the architecture includes a set of time-frequency bins 302 a - 302 c that each include two LSTM blocks: a time-LSTM block denoted by T, and a frequency-LSTM block denoted by F.
- the time-frequency bins 302 a - 302 c update their respective states with new input for each time step. Later states of the time-frequency bins 302 a - 302 c are designated as 302 a ′- 302 c ′ for time step 1 , and 302 a ′′- 302 c ′′ for time step 2 .
- time-frequency bins 302 a ′- 302 c ′ and 302 a ′′- 302 c ′′ do not represent separate memories, but rather reflect different states of the memories shown in time-frequency bins 302 a - 302 c as time progresses.
- each time step inputs including the audio features for the current time step are provided, with each time-frequency bin 302 a - 302 c receiving a subset of the input vector to the neural network.
- the input subset includes the acoustic features for the corresponding frequency step of the time-frequency bin 302 a - 302 c for the current time step.
- bin 302 a may receive inputs in the range [0,7] of the input vector
- bin 302 b may receive inputs in the range [8,15] of the input vector, and so on.
- both of the LSTM blocks may receive the same input values from the input vector.
- time-LSTM blocks each provide information used to calculate the state of the time-LSTM at the next time step, for the same frequency step, shown by horizontal dashed lines.
- arrow 320 shows that the state of the time-LSTM block in bin 302 a , representing time step 0 and frequency step 0 , is used to determine the state of the same time-LSTM block in bin 302 a ′ representing time step 1 and frequency step 0 .
- This represents, for example, the ability of the time-LSTM block to remember its state from one time step to the next.
- the frequency-LSTM blocks each provide information used to calculate the state of the frequency-LSTM at the same time step but at the next frequency step, shown by vertical dashed lines.
- arrow 330 shows that the state of the frequency-LSTM block in bin 302 a , representing time step 0 and frequency step 0 , is used to determine the state of the frequency-LSTM in bin 302 b representing time step 0 and frequency step 1 .
- Time-LSTM blocks can also be shared between the time-LSTM blocks and frequency LSTM blocks of different time-frequency bins.
- the time-LSTM blocks share information with certain frequency-LSTM blocks
- the frequency-LSTM blocks share information with certain time-LSTM blocks.
- These connections are shown by solid arrows between LSTM blocks.
- Each LSTM block may share information with a subset of the other LSTM blocks, and in some implementations, exactly one other LSTM block. In some implementations, this involves providing information to at least one LSTM for a different dimension (e.g., from time-LSTM block to frequency-LSTM block, or from frequency-LSTM block to time-LSTM block), and in some architectures exactly one LSTM for a different dimension.
- the LSTMs may provide information to LSTMs in one or more adjacent time-frequency bins, e.g., time-frequency bins that differ by no more than one step in either dimension.
- LSTMs in a particular time-frequency bin may share information with LSTMs in one or more time-frequency bins that are more than one step different from the particular time-frequency bin.
- the frequency-LSTM blocks each provide information to the time-LSTM block that is at the next frequency step and the same time step.
- the state of the time-LSTM in bin 302 b ′ is determined using the state of the frequency-LSTM block in time-frequency bin 302 a ′, as shown by arrow 340 .
- the state of a frequency-LSTM block is used to determine the states of both the time-LSTM block and the frequency-LSTM block for the next frequency step.
- the time-LSTM blocks each provide information to the frequency-LSTM block that is at the next time step and the same frequency step.
- the state of the frequency-LSTM block in time-frequency bin 302 b ′ is determined using the state of the time-LSTM block in time-frequency bin 302 b , as shown by arrow 350 .
- the state of at time-LSTM block is used to determine the states of both the time-LSTM block and the frequency LSTM block for the next time step.
- connections between time-LSTMs and frequency-LSTMs in the grid architecture is provided as an example, and other arrangements having more or fewer connections between the LSTM blocks can be used.
- FIG. 4 is a flow diagram that illustrate an example of a process 400 for performing speech recognition.
- the process 400 may be performed by one or more computers, such as the computing system 120 of FIG. 1 .
- Audio data for an utterance is received ( 402 ).
- This audio data may include audio data for a single channel or multiple channels of audio.
- one or more channels of audio detected by one or more microphones of a user device such as a phone, watch, tablet computer, etc.
- the audio can be provided to a speech recognition module, for example, to a server system over a network, or to a local software module of the user device.
- Feature vectors are determined from the audio data ( 404 ).
- the feature vectors may include, for example, log-mel filterbank features.
- audio samples rather than filter outputs or other designed features may be used.
- the feature vectors are provided as input to a neural network acoustic model ( 406 ).
- the input feature vectors can include values for log-mel features.
- the input vectors may comprise values representing audio waveform characteristics.
- the acoustic model may include filterbank parameters trained jointly with weights of the neural network.
- the audio waveform data may describe two channels of audio that include the utterance, for example, data describing recordings of the utterance from different spatial positions.
- a frequency processing module, or other portion of the neural network acoustic model may implement spatial filtering, e.g., beamforming, to cause enhance recognition.
- the neural network acoustic model can include a frequency processing module that performs multi-dimensional modeling of time and frequency information.
- the neural network can have first memory blocks for time information and second memory blocks for frequency information, where the first memory blocks are different from the second memory blocks.
- the first memory blocks and the second memory blocks can be long short-term memory (LSTM) blocks.
- the frequency processing module can include a grid-LSTM architecture separate LSTM blocks for both time and frequency.
- the neural network may be configured to share information from each of the first memory blocks with a respective proper subset of the second memory blocks, and the neural network can be configured to share information from each of the second memory blocks with a respective proper subset of the first memory blocks.
- the first memory blocks are time-LSTM blocks that each have a state
- the second memory blocks are frequency-LSTM blocks that each have a state.
- Each of the time-LSTM blocks and the frequency-LSTM blocks has a corresponding frequency step in a sequence of multiple frequency steps.
- the states are determined for each of a sequence of multiple time steps.
- the frequency-LSTM block determines its state using the state of the time-LSTM block corresponding to the same frequency step at the previous time step.
- the time-LSTM block determines its state using the state of the frequency-LSTM block corresponding to the same time step and the previous frequency step.
- the time-LSTM block determines its state based on (i) input received for a current time step, (ii) the state of the time-LSTM block at a previous time step, and (iii) a state of exactly one of the frequency-LSTM blocks.
- the frequency-LSTM block determines its state based on (i) input received for a current time step, (ii) the state of the frequency-LSTM block for a previous frequency step at the current time step, and (iii) a state of exactly one of the time-LSTM blocks.
- the neural network includes a grid-LSTM module, a linear projection layer, one or more LSTM layers, and a deep neural network (DNN).
- the grid-LSTM module includes the first memory blocks and the second memory blocks, and the grid-LSTM module provides output to the linear projection layer.
- the linear projection layer provides output to the one or more LSTM layers, and the one or more LSTM layers provide output to the DNN.
- Output of the neural network acoustic model is received ( 408 ). For example, one or more outputs can be received that each indicate a likelihood that a respective phonetic unit represents a portion of the utterance.
- the neural network can provide multiple outputs corresponding to different context-dependent states of phones. Each of the multiple outputs can indicate a likelihood of occurrence for the corresponding context-dependent state.
- the neural network may provide outputs that indicate a probability distribution showing the likelihood that the particular portion of the utterance represents various context-dependent states.
- a transcription for the utterance is determined using the output of the neural network acoustic model ( 410 ).
- the outputs of the neural network acoustic model for several time steps can be used to determine which sounds or states are most likely part of the utterance.
- a language model can be used to determine which sequence of words most closely match the sequences of sounds or states determined by the neural network acoustic model.
- the transcription is provided, for example, for display, to an application, or to a device ( 412 ).
- the audio data can be audio data generated by a client device and received over a network, and the transcription can be provided over the network to the client device.
- a server system can determine that the utterance represents a request, such as a query or a voice command, and the server system can provide results to the query or information allowing the client device to carry out the request.
- each of the time-LSTM blocks and the frequency-LSTM blocks has one or more weights, and the weights for the time-LSTM blocks are independent of the weights for the frequency-LSTM blocks.
- each of the time-LSTM blocks and the frequency-LSTM blocks has one or more weights, and at least some of the weights are shared between the time-LSTM blocks and the frequency-LSTM blocks. In some implementations, all of the weights are shared between the time-LSTM and frequency-LSTM blocks.
- the neural network is configured so that the first memory blocks do not share information with the second memory blocks.
- ReNets can be used in the neural network.
- Embodiments of the invention and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the invention may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer-readable medium for execution by, or to control the operation of, data processing apparatus.
- the computer readable medium may be a non-transitory computer readable storage medium, a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- a computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- FPGA field programmable gate array
- ASIC application specific integrated circuit
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media, and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments of the invention may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Embodiments of the invention may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the invention, or any combination of one or more such back end, middleware, or front end components.
- the components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system may include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- HTML file In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.
Abstract
Description
i j=σ(W ix x j +W im m t-1 +W ic c t-1 +b i) (1)
f j=σ(W fx x j +W mf m t-1 +W cf c t-1 +b f) (2)
c j =f j ⊙c t-1 +i j ⊙g(W cx x j +W cm m t-1 +b c) (3)
o j=σ(W ox x j +W om m t-1 +W oc c t +b o) (4)
m j =o j ⊙h(c j) (5)
i t,k=σ(W ix x t,k +W im (t) m t-1,k +W im (k) m t,k-1 +W ic c t-1,k +b i) (6)
f t,k=σ(W fx x t,k +W fm (t) m t-1,k +W fm (k) m t,k-1 +W cf c t-1,k +b f) (7)
c t,k =f t,k ⊙c t-1,k +i t,k ⊙g(W cx x j +W cm m t-1,k +b c) (8)
o t,k=σ(W ox x t,k +W om (t) m t-1,k +W om (k) m t,k-1 +W oc c t,k +b o) (9)
m t,k =o t,k ⊙h(c t,k) (10)
i t,k (j)=σ(W ix (j) x t,k +W im (t) m t-1,k (t) +W im (k) m t,k-1 (k) +W ic (t) c t-1,k (t) +W ic (k) c t,k-1 (k) +b i (j)) (11)
f t,k (j)=σ(W fx (j) x t,k +W mf (t) m t-1,k (t) +W mf (k) m k-1,t (k) +W cf (t) c t-1,k (t) +W cf (k) c t,k-1 (k) +b f (j)) (12)
c t,k (t) =f t,k (t) ⊙c t-1,k (t) +i t,k (t) ⊙g(W cx (t) x t,k +W cm (t) m t-1,k (t) +W cm (k) m t,k-1 (k) +b c (t)) (13)
c t,k (k) =f t,k (k) ⊙c t,k-1 (k) +i t,k (k) ⊙g(W cx (k) x t,k +W cm (t) m t-1,k (t) +W cm (k) m t,k-1 (k) +b c (k)) (14)
o t,k (j)=σ(W ox (j) x t,k +W om (t) m t-1,k (t) +W om (k) m t,k-1 (k) +W oc (t) c t,k (t) +W oc (k) c t,k (k) +b o (j)) (15)
m t,k (j) =o t,k (j) ⊙h(c t,k (j)) (16)
Claims (16)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/217,457 US9984683B2 (en) | 2016-07-22 | 2016-07-22 | Automatic speech recognition using multi-dimensional models |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/217,457 US9984683B2 (en) | 2016-07-22 | 2016-07-22 | Automatic speech recognition using multi-dimensional models |
Publications (2)
Publication Number | Publication Date |
---|---|
US20180025721A1 US20180025721A1 (en) | 2018-01-25 |
US9984683B2 true US9984683B2 (en) | 2018-05-29 |
Family
ID=60988784
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/217,457 Active US9984683B2 (en) | 2016-07-22 | 2016-07-22 | Automatic speech recognition using multi-dimensional models |
Country Status (1)
Country | Link |
---|---|
US (1) | US9984683B2 (en) |
Cited By (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160284347A1 (en) * | 2015-03-27 | 2016-09-29 | Google Inc. | Processing audio waveforms |
US20180322865A1 (en) * | 2017-05-05 | 2018-11-08 | Baidu Online Network Technology (Beijing) Co., Ltd . | Artificial intelligence-based acoustic model training method and apparatus, device and storage medium |
US20180366107A1 (en) * | 2017-06-16 | 2018-12-20 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and device for training acoustic model, computer device and storage medium |
US20190005946A1 (en) * | 2017-06-28 | 2019-01-03 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and apparatus for correcting speech recognition result, device and computer-readable storage medium |
US10249292B2 (en) * | 2016-12-14 | 2019-04-02 | International Business Machines Corporation | Using long short-term memory recurrent neural network for speaker diarization segmentation |
US20190147855A1 (en) * | 2017-11-13 | 2019-05-16 | GM Global Technology Operations LLC | Neural network for use in speech recognition arbitration |
US10339921B2 (en) | 2015-09-24 | 2019-07-02 | Google Llc | Multichannel raw-waveform neural networks |
US10373022B1 (en) * | 2018-02-28 | 2019-08-06 | Konica Minolta Laboratory U.S.A., Inc. | Text image processing using stroke-aware max-min pooling for OCR system employing artificial neural network |
CN110164418A (en) * | 2019-07-10 | 2019-08-23 | 哈尔滨工业大学 | Automatic speech recognition accelerated method based on the long short-term memory recurrent neural network of convolution grid |
US10546575B2 (en) | 2016-12-14 | 2020-01-28 | International Business Machines Corporation | Using recurrent neural network for partitioning of audio data into segments that each correspond to a speech feature cluster identifier |
US10665243B1 (en) * | 2016-11-11 | 2020-05-26 | Facebook Technologies, Llc | Subvocalized speech recognition |
US10923112B2 (en) * | 2013-12-17 | 2021-02-16 | Google Llc | Generating representations of acoustic sequences |
US20210272337A1 (en) * | 2017-06-28 | 2021-09-02 | Shanghai United Imaging Healthcare Co., Ltd. | Systems and methods for determining parameters for medical image processing |
US11176443B1 (en) * | 2017-12-21 | 2021-11-16 | Automation Anywhere, Inc. | Application control and text detection from application screen images |
Families Citing this family (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10546230B2 (en) * | 2016-08-12 | 2020-01-28 | International Business Machines Corporation | Generating labeled data by sequence-to-sequence modeling with added perturbations to encoded information |
US11080591B2 (en) * | 2016-09-06 | 2021-08-03 | Deepmind Technologies Limited | Processing sequences using convolutional neural networks |
EP3497629B1 (en) | 2016-09-06 | 2020-11-04 | Deepmind Technologies Limited | Generating audio using neural networks |
EP3767547A1 (en) | 2016-09-06 | 2021-01-20 | Deepmind Technologies Limited | Processing sequences using convolutional neural networks |
CN110023963B (en) | 2016-10-26 | 2023-05-30 | 渊慧科技有限公司 | Processing text sequences using neural networks |
US10229685B2 (en) * | 2017-01-18 | 2019-03-12 | International Business Machines Corporation | Symbol sequence estimation in speech |
CN106920545B (en) * | 2017-03-21 | 2020-07-28 | 百度在线网络技术（北京）有限公司 | Speech feature extraction method and device based on artificial intelligence |
JP6812381B2 (en) * | 2018-02-08 | 2021-01-13 | 日本電信電話株式会社 | Voice recognition accuracy deterioration factor estimation device, voice recognition accuracy deterioration factor estimation method, program |
US10621990B2 (en) * | 2018-04-30 | 2020-04-14 | International Business Machines Corporation | Cognitive print speaker modeler |
CN108694951B (en) * | 2018-05-22 | 2020-05-22 | 华南理工大学 | Speaker identification method based on multi-stream hierarchical fusion transformation characteristics and long-and-short time memory network |
US11416741B2 (en) | 2018-06-08 | 2022-08-16 | International Business Machines Corporation | Teacher and student learning for constructing mixed-domain model |
US10380997B1 (en) | 2018-07-27 | 2019-08-13 | Deepgram, Inc. | Deep learning internal state index-based search and classification |
CN109308731B (en) * | 2018-08-24 | 2023-04-25 | 浙江大学 | Speech driving lip-shaped synchronous face video synthesis algorithm of cascade convolution LSTM |
US10963721B2 (en) | 2018-09-10 | 2021-03-30 | Sony Corporation | License plate number recognition based on three dimensional beam search |
US11282512B2 (en) * | 2018-10-27 | 2022-03-22 | Qualcomm Incorporated | Automatic grammar augmentation for robust voice command recognition |
US11017177B2 (en) | 2019-06-27 | 2021-05-25 | Conduent Business Services, Llc | Neural network systems and methods for target identification from text |
CN110379412B (en) * | 2019-09-05 | 2022-06-17 | 腾讯科技（深圳）有限公司 | Voice processing method and device, electronic equipment and computer readable storage medium |
CN110610697B (en) * | 2019-09-12 | 2020-07-31 | 上海依图信息技术有限公司 | Voice recognition method and device |
US11657304B2 (en) * | 2020-05-01 | 2023-05-23 | Microsoft Technology Licensing, Llc | Assessing similarity between items using embeddings produced using a distributed training framework |
WO2024054556A2 (en) * | 2022-09-07 | 2024-03-14 | Google Llc | Generating audio using auto-regressive generative neural networks |
Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160071526A1 (en) * | 2014-09-09 | 2016-03-10 | Analog Devices, Inc. | Acoustic source tracking and selection |
US20160099010A1 (en) * | 2014-10-03 | 2016-04-07 | Google Inc. | Convolutional, long short-term memory, fully connected deep neural networks |
US9368110B1 (en) * | 2015-07-07 | 2016-06-14 | Mitsubishi Electric Research Laboratories, Inc. | Method for distinguishing components of an acoustic signal |
US20160171974A1 (en) * | 2014-12-15 | 2016-06-16 | Baidu Usa Llc | Systems and methods for speech transcription |
US20160284347A1 (en) * | 2015-03-27 | 2016-09-29 | Google Inc. | Processing audio waveforms |
US20160358602A1 (en) * | 2015-06-05 | 2016-12-08 | Apple Inc. | Robust speech recognition in the presence of echo and noise using multiple signals for discrimination |
US20170092265A1 (en) * | 2015-09-24 | 2017-03-30 | Google Inc. | Multichannel raw-waveform neural networks |
US20170103752A1 (en) * | 2015-10-09 | 2017-04-13 | Google Inc. | Latency constraints for acoustic modeling |
US9653093B1 (en) * | 2014-08-19 | 2017-05-16 | Amazon Technologies, Inc. | Generative modeling of speech using neural networks |
US20170148433A1 (en) * | 2015-11-25 | 2017-05-25 | Baidu Usa Llc | Deployed end-to-end speech recognition |
US20170147910A1 (en) * | 2015-10-02 | 2017-05-25 | Baidu Usa Llc | Systems and methods for fast novel visual concept learning from sentence descriptions of images |
US20170154033A1 (en) * | 2015-11-30 | 2017-06-01 | Samsung Electronics Co., Ltd. | Speech recognition apparatus and method |
US20170178664A1 (en) * | 2014-04-11 | 2017-06-22 | Analog Devices, Inc. | Apparatus, systems and methods for providing cloud based blind source separation services |
-
2016
- 2016-07-22 US US15/217,457 patent/US9984683B2/en active Active
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170178664A1 (en) * | 2014-04-11 | 2017-06-22 | Analog Devices, Inc. | Apparatus, systems and methods for providing cloud based blind source separation services |
US9653093B1 (en) * | 2014-08-19 | 2017-05-16 | Amazon Technologies, Inc. | Generative modeling of speech using neural networks |
US20160071526A1 (en) * | 2014-09-09 | 2016-03-10 | Analog Devices, Inc. | Acoustic source tracking and selection |
US20160099010A1 (en) * | 2014-10-03 | 2016-04-07 | Google Inc. | Convolutional, long short-term memory, fully connected deep neural networks |
US20160171974A1 (en) * | 2014-12-15 | 2016-06-16 | Baidu Usa Llc | Systems and methods for speech transcription |
US20160284347A1 (en) * | 2015-03-27 | 2016-09-29 | Google Inc. | Processing audio waveforms |
US20160358602A1 (en) * | 2015-06-05 | 2016-12-08 | Apple Inc. | Robust speech recognition in the presence of echo and noise using multiple signals for discrimination |
US9368110B1 (en) * | 2015-07-07 | 2016-06-14 | Mitsubishi Electric Research Laboratories, Inc. | Method for distinguishing components of an acoustic signal |
US20170092265A1 (en) * | 2015-09-24 | 2017-03-30 | Google Inc. | Multichannel raw-waveform neural networks |
US20170147910A1 (en) * | 2015-10-02 | 2017-05-25 | Baidu Usa Llc | Systems and methods for fast novel visual concept learning from sentence descriptions of images |
US20170103752A1 (en) * | 2015-10-09 | 2017-04-13 | Google Inc. | Latency constraints for acoustic modeling |
US20170148433A1 (en) * | 2015-11-25 | 2017-05-25 | Baidu Usa Llc | Deployed end-to-end speech recognition |
US20170154033A1 (en) * | 2015-11-30 | 2017-06-01 | Samsung Electronics Co., Ltd. | Speech recognition apparatus and method |
Non-Patent Citations (38)
Title |
---|
Bluche et al., "Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with MDLSTM Attention," arXiv preprint arXiv:1604.03286, Apr. 2016, pp. 1-10. |
Chherawala et al., "Context-dependent BLSTM models. Application to offline handwriting recognition," 2014 IEEE International Conference on Image Processing (ICIP), Oct. 2014, pp. 2565-2569. |
Chu, "Grid-LSTM," MIT Content Upload, Sep. 2015, PowerPoint Presentation, 17 pages. |
Dean et al., "Large Scale Distributed Deep Networks," Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1223-1231. |
Glorot et al., "Understanding the Difficulty of Training Deep Feedforward Neural Networks," in Proc. AISTATS, May 2010, vol. 9, pp. 249-256. |
Graves et al., "Multi-Dimensional Recurrent Neural Networks," in Proc. ICANN, May 2007, 10 pages. |
Graves, "Offline Arabic Handwriting Recognition with Multidimensional Recurrent Neural Networks," Guide to OCR for Arabic Scripts. Springer London, 2012, pp. 297-313. |
Hinton et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition," IEEE Signal Processing Magazine, vol. 29, No. 6, Nov. 2012, pp. 82-97. |
Hochreiter et al., "Long Short-Term Memory," Neural Computation, vol. 9, No. 8, pp. 1735-1780, 1997. |
Kaiser et al., "Neural GPUs Learn Algorithms," arXiv preprint arXiv:1511.08228, Mar. 2016, pp. 1-9. |
Kalchbrenner et al., "Grid Long Short-Term Memory," arXiv preprint arXiv:1507.01526, Jan. 2016, pp. 1-15. |
Kalchbrenner et al., "Grid LSTM," Joost Bastings, Institute for Logic, Language and Computation, PowerPoint Presentation, Mar. 10, 2016, 194 pages. |
Koutnik, "The Deepest Learning" IDSIA, 2015, PowerPoint, retrieved on Jul. 21, 2016. Retrieved from the Internet: URL<http://www.s-i.ch/fileadmin/daten/sgaico/Koutnik_sgaico2015.pdf>, 37 pages. |
Le, "A Tutorial on Deep Learning Part 2: Autoencoders, Convolutional Neural Networks and Recurrent Neural Networks," Stanford, Oct. 2015, pp. 1-20. |
Li (Li, Jinyu, et al. "Exploring multidimensional LSTMs for large vocabulary ASR." Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016.; ICASSP 2016 was held from Mar. 20-Mar. 25; also cited by Applicant in IDS). * |
Li et al., "Exploring Multidimensional LSTMs for Large Vocabulary ASR," 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2016, pp. 4940-4944. |
Li et al., "LSTM Time and Frequency Recurrence for Automatic Speech Recognition," 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Dec. 2015, pp. 187-191. |
Li et al., "Neural Network Adaptive Beamforming for Robust Multichannel Speech Recognition," in submitted to Proc. INTERSPEECH, 2016, 5 pages. |
Li, "Improving Information Flow in Recurrent Networks," Yanran's Attic, Nov. 28, 2015 [retrieved on Jul. 21, 2016]. Retrieved from the Internet: URL<http://yanran.li/peppypapers/2015/11/28/improving-information-flow-in-recurrent-networks.html>, 13 pages. |
Li, Jinyu, et al. "Exploring multidimensional LSTMs for large vocabulary ASR." Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016.; ICASSP 2016 was held from Mar. 20-Mar. 25. * |
Olah, "Understanding LSTM Networks," colah's blog, Aug. 27, 2015 [retrieved on Jul. 21, 2016]. Retrieved from the Internet: URL<http://colah.github.io/posts/2015-08-Understanding-LSTMs/>, 9 pages. |
Raja, "FNNs, RNNs, LSTM and BLSTM," PowerPoint, retrieved on Jul. 21, 2016. Retrieved from the Internet: URL<http://cse.jitkgp.ac.in/˜psraja/FNNs%20.RNNs%20.LSTM%20and%20BLSTM.pdf>, 36 pages. |
Sainath (Sainath, Tara N., et al. "Convolutional, long short-term memory, fully connected deep neural networks." Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015; ICASSP 2015 was held from Apr. 19-Apr. 24.; also cited by Applicant in IDS). * |
Sainath et al., "Convolutional, Long Short-Tenn Memory, Fully Connected Deep Neural Networks," 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Apr. 2015, pp. 4580-4584. |
Sainath et al., "Factored Spatial and Spectral Multichannel Raw Waveform CLDNNs," International Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2016. |
Sainath et al., "Improvements to Deep Convolutional Neural Networks for LVCSR," Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, Dec. 2013, pp. 315-320. |
Sainath et al., "Learning the Speech Front-end with Raw Waveform CLDNNs," in Proc. Interspeech, Sep. 2015, 5 pages. |
Sainath, Tara N., et al. "Convolutional, long short-term memory, fully connected deep neural networks." Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015; ICASSP 2015 was held from Apr. 19-Apr. 24. * |
Sainath. "Towards End-to-End Speech Recognition Using Deep Neural Networks," PowerPoint presentation, Deep Learning Workshop, ICML, Jul. 10, 2015, 51 pages. |
Sak et al., "Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition," arXiv preprint arXiv:1402.1128, Feb. 2014, pp. 1-5. |
Sak et al., "Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling," in Proc. Interspeech 2014, Sep. 2014, pp. 338-342. |
Sapunov, "Multi-dimensional RNNs," SlideShare, Apr. 27, 2016 [retrieved on Jul. 21, 2016]. Retrieved from the Internet: URL<http://www.slideshare.net/grigorysapunov/multidimensional-rnn>, 36 pages. |
Schmidhuber, "Recurrent Neural Networks," online citation, updated 2015 [retrieved on Jul. 21, 2016]. Retrieved from the Internet: URL<http://people.idsia.ch/˜juergen/rnn.html>, 10 pages. |
Seide (Seide, Frank, Gang Li, and Dong Yu. "Conversational speech transcription using context-dependent deep neural networks." Twelfth Annual Conference of the International Speech Communication Association. 2011.) * |
Seide, Frank, Gang Li, and Dong Yu. "Conversational speech transcription using context-dependent deep neural networks." Twelfth Annual Conference of the International Speech Communication Association. 2011. * |
Stollenga et al., "Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation," In Advances in Neural Information Processing Systems (NIPS), 2015, pp. 2998-3006. |
Visin et al., "ReNet: A recurrent neural network based alternative to convolutional networks," arXiv preprint arXiv:1505.00393, Jul. 2015, pp. 1-9. |
Zhang et al., "Highway Long Short-Term Memory RNNS for Distant Speech Recognition," arXiv preprint arXiv:1510.08983, Jan. 2016, pp. 1-5. |
Cited By (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11721327B2 (en) | 2013-12-17 | 2023-08-08 | Google Llc | Generating representations of acoustic sequences |
US10923112B2 (en) * | 2013-12-17 | 2021-02-16 | Google Llc | Generating representations of acoustic sequences |
US10403269B2 (en) * | 2015-03-27 | 2019-09-03 | Google Llc | Processing audio waveforms |
US10930270B2 (en) | 2015-03-27 | 2021-02-23 | Google Llc | Processing audio waveforms |
US20160284347A1 (en) * | 2015-03-27 | 2016-09-29 | Google Inc. | Processing audio waveforms |
US10339921B2 (en) | 2015-09-24 | 2019-07-02 | Google Llc | Multichannel raw-waveform neural networks |
US10665243B1 (en) * | 2016-11-11 | 2020-05-26 | Facebook Technologies, Llc | Subvocalized speech recognition |
US10902843B2 (en) | 2016-12-14 | 2021-01-26 | International Business Machines Corporation | Using recurrent neural network for partitioning of audio data into segments that each correspond to a speech feature cluster identifier |
US10546575B2 (en) | 2016-12-14 | 2020-01-28 | International Business Machines Corporation | Using recurrent neural network for partitioning of audio data into segments that each correspond to a speech feature cluster identifier |
US10249292B2 (en) * | 2016-12-14 | 2019-04-02 | International Business Machines Corporation | Using long short-term memory recurrent neural network for speaker diarization segmentation |
US20180322865A1 (en) * | 2017-05-05 | 2018-11-08 | Baidu Online Network Technology (Beijing) Co., Ltd . | Artificial intelligence-based acoustic model training method and apparatus, device and storage medium |
US10565983B2 (en) * | 2017-05-05 | 2020-02-18 | Baidu Online Network Technology (Beijing) Co., Ltd. | Artificial intelligence-based acoustic model training method and apparatus, device and storage medium |
US20180366107A1 (en) * | 2017-06-16 | 2018-12-20 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and device for training acoustic model, computer device and storage medium |
US10522136B2 (en) * | 2017-06-16 | 2019-12-31 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and device for training acoustic model, computer device and storage medium |
US20190005946A1 (en) * | 2017-06-28 | 2019-01-03 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and apparatus for correcting speech recognition result, device and computer-readable storage medium |
US10380996B2 (en) * | 2017-06-28 | 2019-08-13 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and apparatus for correcting speech recognition result, device and computer-readable storage medium |
US20210272337A1 (en) * | 2017-06-28 | 2021-09-02 | Shanghai United Imaging Healthcare Co., Ltd. | Systems and methods for determining parameters for medical image processing |
US11908046B2 (en) * | 2017-06-28 | 2024-02-20 | Shanghai United Imaging Healthcare Co., Ltd. | Systems and methods for determining processing parameter for medical image processing |
US20190147855A1 (en) * | 2017-11-13 | 2019-05-16 | GM Global Technology Operations LLC | Neural network for use in speech recognition arbitration |
US11176443B1 (en) * | 2017-12-21 | 2021-11-16 | Automation Anywhere, Inc. | Application control and text detection from application screen images |
US10373022B1 (en) * | 2018-02-28 | 2019-08-06 | Konica Minolta Laboratory U.S.A., Inc. | Text image processing using stroke-aware max-min pooling for OCR system employing artificial neural network |
CN110164418A (en) * | 2019-07-10 | 2019-08-23 | 哈尔滨工业大学 | Automatic speech recognition accelerated method based on the long short-term memory recurrent neural network of convolution grid |
Also Published As
Publication number | Publication date |
---|---|
US20180025721A1 (en) | 2018-01-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9984683B2 (en) | Automatic speech recognition using multi-dimensional models | |
US11393457B2 (en) | Complex linear projection for acoustic modeling | |
US11783849B2 (en) | Enhanced multi-channel acoustic models | |
KR102213013B1 (en) | Frequency-based audio analysis using neural networks | |
US11257485B2 (en) | Adaptive audio enhancement for multichannel speech recognition | |
US10930270B2 (en) | Processing audio waveforms | |
US10529320B2 (en) | Complex evolution recurrent neural networks | |
US11158305B2 (en) | Online verification of custom wake word | |
US9824683B2 (en) | Data augmentation method based on stochastic feature mapping for automatic speech recognition | |
US11132992B2 (en) | On-device custom wake word detection | |
US20180174576A1 (en) | Acoustic-to-word neural network speech recognizer | |
US20170103752A1 (en) | Latency constraints for acoustic modeling | |
Aggarwal et al. | Filterbank optimization for robust ASR using GA and PSO | |
US20210358493A1 (en) | Method and apparatus with utterance time estimation | |
Bawa et al. | Developing sequentially trained robust Punjabi speech recognition system under matched and mismatched conditions | |
Zweig et al. | Speech recognition with segmental conditional random fields: final report from the 2010 JHU summer workshop | |
Abdelmoula | Noise robust keyword spotting using deep neural networks for embedded platforms | |
Samanta et al. | An energy-efficient voice activity detector using reconfigurable Gaussian base normalization deep neural network | |
Mirsamadi | Robust acoustic modeling and front-end design for distant speech recognition | |
Rangslang | Segment phoneme classification from speech under noisy conditions: Using amplitude-frequency modulation based two-dimensional auto-regressive features with deep neural networks |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LI, BO;SAINATH, TARA N.;REEL/FRAME:039241/0286Effective date: 20160721 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |