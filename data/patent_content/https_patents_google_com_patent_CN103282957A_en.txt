CN103282957A - Automatically monitoring for voice input based on context - Google Patents
Automatically monitoring for voice input based on context Download PDFInfo
- Publication number
- CN103282957A CN103282957A CN2011800471540A CN201180047154A CN103282957A CN 103282957 A CN103282957 A CN 103282957A CN 2011800471540 A CN2011800471540 A CN 2011800471540A CN 201180047154 A CN201180047154 A CN 201180047154A CN 103282957 A CN103282957 A CN 103282957A
- Authority
- CN
- China
- Prior art keywords
- computing device
- mobile computing
- user
- speech input
- described mobile
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L17/00—Speaker identification or verification
- G10L17/22—Interactive procedures; Man-machine interfaces
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72403—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72403—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality
- H04M1/72409—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality by interfacing with external accessories
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72448—User interfaces specially adapted for cordless or mobile telephones with means for adapting the functionality of the device according to specific conditions
- H04M1/72454—User interfaces specially adapted for cordless or mobile telephones with means for adapting the functionality of the device according to specific conditions according to context-related or environment-related conditions
-
- H04W4/046—
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/227—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of the speaker; Human-factor methodology
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/02—Constructional features of telephone sets
- H04M1/04—Supports for telephone transmitters or receivers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72403—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality
- H04M1/72409—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality by interfacing with external accessories
- H04M1/72412—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality by interfacing with external accessories using two-way short-range wireless interfaces
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72403—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality
- H04M1/72445—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality for supporting Internet browser applications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2201/00—Electronic components, circuits, software, systems or apparatus used in telephone systems
- H04M2201/40—Electronic components, circuits, software, systems or apparatus used in telephone systems using speech recognition
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2250/00—Details of telephonic subscriber devices
- H04M2250/74—Details of telephonic subscriber devices with voice recognition means
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W4/00—Services specially adapted for wireless communication networks; Facilities therefor
- H04W4/02—Services making use of location information
- H04W4/029—Location-based management or tracking services
Abstract
In one implementation, a computer-implemented method includes detecting a current context associated with a mobile computing device and determining, based on the current context, whether to switch the mobile computing device from a current mode of operation to a second mode of operation during which the mobile computing device monitors ambient sounds for voice input that indicates a request to perform an operation. The method can further include, in response to determining whether to switch to the second mode of operation, activating one or more microphones and a speech analysis subsystem associated with the mobile computing device so that the mobile computing device receives a stream of audio data. The method can also include providing output on the mobile computing device that is responsive to voice input that is detected in the stream of audio data and that indicates a request to perform an operation.
Description
The cross reference of related application
The application requires to submit on August 6th, 2010, name is called the U. S. application sequence No.12/852 of " AUTOMATICALLY MONITORING FOR VOICE INPUT BASED ON CONTEXT ", 256 right of priority, its disclosure is incorporated into this by introducing.
Technical field
This document has been described generally and be used for have been used mobile computing device (such as, mobile phone) automatically to monitor method, system and the technology of speech input.
Background technology
Mobile computing device (for example, mobile phone, smart phone, PDA(Personal Digital Assistant), portable electronic device etc.) has been configured to when the user clearly points out reception and processing speech or language input speech or language input be received and processing.For example, mobile computing device has been configured to press and hold button is pressed threshold time (for example, a second) and begun to monitor speech in response to the user.For example, if user's expectation is submitted oral searching request to this type of mobile computing device, then the user need press before submitting the speech input to and hold button is pressed threshold time at least, received that searching request can be not processed yet then otherwise the speech input will can not be moved computing equipment.
Summary of the invention
By the technology of describing in this document, the context of computing equipment (such as, mobile phone (for example, smart phone or application program phone)) will be considered in order to determine when the input of monitoring speech automatically, such as oral searching request.Automatically do not need to determine clear and definite user guided determining.By the technology of describing in this document, mobile computing device can determine when the input of monitoring speech automatically based on the current context that is associated with this mobile computing device, rather than wait for that this mobile computing device of user prompt begins to monitor speech input (for example, pressing also hold button threshold time).The current context that is associated with mobile computing device (and/or user of this mobile computing device) can comprise: device external context (such as the environment around the indication equipment), perhaps the device interior context (such as in the equipment of being stored in about the historical information of this equipment).The device external context for example can comprise the residing physical location of mobile computing device (for example, the family that is determined by GPS in the equipment or other technologies, unit, automobile etc.), and the motion state of mobile computing device (for example, accelerate, static etc.).The device interior context can comprise nearest activity on the mobile computing device (for example, the Email of social networking activities, sending/receiving, the call of dialing/answering etc.).The current context of mobile computing device (and/or its user) is independent of user's input itself of instructing equipment to listen to the language input.
For example, the imagination user is being with his/her mobile computing device to come home from work and this user begins to cook supper.Be arranged in user family's (context of this mobile computing device) afterwards detecting mobile computing device, this mobile computing device begins to monitor the speech input from the user automatically in this example.This equipment for example can be via the GPS reading or by determining that it parks specific music harbour (dock) or the particular type of music harbour is determined its context.The user recognizes when cooking supper that it can't be write down and add how many specific batchings in dish.The user can inquire simply that should rather than need stop to prepare food to how many batchings of interpolation in the dish (for example removes to search recipe, wash one's hands and find recipe in the book or in the electronic document), this is that this mobile computing device can receive and handle oral request because mobile computing device is imported at the monitoring speech.For example, mobile computing device can be located the electronic document that comprises this recipe, identifies the consumption of described batching, and acoustically responds the user by consumption information (for example, " your recipe requires 1 glass of sugar ").By the technology of describing in this document, the user in this example can obtain the answer of its problem and not need to interrupt its food preparation (for example, not needing at first physically to point out mobile computing device to receive the speech input).
Continue the epimere example, mobile computing device can determine that it is arranged in user's family based on the harbour type that this mobile computing device is placed in user family.For example, mobile computing device can electrically contact or identify via the electronic communication between harbour and the equipment (for example, via BLUETOOTH or RFID) type of harbour based on the physics on the harbour that matches each other and the equipment.For example, can provide on the harbour and be intended to the specific pin arrangement that family uses, and can provide different layouts to the harbour that is intended to selling for using in the car.
Intercept by only enabling this type of in the specific context that the user can limit, the technology here provides powerful user interface also to allow the user to control visit to its information simultaneously.Therefore, this type of monitoring can be provided as to enable intercepts the option of its equipment support of the necessary active arrangement of user before, in order to give the user control to this feature.In addition, when equipment entered listen mode, this equipment can be to user's blare.In addition, processing described here can been separated between any server system that equipment and this equipment communicates by letter with it, so that monitoring can take place at this equipment, and when this type of action that monitoring triggering requires with server system is communicated by letter, this equipment can be announced this fact and/or seek approval from this user to the user.In addition, use technology described here can (for example be limited in advance by the user by the specific action that equipment is taked, in tabulation), so that the user can comprise the action that this user can comfortable execution (for example, obtaining the similar action that information and user at weather, film time, flight determine not involve privacy concern).
In one implementation, computer implemented method comprises the current context that detection is associated with mobile computing device, described context is in described mobile device outside and indicate described equipment in the current state of its surrounding environment, and determine whether described mobile computing device is switched to second operator scheme from current operator scheme based on described current context, described mobile computing device is at speech Input Monitor Connector ambient sound during described second operator scheme, and described speech input indication is used for the request of executable operations.In response to determining whether to switch to described second operator scheme, this method may further include and activates one or more microphones and the speech analysis subsystem that is associated with described mobile computing device, so that described mobile computing device receives audio data stream.The output that provides on the described mobile computing device in response to the speech input can also be provided this method, and described speech input is detected in the described audio data stream and indicates the request that is used for executable operations.
In another is realized, a kind of system for monitoring speech input automatically comprises mobile computing device and one or more microphone, and described one or more microphones are configured to the reception environment sound signal and provide the electronic audio frequency data to described mobile computing device.This system can also comprise the context determining unit, be configured to detect the current context that is associated with described mobile computing device, described context is in described mobile device outside and indicate the current state of described equipment in its surrounding environment, and mode selecting unit, be configured to determine whether described mobile computing device is switched to second operator scheme from current operator scheme based on the described current context of being determined by described context determining unit, described mobile computing device is at speech Input Monitor Connector ambient sound during described second operator scheme, and described speech input indication is used for the request of executable operations.This system may further include the input subsystem of described mobile computing device, be configured in response to determining whether to switch to described second operator scheme, activate the one or more microphones and the speech analysis subsystem that are associated with described mobile computing device, so that described mobile computing device receives audio data stream.This system can also comprise the output subsystem of described mobile computing device, being configured to provides the output of importing in response to speech at described mobile computing device, and described speech input is detected in the described audio data stream and indicates the request that is used for executable operations.
In another realization, a kind of system for monitoring speech input automatically comprises mobile computing device and one or more microphone, and described one or more microphones are configured to the reception environment sound signal and provide the electronic audio frequency data to described mobile computing device.This system can also comprise the context determining unit, be configured to detect the current context that is associated with described mobile computing device, described context is in described mobile device outside and indicate the current state of described equipment in its surrounding environment, and for the device that determines whether described mobile computing device is switched to from current operator scheme second operator scheme based on described current context, described mobile computing device is at speech Input Monitor Connector ambient sound during described second operator scheme, and described speech input indication is used for the request of executable operations.This system may further include the input subsystem of described mobile computing device, be configured in response to determining whether to switch to described second operator scheme, activate the one or more microphones and the speech analysis subsystem that are associated with described mobile computing device, so that described mobile computing device receives audio data stream.This system can also comprise the output subsystem of described mobile computing device, being configured to provides the output of importing in response to speech at described mobile computing device, and described speech input is detected in the described audio data stream and indicates the request that is used for executable operations.
The details of one or more embodiments is set forth in the following drawings with in describing.Various advantages can be achieved by some implementation, such as provide better convenience to the user when the speech input is provided to computing equipment.When he needed the speech input, the user can provide the speech input simply, rather than at first must receive the speech input through form step prompting mobile computing device.In addition, mobile computing device can infer when the user expects to provide the speech input and monitoring speech input during those time periods.Consider that the input of monitoring speech can be so that this equipment of mobile computing device consumption rate be in the standby mode more power, this feature can help to preserve the energy that is moved computing equipment consumption, especially using compact power when (such as, battery) at mobile computing device.
At accompanying drawing with set forth the details of one or more embodiment in hereinafter describing.By description and accompanying drawing and the easy understanding of claims other features of the present invention, purpose and advantage.
Description of drawings
Figure 1A to Fig. 1 C is the concept map for the example mobile computing device of monitoring the speech input based on context automatically.
Fig. 2 A to Fig. 2 B is the diagram for the example system of monitoring the speech input based on the current context that is associated with mobile computing device automatically.
Fig. 3 A to Fig. 3 C is the process flow diagram for the example technique of monitoring the speech input based on the context of mobile computing device automatically.
Fig. 4 can be used for the concept map of system of technology, system, mechanism and the method implementing to describe in this document.
Fig. 5 as client or as one or more server, can be used for the block diagram of computing equipment of the system and method implementing to describe in this document.
Similar reference symbol indication similar components among each figure.
Embodiment
This document has been described and to mobile computing device (for example has been used for monitoring automatically, mobile phone, smart phone are (for example, IPHONE, BLACKBERRY), technology, method, system and the mechanism of speech/language input of PDA(Personal Digital Assistant), portable electronic device (for example, IPOD) etc.).About when begin and finish the input of monitoring speech determine can be based on the context that is associated with mobile computing device (and/or user of mobile computing device).For example, when the context that is associated with mobile computing device (and/or user of mobile computing device) indication user expectation provides the speech input and/or provides based on the feature of speech will be convenient to the user time, this mobile computing device can be monitored the speech input automatically.
Along with mobile computing device becomes more powerful, the number of the speech correlated characteristic that is provided by mobile computing device has also increased.For example, the user can adopt voice commands to instruct mobile computing device telephone calling (for example, " calling out Bob ") and play music (for example, " playing the music of Beck ").Yet mobile computing device only has been configured to monitor this type of speech input being Shi Caihui like this by user prompt.For example, the user may need to press the button on the mobile computing device or activate voice characteristics on the application-specific and is used for mobile computing device and receives and handle this type of speech input.
The technology of describing in this document, method, system and mechanism allow the user that the speech input is provided and do not need the additional formality of using the speech input to be associated with the prompting mobile computing device.On the contrary, mobile computing device can determine when based on the current context that is associated with this mobile computing device (and/or user of this mobile computing device) and begin to monitor speech input, and need be when not determining not clear and definite user guided.Current context at mobile computing device can comprise the various information that are associated with the user of this mobile computing device and/or this mobile computing device.This type of information can be in device external and by the sensor identification in the equipment, such as current physical location (for example, family, unit, automobile, be positioned near the wireless network " testnet2010 " etc.), the direction that moves of equipment and speed (for example, with 20 mph.s north row), current geographic position (for example, the turning in the tenth street and Marquette main road), the mobile computing device harbour type of parking (for example, the adaptive harbour of automobile), neighbourhood noise (for example, overcast buzz, music etc.) and from the present image of mobile computing device camera.
Context can be at device interior, such as equipment about time and date (for example, afternoon on the 29th July in 2010 2:00), (for example date schedule at hand and/or in the recent period, July in 2010, afternoon on the 29th, 2:30 met with John), recent device activity (for example, sending Email about the 2:30 meeting to John) and do not reflect determining that the history image of current state carries out around this equipment from the mobile computing device camera.
For example, the two-forty that mobile computing device can move based on this equipment that detects (for example, use is as any sensor in the various motion sensors of this equipment standard assembly) and/or determine its current movement in automobile based on the mobile device harbour (for example, detecting the pin arrangement that physical electronic is connected between mobile computing device and the harbour) that this equipment just is being anchored in adaptive automobile.Mobile computing device can determine whether to monitor the speech input based on this current context.
The whole bag of tricks can be used for determining which context allows speech Input Monitor Connector and which context not to allow.For example, mobile computing device can attempt inferring whether current context is indicated this user to have at least the threshold value possibility of speech input is provided, and then monitors the speech input if so in response.In another example, mobile computing device can be attempted inferring based on current context whether the input of monitoring speech can provide the convenience of threshold level at least to the user, and then monitor the speech input if so.In another example, the context of sign and/or user ID can be used for determining when the input of monitoring speech in advance.Can also use be used to the other technologies that determine when the input of monitoring speech.
Car context example is above expanded, be arranged in automobile based on definite mobile computing device, mobile computing device can infer that the user can provide speech input very convenient (and safety).Based on about contextual deduction of determine, mobile computing device can begin to monitor and the speech input of process user.Mobile computing device can continue to monitor the speech input to be taken place up to various End Events, current context such as mobile computing device (for example changes, the user removes mobile computing device from automobile), the user indicates its expectation speech Input Monitor Connector (for example to finish, the user is provided for providing the speech input such as the indication of " stopping to monitor the speech input "), the battery of mobile computing device is about to use up electric weight (for example, battery dump energy is below 25%) of storing etc.
The input of monitoring speech can relate to the speech input from determining then whether this speech input can be applicable to this mobile computing device by other neighbourhood noises (for example, background music, car horn etc.) separation of mobile computing device reception.For example, when talking before two users are monitoring the mobile computing device of speech input, this mobile computing device can determine which speech input is that user's part of talking and which are request mobile computing device executable operations.Various technology can be used for carrying out this type of to be determined, such as monitoring special key words (for example, " search ", " mobile device " etc.), inspection grammer (for example, identified problems, marking command etc.) etc.
As hereinafter further in detail as described in, mobile computing device is can be at this mobile computing device local and/or in conjunction with away from the computer system monitoring of this mobile computing device and handle the speech input.For example, mobile computing device can be determined its current context, determine whether to monitor the speech input, sign is used for the speech input of mobile computing device, and make the order that is associated with speech input as specific installation (for example, do not need by network and other equipment mutual) and/or by carrying out alternately with remote server system.
Figure 1A to Fig. 1 C is for the example mobile computing device 102a-b that monitors speech input based on context automatically, 142 and concept Figure 100,140 and 160 of 162a-d.With reference to Figure 1A, diagram 100 has been described the example of monitoring speech input, and wherein mobile computing device 102a-b (being intended to refer to identical calculations equipment) is in two different contexts (context A 104 and context B 106).
In context A 104, mobile computing device 102a is depicted as and is just held by user's hand 108 and do not have physical connection or be to other equipment or line.Mobile computing device 102a is depicted as in this example and uses portable power source (for example, battery) to operate.
In context B 106, mobile computing device 102b is depicted as and just is anchored in the mobile device harbour 110 that comprises loudspeaker 112 and microphone 114 and 116.Mobile computing device 102b is depicted as with the mobile device interface 118 electronics physics of harbour 110 and contacts.Mobile computing device 102b and harbour 110 can communicate by this electronics physical connection.For example, mobile device 102b can be by being transferred to harbour 110 with being connected of interface 118 with audio data stream, and it can be so that harbour 110 uses loudspeaker 112 to play music.Similarly, harbour 110 can provide by loudspeaker 114 and 116 and the voice data that receives of interface 118 to mobile device 102b.
Further at context B 106, harbour 110 is depicted as from inserting power lead 120 received powers of supply socket 122.Mobile computing device 102b can be by harbour 110 interface 118 from external power source (for example, directly from harbour 110, indirectly from power lead 122 etc.) received power.
Based on context 104 and 106, mobile computing is established, and 102a-b determines whether autonomous ground (not needing at first how to be done by user prompt or indication) monitoring speech input.At context A104, mobile computing device 102a uses compact power (battery) rather than external power source to determine not monitor the speech input based on equipment at least.For compact power, the power supply supply is limited.Yet the input of monitoring speech can expend more power than the normal standby operation of mobile computing device 102a, and may continue uncertain time quantum.Therefore, in context A 104, mobile computing device 102a can determine that any potential convenience of monitoring the speech input for the user is surpassed by the non-convenience of potential consumes battery in the relative short time (short when comparing with standby operation) to mobile computing device 102a.In addition, mobile computing device 102a can determine customer-furnished any speech input based on the mobile computing device 102a that depends on its own microphone (different with external microphone, as microphone 114 and 116) can't the reception of sufficiently clear ground with accurate processing.Therefore, the mobile computing device 102a among the context A 104 does not monitor the speech input, shown in symbol 124.
In contrast, with reference to context B 106, mobile computing device 102b determines monitoring speech input (as not existing shown in the symbol of symbol 124 among the context A 104) based on the mobile computing device 102b that just is connected to harbour 110.As indicated above, mobile computing device 102b can be designated harbour 110 harbour of particular type based on the pin arrangement of using in the interface 118.By being connected with harbour 110, mobile computing device 102b receives the benefit of external power source (for example, harbour 110, socket 122) and external microphone 114 and 116.In this example, mobile computing device 102b can be based on the combination in any monitoring speech input of the availability of the availability that connects harbour 110, harbour type (for example, home stereo harbour) that mobile computing device 102b connects, external power source and external microphone 114 and 116.As the part of monitoring speech input, mobile computing device 102b can receive audio data stream from microphone 114 and 116, therefrom is used for the input of sign (and processing) speech.Equally, by restriction monitoring specific context B, system can assist in ensuring that the user recognizes by system monitoring (when it takes place).
Equipment 102b can also announce when it switches to monitoring pattern.For example, when equipment was parked, the loudspeaker on the harbour can be announced " equipment monitor now request-please say stop monitoring coming disablement feature ".This type of declaration can provide monitoring occurent additional notifications to the user, so that the advantage that the user can obtain to monitor is kept the control to the monitoring content simultaneously.
The Alice 126 that describes and the talk between the Bob 128 have been demonstrated the speech Input Monitor Connector of being carried out by mobile computing device 102a-b.Alice says that to Bob " hello, Bob.How are you getting along recently? " (130).Alice answers " well.You? " (132).Alice answers " good.You know the weather forecast of this weekend? " (134), and Bob say and " do not know.Wait, I look into down mobile device.What the weather forecast of this weekend is? " (136).
Shown in symbol 124, based on determining that the talk 130-136 that does not monitor between speech input Alice 126 and the Bob 128 is not received by the mobile computing device 102a among the context A 104.
On the contrary, the talk 130-136 between Alice 126 and the Bob 128 is received the part as the interface 118 that is used harbour 110 by mobile computing device 102b and microphone 114 and 116 audio data streams that receive.Mobile computing device 102b can use the speech analysis subsystem to detect speech input 130-136 from other neighbourhood noises (such as, background music), and whether any input among the sign speech input 130-136 is the request to mobile computing device 102b.
As described in previously, whether mobile computing device 102b can use various technology to identify any input among the speech input 130-136 is request to mobile computing device 102b.For example, mobile computing device 102b can for example, order the word " search " that uses in " near the restaurant search " at keyword scan speech input 130-136, and problem " mobile device, how many current scores of Basketball Match is? " the middle word " mobile device " that uses.In another example, mobile computing device 102b can monitor the grammer of speech input 130-136 and attempt identifying a plurality of parts that can be used for mobile computing device 102b in the voice, such as problem and order.In another example, mobile computing device 102b can based on the change in the speech input structure (such as, (for example pause, the user waits for the response from mobile computing device 102b)), change on the obvious direction of sound signal (for example, the user is towards mobile computing device 102b when order is provided), the change of delivery rate (for example, user's slow-down when being used for mobile computing device 102b), the change of tone and tone (for example, when handling mobile computing device 102b, the user lowers the tone and reduces the tone level) wait and reveal that the input of some speech is used to mobile computing device 102b.Can also use the combination of other technologies and technology.
In this example, there are a plurality of problems among the speech input 130-136 between Alice 126 and the Bob 128, but have only the problem in the speech input 136 to be used for mobile computing device 102b.Use the combination in any of technology described in the first previous paragraphs, mobile computing device 102b can be with these speech input 136 requests of correctly isolating as mobile computing device 102b executable operations.For example, mobile computing device 102b can be from the input 136 of the speech of Bob identified phrases " mobile device ", the grammer of analyzing speech input 136 then with problem " what the weather forecast of this weekend is? " isolate and be used for mobile computing device 102b.
In response to carrying out this type of sign, mobile computing device 102b can initiate to search for for the weather forecast of determining at mobile computing device 102b current geographic position at upcoming weekend.Mobile computing device 102b can be local (for example, inquiry mobile computing device 102b goes up the weather application of periodically obtaining and storing weather forecast) and/or by identifying this information alternately via network (for example, the Internet, cellular network, 3G/4G network etc.) and remote information server system.
Mobile computing device 102b can use any apparatus in the various available output devices to provide the Weather information of asking to Alice 126 and Bob 128, described output device such as display (for example, the display of mobile computing device 102b, computer monitor, TV etc.), speaker system (for example, the loudspeaker 112 of the internal loudspeaker on the mobile computing device 102b, harbour 110 etc.), projector projector of the part of mobile computing device 102b and/or harbour 110 (for example, as) etc.In this example, mobile computing device 102b uses Text To Speech (TTS) subsystem of mobile computing device 118 and the loudspeaker 112 of harbour 110 acoustically to export Weather information (138).
With reference to Figure 1B, diagram 140 has been described to determine whether to monitor speech input, sign from user's request of speech input and the example that the mobile computing device 142 of the output of asking in response to this user is provided.
In steps A, mobile computing device 142 detects mobile computing devices 142 and the current context (144) of user's (not shown) of being associated with this mobile computing device.As shown in example current context 146, mobile computing device 142 is currently located at user's family (148a), current date and time is the afternoon of Monday 7:00 (148b), the schedule of user excess time Monday is appointment (148c) not, and the battery of mobile computing device 142 current uses 90% chargings is as its power supply (148d).The current location of mobile computing device 142 can be determined by variety of way, such as (for example using geographical location information, geo-positioning system (GPS) information), computing equipment and/or wireless network are (for example around the sign, detect the wireless network that occurs in the user family), mobile computing device 142 is sitting at the harbour (for example, harbour 110) of particular type etc.
At step B, mobile computing device 142 determines whether to ask to monitor sound signal (150) at the user based on this equipment 142 and user's thereof current context 146.Described with reference to Figure 1A as mentioned, various technology can be for the speech input that determines whether to monitor from the user.In this example, mobile computing device 142 is based on inferring that the user will both provide the user to ask to determine to continue to ask the monitoring of environmental sound signal at the user with the possibility (shown in context 146) of convenience to this user and mobile computing device 142.The possibility that provides the user to ask can be inferred from time (point in afternoons 7) and user's schedule at least.Though be at night, that the user may also not have is sleeping (only afternoons 7 point) and this user provides threshold value possibility based on voice request without any appointment-user next expecting in some hours can to indicate at least free time to mobile computing device 142 in excess time at this night.At least be sitting in the user family based on mobile computing device 142, the input of monitoring speech can make things convenient for the user, wherein the user can with mobile computing device 142 greater than an arm lengths (for example, the user is mobile around can be in the house says his/her request simply rather than needs location mobile computing device 142 at each request manual prompt computing equipment 142 so that can more convenient user).In addition, have the threshold value electric charge at least and will only continue the supposition of (for example, mobile computing device 142 can predictive user may fall asleep) of finite time section based on monitoring after some hours based on battery at least, monitor speech input and can make things convenient for mobile computing device.
In response to determining the monitoring sound signal, at step C, mobile computing device can activate microphone and the speech analysis subsystem (152) that can be used for mobile computing device.Microphone and/or speech analysis subsystem can be positioned at and/or away from mobile computing device 142.For example, the microphone that uses of mobile computing device 142 can be embodied in the mobile computing device and/or away from mobile computing device (for example, the microphone 114 and 116 of harbour 110).In another example, the long-range realization of speech analysis subsystem, mobile computing device 142 can provide the sound signal of reception to the remote speech analyzing subsystem, and in response, receives the information that indicates whether to detect any speech input.
At step D, at user's request, mobile computing device 142 continues to receive and monitoring of environmental sound signal (154).For example, TV 156a, people 156b and pet 156c can produce the sound signal 158a-c that is received and checked by mobile computing device 142 respectively.
In the middle of all these sound signals, user 156b to mobile computing device 142 asks a question " provincial capital of the Maine State is? " (158b) ask as the user.Mobile computing device 142 (may in conjunction with the remote speech analyzing subsystem) can use as mentioned and detect this user's request with reference to any technology in the described various technology of Figure 1A from sound signal 158a-c.Mobile computing device 142 then can local (for example, searching for local canned data database) or by handling this user's request alternately with the remote information server system.
Obtained institute's identifying user request responding, mobile computing device can provide the output (162) at user's request shown in step F.In this example, the answer 164 of mobile computing device explicit user problem on the display of this mobile computing device 142.Described with reference to Figure 1A as mentioned, can also be other modes that this type of output is provided to mobile computing device 142.
With reference to figure 1C, diagram 170 has described to use the example of mobile computing device 172a-d (being intended to as the single mobile computing device of describing in the variant context) monitoring speech input in four different contexts (context A 174, context B 176, context C 178 and context D 180).
With reference to context A 174, mobile computing device 172a is depicted as and is sitting at user's office 182.In this example, mobile computing device 172a can be based on the wireless network " workwifi " that is associated with office 182 184 its current locations of sign that occur.Shown in symbol 186, mobile computing device 172a determines not in user's office 182 monitoring speech inputs based on context A 174.Should determine can be based on above referring to figs. 1A to any factor in the described various factors of Figure 1B.
With reference to context B 176, mobile computing device 172b is depicted as and is sitting in user's automobile 188.In this example, mobile computing device 172b can be at least based on park with automobile is adaptive/charge cable 190 is connected and determines its current context.As occur shown in the symbol of symbol 186, mobile computing device 172b determines user's request that monitoring is carried out in user's automobile 188 inside based on context B 176.Should determine can be based on above referring to figs. 1A to any factor in the described various factors of Figure 1B.
Context C 178 has described mobile computing device 172c and has been sitting in user's the family 192.Mobile computing device 172 can just place mobile device harbour 194 to determine its current context based on the wireless network " homenet " 193 that is associated with user family 192 that occurs and equipment 172c at least.As indicated previously, mobile device 172 can distinguish based on various factors (such as, different pin arrangement) park with automobile is adaptive/it still is to be connected with mobile device harbour 194 that charge cable 190 is connected.As not existing shown in the symbol of symbol 186, mobile computing device 172c determines user's request that monitoring is carried out in user family 192 inside based on context C 178.Should determine can be based on above referring to figs. 1A to any factor in the described various factors of Figure 1B.
Mobile computing device 172 can monitored the speech input and not switch between the monitor user ' request along with the context changes of this mobile computing device 172.For example, when the user had been with mobile computing device 172 to leave office 182 to enter automobile 188, never monitor user ' request of mobile computing device 172 (in office 182) switched to monitor user ' request (in automobile 188).
Wherein the context of mobile computing device 172 monitor user ' requests can be different between equipment and/or the user who is associated, and can change in time.Feedback control loop can be used for the wherein context of mobile computing device 172 monitoring speech inputs of continuous refinement.For example, do not have upwards the computing equipment 172 among the C 178 hereinafter that a plurality of requests based on speech are provided if the user is overtime, then mobile computing device 172 can stop at monitoring speech input among the context C 178.On the contrary, if the user continually manual prompt computing equipment 172 in context A 174, receive speech input, then mobile computing device 172 can begin monitoring speech input in context A 174.
Fig. 2 A to Fig. 2 B is the diagram for the example system 200 of monitoring the speech input based on the current context that is associated with mobile computing device 202 automatically.In this example, the current context that mobile computing device 202 is configured to be associated based on the user with mobile computing device and/or this mobile computing device determines when beginning automatically and when finishes to monitor the speech input, is similar to above referring to figs. 1A to the described mobile computing device 102 of Fig. 1 C, 142 and 172.
The input resolver 210 of mobile computing device 202 can be configured to receive input (such as the electronic audio frequency data) from input subsystem 204, and determines whether the voice data that receives comprises the speech input.Input resolver 210 can comprise speech analysis subsystem 212.This speech analysis subsystem 212 can analyze and determine whether have any speech input in the voice data that microphone 206a receives when the monitor user ' request.Input resolver 210 can comprise other modules of not describing for explaining the user's input that receives by input subsystem 204, such as the computer vision module, be used for explaining the image that obtains by camera 206f, and the gesture module, be used for explaining that the physics that is provided by accelerometer 206d moves data.
Mobile device context determining unit 214 can be determined the current context of mobile computing device 202.This mobile device context determining unit 214 can be used the current context that is received and determined by the various context monitoring means of the input of importing resolver 210 explanations and mobile computing device 202 mobile device 202 by input subsystem 204.
For example, GPS (GPS) unit 216 can provide geographical location information to mobile device context determining unit 214, and power/connection management unit 217 about the information of the current power supply of mobile computing device and/or power rating (for example can provide, be connected to external power source, battery charge 80% etc.), and about the charging of mobile computing device 202 and/or the information that communicates to connect (for example, equipment is parked, equipment be connected to wireless network etc.).Mobile monitoring unit 218 (in conjunction with mobile data warehouse 220) can provide the custom route that moves with the current route that is moving and mobile computing device 202 relevant information.Movement monitoring unit 222 (combining movement data warehouse 224) can provide with mobile device 202 on the nearest information relevant with custom User Activity (customizing messages of for example, the application of use, repeatedly visit etc.).Position monitoring unit 226 can provide the information about mobile computing device 202 current physical locations (for example, family, unit, automobile are medium).Current physical location can be determined by use location data warehouse 227 in position monitoring unit 226.Position data warehouse 227 can be associated the information that detects surrounding (for example, available wireless network, ambient sound, near computing equipment etc.) about mobile computing device 202 with physical location.Position monitoring unit 226 can also identify near the entity (for example, enterprise, park, festival celebration place, public transport etc.) that is physically located at this mobile device 202.
Time and date unit 228 can provide current time and date and time information, and calendar cells 230 (in conjunction with calendar data warehouse 232) can provide the relevant information of dating with the user.Email unit 234 (in conjunction with e-mail data warehouse 236) can be by the relevant information of Email (for example, the Email of nearest sending/receiving).Other context monitoring means that mobile context determining unit 214 can never be mentioned or describe receive information.
In some implementations, context monitoring means 216-236 can a part or whole part realize away from mobile computing device 202.For example, Email unit 234 can be safeguarded and the thin-client of the Email related data that provides for only showing by remote server system.In this example, Email unit 234 can obtain to be used for the email-related information that provides to mobile device context determining unit 214 with remote server system is mutual.
Except using current context, mode selecting unit 238 can also be determined to begin or stops to ask to monitor voice data at the user based on the user behavior data that is associated with the voice data monitoring of storage in the user behavior data warehouse 242.User behavior data warehouse 242 can record preceding mode select, when carrying out model selection mobile device 202 context and at the follow-up behavior of selected mode user (for example, the user has during the audio frequency monitoring pattern or does not provide request, user's manual switchover to different operator schemes, user's manual prompt equipment receives and processing speech input etc. when being in non-monitoring pattern by the speech input).It is useful and/or convenient to the user whether the user behavior data of storing in the user behavior data warehouse 242 can indicate the pattern of selecting based on the context of equipment 202 correctly to be inferred.Above with reference to figure 1C the example of using the user behavior data select operating mode has been described.
About model selection, mode selecting unit 238 can be notified input subsystem 204 and input resolver 210 at least.For example, be switched to the audio frequency monitoring pattern in response to notice mobile computing device 202, input subsystem 204 can activate microphone 206a and begin to receive voice data, and input resolver 210 can be handled the voice data that is provided by microphone 206a by voice activated analyzing subsystem.In another example, be switched to non-monitoring operator scheme in response to notice mobile computing device 202, input subsystem 204 can be separated and activate microphone 206a, and input resolver 210 can be separated voice activated analyzing subsystem.
When microphone 206a and speech analysis subsystem 212 at least are activated during audio frequency monitoring operator scheme and speech analysis subsystem 212 provides from microphone 206a and input subsystem 204 audio data stream detects the speech input, user's request mark device 241 can be notified should sign.User's request mark device 241 can determine the speech that detects input whether indicate the user to mobile computing device request executable operations (for example, search information, play media file, traffic route etc. is provided).User's request mark device 241 can use various subsystems are auxiliary determines whether the particular voice input indicates the user to ask, such as key word concentrator marker 242a, grammer module 242b and speech structure analysis module 242c.
Key word concentrator marker 242a can determine whether this particular voice input is used for mobile computing device 202 based on the key word in the predetermined key word grouping that has storage in the key word warehouse 243 in the particular voice input.For example, the user to be used in reference to the title (for example, " mobile device ") for mobile computing device 202 can be the key word in the key word warehouse 243.In another example, the computing equipment 202 frequent orders of handling (such as " search " (as in " search local news ") and " broadcast " (as in " playing the song of Beatles ")) can be moved and key word warehouse 243 can be included in.Key word in the key word warehouse 243 can be determined in advance and/or the user limits, and key word can change in time.For example, feedback control loop can be used for determine based on user's request mark of key word whether correct (for example, does the user expect that the speech input is identified as user's request?).This feedback control loop can use the deduction that obtains from user action subsequently to determine whether key word should be added into key word warehouse 243 or remove from key word warehouse 243.For example, if the user to the Search Results that provides in response to word " search " in the identifying user voice frequently provide eccentric response (such as, " Kazakhstan? " " what that is? "), then word " search " can remove from key word warehouse 243.
Be similar to grammer that use above provides at Figure 1A and the argumentation of speech input structure, grammer module 242b can analyze the grammer of speech input, and speech structure analysis module 242c can analyze the speech input structure to determine whether this speech input may be used for mobile computing device 202.Be similar to key word concentrator marker 242a, grammer module 242b and/or speech structure analysis module 242c can use feedback control loop in time the sign of speech input to be refined as user's request.
Use user's request of user's request mark device 241 signs, input processing unit 244 can the process user request.In some implementations, input processing unit 244 can be transmitted user's request (for example, being provided for playing user's request of music to music player application) to the application that is associated with user's input and/or service.In some implementations, input processing unit 244 can be so that the one or more operations that are associated with user's request be performed.For example, input processing unit 244 can be communicated by letter with remote server system, and this remote server system is configured to carry out at least the part of the operation that is associated with user's input.
Described at Figure 1A to Fig. 1 C as mentioned, determine with context, that model selection, speech input sign, user's request mark and/or user ask to handle the operation that is associated is can be at mobile computing device 202 local and/or carry out away from mobile computing device 202.For example, in calendar application in the local realizations that realize of mobile computing device 202, user's request at calendar information can be in mobile computing device 202 local execution (the relevant calendar information inquiry calendar cells 230 of storage in for example, at calendar data warehouse 232).In another example, at the calendar data of calendar application in the realization that remote server system provides, mobile computing device 202 can with remote server system alternately to visit relevant calendar information.
The output subsystem 246 of mobile computing device 202 can provide the output that is obtained by input processing unit 244 to the user of equipment 202.Output subsystem 246 can comprise various output devices, such as display 248a (for example, LCD (LCD), touch-screen), projector 248b (for example, can to the image projector of equipment 202 outside projects images), loudspeaker 248c, earphone jack 248d etc.Network interface 208 can also and can be configured to provide the result who is obtained by input processing unit 244 (for example, to BLUETOOTH headphone transmission result) as the part of output subsystem 246.Output subsystem 246 can also comprise Text To Speech (TTS) module 248e, and being configured to text-converted is become can be by the voice data of loudspeaker 248c output.For example, TTS module 248e can convert the text based output of input processing unit 244 process user requests generation to the audio frequency output that can play to the user of mobile computing device 202.
With reference to figure 2A, mobile computing device 202 can with wireless launcher 250 (for example, cellular network transceiver, wireless network router etc.) radio communication, and obtain the visit to network 252 (for example, the Internet, PSTN, cellular network, Local Area Network, VPN(Virtual Private Network) etc.).By network 252, mobile computing device 202 can be communicated by letter with mobility device server system 254 (server computers of one or more networkings), it can be configured to provide mobile device related service and data (for example, provide calendar data, e-mail data, call is connected to other phones etc.) to mobile device 202.
Fig. 3 A to Fig. 3 C is for the example technique 300 of monitoring the speech input based on the context of mobile computing device automatically, 330 and 350 process flow diagram.Example technique 300,330 and 350 can be carried out by any mobile computing device in the various mobile computing devices, all mobile computing devices of describing at Figure 1A to Fig. 1 C as mentioned 102,142 and 172 and/or the mobile computing device 202 above described at Fig. 2 A to Fig. 2 B.
With reference to figure 3A, example technique 300 is used for monitoring the speech input automatically based on the context of mobile computing device generally.Technology 300 by detect and current context that mobile computing device (and/or the user who is associated with mobile computing device) is associated from step 302.For example, mobile device context determining unit 214 can based on as at the described various context-related information of Fig. 2 B source (such as, input subsystem 204 and context monitoring means 216-236) detect the current context be associated with the user of mobile computing device 202 and/or mobile computing device 202.
Can determine whether to switch to second operator scheme (304) from current operator scheme based on current context.For example, the mode selecting unit 238 of mobile computing device 202 can be based on being determined that by mobile device context determining unit 214 current context determines whether to begin to monitor speech input (switching to second operator scheme from current operator scheme).
In response to determining whether that switching to second operator scheme can activate one or more microphones and/or speech analysis subsystem (306).For example, in response to determining to begin to monitor the speech input, mode selecting unit 238 can indicate input subsystem 204 and input resolver 210 to activate microphone 206a and speech analysis subsystem 212.
The audio data stream that continuing monitoring provides from the activation microphone can be monitoring speech input (308).For example, speech analysis subsystem 212 can be monitored by activating audio data stream that microphone provides to detect speech input in other sound from be included in this stream and the noise.
Can determine whether the speech input that detects indicates the request (310) for executable operations during continuing monitoring.For example, user's request mark device 241 can check by the speech input of speech analysis subsystem 212 signs and determine whether the speech input indicates the user to ask mobile computing device 202 executable operations.
Indicated by the input of detection speech in response to definite user's request, can be so that institute's solicit operation of being asked to indicate by the user be performed (312).For example, user's request mark device 241 can indicate input processing unit 244 execution users to ask the operation of indicating.In some implementations, input processing unit 244 can be in mobile computing device 202 local executable operations (for example, visit local data, service and/or application be with executable operations).In some implementations, input processing unit 244 can with mobility device server system 254 and/or information server system 256 alternately to carry out institute's solicit operation.
The output of asking in response to the user by detection speech input indication can be provided (314).For example, output subsystem 246 can use the one or more assemblies among the assembly 248a-e of subsystem 246 that output is provided based on the performance of institute's solicit operation.
Change for the current context of mobile computing device (and/or user of mobile computing device) can be detected (316).For example, the event that is generated by input subsystem 204 and/or context monitoring means 216-234 can so that whether 214 assessments of mobile device context determining unit change at the user's of mobile computing device and/or mobile computing device context.
Change in response to (threshold value at least) that detect in the context, can determine whether to switch to the 3rd operator scheme (318) based on the context that changes.For example, mode selecting unit 238 can check that the context of mobile computing device 202 changes is to determine whether to stop to monitor speech input (switching to the 3rd operator scheme).
Based on determining to switch to the 3rd operator scheme, can separate and activate one or more microphones and/or speech analysis subsystem (320).For example, determining to stop to monitor speech input (switching to the 3rd operator scheme) afterwards, mode selecting unit 238 can be indicated input subsystem 204 and input resolver 210 to separate respectively and be activated microphone 206a and speech analysis subsystem 212.
With reference to figure 3B, example technique 330 is used for determining whether to begin to monitor speech input (switching to second operator scheme from current operator scheme) based on the current context of mobile computing device generally.Example technique 330 can be performed as above at the part of the described technology 300 of Fig. 3 A.For example, technology 330 can be carried out in the step 304 of technology 300.
Based on various factors (such as, be designated the user behavior data relevant with current context) can determine the threshold value possibility (334) that whether user has at least provides speech input.For example, if mobile computing device 202 begins to monitor the speech input, then mode selecting unit 238 can determine whether the user may provide the speech input based on various factors (such as, the previous user action of before carrying out in response to speech monitoring (user behavior data)) in similar context.If exist the user that the threshold value possibility of speech input is provided at least, then mode selecting unit 238 can begin to monitor the speech input.
Can determine to monitor the speech input and whether will have convenience (336) at user and mobile computing device threshold level at least.For example, be similar to above described at the B 150 of step shown in Figure 1B, mode selecting unit 238 can check whether the input of monitoring speech will make things convenient for user and/or the input of monitoring speech of mobile computing device 202 whether will make things convenient for mobile computing device 202 (for example, checking whether mobile computing device 202 has enough power supply supplies and be used for continuing the input of monitoring speech).
With reference to figure 3C, whether the speech input that example technique 350 detects when be used for determining the monitoring voice data generally is the user's request for executable operations.Example technique 350 can be performed as above at the part of the described technology 300 of Fig. 3 A.For example, technology 350 can be carried out in the step 310 of technology 300.
Whether technology 350 can be present in the speech input that detects from step 352 by identifying one or more key words in the grouping of predetermined key word.For example, the key word concentrator marker 242a of user's request mark device 241 can check whether one or more key words of storage in the key data warehouse 243 are present in the speech input that speech analysis subsystem 212 detects when continuing the input of monitoring speech.
Can determine that based on the grammer of speech input this speech input is order or problem (354).For example, grammer module 242b can determine that the grammer of speech analysis subsystem 212 speech that detects input is order or the problem that the indication user is used for mobile computing device 202.
Can identified (356) with change in the structure that speech input is associated, and based on the change that identifies, can determine whether this speech input is used for mobile computing device (358).For example, the speech structure analysis module 242c of user's request mark device 241 can determine whether the structure of being imported by the speech that speech analysis subsystem 212 detects changes in the mode of indication speech input for mobile computing device 202.
Fig. 4 can be used for the concept map of system of technology, system, mechanism and the method implementing to describe in this document.Mobile computing device 410 can with base station 440 radio communications, this base station can provide visit to a plurality of services 460 to mobile computing device by network 450.
In this illustration, mobile computing device 410 is depicted as hand held mobile phone (for example, smart phone or application phone), this hand held mobile phone comprises for the touch-screen display device 412 to user's rendering content of mobile computing device 410.Mobile computing device 410 comprises for the various input equipments (for example, keyboard 414 and touch-screen display device 412) that receive the influential user's input of the operation of mobile computing device 410.In more implementations, mobile computing device 410 can be laptop computer, flat computer, personal digital assistant, embedded system (for example, auto-navigation system), desk-top computer or computerized work station.
Example sense of touch output mechanism is the small electrical motor, and it is connected to unbalance weight so that vibrating alert (for example, vibration is in order to import call into or confirm that the user contacts with touch-screen 412 to user reminding) to be provided.In addition, mobile computing device 410 can comprise one or more loudspeaker 420 that converts the electrical signal to sound (for example, the music in the call, can listen and remind or individual's voice).
The example mechanism that is used for reception user input comprises keyboard 414, and it can be full qwerty keyboard or the conventional keypad that comprises numeral " 0-9 ", " * " and " # ".Keyboard 414 receives input when the user physically contacts or presses keyboard key.The user control trace ball 416 or with track pad allow alternately the user can to mobile computing device 410 suppliers to speed of rotation information (for example, with the position of controlling cursor on display device 412).
Operating system can be provided in the hardware (for example, the processor of I/O mechanism and the execution instruction of fetching from computer-readable medium) of mobile computing device and the interface between the software.The exemplary operations system comprises ANDROID mobile computing device platform; APPLE IPHONE/MAC OS X operating system; MICROSOFT WINDOWS 7/WINDOWS MOBILE operating system; SYMBIAN operating system; RIM BLACKBERRY operating system; PALM WEB operating system; The operating system of multiple support UNIX; The private operations system that perhaps is used for computerized equipment.Operating system can be provided for the platform of executive utility, and these application programs help mutual between computing equipment and user.
The graphical interfaces element can be text, lines, shape, figure or its combination.For example, the graphical interfaces element can be to be shown in icon on the desktop and the text that is associated of icon.In some instances, available subscribers input is selected the graphical interfaces element.For example, the user can select the graphical interfaces element by the corresponding zone of the demonstration with the graphical interfaces element of pressing touch-screen.In some instances, the user can control trace ball with the single graphical interfaces element of eye-catching demonstration for having focusing.The user selects the graphical interfaces element can call the predefine action of mobile computing device.In some instances, optional graphical interfaces element also corresponding to or alternatively corresponding to the button on the keyboard 404.The user selects button can call the predefine action.
In some instances, operating system be provided at when opening mobile computing device 410, when activating mobile computing device 410 from sleep state, at " release " mobile computing device 410 time or " desktop " user interface that when receiving the user to the selection of " home " button 418c, shows.The desktop graphical interfaces calls some icons of corresponding application program when may be displayed on and import to select with the user.The application program of calling can present the graphical interfaces that replaces the desktop graphical interfaces and stop or be hidden from view until application program.
User's input can be controlled the sequence of operation of mobile computing device 410.For example, the operation that one action user input (for example, singly the striking of touch-screen, inswept touch-screen, contact with button or make up these actions the time) can be called the display change that makes user interface.When not having the user to import, user interface may not change at special time as yet.For example, can call the application of drawing with " amplification " position with many touch user inputs of touch-screen 412, even the application of drawing can acquiescence amplification after some seconds.
The desktop graphical interfaces also can show " accessory ".Accessory is one or more graphical interfaces element that is associated with the application program that has been performed and shows at the desktop contents by executive utility control.Be different from the application program of selecting corresponding icon just can call until the user, the application program of accessory can start from mobile phone.In addition, the accessory focusing that can not show entirely.Alternatively, accessory can only " have " fraction of desktop, thus displaying contents and the input of reception touch screen user in this part of desktop.
The service provider of network of operation base station can be connected to mobile computing device 410 network 450 and serve communicating by letter between other computerized equipments of 460 to be implemented in mobile computing device 410 with providing.Provide service 460 though can pass through heterogeneous networks (for example, service provider's internal network, PSTN and the Internet), network 450 is illustrated as single network.The service provider can operations server system 452, this server system 452 mobile computing device 410 and with service 460 computing equipments that are associated between to information block and speech data route.
Using shop 466 can provide to the user of mobile computing device 410 and browse the user and can download and in the ability of the tabulation of mobile computing device 410 application programs that install, remote storage by network 450.Use the depots that shop 466 can be used as the application of being developed by third party's application developer.Can communicate by letter with the server system of assigning at application program by network 450 in the application program that mobile computing device 410 is installed.For example, thus can download the VoIP application programs and allow the user serve 464 with VoIP to communicate by letter from using shop 466.
Mobile computing device can be communicated by letter with personal computer 470.For example, personal computer 470 can be the home computer for the user of mobile computing device 410.Therefore, the user can use streaming media from his personal computer 470.The user also can check he personal computer 470 file structure and between computerized equipment the transmission selected document.
Speech recognition service 472 can receive the voice communication data with microphone 422 records of mobile computing device, and voice communication is translated into corresponding text data.In some instances, provide the text of translating to inquire about as web to search engine, and to mobile computing device 410 transmission response search engine search results.
The service 480 of drawing can provide street map, route planning information and satellite image to mobile computing device 410.The example plots service is GOOGLE MAPS.The service 480 of drawing also can receive inquiry and the exclusive result of home position.For example, mobile computing device 410 can send the position of estimation of mobile computing devices and the inquiry that is used for " Pizza shop " of user's typing to the service of drawing 480.The service 480 of drawing can be returned street map, and this street map has " mark " that superposes at map, near the geographic position in " Pizza shop " these " marks " sign.
Branch highway section service 482 can provide the branch highway section of the destination of going to user's supply to guide to mobile computing device 410.For example, branch highway section service 482 can be used flow transmission with the data that are used for providing voice command with the stack arrow that the user of equipment 410 guides the destination into to equipment 410 with the street-level diagram of the position of the estimation of equipment.
Microblogging service 486 can receive the user from mobile computing device 410 and import bulletin, and this bulletin is the recipient of sign bulletin not.Microblogging service 486 can be served other members that 486 same purpose user subscribes to microblogging and be scattered bulletin.
Search engine 488 can from mobile computing device 410 receive user's typings text or interview, determine to be provided for showing information for the tabulation of the Search Results of response document in response to the addressable document sets in the Internet of inquiry and to equipment 410.Receive therein in the example of interview, speech recognition service 472 can be translated into the audio frequency that receives the text query that sends to search engine.
These and other services can be implemented in the server system 490.Server system can provide service or the hardware of services set and the combination of software.For example, physically separate and the computerized equipment collection of networking can operate to handle the essential operation for service is provided to hundreds of indivedual computing equipments as the logical server system unit together.
In various implementations, if success of operation formerly (for example, if carry out to determine), then do not carry out " in response to " another operation (for example, determine or sign) and the operation carried out.The feature with the conditional statement description in this document can be described optional implementation.In some instances, comprise that from first equipment to second equipment " transmission " first equipment puts into network with data, but can not comprise second equipment receiving data.On the contrary, can comprise from network reception data from first equipment " reception ", but can not comprise first equipment transmission data.
Fig. 5 is that the system and method that can be used for describing in this document is embodied as the computing equipment 500 of client or one or more server, the block diagram of computing equipment 550.Computing equipment 500 is intended to represent various forms of digital machines, such as laptop computer, desk-top computer, workstation, personal digital assistant, server, blade server, main frame and other suitable computing machines.Computing equipment 550 is intended to represent various forms of mobile devices, such as personal digital assistant, cell phone, the similar computing equipment with other of smart phone.In addition, computing equipment 500 or computing equipment 550 can comprise USB (universal serial bus) (USB) flash drives.The USB flash drives can storage operating system and other application.The USB flash drives can comprise the I/O parts, such as wireless launcher or the USB connector that can insert in the USB port of another computing equipment.Here parts shown in, their connection and relation and their function only to illustrate for example and to be not to describe and/or claimed implementation in order limiting in this document.
Computing equipment 500 comprises processor 502, storer 504, memory device 506, is connected to the high-speed interface 508 of storer 504 and high speed Extended Capabilities Port 510 and is connected to low speed bus 514 and the low-speed interface 512 of memory device 506.Each parts in the parts 502,504,506,508,510 and 512 use various buses to interconnect, and can be assemblied on the common motherboard or otherwise assemble as being fit to.Processor 502 can be handled for the instruction of carrying out in computing equipment 500, and these instructions comprise and are stored in the storer 504 or go up the instruction that shows the graphical information that is used for GUI to be used for externally input-output apparatus (such as the display 516 that is coupled to high-speed interface 508) on the memory device 506.In other implementations, a plurality of processors and/or a plurality of bus can be used with a plurality of storeies and type of memory as being fit to.Also can connect a plurality of computing equipments 500 and each equipment provides the part (for example, as server group, one group of blade server or multicomputer system) of essential operation.
Information in the storer 504 storage computing equipments 500.In an implementation, storer 504 is one or more volatile memory-elements.In another implementation, storer 504 is one or more Nonvolatile memery units.Storer 504 also can be the computer-readable medium of another form, such as disk or CD.
Memory device 506 can be provided for the mass memory of computing equipment 500.In an implementation, memory device 506 can be or comprise computer-readable medium, such as floppy device, hard disc apparatus, compact disk equipment or carrying device, flash memory or other similar solid-state memory device or equipment array, this equipment array comprises the equipment in storage area network or other configurations.Computer program can visibly be embodied in the information carrier.Computer program also can be included in the instruction of carrying out one or more methods (such as above-described method) when being performed.Information carrier is the storer on computing machine or the machine readable media (such as storer 504, memory device 506 or at processor 502).
High-speed controller 508 management are used for the bandwidth-intensive operations of computing equipment 500, and low speed controller 512 is managed more low bandwidth intensive action.It only is example that such function is distributed.In an implementation, high-speed controller 508 is coupled to storer 504, display 516 (for example, by graphic process unit or accelerator) and can accepts the high speed Extended Capabilities Port 510 of various expansion card (not shown).In this implementation, low speed controller 512 is coupled to memory device 506 and low speed Extended Capabilities Port 514.The low speed Extended Capabilities Port that can comprise various communication port (for example, USB, bluetooth, Ethernet, wireless ethernet) can for example be coupled to one or more input-output apparatus (such as keyboard, indicating equipment, scanner) or networked devices (such as switch or router) by network adapter.
As shown in the figure, can be with a plurality of multi-form enforcement computing equipments 500.For example, it may be implemented as standard server 520 or repeatedly is implemented in one group of such server.It also may be implemented as the part of rack server system 524.In addition, it can also be implemented in the personal computer (such as laptop computer 522).Alternatively, the parts from computing equipment 500 can make up with mobile device (not shown) (such as the miscellaneous part in the equipment 550).Each equipment in such equipment can comprise one or more computing equipment in the computing equipment 500,550, and total system can be made up of a plurality of computing equipments 500,550 of intercommunication mutually.
Information in the storer 564 storage computing equipments 550.Storer 564 may be implemented as one or more medium or the unit in one or more computer-readable medium, one or more volatile memory-elements or one or more Nonvolatile memery unit.Extended memory 574 also can be provided and be connected to equipment 550 by expansion interface 572, this expansion interface can for example comprise SIMM (single-row memory module) clamping mouth.Such extended memory 574 can be provided for the additional storage space of equipment 550 or also can store application or other information for equipment 550.Particularly, extended memory 574 can comprise for realizing or replenishing the instruction of above-described process and also can comprise security information.Therefore, for example, can provide extended memory 574 as the security module that is used for equipment, and can be with the instruction that allows safe handling equipment 550 to extended memory 574 programmings.In addition, can also Secure Application be provided with additional information via the SIMM card, such as can not identification information being positioned on the SIMM card hacker's mode.
As hereinafter discussing, storer can for example comprise flash memory and/or NVRAM storer.In an implementation, computer program visibly is embodied in the information carrier.Computer program is included in the instruction of carrying out one or more methods (such as above-described method) when being performed.Information carrier is computing machine or machine readable media, such as storer 564, extended memory 574 or the storer on processor 552 that can for example receive by transceiver 568 or external interface 562.
As shown in FIG., can be with a plurality of multi-form enforcement computing equipments 550.For example, it may be implemented as cell phone 580.It also may be implemented as the part of smart phone 582, personal digital assistant or other similar mobile devices.
The various implementations of system described herein and technology can be implemented in ASIC (special IC), computer hardware, firmware, software and/or its combination of Fundamental Digital Circuit, integrated circuit, particular design.But these various implementations can be included in can carry out on the programmable system and/or one or more computer program of decipher in implementation, this programmable system comprises it can being special or special-purpose at least one programmable processor, at least one input equipment and at least one output device, and this at least one programmable processor is coupled into from storage system and receives data and instruction and to storage system transmission data and instruction.
These computer programs (being also referred to as program, software, software application or code) comprise for the machine instruction of programmable processor and can implement with level process and/or Object-Oriented Programming Language and/or with compilation/machine language.As used herein, term " machine readable media ", " computer-readable medium " (for example refer to any computer program, device and/or equipment that is used for providing machine instruction and/or data to programmable processor, disk, CD, storer, programmable logic device (PLD) (PLD)), this computer program, device and/or equipment comprise that the reception machine instruction is as the machine readable media of machine-readable signal.Term " machine-readable signal " refers to any signal that is used for providing to programmable processor machine instruction and/or data.
For mutual with the user is provided, system described herein and technology can be implemented on the computing machine, this computing machine has for the display device (for example, CRT (cathode-ray tube (CRT)) or LCD (LCD) monitor) that shows information to user and keyboard and user can be used for providing to computing machine keyboard and the indicating equipment (for example mouse or trace ball) of input.The equipment of other kinds also can be used to provide mutual with the user; For example, the feedback that provides to the user can be any type of sensing feedback (for example, visible feedback, can listen feedback or tactile feedback); And can receive input from the user with any form that comprises acoustics, speech or sense of touch input.
System described herein and technology can be implemented in the computing system, this computing system comprises that back-end component (for example, as data server) or comprise that middleware component is (for example, application server) or comprise that front end component (for example, client computer with graphic user interface or web browser, the user can be mutual by the implementation of this graphic user interface or web browser and system described herein and technology) or any combination of such rear end, middleware or front end component.The parts of system can be by any digital data communication form or medium (for example communication network) interconnection.The example of communication network comprises LAN (Local Area Network) (" LAN "), wide area network (" WAN "), peer-to-peer network (having self-organization or static member), grid computing foundation structure and the Internet.
Computing system can comprise client and server.Client and server general mutual away from and mutual by communication network usually.The relation of client and server is by producing in corresponding computer operation and the computer program that has the client-server relation mutually.
Though above specifically described several implementations, other modifications are possible.In addition, can use other to be used for monitoring automatically the mechanism of speech input.In addition, particular order shown in the logic flow of describing in the drawings need not or the result who wishes with realization in proper order in regular turn.Other steps or can be from the process removal process of describing can be provided, and can add or remove miscellaneous part from the system of describing to the system of describing.Thereby other implementations within the scope of the appended claims.
Claims (20)
1. computer implemented method comprises:
Detect the current context that is associated with mobile computing device, described context is in described mobile device outside and indicate the current state of described equipment in its surrounding environment;
Determine whether described mobile computing device is switched to second operator scheme from current operator scheme based on described current context, described mobile computing device is at speech Input Monitor Connector ambient sound during described second operator scheme, and described speech input indication is used for the request of executable operations;
In response to determining whether to switch to described second operator scheme, activate the one or more microphones and the speech analysis subsystem that are associated with described mobile computing device, so that described mobile computing device receives audio data stream; And
Provide the output of importing in response to speech at described mobile computing device, described speech input is detected in the described audio data stream and indicates the request that is used for executable operations.
2. computer implemented method according to claim 1 further comprises:
Input continues the described audio data stream of monitoring at speech to use described speech analysis subsystem;
After detecting the speech input from described audio stream, determine that whether described speech import the described mobile computing device executable operations of indication request; And
Based on whether described speech input is indicated for the described of request of executable operations determine, make and carry out institute's solicit operation.
3. computer implemented method according to claim 2, wherein said request for executable operations comprises searching request; And
Make that wherein carrying out institute's solicit operation comprises the feasible described searching request of carrying out.
4. computer implemented method according to claim 2 makes that wherein carrying out institute's solicit operation comprises:
Provide the first information about institute's solicit operation by network to the remote computer system that is configured to carry out institute's solicit operation; And
The described remote computer system of the output that generates from indication from the execution of institute's solicit operation by described network receives second information, and at least a portion of wherein said second information is provided as the output on the described mobile computing device.
5. computer implemented method according to claim 2 further comprises:
Whether any key word that identifies in the predetermined key word grouping is present in described speech input; And
Whether wherein said speech input indicates for the definite of the request of executable operations whether be present in determining of described speech input based on described any key word.
6. computer implemented method according to claim 2 further comprises:
At least determine that based on the grammatical analysis of described speech input described speech input is order or the problem for described mobile computing device;
Whether wherein said speech input indicates for the definite of the request of executable operations is for the order of described mobile computing device or determining of problem based on described speech input at least.
7. computer implemented method according to claim 2 further comprises:
Identify the change in the structure that is associated with described speech input; And
Determine based on the change that identifies whether described speech input is used for described mobile computing device;
Whether wherein said speech input indicates for the definite of the request of executable operations whether be used for determining of described mobile computing device based on described speech input at least.
8. computer implemented method according to claim 1 further comprises:
Determine whether the user who is associated with described mobile computing device has at least provides threshold value possibility from the speech input to described mobile computing device based on described current context;
What wherein whether switch to described second operator scheme determines additionally at least whether to have determining of described threshold value possibility that the speech input is provided at least based on described user.
9. computer implemented method according to claim 1 further comprises:
Determine to have convenience at the user who is associated with described mobile computing device and described mobile computing device threshold level at least at speech Input Monitor Connector voice data based on described current context;
What wherein whether switch to described second operator scheme determines at least whether to have determining of described threshold value possibility that the speech input is provided at least based on described user.
10. computer implemented method according to claim 9, wherein when the manual operation of described mobile computing device be not can obtain easily the time, described current context at least pointer to the convenience of described user's described threshold level.
11. computer implemented method according to claim 9, wherein as i) described mobile computing device using external power source or when ii) described mobile computing device is using the compact power that has the threshold value residual charge at least, described current context at least pointer to the convenience of the described threshold level of described mobile computing device.
12. computer implemented method according to claim 1 further comprises:
Detection changes at the described current context of described mobile computing device;
Context based on changing determines whether described mobile computing device is switched to the 3rd operator scheme from described second operator scheme, and described mobile computing device is not at speech Input Monitor Connector ambient sound during described the 3rd operator scheme; And
In response to determining whether to switch to described the 3rd operator scheme, separate and activate described microphone and described speech analysis subsystem.
13. computer implemented method according to claim 1, wherein said current context comprise the residing physical location of described mobile computing device or physical arrangement.
14. computer implemented method according to claim 1, wherein said current context comprise the residing geographic position of described mobile computing device.
15. computer implemented method according to claim 1, wherein said current context comprise whether described mobile computing device is connected to the indication of equipment harbour.
16. computer implemented method according to claim 15, wherein said current context further comprise the indication of the device code head type that described mobile computing device connects.
17. computer implemented method according to claim 1, wherein said current context comprise that described mobile computing device is from the external power source received power.
18. computer implemented method according to claim 1 wherein carry out to detect described current context, determines whether to switch to described second operator scheme and activates described microphone and described speech analysis subsystem does not need user's guidance.
19. one kind is used for the system of monitoring speech input automatically, described system comprises:
Mobile computing device;
One or more microphones are configured to the reception environment sound signal and provide the electronic audio frequency data to described mobile computing device;
The context determining unit is configured to detect the current context that is associated with described mobile computing device, and described context is in described mobile device outside and indicate the current state of described equipment in its surrounding environment;
Mode selecting unit, be configured to determine whether described mobile computing device is switched to second operator scheme from current operator scheme based on the described current context of being determined by described context determining unit, described mobile computing device is at speech Input Monitor Connector ambient sound during described second operator scheme, and described speech input indication is used for the request of executable operations;
The input subsystem of described mobile computing device, be configured in response to determining whether to switch to described second operator scheme, activate the one or more microphones and the speech analysis subsystem that are associated with described mobile computing device, so that described mobile computing device receives audio data stream;
The output subsystem of described mobile computing device, being configured to provides the output of importing in response to speech at described mobile computing device, and described speech input is detected in the described audio data stream and indicates the request that is used for executable operations.
20. one kind is used for the system of monitoring speech input automatically, described system comprises:
Mobile computing device;
One or more microphones are configured to the reception environment sound signal and provide the electronic audio frequency data to described mobile computing device;
The context determining unit is configured to detect the current context that is associated with described mobile computing device, and described context is in described mobile device outside and indicate the current state of described equipment in its surrounding environment;
Be used for based on the device that is determined whether described mobile computing device is switched to from current operator scheme second operator scheme by described context, described mobile computing device is at speech Input Monitor Connector ambient sound during described second operator scheme, and described speech input indication is used for the request of executable operations;
The input subsystem of described mobile computing device, be configured in response to determining whether to switch to described second operator scheme, activate the one or more microphones and the speech analysis subsystem that are associated with described mobile computing device, so that described mobile computing device receives audio data stream;
The output subsystem of described mobile computing device, being configured to provides the output of importing in response to speech at described mobile computing device, and described speech input is detected in the described audio data stream and indicates the request that is used for executable operations.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN201610473719.6A CN106126178B (en) | 2010-08-06 | 2011-08-04 | Monitor speech input automatically based on context |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/852,256 US8359020B2 (en) | 2010-08-06 | 2010-08-06 | Automatically monitoring for voice input based on context |
US12/852,256 | 2010-08-06 | ||
PCT/US2011/046616 WO2012019020A1 (en) | 2010-08-06 | 2011-08-04 | Automatically monitoring for voice input based on context |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201610473719.6A Division CN106126178B (en) | 2010-08-06 | 2011-08-04 | Monitor speech input automatically based on context |
Publications (2)
Publication Number | Publication Date |
---|---|
CN103282957A true CN103282957A (en) | 2013-09-04 |
CN103282957B CN103282957B (en) | 2016-07-13 |
Family
ID=45556503
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201610473719.6A Active CN106126178B (en) | 2010-08-06 | 2011-08-04 | Monitor speech input automatically based on context |
CN201180047154.0A Active CN103282957B (en) | 2010-08-06 | 2011-08-04 | Automatically speech input is monitored based on context |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201610473719.6A Active CN106126178B (en) | 2010-08-06 | 2011-08-04 | Monitor speech input automatically based on context |
Country Status (6)
Country | Link |
---|---|
US (5) | US8359020B2 (en) |
EP (5) | EP3182408B1 (en) |
KR (2) | KR20160033233A (en) |
CN (2) | CN106126178B (en) |
AU (1) | AU2011285702B2 (en) |
WO (1) | WO2012019020A1 (en) |
Cited By (31)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN103578474A (en) * | 2013-10-25 | 2014-02-12 | 小米科技有限责任公司 | Method, device and equipment for voice control |
CN104464737A (en) * | 2013-09-12 | 2015-03-25 | 联发科技股份有限公司 | Voice verifying system and voice verifying method |
CN104715757A (en) * | 2013-12-13 | 2015-06-17 | 华为技术有限公司 | Terminal voice control operation method and device |
CN105556595A (en) * | 2013-09-17 | 2016-05-04 | 高通股份有限公司 | Method and apparatus for adjusting detection threshold for activating voice assistant function |
CN105788598A (en) * | 2014-12-19 | 2016-07-20 | 联想(北京)有限公司 | Speech processing method and electronic device |
CN105988589A (en) * | 2015-03-23 | 2016-10-05 | 国际商业机器公司 | Device and method used for wearable device |
CN106062661A (en) * | 2014-03-31 | 2016-10-26 | 英特尔公司 | Location aware power management scheme for always-on-always-listen voice recognition system |
CN107390851A (en) * | 2016-04-25 | 2017-11-24 | 感官公司 | Support the accurate intelligent listening pattern listened to all the time |
CN107643922A (en) * | 2016-07-22 | 2018-01-30 | 联想（新加坡）私人有限公司 | Equipment, method and computer-readable recording medium for voice auxiliary |
CN107833574A (en) * | 2017-11-16 | 2018-03-23 | 百度在线网络技术（北京）有限公司 | Method and apparatus for providing voice service |
CN108958806A (en) * | 2017-05-24 | 2018-12-07 | 联想（新加坡）私人有限公司 | The system and method for determining the response prompt for digital assistants based on situation |
CN109313898A (en) * | 2016-06-10 | 2019-02-05 | 苹果公司 | The digital assistants of voice in a low voice are provided |
CN109671427A (en) * | 2018-12-10 | 2019-04-23 | 珠海格力电器股份有限公司 | A kind of sound control method, device, storage medium and air-conditioning |
CN110001558A (en) * | 2019-04-18 | 2019-07-12 | 百度在线网络技术（北京）有限公司 | Method for controlling a vehicle and device |
CN110192246A (en) * | 2017-06-09 | 2019-08-30 | 谷歌有限责任公司 | Modification to the computer program output based on audio |
CN110741347A (en) * | 2017-10-03 | 2020-01-31 | 谷歌有限责任公司 | Multiple digital assistant coordination in a vehicle environment |
CN111919248A (en) * | 2018-03-08 | 2020-11-10 | 三星电子株式会社 | System for processing user utterances and control method thereof |
US11582169B2 (en) | 2017-06-09 | 2023-02-14 | Google Llc | Modification of audio-based computer program output |
US11750962B2 (en) | 2020-07-21 | 2023-09-05 | Apple Inc. | User identification using headphones |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11809886B2 (en) | 2015-11-06 | 2023-11-07 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11838579B2 (en) | 2014-06-30 | 2023-12-05 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11837237B2 (en) | 2017-05-12 | 2023-12-05 | Apple Inc. | User-specific acoustic models |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11862151B2 (en) | 2017-05-12 | 2024-01-02 | Apple Inc. | Low-latency intelligent automated assistant |
US11862186B2 (en) | 2013-02-07 | 2024-01-02 | Apple Inc. | Voice trigger for a digital assistant |
US11893992B2 (en) | 2018-09-28 | 2024-02-06 | Apple Inc. | Multi-modal inputs for voice commands |
US11907436B2 (en) | 2018-05-07 | 2024-02-20 | Apple Inc. | Raise to speak |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11954405B2 (en) | 2015-09-08 | 2024-04-09 | Apple Inc. | Zero latency digital assistant |
US11979836B2 (en) | 2007-04-03 | 2024-05-07 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
Families Citing this family (461)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
AU6630800A (en) * | 1999-08-13 | 2001-03-13 | Pixo, Inc. | Methods and apparatuses for display and traversing of links in page character array |
US8645137B2 (en) * | 2000-03-16 | 2014-02-04 | Apple Inc. | Fast, language-independent method for user authentication by voice |
ITFI20010199A1 (en) | 2001-10-22 | 2003-04-22 | Riccardo Vieri | SYSTEM AND METHOD TO TRANSFORM TEXTUAL COMMUNICATIONS INTO VOICE AND SEND THEM WITH AN INTERNET CONNECTION TO ANY TELEPHONE SYSTEM |
US7669134B1 (en) | 2003-05-02 | 2010-02-23 | Apple Inc. | Method and apparatus for displaying information during an instant messaging session |
US7764641B2 (en) | 2005-02-05 | 2010-07-27 | Cisco Technology, Inc. | Techniques for determining communication state using accelerometer data |
US8677377B2 (en) | 2005-09-08 | 2014-03-18 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US8073700B2 (en) | 2005-09-12 | 2011-12-06 | Nuance Communications, Inc. | Retrieval and presentation of network service results for mobile device using a multimodal browser |
US7633076B2 (en) | 2005-09-30 | 2009-12-15 | Apple Inc. | Automated response to and sensing of user activity in portable devices |
US7477909B2 (en) * | 2005-10-31 | 2009-01-13 | Nuance Communications, Inc. | System and method for conducting a search using a wireless mobile device |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US20080075237A1 (en) * | 2006-09-11 | 2008-03-27 | Agere Systems, Inc. | Speech recognition based data recovery system for use with a telephonic device |
US20080129520A1 (en) * | 2006-12-01 | 2008-06-05 | Apple Computer, Inc. | Electronic device with enhanced audio feedback |
US7912828B2 (en) * | 2007-02-23 | 2011-03-22 | Apple Inc. | Pattern searching methods and apparatuses |
US8843376B2 (en) * | 2007-03-13 | 2014-09-23 | Nuance Communications, Inc. | Speech-enabled web content searching using a multimodal browser |
ITFI20070177A1 (en) | 2007-07-26 | 2009-01-27 | Riccardo Vieri | SYSTEM FOR THE CREATION AND SETTING OF AN ADVERTISING CAMPAIGN DERIVING FROM THE INSERTION OF ADVERTISING MESSAGES WITHIN AN EXCHANGE OF MESSAGES AND METHOD FOR ITS FUNCTIONING. |
US9053089B2 (en) | 2007-10-02 | 2015-06-09 | Apple Inc. | Part-of-speech tagging using latent analogy |
US8165886B1 (en) | 2007-10-04 | 2012-04-24 | Great Northern Research LLC | Speech interface system and method for control and interaction with applications on a computing system |
US8595642B1 (en) | 2007-10-04 | 2013-11-26 | Great Northern Research, LLC | Multiple shell multi faceted graphical user interface |
US8364694B2 (en) * | 2007-10-26 | 2013-01-29 | Apple Inc. | Search assistant for digital media assets |
US8620662B2 (en) | 2007-11-20 | 2013-12-31 | Apple Inc. | Context-aware unit selection |
US10002189B2 (en) | 2007-12-20 | 2018-06-19 | Apple Inc. | Method and apparatus for searching using an active ontology |
US9330720B2 (en) | 2008-01-03 | 2016-05-03 | Apple Inc. | Methods and apparatus for altering audio output signals |
US8327272B2 (en) | 2008-01-06 | 2012-12-04 | Apple Inc. | Portable multifunction device, method, and graphical user interface for viewing and managing electronic calendars |
US8065143B2 (en) | 2008-02-22 | 2011-11-22 | Apple Inc. | Providing text input using speech data and non-speech data |
US8289283B2 (en) * | 2008-03-04 | 2012-10-16 | Apple Inc. | Language input interface on a device |
US8996376B2 (en) | 2008-04-05 | 2015-03-31 | Apple Inc. | Intelligent text-to-speech conversion |
US10496753B2 (en) | 2010-01-18 | 2019-12-03 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US8464150B2 (en) | 2008-06-07 | 2013-06-11 | Apple Inc. | Automatic language identification for dynamic text processing |
US20100030549A1 (en) | 2008-07-31 | 2010-02-04 | Lee Michael M | Mobile device having human language translation capability with positional feedback |
US8768702B2 (en) | 2008-09-05 | 2014-07-01 | Apple Inc. | Multi-tiered voice feedback in an electronic device |
US8898568B2 (en) | 2008-09-09 | 2014-11-25 | Apple Inc. | Audio user interface |
US8355919B2 (en) * | 2008-09-29 | 2013-01-15 | Apple Inc. | Systems and methods for text normalization for text to speech synthesis |
US8712776B2 (en) | 2008-09-29 | 2014-04-29 | Apple Inc. | Systems and methods for selective text to speech synthesis |
US8583418B2 (en) | 2008-09-29 | 2013-11-12 | Apple Inc. | Systems and methods of detecting language and natural language strings for text to speech synthesis |
US8352268B2 (en) * | 2008-09-29 | 2013-01-08 | Apple Inc. | Systems and methods for selective rate of speech and speech preferences for text to speech synthesis |
US8396714B2 (en) * | 2008-09-29 | 2013-03-12 | Apple Inc. | Systems and methods for concatenation of words in text to speech synthesis |
US8352272B2 (en) * | 2008-09-29 | 2013-01-08 | Apple Inc. | Systems and methods for text to speech synthesis |
US20100082328A1 (en) * | 2008-09-29 | 2010-04-01 | Apple Inc. | Systems and methods for speech preprocessing in text to speech synthesis |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
WO2010067118A1 (en) | 2008-12-11 | 2010-06-17 | Novauris Technologies Limited | Speech recognition involving a mobile device |
US8862252B2 (en) | 2009-01-30 | 2014-10-14 | Apple Inc. | Audio user interface for displayless electronic device |
US8380507B2 (en) * | 2009-03-09 | 2013-02-19 | Apple Inc. | Systems and methods for determining the language to use for speech generated by a text to speech engine |
US10540976B2 (en) | 2009-06-05 | 2020-01-21 | Apple Inc. | Contextual voice commands |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US10241752B2 (en) | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US8983640B2 (en) | 2009-06-26 | 2015-03-17 | Intel Corporation | Controlling audio players using environmental audio analysis |
US9431006B2 (en) | 2009-07-02 | 2016-08-30 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
US20110010179A1 (en) * | 2009-07-13 | 2011-01-13 | Naik Devang K | Voice synthesis and processing |
US20110066438A1 (en) * | 2009-09-15 | 2011-03-17 | Apple Inc. | Contextual voiceover |
US9183580B2 (en) | 2010-11-04 | 2015-11-10 | Digimarc Corporation | Methods and systems for resource management on portable devices |
US9197736B2 (en) | 2009-12-31 | 2015-11-24 | Digimarc Corporation | Intuitive computing methods and systems |
US8682649B2 (en) | 2009-11-12 | 2014-03-25 | Apple Inc. | Sentiment prediction from textual data |
KR101605347B1 (en) * | 2009-12-18 | 2016-03-22 | 삼성전자주식회사 | Method and apparatus for controlling external output of a portable terminal |
US20110167350A1 (en) * | 2010-01-06 | 2011-07-07 | Apple Inc. | Assist Features For Content Display Device |
US8600743B2 (en) * | 2010-01-06 | 2013-12-03 | Apple Inc. | Noise profile determination for voice-related feature |
US8381107B2 (en) | 2010-01-13 | 2013-02-19 | Apple Inc. | Adaptive audio feedback system and method |
US8311838B2 (en) | 2010-01-13 | 2012-11-13 | Apple Inc. | Devices and methods for identifying a prompt corresponding to a voice input in a sequence of prompts |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US10705794B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10679605B2 (en) | 2010-01-18 | 2020-06-09 | Apple Inc. | Hands-free list-reading by intelligent automated assistant |
US10553209B2 (en) | 2010-01-18 | 2020-02-04 | Apple Inc. | Systems and methods for hands-free notification summaries |
US8626511B2 (en) * | 2010-01-22 | 2014-01-07 | Google Inc. | Multi-dimensional disambiguation of voice commands |
US8682667B2 (en) | 2010-02-25 | 2014-03-25 | Apple Inc. | User profiling for selecting user specific voice input processing information |
US8639516B2 (en) | 2010-06-04 | 2014-01-28 | Apple Inc. | User-specific noise suppression for voice quality improvements |
US8713021B2 (en) | 2010-07-07 | 2014-04-29 | Apple Inc. | Unsupervised document clustering using latent semantic density analysis |
US9104670B2 (en) | 2010-07-21 | 2015-08-11 | Apple Inc. | Customized search or acquisition of digital media assets |
US8359020B2 (en) | 2010-08-06 | 2013-01-22 | Google Inc. | Automatically monitoring for voice input based on context |
JP2012047924A (en) * | 2010-08-26 | 2012-03-08 | Sony Corp | Information processing device and information processing method, and program |
US8719006B2 (en) | 2010-08-27 | 2014-05-06 | Apple Inc. | Combined statistical and rule-based part-of-speech tagging for text-to-speech synthesis |
US8719014B2 (en) | 2010-09-27 | 2014-05-06 | Apple Inc. | Electronic device with text error correction based on voice recognition data |
US9484046B2 (en) | 2010-11-04 | 2016-11-01 | Digimarc Corporation | Smartphone-based methods and systems |
US8855919B2 (en) * | 2010-12-02 | 2014-10-07 | Telenav, Inc. | Navigation system with destination-centric en-route notification delivery mechanism and method of operation thereof |
US10515147B2 (en) | 2010-12-22 | 2019-12-24 | Apple Inc. | Using statistical language models for contextual lookup |
US10762293B2 (en) | 2010-12-22 | 2020-09-01 | Apple Inc. | Using parts-of-speech tagging and named entity recognition for spelling correction |
US20120191454A1 (en) * | 2011-01-26 | 2012-07-26 | TrackThings LLC | Method and Apparatus for Obtaining Statistical Data from a Conversation |
US8781836B2 (en) | 2011-02-22 | 2014-07-15 | Apple Inc. | Hearing assistance system for providing consistent human speech |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
US9196028B2 (en) * | 2011-09-23 | 2015-11-24 | Digimarc Corporation | Context-based smartphone sensor logic |
US20120278078A1 (en) * | 2011-04-26 | 2012-11-01 | Avaya Inc. | Input and displayed information definition based on automatic speech recognition during a communication session |
US10057736B2 (en) | 2011-06-03 | 2018-08-21 | Apple Inc. | Active transport based notifications |
US10672399B2 (en) | 2011-06-03 | 2020-06-02 | Apple Inc. | Switching between text data and audio data based on a mapping |
US8812294B2 (en) | 2011-06-21 | 2014-08-19 | Apple Inc. | Translating phrases from one language into another using an order-based set of declarative rules |
US8706472B2 (en) | 2011-08-11 | 2014-04-22 | Apple Inc. | Method for disambiguating multiple readings in language conversion |
WO2013027908A1 (en) * | 2011-08-25 | 2013-02-28 | Lg Electronics Inc. | Mobile terminal, image display device mounted on vehicle and data processing method using the same |
US8994660B2 (en) | 2011-08-29 | 2015-03-31 | Apple Inc. | Text correction processing |
US8798995B1 (en) * | 2011-09-23 | 2014-08-05 | Amazon Technologies, Inc. | Key word determinations from voice data |
US9477943B2 (en) | 2011-09-28 | 2016-10-25 | Elwha Llc | Multi-modality communication |
US9794209B2 (en) | 2011-09-28 | 2017-10-17 | Elwha Llc | User interface for multi-modality communication |
US9503550B2 (en) | 2011-09-28 | 2016-11-22 | Elwha Llc | Multi-modality communication modification |
US9002937B2 (en) | 2011-09-28 | 2015-04-07 | Elwha Llc | Multi-party multi-modality communication |
US20130079029A1 (en) * | 2011-09-28 | 2013-03-28 | Royce A. Levien | Multi-modality communication network auto-activation |
US9788349B2 (en) * | 2011-09-28 | 2017-10-10 | Elwha Llc | Multi-modality communication auto-activation |
US9699632B2 (en) | 2011-09-28 | 2017-07-04 | Elwha Llc | Multi-modality communication with interceptive conversion |
US9906927B2 (en) | 2011-09-28 | 2018-02-27 | Elwha Llc | Multi-modality communication initiation |
US8762156B2 (en) | 2011-09-28 | 2014-06-24 | Apple Inc. | Speech recognition repair using contextual information |
US9992745B2 (en) | 2011-11-01 | 2018-06-05 | Qualcomm Incorporated | Extraction and analysis of buffered audio data using multiple codec rates each greater than a low-power processor rate |
US20130120106A1 (en) | 2011-11-16 | 2013-05-16 | Motorola Mobility, Inc. | Display device, corresponding systems, and methods therefor |
US9214157B2 (en) * | 2011-12-06 | 2015-12-15 | At&T Intellectual Property I, L.P. | System and method for machine-mediated human-human conversation |
WO2013085507A1 (en) | 2011-12-07 | 2013-06-13 | Hewlett-Packard Development Company, L.P. | Low power integrated circuit to analyze a digitized audio stream |
KR101912409B1 (en) * | 2012-01-06 | 2018-10-26 | 엘지전자 주식회사 | Mobile terminal and mothod for controling of the same |
US20130201316A1 (en) | 2012-01-09 | 2013-08-08 | May Patents Ltd. | System and method for server based control |
US10134385B2 (en) | 2012-03-02 | 2018-11-20 | Apple Inc. | Systems and methods for name pronunciation |
US9483461B2 (en) | 2012-03-06 | 2016-11-01 | Apple Inc. | Handling speech synthesis of content for multiple languages |
US9317605B1 (en) | 2012-03-21 | 2016-04-19 | Google Inc. | Presenting forked auto-completions |
US9280610B2 (en) | 2012-05-14 | 2016-03-08 | Apple Inc. | Crowd sourcing information to fulfill user requests |
US8775442B2 (en) | 2012-05-15 | 2014-07-08 | Apple Inc. | Semantic search using a single-source semantic model |
US10417037B2 (en) | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11023520B1 (en) | 2012-06-01 | 2021-06-01 | Google Llc | Background audio identification for query disambiguation |
US9721563B2 (en) | 2012-06-08 | 2017-08-01 | Apple Inc. | Name recognition system |
WO2013185109A2 (en) | 2012-06-08 | 2013-12-12 | Apple Inc. | Systems and methods for recognizing textual identifiers within a plurality of words |
US9142215B2 (en) * | 2012-06-15 | 2015-09-22 | Cypress Semiconductor Corporation | Power-efficient voice activation |
US9495129B2 (en) | 2012-06-29 | 2016-11-15 | Apple Inc. | Device, method, and user interface for voice-activated navigation and browsing of a document |
US9218333B2 (en) * | 2012-08-31 | 2015-12-22 | Microsoft Technology Licensing, Llc | Context sensitive auto-correction |
US9576574B2 (en) | 2012-09-10 | 2017-02-21 | Apple Inc. | Context-sensitive handling of interruptions by intelligent digital assistant |
US9547647B2 (en) | 2012-09-19 | 2017-01-17 | Apple Inc. | Voice-based media searching |
US10042603B2 (en) * | 2012-09-20 | 2018-08-07 | Samsung Electronics Co., Ltd. | Context aware service provision method and apparatus of user device |
KR102070196B1 (en) | 2012-09-20 | 2020-01-30 | 삼성전자 주식회사 | Method and apparatus for providing context aware service in a user device |
US9922646B1 (en) | 2012-09-21 | 2018-03-20 | Amazon Technologies, Inc. | Identifying a location of a voice-input device |
US8935167B2 (en) | 2012-09-25 | 2015-01-13 | Apple Inc. | Exemplar-based latent perceptual modeling for automatic speech recognition |
CN103701981B (en) * | 2012-09-27 | 2016-05-25 | 中兴通讯股份有限公司 | A kind of method and device of realizing speech identifying function |
KR102009423B1 (en) | 2012-10-08 | 2019-08-09 | 삼성전자주식회사 | Method and apparatus for action of preset performance mode using voice recognition |
US9477993B2 (en) | 2012-10-14 | 2016-10-25 | Ari M Frank | Training a predictor of emotional response based on explicit voting on content and eye tracking to verify attention |
US9104467B2 (en) | 2012-10-14 | 2015-08-11 | Ari M Frank | Utilizing eye tracking to reduce power consumption involved in measuring affective response |
US9124795B2 (en) * | 2012-10-26 | 2015-09-01 | Nokia Technologies Oy | Method and apparatus for obtaining an image associated with a location of a mobile terminal |
KR102211595B1 (en) * | 2012-12-07 | 2021-02-04 | 삼성전자주식회사 | Speech recognition apparatus and control method thereof |
KR102091003B1 (en) * | 2012-12-10 | 2020-03-19 | 삼성전자 주식회사 | Method and apparatus for providing context aware service using speech recognition |
US20140181715A1 (en) * | 2012-12-26 | 2014-06-26 | Microsoft Corporation | Dynamic user interfaces adapted to inferred user contexts |
BR112015014830B1 (en) * | 2012-12-28 | 2021-11-16 | Sony Corporation | DEVICE AND METHOD OF INFORMATION PROCESSING, AND MEMORY STORAGE MEANS. |
KR102009316B1 (en) * | 2013-01-07 | 2019-08-09 | 삼성전자주식회사 | Interactive server, display apparatus and controlling method thereof |
DE102013001219B4 (en) * | 2013-01-25 | 2019-08-29 | Inodyn Newmedia Gmbh | Method and system for voice activation of a software agent from a standby mode |
AU2021202255B9 (en) * | 2013-02-07 | 2022-06-09 | Apple Inc. | Voice trigger for a digital assistant |
US9622365B2 (en) | 2013-02-25 | 2017-04-11 | Google Technology Holdings LLC | Apparatus and methods for accommodating a display in an electronic device |
US9311640B2 (en) | 2014-02-11 | 2016-04-12 | Digimarc Corporation | Methods and arrangements for smartphone payments and transactions |
US9310957B2 (en) * | 2013-03-07 | 2016-04-12 | Tencent Technology (Shenzhen) Company Limited | Method and device for switching current information providing mode |
CN104035550B (en) * | 2013-03-07 | 2017-12-22 | 腾讯科技（深圳）有限公司 | Information provides mode switching method and device |
US9112984B2 (en) | 2013-03-12 | 2015-08-18 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US11393461B2 (en) | 2013-03-12 | 2022-07-19 | Cerence Operating Company | Methods and apparatus for detecting a voice command |
WO2014159581A1 (en) * | 2013-03-12 | 2014-10-02 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
EP3611627A1 (en) * | 2013-03-13 | 2020-02-19 | INTEL Corporation | Device resource management based on contextual planning |
US9977779B2 (en) | 2013-03-14 | 2018-05-22 | Apple Inc. | Automatic supplementation of word correction dictionaries |
US9674922B2 (en) | 2013-03-14 | 2017-06-06 | Google Technology Holdings LLC | Display side edge assembly and mobile device including same |
US10572476B2 (en) | 2013-03-14 | 2020-02-25 | Apple Inc. | Refining a search based on schedule items |
US10642574B2 (en) | 2013-03-14 | 2020-05-05 | Apple Inc. | Device, method, and graphical user interface for outputting captions |
US9368114B2 (en) | 2013-03-14 | 2016-06-14 | Apple Inc. | Context-sensitive handling of interruptions |
US9733821B2 (en) | 2013-03-14 | 2017-08-15 | Apple Inc. | Voice control to diagnose inadvertent activation of accessibility features |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US10748529B1 (en) | 2013-03-15 | 2020-08-18 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
KR102057795B1 (en) | 2013-03-15 | 2019-12-19 | 애플 인크. | Context-sensitive handling of interruptions |
EP2973002B1 (en) | 2013-03-15 | 2019-06-26 | Apple Inc. | User training by intelligent digital assistant |
WO2014144579A1 (en) | 2013-03-15 | 2014-09-18 | Apple Inc. | System and method for updating an adaptive speech recognition model |
KR101759009B1 (en) | 2013-03-15 | 2017-07-17 | 애플 인크. | Training an at least partial voice command system |
US9380039B2 (en) * | 2013-03-15 | 2016-06-28 | Google Inc. | Systems and methods for automatically logging into a user account |
US9626963B2 (en) * | 2013-04-30 | 2017-04-18 | Paypal, Inc. | System and method of improving speech recognition using context |
US9892729B2 (en) | 2013-05-07 | 2018-02-13 | Qualcomm Incorporated | Method and apparatus for controlling voice activation |
CN104142791A (en) * | 2013-05-09 | 2014-11-12 | 腾讯科技（深圳）有限公司 | Resource replacing method, device and terminal |
CN105122181B (en) * | 2013-05-16 | 2018-12-18 | 英特尔公司 | Technology for the natural user interface input based on scene |
WO2014197336A1 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for detecting errors in interactions with a voice-based digital assistant |
US9582608B2 (en) | 2013-06-07 | 2017-02-28 | Apple Inc. | Unified ranking with entropy-weighted information for phrase-based semantic auto-completion |
WO2014197334A2 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
WO2014197335A1 (en) | 2013-06-08 | 2014-12-11 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
KR101959188B1 (en) | 2013-06-09 | 2019-07-02 | 애플 인크. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
WO2014200731A1 (en) | 2013-06-13 | 2014-12-18 | Apple Inc. | System and method for emergency calls initiated by voice command |
US9747899B2 (en) | 2013-06-27 | 2017-08-29 | Amazon Technologies, Inc. | Detecting self-generated wake expressions |
US9997160B2 (en) * | 2013-07-01 | 2018-06-12 | Toyota Motor Engineering & Manufacturing North America, Inc. | Systems and methods for dynamic download of embedded voice components |
US9646606B2 (en) | 2013-07-03 | 2017-05-09 | Google Inc. | Speech recognition using domain knowledge |
US9575721B2 (en) * | 2013-07-25 | 2017-02-21 | Lg Electronics Inc. | Head mounted display and method of controlling therefor |
KR101749009B1 (en) | 2013-08-06 | 2017-06-19 | 애플 인크. | Auto-activating smart responses based on activities from remote devices |
US9002835B2 (en) | 2013-08-15 | 2015-04-07 | Google Inc. | Query response using media consumption history |
WO2015029362A1 (en) * | 2013-08-29 | 2015-03-05 | パナソニック インテレクチュアル プロパティ コーポレーション オブ アメリカ | Device control method and device control system |
KR102158315B1 (en) * | 2013-10-14 | 2020-09-21 | 삼성전자주식회사 | Display apparatus for performing a voice control and method thereof |
TWI502487B (en) * | 2013-10-24 | 2015-10-01 | Hooloop Corp | Methods for voice management, and related devices and computer program prodcuts |
US9400634B2 (en) | 2013-10-28 | 2016-07-26 | Google Technology Holdings LLC | Systems and methods for communicating notifications and textual data associated with applications |
US10158730B2 (en) | 2013-10-30 | 2018-12-18 | At&T Intellectual Property I, L.P. | Context based communication management |
US10296160B2 (en) | 2013-12-06 | 2019-05-21 | Apple Inc. | Method for extracting salient dialog usage from live data |
US10720153B2 (en) * | 2013-12-13 | 2020-07-21 | Harman International Industries, Incorporated | Name-sensitive listening device |
US10147441B1 (en) * | 2013-12-19 | 2018-12-04 | Amazon Technologies, Inc. | Voice controlled system |
US9484001B2 (en) | 2013-12-23 | 2016-11-01 | Google Technology Holdings LLC | Portable electronic device controlling diffuse light source to emit light approximating color of object of user interest |
US8938394B1 (en) * | 2014-01-09 | 2015-01-20 | Google Inc. | Audio triggers based on context |
KR20150087544A (en) | 2014-01-22 | 2015-07-30 | 엘지이노텍 주식회사 | Gesture device, operating method thereof and vehicle having the same |
US9516165B1 (en) * | 2014-03-26 | 2016-12-06 | West Corporation | IVR engagements and upfront background noise |
US9372851B2 (en) * | 2014-04-01 | 2016-06-21 | Microsoft Technology Licensing, Llc | Creating a calendar event using context |
US9430461B2 (en) | 2014-04-11 | 2016-08-30 | International Business Machines Corporation | Mobile based lexicon and forecasting |
US10770075B2 (en) * | 2014-04-21 | 2020-09-08 | Qualcomm Incorporated | Method and apparatus for activating application by speech input |
US9620105B2 (en) | 2014-05-15 | 2017-04-11 | Apple Inc. | Analyzing audio input for efficient speech and music recognition |
US10592095B2 (en) | 2014-05-23 | 2020-03-17 | Apple Inc. | Instantaneous speaking of content on touch devices |
US9502031B2 (en) | 2014-05-27 | 2016-11-22 | Apple Inc. | Method for supporting dynamic grammars in WFST-based ASR |
US9842101B2 (en) | 2014-05-30 | 2017-12-12 | Apple Inc. | Predictive conversion of language input |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US9760559B2 (en) | 2014-05-30 | 2017-09-12 | Apple Inc. | Predictive text input |
US9633004B2 (en) | 2014-05-30 | 2017-04-25 | Apple Inc. | Better resolution when referencing to concepts |
US9785630B2 (en) | 2014-05-30 | 2017-10-10 | Apple Inc. | Text prediction using combined word N-gram and unigram language models |
US10289433B2 (en) | 2014-05-30 | 2019-05-14 | Apple Inc. | Domain specific language for encoding assistant dialog |
US9715875B2 (en) * | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9430463B2 (en) | 2014-05-30 | 2016-08-30 | Apple Inc. | Exemplar-based natural language processing |
US10078631B2 (en) | 2014-05-30 | 2018-09-18 | Apple Inc. | Entropy-guided text prediction using combined word and character n-gram language models |
US9734193B2 (en) | 2014-05-30 | 2017-08-15 | Apple Inc. | Determining domain salience ranking from ambiguous words in natural speech |
US9697828B1 (en) * | 2014-06-20 | 2017-07-04 | Amazon Technologies, Inc. | Keyword detection modeling using contextual and environmental information |
US9632748B2 (en) * | 2014-06-24 | 2017-04-25 | Google Inc. | Device designation for audio input monitoring |
US10659851B2 (en) | 2014-06-30 | 2020-05-19 | Apple Inc. | Real-time digital assistant knowledge updates |
US9361442B2 (en) | 2014-08-12 | 2016-06-07 | International Business Machines Corporation | Triggering actions on a user device based on biometrics of nearby individuals |
US10446141B2 (en) | 2014-08-28 | 2019-10-15 | Apple Inc. | Automatic speech recognition based on user feedback |
US9818400B2 (en) | 2014-09-11 | 2017-11-14 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10789041B2 (en) | 2014-09-12 | 2020-09-29 | Apple Inc. | Dynamic thresholds for always listening speech trigger |
US9646609B2 (en) | 2014-09-30 | 2017-05-09 | Apple Inc. | Caching apparatus for serving phonetic pronunciations |
US9886432B2 (en) | 2014-09-30 | 2018-02-06 | Apple Inc. | Parsimonious handling of word inflection via categorical stem + suffix N-gram language models |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
US10074360B2 (en) | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10552013B2 (en) | 2014-12-02 | 2020-02-04 | Apple Inc. | Data detection |
US10575117B2 (en) | 2014-12-08 | 2020-02-25 | Harman International Industries, Incorporated | Directional sound modification |
US9711141B2 (en) | 2014-12-09 | 2017-07-18 | Apple Inc. | Disambiguating heteronyms in speech synthesis |
CN104601818B (en) * | 2015-01-26 | 2018-02-13 | 宇龙计算机通信科技(深圳)有限公司 | A kind of method and device of the switching operating system in communication process |
US9633661B1 (en) * | 2015-02-02 | 2017-04-25 | Amazon Technologies, Inc. | Speech-responsive portable speaker |
US10152299B2 (en) | 2015-03-06 | 2018-12-11 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US9865280B2 (en) | 2015-03-06 | 2018-01-09 | Apple Inc. | Structured dictation using intelligent automated assistants |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US9899019B2 (en) | 2015-03-18 | 2018-02-20 | Apple Inc. | Systems and methods for structured stem and suffix language models |
US9842105B2 (en) | 2015-04-16 | 2017-12-12 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US10460227B2 (en) | 2015-05-15 | 2019-10-29 | Apple Inc. | Virtual assistant in a communication session |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US10504509B2 (en) * | 2015-05-27 | 2019-12-10 | Google Llc | Providing suggested voice-based action queries |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
US10127220B2 (en) | 2015-06-04 | 2018-11-13 | Apple Inc. | Language identification from short strings |
US9578173B2 (en) | 2015-06-05 | 2017-02-21 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10101822B2 (en) | 2015-06-05 | 2018-10-16 | Apple Inc. | Language input correction |
US10255907B2 (en) | 2015-06-07 | 2019-04-09 | Apple Inc. | Automatic accent detection using acoustic models |
US10186254B2 (en) | 2015-06-07 | 2019-01-22 | Apple Inc. | Context-based endpoint detection |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US9807045B2 (en) | 2015-06-10 | 2017-10-31 | Google Inc. | Contextually driven messaging system |
US20160378747A1 (en) | 2015-06-29 | 2016-12-29 | Apple Inc. | Virtual assistant for media playback |
KR102505347B1 (en) | 2015-07-16 | 2023-03-03 | 삼성전자주식회사 | Method and Apparatus for alarming user interest voice |
CN106469040B (en) | 2015-08-19 | 2019-06-21 | 华为终端有限公司 | Communication means, server and equipment |
CN105161111B (en) * | 2015-08-25 | 2017-09-26 | 百度在线网络技术（北京）有限公司 | Audio recognition method and device based on bluetooth connection |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US10740384B2 (en) | 2015-09-08 | 2020-08-11 | Apple Inc. | Intelligent automated assistant for media search and playback |
WO2017042906A1 (en) * | 2015-09-09 | 2017-03-16 | 三菱電機株式会社 | In-vehicle speech recognition device and in-vehicle equipment |
US9697820B2 (en) | 2015-09-24 | 2017-07-04 | Apple Inc. | Unit-selection text-to-speech synthesis using concatenation-sensitive neural networks |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US11587559B2 (en) | 2015-09-30 | 2023-02-21 | Apple Inc. | Intelligent device identification |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
EP3179472B1 (en) * | 2015-12-11 | 2020-03-18 | Sony Mobile Communications, Inc. | Method and device for recording and analyzing data from a microphone |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10599390B1 (en) | 2015-12-28 | 2020-03-24 | Amazon Technologies, Inc. | Methods and systems for providing multi-user recommendations |
US10380208B1 (en) * | 2015-12-28 | 2019-08-13 | Amazon Technologies, Inc. | Methods and systems for providing context-based recommendations |
EP3414759B1 (en) | 2016-02-10 | 2020-07-01 | Cerence Operating Company | Techniques for spatially selective wake-up word recognition and related systems and methods |
US10743101B2 (en) | 2016-02-22 | 2020-08-11 | Sonos, Inc. | Content mixing |
US9947316B2 (en) | 2016-02-22 | 2018-04-17 | Sonos, Inc. | Voice control of a media playback system |
US10509626B2 (en) | 2016-02-22 | 2019-12-17 | Sonos, Inc | Handling of loss of pairing between networked devices |
US10095470B2 (en) | 2016-02-22 | 2018-10-09 | Sonos, Inc. | Audio response playback |
US10264030B2 (en) | 2016-02-22 | 2019-04-16 | Sonos, Inc. | Networked microphone device control |
US10142754B2 (en) | 2016-02-22 | 2018-11-27 | Sonos, Inc. | Sensor on moving component of transducer |
US9965247B2 (en) | 2016-02-22 | 2018-05-08 | Sonos, Inc. | Voice controlled media playback system based on user profile |
KR20170100309A (en) | 2016-02-25 | 2017-09-04 | 삼성전자주식회사 | Electronic apparatus for providing a voice recognition control and method thereof |
US10192550B2 (en) * | 2016-03-01 | 2019-01-29 | Microsoft Technology Licensing, Llc | Conversational software agent |
US10140986B2 (en) * | 2016-03-01 | 2018-11-27 | Microsoft Technology Licensing, Llc | Speech recognition |
US10140988B2 (en) * | 2016-03-01 | 2018-11-27 | Microsoft Technology Licensing, Llc | Speech recognition |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US11176930B1 (en) * | 2016-03-28 | 2021-11-16 | Amazon Technologies, Inc. | Storing audio commands for time-delayed execution |
US9989376B2 (en) * | 2016-05-12 | 2018-06-05 | Tata Consultancy Services Limited | Systems and methods for generating signature ambient sounds and maps thereof |
US11416212B2 (en) | 2016-05-17 | 2022-08-16 | Microsoft Technology Licensing, Llc | Context-based user agent |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
US9978390B2 (en) | 2016-06-09 | 2018-05-22 | Sonos, Inc. | Dynamic player selection for audio signal processing |
DK179309B1 (en) | 2016-06-09 | 2018-04-23 | Apple Inc | Intelligent automated assistant in a home environment |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
DK179343B1 (en) | 2016-06-11 | 2018-05-14 | Apple Inc | Intelligent task discovery |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
DK179049B1 (en) | 2016-06-11 | 2017-09-18 | Apple Inc | Data driven natural language event detection and classification |
US11600269B2 (en) | 2016-06-15 | 2023-03-07 | Cerence Operating Company | Techniques for wake-up word recognition and related systems and methods |
CN105939424B (en) * | 2016-06-23 | 2019-12-27 | 北京小米移动软件有限公司 | Application switching method and device |
US11232136B2 (en) * | 2016-06-27 | 2022-01-25 | Google Llc | Contextual voice search suggestions |
US10134399B2 (en) | 2016-07-15 | 2018-11-20 | Sonos, Inc. | Contextualization of voice inputs |
US10152969B2 (en) | 2016-07-15 | 2018-12-11 | Sonos, Inc. | Voice detection by multiple devices |
US10438583B2 (en) | 2016-07-20 | 2019-10-08 | Lenovo (Singapore) Pte. Ltd. | Natural language voice assistant |
US10621992B2 (en) * | 2016-07-22 | 2020-04-14 | Lenovo (Singapore) Pte. Ltd. | Activating voice assistant based on at least one of user proximity and context |
US9693164B1 (en) | 2016-08-05 | 2017-06-27 | Sonos, Inc. | Determining direction of networked microphone device relative to audio playback device |
US10115400B2 (en) | 2016-08-05 | 2018-10-30 | Sonos, Inc. | Multiple voice services |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US9794720B1 (en) | 2016-09-22 | 2017-10-17 | Sonos, Inc. | Acoustic position measurement |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US9942678B1 (en) | 2016-09-27 | 2018-04-10 | Sonos, Inc. | Audio playback settings for voice interaction |
US10147423B2 (en) * | 2016-09-29 | 2018-12-04 | Intel IP Corporation | Context-aware query recognition for electronic devices |
US9743204B1 (en) | 2016-09-30 | 2017-08-22 | Sonos, Inc. | Multi-orientation playback device microphones |
US10181323B2 (en) | 2016-10-19 | 2019-01-15 | Sonos, Inc. | Arbitration-based voice recognition |
US10951720B2 (en) | 2016-10-24 | 2021-03-16 | Bank Of America Corporation | Multi-channel cognitive resource platform |
WO2018086033A1 (en) | 2016-11-10 | 2018-05-17 | Nuance Communications, Inc. | Techniques for language independent wake-up word detection |
US10332523B2 (en) | 2016-11-18 | 2019-06-25 | Google Llc | Virtual assistant identification of nearby computing devices |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US9940930B1 (en) * | 2016-12-07 | 2018-04-10 | Google Llc | Securing audio data |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US10747423B2 (en) | 2016-12-31 | 2020-08-18 | Spotify Ab | User interface for media content playback |
US10489106B2 (en) | 2016-12-31 | 2019-11-26 | Spotify Ab | Media content playback during travel |
US11514098B2 (en) | 2016-12-31 | 2022-11-29 | Spotify Ab | Playlist trailers for media content playback during travel |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US10166465B2 (en) | 2017-01-20 | 2019-01-01 | Essential Products, Inc. | Contextual user interface based on video game playback |
US10359993B2 (en) | 2017-01-20 | 2019-07-23 | Essential Products, Inc. | Contextual user interface based on environment |
US9924313B1 (en) * | 2017-02-23 | 2018-03-20 | International Business Machines Corporation | Location based generation of pertinent information |
US11183181B2 (en) | 2017-03-27 | 2021-11-23 | Sonos, Inc. | Systems and methods of multiple voice services |
KR102398649B1 (en) | 2017-03-28 | 2022-05-17 | 삼성전자주식회사 | Electronic device for processing user utterance and method for operation thereof |
CN107122179A (en) | 2017-03-31 | 2017-09-01 | 阿里巴巴集团控股有限公司 | The function control method and device of voice |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
DK201770383A1 (en) | 2017-05-09 | 2018-12-14 | Apple Inc. | User interface for correcting recognition errors |
DK201770439A1 (en) | 2017-05-11 | 2018-12-13 | Apple Inc. | Offline personal assistant |
DK180048B1 (en) | 2017-05-11 | 2020-02-04 | Apple Inc. | MAINTAINING THE DATA PROTECTION OF PERSONAL INFORMATION |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
DK201770431A1 (en) | 2017-05-15 | 2018-12-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US10628570B2 (en) | 2017-05-15 | 2020-04-21 | Fmr Llc | Protection of data in a zero user interface environment |
DK201770432A1 (en) | 2017-05-15 | 2018-12-21 | Apple Inc. | Hierarchical belief states for digital assistants |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US20180336892A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Detecting a trigger of a digital assistant |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
DK179560B1 (en) | 2017-05-16 | 2019-02-18 | Apple Inc. | Far-field extension for digital assistant services |
US11221823B2 (en) * | 2017-05-22 | 2022-01-11 | Samsung Electronics Co., Ltd. | System and method for context-based interaction for electronic devices |
US20180350360A1 (en) * | 2017-05-31 | 2018-12-06 | Lenovo (Singapore) Pte. Ltd. | Provide non-obtrusive output |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10922051B2 (en) | 2017-07-05 | 2021-02-16 | Hewlett-Packard Development Company, L.P. | Application-specific profile managers |
US20190019505A1 (en) * | 2017-07-12 | 2019-01-17 | Lenovo (Singapore) Pte. Ltd. | Sustaining conversational session |
CN110603804A (en) * | 2017-07-28 | 2019-12-20 | 惠普发展公司，有限责任合伙企业 | Interference generation |
US10475449B2 (en) | 2017-08-07 | 2019-11-12 | Sonos, Inc. | Wake-word detection suppression |
KR102389041B1 (en) * | 2017-08-11 | 2022-04-21 | 엘지전자 주식회사 | Mobile terminal and method using machine learning for controlling mobile terminal |
US10048930B1 (en) | 2017-09-08 | 2018-08-14 | Sonos, Inc. | Dynamic computation of system response volume |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10504513B1 (en) * | 2017-09-26 | 2019-12-10 | Amazon Technologies, Inc. | Natural language understanding with affiliated devices |
US10446165B2 (en) | 2017-09-27 | 2019-10-15 | Sonos, Inc. | Robust short-time fourier transform acoustic echo cancellation during audio playback |
US10482868B2 (en) | 2017-09-28 | 2019-11-19 | Sonos, Inc. | Multi-channel acoustic echo cancellation |
US10051366B1 (en) | 2017-09-28 | 2018-08-14 | Sonos, Inc. | Three-dimensional beam forming with a microphone array |
US10621981B2 (en) | 2017-09-28 | 2020-04-14 | Sonos, Inc. | Tone interference cancellation |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10466962B2 (en) | 2017-09-29 | 2019-11-05 | Sonos, Inc. | Media playback system with voice assistance |
WO2019089001A1 (en) * | 2017-10-31 | 2019-05-09 | Hewlett-Packard Development Company, L.P. | Actuation module to control when a sensing module is responsive to events |
KR102517219B1 (en) | 2017-11-23 | 2023-04-03 | 삼성전자주식회사 | Electronic apparatus and the control method thereof |
US11140450B2 (en) | 2017-11-28 | 2021-10-05 | Rovi Guides, Inc. | Methods and systems for recommending content in context of a conversation |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
JP7192208B2 (en) * | 2017-12-01 | 2022-12-20 | ヤマハ株式会社 | Equipment control system, device, program, and equipment control method |
US10880650B2 (en) | 2017-12-10 | 2020-12-29 | Sonos, Inc. | Network microphone devices with automatic do not disturb actuation capabilities |
US10818290B2 (en) | 2017-12-11 | 2020-10-27 | Sonos, Inc. | Home graph |
US10690863B1 (en) * | 2017-12-13 | 2020-06-23 | Amazon Technologies, Inc. | Communication cable identification |
US11409816B2 (en) | 2017-12-19 | 2022-08-09 | Motorola Solutions, Inc. | Methods and systems for determining an action to be taken in response to a user query as a function of pre-query context information |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
WO2019152722A1 (en) | 2018-01-31 | 2019-08-08 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10885910B1 (en) | 2018-03-14 | 2021-01-05 | Amazon Technologies, Inc. | Voice-forward graphical user interface mode management |
US10877637B1 (en) * | 2018-03-14 | 2020-12-29 | Amazon Technologies, Inc. | Voice-based device operation mode management |
US11127405B1 (en) | 2018-03-14 | 2021-09-21 | Amazon Technologies, Inc. | Selective requests for authentication for voice-based launching of applications |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10930278B2 (en) * | 2018-04-09 | 2021-02-23 | Google Llc | Trigger sound detection in ambient audio to provide related functionality on a user interface |
US11145299B2 (en) | 2018-04-19 | 2021-10-12 | X Development Llc | Managing voice interface devices |
KR102612835B1 (en) * | 2018-04-20 | 2023-12-13 | 삼성전자주식회사 | Electronic device and method for executing function of electronic device |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11175880B2 (en) | 2018-05-10 | 2021-11-16 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US10847178B2 (en) | 2018-05-18 | 2020-11-24 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US10959029B2 (en) | 2018-05-25 | 2021-03-23 | Sonos, Inc. | Determining and adapting to changes in microphone performance of playback devices |
DK201870355A1 (en) | 2018-06-01 | 2019-12-16 | Apple Inc. | Virtual assistant operation in multi-device environments |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US11100935B2 (en) | 2018-06-05 | 2021-08-24 | Samsung Electronics Co., Ltd. | Voice assistant device and method thereof |
EP3756087A4 (en) * | 2018-06-05 | 2021-04-21 | Samsung Electronics Co., Ltd. | Methods and systems for passive wakeup of a user interaction device |
CN109068276B (en) * | 2018-06-28 | 2020-09-11 | 维沃移动通信有限公司 | Message conversion method and terminal |
US10681460B2 (en) | 2018-06-28 | 2020-06-09 | Sonos, Inc. | Systems and methods for associating playback devices with voice assistant services |
CN110689882A (en) * | 2018-07-04 | 2020-01-14 | 上海博泰悦臻网络技术服务有限公司 | Vehicle, playing equipment thereof and multimedia playing automatic control method |
JP7055721B2 (en) * | 2018-08-27 | 2022-04-18 | 京セラ株式会社 | Electronic devices with voice recognition functions, control methods and programs for those electronic devices |
US10461710B1 (en) | 2018-08-28 | 2019-10-29 | Sonos, Inc. | Media playback system with maximum volume setting |
US11076035B2 (en) | 2018-08-28 | 2021-07-27 | Sonos, Inc. | Do not disturb feature for audio notifications |
NO20181210A1 (en) * | 2018-08-31 | 2020-03-02 | Elliptic Laboratories As | Voice assistant |
US10587430B1 (en) | 2018-09-14 | 2020-03-10 | Sonos, Inc. | Networked devices, systems, and methods for associating playback devices based on sound codes |
US10878811B2 (en) | 2018-09-14 | 2020-12-29 | Sonos, Inc. | Networked devices, systems, and methods for intelligently deactivating wake-word engines |
US11024331B2 (en) | 2018-09-21 | 2021-06-01 | Sonos, Inc. | Voice detection optimization using sound metadata |
US10811015B2 (en) | 2018-09-25 | 2020-10-20 | Sonos, Inc. | Voice detection optimization based on selected voice assistant service |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11094327B2 (en) * | 2018-09-28 | 2021-08-17 | Lenovo (Singapore) Pte. Ltd. | Audible input transcription |
US11100923B2 (en) | 2018-09-28 | 2021-08-24 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US10692518B2 (en) | 2018-09-29 | 2020-06-23 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection via multiple network microphone devices |
US11899519B2 (en) | 2018-10-23 | 2024-02-13 | Sonos, Inc. | Multiple stage network microphone device with reduced power consumption and processing load |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
WO2020096218A1 (en) * | 2018-11-05 | 2020-05-14 | Samsung Electronics Co., Ltd. | Electronic device and operation method thereof |
US10971160B2 (en) | 2018-11-13 | 2021-04-06 | Comcast Cable Communications, Llc | Methods and systems for determining a wake word |
EP3654249A1 (en) | 2018-11-15 | 2020-05-20 | Snips | Dilated convolutions and gating for efficient keyword spotting |
US10984791B2 (en) | 2018-11-29 | 2021-04-20 | Hughes Network Systems, Llc | Spoken language interface for network management |
US11183183B2 (en) | 2018-12-07 | 2021-11-23 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11242032B2 (en) * | 2018-12-11 | 2022-02-08 | GM Global Technology Operations LLC | Custom vehicle alarm based on electronic device identifier |
US11132989B2 (en) | 2018-12-13 | 2021-09-28 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US10602268B1 (en) | 2018-12-20 | 2020-03-24 | Sonos, Inc. | Optimization of network microphone devices using noise classification |
CN111475206B (en) * | 2019-01-04 | 2023-04-11 | 优奈柯恩(北京)科技有限公司 | Method and apparatus for waking up wearable device |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US10867604B2 (en) | 2019-02-08 | 2020-12-15 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing |
US11315556B2 (en) | 2019-02-08 | 2022-04-26 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing by transmitting sound data associated with a wake word to an appropriate device for identification |
JP7205697B2 (en) * | 2019-02-21 | 2023-01-17 | 株式会社リコー | Communication terminal, shared system, display control method and program |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11817194B2 (en) * | 2019-04-30 | 2023-11-14 | Pixart Imaging Inc. | Smart control system |
US11137770B2 (en) * | 2019-04-30 | 2021-10-05 | Pixart Imaging Inc. | Sensor registering method and event identifying method of smart detection system |
US11120794B2 (en) | 2019-05-03 | 2021-09-14 | Sonos, Inc. | Voice assistant persistence across multiple network microphone devices |
DK201970509A1 (en) | 2019-05-06 | 2021-01-15 | Apple Inc | Spoken notifications |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
DK201970510A1 (en) | 2019-05-31 | 2021-02-11 | Apple Inc | Voice identification in digital assistant systems |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
DK180129B1 (en) | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US10586540B1 (en) | 2019-06-12 | 2020-03-10 | Sonos, Inc. | Network microphone device with command keyword conditioning |
US11200894B2 (en) | 2019-06-12 | 2021-12-14 | Sonos, Inc. | Network microphone device with command keyword eventing |
US11361756B2 (en) | 2019-06-12 | 2022-06-14 | Sonos, Inc. | Conditional wake word eventing based on environment |
US11138969B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US11138975B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US10871943B1 (en) | 2019-07-31 | 2020-12-22 | Sonos, Inc. | Noise classification for event detection |
US10839060B1 (en) | 2019-08-27 | 2020-11-17 | Capital One Services, Llc | Techniques for multi-voice speech recognition commands |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11749265B2 (en) * | 2019-10-04 | 2023-09-05 | Disney Enterprises, Inc. | Techniques for incremental computer-based natural language understanding |
US11189286B2 (en) | 2019-10-22 | 2021-11-30 | Sonos, Inc. | VAS toggle based on device orientation |
US11489794B2 (en) | 2019-11-04 | 2022-11-01 | Bank Of America Corporation | System for configuration and intelligent transmission of electronic communications and integrated resource processing |
US11061958B2 (en) | 2019-11-14 | 2021-07-13 | Jetblue Airways Corporation | Systems and method of generating custom messages based on rule-based database queries in a cloud platform |
US11200900B2 (en) | 2019-12-20 | 2021-12-14 | Sonos, Inc. | Offline voice control |
CN111081225B (en) * | 2019-12-31 | 2022-04-01 | 思必驰科技股份有限公司 | Skill voice awakening method and device |
US11562740B2 (en) | 2020-01-07 | 2023-01-24 | Sonos, Inc. | Voice verification for media playback |
CN111312239B (en) | 2020-01-20 | 2023-09-26 | 北京小米松果电子有限公司 | Response method, response device, electronic equipment and storage medium |
US11556307B2 (en) | 2020-01-31 | 2023-01-17 | Sonos, Inc. | Local voice data processing |
US11308958B2 (en) | 2020-02-07 | 2022-04-19 | Sonos, Inc. | Localized wakeword verification |
US11290834B2 (en) | 2020-03-04 | 2022-03-29 | Apple Inc. | Determining head pose based on room reverberation |
US11043220B1 (en) | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11727919B2 (en) | 2020-05-20 | 2023-08-15 | Sonos, Inc. | Memory allocation for keyword spotting engines |
US11482224B2 (en) | 2020-05-20 | 2022-10-25 | Sonos, Inc. | Command keywords with input detection windowing |
US11308962B2 (en) | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
US11698771B2 (en) | 2020-08-25 | 2023-07-11 | Sonos, Inc. | Vocal guidance engines for playback devices |
JP7152043B2 (en) * | 2020-09-08 | 2022-10-12 | 株式会社ユピテル | Device and program |
EP4002061A1 (en) * | 2020-11-24 | 2022-05-25 | Inter IKEA Systems B.V. | A control device and a method for determining control data based on audio input data |
US11250855B1 (en) * | 2020-12-23 | 2022-02-15 | Nuance Communications, Inc. | Ambient cooperative intelligence system and method |
US11551700B2 (en) | 2021-01-25 | 2023-01-10 | Sonos, Inc. | Systems and methods for power-efficient keyword detection |
EP4220628A1 (en) | 2021-02-19 | 2023-08-02 | Samsung Electronics Co., Ltd. | Electronic device for supporting service for artificial intelligent agent that talks with user |
KR20230023212A (en) * | 2021-08-10 | 2023-02-17 | 삼성전자주식회사 | Electronic device for outputting result for processing voice command according to the change of the state and operating method thereof |
Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020077830A1 (en) * | 2000-12-19 | 2002-06-20 | Nokia Corporation | Method for activating context sensitive speech recognition in a terminal |
US6615170B1 (en) * | 2000-03-07 | 2003-09-02 | International Business Machines Corporation | Model-based voice activity detection system and method using a log-likelihood ratio and pitch |
CN1623152A (en) * | 2002-01-24 | 2005-06-01 | 英特尔公司 | Context-based information processing |
CN1692407A (en) * | 2002-06-20 | 2005-11-02 | 英特尔公司 | Improving speech recognition of mobile devices |
US20070011133A1 (en) * | 2005-06-22 | 2007-01-11 | Sbc Knowledge Ventures, L.P. | Voice search engine generating sub-topics based on recognitiion confidence |
US7523226B2 (en) * | 2005-11-09 | 2009-04-21 | Microsoft Corporation | Controlling an auxiliary display user interface based on usage context |
US20090259691A1 (en) * | 2008-04-10 | 2009-10-15 | Nokia Corporation | Methods, Apparatuses and Computer Program Products for Updating a Content Item |
US20100069123A1 (en) * | 2008-09-16 | 2010-03-18 | Yellowpages.Com Llc | Systems and Methods for Voice Based Search |
Family Cites Families (28)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3674990B2 (en) | 1995-08-21 | 2005-07-27 | セイコーエプソン株式会社 | Speech recognition dialogue apparatus and speech recognition dialogue processing method |
EP0847003A3 (en) * | 1996-12-03 | 2004-01-02 | Texas Instruments Inc. | An audio memo system and method of operation thereof |
SE9902229L (en) * | 1999-06-07 | 2001-02-05 | Ericsson Telefon Ab L M | Apparatus and method of controlling a voice controlled operation |
US6904405B2 (en) * | 1999-07-17 | 2005-06-07 | Edwin A. Suominen | Message recognition using shared language model |
US20020055844A1 (en) * | 2000-02-25 | 2002-05-09 | L'esperance Lauren | Speech user interface for portable personal devices |
KR20010094229A (en) * | 2000-04-04 | 2001-10-31 | 이수성 | Method and system for operating a phone by voice recognition technique |
DE10164799B4 (en) * | 2001-03-21 | 2006-03-30 | Audioton Kabelwerk Gmbh | Mobile telephone device with multicore electrical connection devices |
US20030101060A1 (en) * | 2001-11-29 | 2003-05-29 | Bickley Corine A. | Use of historical data for a voice application interface |
WO2003078930A1 (en) * | 2002-03-15 | 2003-09-25 | Mitsubishi Denki Kabushiki Kaisha | Vehicular navigation device |
US7200413B2 (en) * | 2002-07-31 | 2007-04-03 | Interchange Corporation | Methods and system for enhanced directory assistance using wireless messaging protocols |
US6993615B2 (en) * | 2002-11-15 | 2006-01-31 | Microsoft Corporation | Portable computing device-integrated appliance |
EP1611504B1 (en) * | 2003-04-07 | 2009-01-14 | Nokia Corporation | Method and device for providing speech-enabled input in an electronic device having a user interface |
US8244536B2 (en) * | 2003-08-27 | 2012-08-14 | General Motors Llc | Algorithm for intelligent speech recognition |
US7418392B1 (en) * | 2003-09-25 | 2008-08-26 | Sensory, Inc. | System and method for controlling the operation of a device by voice commands |
US20060085199A1 (en) * | 2004-10-19 | 2006-04-20 | Yogendra Jain | System and method for controlling the behavior of a device capable of speech recognition |
US20060287864A1 (en) * | 2005-06-16 | 2006-12-21 | Juha Pusa | Electronic device, computer program product and voice control method |
US7949529B2 (en) * | 2005-08-29 | 2011-05-24 | Voicebox Technologies, Inc. | Mobile systems and methods of supporting natural language human-machine interactions |
DE112006002989T5 (en) * | 2005-12-14 | 2009-02-19 | Mitsubishi Electric Corp. | Voice recognition device |
US20070299670A1 (en) * | 2006-06-27 | 2007-12-27 | Sbc Knowledge Ventures, Lp | Biometric and speech recognition system and method |
US8041025B2 (en) * | 2006-08-07 | 2011-10-18 | International Business Machines Corporation | Systems and arrangements for controlling modes of audio devices based on user selectable parameters |
KR101327445B1 (en) * | 2006-09-15 | 2013-11-11 | 삼성전자주식회사 | Mobile Communication Terminal for performing Auto Receiving Notification Changing Mode and Method thereof |
US7581188B2 (en) * | 2006-09-27 | 2009-08-25 | Hewlett-Packard Development Company, L.P. | Context-based user interface system |
US8880402B2 (en) * | 2006-10-28 | 2014-11-04 | General Motors Llc | Automatically adapting user guidance in automated speech recognition |
DE102008051756A1 (en) * | 2007-11-12 | 2009-05-14 | Volkswagen Ag | Multimodal user interface of a driver assistance system for entering and presenting information |
US8958848B2 (en) * | 2008-04-08 | 2015-02-17 | Lg Electronics Inc. | Mobile terminal and menu control method thereof |
KR101545582B1 (en) * | 2008-10-29 | 2015-08-19 | 엘지전자 주식회사 | Terminal and method for controlling the same |
US8428759B2 (en) * | 2010-03-26 | 2013-04-23 | Google Inc. | Predictive pre-recording of audio for voice input |
US8359020B2 (en) | 2010-08-06 | 2013-01-22 | Google Inc. | Automatically monitoring for voice input based on context |
-
2010
- 2010-08-06 US US12/852,256 patent/US8359020B2/en active Active
-
2011
- 2011-08-04 EP EP17154054.5A patent/EP3182408B1/en active Active
- 2011-08-04 EP EP21207785.3A patent/EP3998603A3/en active Pending
- 2011-08-04 WO PCT/US2011/046616 patent/WO2012019020A1/en active Application Filing
- 2011-08-04 EP EP20188566.2A patent/EP3748630B1/en active Active
- 2011-08-04 AU AU2011285702A patent/AU2011285702B2/en active Active
- 2011-08-04 CN CN201610473719.6A patent/CN106126178B/en active Active
- 2011-08-04 CN CN201180047154.0A patent/CN103282957B/en active Active
- 2011-08-04 KR KR1020167006078A patent/KR20160033233A/en not_active Application Discontinuation
- 2011-08-04 EP EP18191886.3A patent/EP3432303B1/en active Active
- 2011-08-04 KR KR1020137005725A patent/KR101605481B1/en active IP Right Grant
- 2011-08-04 EP EP11815329.5A patent/EP2601650A4/en not_active Withdrawn
- 2011-09-29 US US13/248,751 patent/US8326328B2/en active Active
-
2012
- 2012-12-06 US US13/706,696 patent/US8918121B2/en active Active
-
2014
- 2014-12-04 US US14/560,696 patent/US9105269B2/en active Active
-
2015
- 2015-07-07 US US14/793,253 patent/US9251793B2/en active Active
Patent Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6615170B1 (en) * | 2000-03-07 | 2003-09-02 | International Business Machines Corporation | Model-based voice activity detection system and method using a log-likelihood ratio and pitch |
US20020077830A1 (en) * | 2000-12-19 | 2002-06-20 | Nokia Corporation | Method for activating context sensitive speech recognition in a terminal |
CN1623152A (en) * | 2002-01-24 | 2005-06-01 | 英特尔公司 | Context-based information processing |
CN1692407A (en) * | 2002-06-20 | 2005-11-02 | 英特尔公司 | Improving speech recognition of mobile devices |
US20070011133A1 (en) * | 2005-06-22 | 2007-01-11 | Sbc Knowledge Ventures, L.P. | Voice search engine generating sub-topics based on recognitiion confidence |
US7523226B2 (en) * | 2005-11-09 | 2009-04-21 | Microsoft Corporation | Controlling an auxiliary display user interface based on usage context |
US20090259691A1 (en) * | 2008-04-10 | 2009-10-15 | Nokia Corporation | Methods, Apparatuses and Computer Program Products for Updating a Content Item |
US20100069123A1 (en) * | 2008-09-16 | 2010-03-18 | Yellowpages.Com Llc | Systems and Methods for Voice Based Search |
Cited By (45)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11979836B2 (en) | 2007-04-03 | 2024-05-07 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US11862186B2 (en) | 2013-02-07 | 2024-01-02 | Apple Inc. | Voice trigger for a digital assistant |
CN104464737B (en) * | 2013-09-12 | 2018-09-04 | 联发科技股份有限公司 | Voice authentication system and sound verification method |
US9928851B2 (en) | 2013-09-12 | 2018-03-27 | Mediatek Inc. | Voice verifying system and voice verifying method which can determine if voice signal is valid or not |
CN104464737A (en) * | 2013-09-12 | 2015-03-25 | 联发科技股份有限公司 | Voice verifying system and voice verifying method |
CN105556595A (en) * | 2013-09-17 | 2016-05-04 | 高通股份有限公司 | Method and apparatus for adjusting detection threshold for activating voice assistant function |
CN105556595B (en) * | 2013-09-17 | 2019-11-01 | 高通股份有限公司 | For adjusting the method and apparatus for activating the detection threshold value of speech miscellaneous function |
CN103578474B (en) * | 2013-10-25 | 2017-09-12 | 小米科技有限责任公司 | A kind of sound control method, device and equipment |
CN103578474A (en) * | 2013-10-25 | 2014-02-12 | 小米科技有限责任公司 | Method, device and equipment for voice control |
CN104715757A (en) * | 2013-12-13 | 2015-06-17 | 华为技术有限公司 | Terminal voice control operation method and device |
CN106062661A (en) * | 2014-03-31 | 2016-10-26 | 英特尔公司 | Location aware power management scheme for always-on-always-listen voice recognition system |
US10133332B2 (en) | 2014-03-31 | 2018-11-20 | Intel Corporation | Location aware power management scheme for always-on-always-listen voice recognition system |
US11838579B2 (en) | 2014-06-30 | 2023-12-05 | Apple Inc. | Intelligent automated assistant for TV user interactions |
CN105788598A (en) * | 2014-12-19 | 2016-07-20 | 联想(北京)有限公司 | Speech processing method and electronic device |
CN105788598B (en) * | 2014-12-19 | 2019-12-24 | 联想(北京)有限公司 | Voice processing method and electronic equipment |
CN105988589A (en) * | 2015-03-23 | 2016-10-05 | 国际商业机器公司 | Device and method used for wearable device |
US10628337B2 (en) | 2015-03-23 | 2020-04-21 | International Business Machines Corporation | Communication mode control for wearable devices |
US10275369B2 (en) | 2015-03-23 | 2019-04-30 | International Business Machines Corporation | Communication mode control for wearable devices |
US11954405B2 (en) | 2015-09-08 | 2024-04-09 | Apple Inc. | Zero latency digital assistant |
US11809886B2 (en) | 2015-11-06 | 2023-11-07 | Apple Inc. | Intelligent automated assistant in a messaging environment |
CN107390851A (en) * | 2016-04-25 | 2017-11-24 | 感官公司 | Support the accurate intelligent listening pattern listened to all the time |
US10880833B2 (en) | 2016-04-25 | 2020-12-29 | Sensory, Incorporated | Smart listening modes supporting quasi always-on listening |
CN109313898A (en) * | 2016-06-10 | 2019-02-05 | 苹果公司 | The digital assistants of voice in a low voice are provided |
CN107643922A (en) * | 2016-07-22 | 2018-01-30 | 联想（新加坡）私人有限公司 | Equipment, method and computer-readable recording medium for voice auxiliary |
US11837237B2 (en) | 2017-05-12 | 2023-12-05 | Apple Inc. | User-specific acoustic models |
US11862151B2 (en) | 2017-05-12 | 2024-01-02 | Apple Inc. | Low-latency intelligent automated assistant |
CN108958806A (en) * | 2017-05-24 | 2018-12-07 | 联想（新加坡）私人有限公司 | The system and method for determining the response prompt for digital assistants based on situation |
CN108958806B (en) * | 2017-05-24 | 2021-10-15 | 联想（新加坡）私人有限公司 | System and method for determining response prompts for a digital assistant based on context |
CN110192246A (en) * | 2017-06-09 | 2019-08-30 | 谷歌有限责任公司 | Modification to the computer program output based on audio |
US11582169B2 (en) | 2017-06-09 | 2023-02-14 | Google Llc | Modification of audio-based computer program output |
CN110192246B (en) * | 2017-06-09 | 2023-11-21 | 谷歌有限责任公司 | Modification of audio-based computer program output |
US11646029B2 (en) | 2017-10-03 | 2023-05-09 | Google Llc | Multiple digital assistant coordination in vehicular environments |
CN110741347B (en) * | 2017-10-03 | 2023-08-01 | 谷歌有限责任公司 | Multiple digital assistant coordination in a vehicle environment |
CN110741347A (en) * | 2017-10-03 | 2020-01-31 | 谷歌有限责任公司 | Multiple digital assistant coordination in a vehicle environment |
CN107833574B (en) * | 2017-11-16 | 2021-08-24 | 百度在线网络技术（北京）有限公司 | Method and apparatus for providing voice service |
CN107833574A (en) * | 2017-11-16 | 2018-03-23 | 百度在线网络技术（北京）有限公司 | Method and apparatus for providing voice service |
CN111919248A (en) * | 2018-03-08 | 2020-11-10 | 三星电子株式会社 | System for processing user utterances and control method thereof |
US11907436B2 (en) | 2018-05-07 | 2024-02-20 | Apple Inc. | Raise to speak |
US11893992B2 (en) | 2018-09-28 | 2024-02-06 | Apple Inc. | Multi-modal inputs for voice commands |
CN109671427A (en) * | 2018-12-10 | 2019-04-23 | 珠海格力电器股份有限公司 | A kind of sound control method, device, storage medium and air-conditioning |
CN110001558A (en) * | 2019-04-18 | 2019-07-12 | 百度在线网络技术（北京）有限公司 | Method for controlling a vehicle and device |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11750962B2 (en) | 2020-07-21 | 2023-09-05 | Apple Inc. | User identification using headphones |
Also Published As
Publication number | Publication date |
---|---|
US9251793B2 (en) | 2016-02-02 |
US20130095805A1 (en) | 2013-04-18 |
CN103282957B (en) | 2016-07-13 |
EP3748630A2 (en) | 2020-12-09 |
US9105269B2 (en) | 2015-08-11 |
EP3432303A2 (en) | 2019-01-23 |
KR101605481B1 (en) | 2016-03-22 |
US20150310867A1 (en) | 2015-10-29 |
AU2011285702A1 (en) | 2013-03-07 |
EP3998603A2 (en) | 2022-05-18 |
KR20160033233A (en) | 2016-03-25 |
EP3432303B1 (en) | 2020-10-07 |
AU2011285702B2 (en) | 2014-08-07 |
EP3748630B1 (en) | 2021-12-22 |
EP3182408B1 (en) | 2018-12-26 |
KR20130100280A (en) | 2013-09-10 |
WO2012019020A1 (en) | 2012-02-09 |
US8918121B2 (en) | 2014-12-23 |
CN106126178B (en) | 2019-09-06 |
US8359020B2 (en) | 2013-01-22 |
US20120034904A1 (en) | 2012-02-09 |
EP3182408A1 (en) | 2017-06-21 |
EP2601650A1 (en) | 2013-06-12 |
US20120035931A1 (en) | 2012-02-09 |
US20150112691A1 (en) | 2015-04-23 |
EP2601650A4 (en) | 2014-07-16 |
EP3748630A3 (en) | 2021-03-24 |
CN106126178A (en) | 2016-11-16 |
US8326328B2 (en) | 2012-12-04 |
EP3998603A3 (en) | 2022-08-31 |
EP3432303A3 (en) | 2019-03-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN103282957A (en) | Automatically monitoring for voice input based on context | |
CN103404118B (en) | Profile of knowing on mobile computing device switches | |
AU2011285618B2 (en) | Disambiguating input based on context | |
CN103677261A (en) | Context aware service provision method and apparatus of user equipment | |
KR20130116269A (en) | Loading a mobile computing device with media files | |
KR20130132765A (en) | State-dependent query response | |
KR20130083905A (en) | Providing results to parameterless search queries | |
KR102084963B1 (en) | Electro device for decreasing consumption power and method for controlling thereof | |
AU2011326157B2 (en) | Self-aware profile switching on a mobile computing device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
C14 | Grant of patent or utility model | ||
GR01 | Patent grant | ||
CP01 | Change in the name or title of a patent holder | ||
CP01 | Change in the name or title of a patent holder |
Address after: American CaliforniaPatentee after: Google limited liability companyAddress before: American CaliforniaPatentee before: Google Inc. |