CN114846540A - Using video clips as dictionary use examples - Google Patents
Using video clips as dictionary use examples Download PDFInfo
- Publication number
- CN114846540A CN114846540A CN201980103316.4A CN201980103316A CN114846540A CN 114846540 A CN114846540 A CN 114846540A CN 201980103316 A CN201980103316 A CN 201980103316A CN 114846540 A CN114846540 A CN 114846540A
- Authority
- CN
- China
- Prior art keywords
- target
- candidate video
- video clip
- gram
- video
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
- G06F40/295—Named entity recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7834—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using audio features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/20—Movements or behaviour, e.g. gesture recognition
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/68—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/683—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/685—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using automatically derived transcript of audio data, e.g. lyrics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/237—Lexical tools
- G06F40/242—Dictionaries
Abstract
Implementations are provided for automatically mining a corpus of electronic video files for video clips containing spoken dialogs as suitable use cases for accompanying or supplemental dictionary definitions. These video clips may then be associated with the target n-grams in a searchable database, such as the base database of the online dictionary. In various implementations, a set of candidate video clips in which a target n-gram is voiced in a target context may be identified from a corpus of electronic video files. For each candidate video clip in the set, the pre-existing manual subtitles associated with the candidate video clip may be compared to text generated based on speech recognition processing of the audio portion of the candidate video clip. Based at least in part on the comparison, a measure of suitability as a dictionary use example may be calculated for the candidate video clip.
Description
Background
In addition to definitions, dictionaries typically provide other information about words and/or phrases (referred to herein as "n-grams"), including but not limited to synonyms, antonyms, phonetic information, word source information, and/or use examples. In particular, use of examples may be very beneficial to individuals (sometimes referred to herein as "users") who attempt to understand how a given word or phrase is used in various contexts. Examples of use provided in traditional dictionaries in both paper and electronic form are typically taken from literary works, newspapers, and the like.
Whether electronic or printed on paper, these use examples are typically provided in printed form, which may not be helpful in understanding certain aspects of a word or phrase, such as its pronunciation (e.g., generally or regionally). Furthermore, as users increasingly acquire information, including dictionary definitions of words/phrases, through spoken human-machine conversations with automated assistants, the print use case may have various limitations. For example, printed text-to-speech ("TTS") processing using examples may generate auditory output of words or phrases used in context, but computer-generated speech may not necessarily capture the correct pronunciation or other subtle idioms often used when words/phrases are spoken by humans.
The electronic video file may be associated with (e.g., as metadata) text data conveying spoken dialog and other sounds contained in the electronic video file. These text data are typically used to present subtitles and/or captions (captions) on the screen when playing a video file. Subtitles may include spoken dialog and commentary may include dialog and may also include other sounds contained in the video file, such as music, sound effects, and the like. In other words, subtitles convey only the words that people speak in the video, while captions (sometimes referred to as "closed captioning") also convey information about non-verbal sounds in the video, such as "dramatic music play," "door slam," "bird chirp," and the like. For purposes of this disclosure, the terms "caption" and "subtitle" will be used interchangeably to refer to printed text conveying spoken dialog contained in an electronic video file and/or video clip.
Disclosure of Invention
The present disclosure relates generally to methods, apparatus, and computer-readable media (transitory and non-transitory) for automatically mining a corpus of electronic video files for video clips containing spoken dialogs as suitable use cases with or in addition to dictionary definitions. These video clips may then be associated with the target n-grams in a searchable database, such as the base database of the online dictionary. As used herein, a "video clip" or "video segment" may include an entire electronic video file or a portion of an electronic video file. For example, a video clip containing an utterance of the target n-gram may be cut or extracted from a longer electronic video file containing other unrelated content that is not related to the target n-gram.
In some implementations, subtitles associated with an electronic video file may be used to identify a video file or portion thereof that contains candidate use cases for a target n-gram. Additionally or alternatively, in some implementations, automatic speech recognition ("ASR") processing (alternatively referred to as "speech-to-text" or "STT" processing) may be used to generate a transcript of a spoken dialog in an electronic video file or video clip. These transcripts may be used to identify video files or portions thereof containing candidate use cases for the target n-gram.
As described above, electronic video files may often contain a large amount of extraneous information that is not needed to generate dictionary use examples. Thus, the portion of the video containing the utterance of the target n-gram may be cut out as a video clip. Various different techniques and/or heuristics may be used to identify the portion to be excised. For example, one or more complete spoken sentences containing the target n-gram may be identified, e.g., using natural language processing and/or audio processing (e.g., detecting pauses, timing, etc.), and at least portions of the video clip containing the identified one or more complete sentences may be cut away to potentially serve as dictionary use examples. In implementations where the subtitles (or ASR-generated transcript) of the video contain punctuation (e.g., capital letters, periods, commas, etc.), the punctuation can also be used to depict a complete sentence speaking the target n-gram.
Just because the video clip contains the target n-gram, it is not meant to fit the dictionary use example. Video clips may be noisy or of low quality, both of which may make spoken dialog unintelligible. Alternatively, the video clip may contain bare content that is not suitable for some viewers, and therefore not suitable for the dictionary use example. In addition, many n-grams have multiple meanings or meanings. The unigram "test" may be a noun or verb, so a video clip using "test" as a noun may not fit into an example of use of a verb definition as "test".
Even in video clips without these drawbacks, some video clips may be more suitable than others as dictionary use examples. Thus, in various implementations, one or more video features and/or other signals may be analyzed to determine a measure of "suitability as a dictionary use example" (or "SDUE") for a video clip. Based on the metrics of the SDUE to which the video clip corresponds, the video clip can be selected, e.g., for association with a target n-gram in a searchable database and/or for output to a querying user. In various implementations, metrics for SDUE may be calculated for the entire electronic video file, portions of the electronic video file, and/or video clips cut from the electronic video file.
In general, videos with spoken dialog that are more easily understood by the viewer may be more suitable for use as an example of dictionary use. Accordingly, various characteristics of the video may be used to determine metrics for the SDUE. The more spoken dialog in a video that exhibits the appropriate tempo, pronunciation, announcement, etc., the more likely it is to be suitable as a dictionary use example.
In some implementations, a metric for SDUE of a video may be determined based on a comparison of subtitles (e.g., pre-existing, manually generated) associated with the video and a transcript generated by performing ASR processing on the video. The captions and the ASR-generated transcript may be compared to each other to determine a similarity (or dissimilarity) metric. By using ASR to obtain text from audio associated with a video and comparing the text to existing subtitles, the text comparison can be effectively used to determine the quality of an audio signal. That is, the quality of text output from ASR processing is directly related to the quality of the audio signal processed by ASR to generate the text. Comparing the text output of the ASR in this manner therefore allows the quality of the audio signal to be analyzed. The greater similarity between the two indicates that the dialog in the video is clear and more likely to be understood by the viewer than a different video with a large deviation between the captions and the ASR-generated transcript. Various techniques may be used to determine a similarity measure between the subtitles and the ASR generated transcript, such as an edit distance between the two, a distance between the two's embedding in the embedding space, a difference between the word packets generated for each, and the like.
A video clip in which the lips of a person speaking the target n-gram are visible may be more suitable as a dictionary use example than a video clip in which the lips of a speaker are not visible (e.g., they are not within the camera as a speaker), for example, because the person is facing the camera or at least in the field of view of the camera. Thus, in some implementations, the gaze and/or pose of the speaker detected when the speaker vocalizes out of the target n-gram can be considered in determining the metrics for the SDUE of the video.
Videos with less background noise (e.g., traffic, music, etc.) may be easier to understand and therefore better suited to dictionary use examples. Moreover, a video in which the speaker is speaking slowly and/or clearly may be more easily understood and therefore more suitable for the dictionary use example than another video in which the speaker is speaking quickly and/or unclear, and therefore, in some implementations, the detected background noise level of the candidate video clip or the measured rate of speech uttered in the video clip may be considered in determining the metric for the SDUE.
In addition to signals related to sound and/or speech quality, other signals may also be considered in various implementations. Highly popular videos may be considered more trustworthy and/or of higher quality than less popular or blurred videos, and thus may be more suitable for collecting video usage examples. This may be because, for example, a person depicted in a popular video may itself tend to be popular and/or considered trustworthy. Alternatively, this may be because popular videos are more likely to have been viewed by the user, and repeated viewing may be beneficial to learn words, as described below. Thus, in some implementations, the popularity metric of the video clip may be considered in determining the metric of the SDUE.
On an individual level, a video that a person has previously seen may be more efficient at teaching that person about the target n-gram than a video that has not been previously seen. Thus, in some implementations, the determination that a given user has previously viewed a video clip may be considered in determining the metrics for SDUE. Also on an individual level, characteristics of the individual, such as their location, demographics, gender, age, etc., may be used to determine metrics for the SDUE of the video. It is assumed that the user is located in a specific geographical area known by a specific accent, dialect or a specific spoken language. In some implementations, video clips (e.g., metrics assigned higher SDUEs) may be promoted that are more likely to include utterances of the same region-specific accent or dialect of the target n-gram (e.g., based on shooting location, story settings, etc.).
In some implementations in which a single electronic video file contains multiple instances of the spoken target n-gram, multiple video clips may be identified (or even cut), each video clip containing at least one instance of the target n-gram. These multiple video clips can then be used to compute metrics of the SDUEs relative to each other, and can then be used to select which will be used as a dictionary use example. In some such implementations, the most popular portions of the electronic video file (e.g., the portions viewed by the most people) may receive a higher metric for SDUE than the less popular/viewed portions of the video.
As previously mentioned, many n-grams have multiple meanings. In order for the video dictionary use case to be effective, it should include the n-gram in the appropriate context, i.e., the user wants to know more about the target context of the n-gram. Various techniques may be used to determine the context of an n-gram spoken in a video clip. For example, when identifying a set of candidate video clips to consider as a dictionary use example, natural language processing may be performed on the text associated with the electronic video file from which the clips are cut to identify those texts in which the target n-gram is spoken in the target context. Additionally or alternatively, in some implementations, text embedding may be generated from text associated with the electronic video file. In some such implementations, these embeddings may be applied as inputs to a trained machine learning model to generate an output. The output may be used to identify a set of candidate video clips in which the target n-gram is voiced in the target context.
In some implementations, a method may be implemented using one or more processors and may include: identifying a set of candidate video clips from a corpus of electronic video files, wherein a target n-gram is spoken in a target context in each candidate video clip of the set; for each candidate video clip in the set: comparing pre-existing manual captions associated with the candidate video clip to text generated based on speech recognition processing of an audio portion of the candidate video clip, and based at least in part on the comparison, calculating a measure of suitability of the dictionary use examples as the candidate video clip; selecting one or more of the candidate video clips from the set of candidate video clips based on a measure of suitability as a dictionary use example; and associating the one or more selected video clips with the target n-gram in the searchable database.
In various implementations, the identifying may include performing natural language processing on text associated with the electronic video file to identify those texts that uttered the target n-gram in the target context. In various implementations, the identifying may include applying text embedding generated from text associated with the electronic video file as input to a trained machine learning model to generate output, where the output is used to identify a set of candidate video clips in which a target n-gram is spoken in a target context.
In various implementations, the calculation may also be based on a gaze of a speaker in the candidate video clip detected when the speaker vocalizes the target n-gram in the target context. In various implementations, the calculation may also be based on a detected gesture in the candidate video clip at which the speaker vocalizes the target n-gram in the target context, or whether the lips of the speaker are visible in the video clip. In various implementations, the calculation may also be based on a detected background noise level of the candidate video clip or a measured speech rate of speech uttered in the candidate video clip. In various implementations, the calculation may also be based on a popularity metric of the video clip.
In various implementations, the calculation may also be based on a determination that a given user seeking information about the target n-gram has previously viewed the video clip. In various implementations, the calculation may also be based on the identity of the speaker of the target n-gram in the video clip or the identity of the staff who helped create the video clip. In various implementations, the calculation may also be based on an accent of a speaker of the target n-gram in the video clip.
In various implementations, the one or more selected video clips may include a plurality of selected video clips. In various implementations, the method further includes causing the plurality of video clips to be played one after another as a sequence. In various implementations, the method further includes causing a graphical user interface ("GUI") to be presented on the client device, wherein the GUI is operable by a user to slide through the plurality of selected video clips. The method may also include processing the audio portion of the video clip to generate text based on the speech.
In at least one further aspect, there is provided a system comprising one or more processors and a memory storing instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to: identifying a set of candidate video clips from a corpus of electronic video files, wherein a target n-gram is spoken in a target context in each candidate video clip of the set; for each candidate video clip in the set: comparing pre-existing manual captions associated with the candidate video clip to text generated based on speech recognition processing of an audio portion of the candidate video clip, and based at least in part on the comparison, calculating a measure of suitability of the dictionary use examples as the candidate video clip; selecting one or more of the candidate video clips from the set of candidate video clips based on a measure of suitability as a dictionary use example; and associating the one or more selected video clips with the target n-gram in the searchable database.
The identifying may include performing natural language processing on text associated with the electronic video file to identify those texts in which the target n-gram is spoken in the target context. The recognition may include applying text embedding generated from text associated with the electronic video file as input to a trained machine learning model to generate output, where the output is used to identify a set of candidate video clips in which a target n-gram is uttered in a target context. The calculation may also be based on a gaze of a speaker in the candidate video clip detected when the speaker vocalizes the target n-gram in the target context. The calculation may also be based on detected gestures in the candidate video clips at which the speaker vocalizes the target n-gram in the target context. The calculation may also be based on a detected background noise level of the candidate video clip or a measured speech rate of speech uttered in the candidate video clip.
In another aspect, a non-transitory computer-readable medium is provided that includes instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to: identifying a set of candidate video clips from a corpus of electronic video files, wherein a target n-gram is spoken in a target context in each candidate video clip of the set; for each candidate video clip in the set: comparing pre-existing manual captions associated with the candidate video clip to text generated based on speech recognition processing of an audio portion of the candidate video clip, and based at least in part on the comparison, calculating a measure of suitability of the dictionary use examples as the candidate video clip; selecting one or more of the candidate video clips from the set of candidate video clips based on a measure of suitability as a dictionary use example; and associating the one or more selected video clips with the target n-gram in the searchable database.
Other implementations may include a non-transitory computer-readable storage medium storing instructions executable by a processor to perform a method, such as one or more of the methods described above. Yet another implementation may include a system comprising a memory and one or more processors operable to execute instructions stored in the memory to implement one or more modules or engines that individually or collectively perform a method such as one or more of the methods described above.
It should be understood that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
FIG. 1 illustrates an environment in which various aspects of the disclosure may be implemented, according to various implementations.
Fig. 2A, 2B, and 2C depict examples of how video clips may be presented as dictionary use examples according to various implementations.
FIG. 3 depicts one example of a graphical interface that may include video clips as a dictionary use example according to various implementations.
FIG. 4 depicts a flow diagram that illustrates an example method in accordance with various implementations.
FIG. 5 schematically depicts an example architecture of a computer system.
Detailed Description
FIG. 1 illustrates an environment in which selected aspects of the present disclosure may be implemented. The example environment includes one or more client devices 106 and a knowledge system 102. The knowledge system 102 may be implemented in one or more computers (sometimes referred to as a "cloud"), for example, that communicate over a network. Knowledge system 102 is an example of an information retrieval system in which the systems, components, and techniques described herein can be implemented and/or interfaced with.
One or more users may interact with the knowledge system 102 via one or more client devices 106. Each client device 106 may be a computer coupled to the knowledge system 102 through one or more networks 110, such as a Local Area Network (LAN) or a Wide Area Network (WAN) such as the internet. Each client device 106 may be, for example, a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a wearable apparatus including a user of a computing device (e.g., a watch of a user having a computing device, glasses of a user having a computing device), and so forth. Additional and/or alternative client devices may be provided.
For example, some client devices, referred to herein as "assistant devices," may be primarily designed to allow users to interact with software processes, referred to herein as "automated assistants" (also referred to as "virtual assistants," "voice assistants," "chat robots," etc.), using free-form natural language input. As used herein, a free-form input is an input formulated by a user and is not limited to a set of options for the user to select. The assistant device may take various forms, such as a stand-alone interactive speaker, a stand-alone interactive speaker with a touch screen display, and so forth. Other client devices 106 besides assistant devices may also enable interaction with the automated assistant.
Each client device 106 and knowledge system 102 may include one or more memories for storing data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. Operations performed by one or more client devices 106 and/or knowledge system 102 may be distributed across multiple computer systems. The knowledge system 102 may be implemented as a computer program running on one or more computers at one or more locations coupled to each other via a network, for example.
Each client device 106 may operate a variety of different applications. In FIG. 1, the client device 106 includes a web browser 107, a miscellaneous application 108, and an "assistant" application 109, which "assistant" application 109 enables a user to participate in the automated assistant described above using free-form natural language input. Miscellaneous applications 108 can take various forms, including but not limited to an email client, a file transfer client (e.g., FTP, cloud drive, etc.), a proprietary application, a single or multi-language dictionary application or applet (e.g., as a feature or plug-in to a word processing application), a language translation application, a video playback application, a social networking application, and the like.
Although depicted as part of the client device 106, the assistant application 109 can be implemented to varying degrees on the client device 106 and the knowledge system 102. The assistant application 109 can provide an interface for interacting with an automated assistant (not depicted). The automated assistant can syntactically and/or semantically process free-form natural language input, such as queries, commands, etc., to determine the user's intent and/or any parameters for accomplishing that intent. The automated assistant can then attempt to fulfill the intent, for example, by searching for information responsive to the input (e.g., dictionary definitions and/or dictionary usage examples), performing an action responsive to the input (e.g., playing music, turning on a networked appliance), and so forth. In some implementations, the automated assistant can include (on the client device and/or knowledge system 102) various components not depicted in fig. 1, such as a natural language processor, an entity tagger, a speech-to-text ("STT") component (also referred to as an "automatic speech recognition" or "ASR" component), a text-to-speech ("TTS") component that generates computerized speech output from the automated assistant, and so forth.
In various implementations, the knowledge system 102 may include a search engine 120, a knowledge graph engine 124, a dictionary engine 128, a video clip engine 132, and a video crawler 136. One or more of the components 120, 124, 128, 132, and/or 136 may be combined with each other, omitted, or implemented external to the knowledge system 102. Further, one or more of the components 120, 124, 128, 132, and/or 136 may be implemented using any combination of software and hardware, and may be implemented on a single computer or across multiple computers, similar to the knowledge system 102 itself.
The search engine 120 may maintain an index 122 for use by the knowledge system 102. The search engine 120 may process the documents and update the index entries in the index 122, e.g., using conventional and/or other indexing techniques. For example, search engine 120 may crawl one or more resources, such as the world Wide Web, and index documents accessed via such crawlers. As another example, search engine 120 may receive information related to one or more documents from one or more resources, such as a website administrator that controls the documents, and index the documents based on the information. A document is any data associated with a document address. Documents include web pages, word processing documents, Portable Document Format (PDF) documents, images, emails, calendar entries, videos, and network feeds, among others. Each document may include content such as, for example: text, images, video, sound, embedded information (e.g., meta information and/or hyperlinks); and/or embedded instructions (e.g., ECMAScript implementations such as JavaScript).
In various implementations, the knowledge graph engine 124 can maintain an index 126 that stores a knowledge graph that includes nodes representing various entities and weighted edges representing relationships between the entities. For example, a "banana" node may be connected (e.g., as a child node) to a "fruit" node, which in turn may be connected (e.g., as a child node) to a "product" and/or "food" node. As another example, a restaurant called "Hypothetical Caf" may be represented by a node that also includes attributes such as its address, the type of food served, business hours, contact information, and the like. In some implementations, a "contextual Caf" node may be connected by an edge (e.g., representing a child-parent relationship) to one or more other nodes, such as a "retaurant" node, a "bussiness" node, a node representing the city and/or state in which the restaurant is located, and so forth. Such a knowledge graph can be built, for example, by a crawler multiple databases, online encyclopedias, etc., to accumulate nodes representing entities and edges representing relationships between the entities. In some implementations, the knowledge graph can be populated with information regarding similarities and/or differences between entities in the document.
The dictionary engine 128 may be configured to perform selected aspects of the present disclosure to enable access to dictionary definitions and other associated information stored in the dictionary index 130. For example, as described above, in addition to definitions, dictionary index 130 may store other information about words and/or phrases, including but not limited to synonyms, antonyms, phonetic information, word source information, and/or use examples. In particular, use of examples may be very beneficial to individuals (sometimes referred to herein as "users") who attempt to understand how a given word or phrase is used in various contexts. Usage examples stored in dictionary index 130 (and accessible through dictionary engine 128) may include usage examples extracted from documents, newspapers, etc., as well as audio and/or video usage examples obtained and/or generated using the techniques described herein, including techniques performed by video clip engine 132 and/or video crawler 136.
The video clip engine 132 may be configured to store a plurality of video clips selected/generated from an electronic video file in the video clip index 134 to be available as dictionary use examples. For example, video clip engine 132 may store one or more video clips in which a target n-gram is spoken in a video clip index 134 associated with the target n-gram. In some implementations, the video clip engine 132 may be implemented as part of the dictionary engine 128 or in conjunction with the dictionary engine 128, and video clips that may otherwise be stored in the video clip index 134 may be stored with other dictionary information in the dictionary index 130.
The video crawler 136 may be configured to crawl various online repositories of electronic video files to identify, retrieve, extract, cut, and/or generate video clips containing utterances of the target n-grams. These video clips may then be provided to the video clip engine 132, which the video clip engine 132 may store in the video clip index as previously described. As used herein, an "electronic video file" may refer to any digital file that stores synchronized video and audio data. Electronic video files may be in a variety of formats including, but not limited to, audio video interleave ("AVI"), Flash video format ("FLV"), mathska ("MKV"), (r;), (r);) and (r);) and (r); (r),
For example, the video crawler 136 may be configured to interface with one or more video systems 140 and obtain video clips from the one or more video systems 140. The video system 140 may be configured to store, maintain, distribute (e.g., stream) and/or index electronic video files stored in a video file database (or "corpus") 144. In many cases, the video system 140 can include a video search engine 142 that is configured to receive search queries, for example, from the browser 107, the miscellaneous application(s) 108, and/or the automated assistant 109, and a search index 144 for responsive video files. In some implementations, the video search engine 142 may provide the responsive video files for streaming playback, for example, in the browser 107 or another application.
The video crawler 136 may be configured to perform selected aspects of the present disclosure to identify, generate, cut/extract, and/or obtain video clips containing utterances of the target n-grams from electronic video files stored in a video repository, such as the video file index 144. The video crawler 136 may perform these operations offline (e.g., during downtime or low network traffic hours, midnight, etc.), batch, and/or on-demand (e.g., in response to a user request for a dictionary use case or a user request for a dictionary definition). As previously described, as used herein, a "video clip" or "video clip" may comprise an entire electronic video file or a portion of an electronic video file. For example, a video clip containing an utterance of the target n-gram may be cut or extracted from a longer electronic video file containing other extraneous content unrelated to the target n-gram. The video clips provided by the video crawler 136 may be stored in the video clip index 134 along with dictionary definitions and/or other dictionary information stored in the dictionary index 130. This information may be accessed, for example, by the dictionary engine 128 and/or the video clip engine 132 as searchable databases, or by any other means.
In various implementations, the video crawler 136 may also obtain text data associated with the electronic video files from which it crawls using the techniques described herein, e.g., as metadata. These text data may convey spoken dialog and other sound(s) contained in the electronic video file. These text data are typically used to present subtitles and/or captions on the screen when playing a video file. Subtitles may include spoken dialog and commentary may include dialog and may also include other sounds contained in the video file, such as music, sound effects, and the like. In many implementations, the video crawler may store these text data in association with video clips stored in video clip index 134. Thus, video clips stored in the index 134 may be searched using these text data.
In this specification, the terms "database" and "index" will be used broadly to refer to any collection of data. The database and/or indexed data need not be structured in any particular way, and it may be stored on storage devices in one or more geographic locations. Thus, for example, indexes 122, 126, 130, 134, and 144 may include multiple data sets, each of which may be organized and accessed differently.
In some implementations, video crawler 136 and/or other components of knowledge system 102 (or even video system 140 in some cases) may be configured to identify (e.g., obtain, cut, generate) a set of candidate video clips from a corpus of electronic video files, such as video file index 144, in which a target n-gram is spoken in a target context. Assume that the target n-gram is "expiate", meaning "redemption". The video crawler 136 may search the index 144 for text data associated with the electronic video file or may request the video search engine 142 to search the text data to identify an electronic video file containing an utterance in the target n-gram.
In some implementations, the video crawler 136 and/or the previously described automated assistant may perform natural language processing on the text data associated with the electronic video file to identify those text data that uttered the target n-gram in the target context. For example, the unary grammar "test" may be a noun or a verb, so a video clip using "test" as a noun may not fit into a use example of a verb definition as "test". For example, a natural language process may be used to tag an n-gram with parts of speech so that videos in which "test" is used as a verb may be easily identified and other videos in which "test" is used as a noun may be excluded from the set of candidate video clips.
Additionally or alternatively, in some implementations, the video crawler 136 may apply text embedding generated from text data associated with the electronic video files and/or video clips extracted therefrom as input to a trained machine learning model to generate output. The output may then be used to identify a set of candidate video clips in which the target n-gram was spoken in the target context.
For example, a superset of video clips where the target n-gram is voiced in any context may be identified, for example, by simple text matching. Then, a distance may be determined in the embedding space between text embedding generated from text data associated with the superset of identified video clips and embedding generated using the target form of the target n-gram (e.g., noun-to-verb). For example, these distances may be used to determine which identified video clips in the superset should be selected for inclusion in the subset of candidate dictionary use examples of the target n-gram, and/or to rank the subset of video clips. In some such implementations, the deep neural network may be trained to classify video clips and/or electronic video files based on these text-embedded and/or other text-based signals. The neural network may take various forms, such as a feedforward neural network, a recurrent neural network ("RNN"), a long short term memory ("LSTM") network, a gated recursive unit ("GRU") network, a transform network, and so forth.
In some implementations, for each candidate video clip in the set of utterances that includes the n-gram "expire," the video crawler 136 or another component can compare text data (e.g., such as pre-existing manual subtitles associated with the candidate video clip) to text generated based on automatic speech recognition ("ASR") processing of the audio portion of the candidate video clip. In some implementations, based at least in part on the comparison and/or based at least in part on other signals described herein, the video crawler 136 may calculate a measure of suitability of the candidate video clip as an example of dictionary use ("SDUE"). If the subtitles of the video clip are determined to be similar to text generated using ASR processing of the audio of the video clip, the subtitles are likely to accurately convey what is spoken in the video clip. It also shows that the audio portion of the video is sufficiently clear and intelligible, with appropriate cadence, announcement, pronunciation, etc., that ASR processing can accurately generate text based on spoken dialog in the video clip.
Once the metrics for SDUE are calculated for the set of candidate video clips, one or more candidate video clips may be selected from the set of candidate video clips, for example, by video crawler 136 based on the metrics for SDUE. The selected video clip can be associated with a target grammar, for example, in a searchable database such as the video clip index 134 and/or in the dictionary index 130.
Metrics for SDUE may be computed, for example, by video crawler 136 and/or other components described herein, based on signals and/or factors other than (or in addition to) the comparison of subtitles to ASR processing outputs described above. For example, in some implementations, metrics for the SDUE may be calculated based on gaze and/or gestures in the video clip that were detected when the speaker vocalized the target n-gram in the target context. Intuitively, it would be right if the user were able to read the speaker's lips, the user might more easily understand the speaker in the video, if the speaker's gaze was directed towards the camera, or at least towards some point near the camera (which is common in interviews). Additionally or alternatively, the speaker's posture may be considered, for example, to determine that the speaker is standing still while speaking, indicating that the speaker may be presenting. In some implementations, if a speaker is detected behind the podium, this may be an aggressive signal, indicating that the speaker may receive a relatively strong measure of SDUE speaking.
Even if the speaker's lips are not visible to the user, for example, when the user is using a non-display assistant device, the speaker's utterance of words may be clearer and more understandable in a camera-oriented video clip of the speaker, for example, because the speaker is facing the boom microphone while speaking. Additionally or alternatively, the camera-facing speaker is more likely to make greater efforts to clearly announce and/or speak at an understandable rhythm, for example, because he or she knows that the content they are saying will be viewed later. Furthermore, a speaker facing the camera (and/or standing behind the podium) may speak a pre-written conversation from the prompter rather than an ad hoc utterance. Pre-written conversations may be easier to understand when read aloud and are suitable for use as examples of dictionary use.
As another example, the metric for SDUE may be calculated, for example, by video crawler 136 based on a detected background noise level of the candidate video clip or a measured speech rate of speech uttered in the candidate video clip. If the speaker is in a noisy environment, such as a music venue or a sports stadium, the speaker's voice may be more difficult to distinguish from background noise. In contrast, a person speaking in a recording studio without background noise is more easily understood, and thus, the words he speaks are suitable as an example of the use of a dictionary. And relatively fast speech rates may be relatively difficult to understand, especially as compared to relatively slow speech rates. Thus, a video clip featuring a slow conversation may be promoted over a video clip featuring a faster conversation.
In general, if a user has previously viewed a video clip, the user is more likely to be able to understand and/or consider spoken dialog in the video at a subsequent viewing time because the user has previously viewed. Thus, in some implementations, metrics for SDUEs of a video clip may be calculated based on a determination that a given user seeking information about a target n-gram has previously viewed the video clip. Similarly, popular video clips are more likely to be viewed by a wide range of users in general, or are at least well known in the current cultural age spirit. Thus, those video clips may be more likely to include conversations suitable for use as examples of dictionary usage than blurry video clips (e.g., determined by number of views, number of likes, number of comments, etc.). Even if a particular user has not seen popular video before, they may know it in general and thus may be more ready or "at issue" to think and learn the spoken dialog contained therein.
In some implementations, metrics for the SDUE of the video clip may be calculated, for example, by the video crawler 136 based on the identity of the speaker of the target n-gram in the video clip, or based on the identity of the staff or other person who helped create the video clip. Assume that the user is a super fan of a particular actor. The user is more likely to have a video clip of the scene from which the actor uttered various conversations. Further, the user is more likely to be interested in video clips that are starring at the actor. Thus, if video clips can be found in which the actor utters a target n-gram of interest to the user, these video clips can be promoted over other video clips, e.g., due to the metric being assigned a larger SDUE.
In some implementations, metrics for the SDUE of the video clip may be calculated, for example, by video crawler 136 based on the accent or dialect of the speaker of the target n-gram in the video clip. Assume that the user is located in a particular area where a particular accent or dialect prevails. The user may wish to be able to talk as much as possible with the local dialect/accent. Thus, a video clip in which the target n-gram is spoken in a local accent and/or dialect may be promoted over other video in a different accent and/or dialect.
Fig. 2A-2C depict examples of interactions between a user 101 and an automated assistant (not shown) implemented at least in part on a client device 206. In fig. 2A-2C, the client device 206 takes the form of an assistant device having a speaker, a microphone, and a touch screen. The user 101 may interact with the automated assistant by speaking commands/queries to the client device 206, or by interacting with their touchscreen.
In FIG. 2A, the user 101 invokes the automated Assistant by saying "Hey Assistant". This may cause the automated assistant to begin "listening" to anything the user 101 next speaks, for example, by beginning STT processing to be performed on the utterance captured by the microphone of the client device 206. The user 101 then asks, "What does 'textual' mean? "automated assistant audibly answers the track," 'graphical' means 'slow-moving, smooth, elementary' ". Although not depicted in fig. 2A, in some implementations, all or part of the response of the automated assistant can be presented on a touch screen.
The user 101 then requests a dictionary use example of the target n-gram, asking "can you use that is in a content? The "automated assistant responds by playing a video clip selected in real time by the video clip engine 132 or the dictionary engine 128 based on the metrics of the high SDUE or previously associated with the target n-gram. In FIG. 2A, the feature of the video clip is that speaker 248 has a tired or tired appearance, uttering the sentence "ugh … I couldn't slide at all last bright and I'm feeling super textual right now". The subtitles 250 are presented simultaneously with the spoken dialog and the target n-gram is visually emphasized (and may be selected as a hyperlink to navigate to another interface). This particular video clip can be presented first because it is assigned a relatively strong metric of SDUE. This relatively strong measure of SDUE may be due to the fact that speaker 248 is visible to the camera, the speaker's lips, for example, and because the meaning of the n-gram is made very clear using the context of "phlegmatic". Furthermore, there does not appear to be much background noise in the video clip.
In some implementations, multiple video clips containing the utterance of the target n-gram may be played as a sequence one after another as required until the user 101 instructs the automated assistant to stop, or until all available videos or videos with a threshold metric of SDUE have been played. For example, in FIG. 2B, a second video clip is played in which an infant 252 is depicted and an off-screen speaker says, "She did't sleep well last night light so's active textual this morning". Likewise, the context of the term "phlegmatic" is closely related to and explains its definition. For this reason only, the video clip of fig. 2B may also receive a relatively strong metric for SDUE. However, for various reasons, its SDUE metric may be slightly weaker than the video played in fig. 2A. The speaker of fig. 2B is off-screen so their lips are not visible, and there is less likelihood that the speaker is intentionally speaking loudly and clearly, because they are not intentionally facing the camera or another point near the camera (e.g., as is common in interviews).
In FIG. 2C, a third video clip is played, where a person 254 in a noisy environment such as a music venue vocalizes the statement, "I don't have had recent graphical means! | A | A "this statement does not provide much, if any, context about the meaning of phlegmatic, and much of the content spoken by the person 254 is rendered inaudible by background music as indicated in the speech balloon. Person 254 is also not facing the camera, although their lips are visible. In addition, the presence of background music makes it less likely that the viewer will understand the person 254. All these signals may add up such that the third video clip is assigned a weaker metric of SDUE than the first and second video clips of fig. 2A and 2B, respectively.
Additionally or alternatively, in some implementations, if the speaker is detected as singing the target n-gram, rather than it, this may affect the metrics of the SDUE of the video clip. Singing may be detected, for example, by detecting that the pitch of the speaker's voice varies more than is normally observed when the speaker is speaking normally. Additionally or alternatively, if prosody and/or pitch alignment is detected between the speaker's voice and the background music of the video clip, this may prove that the speaker is singing rather than speaking the target n-gram.
Fig. 3 depicts an example client device 306 in the form of a smartphone or tablet. The client device 306 includes a touchscreen 360. A graphical user interface ("GUI") including a search bar 362 is presented on the touch screen 360. In this example, a user (not shown) has entered the term "vituperete" in the search bar 362. Response result 364 includes a definition of the term: "blank or intult (someone) in string or vitamin language". Response results 364 also include examples of the use of electronic printing:
"Heat through the connecting the lower issues or the stringgths, most model peptides of the pipeline of the upper nodes of the oppressions". Similar (Similar) n-grams, such as "repeat", "rail against", and "attack", are also provided as part of response result 364.
At the bottom, two video clips 366A-B are provided, in which the target n-gram "vituperate" is spoken. In various implementations, the video clips may be presented in an order selected (e.g., ranked) based on the metrics of their respective SDUEs. For example, a first video clip 366A includes the same use case as the printed case described above, and is placed on the far left as a way to promote it over the other video clips on the right. In various implementations, the user can slide through multiple video clips 366A-B (and possibly further to the right, not visible in FIG. 3) and pick individual clips to view. In some implementations, a graphical element 368 such as an arrow or other similar symbol may also operate to scroll the video. In some implementations, the user may issue a voice command, such as "next clip" or "scroll right," to scroll and/or play more video clips. In some implementations, for each video clip, for example, a subtitle is presented below the video when the corresponding dialog is spoken. In some implementations, including the implementation of fig. 3, the target n-gram may be highlighted or otherwise visually distinguished, and in some cases may be operable (e.g., as a hyperlink) to navigate to another interface that provides more information about the n-gram, a video clip from which the n-gram was spoken, and so forth.
Although video clips and electronic video files are described herein to obtain examples of dictionary use, this is not meant to be limiting. The techniques described herein may be used to generate dictionary use examples in and/or from data in other formats, such as audio files. For example, a user interacting with the non-display assistant device may still wish to learn more about the target n-gram, including hearing an audio clip of the n-gram spoken in the target context. Thus, the techniques described herein may be used to extract audio clips from audio files (or from video files with audio tracks), where the audio clips contain spoken dialog of a target n-gram.
Referring now to fig. 4, one example method 400 for practicing aspects of the present disclosure is described. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include various components of various computer systems, including various classifiers, engines, and/or client applications described herein. Further, while the operations of method 400 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 402, the system, e.g., via video crawler 136, may identify a set of candidate video clips from a corpus of electronic video files, such as index 144, in which a target n-gram is spoken in a target context. These candidate video clips may be indexed in index 144 by an n-gram, such as found in their accompanying text data (e.g., subtitles) and/or STT output generated from the audio portion of the video clip. Thus, these text data may be used by, for example, the video crawler 136 to identify a subset of candidate video clips.
In some implementations, the system can cut out or extract the portion of the entire electronic video file that vocalizes the target n-gram as a video clip. These portions of the entire electronic video file may be identified and/or depicted in various ways-e.g., ensuring that the resulting video clip includes a target n-gram that is spoken as a complete sentence and/or as part of a sufficient context. In some implementations, subtitles and/or STT output generated with and/or from a video file may be analyzed to identify punctuation (e.g., periods, commas) or other signals (e.g., line breaks in subtitles, audible pauses in spoken dialog) that represent logical breaks in spoken dialog. Various heuristics may be employed to identify signals such as text timing, punctuation, capitalization, time of occurrence, etc., any of which may be used to cut out from an electronic video file a portion of a video clip containing an utterance in the target n-gram.
Referring back to FIG. 4, at block 404, the system may determine whether there are more candidate video clips in the set. If the answer is in the affirmative, the method 400 may proceed to block 406, at which point the next video clip in the set may be selected for analysis.
At block 408, the system may compare the pre-existing manual subtitles associated with the selected candidate video clip to text generated based on speech recognition processing (i.e., STT) of the audio portion of the candidate video clip, such that at block 410, the system may calculate a metric of SDUE of the video clip. As previously described, the text generated from the STT processing may already exist for the video clip, or it may be generated as needed, for example, when analyzing the video clip to determine its SDUE metrics.
In various implementations, the calculation of block 408 may be further or alternatively based on other signals described herein, such as whether the speaker is facing a camera, background noise in the video clip, popularity of the video clip and/or the actors/play members, and so forth. Other signals are also contemplated herein. For example, in some implementations, the video clip may be submitted by the user to, for example, the dictionary engine 128, particularly for use as a dictionary use example. In some such implementations, the fact that the user submits the video clip for this purpose may have a positive impact on the metrics of their SDUE. Intuitively, a video clip submitted by a user specifically for use as a dictionary use example is likely to be suitable for that use.
Additionally or alternatively, the user feedback may affect a metric of SDUE calculated for the video clip. If one or more users provide positive feedback (e.g., "like") when presenting a particular video clip as a dictionary use example, the positive feedback can increase the metric strength of the SDUE that the video clip is moving forward. Similarly, negative feedback may reduce the video clip's metric for SDUEs moving forward.
The feedback need not be explicit. Assume that multiple users view multiple video clips of a target n-gram sequence of video clips and that all those users stop viewing after a particular video clip. This may indicate that the last video clip of the sequence viewed by the user is particularly effective in teaching how the target n-gram should be used. This video clip may receive the stronger SDUE's metrics to move forward and may in many cases be promoted over (e.g., previously presented) other video clips of the sequence. Likewise, if multiple users tend to watch at least one additional video clip after a particular video clip, and few or no users stop after the particular video clip, this may suggest that the particular video clip is not a suitable dictionary use example.
At optional block 412, the system may determine whether the metric of SDUE calculated for the selected candidate video clip at block 410 satisfies some minimum threshold. If the answer is no, then at block 414, the selected candidate video clip may be discarded or otherwise excluded from the set, and the method 400 may return to block 404. However, if the answer at block 412 is yes, then at block 416, the selected candidate video clip may remain in the set as a candidate, and the method 400 may return to block 404.
If there are no more candidate video clips in the set at block 404, the system may select one or more of the candidate video clips from the set of candidate video clips based on the metrics of their respective SDUEs at block 418. At block 420, the system may associate the one or more video clips selected at block 420 with a target n-gram in a searchable database (e.g., video clip index 134). In some implementations, at block 422 (which may occur, for example, sometime while the user is looking for information about the target n-gram), the system may cause the video clips selected at block 420 to be output to the user, e.g., one by one (fig. 2A-C), as a slideable list (fig. 3), as audio-only output, and so on.
Fig. 5 is a block diagram of an example computer system 510. Computer system 510 typically includes at least one processor 514 that communicates with a number of peripheral devices via a bus subsystem 512. These peripheral devices may include a storage subsystem 526, including for example, memory subsystem 525 and file storage subsystem 526, user interface output device 520, user interface input device 522, and network interface subsystem 516. The input and output devices allow a user to interact with computer system 510. Network interface subsystem 516 provides one or more network interfaces to an external network and couples to corresponding interface devices in other computer systems.
The user interface input devices 522 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information to computer system 510 or on a communication network.
User interface output devices 520 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visual image. The display subsystem may also provide non-visual displays, such as via audio output devices. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computer system 510 to a user or to another machine or computer system.
These software modules are typically executed by processor 514 alone or in combination with other processors. Memory 525 used in storage subsystem 526 may include a number of memories, including a main Random Access Memory (RAM)530 for storing instructions and data during program execution and a Read Only Memory (ROM)532 for storing fixed instructions. File storage subsystem 526 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. Modules implementing the functionality of some implementations may be stored by file storage subsystem 526 in storage subsystem 526, or in other machines accessible to processor 514.
Although several implementations have been described and illustrated herein, various other means and/or structures for performing the function and/or obtaining the result and/or one or more of the advantages described herein may be utilized, and each such variation and/or modification is considered to be within the scope of the implementations described herein. More generally, all parameters, dimensions, materials, and configurations described herein are exemplary and the actual parameters, dimensions, materials, and/or configurations will depend on the specific application for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, implementations may be practiced otherwise than as specifically described and claimed. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (20)
1. A method implemented using one or more processors, comprising:
identifying a set of candidate video clips from a corpus of electronic video files, wherein a target n-gram is spoken in a target context in each candidate video clip of the set;
for each candidate video clip in the set:
comparing pre-existing manual subtitles associated with the candidate video clip to text generated based on speech recognition processing of an audio portion of the candidate video clip, and
calculating a measure of suitability as a dictionary use case for the candidate video clip based at least in part on the comparison;
selecting one or more of the candidate video clips from the set of candidate video clips based on a measure of suitability as a dictionary use example; and
associating the selected one or more video clips with the target n-gram in a searchable database.
2. The method of claim 1, wherein the identifying further comprises performing natural language processing on text associated with the electronic video file to identify text that uttered the target n-gram in the target context.
3. The method of claim 1 or 2, wherein the identifying further comprises applying text embedding generated from text associated with the electronic video file as input to a trained machine learning model to generate an output, wherein the output is used to identify a set of candidate video clips in which the target n-gram is voiced in the target context.
4. The method of any of the preceding claims, wherein the calculating is further based on a gaze of a speaker in the candidate video clip detected when the speaker vocalizes the target n-gram in the target context.
5. The method of any of the preceding claims, wherein the calculating is further based on a pose of a speaker in the candidate video clip detected when the speaker vocalizes the target n-gram in the target context.
6. The method of any preceding claim, wherein the calculating is further based on a detected background noise level of the candidate video clip or a measured speech rate of speech uttered in the candidate video clip.
7. The method of any preceding claim, wherein the calculating is further based on a popularity metric of the candidate video clips.
8. The method of any of the preceding claims, wherein the calculating is further based on a determination that a given user seeking information about the target n-gram has previously viewed the candidate video clip.
9. The method of any of the preceding claims, wherein the calculating is further based on an identity of a speaker of the target n-gram in the candidate video clip or an identity of a worker assisting in creating the candidate video clip.
10. The method of any of the preceding claims, wherein the calculating is further based on an accent of a speaker of a target n-gram in the candidate video clip.
11. The method of any preceding claim, wherein the one or more selected video clips comprise a plurality of selected video clips.
12. The method of claim 11, wherein the method further comprises causing the plurality of video clips to be played sequentially one after another.
13. The method of claim 11 or 12, wherein the method further comprises causing a Graphical User Interface (GUI) to be presented on the client device, wherein the GUI is operable by a user to slide through a plurality of selected video clips.
14. A system comprising one or more processors and memory storing instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to:
identifying a set of candidate video clips from a corpus of electronic video files, wherein a target n-gram is spoken in a target context in each candidate video clip of the set;
for each candidate video clip in the set:
comparing pre-existing manual captions associated with the candidate video clip to text generated based on speech recognition processing of an audio portion of the candidate video clip, and
calculating a measure of suitability as a dictionary use case for the candidate video clip based at least in part on the comparison;
selecting one or more of the candidate video clips from the set of candidate video clips based on a measure of suitability as a dictionary use example; and
associating one or more selected video clips with the target n-gram in a searchable database.
15. The system of claim 14, wherein the identifying further comprises performing natural language processing on text associated with the electronic video file to identify text that uttered the target n-gram in the target context.
16. The system of claim 14 or 15, wherein the identifying further comprises applying text embedding generated from text associated with the electronic video file as input to a trained machine learning model to generate an output, wherein the output is used to identify a set of candidate video clips in which the target n-gram is vocalized in the target context.
17. The system of any of claims 14-16, wherein the calculating is further based on a gaze of a speaker in the candidate video clip detected when the speaker vocalizes the target n-gram in the target context.
18. The system of any of claims 14-17, wherein the calculating is further based on a pose of a speaker in the candidate video clip detected when the speaker vocalizes the target n-gram in the target context.
19. The system of any of claims 14-18, wherein the calculating is further based on a detected background noise level of the candidate video clip or a measured speech rate of speech uttered in the candidate video clip.
20. At least one non-transitory computer-readable medium comprising instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to:
identifying a set of candidate video clips from a corpus of electronic video files, wherein a target n-gram is spoken in a target context in each candidate video clip of the set;
for each candidate video clip in the set:
comparing pre-existing manual subtitles associated with the candidate video clip to text generated based on speech recognition processing of an audio portion of the candidate video clip, and
calculating a measure of suitability as a dictionary use case for the candidate video clip based at least in part on the comparison;
selecting one or more of the candidate video clips from the set of candidate video clips based on a measure of suitability as a dictionary use example; and
associating one or more selected video clips with the target n-gram in a searchable database.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/059721 WO2021091526A1 (en) | 2019-11-04 | 2019-11-04 | Using video clips as dictionary usage examples |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114846540A true CN114846540A (en) | 2022-08-02 |
Family
ID=68699528
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980103316.4A Pending CN114846540A (en) | 2019-11-04 | 2019-11-04 | Using video clips as dictionary use examples |
Country Status (4)
Country | Link |
---|---|
US (1) | US20220405478A1 (en) |
EP (1) | EP4049270B1 (en) |
CN (1) | CN114846540A (en) |
WO (1) | WO2021091526A1 (en) |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1843036A (en) * | 2003-08-25 | 2006-10-04 | 皇家飞利浦电子股份有限公司 | Real-time media dictionary |
US9191639B2 (en) * | 2010-04-12 | 2015-11-17 | Adobe Systems Incorporated | Method and apparatus for generating video descriptions |
US11140450B2 (en) * | 2017-11-28 | 2021-10-05 | Rovi Guides, Inc. | Methods and systems for recommending content in context of a conversation |
-
2019
- 2019-11-04 CN CN201980103316.4A patent/CN114846540A/en active Pending
- 2019-11-04 EP EP19809965.7A patent/EP4049270B1/en active Active
- 2019-11-04 US US17/774,460 patent/US20220405478A1/en active Pending
- 2019-11-04 WO PCT/US2019/059721 patent/WO2021091526A1/en unknown
Also Published As
Publication number | Publication date |
---|---|
WO2021091526A1 (en) | 2021-05-14 |
EP4049270B1 (en) | 2023-10-04 |
EP4049270A1 (en) | 2022-08-31 |
US20220405478A1 (en) | 2022-12-22 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11238854B2 (en) | Facilitating creation and playback of user-recorded audio | |
US9548052B2 (en) | Ebook interaction using speech recognition | |
US11836183B2 (en) | Digital image classification and annotation | |
JP6726354B2 (en) | Acoustic model training using corrected terms | |
EP3736807A1 (en) | Apparatus for media entity pronunciation using deep learning | |
US10606453B2 (en) | Dynamic system and method for content and topic based synchronization during presentations | |
US20150356971A1 (en) | Modification of visual content to facilitate improved speech recognition | |
US20200151220A1 (en) | Interactive representation of content for relevance detection and review | |
US20220121712A1 (en) | Interactive representation of content for relevance detection and review | |
Skidmore | Incremental disfluency detection for spoken learner english | |
JP7481488B2 (en) | Automated Assistants Using Audio Presentation Dialogue | |
EP4049270B1 (en) | Using video clips as dictionary usage examples | |
Riedhammer | Interactive approaches to video lecture assessment | |
US20210103851A1 (en) | Rehearsal-based presentation assistance | |
Akita et al. | Language model adaptation for academic lectures using character recognition result of presentation slides | |
US11983217B2 (en) | Responding to queries with voice recordings | |
EP3910626A1 (en) | Presentation control | |
US20230009983A1 (en) | Responding to queries with voice recordings | |
Racca | Spoken content retrieval beyond pipeline integration of automatic speech recognition and information retrieval | |
Kanevsky et al. | Speech transformation solutions | |
Satink | The adaptive presentation assistant using grammar-based recognition to support the process of presenting |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |