CN104756473B - Handle concurrent voice - Google Patents
Handle concurrent voice Download PDFInfo
- Publication number
- CN104756473B CN104756473B CN201380053617.3A CN201380053617A CN104756473B CN 104756473 B CN104756473 B CN 104756473B CN 201380053617 A CN201380053617 A CN 201380053617A CN 104756473 B CN104756473 B CN 104756473B
- Authority
- CN
- China
- Prior art keywords
- voice
- participant
- output
- session
- overlapping
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/56—Arrangements for connecting several subscribers to a common circuit, i.e. affording conference facilities
- H04M3/568—Arrangements for connecting several subscribers to a common circuit, i.e. affording conference facilities audio processing specific to telephonic conferencing, e.g. spatial distribution, mixing of participants
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B17/00—Surgical instruments, devices or methods, e.g. tourniquets
- A61B17/32—Surgical cutting instruments
- A61B17/3203—Fluid jet cutting instruments
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B17/00—Surgical instruments, devices or methods, e.g. tourniquets
- A61B17/32—Surgical cutting instruments
- A61B17/3205—Excision instruments
- A61B17/3207—Atherectomy devices working by cutting or abrading; Similar devices specially adapted for non-vascular obstructions
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B17/00—Surgical instruments, devices or methods, e.g. tourniquets
- A61B17/32—Surgical cutting instruments
- A61B17/3205—Excision instruments
- A61B17/3207—Atherectomy devices working by cutting or abrading; Similar devices specially adapted for non-vascular obstructions
- A61B17/320725—Atherectomy devices working by cutting or abrading; Similar devices specially adapted for non-vascular obstructions with radially expandable cutting or abrading elements
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B17/00—Surgical instruments, devices or methods, e.g. tourniquets
- A61B17/32—Surgical cutting instruments
- A61B17/3209—Incision instruments
- A61B17/3211—Surgical scalpels, knives; Accessories therefor
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B17/00—Surgical instruments, devices or methods, e.g. tourniquets
- A61B17/50—Instruments, other than pincettes or toothpicks, for removing foreign bodies from the human body
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B18/00—Surgical instruments, devices or methods for transferring non-mechanical forms of energy to or from the body
- A61B18/18—Surgical instruments, devices or methods for transferring non-mechanical forms of energy to or from the body by applying electromagnetic radiation, e.g. microwaves
- A61B18/20—Surgical instruments, devices or methods for transferring non-mechanical forms of energy to or from the body by applying electromagnetic radiation, e.g. microwaves using laser
- A61B18/22—Surgical instruments, devices or methods for transferring non-mechanical forms of energy to or from the body by applying electromagnetic radiation, e.g. microwaves using laser the beam being directed along or through a flexible conduit, e.g. an optical fibre; Couplings or hand-pieces therefor
- A61B18/24—Surgical instruments, devices or methods for transferring non-mechanical forms of energy to or from the body by applying electromagnetic radiation, e.g. microwaves using laser the beam being directed along or through a flexible conduit, e.g. an optical fibre; Couplings or hand-pieces therefor with a catheter
- A61B18/245—Surgical instruments, devices or methods for transferring non-mechanical forms of energy to or from the body by applying electromagnetic radiation, e.g. microwaves using laser the beam being directed along or through a flexible conduit, e.g. an optical fibre; Couplings or hand-pieces therefor with a catheter for removing obstructions in blood vessels or calculi
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B90/00—Instruments, implements or accessories specially adapted for surgery or diagnosis and not covered by any of the groups A61B1/00 - A61B50/00, e.g. for luxation treatment or for protecting wound edges
- A61B90/02—Devices for expanding tissue, e.g. skin tissue
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61N—ELECTROTHERAPY; MAGNETOTHERAPY; RADIATION THERAPY; ULTRASOUND THERAPY
- A61N1/00—Electrotherapy; Circuits therefor
- A61N1/02—Details
- A61N1/04—Electrodes
- A61N1/05—Electrodes for implantation or insertion into the body, e.g. heart electrode
- A61N1/056—Transvascular endocardial electrode systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L65/00—Network arrangements, protocols or services for supporting real-time applications in data packet communication
- H04L65/40—Support for services or applications
- H04L65/403—Arrangements for multi-party communication, e.g. for conferences
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B17/00—Surgical instruments, devices or methods, e.g. tourniquets
- A61B17/32—Surgical cutting instruments
- A61B2017/320044—Blunt dissectors
Abstract
A kind of system has one or more processors and memory, receives the speech data of the first and second participants from session.The system exports the voice of first participant.When the first predetermined threshold amount of the end section of the voice voice less than first participant overlapping in time of second participant, the system exports the voice of second participant according to the adjustment of the voice of the participant of session.When the first predetermined threshold amount of the end section of the voice voice more than first participant overlapping in time of second participant, the system abandons the voice of second participant.Alternatively, the system by postpone second participant voice output come the voice of the participant that adjusts session.
Description
Background technology
The disclosed embodiments are usually directed to distributed client-server computer network system, and specifically related to
System and method for handling concurrent voice.Daily, hundreds of meeting, particularly videoconference can all be held.Unlike
Those on-the-spot meetings, the attendant of videoconference generally participates in and talked from diverse geographic location.But, due to lacking visual line
Rope and other reasonses, the attendant of videoconference more likely talk at the same time or at about, cause simultaneously, concurrently or with it
The overlapping speech of his mode.Although adding the possibility of the overlapping speech, still the user of videoconference expects they and its
His user communication is ordered into and effectively.
In conference system, when many attendants talk at the same time or at about, the speech of talker may prolong
It will not be audible in long period, until all " early " speeches have been output.The delay of extension typically result in user's setback and
Confusion, because talker attempts to tell or determine again whether conference system wrong or loss of data.When frequently quilt of talking
When interrupting, the situation deteriorates, and must be repeated many times over that the validity of conference system by totally tansitive, could be reduced.
The content of the invention
Being reduced or eliminated by disclosed system and method described below above-mentioned is used to handle concurrent voice
The problem of conventional method.
In most cases there is provided selectively eliminate concurrent voice but still meet user be properly handled its speech
Or the system and method for the perception of transmission are favourable.Particularly, the attribute of the speech based on other one or more participants
Come the output of the speech that selectively adjusts particular participant, the validity of conference system can be increased, without with it is serial first
Enter first to go out the adjoint cost that mode broadcasts a large amount of participant's speeches.
In certain embodiments, a kind of method is performed at server system, the server system has one or more
Processor and memory, the one or more programs of memory storage, for by one or more of computing devices, with
Just methods described is performed.Methods described includes：Speech data is received from the first participant of session, is participated in from the second of the session
Person receives speech data, and exports the voice of first participant.Methods described further comprises：When the voice of second participant
It is overlapping in time with the end section of the voice of first participant when being less than the first predetermined threshold amount, according to the ginseng to the session
Adjustment with the voice of person exports the voice of second participant, and when the voice and the voice of first participant of second participant
End section it is overlapping in time be more than first predetermined threshold amount when, abandon second participant voice.In some implementations
In example, methods described alternatively include the output by the voice that postpones second participant and according to the participant's to the session
The voice of the adjustment output second participant of voice.
According to some embodiments, a kind of computer system (such as FTP client FTP or server system) includes one or many
Individual processor, memory and one or more programs.One or more of program storages in memory and are configured as
Include being used to perform the finger of the operation of the above method by one or more of computing devices and one or more of programs
Order.According to some embodiments, a kind of non-transitory computer-readable storage medium store instruction wherein, when the instruction is by one
Or during multiple computing devices so that computer system (for example, FTP client FTP or server system) performs the above method
Operation.
Brief description of the drawings
In order to be best understood from the disclosed embodiments, below should reference implementation example description, and combine following attached
Figure, in the accompanying drawings, similar reference marker refers to corresponding part.
Fig. 1 is the block diagram that distributed client-server system is illustrated according to some embodiments.
Fig. 2 is the block diagram that FTP client FTP is illustrated according to some embodiments.
Fig. 3 is the block diagram that server system is illustrated according to some embodiments.
Fig. 4 includes the flow chart that the client-server system for handling concurrent voice is illustrated according to some embodiments.
Fig. 5 is the flow chart of the method according to some embodiment illustrated process concurrent voices.
Fig. 6 is the block diagram of the example according to some embodiment illustrated process concurrent voices.
Fig. 7 is the block diagram of the second example according to some embodiment illustrated process concurrent voices.
Fig. 8 is the block diagram of the 3rd example according to some embodiment illustrated process concurrent voices.
Fig. 9 is the block diagram of the 4th example according to some embodiment illustrated process concurrent voices.
Figure 10 is the block diagram of the 5th example according to some embodiment illustrated process concurrent voices.
Embodiment
It will be understood that, although term " first ", " second " etc. may be used herein to describe various elements, these elements
It should not be limited by these terms.These terms are served only for distinguishing an element and another element.For example, first participant can
To be referred to as second participant, and similarly, second participant can be referred to as first participant, without changing description
Implication, if all concurrent " first participants " as one man renamed and all concurrent " second participants " as one man by
Rename.First participant and second participant are participants, but they are not identical participants.
The purpose of term used herein is only the purpose for describing specific embodiment, and is not intended to claim
Limitation.Used in description and appended claims such as embodiment, singulative " one " is intended to also include most forms, removes
Non- context is explicitly pointed out in addition.Also it will be understood that, term "and/or" used herein is referred to and including one or more phases
The Listed Items of association any and it is possible to combine.It will also be understood that term " comprising " and/or "comprising", when in this explanation
In use, the stated feature of regulation, integer, step, operation, the presence of element and/or component in book, and be not excluded for one or
Other multiple features, integer, step, operation, element, component and/or its presence or addition for combining.
As used herein term " if " can be construed as to imply that " when ... " or " when " or " in response to true
It is fixed " or the condition premise stated of " according to determining " or " in response to detection " be true, depending on context.Similarly, phrase is " such as
Fruit determines [the condition premise stated is true] " or " if [the condition premise stated is true] " or " when [condition stated
Premise is true] when " can be construed as to imply that " when it is determined that " or " in response to determining " or " according to determining " or " when detection " or
The condition premise that " in response to detection " is stated is true, depending on context.
The embodiments described below includes client and server system, typically in distributed client-server system
Interoperated in system, and handle the corresponding method of concurrent voice, wherein, some parts of concurrent voice are dropped, or are adjusted
It is whole, so as to efficient and effectively handle concurrent voice.
Fig. 1 includes the block diagram that diagram is used to handle the distributed client-server system 100 of concurrent voice.It is distributed
Client-server system 100 include one or more FTP client FTPs 102 (its represent be referred herein as " client 102 "),
One or more dispatch server system 106-A ... 106-N (its represent be referred herein as " dispatch server 106 ") and it is used for
Client 102 is connected to the communication network 104 of dispatch server 106.Communication network 104 alternatively include internet, one
Or multiple LANs (LAN), one or more wide area networks (WAN), the combination of other kinds of network or such network.
Client 102 alternatively includes browser 150 and/or conference applications 108.In certain embodiments, conference applications
108 be a part for browser 150.In certain embodiments, browser 150 is that common the Internet browser (is sometimes referred to as
Web browser), with for the other users of conference system communication-voice communication and/or video communication, such as voice
Browser window.Web application user interfaces alternatively use the HTML (HTML) that is presented by browser 106 or can
Extending mark language (XML) member is usually realized.Alternatively, the other users of user and conference system are via unit conference applications
108 communicate.User start she with other users via browser 150 or unit conference applications 108 communication (audio and/
Or video) after, client 102 is via the relay voice data of communication network 104 --- in certain embodiments including audio and/
Or video information (or one part) and corresponding metadata (such as timestamp, the length of communication, format data) ---
To dispatch server 106.In certain embodiments, dispatch server 106 handles speech data and alternatively by speech data
Other one or more dispatch servers 106 are transmitted into concurrently or sequentially to handle.Once speech data (including audio/regard
Frequency information and correspondence metadata) handled by one or more dispatch servers 106, it is transmitted back to one or more clients
102, wherein, audio and/or video communication (for example, one or more voices) are passed to its each user.In some implementations
In example, client 102 is phone.In certain embodiments, dispatch server 106 is with software (for example, programming bag) or hardware
(for example, IC chip) is realized on client 102.In certain embodiments, for example, wherein, dispatch server 106 is in client
Realized on end 102, speech data is launched between client 102, does not pass through centralized server.In certain embodiments,
Voice is differently exported in a client 102 and another client 102, such as with different retardations, or with not
Same speed.
In certain embodiments, between client 102 using consistency algorithm with from client 102 collect information, it is all
Such as, which voice or one part are passed or are ignored in which client 102, and in one or more particular clients
102 voices be delayed by how long.In certain embodiments, according to collected information, consistency algorithm also takes to scheduling
Business device 106 provides one or more scheduling options (for example, client feedback), can provide best in some clients 102
Overall performance.In certain embodiments, collected by consistency algorithm information (including scheduling option) is launched into tune at them
Spend after server 103, be stored in scheduling parameter 130, dispatched for future speech.In certain embodiments, in order to
Avoid or minimize and be likely to result in the chaotic difference transmitted at some clients 102 between the mode of voice of user, according to
The information that consistency algorithm is provided, dispatch server 106 dynamically adjusts voice output.In certain embodiments, scheduling clothes
Business device 106 dynamically adjusts voice output so that client 102 (constantly) converges to a kind of state, in this case, at least
Any voice is transmitted at the client 102 more than predefined quantity, or the voice transmitted at some clients 102 is substantially
Identical (according to output speed or the order of voice).
In certain embodiments, dispatch server 106 includes front-end server module 120, audio processing modules 122, regarded
Frequency processing module 124, sound identification module 126, phonetic transcription module 128, scheduling parameter 130, output scheduling module 132 and defeated
Go out adjusting module 134.In certain embodiments, front-end server module 120 receives voice number from one or more clients 102
According to, be relayed to audio processing modules 122, video processing module 124 or the two.In certain embodiments, front-end server
Module 120 is also by the output voice transmission received from output scheduling module 132 or output adjustment module 134 to one or more
Client 102 is used to transmit.In certain embodiments, front-end server module 120 is also by by audio/visual information therein
It is converted into may be easy to the form of the processing of scheduled server 106 and changing speech data.Audio processing modules 122 are from voice number
According to middle extraction audio-frequency information, and corresponding metadata is alternatively extracted, and be transmitted to output scheduling module 132 or voice
Identification module 126.In certain embodiments, video processing module 124 extracts video information and corresponding member from speech data
Data, and alternatively, it is transmitted to output scheduling module 132 or sound identification module 126.In certain embodiments, sound
Frequency processing module 122 and video processing module 124 are by audio and/or video information and optional corresponding element data output to language
Sound identification module 126.In certain embodiments, sound identification module 126 uses and (realized in hardware or in software) voice to know
Other technology to recognize letter, word, phrase, term, sentence, the voice in audio and/or video information according to voice metadata
Change of the tone or facial expression etc..In certain embodiments, according to voice metadata, phonetic transcription module 128 is by audio
The audio-frequency unit of information and/or video information is transcribed into text.Scheduling parameter 130 includes output scheduling information, such as voice point
Class, voice priority and talker role.Output scheduling module 132 receives audio and/or video information and according to voice
Metadata and/or scheduling parameter 130 export voice in the scheduled time.Alternatively, output scheduling module 132 be at least partially based on from
Information that sound identification module 126, phonetic transcription module 128 and scheduling parameter 130 are received determines scheduled output time.It is defeated
Go out adjusting module 134 and alternatively adjust output voice, such as addition further postpones, removes existing delay, is lengthened or shortened
Pause in voice, and keep algorithm to increase or decrease speech speed using one or more fundamental tones.In certain embodiments,
Delay voice is by recording voice and the at preset time intervals voice of rear playback, alternatively with than raw tone more
Fast or slower speed, come what is obtained.
In certain embodiments, when there are multiple server systems (for example, dispatch server 106-A ... dispatch servers
When 106-N), speech data or one part from client 102 are launched into two or more dispatch servers 106
For concurrently or sequentially handling and dispatching output.In certain embodiments, output scheduling module 132 is (for example, output scheduling module
132-A ... output schedulings module 132N), in multiple server systems, communicate to coordinate voice output.In some implementations
In example, if the speech data received from client 102 is divided into equal or different length stem portion, and same voice is not
With different server system is partly launched into, particularly different audio processing modules 122 are (for example, audio processing modules 122-
A ... audio processing modules 122-N), wherein, they are processed and dispatch for exporting.
Fig. 2 is the block diagram that FTP client FTP 102 (referred to herein as " client 102 ") is illustrated according to some embodiments.
Client 102 typically comprises one or more processing unit CPU 202 (referred to herein as processor), one or more networks
Or other communication interfaces 204, memory 206 including display device and keyboard, mouse, touch pad, touch-screen or other inputs are set
Standby user interface 205 and one or more communication bus 208 for interconnecting these components.Communication bus 208 is alternatively
Circuit (sometimes referred to as chipset) including the communication between interconnection and control system component.Memory 206 typically comprises height
Fast random access memory, such as DRAM, SRAM, DDR RAM or other random access solid state memory devices；And alternatively
Including nonvolatile memory, such as one or more disk storage equipments, optical disc memory apparatus, flash memory device or
Other non-volatile solid-state memory devices of person.Memory 206 alternatively includes one or more storages away from CPU 202 and set
It is standby.Memory 206, or alternatively, the non-volatile memory devices in memory 206, including non-momentary are computer-readable
Storage medium.In certain embodiments, memory 206, or alternatively, non-transitory computer-readable storage medium, under storage
Program, module and the data structure or its subset in face：
Operating system 210, it includes being used to handle various basic system services and for performing hardware-dependent task
Flow；
Network communication module (or instruction) 212, for via one or more network interfaces 204 (wired or wireless) and
One or more communication networks 104 (Fig. 1), internet, other wide area networks, LAN, Metropolitan Area Network (MAN) etc. connect client 102
It is connected to other computers (for example, dispatch server 106 or other clients 102)；
Browser 150, for loading webpage, it alternatively includes being used to performing or explaining that conference applications 108 are insertion
Using the code of webpage；
Conference applications 108 --- in such as unit conference client or web browser 150 insertion program (for example, mutually
Networking browser plug-in) --- for launching user's communication (audio from client 102 to one or more dispatch servers 106
And/or video, for example, voice), and receive communication to transmit in client 102 from one or more dispatch servers 106；
Communication interface 214, for the speech data of audio and/or video information, and corresponding metadata will to be included
One or more dispatch servers 106 are transmitted into, and output voice is received from dispatch server 106 via communication network 104
(audio and/or video, and corresponding metadata)；And
Alternatively, data 216 include it is associated with one or more users communication, cache speech data (for example,
The audio/visual information for being most recently received or recording, corresponding metadata, schedule information etc.).
In some implementations, one or more elements being identified above are stored in one or more foregoing memories and set
In standby, and corresponding to the instruction set for being used to perform above-mentioned functions.The module or program (for example, instruction set) being identified above are not required to
Be embodied as single software program, flow or module, thus these modules each subset can in various embodiments by
Combine or rearrange.In certain embodiments, the optionally stored module identified above of memory 206 and data structure
Subset.Moreover, memory 206 can store the extra module and data structure being not described above.
Fig. 3 is to illustrate conference dispatching server system 106 according to some embodiments (to be referred to herein as " dispatch server
106 ") block diagram.Dispatch server 106 typically comprises one or more processing unit CPU 302 and (referred to herein as handled
Device), one or more networks or other communication interfaces 308, memory 306 and one or many for interconnecting these components
Individual communication bus 308.Communication bus 308 alternatively include interconnection and control system component between communication circuit (sometimes by
Referred to as chipset).Memory 306 include high-speed random access memory, such as DRAM, SRAM, DDR RAM or other deposit at random
Take solid-state memory device；And alternatively include nonvolatile memory, such as one or more disk storage equipments, CD
Storage device, flash memory device or other non-volatile solid-state memory devices.Memory 306 alternatively includes remote
CPU 302 one or more storage devices.Memory 306, or alternatively, the nonvolatile memory in memory 306
Equipment, including non-transitory computer-readable storage medium.In certain embodiments, memory 306, or alternatively, non-momentary
Computer-readable recording medium, stores following program, module and data structure or its subset：
Operating system 310, it includes being used to handle various basic system services and for performing hardware-dependent task
Flow；
Network communication module (or instruction) 312, for via one or more network interfaces 304 (wired or wireless) and
One or more communication networks 104 (Fig. 1), internet, other wide area networks, LAN, Metropolitan Area Network (MAN) etc. are by dispatch server
106 are connected to other computers (for example, client 102 or other dispatch servers 106)；
Front-end server module 120, for receiving speech data and being relayed to sound with parallel or predefined order
Frequency processing module 122 and/or video processing module 124, and will output voice transmission to one or more clients 102 with
In transmission；
Audio processing modules 122, for handling the audio-frequency information that speech data includes according to correspondence metadata, and
And be transmitted into sound identification module 126 to handle in the future by audio-frequency information and/or corresponding metadata, or alternatively
Output scheduling module 132 is transmitted into for output；
Alternatively, video processing module 124, for handling the video that speech data includes according to correspondence metadata
Information, and video information and/or corresponding metadata are transmitted into sound identification module 126 to handle in the future, or
Output scheduling module 132 is alternatively transmitted into for output；
Alternatively, sound identification module 126, for being recognized according to correspondence metadata in audio and/or video information
Change of letter, word, phrase, term or sentence, the voice tone of participant or facial expression etc.；
Alternatively, phonetic transcription module 128, for being turned audio-frequency information and/or video information according to correspondence metadata
Record into correspondence text；
Alternatively, scheduling parameter 130, it include on Classification of Speech, voice/speaker priority, talker role,
(for example, participant is typically slow or fast talker, and participant's history participant behavior talks when whether having long in the past
The tendency being not interrupted) and client feedback past or current schedule information；
Output scheduling module 132, in the scheduled time according to corresponding element data output voice (audio and/or video
Information), and/or the information received from sound identification module 126, phonetic transcription module 128 and scheduling parameter 130；
Alternatively, output adjustment module 134, for adjusting output voice, for example, by being added to One-step delay, going
Algorithm is kept to increase or decrease speech speed except the pause in existing delay, extension or shortening voice and using fundamental tone, it is as follows
Face is described in more detail with reference to Fig. 7-10；And
Alternatively, data 314, it includes the speech data (example of caching associated with one or more users communication
Such as, speech data, the voice received recently waits scheduling output etc.).
In some implementations, the element of one or more above-mentioned marks is stored in one or more foregoing storage devices
In, and corresponding to the instruction set for being used to perform above-mentioned functions.The module or program (for example, instruction set) being identified above need not
It is embodied as single software program, flow or module, therefore each subset of these modules can be in various embodiments by group
Close or otherwise rearrange.In certain embodiments, the optionally stored module sum identified above of memory 306
According to the subset of structure.Moreover, the optionally stored extra module and data structure being not described above of memory 306.
Although Fig. 3 shows that " dispatch server system 106 ", what Fig. 3 was more desirable to is to may be in server set
The functional descriptions of the various features of middle appearance, rather than the structural signal to embodiment described here.In practice, and
As of ordinary skill in the art recognize, separately shown project can be combined and some projects can be separated.
For example, some separately shown projects can be realized on single server in figure 3, and unitem can be by one or many
Individual server is realized." actual number of the server of dispatch server system 106 " and wherein how spy is distributed for realizing
Levy, will be changed according to the difference of realization, and alternatively, depend in part on during the peak value use age and averagely make
The amount for the data service that must be handled with system during period.
Fig. 4 include according to some embodiments illustrate for server system manage everywhere it is being received from FTP client FTP and
Hair voice and the flow chart for exporting voice or one part and the method being adjusted.Method 400 is alternatively by being stored in
In non-transitory computer-readable storage medium and by the one of one or more servers (for example, dispatch server 106 in Fig. 3)
Individual or multiple processors manage come the instruction performed.Operation shown in Fig. 4 typically corresponds to be stored in computer storage
Instruction in device or non-transitory computer-readable storage medium (for example, memory 306 of the dispatch server 106 in Fig. 3).
During some are realized, non-transitory computer-readable storage medium includes disk or optical disc memory apparatus, such as flash memories and consolidated
State storage device or other non-volatile memory devices.In some implementations, it is stored in the computer-readable storage of non-momentary
Computer-readable instruction on medium includes one or more of following：Source code, assembler language code, object identification code or
Other instruction formats that person's one or more processors are explained or performed.In various embodiments, some behaviour in method 400
Make can be combined and/or certain operations order can be shown in Fig. 4 order and change.
In certain embodiments, client 102-A sends (402) from session (for example, in visitor to dispatch server 106
The teleconference session set up between family end 102 and dispatch server 106) first participant speech data.In some realities
Apply in example, (for example, in same time or about the same time) in predefined threshold time interval, client 102-B also to
Dispatch server 106 sends the speech data of the second participant of (404) from session.Dispatch server 106 and then reception come
From the speech data (406) of the first participant of session and the second participant from session speech data (408).At some
In embodiment, concurrent voice is included in client 102 and is in the interior voice carried out of threshold time interval and by dispatch server
106 voices received in threshold time interval.In certain embodiments, voice number is being received from the first and second participants
After, dispatch server 106 is also received from other participants (for example, the 3rd and/or the 4th participant of session) of session
(410) speech data.The voice output (412) of first participant is then arrived one or more clients by dispatch server 106
102.In certain embodiments, after the voice of output first participant, client 102-A (414) and client 102-B
(416) voice of first participant is all received.In some implementations, first is received by client 102 with predefined order to join
With the voice of person.In certain embodiments, predefined order is at least partially based in scheduling parameter 130 and speech data
Including metadata (for example, speaker priority, speaker identity, the length of voice etc.) determine.Alternatively, at it
During he realizes, the voice of first participant is substantially simultaneously received by client 102-A and 102-B.
In certain embodiments, after voice of the output from first participant, dispatch server 106 is determined (418)
The voice of second participant whether the first predetermined threshold of the end section of the voice more than first participant overlapping in time
Amount.In certain embodiments, if the voice of second participant does not have (420 "No") is overlapping to be in time more than first participant
Voice end section the first predetermined threshold amount, then dispatch server 106 is according to the adjustment of the voice of the participant of session
To export the voice of second participant.In other words, if the voice of the first and second participants does not overlap each other, Huo Zhechong
Folded to be less than the first predetermined threshold, then the voice of second participant is exported by adjustment.In certain embodiments, predetermined threshold amount
It it is the threshold value section time, its duration is selected from 100 milliseconds to 1000 milliseconds of scope.In certain embodiments, the first predetermined threshold
Value amount is the threshold value section time, and its duration is selected from the scope of 1 second to 10 seconds.In certain embodiments, the first predetermined threshold amount
It it is the threshold value section time, its duration is selected from the scope of 5 seconds to 40 seconds.In certain embodiments, predetermined threshold amount be 10,20,
30th, 40,50,60,70 or 80 milliseconds, or more than 1 second.
In certain embodiments, voice is adjusted by dispatch server 106 in time, without user's intervention.At other
In embodiment, voice is adjusted by both user and dispatch server 106 in time.In certain embodiments, user
Temporal adjustment and the adjustment in time of dispatch server 106 are assigned different priority.When user when
Between on adjustment when mutually conflicting with the adjustment of dispatch server 106, low priority is overthrown in the adjustment in time of high priority
Adjustment.
In certain embodiments, the voice of single participant is adjusted.In other embodiments, two or more of session
The voice of participant is adjusted.In certain embodiments, the voice of second participant is adjusted in time；In other embodiment
In, the voice of the participant beyond the first and second participants is adjusted.In certain embodiments, the voice quilt of first participant
Adjust in time.The adjustment in time of voice is described in more detail below with reference to Fig. 7-10.
In certain embodiments, if (424 "Yes") is overlapping in time is more than first really for the voice of second participant
First predetermined threshold amount of the end section of the voice of participant, then dispatch server 106 abandon the language of (426) second participant
Sound.In other words, if the voice of the first and second participants is overlapping to be more than the first predetermined threshold, the voice of second participant
It is dropped.In certain embodiments, abandoning voice is included not by one or more participants of voice output to session.At other
In embodiment, abandoning voice is included some but not all participants of voice output to session --- for example, by voice output
Participant to beyond first or second participant, or by voice output to original speech person, without be output to session its
He is participant.The validity of conference system is this method increase, because by selectively abandoning the language overlapping with other voices
Sound and be reduced or eliminated concurrent or overlapping voice.When the voice output of discarding is to its original speech person so that talker
When always hearing the voice of (or watching) their own, this method also meets the sense that its voice of user is properly handled
Know.In certain embodiments, predetermined threshold amount is time threshold value phase, and its duration is selected from 100 milliseconds to 1000 milliseconds of model
Enclose.In certain embodiments, the first predetermined threshold amount is the threshold value section time, and its duration is selected from the scope of 1 second to 10 seconds.
In some embodiments, the first predetermined threshold amount is the threshold value section time, and its duration is selected from the scope of 5 seconds to 40 seconds.At some
In embodiment, the first predetermined threshold amount is 10,20,30,40,50,60,70 or 80 milliseconds, or more than 1 second.
In certain embodiments, after the voice in second participant is output or abandoned, dispatch server 106 is by session
Other participants voice (for example, voice of the 3rd participant) output (428) arrive one or more clients 102.At it
After being output, in certain embodiments, the voice of other participants (for example, the 3rd participant) is by client 102-A (430)
Received with client 102-B (432), and be passed to its each user.
In certain embodiments, when the voice voice in time at least in part with first participant of the 3rd participant
When being overlapped with the voice of second participant, the voice of second participant is exported according to the adjustment of the voice of the participant of session
Including：When second participant in session priority be higher than the 3rd participant priority when, the 3rd participant voice it
The voice of preceding output second participant.In other words, if the overlapping first participant of voice of the 3rd participant and second participates in
Both voice of both persons, then dispatch server 106 second participant is exported according to the adjustment of the voice of the participant of session
Voice, such as when second participant is associated with the priority higher than the 3rd participant (for example, second participant is recognized
To be the talker of higher priority) voice of second participant is exported before the voice of the 3rd participant.Some other
In embodiment, second and the 3rd the priority of participant be (for example, main speech based on FIFO (FIFO), based role
Person, interrupt talker etc.), determined based on social network status or votes.Example based on social network status is
Interrupt relation of the talker (for example, the second talker, the 3rd talker) between the first talker.Using social networks shape
In example of the state basis to determine speaker priority, it is considered to have playing first and the first talker more close relation
The voice of talker is interrupted, other voices for interrupting talker will be played or abandon later.In another example, it will broadcast first
Put to be considered to have and interrupt the voice of talker with the participant of session overall more close relation, and will play or abandon later
Other interrupt the voice of talker.In another example of talker's priority is determined using social network status basis, in meeting
Received during words the participant of other in session at most vote (for example, at most " liking ") so as to will play first with and first
The voice for interrupting talker of talker's more close relation, will play or abandon other voices for interrupting talker later.At this
In the example of sample, if other people like talker in the content described in ession for telecommunication and vote or agree with talker's certainly
Words, the priority of talker can actually increase in session.In the case of one such, participant can be another speech
Person launches more than one ballot.In another scenario, participant can be session in each predetermined time interval of session
Another talker launch more than one ballot (for example, per minute allow the ballot per participant one).
It should be understood that what the certain order operated in description Fig. 4 was merely exemplary, and it is not intended to the expression order
It is that can perform the but one sequence of operation.It will be appreciated by those of ordinary skill in the art that various modes are come to operation described here
Resequenced.In addition, it should be noted that：Other processing of (here in conjunction with described in Fig. 5) on method 500 described here
Details can with similar manner be applied to above in conjunction with the method 400 described in Fig. 4.For example, above in conjunction with described in method 400
Speech data and its reception, voice and its output, the adjustment of voice or discarding and voice it is overlapping can have with next or
Multiple features：Herein with reference to the speech data and its reception described in method 500, voice and its output, the adjustment of voice or discarding,
And voice is overlapping.In order to brief, these details are not repeated herein.
Fig. 5 includes the flow chart of the method for representing the concurrent voice at processing server system according to specific embodiment.
Method 500 alternatively by be stored in non-transitory computer-readable storage medium and can by one or more servers (for example, figure
Dispatch server 106 in 3) one or more processors perform instruction control.In some implementations, shown in Fig. 5
Each operation corresponds in computer storage or non-transitory computer-readable storage medium (for example, dispatch server in Fig. 3
106 memory 306) the middle instruction stored.In some implementations, non-transitory computer-readable storage medium includes disk or light
The solid storage device of disk storage device, such as flash memories or other non-volatile memory devices.In some realizations
In, the computer-readable instruction stored in non-transitory computer-readable storage medium includes one or more of following：Source generation
Code, assembler language code, object identification code or other instruction formats explained by one or more processors or performed.Various
In embodiment, the certain operations in method 500 can be combined and/or certain operations order can be shown in Fig. 5 time
Sequence and change.
In certain embodiments, before meeting starts, meeting is predicted according to Bayesian model (disclosing in further detail below)
Dynamic priori concept (bayesian prior).In certain embodiments, before meeting starts, the meeting dynamic based on prediction,
Determine one or more scheduling options.In certain embodiments, the property by conference system is maximized is selected before meeting starts
The scheduling option of energy.
In certain embodiments, after meeting starts, dispatch server 106 is received from the first participant of session first
(502) speech data.In certain embodiments, from first participant receive speech data after, dispatch server 106 also from
The second participant of session receives (504) speech data.In some cases, the voice of first participant and second participant
Voice overlaps each other in time, also, if exporting any voice without adjustment, will constitute concurrent voice.It is as discussed above,
Concurrent voice is included in the voice sent in predefined threshold time interval by different participants, and in predefined threshold time
The voice received in interval by dispatch server 106.In certain embodiments, predefined threshold time interval is according to Bayes
What model was calculated.In certain embodiments, Bayesian model includes coming from sound identification module (126), phonetic transcription module
(128) information and the timing information of voice.In certain embodiments, by dispatch server 106 using Bayesian model Lai
It is determined that what is likely to occur in following meeting or in the future speech of particular participant, for example, particular participant may talk
How long, and amount overlapping between the voice for some particular participants estimated.In certain embodiments, Bayesian model
Periodically update as the new data from the source being identified above.
In certain embodiments, dynamically predefined threshold time interval is determined for each voice.In certain embodiments,
Predefined threshold time interval includes dispatch server 106 and determines output voice or delay voice until potential concurrent voice is reached
A period of time.In certain embodiments, the predefined threshold time interval of delay voice is according to from Bayesian model
Information and determine so that before dispatch server 106 decides whether to find potential concurrent voice voice be only delayed by (if
Need if being delayed by) as small as possible amount.
In certain embodiments, predefined threshold time interval is the threshold value section time, and its duration arrives selected from 100 milliseconds
1000 milliseconds of scope.In certain embodiments, predefined threshold time interval is the threshold value section time, and its duration is selected from 1
Second to the scope of 10 seconds.In certain embodiments, predefined threshold time interval is the threshold value section time, and its duration is selected from 5
Second to the scope of 40 seconds.In certain embodiments, predefined threshold time interval is 10,20,30,40,50,60,70 or 80 millis
Second, or more than 1 second.
In certain embodiments, dispatch server 106 exports the voice (506) of first participant.In certain embodiments,
The voice of first participant is received in one to be just output, and is not postponed.In certain embodiments, the language of first participant
The predefined threshold time interval of the delay of sound scheduled server 106, to find any potential concurrent voice.In some embodiments
In, in the predefined threshold time interval that is delayed by of voice of first participant, also receive the voice of second participant.
In some embodiments, therefore the voice of the first and second participants is considered as concurrent, because they are all in predefined threshold value
Received in time interval.
In certain embodiments, after the voice latency of first participant to be predefined to threshold time interval, herein in advance
Definition threshold time interim is not received by any other voice (for example, occurring without concurrent voice), by first participant
Voice speedup (" acceleration ") output, to minimize the perception of the relevant delay of any potential user.In certain embodiments, one
Or Multiple factors --- such as the factor from Bayesian model --- are utilized to dynamically determine (for example, expand or reduce) and made a reservation for
Adopted threshold time interval, dispatch server 106 finds possible concurrent voice during predefining threshold time interval herein.
In some cases, when second participant the voice voice more than first participant overlapping in time end
When partial first predefines threshold quantity, dispatch server 106 abandons the voice of (512) second participant.As discussed above,
Abandoning voice includes not exporting voice or one part, and by some but not all participations of voice output to same session
Person.
In some other situations, when the voice voice less than first participant overlapping in time of second participant
During the first predetermined threshold amount of end section, dispatch server 106 according to the adjustment of the voice of the participant of session (for example, by
Output adjustment module 134) output (508) second participant voice.
In certain embodiments, the adjustment of the voice of the participant of session includes the voice of delay (510) second participant
Output.In certain embodiments, the output of the voice of delay second participant includes one or more of following：(1) prolong
The output of the voice of slow second participant is until having exported the voice of first participant (for example, the voice of second participant exists
It is output after the voice of first participant)；(2) output of the voice of delay second participant is until the voice of first participant
It is middle pause occur (for example, the voice of second participant is delayed by and the pause in the voice of first participant (is sometimes referred to as
" free time ") during be output)；(3) the second participation is postponed according to the relation between second participant and first participant
The output of the voice of person is (for example, based on the talker role delay second participant's associated with the first and second participants
Output)；(4) output for the voice that second participant is postponed according to the classification of the content of the voice of second participant, by voice
Identification module 126 and/or phonetic transcription module 128 are determined (for example, whether the voice of second participant is to the language of first participant
Whether the answer or clarification of sound, or the voice of second participant can be classified as interrupt voice or main speech, high priority
Voice or low priority voice, referring to the more details below with reference to Fig. 7-10)；(5) in response to the identification pointed out in session and
Exporting the voice of second participant, (for example, predefined phrase, " problematic " etc. predefines sentence, such as " next
The change (if voice includes video information) of problem " etc., the change of the tone, or participant's facial expression)；(6) in response to
Specific user's input (for example, clicked on " exporting immediately " button that user shows in browser 106 or conference applications 108, or
Person user presses the Similar buttons on phone or mobile phone), export the voice of second participant.
In other embodiments, the adjustment of the voice of the participant of session adds also including the use of output adjustment module 134
Hurry up (" acceleration ") or slow down (" slowing down ") first participant voice output.In some embodiments, when some talkers are slow
Fast talker, and some other talkers are quick talkers, and the voice of talker is accelerated more than quick talker at a slow speed.
In certain embodiments, talker is quick talker or talker is determined according to the transcription of the voice of talker at a slow speed
's.In certain embodiments, talker is quick talker or talker is the archives base by being recorded for talker at a slow speed
Determined in the past voice of talker.In certain embodiments, in some way (for example, due to voice have it is relatively low excellent
The phoneme of first level) delay voice be output to minimize perception of the user on delay with increased speed (" acceleration ").
In certain embodiments, when being by using audio to the output " acceleration " or " deceleration " of the voice of first participant
Between scale-fundamental tone modification algorithm the voice of first participant is accelerated or slowed down, or (ii) passes through and shortens or remove in voice
One or more pauses and obtain.In some other embodiments, the adjustment of the voice of the participant of session is further wrapped
(i) is included when the length of the voice of first participant exceedes scheduled time threshold value, the voice of first participant, and (ii) is cut off
When alreadying switch off the voice of first participant, the voice of second participant is exported.In other words, long voice at least temporarily can quilt
Cut-out, too long of another voice has been delayed by export.In certain embodiments, scheduled time threshold value is scope in 100 millis
Second to 1000 milliseconds of time quantum.In certain embodiments, scheduled time threshold value is a period of time, and its duration is selected from 1 second
To the scope of 10 seconds.In certain embodiments, scheduled time threshold value is the scope of 5 seconds to 40 seconds.In certain embodiments, make a reservation for
Time threshold is 10,20,30,40,50,60,70 or 80 milliseconds, or more than 1 second.
It should be understood that what the certain order operated in description Fig. 5 was merely exemplary, and it is not intended to indicate that the order
It is that can perform the but one sequence of operation.It will be appreciated by those of ordinary skill in the art that various modes are come to operation described here
Resequenced.In addition, it should be noted that：Other processing of (here in conjunction with described in Fig. 5) on method 500 described here
Details can with similar manner be applied to above in conjunction with the method 400 described in Fig. 4.For example, above with reference to described in method 500
Speech data and its reception, voice and its output, the adjustment of voice or discarding and voice it is overlapping can have it is following in one
Individual or multiple features：Herein with reference to the speech data and its reception described in method 400, voice and its output, the adjustment of voice or
Abandon and voice is overlapping.In order to brief, these details are not repeated herein.
Fig. 6 illustrates the example that concurrent voice is handled according to some embodiments.As shown in Figure 6, four talker's (speeches
Person 1-4) talked by one or more clients 102 in the identical or about the same time (time=T1).From talker 1-4
Speech data (SP 1-4) --- including audio and/or video information, and correspondence metadata --- be then transmitted to tune
Spend server 106.Dispatch server 106 handles speech data and has adjustment or export voice without adjustment.
As shown in Figure 6, SP 1 (voice of talker 1) is exported in time=T2 or about in the time.In some realities
Apply in example, T2 is equal to T1.In other words, SP 1 exports (for example, not postponing) when received.In other embodiments, T2
After tl.In some cases, the time interval between T1 and T2 is from unartificial delay (or intrinsic delay), such as network
Stand-by period (stand-by period in such as communication network 104 (Fig. 1)), and/or equipment stand-by period (such as dispatch server
106 or client 102 in processing latency).In other situations, the time interval between T1 and T2, which comes from, manually prolongs
Late, such as dispatch server 106 intentionally or deliberately increased delay.
After SP 1 output, SP 2 (voice of talker 2) is also exported.In this illustration, stood after SP 1
SP 2 is exported, for example, it is idle to minimize system, or obtain coherent between specific communications purposes, such as holding voice.
In other embodiments, artificial delay is added between SP 1 ending and SP 2 beginning, for example, to provide definition.
As shown in Figure 6, after output SP 2, SP 3 and SP 4 are exported at the same time or at about, as concurrent or
Overlapping voice.In certain embodiments, it is allowed to predefined degree it is concurrent or overlapping, for example, when speech volume is high, to increase
Plus handling capacity.In certain embodiments, predefine degree be concurrently it is revisable, by the user of client 102 by browsing
The interface component (for example, button) shown in device 106 or conference applications 108, or by dispatch server 106, joined according to scheduling
Number 130 and/or voice metadata are controlled.Referring to Fig. 6, in some alternative embodiments, SP 3 or SP 4 is abandoned.
Fig. 7 illustrates the second example that concurrent voice is handled according to some embodiments.In this illustration, in order to handle simultaneously
Voice is sent out, is postponed according to voice metadata or abandons special sound.
As shown in Figure 7, three talkers (talker 1-3) talk (time=T1) at the same time or at about.Come from
Talker 1-3 speech data (SP 1-3) --- including audio and/or video information, and correspondence metadata --- it is launched
To dispatch server 106.Dispatch server 106 handles speech data, and with various adjustment output correspondence voices.
Based on received speech data, based on dispatch server 106 classifies SP 2 (voice from talker 2)
Voice.According to this classification, SP 2 is output first, is not adjusted (for example, as former state).In certain embodiments, point of voice
Class is based at least partially on speech data.For example, if the content of voice includes meeting the term of predefined important criteria collection,
Voice is classified as main speech.In another example, if voice metadata includes meeting the letter of predefined important criteria set
Cease (for example, length or timing of voice of voice), then voice is classified as main speech.In other embodiments, Classification of Speech
Made according to speech data, such as based on the relation between pre-existing talker.For example, SP 2 is classified as subject
Sound, because talker 2 is the higher level of talker 1 and 3 in the tissue.
Before or during SP 2 output, SP 1 is categorized as interrupting voice by dispatch server 106, because SP 2 is overlapping
SP 1 is the more early part of main speech.In certain embodiments, interrupt voice to be dropped or postpone, until detecting in main speech
Pause.Here, because SP 1 does not include pause, and because SP 1 (is interrupted " too in the overlapping SP 1 of SP 2 relatively early part
It is early "), SP 2 is dropped and (is illustrated as " X ").
In certain embodiments, under predetermined voice management strategy set, voice scheduled server 106 is abandoned.
In some embodiments, voice management strategy set includes determining that voice is the relatively early part (example for interrupting voice and overlapping main speech
Such as, interrupt too early).In other embodiments, voice management strategy includes determining that voice is delayed by being more than predetermined time amount.
In realization, when voice is in the queue for being used for dispatching output, voice management strategy includes determining that voice is in output queue
It is more than predetermined time amount.In other words, just it is dropped when voice is considered as " old ".In some cases, old voice is lost
Abandon is because it may become unrelated with session due to the passage for focus of talking.In certain embodiments, when voice is prolonged
It is regarded as when late more than 1 second, more than 5 seconds, more than 10 seconds, more than 20 seconds or more than 1 minute old.
In certain embodiments, voice is additionally in response to specific user's input, and such as user's click browser 106 or meeting should
With " discarding " button in 108, and it is dropped.In some cases, by prevent important voice early stage be interrupted from
Without being repeated before its totally tansitive, abandon voice and add communication efficient and speech intelligibility.
Before or during SP 2 output, dispatch server 106 also determines the overlapping SP 2 of SP 3 relatively rear part.According to
This determination, SP 3 adjusts output after SP 2.In this illustration, dispatch server 106 intentionally or intentionally makes SP
The output delay time interval D LY1 (702) of 3 (voices of talker 3).In certain embodiments, the retardation that intentionally adds with
The interior of voice is perhaps categorized into ratio.For example, important voice and interrupt and wish that there is bigger delay between voice, so as to listening
Crowd provides enough time to digest the content of important voice.But, in another example, in statement and to being wished between its clarification
Hoping has less delay or does not postpone, and obscures to minimize.
Fig. 8 illustrates the 3rd example that concurrent voice is handled according to some embodiments.In this illustration, according to talker
Role, and by amended delay, handle concurrent voice.
As shown in Figure 8, four talkers with different role --- main talker, talker, high priority are interrupted
Talker and low priority talker --- (time=T1) talks at the same time or at about.In certain embodiments, talk
Person role is independently of speech data and determined, such as based on the relation between pre-existing talker.In other implementations
In example, be based at least partially on speech data, the content of such as voice, the timestamp associated with voice, the length of voice and
The classification of voice and determine talker role.In certain embodiments, voice priority is excellent from talker role or talker
Derived from first level.
In fig. 8 in shown example, SP 2 (voice of main talker) is received and exported, without adjusting.SP 1
(voice for interrupting talker) is dropped (" X "), because it is partly mutually disturbed by the relatively early of overlapping SP 2 with main talker.
In certain embodiments, the voice with the priority equal or higher with predefined valve value PRI is not lost
Abandon, even if the overlapping main speech of the voice.For example, SP 3 (voice of high priority talker) is not dropped, although SP 3 is (as SP
1) also overlapping SP 2 part.Because talkers of the SP 3 from high priority, dispatch server 106 is exported after SP 2
SP 3, rather than abandon SP 3.Similarly, SP 4 (voice of low priority talker) is also not dropped.In some embodiments
In, higher priority voice is exported before lower priority voice.For example, SP 4 is output after SP 3, because SP 3
There is higher priority than SP 4.
In certain embodiments, according to main speech is determined more than predetermined length, the voice exported after main speech passes through
Shorten wherein included delay and adjust (for example, removing the pause in main speech).In such example, the Hes of SP 3
SP 4 includes pause：It is DLY 3 (802) and DLY 4 (804) respectively.Because SP 3 and SP 4 all SP 2 (main speech) it
After export, and SP 2 exceedes predetermined threshold length, pause --- the DLY 3 (802) and DLY 4 in SP 3 and SP 4
(804) --- DLY 3 ' (806) and DLY 4 ' (808) are shortened into respectively.This method not only interrupts voice by discarding and increased
Speech intelligibility is added, and Consumer's Experience is enhanced by exporting high priority voice before low priority voice.
In some embodiments, predetermined threshold length is time quantum of the scope at 100 milliseconds to 1000 milliseconds.In certain embodiments, in advance
It is a period of time to determine threshold length, and its duration is selected from the scope of 1 second to 10 seconds.In certain embodiments, predetermined threshold is long
The scope of degree is 5 seconds to 40 seconds.In certain embodiments, predetermined threshold length is 10,20,30,40,50,60,70 or 80 millis
Second, or more than 1 second.
In certain embodiments, at client 102, user interface (" UI ") root of browser 150 or conference applications 108
It is determined according to talker role.For example, in some implementations, the user interface of main talker includes " discarding " button, works as quilt
During click, the ongoing voice of another talker is abandoned (so that main talker can feel suitable in main talker
Whenever interrupt other talkers).In some implementations, it is classified as interrupt talker or low excellent according to determination talker
" discarding " button in first level talker, the browser 150 or conference applications 108 of the talker is become unavailable (for example, " becoming
Ash "), the situation of speaker words person or high priority talker can be disturbed by thus avoiding interrupting talker or low priority talker.
In certain embodiments, browser 150 or the user interface of conference applications 108 also include one or more users control
The state of participant or voice in part, the use information of one or more of user controls offer conference systems and/or session
Information.In some implementations, use information includes：The quantity (for example, size of output queue) of the voice to be output such as current.
In certain embodiments, the conversion including the use of information.For example, the size of output queue is changed and is shown as currently waiting defeated
The voice gone out total length (for example, when output queue close to it is full when be 10 seconds, or when queue close to space-time be 10 milliseconds).
The status information of participant or voice includes in some implementations in session：Postpone position of the voice in output queue
Put (for example, before delay voice in identical output queue voice number), be delayed by the number (example of particular speaker voice
Such as, for low priority talker, the number or length of voice are delayed by), indicate future speech whether by " overriding " specific ginseng
(for example, representing the sudden strain of a muscle red light on the icon of low priority talker, represented with the information of the delay voice of person if participant is existing
In speech, the delay voice for causing low priority talker is further delayed or even abandoned by the voice of particular participant),
The information (for example, the voice of quick flashing red light indication lag is just dropped) whether the voice of indication lag is just being dropped, indicates to work as
Before the number of the voice of participant that is delayed by close to or beyond threshold number information (for example, counter or progress bar,
Indicate output queue have how " crowded "), indicate future speech by the information for the possibility being delayed by (for example, dodging amber light at a slow speed
Indicate, if participant talks now, the voice of participant will likely be delayed by), and indicate whether real time transcription can be used for
The information (for example, whether " transcription in chat window " button can be clicked on or " graying ") of the participant of conference system.
The display for providing the user control of use or status information (or feedback) reduces user's setback, is learnt because user becomes
The state of its voice of road, and/or the voice of the participant of other in same session state, thus strengthen conference system user's body
Test.
Fig. 9 illustrates the 4th example that concurrent voice is handled according to some embodiments.In this illustration, according to talker
Role handles concurrent voice, and with overlapping output.
In fig .9 in shown example, two talkers and interrupt talker at main talker, at the same time or at about
Speech.Speech data is launched into dispatch server 106 to handle and dispatch output.As discussed above, can be at least partly
Ground determines talker role and voice priority based on speech data or independently of speech data.
In this illustration, because SP 1 is the voice from main talker, SP 1 is first exported and not changed
Or adjustment.Length according to SP 1 is determined exceedes predetermined threshold length and not suspended in SP 1, and SP 2 (comes from and interrupts speech
The voice of person) concurrently exported with SP 1 part.In other words, it has been spoken in main talker oversize without suspending or wanting
In the situation of the sign of stopping, the voice of another talker can be output, although it is by a part for overlapping main speech.This
Method ensures the voice from talker, but regardless of its priority, can always be heard within user's acceptable period.
But, SP 2 of the output with delay and adjustment is used as SP 2 '.As shown in Figure 9, SP 2 postpones DLY 2 (902)
Output.In certain embodiments, delay includes a period of time, and dispatch server 106 attempts detection on this period and entered
Pause in capable voice.In other embodiments, delay includes a period of time, and dispatch server 106, which retains, to be used this period
In the main speech not interrupted so that at least relatively early part of main speech and be also sometimes pith, by not overlapping or beat
Transmit disconnectedly.
In some embodiments according to the example presented in Fig. 9, SP 2 is also adjusted to its speed and exported.In Fig. 9, adjust
Spending server 106 increases SP 2 speed using output adjustment module 134.SP 2 keeps algorithm to be shorter than its normal using fundamental tone
(otherwise referred to as " shrinking ") is exported in the period of length.
Also as shown in Figure 9, in overlapping period (904), voice SP 1 and SP 2 ' are concurrently exported.It is output in SP 2 '
Afterwards, SP 1 continues to be output without adjustment.
In certain embodiments, be added to the delay for interrupting voice, such as DLY 2 (902), be at least partially based on
The associated speech data of voice or alternatively, independently of speech data, and determine.If for example, SP 2 voice is known
It Fen Xi not point out that SP 2 represents that SP 2 is delayed by, for example, not defeated the problem of the relatively rear part for the SP 1 not exported also specifically related to
Go out, until SP relatively after be partly output, with raising efficiency and definition.In other realizations, it is added to and interrupts prolonging for voice
It is to be determined according to the relation between pre-existing talker late.For example, when interrupt talker be main talker (for example,
Rank and file employees) higher level's (for example, manager) when, during compared with interrupting talker and reciprocity main talker, SP 2 is by less delay.
In certain embodiments, speech data or alternatively is based at least partially on, independently of speech data, and it is true
Fixed overlapping (904).In some implementations, overlapping (904) correspond to one of the main speech for not predefining important criteria set
Point.In other words, SP 2 ' is allowed to overlap with main speech SP 1 less important part.In some implementations, it is overlapping
(904) it is proportional or inversely proportional to the length of voice.For example, the length of overlapping (904) is predetermined the hundred of the length of main speech
Point ratio, either interrupt the length of voice predetermined percentage or the two.
Figure 10 illustrates the 5th example that concurrent voice is handled according to some embodiments.In this illustration, according to voice
Context and/or classify and handle concurrent voice.
In Fig. 10 in shown example, three talkers talk at the same time or at about.Speech data is transmitted to
Dispatch server 106 is handled and dispatched output.In application speech recognition and/or phonetic transcription technology (such as respectively in language
Sound identification module 126 and phonetic transcription module 128) after, voice SP 1-3 are classified as three classifications：Main speech, clarification remark
Sound and inquiry voice.In certain embodiments, talker role distributes according to Classification of Speech.
In certain embodiments, different classes of voice is assigned different priority, and its output is at least partly
Ground be based on its each priority and be adjusted.In some cases, the voice in main speech classification is assigned than other classifications
In the higher priority of voice because main speech is considered as than problem or clarified more important.In other situations, voice is clarified
Voice in classification is assigned the priority higher than inquiring the voice in voice class, because clarification is directly solved sometimes
Problem, and therefore should be output to reduce redundancy before problem.
In Fig. 10 in shown example, main speech SP 1 is included by suspending two parts that DLY 1 (1002) separates.It is main
The Part I of voice is output, without postponing or adjusting.The Part I that voice SP 2 is related to SP 1 is clarified detecting
After (for example, by clarifying the term wherein used), during SP 1 pause such as DLY 1 (1002), clarification voice SP
2 are adjusted output, are used as SP 2 '.Because SP 2 length exceedes the length of pause, SP 2 speed increase uses base
Sound keeps algorithm so that SP 2 ' is output completely in pause DLY 1 (1002).
After the clarification voice SP 2 ' after outputing adjustment, output main speech SP 1 Part II is also adjusted
It is whole.SP 1 Part II is also by " acceleration ".After output SP 1 Part II, voice SP 3 is also by with increased for inquiry
Speed output is used as SP 3 '.
In certain embodiments, voice or one part are adjusted according to the content for other voices for providing voice context
It is whole.For example, main speech SP1 Part II, including similar to SP 2 ' clarification, be accelerated, because the second of main speech SP 1
Partially due to clarifying voice SP 2 ' previous output and having become inessential, and it is not required to be repeated or is described in detail.
In certain embodiments, inquiry voice can also be adjusted according to the content (for example, voice context) of another voice.
For example, inquiring voice by previous voice answering or when solving the problem of the part for inquiring voice includes
Corresponding part is accelerated to reduce redundancy and increase handling capacity.
In certain embodiments, the output of inquiry voice is delayed by more than clarification voice and main speech.In other words, one
In a little embodiments, inquiry voice is not output, until main speech and/or clarification voice are output.This method can increase entirety
Validity, because in some cases, inquiring the problem of voice includes of quilt in the part later of clarification voice or main speech
Answer.In other embodiments, when inquiring that voice includes needing to be solved or be disclosed major issue as early as possible, inquiry
Voice is output before a part for clarification voice or main speech.
Description above has been described in conjunction with specific embodiments, it is therefore an objective to explained.But, illustrative discussion above is not
Be intended to limit or limitation invention arrive disclosed precise forms.In view of above-mentioned teaching, many modification and variation are all
It is possible.Embodiment is selected and described most preferably to explain the principle and its practical application of invention, so that this area
Others skilled in the art can be most preferably using the present invention and the various embodiments with various modifications, to be adapted to desired spy
Surely use.
Claims (28)
1. a kind of method for handling concurrent voice, including：
Including one or more processors and storing for one or more journeys by one or more of computing devices
At the system of the memory of sequence：
Speech data is received from the first participant of session；
Speech data is received from the second participant of the session；
Determine whether the voice of the second participant is overlapping in time with the voice of the first participant, wherein
When the voice of the second participant is overlapping in time not with the voice of the first participant, output described first
The voice of participant, and then export the voice of the second participant,
When the voice of the second participant is overlapping in time with the voice of the first participant, second ginseng is determined
The amount overlapping with the voice of the first participant with the voice of person, wherein
When the first predetermined threshold amount of the end section of the overlapping voice less than the first participant, according to described
The voice of the adjustment output first participant of the voice of one or more participants of session, and then export described the
The voice of two participants, according to the adjustment of the voice of one or more of participants to session output second ginseng
Include postponing the output of the voice of the second participant with the voice of person；And
When first predetermined threshold amount of the end section of the overlapping voice more than the first participant, institute is exported
The voice of first participant is stated, and optionally abandons the voice of the second participant.
2. the method for claim 1, wherein postponing the output of the voice of the second participant includes：Delay is described
The output of the voice of second participant, the voice until having exported the first participant.
3. the method for claim 1, wherein postponing the output of the voice of the second participant includes：Delay is described
The output of the voice of second participant, suspends until in the voice of the first participant.
4. the method for claim 1, wherein postponing the output of the voice of the second participant includes：According to described
Relation between second participant and the first participant postpones the output of the voice of the second participant.
5. the method for claim 1, wherein postponing the output of the voice of the second participant includes：According to described
The output of the voice for classifying to postpone the second participant of the content of the voice of second participant.
6. method as claimed in claim 5, wherein, the classification of the content of the voice of the second participant is known by voice
Complete not and to the transcription of the content of the voice of the second participant.
7. the method for claim 1, wherein postponing the output of the voice of the second participant includes：In response to
The voice for pointing out and exporting the second participant is recognized in the session.
8. the method as any one of claim 2-7, wherein, postpone the output bag of the voice of the second participant
Include：The voice of the second participant is inputted and exported in response to user.
9. the method as any one of claim 1-7, wherein, according to the tune of the voice of the participant to the session
The voice of the whole output second participant includes：Accelerate the voice of the first participant.
10. method as claimed in claim 9, wherein, accelerating the voice of the first participant includes：Use audio time mark
Degree-fundamental tone changes algorithm to accelerate the voice of the first participant.
11. method as claimed in claim 9, wherein, accelerating the voice of the first participant includes：Shorten or remove described
One or more pauses in the voice of first participant.
12. the method as any one of claim 1-7, wherein, the adjustment to the voice of the participant of the session
Including：(i) when the length of the voice of the first participant exceeds scheduled time threshold value, the language of the first participant is cut off
Sound, and (ii) export the voice of the second participant when the voice of the first participant is cut off.
13. the method as any one of claim 1-7, methods described further comprises：
Receive speech data from the 3rd participant of the session, wherein, the voice of the 3rd participant at least in part with
The voice of the first participant and the voice of the second participant are overlapping in time；And
Adjustment to the voice of one or more participants of the session further comprises：The second ginseng described in the session
When being higher than the priority of the 3rd participant with the priority of person, described the is exported before the voice of the 3rd participant
The voice of two participants.
14. method as claimed in claim 13, wherein, the priority of the second participant and the 3rd participant's is excellent
First level is determined based on FIFO, based role, based on social network status or votes.
15. a kind of server system, including：
One or more processors；
Memory；And
One or more programs, wherein, one or more of programs be stored in the memory and be configured to by
One or more of computing devices, one or more of programs include instruction, and described instruct is used for：
Speech data is received from the first participant of session；
Speech data is received from the second participant of the session；
Determine whether the voice of the second participant is overlapping in time with the voice of the first participant, wherein
When the voice of the second participant is overlapping in time not with the voice of the first participant, output described first
The voice of participant, and the voice of the second participant is then exported, and
When the voice of the second participant is overlapping in time with the voice of the first participant, second ginseng is determined
The amount overlapping with the voice of the first participant with the voice of person, wherein
When the first predetermined threshold amount of the end section of the overlapping voice less than the first participant, according to described
The voice of the adjustment output first participant of the voice of one or more participants of session, and then export described the
The voice of two participants, according to the adjustment of the voice of one or more of participants to session output second ginseng
Include postponing the output of the voice of the second participant with the voice of person；And
When first predetermined threshold amount of the end section of the overlapping voice more than the first participant, institute is exported
The voice of first participant is stated, and optionally abandons the voice of the second participant.
16. server system as claimed in claim 15, wherein, postponing the output of the voice of the second participant includes：
Postpone the output of the voice of the second participant, the voice until having exported the first participant.
17. server system as claimed in claim 15, wherein, postponing the output of the voice of the second participant includes：
Postpone the output of the voice of the second participant, suspend until in the voice of the first participant.
18. server system as claimed in claim 15, wherein, postponing the output of the voice of the second participant includes：
Postpone the output of the voice of the second participant according to the relation between the second participant and the first participant.
19. server system as claimed in claim 15, wherein, postponing the output of the voice of the second participant includes：
The output for the voice that the second participant is postponed according to the classification of the content of the voice of the second participant.
20. server system as claimed in claim 19, wherein, the classification of the content of the voice of the second participant is logical
Cross speech recognition and the transcription of the content of the voice of the second participant is completed.
21. server system as claimed in claim 15, wherein, postponing the output of the voice of the second participant includes：
The voice of the second participant is pointed out and exported in response to being recognized in the session.
22. the server system as any one of claim 16-21, wherein, postpone the language of the second participant
The output of sound includes：The voice of the second participant is inputted and exported in response to user.
23. the server system as any one of claim 15-21, wherein, according to the participant to the session
The voice of the adjustment output second participant of voice include：Accelerate the voice of the first participant.
24. server system as claimed in claim 23, wherein, accelerating the voice of the first participant includes：Use sound
Frequency time scale-fundamental tone changes algorithm to accelerate the voice of the first participant.
25. server system as claimed in claim 24, wherein, accelerating the voice of the first participant includes：Shorten or
Remove one or more pauses in the voice of the first participant.
26. the server system as any one of claim 15-21, wherein, to the language of the participant of the session
The adjustment of sound includes：(i) when the length of the voice of the first participant exceeds scheduled time threshold value, first ginseng is cut off
With the voice of person, and (ii) voice of the second participant is exported when the voice of the first participant is cut off.
27. the server system as any one of claim 15-21, the system further comprises：
Receive speech data from the 3rd participant of the session, wherein, the voice of the 3rd participant at least in part with
The voice of the first participant and the voice of the second participant are overlapping in time；And
Included according to the voice that the adjustment of the voice of the participant to the session exports the second participant：When the session
Described in second participant priority be higher than the 3rd participant priority when, the 3rd participant voice it
The voice of the preceding output second participant.
28. server system as claimed in claim 27, wherein, the priority of the second participant and the described 3rd is participated in
The priority of person is determined based on FIFO, based role, based on social network status or votes.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261701520P | 2012-09-14 | 2012-09-14 | |
US61/701,520 | 2012-09-14 | ||
PCT/US2013/059786 WO2014043555A2 (en) | 2012-09-14 | 2013-09-13 | Handling concurrent speech |
Publications (2)
Publication Number | Publication Date |
---|---|
CN104756473A CN104756473A (en) | 2015-07-01 |
CN104756473B true CN104756473B (en) | 2017-08-29 |
Family
ID=49230889
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201380053617.3A Active CN104756473B (en) | 2012-09-14 | 2013-09-13 | Handle concurrent voice |
Country Status (4)
Country | Link |
---|---|
US (4) | US9313335B2 (en) |
EP (1) | EP2896194B1 (en) |
CN (1) | CN104756473B (en) |
WO (1) | WO2014043555A2 (en) |
Families Citing this family (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6066471B2 (en) * | 2012-10-12 | 2017-01-25 | 本田技研工業株式会社 | Dialog system and utterance discrimination method for dialog system |
EP3448006B1 (en) * | 2013-07-02 | 2023-03-15 | Family Systems, Limited | System for improving audio conferencing services |
US9600474B2 (en) * | 2013-11-08 | 2017-03-21 | Google Inc. | User interface for realtime language translation |
US8719032B1 (en) * | 2013-12-11 | 2014-05-06 | Jefferson Audio Video Systems, Inc. | Methods for presenting speech blocks from a plurality of audio input data streams to a user in an interface |
US20160212273A1 (en) * | 2015-01-21 | 2016-07-21 | Wyde Voice, LLC | Systems and methods for distributed audio conferencing |
EP3254454B1 (en) * | 2015-02-03 | 2020-12-30 | Dolby Laboratories Licensing Corporation | Conference searching and playback of search results |
US11076052B2 (en) * | 2015-02-03 | 2021-07-27 | Dolby Laboratories Licensing Corporation | Selective conference digest |
JP6464411B6 (en) * | 2015-02-25 | 2019-03-13 | Ｄｙｎａｂｏｏｋ株式会社 | Electronic device, method and program |
US10089061B2 (en) | 2015-08-28 | 2018-10-02 | Kabushiki Kaisha Toshiba | Electronic device and method |
US20170075652A1 (en) | 2015-09-14 | 2017-03-16 | Kabushiki Kaisha Toshiba | Electronic device and method |
US10339224B2 (en) * | 2016-07-13 | 2019-07-02 | Fujitsu Social Science Laboratory Limited | Speech recognition and translation terminal, method and non-transitory computer readable medium |
EP3301895B1 (en) * | 2016-09-28 | 2019-07-24 | British Telecommunications public limited company | Streamed communication |
EP3301896B1 (en) * | 2016-09-28 | 2019-07-24 | British Telecommunications public limited company | Streamed communication |
US20180166073A1 (en) * | 2016-12-13 | 2018-06-14 | Ford Global Technologies, Llc | Speech Recognition Without Interrupting The Playback Audio |
WO2019010250A1 (en) | 2017-07-05 | 2019-01-10 | Interactions Llc | Real-time privacy filter |
CN107613151A (en) * | 2017-08-11 | 2018-01-19 | 华迪计算机集团有限公司 | A kind of sound mixing method and system for videoconference interactive between delegate to the meeting |
CN107613400B (en) * | 2017-09-21 | 2021-03-26 | 北京奇艺世纪科技有限公司 | Method and device for realizing voice barrage |
CN107910006A (en) * | 2017-12-06 | 2018-04-13 | 广州宝镜智能科技有限公司 | Audio recognition method, device and multiple source speech differentiation identifying system |
WO2019122343A1 (en) * | 2017-12-22 | 2019-06-27 | British Telecommunications Public Limited Company | Managing streamed audio communication sessions |
US11227117B2 (en) * | 2018-08-03 | 2022-01-18 | International Business Machines Corporation | Conversation boundary determination |
CN109348355B (en) * | 2018-09-28 | 2020-06-30 | 东南（福建）汽车工业有限公司 | Real person voice alarm signal arbitration method and system |
RU2761940C1 (en) | 2018-12-18 | 2021-12-14 | Общество С Ограниченной Ответственностью "Яндекс" | Methods and electronic apparatuses for identifying a statement of the user by a digital audio signal |
EP3723354B1 (en) | 2019-04-09 | 2021-12-22 | Sonova AG | Prioritization and muting of speakers in a hearing device system |
CN111756723B (en) * | 2020-06-19 | 2022-08-19 | 北京联想软件有限公司 | Audio processing method, device and equipment applied to multi-party call |
CN112628695B (en) * | 2020-12-24 | 2021-07-27 | 深圳市轻生活科技有限公司 | Control method and system for voice control desk lamp |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1747465A (en) * | 2004-09-09 | 2006-03-15 | 华为技术有限公司 | Realization of speech service |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3829485B2 (en) * | 1998-07-30 | 2006-10-04 | ソニー株式会社 | Voice call device, voice call system, and voice call method |
US7774694B2 (en) * | 2002-12-06 | 2010-08-10 | 3M Innovation Properties Company | Method and system for server-based sequential insertion processing of speech recognition results |
JP2006229903A (en) | 2005-02-21 | 2006-08-31 | Fuji Xerox Co Ltd | Conference supporting system, method and computer program |
US7675897B2 (en) * | 2005-09-06 | 2010-03-09 | Current Technologies, Llc | Power line communications system with differentiated data services |
DE102006002603A1 (en) * | 2006-01-13 | 2007-07-19 | Deutsche Telekom Ag | Speech conference executing method for use over telecommunication network, involves recognizing words spoken by subscribers by speech recognition, where request for serial speech is signalized with simultaneous speech of subscribers |
US8085803B2 (en) * | 2007-01-29 | 2011-12-27 | Intel Corporation | Method and apparatus for improving quality of service for packetized voice |
JP2009139592A (en) * | 2007-12-05 | 2009-06-25 | Sony Corp | Speech processing device, speech processing system, and speech processing program |
US8447610B2 (en) * | 2010-02-12 | 2013-05-21 | Nuance Communications, Inc. | Method and apparatus for generating synthetic speech with contrastive stress |
US20110271210A1 (en) | 2010-04-30 | 2011-11-03 | American Teleconferncing Services Ltd. | Conferencing Application Store |
-
2013
- 2013-09-13 WO PCT/US2013/059786 patent/WO2014043555A2/en active Application Filing
- 2013-09-13 EP EP13766217.7A patent/EP2896194B1/en active Active
- 2013-09-13 CN CN201380053617.3A patent/CN104756473B/en active Active
- 2013-09-13 US US14/027,061 patent/US9313335B2/en active Active
-
2016
- 2016-03-02 US US15/059,222 patent/US9491300B2/en active Active
- 2016-10-27 US US15/336,629 patent/US9742921B2/en active Active
-
2017
- 2017-07-18 US US15/653,324 patent/US10084921B2/en active Active
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1747465A (en) * | 2004-09-09 | 2006-03-15 | 华为技术有限公司 | Realization of speech service |
Also Published As
Publication number | Publication date |
---|---|
EP2896194B1 (en) | 2018-05-09 |
US20140078938A1 (en) | 2014-03-20 |
CN104756473A (en) | 2015-07-01 |
US10084921B2 (en) | 2018-09-25 |
US9742921B2 (en) | 2017-08-22 |
US20170318158A1 (en) | 2017-11-02 |
US9313335B2 (en) | 2016-04-12 |
EP2896194A2 (en) | 2015-07-22 |
US20170048394A1 (en) | 2017-02-16 |
WO2014043555A2 (en) | 2014-03-20 |
US20160182728A1 (en) | 2016-06-23 |
WO2014043555A3 (en) | 2014-07-10 |
US9491300B2 (en) | 2016-11-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN104756473B (en) | Handle concurrent voice | |
US8126705B2 (en) | System and method for automatically adjusting floor controls for a conversation | |
CN111866022B (en) | Post-meeting playback system with perceived quality higher than that originally heard in meeting | |
CN107210045B (en) | Meeting search and playback of search results | |
US20190362252A1 (en) | Learning user preferences in a conversational system | |
US7933226B2 (en) | System and method for providing communication channels that each comprise at least one property dynamically changeable during social interactions | |
CN109977218B (en) | A kind of automatic answering system and method applied to session operational scenarios | |
US7698141B2 (en) | Methods, apparatus, and products for automatically managing conversational floors in computer-mediated communications | |
WO2021051506A1 (en) | Voice interaction method and apparatus, computer device and storage medium | |
CN113140216A (en) | Selective meeting abstract | |
CN110299152A (en) | Interactive output control method, device, electronic equipment and storage medium | |
EP4054111A1 (en) | Method for switching between man-machine dialogue modes | |
CN109274922A (en) | A kind of Video Conference Controlling System based on speech recognition | |
CN111833875B (en) | Embedded voice interaction system | |
US20230282207A1 (en) | System and method for electronic communication | |
CN107656923A (en) | Voice translation method and device | |
CN110196900A (en) | Exchange method and device for terminal | |
CN114615381A (en) | Audio data processing method and device, electronic equipment, server and storage medium | |
US20220028417A1 (en) | Wakeword-less speech detection | |
US20210005193A1 (en) | Method for Exiting a Voice Skill, Apparatus, Device and Storage Medium | |
EP1453287A1 (en) | Automatic management of conversational groups | |
WO2024032111A9 (en) | Data processing method and apparatus for online conference, and device, medium and product | |
Edlund et al. | Is it really worth it? Cost-based selection of system responses to speech-in-overlap | |
CN115249473A (en) | Method and device for AI interaction by using natural and intelligent voice |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
EXSB | Decision made by sipo to initiate substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant | ||
CP01 | Change in the name or title of a patent holder |
Address after: American CaliforniaPatentee after: Google limited liability companyAddress before: American CaliforniaPatentee before: Google Inc. |
|
CP01 | Change in the name or title of a patent holder |