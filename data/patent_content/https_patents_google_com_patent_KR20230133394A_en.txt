KR20230133394A - System and method for training models to predict dense correspondence of images using geodetic distances - Google Patents
System and method for training models to predict dense correspondence of images using geodetic distances Download PDFInfo
- Publication number
- KR20230133394A KR20230133394A KR1020237030079A KR20237030079A KR20230133394A KR 20230133394 A KR20230133394 A KR 20230133394A KR 1020237030079 A KR1020237030079 A KR 1020237030079A KR 20237030079 A KR20237030079 A KR 20237030079A KR 20230133394 A KR20230133394 A KR 20230133394A
- Authority
- KR
- South Korea
- Prior art keywords
- feature
- map
- point
- image
- distances
- Prior art date
Links
- 238000000034 method Methods 0.000 title claims abstract description 90
- 238000012549 training Methods 0.000 title claims abstract description 54
- 238000012545 processing Methods 0.000 claims description 59
- 238000013528 artificial neural network Methods 0.000 claims description 29
- 238000005094 computer simulation Methods 0.000 abstract description 2
- 239000013598 vector Substances 0.000 description 47
- 238000005516 engineering process Methods 0.000 description 11
- 238000010586 diagram Methods 0.000 description 6
- 230000000694 effects Effects 0.000 description 4
- 238000012986 modification Methods 0.000 description 4
- 230000004048 modification Effects 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 241000282412 Homo Species 0.000 description 1
- 241001465754 Metazoa Species 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 238000010923 batch production Methods 0.000 description 1
- 238000003384 imaging method Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/771—Feature selection, e.g. selecting representative features from a multi-dimensional feature space
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/30—Determination of transform parameters for the alignment of images, i.e. image registration
- G06T7/33—Determination of transform parameters for the alignment of images, i.e. image registration using feature-based methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T17/00—Three dimensional [3D] modelling, e.g. data description of 3D objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T17/00—Three dimensional [3D] modelling, e.g. data description of 3D objects
- G06T17/20—Finite element generation, e.g. wire-frame surface description, tesselation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/44—Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/74—Image or video pattern matching; Proximity measures in feature spaces
- G06V10/75—Organisation of the matching processes, e.g. simultaneous or sequential comparisons of image or video features; Coarse-fine approaches, e.g. multi-scale approaches; using context analysis; Selection of dictionaries
- G06V10/751—Comparing pixel values or logical combinations thereof, or feature values having positional relevance, e.g. template matching
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20084—Artificial neural networks [ANN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30108—Industrial image inspection
- G06T2207/30124—Fabrics; Textile; Paper
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30196—Human being; Person
Abstract
인간 이미지와 같은 이미지들 간의 조밀한 대응관계를 예측하기 위한 모델을 트레이닝하는 시스템 및 방법이다. 모델은 대상의 하나 이상의 3D 컴퓨터 모델에서 생성된 합성 트레이닝 데이터를 사용하여 트레이닝될 수 있다. 또한, 하나 이상의 3D 모델의 표면으로부터 도출된 하나 이상의 측지 거리가 하나 이상의 손실값을 생성하는데 사용될 수 있으며, 이는 차례로 트레이닝 중에 모델의 파라미터를 수정하는데 사용될 수 있다.A system and method for training a model to predict dense correspondence between images, such as human images. The model may be trained using synthetic training data generated from one or more 3D computer models of the object. Additionally, one or more geodetic distances derived from the surfaces of one or more 3D models may be used to generate one or more loss values, which in turn may be used to modify the parameters of the model during training.
Description
컴퓨터 비전의 다양한 작업은 이미지들 간의 대응관계를 식별하도록 구성된 시스템에 의존한다. 이는 대상의 신체 방향, 옷, 카메라 각도 등에 따라 크게 달라질 수 있고, 대상의 오른손과 왼손과 같은 혼란스러운 유사성을 포함할 수 있는 인간 이미지와 같은 특정 맥락에서 달성하기가 특히 어려울 수 있다. 이는 또한 동물, 로봇 또는 기계 디바이스 등과 같은 다른 유형의 대상에도 어려울 수 있다. 일부 접근법에서는, 유사한 피처를 식별하기 위해 인간 주석자가 수동으로 코딩한 이미지들을 사용하여 모델이 트레이닝될 수 있다(예: 인간 이미지의 경우, 각 이미지는 눈, 귀, 코, 어깨, 팔꿈치, 손 등을 식별하도록 코딩될 수 있다). Various tasks in computer vision rely on systems configured to identify correspondences between images. This can vary greatly depending on the subject's body orientation, clothing, camera angle, etc., and can be particularly difficult to achieve in certain contexts, such as human images, which may contain confusing similarities, such as the subject's right and left hands. This can also be difficult for other types of subjects, such as animals, robots, or mechanical devices. In some approaches, the model may be trained using images manually coded by a human annotator to identify similar features (e.g., for human images, each image has an eye, ear, nose, shoulder, elbow, hand, etc. can be coded to identify).
그러나 인간 주석자가 각 참조 이미지의 모든 픽셀을 식별하는 것은 불가능하고, 인간이 피처들을 충분히 정확하게 식별할 수 없기 때문에, 인간이 주석을 단 트레이닝 예시를 해석하는 모델은 일반적으로 식별된 각 피처를 둘러싼 픽셀들을 해당 피처와 연관될 가능성이 더 높은 것으로 연관시킴으로써 추가 결론을 도출하도록 구성해야 한다. 이로 인해 잘못된 연관성이 학습될 수 있다. 예를 들어, 인간이 주석을 단 트레이닝 데이터를 사용하여, 인간 대상자의 머리가 기울어져 뺨이 어깨에 닿는 경우, 최근접 이웃 검색은 대상자의 뺨 픽셀을 대상자의 어깨에 대응하는 것으로 잘못 식별할 수 있으며 그 반대일 수도 있다. 그러면 모델이 추론 중에 추가 이미지에서 인간 형태를 오해하게 될 수 있다. 사람이 주석을 단 트레이닝 예시를 사용할 때의 또 다른 단점은 그러한 트레이닝 데이터를 생성하는데 드는 비용이 높다는 것이다.However, because it is impossible for a human annotator to identify every pixel in each reference image, and because humans cannot identify features accurately enough, models that interpret human-annotated training examples typically rely on the pixels surrounding each identified feature. It should be structured to draw additional conclusions by associating features with those that are more likely to be associated with that feature. This can result in incorrect associations being learned. For example, using human-annotated training data, if a human subject's head is tilted so that the cheek touches the shoulder, nearest neighbor search may incorrectly identify the subject's cheek pixel as corresponding to the subject's shoulder. And it may be the other way around. This may cause the model to misunderstand the human form in additional images during inference. Another disadvantage of using human-annotated training examples is the high cost of generating such training data.
본 기술은 인간 이미지와 같은 이미지들 간의 조밀한 대응관계를 예측하기 위한 모델을 트레이닝하는 시스템 및 방법에 관한 것이다. 보다 구체적으로, 본 기술은 대상의 하나 이상의 3차원 컴퓨터 모델("3D 모델"), 예를 들어 인간 대상자의 3D 모델로부터 생성된 합성 트레이닝 데이터를 사용하여 모델을 트레이닝하는 것을 제공한다. 또한, 본 기술은 트레이닝 중에 모델의 파라미터를 수정하는데 사용될 수 있는 하나 이상의 손실값을 생성하기 위해 하나 이상의 3D 모델의 표면으로부터 도출된 하나 이상의 측지 거리를 사용하는 것을 제공한다. This technology relates to a system and method for training a model to predict dense correspondence between images, such as human images. More specifically, the present technology provides for training a model using synthetic training data generated from one or more three-dimensional computer models (“3D models”) of a subject, such as a 3D model of a human subject. The technique also provides for using one or more geodetic distances derived from the surfaces of one or more 3D models to generate one or more loss values that can be used to modify parameters of the model during training.
현재 기술을 사용하면 사실상 무제한의 양과 다양한 트레이닝 데이터가 자동으로 생성될 수 있으며, 추가적으로 3D 모델의 측지 표면 정보가 트레이닝 데이터에 통합되어 예측 모델이 관련이 있는 것처럼 보일 수 있는 2차원 이미지(“2D 이미지”)에서 피처들을 정확하게 구별하는 방법을 학습할 수 있다. 따라서 본 기술에 따라 트레이닝된 예측 모델은 두 이미지들 모두에서 볼 수 있는 포인트들(예: 픽셀들) 사이를 포함하여, 이미지들의 쌍 사이의 대응관계를 예측하고, 한 이미지에서 보이는 포인트가 가려진 경우 따라서, 다른 이미지에서는 보이지 않는 경우를 인식할 때 훨씬 더 낮은 오류율을 달성할 수 있다. 또한, 본 기술에 따라 트레이닝된 예측 모델은 두 이미지들(예: 인간 대상자의 왼손과 오른손 사이)의 모호한 피처에 대한 대응 포인트들에 실수를 줄일 수 있다. 이점은 단일 대상의 이미지들 간의 대응관계를 예측하고, 두 개의 서로 다른 대상들의 이미지들 간의 대응관계를 예측할 때 모두 나타날 수 있다. 또한, 본 기술에 따라 트레이닝된 예측 모델은 이미지들 간의 대응관계 예측에 의존하는 다른 엔드투엔드 네트워크 아키텍처에 통합될 수 있어서(예: 모션 추정 또는 "광학 흐름", 인간 포즈 추정 등을 수행하도록 트레이닝된 모델), 이는 현재 기술의 이점을 보이게 하고 더 나은 성능을 달성하는 통합 모델을 만들 수 있게 한다.Using current technology, a virtually unlimited amount and variety of training data can be generated automatically, and additionally geodetic surface information from the 3D model can be incorporated into the training data to create two-dimensional images (“2D images”) with which the predictive model can appear to be related. ”), you can learn how to accurately distinguish features. Therefore, a prediction model trained according to this technique predicts correspondences between pairs of images, including between points (e.g. pixels) that are visible in both images, and when a point visible in one image is occluded. Therefore, much lower error rates can be achieved when recognizing cases that are not visible in other images. Additionally, a prediction model trained according to the present technology can reduce errors in corresponding points for ambiguous features in two images (e.g., between the left and right hands of a human subject). This advantage can appear both when predicting the correspondence between images of a single object and when predicting the correspondence between images of two different objects. Additionally, prediction models trained according to this technique can be integrated into other end-to-end network architectures that rely on predicting correspondences between images (e.g., trained to perform motion estimation or “optical flow”, human pose estimation, etc. model), which allows the creation of integrated models that demonstrate the benefits of current technologies and achieve better performance.
일 양태에서, 본 개시는 이미지들에서 대응관계를 예측하기 위해 신경 네트워크를 트레이닝하는 방법을 설명하며, 상기 방법은: 프로세싱 시스템의 하나 이상의 프로세서에 의해, 신경 네트워크를 사용하여, 대상의 제1 이미지에 기초하여 제1 피처 맵 및 대상의 제2 이미지에 기초하여 제2 피처 맵을 생성하는 단계, 상기 제1 이미지 및 제2 이미지는 상이하며, 상기 대상의 3차원 모델을 사용하여 생성되었으며; 하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제1 포인트와 상기 제2 피처 맵에 표현된 제2 포인트 사이의 제1 피처 거리를 결정하는 단계, 상기 제1 포인트와 제2 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며; 하나 이상의 프로세서에 의해, 제3 포인트와 상기 제1 피처 맵에 표현된 제4 포인트 사이의 제2 피처 거리를 결정하는 단계; 하나 이상의 프로세서에 의해, 제1 표면 맵에 표현된 제3 포인트와 제4 포인트 사이의 제1 측지 거리를 결정하는 단계, 상기 제1 표면 맵은 상기 제1 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며; 하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제3 포인트와 상기 제1 피처 맵에 표현된 제5 포인트 사이의 제3 피처 거리를 결정하는 단계; 하나 이상의 프로세서에 의해, 상기 제1 표면 맵에 표현된 제3 포인트와 제5 포인트 사이의 제2 측지 거리를 결정하는 단계; 하나 이상의 프로세서에 의해, 손실값들의 세트 중 제1 손실값을 결정하는 단계, 상기 제1 손실값은 상기 제1 피처 거리에 기초하며; 하나 이상의 프로세서에 의해, 상기 손실값들의 세트 중 제2 손실값을 결정하는 단계, 상기 제2 손실값은 상기 제2 피처 거리, 상기 제3 피처 거리, 상기 제1 측지 거리 및 상기 제2 측지 거리에 기초하며; 및 하나 이상의 프로세서에 의해, 상기 손실값들의 세트에 적어도 부분적으로 기초하여 신경 네트워크의 하나 이상의 파라미터를 수정하는 단계를 포함한다. 일부 양태에서, 상기 제1 손실값은 추가적인 피처 거리들의 세트에 더 기초하고, 상기 추가적인 피처 거리들의 세트의 각각의 주어진 피처 거리는 상기 제1 피처 맵에 표현된 선택된 포인트와 상기 제2 피처 맵에 표현된 대응 포인트 사이에 있으며, 상기 선택된 포인트와 상기 대응 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응한다. 일부 양태에서, 상기 제1 포인트와 각각의 선택된 포인트는 상기 제1 이미지 내의 모든 픽셀을 집합적으로 나타낸다. 일부 양태에서, 상기 제2 손실값은 적어도 하나의 추가적인 피처 거리들의 쌍과 적어도 하나의 추가적인 측지 거리들의 쌍에 더 기초하고, 상기 적어도 하나의 추가적인 피처 거리들의 쌍 중 각각의 주어진 추가적인 피처 거리들의 쌍은 상기 제1 피처 맵에 표현된 3개의 선택된 포인트들의 세트 사이의 2개의 피처 거리들을 포함하며, 그리고 상기 적어도 하나의 추가적인 측지 거리들의 쌍 중 각각의 주어진 추가적인 측지 거리들의 쌍은 상기 제1 표면 맵에 표시된 3개의 선택된 포인트들의 세트 사이의 2개의 측지 거리들을 포함한다. 일부 양태에서, 상기 방법은: 하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제6 포인트와 상기 제1 피처 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이의 제4 피처 거리들의 세트를 결정하는 단계; 하나 이상의 프로세서에 의해, 상기 제1 표면 맵에 표현된 제6 포인트와 상기 제1 표면 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이의 제3 측지 거리들의 세트를 결정하는 단계; 및 하나 이상의 프로세서에 의해, 상기 손실값들의 세트 중 제3 손실값을 결정하는 단계를 더 포함하며, 상기 제3 손실값은 상기 제4 피처 거리들의 세트 및 상기 제3 측지 거리들의 세트에 기초한다. 일부 양태에서, 상기 제3 손실값은 적어도 하나의 추가적인 피처 거리들의 세트와 적어도 하나의 추가적인 측지 거리들의 세트에 더 기초하고, 상기 적어도 하나의 추가적인 피처 거리들의 세트 중 각각의 주어진 추가적인 피처 거리들의 세트는 상기 제1 피처 맵에 표현된 선택된 포인트와 상기 제1 피처 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이에 있으며, 그리고 상기 적어도 하나의 추가적인 측지 거리들의 세트 중 각각의 주어진 추가적인 측지 거리들의 세트는 상기 제1 표면 맵에 표현된 선택된 포인트와 상기 제1 표면 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이에 있다. 일부 양태에서, 상기 방법은: 하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제6 포인트와 상기 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제4 피처 거리들의 세트를 결정하는 단계; 하나 이상의 프로세서에 의해, 상기 제2 표면 맵에 표현된 제1 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제3 측지 거리들의 세트를 결정하는 단계, 상기 제2 표면 맵은 상기 제2 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며, 그리고 상기 제2 표면 맵의 제1 포인트와 상기 제1 피처 맵의 제6 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며; 및 하나 이상의 프로세서에 의해, 상기 손실값들의 세트 중 제3 손실값을 결정하는 단계를 더 포함하며, 상기 제3 손실값은 상기 제4 피처 거리들의 세트 및 상기 제3 측지 거리들의 세트에 기초한다. 일부 양태에서, 상기 제3 손실값은 적어도 하나의 추가적인 피처 거리들의 세트와 적어도 하나의 추가적인 측지 거리들의 세트에 더 기초하고, 상기 적어도 하나의 추가적인 피처 거리들의 세트 중 각각의 주어진 추가적인 피처 거리들의 세트는 상기 제1 피처 맵에 표현된 선택된 포인트와 상기 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이에 있으며, 그리고 상기 적어도 하나의 추가적인 측지 거리들의 세트 중 각각의 주어진 추가적인 측지 거리들의 세트는 상기 제2 표면 맵에 표현된 대응 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이에 있으며, 상기 제2 표면 맵의 대응 포인트와 상기 제1 피처 맵의 선택된 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응한다. 일부 양태에서, 상기 방법은: 하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제7 포인트와 상기 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제5 피처 거리들의 세트를 결정하는 단계; 하나 이상의 프로세서에 의해, 상기 제2 표면 맵에 표현된 제1 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제4 측지 거리들의 세트를 결정하는 단계, 상기 제2 표면 맵은 상기 제2 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며, 그리고 상기 제2 표면 맵의 제1 포인트와 상기 제1 피처 맵의 제7 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며; 및 하나 이상의 프로세서에 의해, 상기 손실값들의 세트 중 제4 손실값을 결정하는 단계를 더 포함하며, 상기 제4 손실값은 상기 제5 피처 거리들의 세트 및 상기 제4 측지 거리들의 세트에 기초한다. 일부 양태에서, 상기 제2 표면 맵에 표현된 제1 포인트는 상기 제2 피처 맵에 표현되지 않은 상기 대상의 3차원 모델 상의 피처에 대응한다. 일부 양태에서, 하나 이상의 프로세서에 의해, 상기 제1 이미지, 상기 제2 이미지 또는 상기 제1 표면 맵 중 적어도 하나를 생성하는 단계를 더 포함한다. 일부 양태에서, 하나 이상의 프로세서에 의해, 상기 제1 이미지, 상기 제2 이미지, 상기 제1 표면 맵 또는 상기 제2 표면 맵 중 적어도 하나를 생성하는 단계를 더 포함한다. 일부 양태에서, 상기 대상은 인간 또는 인간의 표현이다. 일부 양태에서, 상기 대상은 상기 제1 이미지와 상기 제2 이미지에서 상이한 포즈이다. 일부 양태에서, 상기 제1 이미지는 상기 제2 이미지와는 상기 대상의 3차원 모델의 다른 관점으로부터 생성된다.In one aspect, the present disclosure describes a method of training a neural network to predict correspondences in images, the method comprising: processing, by one or more processors of a processing system, using the neural network, a first image of an object; generating a second feature map based on a first feature map and a second image of the object, wherein the first image and the second image are different and were created using a three-dimensional model of the object; determining, by one or more processors, a first feature distance between a first point represented in the first feature map and a second point represented in the second feature map, wherein the first point and the second point are corresponds to the same feature in a three-dimensional model of the object; determining, by one or more processors, a second feature distance between a third point and a fourth point represented in the first feature map; determining, by one or more processors, a first geodetic distance between a third point and a fourth point represented in a first surface map, the first surface map corresponding to the first image and a three-dimensional model of the object; Created using; determining, by one or more processors, a third feature distance between a third point represented in the first feature map and a fifth point represented in the first feature map; determining, by one or more processors, a second geodetic distance between a third point and a fifth point represented in the first surface map; determining, by one or more processors, a first loss value of a set of loss values, the first loss value being based on the first feature distance; determining, by one or more processors, a second loss value of the set of loss values, wherein the second loss value is the second feature distance, the third feature distance, the first geodetic distance, and the second geodetic distance. Based on; and modifying, by the one or more processors, one or more parameters of the neural network based at least in part on the set of loss values. In some aspects, the first loss value is further based on a set of additional feature distances, wherein each given feature distance of the set of additional feature distances represents a selected point represented in the first feature map and the second feature map. The selected point and the corresponding point correspond to the same feature in the three-dimensional model of the object. In some aspects, the first point and each selected point collectively represent all pixels within the first image. In some aspects, the second loss value is further based on at least one additional pair of feature distances and at least one additional pair of geodetic distances, each of the at least one additional pair of feature distances given a given pair of additional feature distances. contains two feature distances between the set of three selected points represented in the first feature map, and each given pair of at least one additional geodetic distances is in the first surface map. Contains two geodetic distances between the set of three selected points shown in . In some aspects, the method includes: a fourth set of feature distances between a sixth point represented in the first feature map and all other points of the first image represented in the first feature map, by one or more processors; determining; determining, by one or more processors, a third set of geodetic distances between a sixth point represented in the first surface map and all other points of the first image represented in the first surface map; and determining, by one or more processors, a third loss value of the set of loss values, wherein the third loss value is based on the fourth set of feature distances and the third set of geodetic distances. . In some aspects, the third loss value is further based on a set of at least one additional feature distances and at least one set of additional geodetic distances, each of the set of at least one additional feature distances given a set of additional feature distances. is between the selected point represented in the first feature map and all other points of the first image represented in the first feature map, and of each of the given additional geodetic distances of the set of at least one additional geodetic distances. The set is between the selected point represented in the first surface map and all other points of the first image represented in the first surface map. In some aspects, the method includes: generating, by one or more processors, a fourth set of feature distances between a sixth point represented in the first feature map and all points of a second image represented in the second feature map; deciding step; determining, by one or more processors, a third set of geodetic distances between a first point represented in the second surface map and all points in a second image represented in the second surface map, the second surface The map corresponds to the second image and was created using a three-dimensional model of the object, and the first point of the second surface map and the sixth point of the first feature map are the same in the three-dimensional model of the object. Corresponds to a feature; and determining, by one or more processors, a third loss value of the set of loss values, wherein the third loss value is based on the fourth set of feature distances and the third set of geodetic distances. . In some aspects, the third loss value is further based on a set of at least one additional feature distances and at least one set of additional geodetic distances, each of the set of at least one additional feature distances given a set of additional feature distances. is between the selected point represented in the first feature map and all points in the second image represented in the second feature map, and for each given set of additional geodetic distances of the at least one set of additional geodetic distances. is between the corresponding point represented in the second surface map and all points in the second image represented in the second surface map, and the corresponding point in the second surface map and the selected point in the first feature map are Corresponds to the same feature in the three-dimensional model of the object. In some aspects, the method includes: generating, by one or more processors, a fifth set of feature distances between a seventh point represented in the first feature map and all points of a second image represented in the second feature map; deciding step; determining, by one or more processors, a fourth set of geodetic distances between a first point represented in the second surface map and all points in a second image represented in the second surface map, the second surface The map corresponds to the second image and was created using a three-dimensional model of the object, and the first point of the second surface map and the seventh point of the first feature map are the same in the three-dimensional model of the object. Corresponds to a feature; and determining, by one or more processors, a fourth loss value of the set of loss values, wherein the fourth loss value is based on the fifth set of feature distances and the fourth set of geodetic distances. . In some aspects, the first point represented in the second surface map corresponds to a feature on a three-dimensional model of the object that is not represented in the second feature map. In some aspects, the method further includes generating, by one or more processors, at least one of the first image, the second image, or the first surface map. In some aspects, the method further includes generating, by one or more processors, at least one of the first image, the second image, the first surface map, or the second surface map. In some embodiments, the subject is a human or a representation of a human. In some aspects, the subject is in a different pose in the first image and the second image. In some aspects, the first image is generated from a different perspective of the three-dimensional model of the object than the second image.
다른 양태에서, 본 개시는 신경 네트워크를 저장하는 메모리 및 상기 메모리에 연결되고 상기 신경 네트워크를 사용하여 이미지들에서 대응관계를 예측하도록 구성된 하나 이상의 프로세서를 포함하는 프로세싱 시스템을 설명하며, 상기 신경 네트워크는 트레이닝 방법에 따라 이미지들의 대응관계를 예측하도록 트레이닝되었으며, 상기 트레이닝 방법은: 대상의 제1 이미지에 기초하여 제1 피처 맵 및 대상의 제2 이미지에 기초하여 제2 피처 맵을 생성하는 단계, 상기 제1 이미지 및 제2 이미지는 상이하며, 상기 대상의 3차원 모델을 사용하여 생성되었으며; 상기 제1 피처 맵에 표현된 제1 포인트와 상기 제2 피처 맵에 표현된 제2 포인트 사이의 제1 피처 거리를 결정하는 단계, 상기 제1 포인트와 제2 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며; 제3 포인트와 상기 제1 피처 맵에 표현된 제4 포인트 사이의 제2 피처 거리를 결정하는 단계; 제1 표면 맵에 표현된 제3 포인트와 제4 포인트 사이의 제1 측지 거리를 결정하는 단계, 상기 제1 표면 맵은 상기 제1 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며; 상기 제1 피처 맵에 표현된 제3 포인트와 상기 제1 피처 맵에 표현된 제5 포인트 사이의 제3 피처 거리를 결정하는 단계; 상기 제1 표면 맵에 표현된 제3 포인트와 제5 포인트 사이의 제2 측지 거리를 결정하는 단계; 손실값들의 세트 중 제1 손실값을 결정하는 단계, 상기 제1 손실값은 상기 제1 피처 거리에 기초하며; 상기 손실값들의 세트 중 제2 손실값을 결정하는 단계, 상기 제2 손실값은 상기 제2 피처 거리, 상기 제3 피처 거리, 상기 제1 측지 거리 및 상기 제2 측지 거리에 기초하며; 및 상기 손실값들의 세트에 적어도 부분적으로 기초하여 신경 네트워크의 하나 이상의 파라미터를 수정하는 단계를 포함한다. 일부 양태에서, 상기 신경 네트워크는 트레이닝 방법에 따라 이미지들의 대응관계를 예측하도록 트레이닝되었으며, 상기 트레이닝 방법은 추가로: 상기 제1 피처 맵에 표현된 제6 포인트와 상기 제1 피처 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이의 제4 피처 거리들의 세트를 결정하는 단계; 상기 제1 표면 맵에 표현된 제6 포인트와 상기 제1 표면 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이의 제3 측지 거리들의 세트를 결정하는 단계; 및 상기 손실 값들의 세트 중 제3 손실 값을 결정하는 단계를 더 포함하며, 상기 제3 손실 값은 상기 제4 피처 거리들의 세트 및 상기 제3 측지 거리들의 세트에 기초한다. 일부 양태에서, 상기 신경 네트워크는 트레이닝 방법에 따라 이미지들의 대응관계를 예측하도록 트레이닝되었으며, 상기 트레이닝 방법은 추가로: 상기 제1 피처 맵에 표현된 제6 포인트와 상기 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제4 피처 거리들의 세트를 결정하는 단계; 상기 제2 표면 맵에 표현된 제1 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제3 측지 거리들의 세트를 결정하는 단계, 상기 제2 표면 맵은 상기 제2 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며, 그리고 상기 제2 표면 맵의 제1 포인트와 상기 제1 피처 맵의 제6 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며; 및 상기 손실 값들의 세트 중 제3 손실 값을 결정하는 단계를 더 포함하며, 상기 제3 손실 값은 상기 제4 피처 거리들의 세트 및 상기 제3 측지 거리들의 세트에 기초하는, 프로세싱 시스템. 일부 양태에서, 상기 신경 네트워크는 트레이닝 방법에 따라 이미지들의 대응관계를 예측하도록 트레이닝되었으며, 상기 트레이닝 방법은 추가로: 상기 제1 피처 맵에 표현된 제7 포인트와 상기 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제5 피처 거리들의 세트를 결정하는 단계; 상기 제2 표면 맵에 표현된 제1 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제4 측지 거리들의 세트를 결정하는 단계, 상기 제2 표면 맵은 상기 제2 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며, 그리고 상기 제2 표면 맵의 제1 포인트와 상기 제1 피처 맵의 제7 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며; 및 상기 손실값들의 세트 중 제4 손실값을 결정하는 단계를 더 포함하며, 상기 제4 손실값은 상기 제5 피처 거리들의 세트 및 상기 제4 측지 거리들의 세트에 기초한다.In another aspect, the present disclosure describes a processing system including a memory storing a neural network and one or more processors coupled to the memory and configured to predict correspondences in images using the neural network, the neural network Trained to predict correspondence between images according to a training method, the training method comprising: generating a first feature map based on a first image of the object and a second feature map based on a second image of the object; The first image and the second image are different and were created using a three-dimensional model of the object; Determining a first feature distance between a first point represented in the first feature map and a second point represented in the second feature map, wherein the first point and the second point are in the three-dimensional model of the object. Corresponds to the same feature; determining a second feature distance between a third point and a fourth point represented in the first feature map; determining a first geodetic distance between a third point and a fourth point represented in a first surface map, the first surface map corresponding to the first image and generated using a three-dimensional model of the object; determining a third feature distance between a third point represented in the first feature map and a fifth point represented in the first feature map; determining a second geodetic distance between a third point and a fifth point represented in the first surface map; determining a first loss value from a set of loss values, the first loss value being based on the first feature distance; determining a second loss value of the set of loss values, the second loss value being based on the second feature distance, the third feature distance, the first geodetic distance, and the second geodetic distance; and modifying one or more parameters of the neural network based at least in part on the set of loss values. In some aspects, the neural network is trained to predict correspondences of images according to a training method, the training method further comprising: a sixth point represented in the first feature map and a sixth point represented in the first feature map. 1 determining a fourth set of feature distances between all other points in the image; determining a third set of geodetic distances between a sixth point represented in the first surface map and all other points of the first image represented in the first surface map; and determining a third loss value of the set of loss values, wherein the third loss value is based on the fourth set of feature distances and the third set of geodetic distances. In some aspects, the neural network is trained to predict correspondences of images according to a training method, the training method further comprising: a sixth point represented in the first feature map and a sixth point represented in the second feature map. 2 determining a fourth set of feature distances between all points in the image; determining a third set of geodetic distances between a first point represented in the second surface map and all points in a second image represented in the second surface map, the second surface map being corresponds to and was generated using a three-dimensional model of the object, and the first point of the second surface map and the sixth point of the first feature map correspond to the same feature in the three-dimensional model of the object; and determining a third loss value of the set of loss values, wherein the third loss value is based on the fourth set of feature distances and the third set of geodetic distances. In some aspects, the neural network is trained to predict correspondences of images according to a training method, the training method further comprising: a seventh point represented in the first feature map and a seventh point represented in the second feature map. 2 determining a fifth set of feature distances between all points in the image; determining a fourth set of geodetic distances between a first point represented in the second surface map and all points in a second image represented in the second surface map, the second surface map being corresponds to and was generated using a three-dimensional model of the object, and the first point of the second surface map and the seventh point of the first feature map correspond to the same feature in the three-dimensional model of the object; and determining a fourth loss value of the set of loss values, wherein the fourth loss value is based on the fifth set of feature distances and the fourth set of geodetic distances.
도 1은 본 개시의 양태에 따른 예시적 시스템의 기능도이다.
도 2는 본 개시의 양태에 따른 예시적 시스템의 기능도이다.
도 3은 주어진 2D 이미지의 포인트들이 대응하는 피처 맵과 3D 표면 맵에서 어떻게 표현될 수 있는지를 보여주는 다이어그램이다.
도 4a 및 4b는 본 기술의 양태에 따라 이미지들의 대응관계를 식별하기 위해 신경 네트워크를 트레이닝하는 예시적 방법을 보여주는 흐름도이다.
도 5는 본 기술의 양태에 따라 도 4a 및 도 4b의 방법(400a, 400b)의 손실값과 결합하여 추가 손실값을 결정하는 예시적 방법을 도시하는 흐름도이다.
도 6는 본 기술의 양태에 따라 도 4a 및 도 4b의 방법(400a, 400b)의 손실값과 결합하여 추가 손실값을 결정하는 예시적 방법을 도시하는 흐름도이다.
도 7은 본 기술의 양태에 따라 도 4a, 4b 및 도 5의 방법 400a, 400b, 500의 손실값과 결합하여 추가 손실값을 결정하는 예시적 방법을 도시하는 흐름도이다.1 is a functional diagram of an example system in accordance with aspects of the present disclosure.
2 is a functional diagram of an example system in accordance with aspects of the present disclosure.
Figure 3 is a diagram showing how points in a given 2D image can be represented in the corresponding feature map and 3D surface map.
4A and 4B are flowcharts showing an example method of training a neural network to identify correspondences between images in accordance with aspects of the present technology.
FIG. 5 is a flow diagram illustrating an example method for determining additional loss values in combination with the loss values of methods 400a and 400b of FIGS. 4A and 4B in accordance with aspects of the present technology.
FIG. 6 is a flow chart illustrating an example method for determining additional loss values in combination with the loss values of methods 400a and 400b of FIGS. 4A and 4B in accordance with aspects of the present technology.
FIG. 7 is a flow chart illustrating an example method for determining additional loss values in combination with the loss values of methods 400a, 400b, and 500 of FIGS. 4A, 4B, and 5 in accordance with aspects of the present technology.
본 기술은 이제 다음의 예시적 시스템 및 방법과 관련하여 설명될 것이다.The technology will now be described in relation to the following example systems and methods.
본 명세서에 설명된 방법을 수행하기 위한 예시적 프로세싱 시스템의 상위 레벨 시스템 다이어그램(100)이 도 1에 도시되어 있다. 프로세싱 시스템(102)은 하나 이상의 프로세서(104) 및 메모리(106)를 포함할 수 있다. 메모리(106)는 프로세서(들)(104)에 의해 실행되거나 다른 방식으로 사용될 수 있는 명령어(108) 및 데이터(110)를 포함하여, 하나 이상의 프로세서(104)에 의해 액세스 가능한 정보를 저장한다. 메모리(106)는 프로세서(들)(104)에 의해 액세스 가능한 정보를 저장할 수 있는 임의의 비일시적 유형일 수 있다. 예를 들어, 메모리(106)는 하드 드라이브, 메모리 카드, 광학 디스크, 솔리드 스테이트, 테이프 메모리 등과 같은 비일시적 매체를 포함할 수 있다. 명령어들(108) 및 데이터(110)는 본 명세서에 기술된 바와 같이 예측 모델, 트레이닝 데이터(예를 들어, 2D 이미지들 및 대응하는 3D 모델 데이터) 및 트레이닝 데이터를 생성하는데 사용되는 3D 모델을 포함할 수 있다. A high-level system diagram 100 of an example processing system for performing the methods described herein is shown in FIG. 1 . Processing system 102 may include one or more processors 104 and memory 106. Memory 106 stores information accessible by one or more processors 104, including instructions 108 and data 110 that may be executed or otherwise used by processor(s) 104. Memory 106 may be of any non-transitory type capable of storing information accessible by processor(s) 104. For example, memory 106 may include non-transitory media such as hard drives, memory cards, optical disks, solid state, tape memory, etc. Instructions 108 and data 110 include a prediction model, training data (e.g., 2D images and corresponding 3D model data), and a 3D model used to generate the training data, as described herein. can do.
프로세싱 시스템(102)은 임의의 유형의 일반 컴퓨팅 디바이스, 서버 또는 그 세트와 같은 임의의 유형의 컴퓨팅 디바이스(들)에서 구현될 수 있으며, 범용 컴퓨팅 디바이스 또는 서버에 일반적으로 존재하는 다른 컴포넌트를 더 포함할 수 있다. 따라서, 프로세싱 시스템(102)은 서버, 개인용 컴퓨터 또는 모바일 디바이스와 같은 단일 컴퓨팅 디바이스에 상주할 수 있으며, 따라서 여기에 설명된 모델은 해당 단일 컴퓨팅 디바이스에 로컬적일 수 있다. 유사하게, 프로세싱 시스템(102)은 클라우드 컴퓨팅 시스템 또는 다른 분산 시스템에 상주할 수 있으므로, 여기에 설명된 트레이닝 데이터를 생성하는데 사용되는 예측 모델, 트레이닝 데이터 및/또는 3D 모델이 2개 이상의 서로 다른 물리적 컴퓨팅 디바이스에 걸쳐 분산된다.Processing system 102 may be implemented in any type of computing device(s), such as any type of general computing device, server, or set thereof, and may further include other components typically present in a general purpose computing device or server. can do. Accordingly, processing system 102 may reside on a single computing device, such as a server, personal computer, or mobile device, and thus the model described herein may be local to that single computing device. Similarly, processing system 102 may reside in a cloud computing system or other distributed system, such that the prediction models, training data, and/or 3D models used to generate the training data described herein may be stored in two or more different physical locations. Distributed across computing devices.
이와 관련하여, 도 2는 본 명세서에 설명된 방법을 수행하기 위한 예시적 프로세싱 시스템(202)이 제1 컴퓨팅 디바이스(202a), 제2 컴퓨팅 디바이스(202b) 및 데이터베이스(202c)를 포함하는 것으로 도시되어 있는 추가적인 상위 레벨 시스템 다이어그램(200)을 도시하며, 이들 각각은 하나 이상의 네트워크(216)를 통해 서로 연결된다. 제1 컴퓨팅 디바이스(202a), 제2 컴퓨팅 디바이스(202b) 및 데이터베이스(202c)는 각각 단일 컴퓨팅 디바이스일 수 있거나, 2개 이상의 서로 다른 물리적 컴퓨팅 디바이스에 걸쳐 분산될 수 있다. 도 1의 예와 마찬가지로, 제1 컴퓨팅 디바이스(202a)와 제2 컴퓨팅 디바이스(202b)는 각각 명령어(208a, 208b)와 데이터(210a, 210b)를 저장하는 하나 이상의 프로세서(204a, 204b)와 메모리(206a, 206b)를 포함한다. 도시되지는 않았지만, 데이터베이스(202c)는 또한 명령어 및 데이터를 저장하는 하나 이상의 프로세서 및 메모리를 포함할 수 있다.In this regard, FIG. 2 illustrates an example processing system 202 for performing the methods described herein as including a first computing device 202a, a second computing device 202b, and a database 202c. Additional high-level system diagrams 200 are shown, each of which is connected to one another through one or more networks 216. First computing device 202a, second computing device 202b, and database 202c may each be a single computing device, or may be distributed across two or more different physical computing devices. Similar to the example of FIG. 1 , the first computing device 202a and the second computing device 202b include one or more processors 204a and 204b and memory that store instructions 208a and 208b and data 210a and 210b, respectively. Includes (206a, 206b). Although not shown, database 202c may also include one or more processors and memory to store instructions and data.
도 2의 예에 도시된 바와 같이, 제1 컴퓨팅 디바이스(202a)의 데이터(210a)는 예측 모델(212) 및 예측 모델(212)을 트레이닝하는데 사용되는 2D 이미지 및 대응하는 3D 모델 데이터를 포함하는 트레이닝 데이터(214a)를 저장한다. 제2 컴퓨팅 디바이스(202b)의 데이터(210b)는 하나 이상의 3D 모델(218)과 2D 이미지 및 하나 이상의 3D 모델(218)을 사용하여 생성된 대응하는 3D 모델 데이터를 포함하는, 트레이닝 데이터 세트(214b)를 저장할 수 있다. 마지막으로, 데이터베이스(202c)는 또한 2D 이미지 및 하나 이상의 3D 모델(218)을 사용하여 제2 컴퓨팅 디바이스(202b)에 의해 생성된 대응하는 3D 모델 데이터를 포함하는 트레이닝 데이터(214c) 세트를 저장할 수 있다. 이와 관련하여, 트레이닝 데이터(214a, 214b, 214c)의 세트는 서로 동일할 수 있거나, 하나 이상이 다른 것의 서브세트일 수 있다. 예를 들어, 제2 컴퓨팅 디바이스(202b)는 하나 이상의 3D 모델(218)로부터 트레이닝 데이터(214b)를 생성하고, 그 트레이닝 데이터를 데이터베이스(202c)에 전달하여 마스터 세트(214c)에 저장되도록 구성될 수 있다. 결과적으로, 제1 컴퓨팅 디바이스(202a)는 데이터베이스(202c)로부터 트레이닝 데이터(214c)의 일부 또는 전부를 획득하고, 이를 트레이닝 예측 모델(212)에 사용하기 위한 트레이닝 데이터(214a)로서 자신의 메모리(206a)에 로컬로 저장하도록 구성될 수 있다.As shown in the example of FIG. 2 , data 210a of first computing device 202a includes prediction model 212 and 2D images and corresponding 3D model data used to train prediction model 212. Store training data 214a. The data 210b of the second computing device 202b is a training data set 214b, including one or more 3D models 218 and 2D images and corresponding 3D model data generated using the one or more 3D models 218. ) can be saved. Finally, database 202c may also store a set of training data 214c that includes 2D images and corresponding 3D model data generated by second computing device 202b using one or more 3D models 218. there is. In this regard, the sets of training data 214a, 214b, 214c may be identical to each other, or one or more may be a subset of the other. For example, the second computing device 202b may be configured to generate training data 214b from one or more 3D models 218 and pass the training data to the database 202c to be stored in the master set 214c. You can. As a result, the first computing device 202a obtains some or all of the training data 214c from the database 202c and stores it in its memory ( 206a) may be configured to store locally.
모든 경우에, 본 명세서에 설명된 컴퓨팅 디바이스는 사용자 인터페이스 서브시스템과 같은 컴퓨팅 디바이스와 관련하여 일반적으로 사용되는 임의의 다른 컴포넌트를 더 포함할 수 있다. 사용자 인터페이스 서브시스템은 하나 이상의 사용자 입력(예: 마우스, 키보드, 터치 스크린 및/또는 마이크) 및 하나 이상의 전자 디스플레이(예: 화면을 갖춘 모니터 또는 정보를 디스플레이하도록 작동 가능한 기타 전기 디바이스)를 포함할 수 있다. 전자 디스플레이 외에 스피커, 조명, 진동, 펄스 또는 촉각 엘리먼트와 같은 출력 디바이스도 여기에 설명된 컴퓨팅 디바이스에 포함될 수 있다. In all cases, the computing devices described herein may further include any other components commonly used in connection with computing devices, such as a user interface subsystem. The user interface subsystem may include one or more user inputs (e.g., a mouse, keyboard, touch screen, and/or microphone) and one or more electronic displays (e.g., a monitor with a screen or other electrical device operable to display information). there is. In addition to electronic displays, output devices such as speakers, lights, vibration, pulse, or tactile elements may also be included in computing devices described herein.
각 컴퓨팅 디바이스에 포함된 하나 이상의 프로세서는 상업적으로 이용 가능한 중앙 처리 장치("CPU"), 그래픽 처리 장치("GPU"), 텐서 처리 장치("TPU") 등과 같은 임의의 기존 프로세서일 수 있다. 대안적으로, 하나 이상의 프로세서는 ASIC 또는 기타 하드웨어 기반 프로세서와 같은 전용 디바이스일 수 있다. 각 프로세서는 병렬로 동작할 수 있는 여러 코어가 있을 수 있다. 단일 컴퓨팅 디바이스의 프로세서(들), 메모리 및 기타 엘리먼트는 단일 물리적 하우징 내에 저장될 수 있거나 두 개 이상의 하우징 사이에 분산될 수 있다. 유사하게, 컴퓨팅 디바이스의 메모리는 외부 데이터베이스 또는 네트워크로 연결된 저장 디바이스와 같이 프로세서(들)의 하우징과 상이한 하우징에 위치한 하드 드라이브 또는 기타 저장 매체를 포함할 수 있다. 따라서, 프로세서 또는 컴퓨팅 디바이스에 대한 참조는 병렬로 동작하거나 병렬로 동작하지 않을 수 있는 프로세서 또는 컴퓨팅 디바이스 또는 메모리의 집합 뿐만 아니라 로드 밸런싱 서버 팜의 하나 이상의 서버 또는 클라우드 기반 시스템에 대한 참조를 포함하는 것으로 이해될 것이다. The one or more processors included in each computing device may be any conventional processor, such as a commercially available central processing unit (“CPU”), graphics processing unit (“GPU”), tensor processing unit (“TPU”), etc. Alternatively, one or more processors may be dedicated devices, such as ASICs or other hardware-based processors. Each processor may have multiple cores that can operate in parallel. The processor(s), memory and other elements of a single computing device may be stored within a single physical housing or may be distributed between two or more housings. Similarly, the memory of a computing device may include a hard drive or other storage medium located in a different housing than that of the processor(s), such as an external database or networked storage device. Accordingly, a reference to a processor or computing device is intended to include a collection of processors or computing devices or memory that may or may not operate in parallel, as well as a reference to one or more servers in a load balancing server farm or cloud-based system. You will understand.
본 명세서에 기술된 컴퓨팅 디바이스는 프로세서(들)에 의해 직접(머신 코드와 같은) 또는 간접적으로(스크립트와 같은) 실행될 수 있는 명령어들을 저장할 수 있다. 컴퓨팅 디바이스는 또한 명령어들에 따라 하나 이상의 프로세서에 의해 검색, 저장 또는 수정될 수 있는 데이터를 저장할 수 있다. 명령어들은 컴퓨팅 디바이스 판독가능 매체에 컴퓨팅 디바이스 코드로서 저장될 수 있다. 이와 관련하여, 용어 "명령어" 및 "프로그램"은 본 명세서에서 상호교환적으로 사용될 수 있다. 명령어들은 또한 프로세서(들)에 의한 직접 프로세싱을 위해 객체 코드 포맷으로 저장될 수 있거나, 스크립트들 또는 필요에 따라 인터프리트되거나 미리 컴파일되는 독립적인 소스 코드 모듈의 집합을 포함하는 임의의 다른 컴퓨팅 디바이스 언어로 저장될 수 있다. 예를 들어, 프로그래밍 언어는 C#, C++, JAVA 또는 다른 컴퓨터 프로그래밍 언어일 수 있다. 유사하게, 명령어 또는 프로그램의 임의의 컴포넌트는 JavaScript, PHP, ASP 또는 기타 컴퓨터 스크립트 언어와 같은 컴퓨터 스크립트 언어로 구현될 수 있다. 또한, 이들 컴포넌트 중 어느 하나는 컴퓨터 프로그래밍 언어와 컴퓨터 스크립팅 언어의 조합을 사용하여 구현될 수 있다. Computing devices described herein may store instructions that can be executed directly (such as machine code) or indirectly (such as scripts) by processor(s). A computing device may also store data that can be retrieved, stored, or modified by one or more processors according to instructions. The instructions may be stored as computing device code on a computing device-readable medium. In this regard, the terms “instruction” and “program” may be used interchangeably herein. Instructions may also be stored in object code format for direct processing by the processor(s), or in any other computing device language, including scripts or a set of independent source code modules that are interpreted or precompiled as needed. It can be saved as . For example, the programming language may be C#, C++, JAVA, or another computer programming language. Similarly, the instructions or any component of the program may be implemented in a computer scripting language such as JavaScript, PHP, ASP, or other computer scripting language. Additionally, any of these components may be implemented using a combination of computer programming languages and computer scripting languages.
도 2의 예에 도시된 바와 같이, 제1 컴퓨팅 디바이스(202a)의 데이터(210a)는 예측 모델(212) 및 예측 모델(212)을 트레이닝하는데 사용되는 2D 이미지 및 대응하는 3D 모델 데이터를 포함하는 트레이닝 데이터(214a)를 저장한다. 제2 컴퓨팅 디바이스(202b)의 데이터(210b)는 하나 이상의 3D 모델(218)과 2D 이미지 및 하나 이상의 3D 모델(218)을 사용하여 생성된 대응하는 3D 모델 데이터를 포함하는, 트레이닝 데이터 세트(214b)를 저장할 수 있다. 마지막으로, 데이터베이스(202c)는 또한 제2 컴퓨팅 디바이스(202b)에 의해 생성된 2D 이미지 및 대응하는 3D 모델 데이터를 포함하는 트레이닝 데이터(214c) 세트를 저장할 수 있다. 이와 관련하여, 트레이닝 데이터(214a, 214b, 214c)의 세트는 서로 동일할 수 있거나, 하나 이상이 다른 것의 서브세트일 수 있다. 예를 들어, 제2 컴퓨팅 디바이스(202b)는 하나 이상의 3D 모델(218)로부터 트레이닝 데이터(214b)를 생성하고, 그 트레이닝 데이터를 데이터베이스(202c)에 전달하여 마스터 세트(214c)에 저장되도록 구성될 수 있다. 결과적으로, 제1 컴퓨팅 디바이스(202a)는 데이터베이스(202c)로부터 트레이닝 데이터(214c)의 일부 또는 전부를 획득하고, 이를 트레이닝 예측 모델(212)에 사용하기 위한 트레이닝 데이터(214a)로서 자신의 메모리(206a)에 로컬로 저장하도록 구성될 수 있다.As shown in the example of FIG. 2 , data 210a of first computing device 202a includes prediction model 212 and 2D images and corresponding 3D model data used to train prediction model 212. Store training data 214a. The data 210b of the second computing device 202b is a training data set 214b, including one or more 3D models 218 and 2D images and corresponding 3D model data generated using the one or more 3D models 218. ) can be saved. Finally, database 202c may also store a set of training data 214c that includes 2D images and corresponding 3D model data generated by second computing device 202b. In this regard, the sets of training data 214a, 214b, 214c may be identical to each other, or one or more may be a subset of the other. For example, the second computing device 202b may be configured to generate training data 214b from one or more 3D models 218 and pass the training data to the database 202c to be stored in the master set 214c. You can. As a result, the first computing device 202a obtains some or all of the training data 214c from the database 202c and stores it in its memory ( 206a) may be configured to store locally.
도 3은 주어진 2D 이미지의 포인트들이 대응하는 피처 맵과 3D 표면 맵에서 어떻게 표현될 수 있는지를 도시한다. 이에 관하여, 도 3은 인간 대상자의 3D 모델(도시되지 않음)로부터 생성된 2D 이미지(302)를 도시한다. 3D 모델은 또한 2D 이미지(302)를 생성하는데 사용된 것과 동일한 포즈로 대상의 대응 표면 맵(306)을 생성하는데 사용될 수 있다. 전체 3D 표면 맵(306)의 두 발췌 부분(306a 및 306b)이 도 3에 도시되어 있다. 이미지(302)는 예를 들어 학습된 임베딩 함수에 의해 프로세싱되어 피처 맵(304)을 생성할 수 있다. 피처 맵(304)은 도 3에 그림으로 도시되어 있지만, 실제로는 데이터베이스, 텐서, 벡터 등과 같은 임의의 적절한 형태와 차원을 취할 수 있다. 예를 들어, 높이(픽셀)가 H이고 너비(픽셀)가 W인 이미지의 경우, 피처 맵 F는 아래의 식 1에 따라 표현된다:Figure 3 shows how the points of a given 2D image can be represented in the corresponding feature map and 3D surface map. In this regard, Figure 3 shows a 2D image 302 generated from a 3D model (not shown) of a human subject. The 3D model can also be used to generate a corresponding surface map 306 of the object in the same pose used to generate the 2D image 302. Two excerpts 306a and 306b of the full 3D surface map 306 are shown in FIG. 3 . Image 302 may be processed, for example, by a learned embedding function to generate feature map 304. Feature map 304 is depicted graphically in Figure 3, but in practice it can take on any suitable form and dimension, such as a database, tensor, vector, etc. For example, for an image with height (pixels) H and width (pixels) W, the feature map F is expressed according to Equation 1 below:
(1) (One)
이러한 경우, 이미지의 각 픽셀은 피처 맵 F에서 차원 C의 벡터로 표현되며, 이는 임의의 적합한 정수(예: 1, 16 등)일 수 있다.In this case, each pixel in the image is represented in the feature map F by a vector of dimension C, which can be any suitable integer (e.g. 1, 16, etc.).
도 3의 예에서, 3개의 포인트 A, B, C는 2D 이미지(302), 피처 맵(304), 및 3D 표면 맵 발췌(306a 및/또는 306b) 각각에서 식별된다. 여기서 다시, 포인트가 피처 맵(304)에 그림으로 도시되어 있지만, 실제로는 이러한 포인트들은 데이터베이스에서 별도의 값으로 표현되거나 텐서 또는 벡터의 서로 다른 엘리먼트로 표현될 수 있다. 볼 수 있는 바와 같이, 피처 맵(304)은 포인트 A와 포인트 B 사이의 제1 피처 거리(dA,B) 및 포인트 B와 포인트 C 사이의 제2 피처 거리(dB,C)를 결정하는데 사용될 수 있다. 이들 피처 거리들은 임의의 적절한 식을 사용하여 결정될 수 있다. 예를 들어, 2D 이미지(302)의 각 포인트가 피처 맵(304)에서 별개의 벡터로 표현되면, 포인트 A와 B를 나타내는 벡터 사이의 피처 거리는 두 벡터의 내적을 계산함으로써, 두 벡터의 외적을 계산함으로써, 두 벡터를 더함으로써, 두 벡터를 뺌으로써 결정될 수 있다. 이와 관련하여, 식 1과 관련하여 상기 기술된 바와 같이 피처 맵 F에 대해, 이미지 I1의 포인트 p와 이미지 I2의 포인트 q 사이의 피처 거리 d는 다음의 식 2에 따라 계산될 수 있다:In the example of Figure 3, three points A, B, and C are identified in the 2D image 302, feature map 304, and 3D surface map excerpt 306a and/or 306b, respectively. Here again, although the points are depicted graphically in the feature map 304, in reality these points may be represented as separate values in the database or as different elements of tensors or vectors. As can be seen, feature map 304 determines a first feature distance between point A and point B (d A,B ) and a second feature distance between point B and point C (d B,C ). can be used These feature distances can be determined using any suitable equation. For example, if each point in the 2D image 302 is represented by a separate vector in the feature map 304, the feature distance between the vectors representing points A and B is calculated by calculating the dot product of the two vectors, It can be determined by calculating, by adding two vectors, or by subtracting two vectors. In this regard, for a feature map F as described above with respect to equation 1, the feature distance d between point p in image I 1 and point q in image I 2 can be calculated according to equation 2:
(2) (2)
일부 양태에서, 이미지 I1의 포인트 p와 이미지 I2의 포인트 q를 각각 나타내는 피처 벡터 F1(p) 및 F2(q)는 먼저 정규화되어 단위 벡터가 될 수 있다.In some aspects, feature vectors F 1 (p) and F 2 (q) representing point p in image I 1 and point q in image I 2 , respectively, may first be normalized to become unit vectors.
마찬가지로, 3D 표면 맵(306)은 포인트 A와 포인트 B 사이의 제1 측지 거리(gA,B) 및 포인트 B와 포인트 C 사이의 제2 측지 거리(gB,C)를 결정하는데 사용될 수 있다. 측지 거리는 이들 포인트 사이의 거리를 3D 표면을 따라 측정된 대로 표현하며, 3D 표면의 해당 포인트 사이의 최단 거리를 계산(또는 추정)하기 위한 임의의 적합한 수식 또는 방법에 따라 결정될 수 있다.Likewise, 3D surface map 306 can be used to determine a first geodetic distance between point A and point B (g A,B ) and a second geodetic distance between point B and point C (g B,C ). . The geodetic distance expresses the distance between these points as measured along a 3D surface, and may be determined according to any suitable formula or method for calculating (or estimating) the shortest distance between corresponding points on a 3D surface.
도 4a 및 도 4b는 이미지의 대응관계를 식별하기 위해 신경 네트워크이 프로세싱 시스템(예를 들어, 프로세싱 시스템(102 또는 202))에 의해 트레이닝될 수 있는 방법을 보여주는 예시적 방법(400a, 400b)을 도시한다. 이와 관련하여, 단계(402)에서, 대상의 3D 모델로부터 제1 이미지 및 제1 표면 맵이 생성된다. 마찬가지로, 단계(404)에서, 3D 모델로부터 제1 이미지와 상이한 제2 이미지가 생성된다. 제1 이미지는 제1 포즈에서 및/또는 제1 카메라 방향으로부터 생성될 수 있으며, 대응하는 제1 표면 맵은 적어도 그 제1 포즈에서 3D 모델의 표면을 나타낼 것이다. 제2 이미지는 제1 이미지와 상이한 이미지를 생성하는 임의의 방식으로 생성될 수 있다. 예를 들어, 제2 이미지는 3D 모델을 제2 포즈에 배치함으로써, 다른 관점에서 3D 모델을 이미징함으로써(예를 들어, 다른 가상 카메라 포지션으로부터 이미지를 생성함으로써), 3D 모델을 제1 이미지를 생성하는데 사용된 것과 다른 조명에 노출시킴으로써 생성될 수 있다. 제1 이미지, 제1 표면 맵 및 제2 이미지는 신경 네트워크를 트레이닝하는데 사용되는 프로세싱 시스템(예: 프로세싱 시스템(102 또는 202))에 의해 생성되거나, 별도의 시스템에 의해 생성되어 신경 네트워크를 트레이닝하는 프로세싱 시스템에 제공될 수 있다.4A and 4B illustrate example methods 400a and 400b showing how a neural network may be trained by a processing system (e.g., processing system 102 or 202) to identify correspondences in images. do. In this regard, at step 402, a first image and a first surface map are generated from a 3D model of the object. Likewise, at step 404, a second image different from the first image is generated from the 3D model. The first image may be generated at a first pose and/or from a first camera direction, and the corresponding first surface map will represent the surface of the 3D model at least at that first pose. The second image can be created in any way that creates a different image than the first image. For example, the second image creates the first image of the 3D model by placing the 3D model in a second pose, by imaging the 3D model from a different perspective (e.g., by creating an image from a different virtual camera position). It can be produced by exposure to lighting different from that used to produce light. The first image, first surface map, and second image may be generated by a processing system (e.g., processing system 102 or 202) used to train the neural network, or may be generated by a separate system to train the neural network. May be provided to a processing system.
단계(406)에서, 프로세싱 시스템은 제1 이미지로부터 제1 피처 맵을 생성하고, 제2 이미지로부터 제2 피처 맵을 생성한다. 도 3과 관련하여 위에서 설명한 바와 같이, 프로세싱 시스템은 임의의 적절한 방식으로 피처 맵을 생성할 수 있다. 예를 들어, 프로세싱 시스템은 학습된 임베딩 함수를 사용하여 주어진 이미지의 각 포인트(예: 픽셀)을 나타내는 엔트리, 엘리먼트 또는 기타 값을 포함하는 데이터베이스, 벡터 또는 텐서를 생성할 수 있다.At step 406, the processing system generates a first feature map from a first image and a second feature map from a second image. As described above with respect to Figure 3, the processing system may generate the feature map in any suitable manner. For example, a processing system can use the learned embedding function to create a database, vector, or tensor containing an entry, element, or other value representing each point (e.g., pixel) of a given image.
단계(408)에서, 프로세싱 시스템은 제1 피처 맵에 표현된 제1 포인트와 제2 피처 맵에 표현된 제2 포인트 사이의 제1 피처 거리를 결정하고, 상기 제1 포인트와 제2 포인트는 3D 모델에서 동일한 피처에 대응한다. 예를 들어, 제1 이미지와 제2 이미지가 인간 대상자인 경우, 제1 이미지의 제1 포인트와 제2 이미지의 제2 포인트는 모두 대상자의 오른쪽 검지 끝에 대응할 수 있다. 또한, 제1 이미지와 제2 이미지의 각 포인트를 제1 피처 맵과 제2 피처 맵에서 각각 벡터로 표현하는 경우, 제1 피처 맵의 제1 포인트를 나타내는 벡터와 제2 피처 맵의 제2 포인트를 나타내는 벡터를 사용하여 식 2와 관련하여 상기 기술된 바와 같이 제1 피처 거리가 결정될 수 있다. 유사하게, 위에서 설명된 바와 같이, 제1 피처 거리는 예를 들어, 이들 두 벡터의 직선 내적을 계산함으로써, 이들 두 벡터의 외적을 계산함으로써, 두 벡터를 더함으로써, 두 벡터를 뺌으로써와 같이 임의의 적절한 수식을 사용하여 결정될 수 있다.At step 408, the processing system determines a first feature distance between a first point represented in the first feature map and a second point represented in the second feature map, wherein the first point and the second point are Corresponds to the same feature in the model. For example, if the first image and the second image are of a human subject, the first point of the first image and the second point of the second image may both correspond to the tip of the subject's right index finger. In addition, when each point of the first image and the second image is expressed as a vector in the first feature map and the second feature map, the vector representing the first point of the first feature map and the second point of the second feature map The first feature distance may be determined as described above with respect to Equation 2 using a vector representing . Similarly, as explained above, the first feature distance can be calculated as an arbitrary distance, for example, by calculating the straight dot product of these two vectors, by calculating the cross product of these two vectors, by adding the two vectors, by subtracting the two vectors. It can be determined using an appropriate formula.
단계(410)에서, 프로세싱 시스템은 제1 피처 거리에 기초하여 손실값 세트 중 제1 손실값을 결정한다. 이 제1 손실값은 임의의 적절한 수식에 따라 결정될 수 있다. 유사하게, 이 제1 손실값은 제1 이미지와 제2 이미지의 임의 개수의 포인트 쌍에 대해 결정될 수 있다. 예를 들어, 제1 손실값은 아래 식 3에 따라 계산된 일관성 손실값(Lc)일 수 있으며, 여기서 피처 거리(d)는 제1 이미지의 모든 포인트 p(총 n개 포인트들 중)와 제2 이미지의 대응 포인트 corr(p) 사이에서 계산된다.At step 410, the processing system determines a first loss value from the set of loss values based on the first feature distance. This first loss value may be determined according to any suitable formula. Similarly, this first loss value can be determined for any number of point pairs of the first and second images. For example, the first loss value may be the consistency loss value (L c ) calculated according to Equation 3 below, where the feature distance (d) is Calculated between corresponding points corr(p) of the second image.
(3) (3)
식 3의 예에서, 제1 피처 거리와 제1 이미지 및 제2 이미지의 대응 포인트들의 쌍 사이의 모든 피처 거리들에 기초하여 손실값(Lc)가 계산된다. 그런 점에서, 각 포인트 p와 제2 이미지의 대응 포인트 corr(p) 사이의 관계는 각 포인트 p를 3D 모델의 대응 포인트와 상관시킨 다음 제2 이미지의 대응 corr(p)에 3D 모델의 해당 포인트를 상관시킴으로써 프로세싱 시스템에 의해 결정될 수 있다. 또한, 일부 양태에서, 트레이닝 데이터는 프로세싱 시스템이 이러한 결정을 내릴 필요가 없도록 모든 대응 포인트 쌍을 식별하는 목록 또는 다른 데이터 구조를 더 포함할 수 있다.In the example of equation 3, the loss value L c is calculated based on the first feature distance and all feature distances between pairs of corresponding points in the first image and the second image. In that sense, the relationship between each point p and the corresponding point corr(p) in the second image is defined by correlating each point p with the corresponding point in the 3D model, and then correlating the corresponding point p in the 3D model to the corresponding point corr(p) in the second image. It can be determined by the processing system by correlating . Additionally, in some aspects, the training data may further include a list or other data structure that identifies all pairs of corresponding points so that the processing system does not have to make this decision.
단계(412)(도 4b)에서, 프로세싱 시스템은 제1 피처 맵에 표현된 제3 포인트와 제1 피처 맵에 표현된 제4 포인트 사이의 제2 피처 거리를 결정한다. 유사하게, 단계(414)에서, 프로세싱 시스템은 제1 피처 맵에 표현된 제3 포인트와 제1 피처 맵에 표현된 제5 포인트 사이의 제3 피처 거리를 결정한다. 예를 들어, 제3 포인트는 제1 이미지에서 무작위로 선택된 임의의 기준 포인트 pr일 수 있고, 제4 및 제5 포인트는 제1 이미지에서 각각 무작위로 선택된 임의의 다른 포인트 pa 및 pb일 수 있다. 여기서도, 제1 이미지의 각 포인트가 제1 피처 맵의 벡터로서 표현되면, 제2 피처 거리와 제3 피처 거리는 앞서 식 2에서 설명한 바와 같이 결정될 수 있다. 유사하게, 위에서 설명된 바와 같이, 제2 및 제3 피처 거리는 예를 들어, 이들 각 벡터들의 쌍의 직선 내적을 계산함으로써, 각 벡터들의 쌍의 외적을 계산함으로써, 각 벡터들의 쌍을 더함으로써, 각 벡터들의 쌍을 뺌으로써와 같이 임의의 적절한 수식을 사용하여 결정될 수 있다. 일부 양태에서, 제2 및 제3 피처 거리를 계산하는데 사용된 수식은 제1 피처 거리를 계산하는데 사용된 수식과 상이할 수 있다. At step 412 (FIG. 4B), the processing system determines a second feature distance between a third point represented in the first feature map and a fourth point represented in the first feature map. Similarly, at step 414, the processing system determines a third feature distance between a third point represented in the first feature map and a fifth point represented in the first feature map. For example, the third point may be a random reference point p r randomly selected in the first image, and the fourth and fifth points may be random other points p a and p b respectively randomly selected in the first image. You can. Here too, if each point of the first image is expressed as a vector of the first feature map, the second feature distance and the third feature distance can be determined as described above in Equation 2. Similarly, as described above, the second and third feature distances are determined by, for example, calculating the straight dot product of each pair of vectors, calculating the cross product of each pair of vectors, adding each pair of vectors, It can be determined using any suitable formula, such as by subtracting each pair of vectors. In some aspects, the formula used to calculate the second and third feature distances may be different from the formula used to calculate the first feature distance.
단계(416)에서, 프로세싱 시스템은 제1 표면 맵에 표현된 대로 제3 포인트와 제4 포인트 사이의 제1 측지 거리를 결정한다. 유사하게, 단계(418)에서, 프로세싱 시스템은 제1 표면 맵에 표현된 제3 포인트와 제5 포인트 사이의 제2 측지 거리를 결정한다. 제1 표면 맵은 제1 이미지를 생성하는데 사용된 것과 동일한 포즈의 대상의 3D 모델을 사용하여 생성된 제1 이미지에 대응한다. 따라서, 단계(412 및 414)의 설명에서 언급된 예시적 포인트를 사용하면, 제1 표면 맵에 표현된 제3 포인트는 제1 이미지에 표현된 포인트 pr에 대응하고, 제1 표면 맵의 제4 및 제5 포인트는 제1 이미지에서 포인트 pB와 pB에 각각 대응할 것이다. 이러한 경우, 제1 및 제2 측지 거리 g(pr, pa) 및 g(pr, pb)는 각각 제1 표면 지도의 3D 표면을 따라 측정된 해당 포인트들 사이의 거리를 나타낸다.At step 416, the processing system determines a first geodetic distance between the third point and the fourth point as represented in the first surface map. Similarly, at step 418, the processing system determines a second geodetic distance between the third and fifth points represented in the first surface map. The first surface map corresponds to a first image generated using a 3D model of the object in the same pose as that used to generate the first image. Accordingly, using the example points mentioned in the description of steps 412 and 414, the third point represented in the first surface map corresponds to the point p r represented in the first image, and the third point represented in the first surface map corresponds to the point p r represented in the first image. Points 4 and 5 will correspond to points pB and pB respectively in the first image. In this case, the first and second geodetic distances g(p r , p a ) and g(p r , p b ) respectively represent the distances between corresponding points measured along the 3D surface of the first surface map.
단계(420)에서, 프로세싱 시스템은 제2 피처 거리, 제3 피처 거리, 제1 측지 거리 및 제2 측지 거리에 기초하여 손실값 세트 중 제2 손실값을 결정한다. 예를 들어, 제2 손실값은 기준 포인트 pr을 기준으로 두 포인트 pa와 pb의 피처 공간 내 순서가 대응 측지 거리에 의해 측정된 순서와 동일하도록 구성되어, 3D 표면에서 물리적으로 떨어져 있는 포인트 쌍은 더 먼 피처 거리를 갖게 하는 경향이 있도록 한다. 이 제2 손실값은 임의의 적절한 수식에 따라 결정될 수 있다. 예를 들어, 제2 손실값은 제2 피처 거리와 제3 피처 거리의 차이에 기초할 수 있으며, 또한 제1 측지 거리와 제2 측지 거리 차이의 부호(시그넘 또는 sgn 함수라고도 함)에 기초할 수도 있다. 여기서, 제2 손실값은 아래의 수식 4 및 5에 따라 계산된 희소 측지 손실값(Ls)일 수 있다.At step 420, the processing system determines a second loss value from the set of loss values based on the second feature distance, the third feature distance, the first geodetic distance, and the second geodetic distance. For example, the second loss is configured such that the order in the feature space of the two points p a and p b relative to the reference point p r is the same as the order measured by the corresponding geodetic distance, so that the order of the two points p a and p b is physically distant on the 3D surface. Point pairs tend to have longer feature distances. This second loss value may be determined according to any suitable formula. For example, the second loss value may be based on the difference between the second and third feature distances, and may also be based on the sign of the difference between the first and second geodetic distances (also called the signum or sgn function). It may be possible. Here, the second loss value may be a sparse geodesic loss value (L s ) calculated according to Equations 4 and 5 below.
(4) (4)
(5) (5)
유사하게, 이 제2 손실값은 제1 이미지의 하나 이상의 삼중항 {pr, pa, pb}에 기초할 수 있다. 따라서, 제2 손실값은 제2 및 제3 피처 거리와 제1 및 제2 측지 거리를 사용하여 수식 4, 5에 따라 계산된 손실값(Ls) 뿐만 아니라 무작위로 선택된 다른 삼중항에 대해 계산된 추가 손실값(Ls)에 기초하는 하는 값, 벡터, 텐서일 수 있다. 예를 들어, 제2 손실값은 무작위로 선택된 미리 결정된 수의 삼중항(예: 1, 16, 128 등)에 대해 계산된 손실값(Ls)의 평균일 수 있다. 유사하게, 무작위로 선택된 미리 결정된 수의 삼중항에 대해 계산된 손실값(Ls)을 연결하여 제2 손실값이 결정될 수 있다.Similarly, this second loss value may be based on one or more triplets {p r , p a , p b } of the first image. Therefore, the second loss value is calculated for the loss value (L s ) calculated according to Equations 4 and 5 using the second and third feature distances and the first and second geodetic distances, as well as for other randomly selected triplets. It can be a value, a vector, or a tensor based on the additional loss value (L s ). For example, the second loss value may be an average of the loss values (L s ) calculated for a predetermined number of randomly selected triplets (e.g., 1, 16, 128, etc.). Similarly, a second loss value may be determined by concatenating the loss value L s calculated for a predetermined number of randomly selected triplets.
단계(422)에서, 프로세싱 시스템은 손실값 세트에 적어도 부분적으로 기초하여 신경 네트워크의 하나 이상의 파라미터를 수정한다. 이와 관련하여, 신경 네트워크는 이미지들에 걸쳐 조밀한 대응관계를 예측하도록 트레이닝되는 임의의 모델일 수 있다(예를 들어, 예측 모델(212)). 손실값 세트는 신경 네트워크의 하나 이상의 파라미터의 수정에 영향을 미치기 위해 임의의 적절한 방식으로 사용될 수 있다. 이와 관련하여, 프로세싱 시스템은 제1 손실값과 제2 손실값을 합산하여 총 손실값에 도달하고, 해당 총 손실값에 기초하는 예측 모델의 파라미터를 수정할 수 있다. 유사하게, 제1 및 제2 손실값(또는 그 세트)은 총 손실값을 형성하기 위해 결합되기 전에 하나 이상의 미리 선택된 가중치 팩터(예를 들어, 특정 wc 및 ws 값)와 곱해질 수 있다. 일부 양태에서, 프로세싱 시스템은 각각의 트레이닝 예시 직후에 각 총 손실값을 사용하고, 모델의 파라미터를 튜닝하기 위해 역전파 동안 이를 적용한 후, 다음 트레이닝 예시 동안 새로운 총 손실값을 계산하도록 구성될 수 있다. 일부 양태에서, 프로세싱 시스템은 다수의 트레이닝 예를 일괄처리하도록 구성될 수 있다. 그러한 경우, 프로세싱 시스템은 배치의 각 트레이닝 예시 동안 계산된 손실값을 결합(예: 합산 또는 평균)하고, 배치가 끝난 후 역전파 페이즈 동안 결합된 총 손실값을 적용하여, 다음 트레이닝 예시의 배치 중에 새로운 결합된 총 손실값을 계산하도록 구성될 수 있다.At step 422, the processing system modifies one or more parameters of the neural network based at least in part on the set of loss values. In this regard, the neural network may be any model that is trained to predict dense correspondences across images (e.g., prediction model 212). The loss set may be used in any suitable way to effect modification of one or more parameters of the neural network. In this regard, the processing system may add the first loss value and the second loss value to arrive at a total loss value and modify the parameters of the prediction model based on the total loss value. Similarly, the first and second loss values (or set thereof) may be multiplied by one or more preselected weight factors (e.g., specific w c and w s values) before being combined to form the total loss value. . In some aspects, the processing system may be configured to use each total loss value immediately after each training example, apply it during backpropagation to tune the parameters of the model, and then calculate a new total loss value during the next training example. . In some aspects, the processing system can be configured to batch process multiple training examples. In such a case, the processing system combines (e.g., sums or averages) the loss values calculated during each training example in the batch, and applies the combined total loss value during the backpropagation phase after the batch ends, during the batch of the next training example. It may be configured to calculate a new combined total loss value.
도 5는 도 4의 방법(400a, 400b)의 손실값과 결합하여 추가 손실값을 결정하는 예시적 방법(500)을 도시한다. 이와 관련하여, 단계(502)에서, 프로세싱 시스템은 전술한 바와 같이 방법(400a 및 400b)의 단계(402-420)을 수행한다. FIG. 5 illustrates an example method 500 of determining additional loss values in combination with the loss values of methods 400a and 400b of FIG. 4 . In this regard, at step 502, the processing system performs steps 402-420 of methods 400a and 400b as described above.
단계(504)에서, 프로세싱 시스템은 제1 피처 맵에 표현된 제6 포인트와 제1 피처 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이의 제4 피처 거리들의 세트를 결정한다. 제6 포인트는 제1 이미지에서 무작위로 선택된 임의의 기준 포인트 pr일 수 있다. 라벨 pr은 편의상 사용되며, 이 무작위로 선택된 기준 포인트는 방법(400b)의 단계 412에서 사용된 기준 포인트와 동일해야 함을 나타내지 않는다. 여기서도, 제1 이미지의 각 포인트가 제1 피처 맵의 벡터로서 표현되면, 제4 피처 거리들의 세트는 앞서 식 2에서 설명한 바와 같이 결정될 수 있다. 이와 관련하여, 제4 피처 거리들의 세트는 포인트(pr 및 pt)를 표현하는 벡터를 사용하여 계산된, 제1 이미지의 모든 포인트 pt에 대한 별도의 피처 거리 d(pr, pt)를 포함할 것이다. 유사하게, 위에서 설명된 바와 같이, 제4 피처 거리들의 세트는 예를 들어, 이들 각 벡터들의 쌍의 직선 내적을 계산함으로써, 각 벡터들의 쌍의 외적을 계산함으로써, 각 벡터들의 쌍을 더함으로써, 각 벡터들의 쌍을 뺌으로써와 같이 임의의 적절한 수식을 사용하여 결정될 수 있다. 일부 양태에서, 제4 피처 거리들의 세트를 계산하는데 사용된 수식은 제1, 제2 및/또는 제3 피처 거리를 계산하는데 사용된 수식과 상이할 수 있다.At step 504, the processing system determines a fourth set of feature distances between the sixth point represented in the first feature map and all other points of the first image represented in the first feature map. The sixth point may be an arbitrary reference point p r randomly selected from the first image. The label p r is used for convenience and does not indicate that this randomly selected reference point must be the same as the reference point used in step 412 of method 400b. Here too, if each point of the first image is represented as a vector of the first feature map, the set of fourth feature distances can be determined as previously described in Equation 2. In this regard, the fourth set of feature distances is a separate feature distance d(p r , p t ) for every point p t in the first image, calculated using vectors representing the points (p r and p t ). ) will include. Similarly, as described above, the fourth set of feature distances can be generated by, for example, calculating the straight dot product of each pair of vectors, calculating the cross product of each pair of vectors, adding each pair of vectors, for example, It can be determined using any suitable formula, such as by subtracting each pair of vectors. In some aspects, the formula used to calculate the fourth set of feature distances may be different from the formula used to calculate the first, second and/or third feature distances.
단계(506)에서, 프로세싱 시스템은 제1 표면 맵에 표현된 제6 포인트와 제1 표면 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이의 제3 측지 거리들의 세트를 결정한다. 여기서도, 제1 표면 맵에 표현된 제6 포인트는 제1 이미지의 포인트 pr에 대응할 것이다. 따라서, 제3 측지 거리들의 세트는 제1 표면 맵의 3D 표면을 따른 제1 이미지의 포인트 pr과 pt에 대응하는 포인트들 사이의 거리를 표현하는 제1 이미지의 모든 포인트 pt에 대해 별개의 측지 거리 g(pr, pt)를 포함할 것이다.At step 506, the processing system determines a third set of geodetic distances between the sixth point represented in the first surface map and all other points of the first image represented in the first surface map. Here too, the sixth point represented in the first surface map will correspond to the point p r in the first image. Therefore, a third set of geodetic distances is distinct for every point p t in the first image, representing the distance between the points corresponding to points p r and p t in the first image along the 3D surface of the first surface map. It will include the geodetic distance g(p r , p t ) of .
단계(508)에서, 프로세싱 시스템은 손실 값들의 세트 중 제3 손실 값을 결정하며, 상기 제3 손실 값은 상기 제4 피처 거리들의 세트 및 상기 제3 측지 거리들의 세트에 기초한다. 예를 들어, 제3 손실값은 해당 픽셀 사이의 측지 거리에 따라, 일치하지 않는 포인트들 사이의 피처 거리들을 밀어내는 역할을 할 수 있다. 여기서도, 이 제3 손실값은 임의의 적절한 수식에 따라 결정될 수 있다. 예를 들어, 제3 손실값은 제3 측지 거리들의 세트의 각 측지 거리와 제4 피처 거리들의 세트의 각 대응 피처 거리 사이의 차이에 기초할 수 있다. 기술의 일부 양태에서, 제3 측지 거리들의 세트 및 제4 피처 거리들의 세트는 선택된 기준 포인트(또는 픽셀)과 제1 이미지의 다른 모든 포인트(또는 픽셀) 사이에서 측정될 수 있다. 이와 관련하여, 제3 손실값은 제1 이미지의 선택된 포인트 pr 및 (총 n개 포인트 중) 모든 포인트 pt에 대해 아래 수식 6에 따라 계산된 조밀한 측지 손실값 Ld일 수 있다.At step 508, the processing system determines a third loss value of the set of loss values, the third loss value being based on the fourth set of feature distances and the third set of geodetic distances. For example, the third loss value may serve to push out feature distances between non-matching points, depending on the geodesic distance between corresponding pixels. Here too, this third loss value may be determined according to any suitable formula. For example, the third loss value may be based on the difference between each geodetic distance in the third set of geodetic distances and each corresponding feature distance in the fourth set of feature distances. In some aspects of the technology, the third set of geodetic distances and the fourth set of feature distances may be measured between the selected reference point (or pixel) and all other points (or pixels) in the first image. In this regard, the third loss value may be a dense geodetic loss value L d calculated according to Equation 6 below for the selected point p r and all points p t (out of the total n points) of the first image.
(6) (6)
유사하게, 이 제3 손실값은 제1 이미지의 하나 이상의 선택된 포인트 pr에 기초할 수 있다. 따라서, 제3 손실값은 제4 피처 거리들의 세트 및 제3 측지 거리들의 세트를 사용하여 수식 6에 따라 계산된 손실값(Ld) 뿐만 아니라 무작위로 선택된 다른 포인트들 pr에 대해 계산된 추가 손실값(Ld)에 기초하는 하는 값, 벡터, 텐서일 수 있다. 예를 들어, 제3 손실값은 무작위로 선택된 미리 결정된 수의 포인트 pr(예: 1, 5, 16, 128 등)에 대해 계산된 손실값(Ld)의 평균일 수 있다. 유사하게, 무작위로 선택된 미리 결정된 수의 포인트 pr에 대해 계산된 손실값(Ld)을 연결하여 제3 손실값이 결정될 수 있다.Similarly, this third loss value may be based on one or more selected points p r of the first image. Therefore, the third loss value is the loss value L d calculated according to Equation 6 using the fourth set of feature distances and the third set of geodetic distances, as well as the additional loss calculated for other randomly selected points p r It may be a value, vector, or tensor based on the loss value (L d ). For example, the third loss value may be an average of the loss values (L d ) calculated for a predetermined number of randomly selected points p r (e.g., 1, 5, 16, 128, etc.). Similarly, a third loss value may be determined by concatenating the loss value L d calculated for a predetermined number of randomly selected points p r .
단계(510)에서, 프로세싱 시스템은 전술한 바와 같이 결과적인 손실값 세트를 사용하여 방법(400b)의 단계(422)를 수행한다. 이 경우, 손실값 세트는 단계(410 및 420)에서 결정된 제1 손실값과 제2 손실값뿐만 아니라, 단계(508)에서 결정된 제3 손실값도 포함한다. 여기서도, 손실값 세트는 신경 네트워크의 하나 이상의 파라미터의 수정에 영향을 미치기 위해 임의의 적절한 방식으로 사용될 수 있다. 이와 관련하여, 프로세싱 시스템은 제1, 제2 및 제3 손실값을 합산하여 총 손실값에 도달하고, 해당 총 손실값에 기초하는 예측 모델의 파라미터를 수정할 수 있다. 유사하게, 제1, 제2 및 제3 손실값은 총 손실값을 형성하기 위해 결합되기 전에 하나 이상의 미리 선택된 가중치 팩터(예를 들어, 특정 wc, ws 및 wd 값)와 곱해질 수 있다.At step 510, the processing system performs step 422 of method 400b using the resulting set of loss values as described above. In this case, the set of loss values includes the first and second loss values determined in steps 410 and 420, as well as the third loss value determined in step 508. Here again, the set of loss values may be used in any suitable way to effect modification of one or more parameters of the neural network. In this regard, the processing system may add the first, second, and third loss values to arrive at a total loss value and modify parameters of the prediction model based on the total loss value. Similarly, the first, second and third loss values may be multiplied by one or more preselected weight factors (e.g., specific w c , w s and w d values) before being combined to form the total loss value. there is.
도 6는 도 4의 방법(400a, 400b)의 손실값과 결합하여 추가 손실값을 결정하는 예시적 방법(600)을 도시한다. 이와 관련하여, 단계(602)에서, 프로세싱 시스템은 전술한 바와 같이 방법(400a 및 400b)의 단계(402-420)를 수행한다.FIG. 6 illustrates an example method 600 for determining additional loss values in combination with the loss values from methods 400a and 400b of FIG. 4 . In this regard, at step 602, the processing system performs steps 402-420 of methods 400a and 400b as described above.
단계(604)에서, 프로세싱 시스템은 제1 피처 맵에 표현된 제6 포인트와 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제4 피처 거리들의 세트를 결정한다. 방법(600)과 관련하여 설명된 "제6 포인트" 및 "제4 피처 거리들의 세트"는 도 5의 방법(500)의 컨텍스트와 관련하여 설명된 것과 동일하지 않다. 방법(600)에서, 제6 포인트는 제1 이미지에서 무작위로 선택된 임의의 기준 포인트 pr일 수 있다. 일부 양태에서, 프로세싱 시스템은 제1 이미지에서는 보이지만 제2 이미지에서는 보이지 않는 기준 포인트 pr을 선택하도록 구성될 수 있다. 예를 들어, 제1 이미지와 제2 이미지가 걷는 사람 대상자인 경우, 기준 포인트 pr은 제1 이미지에서는 보이지만 제2 이미지에서는 대상자의 오른쪽 다리에 가려져 제2 이미지에서는 보이지 않는 대상자의 왼쪽 다리 상의 포인트에 대응할 수 있다. 제2 이미지에서 보이지 않는 기준 포인트 pr을 선택하도록 프로세싱 시스템을 구성하면, 신경 네트워크는 제2 이미지의 포인트들이 어떻게 제1 이미지에서만 보이는 포인트에 대응할 수 있는지 학습할 수 있다(예: 대상자의 양 이미지들에서 자세를 취하는 방식, 카메라 포지션, 조명 등에서의 차이로 인해). 여기서도, 라벨 pr은 편의상 사용되며, 이 선택된 기준 포인트는 방법(400b)의 단계 412에서 사용된 기준 포인트와 동일해야 함을 나타내지 않는다. At step 604, the processing system determines a fourth set of feature distances between the sixth point represented in the first feature map and all points of the second image represented in the second feature map. The “sixth point” and “fourth set of feature distances” described in relation to method 600 are not the same as those described in relation to the context of method 500 of FIG. 5 . In method 600, the sixth point may be any reference point p r randomly selected in the first image. In some aspects, the processing system can be configured to select a reference point p r that is visible in the first image but not in the second image. For example, if the first and second images are of a walking human subject, the reference point p r is a point on the subject's left leg that is visible in the first image but is not visible in the second image because it is obscured by the subject's right leg in the second image. can respond. By configuring the processing system to select a reference point p r that is not visible in the second image, the neural network can learn how points in the second image can correspond to points that are only visible in the first image (e.g., both images of the subject due to differences in posing in the field, camera position, lighting, etc.) Here again, the label p r is used for convenience and does not indicate that this selected reference point must be the same as the reference point used in step 412 of method 400b.
제1 이미지의 각 포인트를 제1 피처 맵에서 벡터로 표현하고, 제2 이미지의 각 포인트를 제2 피처 맵에서 벡터로 표현하면, 제4 피처 거리들의 세트는 수식 2를 참조하여 전술한 바와 같이 결정될 수 있다. 이와 관련하여, 제4 피처 거리들의 세트는 포인트(pr 및 pt)를 표현하는 벡터를 사용하여 계산된, 제2 이미지의 모든 포인트 pt에 대한 별도의 피처 거리 d(pr, pt)를 포함할 것이다. 유사하게, 위에서 설명된 바와 같이, 제4 피처 거리들의 세트는 예를 들어, 이들 각 벡터들의 쌍의 직선 내적을 계산함으로써, 각 벡터들의 쌍의 외적을 계산함으로써, 각 벡터들의 쌍을 더함으로써, 각 벡터들의 쌍을 뺌으로써와 같이 임의의 적절한 수식을 사용하여 결정될 수 있다. 일부 양태에서, 제4 피처 거리들의 세트를 계산하는데 사용된 수식은 제1, 제2 및/또는 제3 피처 거리를 계산하는데 사용된 수식과 상이할 수 있다.If each point in the first image is expressed as a vector in the first feature map, and each point in the second image is expressed as a vector in the second feature map, the fourth set of feature distances is as described above with reference to Equation 2. can be decided. In this regard, the fourth set of feature distances is a separate feature distance d(p r , p t ) for every point p t in the second image, calculated using vectors representing the points (p r and p t ). ) will include. Similarly, as described above, the fourth set of feature distances can be generated by, for example, calculating the straight dot product of each pair of vectors, calculating the cross product of each pair of vectors, adding each pair of vectors, for example, It can be determined using any suitable formula, such as by subtracting each pair of vectors. In some aspects, the formula used to calculate the fourth set of feature distances may be different from the formula used to calculate the first, second and/or third feature distances.
단계(606)에서, 프로세싱 시스템은 제2 표면 맵에 표현된 제1 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제3 측지 거리들의 세트를 결정한다. 제2 표면 맵은 제2 이미지를 생성하는데 사용된 것과 동일한 포즈의 대상의 3D 모델을 사용하여 생성된 제2 이미지에 대응한다. 또한, 제2 표면 맵의 제1 포인트와 제1 피처 맵의 제6 포인트는 상기 대상의 3D 모델에서 동일한 피처에 대응한다. 예를 들어, 위에서 언급한 바와 같이, 제2 표면 맵의 제1 포인트는 제1 이미지에서 볼 수 있으므로 제1 피처 맵에서는 표현되지만 제2 이미지에서는 볼 수 없으므로 제2 피처 맵에서는 표현되지 않는 대상자의 왼쪽 다리에 있는 포인트에 대응할 수 있다. 따라서, 단계(604)의 설명에서 언급된 예시적 포인트를 사용하여, 제2 표면 맵에 표현된 제1 포인트는 제1 이미지에 표현된 포인트 pr에 대응할 것이다. 그러한 경우, 제3 측지 거리들의 세트는 제2 표면 지도의 3D 표면을 따른 제1 이미지의 포인트 pr(여기서는 corr(pr)이라고 함)에 대응하는 포인트와 제2 이미지의 포인트 pt에 대응하는 포인트 사이의 거리를 표현하는 제2 이미지의 모든 포인트 pt에 대해 별도의 측지 거리 g(corr(pr), pt)를 포함할 것이다. At step 606, the processing system determines a third set of geodetic distances between a first point represented in the second surface map and all points in the second image represented in the second surface map. The second surface map corresponds to the second image generated using a 3D model of the object in the same pose as that used to generate the second image. Additionally, the first point of the second surface map and the sixth point of the first feature map correspond to the same feature in the 3D model of the object. For example, as mentioned above, the first point in the second surface map is visible in the first image and therefore represented in the first feature map, but is not visible in the second image and therefore not represented in the second feature map. You can respond to the point on your left leg. Accordingly, using the example points mentioned in the description of step 604, the first point represented in the second surface map will correspond to the point p r represented in the first image. In that case, the third set of geodetic distances corresponds to the point p t in the second image and the point corresponding to the point p r in the first image (here called corr(p r )) along the 3D surface of the second surface map. It will contain a separate geodetic distance g(corr(p r ), p t ) for every point p t in the second image, which represents the distance between the points.
단계(608)에서, 프로세싱 시스템은 손실 값들의 세트 중 제3 손실 값을 결정하며, 상기 제3 손실 값은 상기 제4 피처 거리들의 세트 및 상기 제3 측지 거리들의 세트에 기초한다. 여기서도, 방법(600)과 관련하여 설명된 "제3 손실값"은 도 5의 방법(500)의 맥락에서 설명된 것과 동일하지 않다. 이와 관련하여, 방법(600)의 제3 손실값은 또한 해당 픽셀 사이의 측지 거리에 따라 일치하지 않는 포인트 사이의 피처 거리를 밀어내는 역할을 할 수 있으며, 제3 측지 거리들 세트의 각 측지 거리와 제4 피처 거리들의 세트의 각 대응 피처 거리 사이의 차이에 기초할 수 있다. 그러나 방법(600)의 "제3 손실값"은 단계(604 및 606)과 관련하여 위에서 설명한 바와 같이 "제3 측지 거리"와 "제4 피처 거리"의 서로 다른 세트에 기초하기 때문에 도 5의 방법(500)에 설명된 "제3 손실값"과는 다르다. 여기서도, 방법(600)의 제3 손실값은 임의의 적절한 수식에 따라 결정될 수 있다. 예를 들면, 제3 손실값은 제1 이미지의 선택된 포인트 pr 및 제2 이미지의 (총 n개 포인트 중) 모든 포인트 pt에 대해 아래 수식 7에 따라 계산된 교차-뷰 측지 손실값 Lcd일 수 있다.At step 608, the processing system determines a third loss value of the set of loss values, the third loss value being based on the fourth set of feature distances and the third set of geodetic distances. Here again, the “third loss value” described in connection with method 600 is not the same as that described in the context of method 500 in FIG. 5 . In this regard, the third loss value of method 600 may also serve to push feature distances between non-matching points according to the geodetic distances between corresponding pixels, each geodetic distance in the set of third geodetic distances. and each corresponding feature distance of the set of fourth feature distances. However, since the “third loss” of method 600 is based on different sets of “third geodetic distances” and “fourth feature distances” as described above with respect to steps 604 and 606 of FIG. This is different from the “third loss value” described in method 500. Here again, the third loss value of method 600 may be determined according to any suitable equation. For example, the third loss value is the cross-view geodetic loss value L cd calculated according to Equation 7 below for the selected point p r of the first image and all points p t (out of the total n points) of the second image. It can be.
(7) (7)
유사하게, 이 제3 손실값은 제1 이미지의 하나 이상의 선택된 포인트 pr에 기초할 수 있다. 따라서, 제3 손실값은 제4 피처 거리들의 세트 및 제3 측지 거리들의 세트를 사용하여 수식 7에 따라 계산된 손실값(Lcd) 뿐만 아니라 무작위로 선택된 다른 포인트들 pr에 대해 계산된 추가 손실값(Lcd)에 기초하는 하는 값, 벡터, 텐서일 수 있다. 예를 들어, 제3 손실값은 선택된 미리 결정된 수의 포인트 pr(예: 1, 5, 16, 128 등)에 대해 계산된 손실값(Lcd)의 평균일 수 있다. 유사하게, 선택된 미리 결정된 수의 포인트 pr에 대해 계산된 손실값(Lcd)을 연결하여 제3 손실값이 결정될 수 있다.Similarly, this third loss value may be based on one or more selected points p r of the first image. Therefore, the third loss value is the loss value (L cd ) calculated according to Equation 7 using the fourth set of feature distances and the third set of geodetic distances, as well as the additional loss calculated for other randomly selected points p r It may be a value, vector, or tensor based on the loss value (L cd ). For example, the third loss value may be an average of the loss values (L cd ) calculated for a selected predetermined number of points p r (e.g., 1, 5, 16, 128, etc.). Similarly, a third loss value may be determined by concatenating the calculated loss value (Lcd) for a selected predetermined number of points p r .
단계(610)에서, 프로세싱 시스템은 전술한 바와 같이 결과적인 손실값 세트를 사용하여 방법(400b)의 단계(422)를 수행한다. 이 경우, 손실값 세트는 도 4의 단계(410 및 420)에서 결정된 제1 손실값과 제2 손실값뿐만 아니라, 단계(608)에서 결정된 제3 손실값도 포함한다. 여기서도, 손실값 세트는 신경 네트워크의 하나 이상의 파라미터의 수정에 영향을 미치기 위해 임의의 적절한 방식으로 사용될 수 있다. 이와 관련하여, 프로세싱 시스템은 제1, 제2 및 제3 손실값을 합산하여 총 손실값에 도달하고, 해당 총 손실값에 기초하는 예측 모델의 파라미터를 수정할 수 있다. 마찬가지로, 제1 손실값, 제2 손실값, 제3 손실값은 미리 선택된 하나 이상의 가중치 팩터(예를 들어, 특정 wc, ws, wcd 값)를 곱한 후 결합하여 총 손실값을 형성할 수 있다.At step 610, the processing system performs step 422 of method 400b using the resulting set of loss values as described above. In this case, the set of loss values includes the first and second loss values determined in steps 410 and 420 of FIG. 4, as well as the third loss value determined in step 608. Here again, the set of loss values may be used in any suitable way to effect modification of one or more parameters of the neural network. In this regard, the processing system may add the first, second, and third loss values to arrive at a total loss value and modify parameters of the prediction model based on the total loss value. Likewise, the first loss value, the second loss value, and the third loss value may be multiplied by one or more pre-selected weight factors (eg, specific wc, ws, and wcd values) and then combined to form a total loss value.
도 7는 도 4 및 5의 방법(400a, 400b 및 500)의 손실값과 결합하여 추가 손실값을 결정하는 예시적 방법(700)을 도시한다. 이와 관련하여, 방법(700)은 도 6의 방법(600)에서 결정된 손실값과 동일한 "제4 손실값"을 결과로 하지만, 이는 방법(400a, 400b 및 500)에서 계산된 손실값과 결합된다. 따라서, 단계(702)에서, 프로세싱 시스템은 전술한 바와 같이 방법(400a 및 400b)의 단계(402-420) 및 방법(500)의 단계(504-508)를 수행한다.FIG. 7 illustrates an example method 700 for determining additional loss values in combination with the loss values of methods 400a, 400b, and 500 of FIGS. 4 and 5. In this regard, method 700 results in a “fourth loss value” that is the same as the loss value determined in method 600 of FIG. 6, but is combined with the loss values calculated in methods 400a, 400b, and 500. . Accordingly, at step 702, the processing system performs steps 402-420 of methods 400a and 400b and steps 504-508 of method 500 as described above.
단계(704)에서, 프로세싱 시스템은 제1 피처 맵에 표현된 제7 포인트와 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제5 피처 거리들의 세트를 결정한다. 여기서도, 제7 포인트는 제1 이미지에서 무작위로 선택된 임의의 기준 포인트 pr일 수 있다. 일부 양태에서, 프로세싱 시스템은 제1 이미지에서는 보이지만 제2 이미지에서는 보이지 않는 기준 포인트 pr을 선택하도록 구성될 수 있다. 예를 들어, 제1 이미지와 제2 이미지가 걷는 사람 대상자인 경우, 기준 포인트 pr은 제1 이미지에서는 보이지만 제2 이미지에서는 대상자의 오른쪽 다리에 가려져 제2 이미지에서는 보이지 않는 대상자의 왼쪽 다리 상의 포인트에 대응할 수 있다. 다시, 제2 이미지에서 보이지 않는 기준 포인트 pr을 선택하도록 프로세싱 시스템을 구성하면, 신경 네트워크는 제2 이미지의 포인트들이 어떻게 제1 이미지에서만 보이는 포인트에 대응할 수 있는지 학습할 수 있다(예: 대상자의 양 이미지들에서 자세를 취하는 방식, 카메라 포지션, 조명 등에서의 차이로 인해). 여기서도, 라벨 pr은 편의상 사용되며, 이 선택된 기준 포인트는 방법(400b)의 단계(412) 또는 방법(500)의 단계(504)에서 사용된 기준 포인트와 동일해야 함을 나타내지 않는다. At step 704, the processing system determines a fifth set of feature distances between the seventh point represented in the first feature map and all points of the second image represented in the second feature map. Here too, the seventh point may be an arbitrary reference point p r randomly selected from the first image. In some aspects, the processing system can be configured to select a reference point p r that is visible in the first image but not in the second image. For example, if the first and second images are of a walking human subject, the reference point p r is a point on the subject's left leg that is visible in the first image but is not visible in the second image because it is obscured by the subject's right leg in the second image. can respond. Again, by configuring the processing system to select a reference point p r that is not visible in the second image, the neural network can learn how points in the second image can correspond to points that are only visible in the first image (e.g., the subject's Due to differences in posing, camera position, lighting, etc. in both images). Here again, the label p r is used for convenience and does not indicate that this selected reference point must be the same as the reference point used in step 412 of method 400b or step 504 of method 500.
제1 이미지의 각 포인트를 제1 피처 맵에서 벡터로 표현하고, 제2 이미지의 각 포인트를 제2 피처 맵에서 벡터로 표현하면, 제5 피처 거리들의 세트는 수식 2를 참조하여 전술한 바와 같이 결정될 수 있다. 이와 관련하여, 제5 피처 거리들의 세트는 포인트(pr 및 pt)를 표현하는 벡터를 사용하여 계산된, 제2 이미지의 모든 포인트 pt에 대한 별도의 피처 거리 d(pr, pt)를 포함할 것이다. 유사하게, 위에서 설명된 바와 같이, 제5 피처 거리들의 세트는 예를 들어, 이들 각 벡터들의 쌍의 직선 내적을 계산함으로써, 각 벡터들의 쌍의 외적을 계산함으로써, 각 벡터들의 쌍을 더함으로써, 각 벡터들의 쌍을 뺌으로써와 같이 임의의 적절한 수식을 사용하여 결정될 수 있다. 일부 양태에서, 제5 피처 거리들의 세트를 계산하는데 사용된 수식은 제1, 제2, 제3 및/또는 제4 피처 거리를 계산하는데 사용된 수식과 상이할 수 있다.If each point in the first image is expressed as a vector in the first feature map, and each point in the second image is expressed as a vector in the second feature map, the fifth set of feature distances is as described above with reference to Equation 2. can be decided. In this regard, the fifth set of feature distances is a separate feature distance d(p r , p t ) for every point p t in the second image, calculated using vectors representing the points (p r and p t ). ) will include. Similarly, as described above, the fifth set of feature distances can be generated by, for example, calculating the straight dot product of each pair of vectors, calculating the cross product of each pair of vectors, adding each pair of vectors, for example, It can be determined using any suitable formula, such as by subtracting each pair of vectors. In some aspects, the formula used to calculate the fifth set of feature distances may be different from the formula used to calculate the first, second, third and/or fourth feature distances.
단계(706)에서, 프로세싱 시스템은 제2 표면 맵에 표현된 제1 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제4 측지 거리들의 세트를 결정한다. 여기서도, 제2 표면 맵은 제2 이미지를 생성하는데 사용된 것과 동일한 포즈의 대상의 3D 모델을 사용하여 생성된 제2 이미지에 대응한다. 또한, 제2 표면 맵의 제1 포인트와 제1 피처 맵의 제7 포인트는 상기 대상의 3D 모델에서 동일한 피처에 대응한다. 예를 들어, 위에서 언급한 바와 같이, 제2 표면 맵의 제1 포인트는 제1 이미지에서 볼 수 있으므로 제1 피처 맵에서는 표현되지만 제2 이미지에서는 볼 수 없으므로 제2 피처 맵에서는 표현되지 않는 대상자의 왼쪽 다리에 있는 포인트에 대응할 수 있다. 따라서, 단계(704)의 설명에서 언급된 예시적 포인트를 사용하여, 제2 표면 맵에 표현된 제1 포인트는 제1 이미지에 표현된 포인트 pr에 대응할 것이다. 그러한 경우, 제4 측지 거리들의 세트는 제2 표면 지도의 3D 표면을 따른 제1 이미지의 포인트 pr(여기서는 corr(pr)이라고 함)에 대응하는 포인트와 제2 이미지의 포인트 pt에 대응하는 포인트 사이의 거리를 표현하는 제2 이미지의 모든 포인트 pt에 대해 별도의 측지 거리 g(corr(pr), pt)를 포함할 것이다.At step 706, the processing system determines a fourth set of geodetic distances between a first point represented in the second surface map and all points in the second image represented in the second surface map. Here again, the second surface map corresponds to a second image generated using a 3D model of the object in the same pose as that used to generate the second image. Additionally, the first point of the second surface map and the seventh point of the first feature map correspond to the same feature in the 3D model of the object. For example, as mentioned above, the first point in the second surface map is visible in the first image and therefore represented in the first feature map, but is not visible in the second image and therefore not represented in the second feature map. You can respond to the point on your left leg. Accordingly, using the example points mentioned in the description of step 704, the first point represented in the second surface map will correspond to the point p r represented in the first image. In that case, the fourth set of geodetic distances corresponds to the point p t in the second image and the point corresponding to the point p r in the first image (here called corr(p r )) along the 3D surface of the second surface map. It will contain a separate geodetic distance g(corr(p r ), p t ) for every point p t in the second image, which represents the distance between the points.
단계(708)에서, 프로세싱 시스템은 손실값들의 세트 중 제4 손실값을 결정하며, 상기 제4 손실값은 제5 피처 거리들의 세트 및 제4 측지 거리들의 세트에 기초한다. 방법(600)의 제4 손실값은 임의의 적절한 수식에 따라 결정될 수 있다. 예를 들면, 제4 손실값은 제1 이미지의 선택된 포인트 pr 및 제2 이미지의 (총 n개 포인트 중) 모든 포인트 pt에 대해 위 수식 7에 따라 계산된 교차-뷰 측지 손실값 Lcd일 수 있다.At step 708, the processing system determines a fourth loss value of the set of loss values, the fourth loss value being based on the fifth set of feature distances and the fourth set of geodetic distances. The fourth loss value of method 600 may be determined according to any suitable equation. For example, the fourth loss value is the cross-view geodetic loss value Lcd calculated according to Equation 7 above for the selected point p r of the first image and all points p t (out of total n points) of the second image. You can.
유사하게, 이 제4 손실값은 제1 이미지의 하나 이상의 선택된 포인트 pr에 기초할 수 있다. 따라서, 제4 손실값은 제5 피처 거리들의 세트 및 제4 측지 거리들의 세트를 사용하여 수식 7에 따라 계산된 손실값(Lcd) 뿐만 아니라 무작위로 선택된 다른 포인트들 pr에 대해 계산된 추가 손실값(Lcd)에 기초하는 하는 값, 벡터, 텐서일 수 있다. 예를 들어, 제4 손실값은 선택된 미리 결정된 수의 포인트 pr(예: 1, 5, 16, 128 등)에 대해 계산된 손실값(Ld)의 평균일 수 있다. 유사하게, 선택된 미리 결정된 수의 포인트 pr에 대해 계산된 손실값(Lcd)을 연결하여 제4 손실값이 결정될 수 있다.Similarly, this fourth loss value may be based on one or more selected points p r of the first image. Therefore, the fourth loss value is the loss value (L cd ) calculated according to Equation 7 using the fifth set of feature distances and the fourth set of geodetic distances, as well as the additional calculated for other randomly selected points p r It may be a value, vector, or tensor based on the loss value (L cd ). For example, the fourth loss value may be an average of the loss values (L d ) calculated for a selected predetermined number of points p r (e.g., 1, 5, 16, 128, etc.). Similarly, a fourth loss value may be determined by concatenating the loss value L cd calculated for a selected predetermined number of points p r .
단계(710)에서, 프로세싱 시스템은 전술한 바와 같이 결과적인 손실값 세트를 사용하여 방법(400b)의 단계(422)를 수행한다. 이 경우, 손실값 세트는 도 4의 단계(410 및 420)에서 결정된 제1 손실값과 제2 손실값, 도 5의 단계(508)에서 결정된 제3 손실값 및 단계(708)에서 결정된 제4 손실값을 포함한다. 여기서도, 손실값 세트는 신경 네트워크의 하나 이상의 파라미터의 수정에 영향을 미치기 위해 임의의 적절한 방식으로 사용될 수 있다. 이와 관련하여, 프로세싱 시스템은 제1, 제2, 제3 및 제4 손실값을 합산하여 총 손실값에 도달하고, 해당 총 손실값에 기초하는 예측 모델의 파라미터를 수정할 수 있다. 유사하게, 제1, 제2, 제3, 제4 손실값은 총 손실값을 형성하기 위해 결합되기 전에 하나 이상의 미리 선택된 가중치 팩터(예를 들어, 특정 wc, ws, wd 및 wcd 값)와 곱해질 수 있다.At step 710, the processing system performs step 422 of method 400b using the resulting set of loss values as described above. In this case, the set of loss values includes the first and second loss values determined in steps 410 and 420 of FIG. 4, the third loss value determined in step 508 of FIG. 5, and the fourth loss value determined in step 708. Includes loss value. Here again, the set of loss values may be used in any suitable way to effect modification of one or more parameters of the neural network. In this regard, the processing system may add the first, second, third and fourth loss values to arrive at a total loss value and modify parameters of the prediction model based on the total loss value. Similarly, the first, second, third and fourth loss values are weighted by one or more preselected weight factors (e.g., specific w c , w s , w d and w cd ) before being combined to form the total loss value. value) can be multiplied.
달리 언급되지 않는 한, 전술한 대안적 예는 상호 배타적이지 않으며, 고유한 이점을 달성하기 위해 다양한 조합으로 구현될 수 있다. 상기 논의된 구성의 이러한 및 다른 변형 및 조합은 청구 범위에 의해 정의된 발명을 벗어나지 않고 활용될 수 있으므로, 예시적 시스템 및 방법에 대한 전술한 설명은 청구 범위에 의해 정의된 주제의 제한이 아닌 예시의 방식으로 받아들여야 한다. 또한, "~와 같은(such as)", "포함하는(including)", “포함하는(comprising)” 등과 같이 표현된 문구 뿐만 아니라 본 명세서에 설명된 예시의 제공은 특정 예에 대한 청구 범위의 발명을 제한하는 것이 아니라 오히려 예는 많은 가능한 실시예 중 일부만을 예시하기 위한 것이라고 해석되어야 한다. 또한, 상이한 도면에서 동일한 참조 번호는 동일하거나 유사한 요소를 식별할 수 있다.Unless otherwise noted, the foregoing alternative examples are not mutually exclusive and may be implemented in various combinations to achieve unique advantages. Since these and other variations and combinations of the configurations discussed above may be utilized without departing from the invention as defined by the claims, the foregoing descriptions of exemplary systems and methods are illustrative and not limiting of the subject matter defined by the claims. It must be accepted in this way. Additionally, phrases such as “such as,” “including,” “comprising,” and the like, as well as the provision of examples described herein, mean that the scope of the claims for specific examples is limited. Rather than limiting the invention, the examples should be construed as illustrative of only some of the many possible embodiments. Additionally, the same reference numbers in different drawings may identify the same or similar elements.
Claims (20)
프로세싱 시스템의 하나 이상의 프로세서에 의해, 신경 네트워크를 사용하여, 대상의 제1 이미지에 기초하여 제1 피처 맵 및 대상의 제2 이미지에 기초하여 제2 피처 맵을 생성하는 단계, 상기 제1 이미지 및 제2 이미지는 상이하며, 상기 대상의 3차원 모델을 사용하여 생성되었으며;
하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제1 포인트와 상기 제2 피처 맵에 표현된 제2 포인트 사이의 제1 피처 거리를 결정하는 단계, 상기 제1 포인트와 제2 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며;
하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제3 포인트와 상기 제1 피처 맵에 표현된 제4 포인트 사이의 제2 피처 거리를 결정하는 단계;
하나 이상의 프로세서에 의해, 제1 표면 맵에 표현된 제3 포인트와 제4 포인트 사이의 제1 측지 거리를 결정하는 단계, 상기 제1 표면 맵은 상기 제1 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며;
하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제3 포인트와 상기 제1 피처 맵에 표현된 제5 포인트 사이의 제3 피처 거리를 결정하는 단계;
하나 이상의 프로세서에 의해, 상기 제1 표면 맵에 표현된 제3 포인트와 제5 포인트 사이의 제2 측지 거리를 결정하는 단계;
하나 이상의 프로세서에 의해, 손실값들의 세트 중 제1 손실값을 결정하는 단계, 상기 제1 손실값은 상기 제1 피처 거리에 기초하며;
하나 이상의 프로세서에 의해, 상기 손실값들의 세트 중 제2 손실값을 결정하는 단계, 상기 제2 손실값은 상기 제2 피처 거리, 상기 제3 피처 거리, 상기 제1 측지 거리 및 상기 제2 측지 거리에 기초하며; 및
하나 이상의 프로세서에 의해, 상기 손실값들의 세트에 적어도 부분적으로 기초하여 신경 네트워크의 하나 이상의 파라미터를 수정하는 단계를 포함하는, 방법.A method of training a neural network to predict correspondences in images, comprising:
generating, by one or more processors of a processing system, using a neural network, a first feature map based on a first image of the object and a second feature map based on a second image of the object, the first image and The second image is different and was created using a three-dimensional model of the object;
determining, by one or more processors, a first feature distance between a first point represented in the first feature map and a second point represented in the second feature map, wherein the first point and the second point are corresponds to the same feature in a three-dimensional model of the object;
determining, by one or more processors, a second feature distance between a third point represented in the first feature map and a fourth point represented in the first feature map;
determining, by one or more processors, a first geodetic distance between a third point and a fourth point represented in a first surface map, the first surface map corresponding to the first image and a three-dimensional model of the object; Created using;
determining, by one or more processors, a third feature distance between a third point represented in the first feature map and a fifth point represented in the first feature map;
determining, by one or more processors, a second geodetic distance between a third point and a fifth point represented in the first surface map;
determining, by one or more processors, a first loss value of a set of loss values, the first loss value being based on the first feature distance;
determining, by one or more processors, a second loss value of the set of loss values, wherein the second loss value is the second feature distance, the third feature distance, the first geodetic distance, and the second geodetic distance. Based on; and
Modifying, by one or more processors, one or more parameters of a neural network based at least in part on the set of loss values.
상기 적어도 하나의 추가적인 피처 거리들의 쌍 중 각각의 주어진 추가적인 피처 거리들의 쌍은 상기 제1 피처 맵에 표현된 3개의 선택된 포인트들의 세트 사이의 2개의 피처 거리들을 포함하며, 그리고
상기 적어도 하나의 추가적인 측지 거리들의 쌍 중 각각의 주어진 추가적인 측지 거리들의 쌍은 상기 제1 표면 맵에 표시된 3개의 선택된 포인트들의 세트 사이의 2개의 측지 거리들을 포함하는, 방법.The method of claim 1, wherein the second loss value is further based on at least one additional pair of feature distances and at least one additional pair of geodetic distances,
Each given pair of additional feature distances of the at least one pair of additional feature distances comprises two feature distances between the set of three selected points represented in the first feature map, and
The method of claim 1 , wherein each given pair of at least one additional geodetic distances comprises two geodetic distances between a set of three selected points displayed in the first surface map.
하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제6 포인트와 상기 제1 피처 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이의 제4 피처 거리들의 세트를 결정하는 단계;
하나 이상의 프로세서에 의해, 상기 제1 표면 맵에 표현된 제6 포인트와 상기 제1 표면 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이의 제3 측지 거리들의 세트를 결정하는 단계; 및
하나 이상의 프로세서에 의해, 상기 손실값들의 세트 중 제3 손실값을 결정하는 단계를 더 포함하며, 상기 제3 손실값은 상기 제4 피처 거리들의 세트 및 상기 제3 측지 거리들의 세트에 기초하는, 방법.In claim 1,
determining, by one or more processors, a fourth set of feature distances between a sixth point represented in the first feature map and all other points of the first image represented in the first feature map;
determining, by one or more processors, a third set of geodetic distances between a sixth point represented in the first surface map and all other points of the first image represented in the first surface map; and
determining, by one or more processors, a third loss value of the set of loss values, wherein the third loss value is based on the fourth set of feature distances and the third set of geodetic distances. method.
상기 적어도 하나의 추가적인 피처 거리들의 세트 중 각각의 주어진 추가적인 피처 거리들의 세트는 상기 제1 피처 맵에 표현된 선택된 포인트와 상기 제1 피처 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이에 있으며, 그리고
상기 적어도 하나의 추가적인 측지 거리들의 세트 중 각각의 주어진 추가적인 측지 거리들의 세트는 상기 제1 표면 맵에 표현된 선택된 포인트와 상기 제1 표면 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이에 있는, 방법.The method of claim 5, wherein the third loss value is further based on at least one additional set of feature distances and at least one additional set of geodetic distances,
Each given set of additional feature distances of the at least one additional feature distance is between a selected point represented in the first feature map and all other points of the first image represented in the first feature map, and
Each given set of additional geodetic distances of the at least one additional set of geodetic distances is between a selected point represented in the first surface map and all other points of the first image represented in the first surface map. method.
하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제6 포인트와 상기 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제4 피처 거리들의 세트를 결정하는 단계;
하나 이상의 프로세서에 의해, 상기 제2 표면 맵에 표현된 제1 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제3 측지 거리들의 세트를 결정하는 단계, 상기 제2 표면 맵은 상기 제2 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며, 그리고 상기 제2 표면 맵의 제1 포인트와 상기 제1 피처 맵의 제6 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며; 및
하나 이상의 프로세서에 의해, 상기 손실 값들의 세트 중 제3 손실 값을 결정하는 단계를 더 포함하며, 상기 제3 손실 값은 상기 제4 피처 거리들의 세트 및 상기 제3 측지 거리들의 세트에 기초한다.In claim 1,
determining, by one or more processors, a fourth set of feature distances between a sixth point represented in the first feature map and all points of a second image represented in the second feature map;
determining, by one or more processors, a third set of geodetic distances between a first point represented in the second surface map and all points in a second image represented in the second surface map, the second surface The map corresponds to the second image and was created using a three-dimensional model of the object, and the first point of the second surface map and the sixth point of the first feature map are the same in the three-dimensional model of the object. Corresponds to a feature; and
and determining, by the one or more processors, a third loss value of the set of loss values, wherein the third loss value is based on the fourth set of feature distances and the third set of geodetic distances.
상기 적어도 하나의 추가적인 피처 거리들의 세트 중 각각의 주어진 추가적인 피처 거리들의 세트는 상기 제1 피처 맵에 표현된 선택된 포인트와 상기 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이에 있으며, 그리고 상기 적어도 하나의 추가적인 측지 거리들의 세트 중 각각의 주어진 추가적인 측지 거리들의 세트는 상기 제2 표면 맵에 표현된 대응 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이에 있으며, 상기 제2 표면 맵의 대응 포인트와 상기 제1 피처 맵의 선택된 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하는, 방법.The method of claim 7, wherein the third loss value is further based on at least one additional set of feature distances and at least one additional set of geodetic distances,
Each given set of additional feature distances of the at least one additional feature distance is between a selected point represented in the first feature map and all points of the second image represented in the second feature map, and Each given set of additional geodetic distances of the at least one additional set of geodetic distances is between a corresponding point represented in the second surface map and all points in the second image represented in the second surface map, The method of claim 1, wherein the corresponding point of the second surface map and the selected point of the first feature map correspond to the same feature in the three-dimensional model of the object.
하나 이상의 프로세서에 의해, 상기 제1 피처 맵에 표현된 제7 포인트와 상기 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제5 피처 거리들의 세트를 결정하는 단계;
하나 이상의 프로세서에 의해, 상기 제2 표면 맵에 표현된 제1 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제4 측지 거리들의 세트를 결정하는 단계, 상기 제2 표면 맵은 상기 제2 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며, 그리고 상기 제2 표면 맵의 제1 포인트와 상기 제1 피처 맵의 제7 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며; 및
하나 이상의 프로세서에 의해, 상기 손실값들의 세트 중 제4 손실값을 결정하는 단계를 더 포함하며, 상기 제4 손실값은 상기 제5 피처 거리들의 세트 및 상기 제4 측지 거리들의 세트에 기초하는, 방법.In claim 5,
determining, by one or more processors, a fifth set of feature distances between a seventh point represented in the first feature map and all points of a second image represented in the second feature map;
determining, by one or more processors, a fourth set of geodetic distances between a first point represented in the second surface map and all points in a second image represented in the second surface map, the second surface The map corresponds to the second image and was created using a three-dimensional model of the object, and the first point of the second surface map and the seventh point of the first feature map are the same in the three-dimensional model of the object. Corresponds to a feature; and
determining, by one or more processors, a fourth loss value of the set of loss values, wherein the fourth loss value is based on the fifth set of feature distances and the fourth set of geodetic distances. method.
신경 네트워크를 저장하는 메모리; 및
상기 메모리에 결합되고, 상기 신경 네트워크를 사용하여 이미지들에서 대응관계를 예측하도록 구성된 하나 이상의 프로세서를 포함하며,
상기 신경 네트워크는 트레이닝 방법에 따라 이미지들의 대응관계를 예측하도록 트레이닝되었으며, 상기 트레이닝 방법은:
대상의 제1 이미지에 기초하여 제1 피처 맵 및 대상의 제2 이미지에 기초하여 제2 피처 맵을 생성하는 단계, 상기 제1 이미지 및 제2 이미지는 상이하며, 상기 대상의 3차원 모델을 사용하여 생성되었으며;
상기 제1 피처 맵에 표현된 제1 포인트와 상기 제2 피처 맵에 표현된 제2 포인트 사이의 제1 피처 거리를 결정하는 단계, 상기 제1 포인트와 제2 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며;
상기 제1 피처 맵에 표현된 제3 포인트와 상기 제1 피처 맵에 표현된 제4 포인트 사이의 제2 피처 거리를 결정하는 단계;
제1 표면 맵에 표현된 제3 포인트와 제4 포인트 사이의 제1 측지 거리를 결정하는 단계, 상기 제1 표면 맵은 상기 제1 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며;
상기 제1 피처 맵에 표현된 제3 포인트와 상기 제1 피처 맵에 표현된 제5 포인트 사이의 제3 피처 거리를 결정하는 단계;
상기 제1 표면 맵에 표현된 제3 포인트와 제5 포인트 사이의 제2 측지 거리를 결정하는 단계;
손실값들의 세트 중 제1 손실값을 결정하는 단계, 상기 제1 손실값은 상기 제1 피처 거리에 기초하며;
상기 손실값들의 세트 중 제2 손실값을 결정하는 단계, 상기 제2 손실값은 상기 제2 피처 거리, 상기 제3 피처 거리, 상기 제1 측지 거리 및 상기 제2 측지 거리에 기초하며; 및
상기 손실값들의 세트에 적어도 부분적으로 기초하여 신경 네트워크의 하나 이상의 파라미터를 수정하는 단계를 포함하는, 프로세싱 시스템.As a processing system,
memory that stores neural networks; and
one or more processors coupled to the memory and configured to predict correspondences in images using the neural network,
The neural network was trained to predict the correspondence between images according to a training method, the training method being:
Generating a first feature map based on a first image of the object and a second feature map based on a second image of the object, wherein the first image and the second image are different and use a three-dimensional model of the object. was created;
Determining a first feature distance between a first point represented in the first feature map and a second point represented in the second feature map, wherein the first point and the second point are in the three-dimensional model of the object. Corresponds to the same feature;
determining a second feature distance between a third point represented in the first feature map and a fourth point represented in the first feature map;
determining a first geodetic distance between a third point and a fourth point represented in a first surface map, the first surface map corresponding to the first image and generated using a three-dimensional model of the object;
determining a third feature distance between a third point represented in the first feature map and a fifth point represented in the first feature map;
determining a second geodetic distance between a third point and a fifth point represented in the first surface map;
determining a first loss value from a set of loss values, the first loss value being based on the first feature distance;
determining a second loss value of the set of loss values, the second loss value being based on the second feature distance, the third feature distance, the first geodetic distance, and the second geodetic distance; and
Modifying one or more parameters of a neural network based at least in part on the set of loss values.
상기 제1 피처 맵에 표현된 제6 포인트와 상기 제1 피처 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이의 제4 피처 거리들의 세트를 결정하는 단계;
상기 제1 표면 맵에 표현된 제6 포인트와 상기 제1 표면 맵에 표현된 제1 이미지의 다른 모든 포인트들 사이의 제3 측지 거리들의 세트를 결정하는 단계; 및
상기 손실값들의 세트 중 제3 손실값을 결정하는 단계를 더 포함하며, 상기 제3 손실값은 상기 제4 피처 거리들의 세트 및 상기 제3 측지 거리들의 세트에 기초하는, 프로세싱 시스템.The method of claim 17, wherein the training method:
determining a fourth set of feature distances between a sixth point represented in the first feature map and all other points of the first image represented in the first feature map;
determining a third set of geodetic distances between a sixth point represented in the first surface map and all other points of the first image represented in the first surface map; and
Determining a third loss value of the set of loss values, wherein the third loss value is based on the fourth set of feature distances and the third set of geodetic distances.
상기 제1 피처 맵에 표현된 제6 포인트와 상기 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제4 피처 거리들의 세트를 결정하는 단계;
상기 제2 표면 맵에 표현된 제1 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제3 측지 거리들의 세트를 결정하는 단계, 상기 제2 표면 맵은 상기 제2 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며, 그리고 상기 제2 표면 맵의 제1 포인트와 상기 제1 피처 맵의 제6 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며; 및
상기 손실 값들의 세트 중 제3 손실 값을 결정하는 단계를 더 포함하며, 상기 제3 손실 값은 상기 제4 피처 거리들의 세트 및 상기 제3 측지 거리들의 세트에 기초하는, 프로세싱 시스템.The method of claim 17, wherein the training method:
determining a fourth set of feature distances between a sixth point represented in the first feature map and all points of a second image represented in the second feature map;
determining a third set of geodetic distances between a first point represented in the second surface map and all points in a second image represented in the second surface map, the second surface map being corresponds to and was generated using a three-dimensional model of the object, and the first point of the second surface map and the sixth point of the first feature map correspond to the same feature in the three-dimensional model of the object; and
Determining a third loss value of the set of loss values, wherein the third loss value is based on the fourth set of feature distances and the third set of geodetic distances.
상기 제1 피처 맵에 표현된 제7 포인트와 상기 제2 피처 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제5 피처 거리들의 세트를 결정하는 단계;
상기 제2 표면 맵에 표현된 제1 포인트와 상기 제2 표면 맵에 표현된 제2 이미지의 모든 포인트들 사이의 제4 측지 거리들의 세트를 결정하는 단계, 상기 제2 표면 맵은 상기 제2 이미지에 대응하고 상기 대상의 3차원 모델을 사용하여 생성되었으며, 그리고 상기 제2 표면 맵의 제1 포인트와 상기 제1 피처 맵의 제7 포인트는 상기 대상의 3차원 모델에서 동일한 피처에 대응하며; 및
상기 손실값들의 세트 중 제4 손실값을 결정하는 단계를 더 포함하며, 상기 제4 손실값은 상기 제5 피처 거리들의 세트 및 상기 제4 측지 거리들의 세트에 기초하는, 프로세싱 시스템.The method of claim 18, wherein the training method:
determining a fifth set of feature distances between a seventh point represented in the first feature map and all points of a second image represented in the second feature map;
determining a fourth set of geodetic distances between a first point represented in the second surface map and all points in a second image represented in the second surface map, the second surface map being corresponds to and was generated using a three-dimensional model of the object, and the first point of the second surface map and the seventh point of the first feature map correspond to the same feature in the three-dimensional model of the object; and
determining a fourth loss value of the set of loss values, wherein the fourth loss value is based on the fifth set of feature distances and the fourth set of geodetic distances.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/CN2021/080137 WO2022188086A1 (en) | 2021-03-11 | 2021-03-11 | Systems and methods for training models to predict dense correspondences in images using geodesic distances |
Publications (1)
Publication Number | Publication Date |
---|---|
KR20230133394A true KR20230133394A (en) | 2023-09-19 |
Family
ID=83226155
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
KR1020237030079A KR20230133394A (en) | 2021-03-11 | 2021-03-11 | System and method for training models to predict dense correspondence of images using geodetic distances |
Country Status (6)
Country | Link |
---|---|
US (1) | US11954899B2 (en) |
EP (1) | EP4268187A1 (en) |
JP (1) | JP2024508038A (en) |
KR (1) | KR20230133394A (en) |
CN (1) | CN116964624A (en) |
WO (1) | WO2022188086A1 (en) |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3028256A4 (en) * | 2013-07-31 | 2016-10-19 | Microsoft Technology Licensing Llc | Geodesic saliency using background priors |
KR20180087994A (en) | 2017-01-26 | 2018-08-03 | 삼성전자주식회사 | Stero matching method and image processing apparatus |
US10733755B2 (en) | 2017-07-18 | 2020-08-04 | Qualcomm Incorporated | Learning geometric differentials for matching 3D models to objects in a 2D image |
CN109389156B (en) | 2018-09-11 | 2022-05-03 | 深圳大学 | Training method and device of image positioning model and image positioning method |
CN112016677A (en) | 2019-09-23 | 2020-12-01 | 南京地平线机器人技术有限公司 | Deep neural network training method and device and electronic equipment |
CN111401521B (en) | 2020-03-11 | 2023-10-31 | 北京迈格威科技有限公司 | Neural network model training method and device, and image recognition method and device |
US11380061B2 (en) * | 2020-06-11 | 2022-07-05 | Samsung Electronics Co., Ltd. | Method and apparatus for three-dimensional (3D) object and surface reconstruction |
CN111915555A (en) | 2020-06-19 | 2020-11-10 | 杭州深睿博联科技有限公司 | 3D network model pre-training method, system, terminal and storage medium |
US20220156528A1 (en) * | 2020-11-16 | 2022-05-19 | Qualcomm Incorporated | Distance-based boundary aware semantic segmentation |
-
2021
- 2021-03-11 JP JP2023555366A patent/JP2024508038A/en active Pending
- 2021-03-11 EP EP21929564.9A patent/EP4268187A1/en active Pending
- 2021-03-11 US US18/274,371 patent/US11954899B2/en active Active
- 2021-03-11 KR KR1020237030079A patent/KR20230133394A/en not_active Application Discontinuation
- 2021-03-11 WO PCT/CN2021/080137 patent/WO2022188086A1/en active Application Filing
- 2021-03-11 CN CN202180094554.0A patent/CN116964624A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
EP4268187A1 (en) | 2023-11-01 |
CN116964624A (en) | 2023-10-27 |
JP2024508038A (en) | 2024-02-21 |
US20240046618A1 (en) | 2024-02-08 |
US11954899B2 (en) | 2024-04-09 |
WO2022188086A1 (en) | 2022-09-15 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Ge et al. | Real-time 3D hand pose estimation with 3D convolutional neural networks | |
US20220270402A1 (en) | Face Reconstruction from a Learned Embedding | |
JP7178396B2 (en) | Method and computer system for generating data for estimating 3D pose of object included in input image | |
CN104637090B (en) | A kind of indoor scene modeling method based on single picture | |
US20220301295A1 (en) | Recurrent multi-task convolutional neural network architecture | |
CN109684969B (en) | Gaze position estimation method, computer device, and storage medium | |
CN111739005B (en) | Image detection method, device, electronic equipment and storage medium | |
US11748937B2 (en) | Sub-pixel data simulation system | |
EP3326156B1 (en) | Consistent tessellation via topology-aware surface tracking | |
EP3872760A2 (en) | Method and apparatus of training depth estimation network, and method and apparatus of estimating depth of image | |
CN110458924B (en) | Three-dimensional face model establishing method and device and electronic equipment | |
CN115205949A (en) | Image generation method and related device | |
JP2022036918A (en) | Uv mapping on 3d object with the use of artificial intelligence | |
CN114677572B (en) | Object description parameter generation method and deep learning model training method | |
WO2021263035A1 (en) | Object recognition neural network for amodal center prediction | |
EP4086853A2 (en) | Method and apparatus for generating object model, electronic device and storage medium | |
JP7452698B2 (en) | Reinforcement learning model for labeling spatial relationships between images | |
KR102068489B1 (en) | 3d object creation apparatus | |
US20230196651A1 (en) | Method and apparatus with rendering | |
KR20230133394A (en) | System and method for training models to predict dense correspondence of images using geodetic distances | |
CN117011449A (en) | Reconstruction method and device of three-dimensional face model, storage medium and electronic equipment | |
Golyanik | Robust Methods for Dense Monocular Non-Rigid 3D Reconstruction and Alignment of Point Clouds | |
EP3929866A2 (en) | Inpainting method and apparatus for human image, and electronic device | |
Nadar et al. | Sensor simulation for monocular depth estimation using deep neural networks | |
Lalande | Three-dimensional understanding through two-dimensional observation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A302 | Request for accelerated examination | ||
E902 | Notification of reason for refusal |