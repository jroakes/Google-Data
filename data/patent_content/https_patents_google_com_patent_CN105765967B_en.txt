CN105765967B - The method, system and medium of the setting of first camera are adjusted using second camera - Google Patents
The method, system and medium of the setting of first camera are adjusted using second camera Download PDFInfo
- Publication number
- CN105765967B CN105765967B CN201480065311.4A CN201480065311A CN105765967B CN 105765967 B CN105765967 B CN 105765967B CN 201480065311 A CN201480065311 A CN 201480065311A CN 105765967 B CN105765967 B CN 105765967B
- Authority
- CN
- China
- Prior art keywords
- image
- image capture
- camera
- setting
- tet
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/204—Image signal generators using stereoscopic image cameras
- H04N13/239—Image signal generators using stereoscopic image cameras using two 2D image sensors having a relative position equal to or related to the interocular distance
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/45—Cameras or camera modules comprising electronic image sensors; Control thereof for generating image signals from two or more image sensors being of different type or operating in different modes, e.g. with a CMOS sensor for moving images in combination with a charge-coupled device [CCD] for still images
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/296—Synchronisation thereof; Control thereof
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/70—Circuitry for compensating brightness variation in the scene
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/70—Circuitry for compensating brightness variation in the scene
- H04N23/741—Circuitry for compensating brightness variation in the scene by increasing the dynamic range of the image compared to the dynamic range of the electronic image sensors
Abstract
In general, imaging can refer in a digital format and/or film format (for example, with photo and/or sport video) captures and show the color and light characteristic of real world.In the presence of a large amount of different image capture devices, therefore the various ways for capturing image data are provided for consumer.As the image capture device of such as camera becomes more to popularize, such equipment is used as individual hardware device, or can be integrated into the equipment of various other types.Such as, still camera and video camera are routinely included in wireless telecom equipment (for example, mobile phone), tablet computer, laptop computer, video game interface, home-automation device and even in automobile and other kinds of vehicle now.
Description
Technical field
In general, imaging can refer in a digital format and/or film format (for example, with photo and/or sport video) is caught
Obtain and show the color and light characteristic of real world.There are a large amount of different image capture devices, therefore are consumption
Person provides the various ways for capturing image data.
Background technique
As the image capture device of such as camera becomes more to popularize, such equipment is used as individual hardware
Equipment, or can be integrated into the equipment of various other types.For example, still camera and video camera now routinely by
It is included in wireless telecom equipment (for example, mobile phone), tablet computer, laptop computer, video game interface, household
In automation equipment and even automobile and other kinds of vehicle.
Summary of the invention
Some digital camera systems do not include the individual of the automation function for such as automatic exposure and auto-focusing
Sensing system, and therefore such function is provided using the same sensing system for being used for image capture.Therefore, this
The video section that the automation function of sample can cause part and/or video that for example exposure setting rapidly changes out of focus.Cause
This, example embodiment may include two or more camera systems being directed in a same direction, so that in the first phase
Second camera can provide such automation function while machine is capturing image data.The configuration of such polyphaser can be with
It helps prevent when equipment simultaneously adjusts processing for image capture and automated image setting using same imaging sensor
It is some in the undesirable effect generated.
In an aspect, method is related to: (a) operating the first image capture system by calculating equipment with capturing scenes
The first image data, wherein the first image capture system is initially captured using the first value being arranged for the first image
First image data；And (b) when the first image capture system is capturing the first image data: (i) is by calculating equipment
To operate the second image capture system to determine the updated value for the setting of the first image, wherein the first image capture system
System and the second image capture system are disposed in on locking equipment and being directed in substantially the same direction；And (ii)
First is sent by the instruction that instruction continues to capture using the updated value for being used for the setting of the first image the first image data
Image capture system.
In another aspect, system includes: control system；And two or more image capture systems, at least wrap
Include the first image capture system and the second image capture system, wherein the first image capture system and the second image capture system
It is oriented in substantially the same direction.Control system is configured as: (a) initially operate the first image capture system so that
The first image data is captured with the first value being arranged for the first image；And it (b) is being caught in the first image capture system
While image data of winning the first place: (i) operates the second image capture system to determine for the updated of the first image setting
Value；And (ii) makes the first image capture system using the updated image value for the setting of the first image to continue to capture
First image data.
In another aspect, non-transitory computer-readable medium is stored with instruction wherein, which is set by calculating
Standby to can be performed so that calculating equipment executes function, which includes: that (a) operates the first image capture system with capturing scenes
First image data, wherein the first image capture system is initially using the first value being arranged for the first image to capture
One image data；And (b) while the first image capture system is capturing the first image data: (i) operates the second figure
As capture systems are to determine the updated value being arranged for the first image, wherein the first image capture system and the second image
Capture systems are disposed in on locking equipment and being directed in substantially the same direction；And instruction is used use by (ii)
The first image capture system is sent in the updated value of the first image setting to continue to capture the instruction of the first image data.
In a further aspect, system may include: that (a) is used to operate the first image capture system with the of capturing scenes
The device of one image data, wherein the first image capture system is initially caught using the first value being arranged for the first image
It wins the first place image data；And (b) under being carried out while the first image capture system is capturing the first image data
The device stated: (i) operates the second image capture system to determine the updated value for the setting of the first image, wherein first
Image capture system and the second image capture system are disposed in on locking equipment and being oriented at substantially the same direction
On；And (ii) will indicate the instruction for continuing to capture the first image data using the updated value for being used for the setting of the first image
It is sent to the first image capture system.
By the way that following detailed description, these and other aspect, advantage and alternative is suitably read with reference to the drawings
Those of ordinary skill in the art will be apparent.In addition, it should be understood that in this Summary and in its of this document
Description provided in his part is intended to by way of example rather than is shown by way of limitation claimed
Theme.
Detailed description of the invention
Figure 1A depicts the front view, right side view and rearview of digital camera devices according to example embodiment.
Figure 1B show according to example embodiment, with the arrangement for being oriented four cameras in a same direction
Digital camera devices.
Fig. 1 C show according to example embodiment, with the arrangement for being oriented four cameras in a same direction
Another digital camera devices.
Fig. 1 D show according to example embodiment, with the arrangement for being oriented six cameras in a same direction
Digital camera devices
Fig. 2 is to show the simplified block diagram of some components of Example Computing Device.
Fig. 3 depicts flow chart according to example embodiment.
Fig. 4 A, which is depicted, to be come to create histogram from one or more captured images according to example embodiment.
Fig. 4 B is depicted carrys out training image database according to example embodiment.
Fig. 5 depicts payload burst (burst) structure for capturing image according to example embodiment.
Fig. 6 depicts flow chart according to example embodiment.
Fig. 7 depicts another flow chart according to example embodiment.
Fig. 8 A depicts division paxel down-sampling according to example embodiment.
Fig. 8 B also illustrates division paxel down-sampling according to example embodiment.
Fig. 8 C, which is depicted, to be come to create histogram from one or more captured images according to example embodiment.
Fig. 9 depicts another flow chart according to example embodiment.
Figure 10 A to Figure 10 C depicts additional flow chart according to example embodiment.
Specific embodiment
I.It summarizes
Some digital cameras of such as high-end DSLR camera include providing such as auto-focusing, automatic exposure and automatic white
The individual personal module of the automation function of balance.For example adjustable focusing of these components and/or exposure, and to by camera
Sensor institute captured image data (for example, video) do not have perceptible influence.Lead to for example, many SLR cameras utilize
Camera lens (through-the-lens) optical AF sensor is crossed for auto-focusing, and is used for using individual sensor array
Light measurement.
Such as it is often used in other digital camera systems of the less expensive camera in mobile phone peace plate-type device
System does not include the individual component for such automation function.Therefore, such camera system can be adjusted iteratively and be set
It sets and analyzes the image data of generation to execute auto-focusing and other automation functions.Such automation function is logical
" experiment and adjusting (experiment and adjust) " function can be often referred to as.As an example, in mobile telephone camera
In, auto-focusing can be related to camera and repeatedly adjust focusing setting, capture image data and analysis captured image data,
Until desired object is confirmed as focusing.The certain types of iteration experiment and regulatory function can be referred to as " focusing
It searches (hunting) ".
When user moves mobile telephone camera while recording video, (one or more into the visual field of camera
It is a) (one or more) distance of object may change, and focus and search possibility.It can be quite fast although focusing is searched
The segment for the video for executing, but being recorded fastly during search of focusing may perceive out of focus.Therefore, example embodiment
It can help to provide the auto-focusing that can be used in applying as mobile phone and other and/or other automations are adjusted
The shortcomings that saving function, being searched without focusing.
Specifically, example apparatus may include two or more camera systems being directed in the same direction.In this way
One or more in the camera system that ground is configured can execute one or more automation experiments and adjusting processing, with
Determine one or more image captures setting of another camera system for just be used to capture image data.This can be with
It prevents when equipment when image capture and automation experiment and regulatory function simultaneously using same imaging sensor for that may lead
The some undesirable influences caused.In addition, can be improved in this way using the individual camera system for testing and adjusting processing
Processing as a result, because experiment do not constrained by the needs for capturing image data using same imaging sensor.
For example, it is contemplated that the mobile phone for including the case where two camera systems, purpose of the situation for three-dimensional imaging
Become more common.If mobile phone is used for three-dimensional imaging without using two cameras, mobile phone can use the first phase
Machine records video, and is used for auto-focusing using second camera simultaneously.In this way, based on using second camera be carrying out from
Dynamic focusing process as a result, in record the focusing setting of first camera can be adjusted.In addition, because about focusing search pair
Worry is not present in the influence of the video of generation, so second camera system can compared with the setting that may be additionally tested
To test the setting and/or the greater amount of setting of test of wider range.The power of test of these extensions can be improved by certainly
The result that dynamic focusing process is realized.
Here, " camera system " can take following forms: system in camera, camera is communicatively coupled to camera
Individual system or camera and one or more other systems combination.In addition, for simplicity, as described herein
Example can assign specific function and/or characteristic to " camera " or " camera apparatus ".It should be appreciated that in many situations, quilt
The function and/or characteristic for assigning camera or camera apparatus can similarly be endowed camera system, or even when this is not by clearly
Narrative tense is also such.
II.Schematical system
The physical assemblies of image capture device may include: the aperture that light is entered by it；Represented by capturing by light
Image recording surface；And it is positioned in front of aperture with the mirror by the focusing of at least part of image on recording surface
Head.Aperture can be fixed size or adjustable.In analogue camera, recording surface can be photographic film.In number
In camera, recording surface may include electronic image sensor (for example, charge coupled device (CCD) or complementary metal oxide half
Conductor (CMOS) sensor) institute's captured image is transmitted and/or is stored in data storage cell (for example, memory).
Shutter can be couple to camera lens or recording surface, or can be coupled near camera lens or recording surface.Shutter can
With in the closed position, it stops light to reach recording surface in this position；Or it is in an open position, light is permitted in this position
Perhaps recording surface is reached.The position of shutter can be controlled by shutter release button.For example, shutter can be acquiescently in the closed position.
When shutter release button is triggered (for example, being pressed), shutter can change to open position to reach from closed position is referred to as shutter
One period in period.During the shutter period, image can be trapped on recording surface.In shutter end cycle, fastly
Door, which can change, returns to closed position.
Alternatively, shutter processing can be electronics.For example, ccd image sensor electronic shutter " opening " it
Before, sensor can be reset to remove any residual signal in its photodiode.When electronic shutter stays open,
Photodiode can be with stored charge.When shutter close or after shutter close, these charges can be transferred to for a long time
Data storage device.The combination of mechanical shutter and electronic shutter is also possible.
Unrelated with type, shutter can be activated and/or be controlled by some except shutter release button.For example, shutter can
To be activated by soft key, timer or some other triggers.Here, term " image capture " can instruct to cause one or more
Any mechanical and/or electronic shutter processing that a photo is recorded, and it is unrelated with how to trigger or control shutter processing.
A. with the equipment of more image capture systems
As was noted earlier, digital camera can be individual equipment or can be integrated with other equipment.As
Example, Figure 1A show the positive element (factor) of digital camera devices 100.Digital camera devices 100, which can be, for example moves
Mobile phone, tablet computer or wearable computing devices.However, other embodiments are possible.Digital camera devices 100 can
To include various elements, such as main body 102, Front camera 104, polynary display 106, shutter release button 108 and other buttons
110.Digital camera devices 100 may further include two rearmounted cameras 112A and 112B.Front camera 104 can be positioned
On the side for usually facing user when main body 102 is in operation, or be positioned in on the same side of polynary display 106.
Rearmounted camera 112A and 112B can be positioned on the side opposite with Front camera 104 of main body 102.Before camera is known as
It is setting or postposition to be arbitrary, and digital camera devices 100 may include be positioned in it is multiple on each side of main body 102
Camera.
The camera lens of rearmounted camera 112A and 112B are disposed on the upper angle at the back side of digital camera devices 100, and by
Orientation is in substantially the same direction.It will be appreciated, however, that other polyphasers arrangement is possible.Specifically, it is directed
The camera lens of two or more cameras in substantially the same direction can be disposed in different on the surface of phone
In composition.It is arranged for example, describing other several polyphasers herein in relation to Figure 1B to Fig. 1 D.
Specifically, Figure 1B is shown with the arrangement for being directed four camera 122A to 122D in a same direction
Mobile device 120, four camera 122A to 122D include two camera 122A on the upper angle of mobile device (similar with 122B
In Figure 1A), and two additional camera 122C and 122D at the inferior horn of mobile device 120.In addition, Fig. 1 C shows tool
There is another arrangement for being directed four cameras in the same direction.Specifically, the arrangement in Fig. 1 C includes in equipment 140
A camera 144A to 144D in each angle.
Further, Fig. 1 D shows the arrangement with six camera 166A to 166F towards identical direction.?
In Fig. 1 D, six camera 166A to 166F are placed on mobile device 160 with " tissue (organic) " arrangement of less structuring
The back side on.Note that the arrangement with three or more cameras can provide multiple baselines between different cameras pair.Example
Such as, all six cameras arrangements as shown in Figure 1 D can for example provide up to 15 different baselines for three-dimensional imaging.More
Generally, the arrangement for being directed n camera in substantially the same direction can provide up to a baseline of C (n, k).
Furthermore it is envisaged that arriving, polyphaser arrangement may include camera more more than two.In addition, the camera lens in polyphaser arrangement
It can be directed at different angles about the surface for being disposed with camera lens on it.Further, it should be understood that polyphaser cloth
It sets on other sides that may be implemented within digital camera devices.Other modifications about the arrangement of polyphaser shown in the accompanying drawings
It is possible.
Polynary display 106 can represent cathode-ray tube (CRT) display, light emitting diode (LED) display, liquid crystal
The display of display (LCD), plasma scope or any other type as known in the art.In some embodiments
In, polynary display 106 can be shown by one or two of Front camera 104 and/or rearmounted camera 112A and 112B just
In the present image of capture or by these magazine any one or figures that are that any combination can capture or being recently captured
The digital representation of picture.Therefore, polynary display 106 may be used as the view finder of any camera.Polynary display 106 can also prop up
It holds and touches screen and/or pressure-sensitive function, can adjust the setting of any aspect of digital camera devices 100 and/or match
It sets.
Front camera 104 may include the associated optical element of imaging sensor and such as camera lens.Front camera 104
Zoom capabilities can be provided or can have fixed focal length.In other embodiments, interchangeable camera lens can be with preposition phase
Machine 104 is used together.Front camera 104 can have variable mechanical aperture and machinery and/or electronic shutter.Front camera
104 can be additionally configured to capture still image, video image or both.In addition, Front camera 104 can represent single view
, three-dimensional or multiple views camera.Rearmounted camera 112A and 112B can be similarly or differently arranged.Extraly, preceding
Any combination for setting camera 104, rearmounted camera 112A and 112B or these cameras can be actually one or more cameras
Array (or the array for directing light to the camera lens on general image sensor).
Any one of Front camera 104 and rearmounted camera 112A and 112B or any combination may include light fixture
Or it is associated with light fixture, which provides light field to illuminate target object.For example, light fixture can be provided to mesh
Mark the flash of light or constant illumination of object.Light fixture can be additionally configured to provide including structure light, polarised light and have spy
Determine the light field of one or more of light of spectral content.In the context of embodiment herein, it is known that and be used for
It is possible for restoring the other kinds of light field of three-dimensional (3D) model from object.
Any one of Front camera 104 and rearmounted camera 112A and 112B or any combination may include that environment light passes
Sensor is associated with ambient light sensor, the ambient light sensor can continuously or or ground determine that camera can capture
Scene ambient brightness.In some equipment, ambient light sensor can be used to adjust for display associated with camera
The display brightness of (for example, view finder).When identified environmental light brightness is high, the brightness degree of screen can be increased to
Screen is obtained to be easier to check.When identified environmental light brightness is low, the brightness degree of screen can be lowered, it is same so that
Screen is easier to check and potentially save electric power.Extraly, the input of ambient light sensor may be used to determine whether correlation
The exposure of the camera of connection is arranged, or in this determination it is helpful.
Digital camera devices 100 can be configured as using polynary display 106 and any Front camera 104 or postposition phase
One or two of machine 112A and 112B capture the image of target object.Institute's captured image can be multiple still images
Or video flowing.Image capture can by activation shutter release button 108, press soft key on polynary display 106 or by
Other mechanism trigger.Dependent on embodiment, image can be automatically captured with specific time interval, such as pressed
When shutter release button 108, in the suitable illumination condition of target object, by the mobile specific distance of digital camera devices 100
When or according to predetermined capture arrange (schedule).
B. schematical apparatus assembly
As noted above, the function of digital camera devices 100 --- or digital camera of other type ---
It is desirably integrated into and calculates in equipment, or the form for calculating equipment can be taken, such as mobile phone, tablet computer, above-knee
Type computer etc..For exemplary purposes, Fig. 2 be show may include photomoduel 224 Example Computing Device 200 it is some
The simplified block diagram of component.Photomoduel 224 may include multiple cameras, such as camera 112A and 112B.
By way of example, and not limitation, calculate equipment 200 can be cellular mobile telephone (for example, smart phone),
It is still camera, video camera, facsimile machine, computer (such as desktop type, notebook type, flat or handheld computer), a
It is personal digital assistant (PDA), household automatic component, digital video recorder (DVR), digital television, remote controllers, wearable
Calculate equipment or equipped at least some image captures and/or the equipment of some other types of image-capable.It should manage
Solution, calculating equipment 200 can represent that the physics camera apparatus of such as digital camera, camera applications are operated on it with software
Specific physical hardware platform or be configured as execute camera function hardware or software other combination.
As shown in Figure 2, calculating equipment 200 may include communication interface 202, user interface 204, processor 206, data
Storage device 208 and photomoduel 224, it is all these can be led to by system bus, network or other connection mechanisms 210
Link together to letter.
Communication interface 202 can operate for allow calculate equipment 200 using analog or digital modulation come with other equipment, connect
Enter network and/or transmission network is communicated.Therefore, communication interface 202 can promote circuit switching and/or packet switching network
Communication, such as traditional telephone service (POTS) communication and/or Internet Protocol (IP) or other packetized communications.For example, communication
Interface 202 may include the chipset and antenna arranged for the wireless communication with Radio Access Network or access point.In addition,
Communication interface 202 can take the form of drawing lines (wireline) interface or including drawing lines interface, drawing lines interface such as Ethernet,
Universal serial bus (USB) or the port high-definition multimedia interface (HDMI).Communication interface 202 can also take wireless interface
Form or including wireless interface, wireless interface such as Wifi,Global positioning system (GPS) or wide area wireless interface
(for example, WiMAX or 3GPP long term evolution (LTE)).However, the physical layer interface of other forms and other kinds of standard or special
Some communication protocol can be used in communication interface 202.In addition, communication interface 202 may include multiple physical communication interfaces
(for example, Wifi interface,Interface and wide area wireless interface).
User interface 204 can operate to allow to calculate equipment 200 with the mankind or non-human user and interacting, and such as receive
From the input of user and provide a user output.Therefore, user interface 204 may include input module, such as keypad, key
Disk touches sensitivity or pressure-sensitive panel, computer mouse, trace ball, control stick, microphone etc..User interface 204 can also include
One or more output precisions, such as can be with the display screen of pressure-sensitive panel combination.Display screen can be based on CRT,
The technology of LCD and/or LED or other technologies that are currently known or developing later.User interface 204 can also be configured
To generate (one via loudspeaker, loudspeaker socket, audio output port, audio output apparatus and/or other similar equipment
It is a or multiple) sense of hearing output.
In some embodiments, user interface 204 may include as the still camera supported by calculating equipment 200
And/or the display of the view finder of video camera function.Extraly, user interface 204 may include one or more buttons,
Switch, knob and/or driver plate, the configuration and focusing and the capture of image (for example, capture picture) of promotion camera function.This
The some or all of functions being implemented as on pressure-sensitive panel in a little button, switch, knob and/or driver plates may be possible.
Processor 206 may include one or more general processors --- for example, microprocessor --- and/or one
Or more application specific processor --- for example, digital signal processor (DSP), graphics processing unit (GPU), floating point unit
(FPU), network processing unit or specific integrated circuit (ASIC).In some instances, application specific processor may be able to carry out image
Processing, image alignment and blending image and other abilities.Data storage device 208 may include one or more volatibility
And/or non-volatile storage components, such as magnetic, light, flash memory or organic memory device, and can wholly or partly with place
It is integrated to manage device 206.Data storage device 208 may include removable and/or non-removable component.
Processor 206 may be able to carry out the program instruction 218 stored in data storage device 208 (for example, compiling
Or uncompiled programmed logic and/or machine code) to execute various functions as described herein.Therefore, data storage dress
Setting 208 may include the non-transitory computer-readable medium for being stored thereon with program instruction, and the program instruction is when by calculating equipment
When 200 execution, executed in method, processing or the function disclosed in this specification and/or attached drawing so that calculating equipment 200
Any one.The execution of the program instruction 218 carried out by processor 206 can cause processor 206 to use data 212.
By way of example, program instruction 218 may include being mounted on the 222 (example of operating system calculated in equipment 200
Such as, operating system kernel, one or more equipment (driver), and/or other modules) and one or more application program
220 (for example, camera function, address book, Email, web-browsing, social networks and/or game applications).Similarly, data
212 may include operating system data 216 and application data 214.Operating system data 216 can be mainly to operating system
222 may have access to, and application data 214 can be mainly to one or more addressable in application program 220.Using number
It can be disposed according to 214 in the file system visible or hiding for the user for calculating equipment 200.
Application program 220 can be communicated by one or more application programming interface (API) with operating system 222.For example,
The application program 220 that these API can promote reads and/or application program 214 is written, sendes or receive via communication interface 202
Information, receive information and/or shown on user interface 204 information, etc..
In some jargons (vernacular), application program 220 is briefly termed as " app ".Extraly, application program
220 can be downloaded to calculating equipment 200 by one or more application on site shops or application market.However, using journey
Sequence can also be otherwise mounted to calculate equipment 200, such as via web browser or by physical interface (for example,
USB port) it is installed in calculating equipment 200.
Photomoduel 224 may include but be not limited to aperture, shutter, recording surface (for example, photographic film and/or figure
As sensor), camera lens and/or shutter release button.Photomoduel 224 can at least by the software performed by processor 206 Lai
Control.In addition, photomoduel 224 may include multiple camera systems, it respectively include aperture, shutter, recording surface, image biography
Sensor, processor and/or shutter release button.When including multiple camera systems, it is understood that there may be shared some groups between the systems
Part, and there are not shared other assemblies.For example, each camera may include the aperture, lens and image sensing of its own
Device and simultaneously share such as processor and shutter release button other assemblies.As another example, each camera may include it
The lens of itself, but share same imaging sensor.Alternatively, the component of each camera system can be only for the camera
System and be utilized, and it is not shared with other camera systems.
C. digital picture
Still camera can capture one or more images when being triggered image capture.Video camera can be with specific
Rate (for example, 24 images --- or frame --- are per second) capture image, as long as image capture holding be triggered (for example,
It is pressed when shutter release button is kept).Some Digital Still Cameras can be opened fast when camera apparatus or application are activated
Door, and shutter may remain in the position until camera apparatus or application be deactivated until.When the shutter is opened, camera is set
Standby or application can with capturing scenes indicate and it will be shown on view finder.When image capture is triggered, current scene
The digital pictures of one or more differences can be captured.Note that example embodiment can use with electronic shutter and/or
The camera of mechanical shutter.
Captured digital picture can be represented as one-dimensional, two-dimentional or multidimensional pixel array.It can be by can be to phase
It answers the color of pixel and/or brightness is encoded one or more values indicates each pixel.For example, a possible coding
It uses YCbCr colour model (it is also referred to as YUV colour model).In this colour model, Y color channel can be indicated
The brightness of pixel and Cb (U) and Cr (V) color channel can respectively indicate the chroma blue and red color of pixel.Example
Such as, each of these color channels can take the value (that is, tone range that single octet can provide) of 0-255.Cause
This, if pixel is black or close to black, the brightness of pixel can be indicated by 0 or close to 0 value, and if
Pixel is white or close to white, then the brightness of pixel can be indicated by 255 or close to 255 value.However, 255 value
It is non-limiting reference point, and different maximum values (for example, 1023,4095 etc.) can be used in some embodiments.
But YCbCr colour model is only a possible colour model, and such as R-G-B (RGB) colour model
Other colour models or cyan-magenta-yellow-black (key) (CMYK) can also be used by embodiment here.In addition, image
In pixel can be expressed with various file formats, including original (uncompressed) format or compressed format, such as joint figure
As expert group (JPEG), portable network image (PNG), image exchange format (GIF) etc..
Some pixel coders --- including YCbCr colour model --- indicate the brightness of each pixel using 8.In this way
Do referred to as LDR imaging.As a result, can only support 256 grades of brightness.However, with can be reasonable by LDR imaging
The dynamic range for the brightness that ground indicates is compared, and real-world scene often exhibits the broader dynamic range of brightness.For example, a
The body scene before window of standing in darkroom may include both extreme bright regions and very dark region.However, using it to catch based on adopting
Obtain the exposure of image, the image that captures such scene using LDR imaging may cause details in bright area and/or dark areas
Loss.
D. automatic exposure
The exposure of captured image can the size by aperture, the brightness of the light into aperture and/or shutter period
The combination of length (also referred to as shutter length or exposure length) determines.Extraly, number and/or analog gain can be answered
For image, to influence to expose.In some embodiments, term " total exposure length " or " total exposure time " can refer to shutter
Length is multiplied by the gain for specific light circle size.Here, term " total exposure time " or " TET " should possibly be interpreted
Any other measurement for the amount that shutter length, time for exposure or control are responded from the light signal generated for reaching recording surface.
In some embodiments, " real time for exposure " can refer to the time span of the exposure before any gain of application.
Camera --- or even analogue camera --- may include software to control one or more camera functions and/or set
It sets, aperture size, TET, gain etc..Extraly, some cameras may include while capturing image or in capture figure
The software of image can be digitally processed as after.Although it is understood that above description generally refers to camera, but it can have
It is related to digital camera to body.
Short TET can cause the moderately accurate of the bright area of scene to indicate, but dark areas in some cases
It may be under-exposed.On the contrary, long TET can cause the moderately accurate of dark areas to indicate, but it is bright in some cases
It region may be over-exposed.In exemplary scene described above, if TET is too long, the feature in room may be presented
Feature suitably to expose, but outside window may be rendered as albefaction (whitewash).But if TET is too short, outside window
Feature may be rendered as normally, but the feature in room may be rendered as it is dimmed.Any of these results are undesirable
's.For some scenes, following single TET may be not present: the single TET causes to indicate clear zone using acceptable details
The captured image of details in both domain and dark areas.
Camera apparatus can support automatic exposure (AE) mode, in this mode, before exporting image capture, camera base
TET is determined in the brightness of scene.For example, user can observe the field in the view finder of camera before triggering image capture
Scape.During this period, camera can carry out the initial estimation of suitable TET, using TET capture preview image and then
Assess the pixel in captured image.Most of then, as a possible embodiment, if in preview image (or
Some other enough parts (fraction)) pixel exposure is excessive, then and camera can reduce TET and capture another preview graph
Picture.If most of (or some other enough parts) pixel exposures in preview image are insufficient, camera can increase TET simultaneously
And the another preview image of capture.
For example, if most of pixels in captured image show it is bright more than high threshold level (for example, 240)
Angle value, then camera can reduce TET.On the other hand, if most of pixels show Low threshold grade (for example, 96) below
Brightness degree, then camera can increase TET.
Alternatively or in addition, some or all of target mean pixel in the pixel for scene can be determined
Value.If actual average pixel value more than target average pixel value, can reduce TET；And if actual average picture
Plain value is in target average pixel value hereinafter, can then increase TET.It may also rely in the scene and have that how many contrast are come not
Target average pixel value is tuned together.For example, in low contrast scene, target average pixel value can be it is bright (for example,
200).But in high contrast scene, target average pixel value can be lower (for example, 128).
The processing can continue, until camera determine image should be captured and stored (for example, user activate shutter
Button) until.During handling herein, if the characteristic of scene is relatively constant, the brightness that camera is typically based on scene is converged to
Estimated " best " TET.In some embodiments, image shown on the view finder of camera can be omitted from capture
Preview image in one or more information, or to two or more information in the preview image of capture
It is combined.
In some cases, when determining " average " brightness of scene, camera can unequally treat all pixels.Make
With the technology for being described as " center weighted average ", the pixel close to the middle part of scene may be considered that more important.Therefore, with
The pixel for showing other regions of scene is compared, these pixels can be weighted more.Alternatively, in other positions of image
Pixel in setting can be given more weights.For example, if camera detects in the specific position except the center of image
To face (or some other attention objects), then camera can give higher weight to associated pixel.
In this way, AE algorithm, which can attempt to determine, generates suitably being exposed for a large amount of (for example, maximum quantities)
The TET of pixel.However, in the case where the scope limitation of given LDR imaging, very make in AE mode that captured image may also
Include albefaction or dimmed part.Therefore, as noted above, single " best " TET may be not present for some scenes.
Here, if its brightness value is in pre-defined range, pixel may be considered that " suitably being exposed ".It is right
In 8 brightness values, which may be such as 32-224,16-240,96-240,128-240.If its brightness value falls in this
Except range, then pixel " is inadequately exposed " (that is, pixel or under-exposure or over-exposed).However, it is possible to make
With more or less positions come coded luminance value, and range predetermined can be different from example ranges given above.
AE algorithm can be different from above description.For example, some may be more complicated, be treated differently different colors,
Consider the contrast between the space and/or structure components, and/or measured zone of scene.However it is possible that being retouched here
The embodiment stated can use the currently known or any AE algorithm to be developed in future, or combines currently known or want in the future
Any AE algorithm of exploitation is operated.
High dynamic range (HDR) imaging is proposed as the mode compensated for the defect of LDR imaging.Possible
And then in embodiment, HDR imaging may relate to following cameras: the camera is with the multiple images of various TET capturing scenes,
These captured images are digitally processed to generate the most of or all areas for including scene --- including non-being always on and non-
Normal those of dark region --- in details reasonable representation single image.However, determining that the TET for capturing image may
It is problematic.Specifically, the difficulty for adjusting the TET for concrete scene creates the limitation to HDR imaging.It is described here
Method and embodiment can provide calculate validity, for the picture quality of artifactitious robustness and/or enhancing.
In the following, term " LDR imaging ", which can refer to, is imaged institute's captured image using LDR, and term " LDR scene " can be with
Refer to and is confirmed as that the scene reasonably indicated is imaged using LDR.Similarly, term " HDR image ", which can refer to, is imaged institute using HDR
Captured image, and term " HDR scene " can refer to the scene for being confirmed as reasonably indicating using HDR imaging.In addition, art
Language " LDR imaging " can be interchangeably used with term " LDR image acquisition ", and term " HDR imaging " can be with term
" HDR image acquisition " is interchangeably used.
Fig. 3 depicts flow chart according to example embodiment.With higher rank, flow chart 300 is indicated for digital phase
The imaging pipeline of machine equipment 302.For example, flow chart 300 can indicate multiple steps as performed by digital camera devices 302
Suddenly, to be to be obtained using LDR image or obtained using HDR image for the determination of specific scene, determining adopt uses it to capture
One or more TET of the image of scene, and whether and/or how captured image is combined to as the reasonable of scene
The output image of the satisfied expression in ground.In this way, digital camera devices 302 can adapt dynamically to the illumination mould in scene
Formula, no matter the mode is dark, bright or the two some combinations.Digital camera devices 302 can have and Figure 1A to figure
The same or similar ability of digital camera devices 100 in 1D.
Flow chart 300 can indicate a series of steps as performed by digital camera devices 302 when shutter release button is triggered
Suddenly.Alternatively or in addition, flow chart 300 can indicate showing scene when the view finder of digital camera devices 302
The step of being consecutively carried out when expression.Therefore, in some embodiments, the feature of flow chart 300 can be not show user
The mode seen is performed.For example, it is primary that user can trigger shutter in the case where having the wish of capture single image.So
And digital camera devices 302 can in the middle capture multiple images of each of the first image capture 306 and the second image capture 314,
And provide the combined output image 318 as the one or more images captured during the second image capture 314.
It should be noted that not being that all steps depicted in figure 3 need to be executed by digital camera devices 302.Some steps
Suddenly --- such as, image procossing 308 and combination image 316 --- it can for example be executed by different equipment.For example, first
The expression of the one or more images captured during image capture 306 and the second image capture 314 can be by from capture device
It is transmitted to remote computing device.Then remote computing device can execute image procossing 308 and combination image 316, may be by it
As a result some or all of in are transmitted to capture device.
Extraly, training image database 304 can be included in digital camera devices 302, or alternatively, training
Image data base 304 can be a part by the accessible individual equipment of digital camera devices 302 or system.Some
In embodiment, training image database 304 may include that can be used for assisting in used in the second image capture 314
Payload burst (payload burst) structure training image expression.
In the first image capture 306, " measurement burst scanning (metering burst sweep) " can be used to catch
Obtain first group of image of scene.In measurement burst scanning, each image in group can be captured using different TET.?
In some examples, measurement burst scanning can across TET range (for example, 1 to 300 millisecond, 0.1 to 500 millisecond or it is some its
His range) capture continuous image.Using such TET range, measurement burst scanning can be according to linear, the logarithm of TET
And/or exponential distribution and other possibilities, a series of images captured using the TET for being designed to cover this range.
In some embodiments, the second camera of equipment can execute measurement burst scanning with it is determined that be used for by
The TET of each frame for the video that first camera is recording.Alternatively, second camera can be in the view carried out by first camera
Frequency periodically carries out measurement burst scanning during recording, periodically to adjust the TET of first camera.For example, measurement is prominent
Hair scanning can be executed primary by every ten frame of the second camera for the videograph carried out by first camera.Other examples are also
It is possible.
As other example, Fig. 3 is depicted including respectively using three digitized maps of the different TET scene captured
Picture.Since different TET be used to capture image, so three picture showings go out the brightness degree of multiplicity.In other examples,
More or fewer images can be captured during the first image capture 306.These captured images can be provided for number
The parameter that vision facilities 302 is used in the subsequent image of capturing scenes.
Measurement burst scanning may be used to determine whether the characteristic of scene, allow to select for the second image capture 314
Subsequent payload burst structure.Therefore, in step 308, can handle in step 306 place captured image.Tool
Body, step 308 may include that will be fused to combination image 310 in one or more of step 306 place captured image
In.Step 308 can also include generating histogram 312 from the image of fusion, and then using histogram and possibly make
With in the information in training image data 304 it is some or all of come classified to scene (for example, be classified as LDR scene or
HDR scene), the structure of payload burst is determined based on the classification of scene, and determine to happen suddenly according to payload and capture
TET to be used when image.In some embodiments, captured image shown by the result as the first image capture 306
It can be downsampled before fusion.In addition, histogram can be LDR histogram, HDR histogram, logarithm HDR histogram or one
The histogram of a little other forms.
The example of this processing is shown in Figure 4 A.For simplicity, it is assumed that measurement burst scanning is by image 400 and image
402 two image constructions.Specifically, in measurement scanning burst, any position from one to eight or more image
Setting can be captured.Image 400 is captured using 10 milliseconds of TET, and 20 milliseconds of TET is used to capture image 402.Therefore, scheme
As the brightness of 402 pixel may be expected to the brightness of the pixel of image 400 approximately twice as.In some cases, image
Pixel in each of 400 and image 402 can be by tone mapping.Therefore, can make at tone mapping for these images
Reason is reversed.After keeping tone mapping processing reversed, the brightness of the pixel of image 402 can be the brightness of the pixel of image 400
Approximately twice as.Tone mapping and reversed tone mapping is discussed more fully below.Furthermore, it is noted that when image burst scanning
It is prominent in image when executing one or more exposure settings to determine the first camera for recording video by second camera
The range of TET used in hair scanning can additionally may be used with if same camera executes scanning and captures image data
The range of the TET of energy is compared to be changed with bigger amount.
Image 400 and image 402 can be downsampled so that image 404 and image 406 is respectively formed.For example, can lead to
It crosses and given image is divided into i × j block of pixels (i and j can take identical value or different values) and passes through single pixel
Implement down-sampling to replace each of these blocks.The value of this displacement pixel can be based on the picture in corresponding i × j block of pixels
The value of element.For example, as a result the value of displacement pixel can lead to " more mould by taking the average of value of the pixel in block to determine
Paste ", more low resolution and smaller down-sampled images.Therefore, as a possible example, if 1600 × 1200 pixels
Image is divided into 2 × 2 block of pixels and is downsampled a grade, then the result is that 800 × 600 pixel images.If 1600
× 1200 pixel images are downsampled two grades (or if 800 × 600 pixel images are downsampled a grade again), tie
Fruit is 400 × 300 pixel images, etc..But segment (tile) can be otherwise downsampled.For example, 4 × 4,8 ×
8 block of pixels or 16 × 16 block of pixels can be replaced by single pixel, and can be executed more than only one grade or two grades
Down-sampling.
In some embodiments, the down-sampling of multiple grades can be executed for each image, therefore creates " pyramid shape
(pyramid) " down-sampling.Distribution by using the image of the down-sampling with multiple grades, with the light grade in image
Related information can be saved with both space structures of these light grades.
Thus, for example, Fig. 4 A, which is depicted, to be had in the block of pixels 400A in its upper left corner and in the block of pixels in its upper right corner
The image 400 of 400B.Extraly, image 402 has in the block of pixels 402A in its upper left corner and in the block of pixels in its upper right corner
402B.Each block of pixels in each image can be down sampled to the single corresponding picture in image 404 and image 406
Element --- pixel 404A indicates that the down-sampling of block of pixels 400A, pixel 404B indicate the down-sampling of block of pixels 400B, pixel 406A
Indicate the down-sampling of block of pixels 402A and the down-sampling of pixel 406B expression block of pixels 402B.
In some cases, both i × j block of pixels and its associated down-sampling pixel can be referred to as " data block
(paxel)".Therefore, both block of pixels 402A and pixel 404A can be referred to as paxel.
For each position in the image of down-sampling (for example, pixel 404A and 406A will be considered in identical position
In), it can choose the pixel with the peak less than 255.In some embodiments, the picture of each color channel in pixel
Plain value can be compared with 255.If all in these pixel values are lower than 255, pixel is the candidate for selection.All
Candidate pixel among, can choose a pixel with maximum pixel value.In other embodiments, it alternatively can be used
The threshold value (for example, 250,245 or value higher than 255) different from 255.
Fig. 4 A shows the example of this processing.In the image 404 of down-sampling, pixel 404A can respectively have 250,
100 and 150 red, green and blue (R, G, B) value.In the image 406 of down-sampling, pixel 406A can respectively have
255,200 and 255 (R, G, B) value.Because (R, G, B) value of pixel 404A is below 255, but pixel 406A (R, G,
B) some in value are 255, so selection pixel 404A.Similarly, pixel 404B can be respectively with 100,50 and 50
(R, G, B) value, and pixel 406B respectively has 200,100 and 100 (R, G, B) value.Because of (R, G, B) value of pixel 406B
Respectively less than 255, but it is greater than (R, G, B) value of pixel 404B, so selection pixel 406B.Other kinds of ratio can also be used
Compared with, or can replace processing shown in Fig. 4 A and come using other kinds of comparison.Alternatively, in YCbCr color space
Brightness (Y) value of pixel can be used to carry out the test relative to threshold value.
Each selected pixel can be placed in the corresponding position of combination image 408.Thus, for example, in group
It closes in image 408, pixel 404A can be placed as pixel 408A, and pixel 406B can be placed as pixel 408B.Additionally
Ground can be reversed tone mapping for the combination selected pixel of image 408.
Tone mapping be include the set that the process of pixel value is mapped according to predetermined function.Thus, for example, one
By pixel value, from linear space, (wherein, increasing or decreasing for k unit indicates proportional in brightness to a little camera apparatus in pixel value
Increase or decrease) it is mapped to non-linear space.Tone mapping can be automatically carried out for the purpose of artistry, in such as blast
Between range pixel value.Anyway, in order to the reversed tone mapping of selected pixel is returned to linear space, inverse tone mapping (ITM) letter
Number can be applied to corresponding pixel value.
It extraly, can be by pixel value divided by the corresponding TET for capturing pixel using it.Do so can will use it is various
The pixel value that TET is captured is normalized to specific range.Therefore, for pixel 408A, (R, G, B) value respectively be can be
25,10,15, and for pixel 408B, (R, G, B) value can be 10,5,5.On the other hand, for the TET (example lower than 1 millisecond
Such as, 0.25 millisecond, 0.5 millisecond etc. of TET), the pixel value in combination image 408 can be increased divided by TET.In some cases
In, this can cause pixel value to be greater than 255, and the HDR that therefore combination image 408 can be scene is indicated.
In addition, histogram 410 can be created from combination image 408.Although in the presence of many modes that can form histogram,
But some embodiments may include assessing the function of the color channel of each pixel.For example, the function can be most
Bigization or some other functions.
In addition, the logarithm of the output of this function can also be used, and the value generated is plotted on histogram 410.
The distribution of light grade is logarithm in real world.Therefore, by using logarithmic scale, coarse there are the range uniformly covers
Lid.For the histogram based on linear light grade, more histogram vertical bars (bin) can be used.In addition, in some implementations
In example, before the output is placed in histogram 410, weight can be applied to the logarithm of the output of function.In for example,
Heart the weighted average technology can be used for for the higher weight of pixel application closer to the center of captured image, and for
The pixel at the center far from image applies lower weight.
Anyway, histogram 410 can indicate the distribution of the pixel value in combination image 408.The vertical axis of histogram
It can indicate the quantity of the pixel of each pixel value, and trunnion axis can indicate the range of pixel value.Pixel value can 0 to
In the range of 255, or some other ranges can be used.For example, HDR histogram may include the pixel value 255 or more.?
In some embodiments, HDR histogram can indicate 15 pixel values, that is, from 0 to 32,767.Therefore, it is presented on HDR histogram
Logarithm in figure can be in the range of 0 to log (32,767)=4.52.
As an example, most of pixel is plotted in the least significant end of its trunnion axis by histogram 410.This instruction drawing image
The major part of 408 histogram 410 is the shade of black or white.However, because data point is also plotted in water by histogram 410
The middle part of flat axis, so combination image also may include the pixel with intermediate range brightness.
A part as image procossing 308 again, histogram 410 can be with the images in training image database 304
One or more histograms are compared.As depicted in fig. 4b, training image database 304 may include reference scene
The set of histogram and/or associated parameter.As described above, the histogram of reference scene can be HDR histogram.
Fig. 4 B shows training image database 304, which includes: being used for histogram 420, (one
It is a or multiple) entry of target pixel value 422, the target percentage 424 for the pixel cut out and scene type 426.Histogram
420 can by with above described in the context of Fig. 4 A it is the same or similar in a manner of, lead from one or more images
Out.Therefore, it is captured that various TET can be used in one or more images of reference scene, and these images can be by under
It samples and is incorporated into single image, histogram 420 can be exported from the single image.In general, being derived from histogram
420 reference scene do not need for the identical scene that is derived from histogram 410.
In some embodiments, down-sampling, combination single image in be tailored pixel (for example, have 255 or
The pixel of higher pixel value) percentage can be stored in the target percentage 424 for the pixel cut out.When being used for catching
Obtain image TET it is too high or too low when, or when in image capture process scene it is some very dark or non-be always on it is thin
When section is lost, possibility is cut out.In some cases, the pixel being only tailored at one end of range can be included in and cut
In the percentage of the pixel of sanction.For example, the percentage for the pixel cut out can only include the picture with 255 or higher pixel value
Element, and the picture that do not cut out can be calculated in every other pixel (for example, all pixels with 0 to 254 pixel value)
The average pixel value of element.
In addition, the instruction that scene is LDR or HDR can be stored in scene type 426.In some embodiments
In, when scene type is LDR, scene type 426 can be taken as zero value；And when scene type is HDR, scene type
426 can be taken as one value.Alternatively, scene type 426 can be in the range of such as from 0 to 1.In this case,
Value less than threshold value (for example, 0.5) can indicate LDR scene type, and any number for being greater than or equal to threshold value can refer to
Show HDR scene type.
(one or more) target pixel value 422 can be intended to be for associated scene desired one or
More pixel values.If scene is LDR, (one or more) target pixel value 422 may include single pixel value, or
The range of (one or more) pixel value.This single pixel value (or range) may be selected so that have and target pixel value
The image of the scene of the average pixel value to match reasonably will well be exposed.Therefore, (one or more) target pixel value
422 can be target average pixel value.Extraly, (one or more) target pixel value 422 can be LDR value (for example, from 0
To 255).
In addition, executing the method for such as method 4A in second camera to adjust the video for being recorded by first camera
In the embodiment of TET, (one or more) target pixel value 422 may include the single pixel for being used by first camera
Value.Specifically, the method that second camera can repeat such as method described herein, to determine for passing through first camera
The single pixel value of each frame of the video captured.Alternatively, second camera can be used to periodically determine use
In the single pixel value (for example, every four frame is primary) used by the first camera for capturing video.
If scene is HDR, (one or more) target pixel value 422 may include short exposure target pixel value (or
Range), long exposure target pixel value (or range) and possible rollbacks (fallback) expose target pixel value (or range).
These pixel value or ranges may be selected so that with the short exposure target pixel value for short exposure and for long exposure
The HDR image of long exposure target pixel value reasonably can well be exposed.If HDR image failure as following (for example, beg for
Opinion) and single TET be used to capture HDR scene, then rollback target pixel value can be used.
It in some cases, can be by checking several synthesis exposure of capture image and selecting to be rendered as eyes
(one or more) pixel value and/or (one or more) range of the most comfortable, manually to determine (one or more) target
The target percentage 424 and scene type 426 of pixel value 422, the pixel cut out.Alternatively, it is possible to algorithm or automatically
Ground determines the target percentage 424 and scene type 426 of (one or more) target pixel value 422, the pixel cut out.
In addition, executing the method for such as method 4A in second camera to adjust the video for being recorded by first camera
TET embodiment in, (one or more) target pixel value 422 may include the short exposure mesh for being used by first camera
It marks pixel value (or range).Specifically, the consecutive frame of video can be used to repeat such method, to determine in second camera
The short exposure target pixel value (or range) of each frame of video for being captured by first camera.This can help to provide
Noise reduction and HDR in video.Alternatively, second camera can be used to simply determine for catching by first camera
The single exposure TET (for example, single pixel value) of each frame of the video obtained.
Training image database 304 can also include about histogram 430, (one or more) target pixel value 432, cut
The target percentage 434 and scene type 436 of the pixel of sanction and about histogram 440, (one or more) target pixel value
442, the target percentage 444 for the pixel cut out and the similar entry of scene type 446.In some embodiments, Ke Yicong
One or more parameters of the target percentage for the pixel such as cut out are omitted in training image database 304.Extraly,
He can be included in training image database 304 parameter.Training image database can store as little as one or tens,
Several hundred or thousands of such entries, each entry may be related from different scenes.
As above discussed in the context of the first image capture 306 period institute's captured image, scheme for training
As each image in database 304 can execute the down-sampling of multiple grades, therefore " pyramid shape " of each image of creation
Downsampled version.By using the image of the down-sampling with multiple grades, distribution about the light grade in image and this
The information of both space structures of a little light grades can be saved.It can be with for histogram in each of these down-sampled images
It is included in training image data 304.
Information in training image database 304 can be before calibration, test and/or other shipments before commercial operation
It is stored in wherein during assessment and other possibilities.Alternatively, be also stored in being capable of management training figure for information
As database 304 various other equipment and system on.Anyway, information can be substantially essentially static, although
Information can be modified by firmware and/or software upgrading or other installations.
By by histogram 410 (and/or the downsampled version based on image 400 and 402 similarly derived histogram) with
Histogram in training image data 304 is compared, and the dynamic range of the scene showed in image 400 and 402 can be with
It is estimated.The dynamic range of this estimation may be used to determine whether, obtain or additionally select having for the second image capture 314
Imitate load burst structure.
For example, each histogram 410 can be compared with each histogram stored in training image data 304
Compared with.To that can be compared in various ways, any one of the various modes can be used histogram.In some embodiments
In, the land mobile distance (earth mover ' s distance, EMD) between histogram pair can be calculated.When histogram one
When cause, EMD 0, and EMD increases with the difference between histogram.Therefore, lower EMD indicate two histograms it
Between good matching, and higher EMD indicate two histograms between inferior matching.
Weight can be exported from the EMD for specific histogram pair.For example, weight can with EMD value associated there at
Inverse proportion.In some cases, weight w can be derived as:
Wherein, EMD is EMD value and n can be in 1 to 10 range.However, it is possible to use the other values of n.Therefore,
In some embodiments, weight can take the value between 0 and 1.In the case where EMD is 0, appropriate big weight can choose
(for example, 1).Alternatively, very small value (for example, 0.001) can be added to denominator to avoid divided by 0.Anyway,
Good matching between two histograms can lead to high weight, and the inferior matching between two histograms can be led
Cause low weight.
For each histogram to (wherein, the first histogram is histogram 410, and the second histogram comes from training image
Entry in database 304), weight w can be applied to the entry phase with the second histogram in training image database 304
Associated corresponding scene type.It as a result can be in histogram to being above averaged, to determine " HRD " of scene.For example, if
" HDR " generated is in 0 to 1 scale at 0.5 or on 0.5, then scene can be specified for HDR processing；But
It is if generating " HDR " in identical scale under 0.5, scene can be specified for LDR processing.
It should be appreciated that embodiment described above is only the brightness of determining scene and a possible mode of " HDR ".It can
Some alternatively to use other technologies, and in these other technologies can be based on will be during the first image capture 306
The parameter (for example, average value of the percentage for the pixel cut out and the pixel not being tailored) of institute's captured image is schemed in training
As the same or similar parameter of image represented in database 304 is compared.Other technology may include by first
The corresponding down-sampled images pyramid of histogram and the second histogram is compared.
In order to determine for the TET of payload burst, following instantiation procedure can be used.However, it is possible to alternatively
Use other processes.
For each histogram to (again, wherein the first histogram is histogram 410, and the second histogram is from instruction
Practice the entry in image data base 304), weight w can be applied to and the second histogram in training image database 304
The associated corresponding target pixel value of entry.As a result it can be averaged to determine for by the scene represented by histogram 410
Target average pixel value.If scene type is HDR, two or more target average pixel values can be determined.
For each target average pixel value, specific TET value can be used to search for by being spaced equal part, so that if to adopt
With specific TET value capturing scenes, then the image generated will have target average pixel value or about target average pixel value.With
Following pseudocode is shown as in a possible method for determining specific TET value based on target average pixel value.
Table 1
At the row 1 and row 2 of table 1, defines initial low TET value and high TET value (is respectively lo_tet and hi_
tet).These values can be expected to be chosen at or near the extreme value end for falling into range therein in final TET value (mid_test)
It selects.In some embodiments, broader range or narrower range can be used.
Row 3 is depicted to row 11 can be iterated t circulation.The value of t may be selected so that mid_tet is being recycled
T iteration after restrain.In some embodiments, t can down to 2 or 3, but in other embodiments, t can be 5,
10,20,50,100 or some other values.But be expert at 4, mid_tet be arranged to lo_tet and hi_tet it is average (in
Point).
It is expert at 5, determines the pixel value of the image at mid_tet.A possible mode for carrying out this determination is to close
HDR image is exposed at ground, seemingly image is captured such using the TET of mid_tet.(it may be HDR figure to the image of generation
Picture) can be by tone mapping, and (pixel_value_at_mid_tet, can for the average pixel value of the image of tone mapping
Can be LDR value) it can be determined.
Synthesis exposure is the mode that LDR image is obtained from HDR image.It is assumed that capturing HDR image using the TET for being T.
By the way that by the pixel value of each pixel in HDR image, multiplied by p (p can be more than or less than 1), which can be synthesized
Ground exposure is the TET that p multiplies T.In the image that result obtains, all pixel values " being cut out " 255 or more are 255.At this
Reason simulates the appearance of scene, and seemingly it is used LDR imaging, is captured like that using the TET for multiplying T for p.Alternatively, HDR schemes
The non-logarithmic HDR histogram (applying or do not apply being averaged for center weighting) of picture can be used.After this step, Ke Yifang
It will be applied to that the whatsoever processing (such as tone mapping) of linear image, usually very to generate the LDR image of synthesis.It should
In image average value (if it is desire to then apply center weighting be averaged) can be taken and can by with target pixel value
Compare.
6 are expert at row 11, if the average pixel value that the result obtains is greater than target average pixel value, mid_tet
It is excessively high, and hi_tet is arranged to mid_tet to reduce the mid_tet in next iteration.On the other hand, if generated
Average pixel value be less than or equal to target average pixel value, then mid_tet is too low, and lo_tet is arranged to mid_tet
To improve the mid_tet in next iteration.
It can be for each TET value weight that can be used in payload burst structure by processing shown in table 1
It is multiple.Therefore, if scene is determined as HDR scene, the processing of table 1 can be executed for a TET.However, if
Scene is determined as HDR scene, then the processing of table 1 can for two or more TET (for example, short TET, long TET and/
Or rollback TET) and be performed, all three TET can have different targets and be averaged LDR pixel value.
In step S314, second group of image can be captured.The quantity of captured image and it be used to capture these images
TET arrangement can be referred to as " payload burst ".For example, the second image capture 314 includes the three of scene in Fig. 3
A image, each image are captured using the TET identified in step 308.It should be appreciated that identified in step 308
TET can be identical or different with the TET that be used to capture image within step 306.Extraly, it is possible to use identical or phase
As TET capture the second image capture 314 in all three images.
In step 316, the image from second group of image can be combined.Combination image may include by two or more
Multiple images alignment.In some instances, image can be globally aligned (that is, with the part for being directed at image on the contrary, alignment is whole
Both a image), be locally directed at (that is, part of alignment image), or possibly globally and be locally aligned.This
Outside, combining two or more images can also include being merged to form output image 318.The fusion can be according to present
Known or exploitation in the future image fusion technology is performed.
Compared with any one of individual images in second group, merge the image in second group of image can cause it is defeated
Image 318 is sharper keen out and/or is preferably exposed.For example, if capturing the second image capture using the same or similar TET
Some in image in 314, then these images can be fused to reduce the noise in one or more parts of image.
Alternatively or in addition, if capturing the image in the second image capture 314 using two or more different TET,
At least some images with the different time for exposure can be fused according to HDR process.Anyway, output image can be by
It may be stored on the computer-readable medium and/or be displayed on the polynary display 106 of such as Figure 1A.
It in some embodiments, can the TET based on determined by step 308 and the combination image to step 316
Understanding determine the arrangements of various possible payload burst structures.Although many arrangements of payload burst structure can
It can be possible, but depict three examples here.
Scene type | Payload burst structure |
LDR | T T T T |
HDR | L S L L S L L S L L |
HDR (has and retracts) | L S L L S L L F FF |
Table 2
Table 2 shows three examples.In the first example, scene type LDR.In this example, payload happens suddenly
Structure includes four images sequentially captured, and can be referred to as " LDR burst structure ".The payload of table 2, which happens suddenly, to be tied
Each of structure column " T " can indicate capture image.Identified phase in step 308 can be used in each of these images
Same or similar TET is captured.In some embodiments, less or more image can LDR payload burst in quilt
Capture.For example, may include the image of as little as one or up to ten or more.
Regardless of the quantity of captured image, some in these images can be aligned in step 316 and by
Combination.For example, most sharp keen one can be chosen in these images if m image is captured in payload burst
It selects as " primary picture ", and remaining m-1 image is considered " assistant images ".In some embodiments,
The sharpness of image can by the boundary between tones different in the resolution ratio and/or image of image and/or color area come
Measurement.Alternatively or in addition, the measurement of other sharpness can be used.
In addition, then zero or more in m-1 assistant images can be aligned and be melted with most sharp keen image
It closes.For example, can respectively attempt to be aligned between each of assistant images and most sharp image.If for corresponding auxiliary
A part alignment failure in image is helped, then these parts can be dropped, and not combined with primary picture.With this side
Formula can use some or all of information in assistant images to most sharp keen image noise reduction.
In the second example, scene type HDR.In this example, payload burst structure include according to long TET and
Ten images of the mode capture of short TET, and " HDR burst structure " can be referred to as.In the payload burst structure of table 2
In column, each " L " can indicate that, using long TET institute captured image, each " S " can indicate the figure captured using short TET
Picture.Therefore, the mode of " L S L L S L L S L L " can indicate: the first image of payload burst is using long TET
It being captured, the second image is captured using short TET, and third image and the 4th image are captured using long TET, the
Five images are captured using short TET, and the 6th image and the 7th image are captured using long TET, and the 8th image is to make
Captured with short TET, the 9th image be captured using long TET and the tenth image be to be captured using long TET.
Long TET and short TET can be determined based on the result of image procossing 308.It is thereby possible to select long TET and short TET
So that HDR process can be used using the image that the result that these TET are captured obtains to combine.Long TET can be used for
Details in the dark-part of capturing scenes, and short TET can be used for the details in the bright part of capturing scenes.
The example of short TET value may include the TET for 1 millisecond, 2 milliseconds and/or 8 milliseconds, and the example of long TET value can be with
Including the TET for 20 milliseconds, 40 milliseconds and/or 80 milliseconds.However, short TET and long TET can take different values.
Although the payload burst in the second example has the specific structure in table 1, other structures can be used.
For example, the payload burst structure of " L S LS L S L S L S " or " L L S L L S L L S L " may be potentially
The suitable mode of long TET and short TET are provided.In addition, some payload burst structures may include medium TET (by " M "
To indicate).Therefore, additional example payload burst may include " S M L L L S M L L L " or " S M L S M
L S M L L " structure.
In some embodiments, payload burst structure may include image more more or fewer than ten.It is general and
Speech, determines that the length of payload burst structure is related to compromising.On the one hand, the burst of long payload is (that is, have larger amt
Image capture payload burst) be desired, this is because increase institute's captured image in one or more quilts
Good a possibility that exposing and is sharp keen.On the other hand, too long, the possibility of ghost image (ghosting) if payload happens suddenly
Property also increases due to the movement of scene.Extraly, darker scene can make from using longer TET institute's captured image to be benefited
Recording surface can be reached by obtaining more light.Therefore, payload burst structure can may be based in part on these considerations.
In third example, scene type is also HDR.However, in this example, associated payload burst structure
(also referred to as HDR burst structure) includes seven images captured according to the mode of long TET and short TET, is returned later for three
Move back TET.Each " F " can be indicated using rollback TET institute captured image, and the TET that retracts can take and long TET and short TET
The different value of the two.
Regardless of the type of payload burst structure, the image of HDR scene can be aligned and combine.For side
Just, " short image " can be referred to as using short TET institute captured image, and for convenience, uses long TET institute captured image
" long image " can be referred to as.
In some embodiments, most sharp keen short image can be selected as main short image from short image.It is remaining
Then zero or more for assisting in short image can be aligned and merge with main short image.For example, can respectively exist
It assists attempting to be aligned between each of short image and main short image.If for assisting a part in short image accordingly
Alignment failure, then these parts can be dropped, and not combined with main short image.In this way it is possible to using coming from
Assist some information in short image to most sharp keen short image noise reduction.
The same or similar processing can be executed for long image.For example, most sharp keen length can be selected from long image
Image is as main long image.Then zero or more in the remaining long image of auxiliary can be aligned simultaneously with main long image
And it merges.It respectively can assist attempting to be aligned between each of long image and main long image.If for corresponding
A part alignment failure in long image is assisted, then these parts can be dropped, and not combined with main long image.
As a result the short image of the combination obtained is (for example, it may be possible to the information by assisting short image from zero or more
And the most sharp keen short image of noise reduction) and the obtained combination of result long image (for example, it may be possible to by from zero or more
Assist the most sharp keen long image of the information of long image and noise reduction) and then can be aligned.If be aligned successfully, the two figures
As (for example, being LDR image) can be combined according to HDR process.For example, they can be combined into HDR image, and
HDR image then can be by tone mapping, so that its brightness is fallen into and the comparable model of the display capabilities of convention video output equipment
In enclosing (for example, in 0 and 255 --- including 0 and 255 --- between pixel value).As a result the HDR image of the tone mapping obtained
It can be designated as output image 318.In some embodiments, if the signal-to-noise ratio of a part in all output images 318
It still is below threshold value, then noise reduction process can be by application to further decrease noise.Extraly, may application noise reduction process it
Afterwards, can also make to export the sharpening of image 318.In general, various types of HRD blending algorithms --- such as, exposure fusion or part
Laplace filter --- it can be used to merge short image and long image.If medium TET is used for payload burst
Structure, then these HDR blending algorithms can also be applied to one or more medium images.
If the alignment between the short image and the long image combined of combination fails, HDR processing failure.However, if
Rollback image is captured, then one or more of the image that retracts can be used to form output image 318.For example, can select
Select most sharp keen rollback image.Zero or more in remaining auxiliary rollback image can be with most sharp keen rollback image pair
It is quasi- and combine, by with above in relation to processing described in short image and long image it is similar in a manner of be performed.For not having
There is the payload burst structure in the rollback image for being wherein directed at failure, the long image or short image of combination can be used for shape
At output image 318.
Exemplary payload burst structure is further illustrated in Fig. 5.Payload burst structure 500 can indicate
Payload for LDR scene happens suddenly, and payload burst structure 502 can indicate that the payload for HDR scene is prominent
Hair and payload burst structure 504 can indicate that the payload for the HDR scene that retracts happens suddenly.Each burst structure
Show the similar duration of the TET for each image capture.For example, for burst structure 502 and 504, short TET and length
TET, which is compared, has the less duration, and the duration for the TET that retracts is between long TET and the duration of short TET.
In Fig. 5, vertical dotted line depicts the time phase (epoch in time) that can start image capture.It is some
Electronic image sensor is operated with specific frequency, and such as 30 hertz.It can capture 30 per second with the sensor of this frequency operation
A image, or one image of approximate every 33.33 milliseconds of captures.Therefore, the duration between each stage in Fig. 5 can be
33.33 milliseconds.But for low light scene, the time for exposure can be longer than 33.33 milliseconds, and the operation frequency of imaging sensor
Rate can be adjusted correspondingly downwards.In this example, it can be realized by the gain component of increase TET and be longer than 30 milliseconds
TET。
For some sensors, image capture can only be activated at the end in such stage.Therefore, as in Fig. 5
Shown, each image capture terminates in the edge in corresponding stage.However, due to the TET that they change, some image captures
It can start at place in different times.Alternatively, for some imaging sensors, image capture can be only in the beginning in stage
Place is activated.
It should be appreciated that various other technologies, process and/or algorithm may be used to determine whether payload burst structure.Cause
This, it is described above to provide only some possible payload burst structures together with above table 2 and Fig. 5.Other technologies can be with
It is used without departing from the range of embodiment here.
For example, in some embodiments, can length based on TET, use RMS method or SMR method to calculate tone
The average pixel value of the image of mapping is (for example, the step 5) in table 1.The RMS of n value can be averagely calculated as:
The SMR of n value can be averagely calculated as:
For shorter TET --- such as short TET and/or rollback TET of HDR payload burst, or possibly LDR
The TET of burst --- it may be desirable that attempting to increase or maximize the brightness of pixel without cutting out them.Therefore, pixel value is taken
RMS more weights are averagely applied to brighter pixel.For longer TET --- for example, the burst of HDR payload
Long TET --- it may be desirable that emphasizing the darker part of scene.Therefore, the SMR of pixel value is taken averagely to apply more weights
It is added to darker pixel.
Another variation is shown in the context of the burst structure 502 of Fig. 5.The variation is related to selecting most sharp keen short figure
Picture.For discussion purposes, it is assumed that most sharp keen long image is long image 508.Then, instead of making in the most sharp keen short image of selection
For main short image, in time closest to most sharp keen long image and captured short image can be selected as it is main short
Image.For example, this may be short image 510, before long image 508.Then, the short image of remaining auxiliary can be with
Main short image alignment and/or combination (if alignment allows).Alternatively, if imaging sensor is caught at the beginning in stage
Image is obtained, then the short image (being likely to immediately in later) after most sharp keen long image can be selected as main short figure
Picture.As alternative, adjacent short image/long image is to can be selected so that these images maximize combination together on the time
Sharpness measurement.
In some embodiments, payload burst structure can be at the beginning of acquisition phase based on imaging sensor
Still terminating place uses the time for exposure for being less than the readout time of pixel of imaging sensor (when referred herein as son reads exposure
Between) capture image.If imaging sensor captures son at the end in image capture stage and reads the time for exposure, effectively carry
Lotus burst structure may include following one or more double TET subsequences of long TET behind short TET closely.If imaging sensor
At the beginning in image capture stage, capture reads the time for exposure, then payload burst structure may include behind long TET
Follow one or more double TET subsequences of short TET closely.
A possible advantage for selecting main long image in this way is to reduce motion blur or " ghost image " to influence.Example
Such as, if scene includes movement, fusion may cause movement in fused image from the multiple images that scene is captured
It is rendered as obscuring.In general, when the time difference between capture image the big then, this is fuzzy bigger.It is connect each other in time by selection
Close main long image and main short image, it is possible to reduce ghost image.
It should be noted that various types of cameras and/or calculating equipment can be passed through by step shown by flow chart 300
It executes, such as by digital camera devices 302 and/or calculates illustrated by equipment 200 these.Furthermore, it is possible to likely
The various aspects of some independent steps are distributed between multiple cameras and/or calculating equipment.For example, the first image capture 306 and
Two image captures 314 can occur in digital camera devices 302.In addition, image procossing 308 and combination image 318 can be
Occur in different calculating equipment.It is also likely to be present other combinations for being distributed independent step.
Fig. 6 depicts flow chart according to example embodiment.It, can be with possibly by imaging sensor at step 600
First multiple images of capturing scenes.Each image in first multiple images can use different total exposure times (TET)
It is captured.
At step 602, the first multiple images are at least based on, can determine the TET sequence of the image for capturing scenes.
Determine that TET sequence can be related at least one image in the first multiple images based on scene to determine scene histogram.?
Scape histogram can be based on the down-sampling and combination that the image in the first multiple images to scene carries out.
In some embodiments, additional histogram can be stored in training image database.These histograms can
With at least two images of the capture based on corresponding scene.Histogram can be associated with corresponding dynamic range parameters,
In, corresponding dynamic range parameters indicate that corresponding scene is LDR or HDR.Determine that TET sequence can be further to by field
Scape histogram is compared at least one histogram in training image database, and based on comparison result come from corresponding
Dynamic range parameters in determine be used for scene dynamic range parameters, wherein TET sequence is further based on for scene
Dynamic range parameters.
If the dynamic range parameters instruction scene for scene is LDR, it is determined that TET sequence can be related to selection will be
Single TET value used in TET sequence.If the dynamic range parameters instruction scene for scene is HDR, it is determined that TET sequence
Column can be related to selection will be in short TET value used in TET sequence and long TET value.If scene is HDR, it is determined that TET sequence
Can also relate to selection will retract used in the TET sequence TET.
In some embodiments, the histogram in tranining database can also be associated with corresponding target pixel value.?
In these embodiments, determine TET sequence can further to based on the corresponding target pixel value in tranining database come
It determines one or more target pixel values for being used for scene, is selected based on one or more identified target pixel values
It will be in one or more TET values used in TET sequence.
At step 604, the second multiple images of imaging sensor capturing scenes can be passed through.In second multiple images
It is captured that TET sequence can be used in image.At step 606, the defeated of scene can be at least constructed based on the second multiple images
Image out.
It determines that TET sequence can be related to determining that scene is LDR scene, and defines for the public of the TET in TET sequence
Value.The output image of building scene can be related to being directed at and being applied in combination one in the second multiple images that common value is captured
Or more image.
Alternatively or in addition, it determines that TET sequence can be related to determining that scene is HDR scene, and defines short TET value
With long TET value.The output image of building scene can be related to being aligned and combine: (i) uses more than second that short TET value is captured
It is one or more in the second multiple images that one or more images in image use long TET value to be captured with (ii)
Image.
Alternatively or in addition, it determines that TET sequence can be related to determining that scene is HDR scene, and defines short TET
Value, long TET value and rollback TET value.The output image of building scene can be related to attempting alignment: (i) is captured using short TET value
The second multiple images in one or more images and (ii) second multiple images for being captured using long TET value in one
A or more image.Construct the output image of scene can be captured further to determination (i) using short TET value the
One or more images in two multiple images and (ii) are captured using long TET value in the second multiple images one or
The alignment of more images has failed.The output image of building scene can also relate to: is aligned unsuccessfully in response to determination to be aligned
And one or more images in the second multiple images captured using rollback TET value are combined, to form output image.
Fig. 7 depicts another flow chart according to example embodiment.At step 700, imaging sensor can be passed through
First multiple images of capturing scenes.Different total exposure times (TET) can be used in each image in first multiple images
It is captured.At step 702, long TET, short TET can be at least determined based on the first multiple images and including long TET and short
The TET sequence of TET.
At step 704, the second multiple images of imaging sensor capturing scenes can be passed through, wherein use and TET sequence
Capture the image in the second multiple images in image sequence with arranging corresponding TET sequence order.Image sequence may include
The first long TET image for being captured using long TET, uses long TET institute at the short TET image captured later using short TET later
Three image sub-sequences of the second long TET image of capture.Alternatively, image sequence may include being captured using long TET
The first long TET image, captured later using long TET the second long TET image, later captured using short TET it is short
Three image sub-sequences of TET image.Can based on camera apparatus characteristic (such as, if using Rolling shutter and/or other
Imaging sensor characteristic) and/or the characteristic (for example, average pixel value or some other measurements) of scene come in selected subsequences
Mode.Anyway, subsequence can be repeated once or more time by payload burst structure.
At step 706, based on one or more images in image sequence, output image can be constructed.Some
In embodiment, image sequence may include the dual image subsequence followed by main long TET image of main short TET image.Figure
As sequence can also include the short TET image of one or more auxiliary and one or more long TET images of auxiliary.It can be used
Short TET captures main short TET image and assists short TET image, and long TET can be used to capture main long TET image
With the long TET image of auxiliary.Building output image can be related to: form the short TET image of combination, wherein the short TET image of combination
Including main short TET image and one or more at least part assisted in short TET image；The long TET image of combination is formed,
Wherein, the long TET image of combination includes at least one in main long TET image and one or more long TET images of auxiliary
Point；And form output image, wherein output image includes at least part and combined length in the short TET image of combination
At least part in TET image.
Alternatively or in addition, image sequence may include the long TET image captured using long TET.Building output
Image can be related to: determine that long TET image is using most sharp keen in all images in long TET institute captured image sequence
Image.It is using the most sharp keen image in all images in long TET institute captured image sequence, choosing based on long TET image
Long TET image is selected as main long TET image, and select in image sequence it is adjacent with mainly long TET image, using short
TET institute's captured image is as main short TET image.Building output image can be further to: the short TET for forming combination schemes
Picture, wherein the short TET image of combination include main short TET image and from use in short TET institute captured image sequence one
At least part in a or more image；Form the long TET image of combination, wherein the long TET image of combination includes main
Long TET image and at least part in one or more images for using long TET institute captured image sequence；And
Form output image, wherein output image includes at least part of the short TET image of combination and the long TET image of combination
At least partially.
In some instances, in image sequence, main short TET image can be before main long TET image.Base
In terminating image capture in the edge in corresponding image capture stage, in image sequence immediately in main long TET image it
Preceding short TET image can be selected as main short TET image.
In other examples, in image sequence, main short TET image can be after main long TET image.Base
In starting image capture in the edge in corresponding image capture stage, in image sequence immediately in main long TET image it
Short TET image afterwards can be selected as main short TET image.
Discribed step can be executed by following in figure 6 and figure 7: such as camera of digital camera devices 100
Equipment, the calculating equipment and/or two or more equipment mutually distinguished for such as calculating equipment 200.For example, in some embodiments
In, image capture step can be executed by imaging sensor, and remaining step can be by individually calculating equipment
To execute.Other arrangements are possible.In addition, Fig. 6 and/or flow chart depicted in figure 7 can according to this specification and/
Or variation example disclosed in attached drawing and modify.
When used herein, term " sequence " can refer to one or more TET sequentially arranged and/or sequentially
One or more images of capture.However, in some embodiments, camera apparatus may include multiple images sensor (for example,
Image sensor array), and these imaging sensors can be captured sequentially, in parallel or using some combinations of the two
Image.For convenience, term " sequence " or " sequence " can also be used to refer at least some figures for concurrently capturing " sequence "
Picture.
Fig. 8 A, Fig. 8 B and Fig. 8 C are depicted based on two or more figures captured during the first image capture 306
As come the alternative method that constructs pixel value histogram.The process of Fig. 8 A, Fig. 8 B and Fig. 8 C can be combined or be replaced with the process of Fig. 4 A
For Fig. 4 A process come using.
For example, Fig. 8 A depicts division paxel down-sampling.As noted above, i × j block of pixels and its associated
Both down-sampling pixels can be referred to as " paxel (data block) ".Block of pixels 800 is paxel --- 3 × 2 comprising pixel value
Block of pixels --- example.These pixel values can be (R, G, B) value of corresponding pixel, (R, G, B) value of respective pixel
It is one or more in some other expressions of brightness (Y) value or pixel of pixel average, in YCbCr color space.Nothing
By how, the average out to 96.67 of this six pixel values.
As a part of six pixel down-samplings to block of pixels 800, other than determining average pixel value, average value
It may be utilized for the pixel " division " in paxel being two groups.First group by having more than or equal to average pixel value
The pixel of pixel value is constituted, and second group is made of the pixel with the pixel value less than average pixel value.First can be calculated
Group pixel value it is average to determine high average paxel802, there is the value for 180.Being averaged for second group of pixel value can be calculated
To determine harmonic(-)mean paxel804, there is the value for 55.It extraly, can be by calculating the quantity of the pixel in first group
The percentage (or score) of high pixel 806 is determined divided by the total quantity of the pixel in paxel.In some embodiments, still
It does not show, can be come by calculating the quantity of the pixel in second group divided by the total quantity of the pixel in paxel true in fig. 8 a
Determine the percentage (or score) of low pixel.Alternatively, it is possible to pass through the percentage (or score) for subtracting high pixel 806 from 1
To determine the percentage (or score) of low pixel.
Each paxel in down-sampled images can be indicated by these three (or four) values.In this way, about
The information --- variation of such as pixel value --- of the structure of paxel can be retained.Extraly, high pixel can also be passed through
806 percentage (or score) to indicate to be distributed is weighted more heavily on average value or under average value.
Fig. 8 B depicts the example of the division paxel down-sampling of 32:1.Image 808 is 1280 × 960 image.By making
Carried out with the paxel of division, with 32 factor pair image 808 down-sampling result obtain 40 × 30 high the average image 810,40 ×
30 harmonic(-)mean image 812 and 40 × 30 high percentage picture element matrix 814.Therefore, high the average image 810 and harmonic(-)mean figure
As each pixel value in 812 can indicate 1024 pixels (32 × 32 paxel) in image 810.Although high mean chart
Picture 810 and harmonic(-)mean image 812 are referred to as " image ", but high the average image 810 and harmonic(-)mean image 812 cannot be direct
Ground viewing.Therefore, alternatively, high the average image 810 and harmonic(-)mean image 812 can be referred to as " matrix ".Although in Fig. 8 B
In be not shown, it is also possible to generate low percentage picture element matrix.
Fig. 8 C, which is depicted, divides paxel image based on down-sampling to construct histogram.Image 820 and image 822 can be
Derived from the corresponding image institute captured during measuring burst, the high the average image of down-sampling.Image 826 and image
It 828 can be from the respective image of same measurement burst derived from institute, the harmonic(-)mean image of down-sampling.For example, 820 He of image
Image 826 can be the high mean chart of down-sampling derived from institute from the same original, full size image in measurement happens suddenly respectively
Picture and down-sampling harmonic(-)mean image.Similarly, image 822 and image 828 can be another original from measurement burst respectively
Begin, the high the average image of down-sampling and down-sampling harmonic(-)mean image derived from institute in full size image.
Not from the context of Fig. 4 A process described above it is different, image 820 and image 822 can be reversed color
Adjust the high the average image 824 that maps and can be incorporated into combination.Therefore, each position in the average image high for down-sampling
(for example, pixel 820A and pixel 822A will be considered as in the same location) is set, can choose with the highest less than 255
The pixel of value.May independently, image 826 and image 828 can be reversed tone mapping and be combined to the high mean chart of combination
As 830.Therefore, for each position in down-sampling harmonic(-)mean image (for example, pixel 826A and pixel 828A will be considered as
In the same location), it can choose the pixel with the peak less than 255.In other embodiments, can alternatively make
With the threshold pixels value (for example, 250,245 or threshold pixels value higher than 255) different from 255.
The Gao Ping of combination can be placed on from each pixel selected in one in high the average image 802 and 822
In its corresponding position in equal image 824.Thus, for example, pixel 820A can be placed as pixel 824A, and pixel 822B
It can be placed as pixel 824B.Extraly, ground colour can be reversed for the selected pixel of the high the average image of combination 824
Adjust mapping.Similarly, combination can be placed on from each pixel selected in one in harmonic(-)mean image 826 and 828
High the average image 830 in its corresponding position in.Thus, for example, pixel 826A can be placed as pixel 830A, and picture
Plain 828B can be placed as pixel 830B.Color can also be reversed for the selected pixel of high the average image 824 of combination
Adjust mapping.
Each of in addition, placed in each of the high the average image 824 of combination and combined harmonic(-)mean image 830
Pixel value can capture the pixel not being downsampled in its corresponding paxel using the corresponding TET divided by corresponding TET.
Specific range can be normalized to for the pixel value for using various TET to be captured by doing so --- for example, the picture of various images
Element can be placed in the unit of same brightness.
Histogram 832 can be created from the high the average image 824 of combination and combined harmonic(-)mean image 830.Histogram 832
It can be the histogram of LDR histogram, HDR histogram, logarithm HDR histogram or some other types.Although in the presence of can be with shape
At many modes of histogram, but some embodiments may include assessing the function of the pixel value of each pixel.Example
Such as, if pixel value is indicated in (R, G, B) color space, then function can take being averaged for the value of (R, G, B) color channel
Or maximum value.
Alternatively, it is possible to construct multiple histograms.For example, use can be created in the case where (R, G, B) color space
A histogram in the channel R, a histogram for the channel G and a histogram for channel B.If many gold
Word tower is used, then can construct a histogram pyramid with every color channel.
In addition, the logarithm of the output of this function can also be used, and result can be used to for vertical bar being located in directly
In the x-axis of square figure.The amount for being added to this vertical bar can be with associated high pixel percentage (or score) or low pixel-based
Pixel percentage (or score).For example it is assumed that pixel 820A with for 0.73 high pixel percentage (or score) it is associated.Into one
Step assumes that pixel 820A is placed as pixel 824A.Then, once histogram vertical bar has been determined for this pixel, then 0.73 power
Weight can be added to the vertical bar.Similarly, it is assumed that pixel 828A with for 0.49 low pixel percentage (or score) it is associated.
It is further assumed that pixel 828A is placed as pixel 830A.Then, once histogram vertical bar has been determined for this pixel, then 0.49
Weight can be added to the vertical bar.In some embodiments, this weight can be modified further, possibly by center
Weighted average or some other technologies.
In some embodiments, the down-sampling of Pixel Information can be executed together with the processing for being fused to histogram 832
Rather than it is sequentially performed.For example, it may be determined that for the high the average image 824 of combination and combined 830 liang of harmonic(-)mean image
The pixel value of the specific pixel location of person, and associated weight can be to be added to before a location of pixels under consideration
Histogram 832.If the high mean pixel in the high the average image 824 of combination is derived from the harmonic(-)mean image an of image, combination
Harmonic(-)mean pixel in 830 is derived from the sum of another image and their corresponding weights not for 1, then doing so can permit pair
The further normalization of weight.Other sequences of process shown by Fig. 8 C are also possible, and are likely to be included in and replace
It selects in embodiment.
Histogram 832 can be prominent for subsequent payload to determine compared with one or more reference histograms
The TET value of hair.For example, histogram 832 can be compared with each histogram stored in training image data 304.Histogram
EMD of the figure to that can be compared in various ways, between such as calculating histogram pair.As discussed above, 0 and 1 may be taken
Between value weight w can from for specific histogram pair EMD export.
For each histogram to (wherein, the first histogram is histogram 832, and the second histogram comes from training image
Entry in database 304), weight w can be applied to the entry phase with the second histogram in training image database 304
Associated corresponding scene type.It as a result can be in histogram to being above averaged, to determine " HDR " of scene.For example, if
" HDR " generated is in 0 to 1 scale at 0.5 or on 0.5, then scene can be specified for HDR processing；But
It is if generating " HDR " in identical scale under 0.5, scene can be specified for LDR processing.The above institute
The embodiment of description is only the brightness of determining scene and a possible mode of " HDR ".Other skills can alternatively be used
Art.For example, it may be determined that being used for the short TET and long TET of scene.If between short TET and long TET difference (or long TET with it is short
The ratio of TET) be less than or equal to threshold value, then scene is considered LDR scene.If the difference between short TET and long TET
(or ratio of long TET and short TET) is greater than threshold value, then scene is considered HDR scene.
In order to determine for the TET of payload burst, following instantiation procedure can be used.However, it is possible to alternatively
Use other processes.
For each histogram to (again, wherein the first histogram is histogram 832, and the second histogram is from instruction
Practice the entry in image data base 304), weight w can be applied to and the second histogram in training image database 304
The associated corresponding target pixel value of entry.As a result it can be averaged to determine for by the scene represented by histogram 832
Target average pixel value.Then, in table 1 discribed process or some other processes may be used to determine whether to be used for
The TET of image capture afterwards.If scene type is HDR, two or more target average pixel values (and two can be determined
A or more corresponding TET).In some embodiments, corresponding TET may include rollback TET.However, it is possible to be not used
Rollback TET captures image, unless the Fusion failure of the short image and long image captured during the second image capture 314.
Alternatively, output image 316 can be only used to form in the first image capture 306 period institute's captured image.
Specifically, embodiment here can support several variation examples.It, can be in image in a possible variation example
Short TET and long TET is determined during processing 308.Then, during the second image capture 314, can respectively using short TET and
Long TET captures short image and long image.If alignment and/or the Fusion failure of the long image of the short image and combination of combination,
The long image of the short image or combination that then combine may be provided as output image 316 or export a part of image 316.
In another possible variation example, short TET, long TET and rollback can be determined during image procossing 308
TET.Then, during the second image capture 314, short figure respectively can be captured using short TET, long TET and rollback TET
Picture, long image and rollback image.If alignment and/or the Fusion failure of the long image of the short image and combination of combination, are returned
Moving back one in image or combined rollback image may be provided as output image 316 or is provided as exporting image 316
At least partially.Combined rollback image can be in a manner of similar with the long image of the short image and/or combination that combine by structure
It builds.
In another possible variation example, short TET, long TET and rollback can be determined during image procossing 308
TET.Then, during the second image capture 314, short image and long figure can be only respectively captured using short TET and long TET
Picture.If the alignment and/or Fusion failure, the short image combined or combined length of the long image of the short image and combination of combination
Image can be selected and be provided as exporting image 316 or be provided as exporting at least part of image 316.However,
In this variation example, if rollback TET be greater than selected blending image TET, digital gain can be applied (for example,
Multiplying for pixel value) the brightness regulation of image will be exported as brightness corresponding with rollback TET.Therefore, it exports
Image 316 can be shown and the consistent brightness of brightness using rollback TET institute captured image.
Fig. 9 depicts flow chart according to example embodiment.At step 900, it can be captured by imaging sensor
First multiple images of scene.It is captured that different TET can be used in each image in first multiple images.In step 902
Locate, the image in multiple images can be downsampled.At step 904, the pixel value of the image based on down-sampling can be constructed
Pixel value histogram.
At step 906, pixel value histogram can be compared with one or more reference pixel value histograms.By pixel
Value histogram can be related to compared with one or more reference pixel value histograms: for pixel value histogram and one or more
Each of multiple reference pixel value histograms determine corresponding similarity measurement, and inverse next true based on similarity measurement
Fixed corresponding weight.One or more reference pixel value histograms can be associated with corresponding target average pixel value, and
And payload TET can be based on the sum of the respective weights for being applied to corresponding target average pixel value.
At step 908, it can be based on pixel value histogram compared with one or more reference pixel value histograms
To determine payload TET.At least one additional image that payload TET carrys out capturing scenes can be used.
In some embodiments, carrying out down-sampling to the image in multiple images may relate to: formed it is multiple it is high it is average under
Sampled images and more than second a harmonic(-)mean down-sampled images.Extraly, pixel value is constructed based on the pixel value of down-sampled images
Histogram may relate to construct picture based on the pixel value of multiple high average down-sampled images and multiple harmonic(-)mean down-sampled images
Plain value histogram.
The each image formed in multiple high average down-sampled images can be related to: will be in multiple high average down-sampled images
Each image be divided into the non-overlap matrix of corresponding paxel, calculate the average pixel value of each paxel, and calculate each
The high average pixel value of paxel.Each pixel in the non-overlap matrix of paxel can indicate have 1 × 2,2 × 1,2 × 2 or
At least one pixel segment of respective image in the multiple images of bigger dimension.Average pixel value can be corresponding paxel
The corresponding average value of interior all pixels, and high average pixel value can be with flat more than or equal to corresponding paxel
The corresponding average value of all pixels in the corresponding paxel of equal pixel value.
The each image for forming multiple harmonic(-)mean down-sampled images may relate to calculate the harmonic(-)mean pixel of each paxel
Value.Harmonic(-)mean pixel value can be all pixels in the corresponding paxel of the value with the average pixel value less than corresponding paxel
Corresponding average value.
Carrying out down-sampling to the image in multiple images can be further to: calculating for the corresponding of each paxel
High pixel fraction and corresponding low pixel score for each paxel.High pixel fraction can be (i) have be greater than or equal to
Pixel in the corresponding paxle of the value of the average pixel value of corresponding paxel is corresponding with total pixel in (ii) accordingly paxel
Ratio.Low pixel score can be the pixel in the corresponding paxel of the value of (i) with the average pixel value less than corresponding paxel
The corresponding ratio of total pixel in paxel corresponding with (ii).
Pixel value histogram is constructed based on the pixel value of the image of down-sampling to be related to: will from it is multiple it is high it is average under
The height that the image of sampled images is combined to combination is averaged down-sampled images, and by the figure from multiple harmonic(-)mean down-sampled images
Harmonic(-)mean down-sampled images as being combined to combination.Image from more than first high average down-sampled images is combined to combination
The high down-sampled images that are averaged can be related to: be averaged each location of pixels in down-sampled images for combined height, from Gao Ping
High average pixel value is selected in the same pixel position in one in equal down-sampled images.It will be under more than second a harmonic(-)means
The harmonic(-)mean down-sampled images that the image of sampled images is combined to combination can be related to: for combined harmonic(-)mean down-sampled images
In each location of pixels, from the same pixel position in one in harmonic(-)mean down-sampled images select harmonic(-)mean pixel
Value.
Pixel value histogram is constructed based on the pixel value of the image of down-sampling may further include: to pixel value histogram
Figure addition indicates that combined height be averaged the first set of each of the down-sampled images amount of high average pixel value and expression is combined
Harmonic(-)mean down-sampled images in each harmonic(-)mean pixel value amount second set.Amount in the first set of amount is based on phase
The associated high pixel fraction answered.Amount in the second set of amount is based on corresponding associated low pixel score.
Step can be executed by following depicted in figure 9: such as camera apparatus of digital camera devices 100,
Such as calculate the calculating equipment and/or two or more equipment mutually distinguished of equipment 200.For example, in some embodiments,
(one or more) image capture step can be executed by imaging sensor, and remaining step can be by individual
Equipment is calculated to execute.Other arrangements are possible.In addition, flow chart depicted in figure 9 can according to this specification and/
Or variation example disclosed in attached drawing and modify.
Therefore, such as schematical equipment shown in Figure 1A to Fig. 1 D first camera system can be used record video or
For finding a view, and implemented simultaneously using second camera system include or based on method shown in Fig. 9 automatic exposure processing.
For example, the camera 112A of digital camera devices 100 can be used to record the video of scene.In the view of camera 112A record scene
When frequency, camera 112B is can be used to implement automatic exposure processing to establish the HDR histogram of scene in digital camera devices 100,
And histogram is compared with reference data, is adjusted to provide in record the exposure as used in camera 112A.
E. auto-focusing
In general, auto-focusing (AF) system may include some species of sensor, the control for automatically determining focusing setting
System processed and the actuator that the mechanical component (for example, camera lens) of camera is adjusted according to focusing setting.It is provided by sensor
Data can be used to assess image recording sensor or the mode of environment will be recorded, and be used to responsively control can be with
Change the Mechatronic Systems of the focusing of camera (for example, carrying out the component of moving lens by using actuator and/or changing the big of aperture
It is small).Various types of Autofocus Technologies can be utilized by the image capture device of such as digital camera devices 100.
Many consumer's cameras include passive autofocus system, the passive autofocus system by passively analyze into
Enter the image of optical system by lens focus on object (for example, its not by the light beam controlled be directed on object so as to
It focuses).Typical passive Autofocus Technology include " phase-detection " auto-focusing (PD-AF) and " Contrast Detection " oneself
Dynamic focusing (CD-AF) can also be referred to as " contrast measurement " auto-focusing.
Passive auto-focusing processing typically relates to computing system (for example, processor) and operates to mechanical lens system
To adjust the focusing setting (for example, changing focal distance or the depth of field) of camera lens, and then analyze from autofocus sensor institute
Whether the image of generation focuses.If the image generated is not focused satisfactorily, computing system adjusts focusing setting simultaneously again
And the focus feature in the image of assessment generation.In some embodiments, each adjusting of focusing setting can be based on
Some measurements of image (or how out of focus the specific part of image is) how out of focus determine.In other embodiments, it adjusts
It can be predetermined.In any case, the processing can be repeated until the image of generation is considered as satisfactorily being focused
Until.
As noted above, some cameras of such as DSLR may include Special Automatic focusing system, may include
One or more dedicated sensors of auto-focusing.Such camera is often used without the image sensing that be used to capture image
Device carrys out the purpose for auto-focusing.In addition, such camera generally includes PD-AF system, within the system, pass through camera lens institute
The light received is split into a pair of of image.Then, two images in a pair of of image may be directed to auto-focusing sensing
On device and it is analyzed to determine whether camera lens focuses.
The PD-AF system of one general-purpose system is assistant images registration (SIR) by lens phase detection system.SIR
Incident light is directed towards autofocus sensor using beam splitter by PD-AF system.More specifically, being located at the opposite of camera lens
The lenticule of side can will be directed towards autofocus sensor from the light for the opposite side for being originated from camera lens, this can be effectively
Create rangefinder, wherein two images are projected on autofocus sensor.Then, figure is formed by by two lenticules
As being compared to determine separation error, which is evaluated to determine whether camera lens is correctly focused.If separation misses
Difference instruction object is (for example, if separation error is not zero or not in certain threshold value with zero) out of focus, for focusing
Adjusting can be calculated based on the separation error, and camera lens can be moved according to setting adjusted.
In device designs, when the size and/or cost of component are significant, it does not include individual that device, which can use,
The camera system of autofocus system.In the case of many mobile phones and/or tablet computer are such, often wrap
Include the camera system using same imaging sensor for both auto-focusing and image capture.In many situations, such as move
Purpose of the camera using CD-AF for imaging in the portable device of mobile phone peace plate-type device.
Although individual sensor dedicated for auto-focusing, most of CD-AF systems can be used in CD-AF system
System is used for both image capture and auto-focusing using same imaging sensor.CD-AF system is by measuring institute on a sensor
Contrast in the image of detection determines whether object focuses.For doing so, CD-AF system can in the picture each
The change of point assessment contrast, the wherein higher instruction for being interpreted that image is sharper keen of contrast.
More specifically, compared with when image object is out of focus, when the object captured is focused in adjacent pixel, sensor
Adjacent pixel between intensity difference it is usually larger.In addition, CD-AF system can measure the contrast at specific pixel,
Or determine being averaged in particular pixel group.In any case, then adjustable focusing is arranged until detection CD-AF system
Until liminal contrast (and may be until detecting maximum-contrast).For example, schematical CD-AF system can be with
Make image data by high-pass filter, and adjusts the focusing of camera lens until the output from filter is more than threshold value
(and may be until the output of filter be at its highest level).
Capturing, there is the processing of the multiple images of scene of different focus setting may relate to focusing scanning.When capture is static
When photography, camera system can only execute focusing run-down before taking pictures.When recording video, camera system can be continuous
Ground executes focusing scanning, or at least periodically executes focusing scanning.Attentively, when the CD-AF system using master image sensor
When system executes focusing scanning while recording video, this can lead to the perceptible short-movie section out of focus of video.Note that adopting
Being arranged with different focusings can also relate in one direction or another come the processing of the multiple images of capturing scenes
Small focusing search.
Therefore, first camera system can be used to record video in the schematical equipment of such as Figure 1A to Fig. 1 D, and same
When using second camera system code first camera system implement CD-AF.For example, the camera 112A of digital camera devices 100
The video of scene can be used to record.When the video of camera 112A record scene, camera is can be used in digital camera 100
112B implements CD-AF processing, is determined so as to the focusing setting for camera 112A and adjusts in recording.
III. the exemplary process for the equipment with multiple cameras
Figure 10 A shows method 1000 according to example embodiment.More specifically, method 1000, which is related to equipment, uses first
Camera captures the first image data, wherein first camera initially captures first using the first value being arranged for image
Image data, as by shown in box 1002.Then, while first camera captures the first image data, equipment uses the
Two cameras come determine for image setting updated value, wherein first camera and second camera are disposed in equipment simultaneously
And be directed in substantially the same direction, as by shown in box 1004.Then, equipment is used for first camera use
The updated value of image setting, to continue to capture the first image data, as by shown in box 1006.
In another aspect, when equipment includes three or more cameras, two or more are can be used in equipment
Camera is provided using other one or more cameras simultaneously with being used for image capture for being currently used to capture image
All cameras auto-focusing, automatic exposure and/or automatic white balance.
In addition, method 1000 may be implemented as various types of image captures.For example, second camera can by with
It is arranged in update image used in first camera to capture video.As another example, second camera can be used for
The setting of the image as used in first camera is updated, to generate the image data for the display in view finder.Other examples
It is possible.
In another aspect, pay attention to representing other cameras to determine that one or more cameras of setting are determining setting
It can operate in low power mode simultaneously.For example, the auxiliary camera for executing auto-focusing processing can be while doing so
Using shearing to save electric power.As another example, the auxiliary camera for executing automatic exposure processing can be used for pixel
Repartition (binning) or skip technology to save electric power.For determining other low power technologies of various camera settings
It is also possible.
A. using the second camera system for testing and adjusting processing
Figure 10 B show can by equipment implement with use second camera determine for image setting update after
Value method 1050, such as at the box 1004 of method 1000.Specifically, equipment can make second camera use be used for
The test value of image setting captures the second image data, as passed through shown in box 1004a.Then, equipment can determine second
Whether image data meets one or more image criterion, as by shown in box 1004b.If the second image data meets
One or more image criterion, then equipment can be set image and be set as test value, as by shown in box 1004c.Otherwise,
Equipment can change the test value for image setting, as by shown in box 1004d.Then, equipment can repeat box
1004a to 1004d, until determining that the second image data meets one or more criterion at box 1004c.
In some embodiments, method 1000 and method 1050 can provide auto-focusing processing, use the second phase
Machine adjusts the focusing for first camera, and first camera is recording video or finding a view simultaneously.For example, in order to be set in movement
Standby first camera just provides auto-focusing while the video of capturing scenes, and the second camera of mobile device can be used for
One or more focusings scanning of scene is executed, so that the focusing setting of first camera can be adjusted.In some embodiment party
In formula, if with same camera execute focusing scanning and capture video compared with, the focusing scanning can scan larger number and/or
The focusing of range is arranged.In turn, the result of auto-focusing processing can be improved.For example, being used for the first phase when second camera executes
When the auto-focusing of machine, auto-focusing processing can be made more responsive to change unexpected or fierce in scene.In addition, this is matched
The visualization that the focusing in the video captured by first camera is searched can be helped to reduce and may eliminate by, which setting, influences.
In some embodiments, it can be assumed that between the camera lens of the second image capture system and the object being focused away from
From and the lens of the first image capture system identical as the distance between object (or connect enough at least for the purpose of focusing
Closely).In such embodiments, the first image capture system can be used simply determined by the second image capture system
Focusing setting (for example, (one or more) corresponding with the object being focused is arranged).
It in other embodiments, can be with needle using focusing setting (for example, focal length) determined by the second image capture system
To the first image capture system use and be conditioned.For example, can be according to the using focusing setting determined by second camera
Baseline (that is, the distance between two cameras) between one camera and second camera and be conditioned for first camera.For this
Sample is done, and can be determined based on following from the camera lens of first camera to the distance of object: (a) camera lens Yu object of second camera it
Between distance and angle, and (b) baseline between first camera and second camera.It is then possible to according to from first camera
Lens to object distance with adjust as used in second camera from the ratio at a distance from the lens to object of second camera
Focal length, so as to the distance between lens and the object for determining first camera.(one or more) focusing for first camera is set
The distance between focal length and object and the camera lens of first camera that sets and then can be set such that first camera are aligned.Other
Example is also possible.
Additionally or alternatively, method 1000 and method 1050 can record video in the first image capture system
Or automatic white balance processing is provided while finding a view, it is determined using the second image capture system for the first image capture system
The white balance of system is arranged.In such embodiments, the second image capture system can be used currently known or develop later
Any white balance processing.
In addition, method 1000 and method 1050 can provide automatic exposure processing, come using the second image capture system
Determine the exposure for being used for the first image capture system.For example, following automatic exposure processing can be implemented in second camera: upwards or to
Lower adjusting TET, until average pixel value is equal to target average pixel value, or and target average pixel value deviation less than in advance really
Fixed amount.Other examples are also possible.
B. the second camera for being arranged based on model is used
Processing is tested and adjusted alternatively other than iteration experiment and adjusting are handled or with iteration, calculating equipment can be with
It is carried out come the setting to the first camera for capturing image data based on mould using the image data from second camera system
The update of type.For example, second camera can be used for automatic exposure processing while first camera is recording video.
As specific example, Figure 10 C show can be implemented by equipment to use second camera to be used for provide
The method 1070 of the data of automatic exposure processing based on model.Method 1070 for example can be at the box 1040 of method 1000
It is carried out.In addition, method 1070 can be repeatedly carried out while the first camera of equipment is recording the video of scene,
With the automatic exposure processing for using the second camera of equipment to provide for first camera.
By implementation method 1070, the second camera of mobile device can be used to capture image data, mobile device (or
With the remote equipment of mobile device communication) the HDR histogram of scene, the picture number can progressively be constructed by the image data
According to the new image data being updated to by second camera using different exposure setting captures.The histogram progressively constructed can
May adjust periodically or even simultaneously by compared with reference data so that the exposure to first camera is provided in record
Section.
As by the way that shown in box 1072, method 1070 is related to the second image capture system of computing device operation and carrys out capturing scenes
Multiple second images, wherein capture each image in multiple images using the different value for exposing setting.In box
At 1074, multiple second images of equipment down-sampling are calculated.Then, calculate equipment can the pixel value based on the image of down-sampling come
Pixel value histogram is constructed, as by shown in box 1076.It can by pixel value histogram and one or more in addition, calculating equipment
Multiple reference pixel value histograms compare, as by shown in box 1078.Then, calculate equipment can based on pixel histogram with
The comparison of one or more reference pixel value histograms come determine for expose setting updated value, such as pass through box
Shown in 1080.
In some embodiments, box 1080 can be related to determining and be used by the first image capture system to continue capture the
The TET of one image data.Therefore, method 1070 can be repeated quickly and easily as many times as required during the videograph that first camera carries out, so that
The TET that video is captured as used in first camera can be conditioned repeatedly through record.Similarly, method 1070 can be
One camera is repeated quickly and easily as many times as required while capturing the image data for being presented in view finder, so that first camera
TET can be conditioned repeatedly while capture is used for the image data of view finder.
In another aspect, at box 1078, pixel value histogram and one or more reference pixel value histograms
Comparison various technologies can be used complete.For example, pixel value histogram and one or more reference pixel value histograms
Comparing can be related to determining for each of pixel value histogram and one or more reference pixel value histograms accordingly
Similarity measurement, and corresponding weight is determined based on the inverse of similarity measurement.One or more reference pixel values can
It, can be flat based on corresponding target is applied to associated with corresponding target average pixel value, and at box 1080
The sum of respective weights of equal pixel value determine TET.Then TET for first camera can be updated accordingly.
In some embodiments, to the image in multiple images carry out down-sampling can be related to being formed it is multiple it is high it is average under adopt
The image of sample and more than second a harmonic(-)mean down-sampled images.Extraly, pixel is constructed based on the pixel value of the image of down-sampling
Value histogram can be related to constructing based on the pixel value of multiple high average down-sampled images and multiple harmonic(-)mean down-sampled images
Pixel value histogram.
The each image formed in multiple high average down-sampled images can be related to: will be in multiple high average down-sampled images
Each image be divided into the non-overlap matrix of corresponding paxel, calculate the average pixel value of each paxel, and calculate every
The high average pixel value of a paxel.Each pixel in the non-overlap matrix of paxel can indicate there is 1 × 2,2 × 1,2 × 2
Or at least one pixel segment of the respective image in the multiple images of bigger dimension.Average pixel value can be accordingly
The corresponding average value of all pixels in paxel, and high average pixel value can be to have and be greater than or equal to corresponding paxel
Average pixel value corresponding paxel in all pixels corresponding average value.
The each image for forming multiple harmonic(-)mean down-sampled images may relate to calculate the harmonic(-)mean pixel of each paxel
Value.Harmonic(-)mean pixel value can be all pixels in the corresponding paxel of the value with the average pixel value less than corresponding paxel
Corresponding average value.
Carrying out down-sampling to the image in multiple images can be further to: calculating for the corresponding of each paxel
High pixel fraction and corresponding low pixel score for each paxel.High pixel fraction can be (i) have be greater than or equal to
The corresponding ratio of total pixel in pixel paxel corresponding to (ii) in the corresponding paxle of the value of the average pixel value of corresponding paxel
Rate.Low pixel score can be (i) have less than corresponding paxel average pixel value value corresponding paxel in pixel with
(ii) the corresponding ratio of total pixel in corresponding paxel.
Pixel value histogram is constructed based on the pixel value of the image of down-sampling to be related to: will from it is multiple it is high it is average under
The height that the image of sampled images is combined to combination is averaged down-sampled images, and by the figure from multiple harmonic(-)mean down-sampled images
Harmonic(-)mean down-sampled images as being combined to combination.Image from more than first high average down-sampled images is combined to combination
The high down-sampled images that are averaged can be related to: be averaged each location of pixels in down-sampled images for combined height, from Gao Ping
High average pixel value is selected in the same pixel position in one in equal down-sampled images.It will be under more than second a harmonic(-)means
The harmonic(-)mean down-sampled images that the image of sampled images is combined to combination can be related to: for combined harmonic(-)mean down-sampled images
In each location of pixels, from the same pixel position in one in harmonic(-)mean down-sampled images select harmonic(-)mean pixel
Value.
Pixel value histogram is constructed based on the pixel value of the image of down-sampling may further include: to pixel value histogram
Figure addition indicates that combined height be averaged the first set of each of the down-sampled images amount of high average pixel value and expression is combined
Harmonic(-)mean down-sampled images in each harmonic(-)mean pixel value amount second set.Amount in the first set of amount is based on phase
The associated high pixel fraction answered.Amount in the second set of amount is based on corresponding associated low pixel score.
More than paying attention to is only building pixel value using the division paxel technology of high average pixel value and harmonic(-)mean pixel value
One mode of histogram.In other cases, division paxel technology described above can be omitted, and histogram
It can directly be created from the image of down-sampling.Other technologies for generating pixel value histogram are also possible.
It is to be noted that automatic exposure processing second camera use --- such as method 1070 --- may allow about
The significant experiment of exposure may improve the result of automatic exposure processing.In addition, because the histogram of scene can regard
Frequency progressively constructs during recording, so histogram can be improved with the progress of videograph.Therefore, the record of scene is held
It is continuous longer, the result of automatic exposure processing can also be improved.
C. the multiple images setting for first camera is updated using one or more additional cameras
In some embodiments, the exemplary method of such as method 1000 can be used to adjust two or more figures in equipment
As setting.Method 1000 or part thereof can be implemented for two or more different types of image settings.In this way,
Just while the first image data of capturing scenes, two or more different types of images are arranged one image capture system
In it is all can for the first image capture system and be updated.
As a specific example, image data is being captured (for example, record video or capture a group are quiet in first camera equipment
State image) while, one or more second cameras in equipment can be used to concomitantly update for same equipment
First camera focusing setting and exposure setting both.As another specific example, image is captured in first camera equipment
While data, one or more second cameras in equipment can be used to update to be set for the near field focusing of first camera
It sets to focus with far field and be arranged.For example, while first camera is capturing video, second camera can far from equipment away from
Focusing scanning is executed from place or focusing is searched, and third camera can execute focusing scanning or right at the distance of proximity device
Coke is searched.
Furthermore, it is noted that one or more second cameras in equipment may be utilized for concomitantly updating for concomitantly
Multiple settings of two or more other cameras in the same equipment of image data are captured (for example, focusing setting and exposure
Setting).For example, first camera can execute AF processing and second camera can simultaneously execute automatic exposure processing, so as to
Adjust the focusing setting and exposure of the third camera and the 4th camera for capturing three-dimensional video-frequency (for example, creation 3D video material)
Setting.Other examples are also possible.
In some embodiments, an auxiliary camera can be used to update the first phase for recording video in equipment
The multiple images of machine are arranged.For example, method 1000 can operate the second image capture system at (a) further to equipment is calculated
The second image capture system is operated to update the first image setting for being used for the first image capture system and (b) to update the second figure
As replacing between setting.
As an example, calculating equipment can forwardly and rearwardly cut between following when first camera is recording video
It changes: (a) determining the focusing setting for first camera using second camera；(b) use is determined using same second camera
It is arranged in the exposure of first camera.
As another example, when first camera is recording video, calculate equipment can between following forward and
Switch backward: (a) determining the focusing setting for first camera using second camera；(b) come using same second camera
Determine that the white balance for first camera is arranged.
And as another example, when first camera is recording video, calculating equipment can take turns between following
It changes: (a) determining the focusing setting for first camera using second camera；(b) it is used for using same second camera to determine
The white balance of first camera is arranged；And (c) determine that the exposure for first camera is arranged using same second camera.Other
Example is also possible.
In some embodiments, exemplary method can be implemented by including the equipment of three or more cameras.At this
In the equipment of sample, two or more additional cameras can be used to update the first camera for recording video simultaneously
Different images settings.In this way, method 1000 can be further to: capturing the first figure in the first image capture system
When as data: after (a) operating third image capture system by calculating equipment to determine the update for the setting of the second image
Value；(b) instruction is sent using the updated value being arranged for the second image to the first image capture system to continue to catch
The instruction for image data of winning the first place.
As an example, second camera may be used to determine whether for first camera when first camera is recording video
Focusing setting (for example, be used for auto-focusing), and third camera can simultaneously be used for determining to be used for first camera simultaneously
Exposure setting (for example, be used for automatic exposure).As another example, when first camera is recording video, calculating is set
It is standby can be simultaneously: the focusing of first camera (a) is updated using second camera；(b) it is updated using third camera for
The white balance of one camera；And the exposure of first camera (c) is updated using the 4th camera.Other examples are also possible.This
Outside, it is noted that such function can be implemented by the set of the camera of array configuration, all to capture image data.So
And other configurations are also possible.
In addition, when equipment includes three or more cameras, a camera can determine for other magazine two
A or more one or more settings.For example, the camera in equipment can represent in equipment two or more other
Camera is handled to execute auto-focusing processing, automatic exposure processing and/or automatic white balance.Other examples are also possible.
IV. conclusion
The various features and function of disclosed system, apparatus and method have been described with reference to the drawings in specification described in detail above
Energy.In the accompanying drawings, similar symbol usually identifies similar component, unless context instruction is really not so.In the explanation of detailed description
Schematical embodiment described in book, drawings and claims is not intended to be restrictive.Other embodiments can be sharp
With, and other changes can be carried out, without departing from the range of main body presented herein.It will readily appreciate that, such as lead to here
Often described ground and in the accompanying drawings shown in, all aspects of this disclosure can with it is various it is extensive it is different configure be arranged, replace,
Combination, separation and design, it is all these herein all it is expressly contemplated that.
About any or all in attached drawing and as discussed herein message flow diagram, situation and flow chart, often
A step, box and/or communication can represent the processing of information according to example embodiment and/or the transmission of information.Alternative is real
Example is applied to be included in the range of these example embodiments.In these alternative embodiments, for example, depending on related function
Can, the function of being described as step, box, transmission, communication, request, response and/or message can not be according to shown or institute
What is discussed is sequentially executed, including according to being sequentially executed substantially concurrently or on the contrary.In addition, more or less step
Suddenly, box and/or function can be used together with any in message flow diagram discussed herein, situation and flow chart, and
These message flow diagrams, situation and flow chart can be partially or completely combined with each other.
The step of processing of representative information or box can execute method or skill as described herein with can be configured as
The circuit of the specific logic function of art is corresponding.Alternatively or in addition, the step of processing of representative information or box can be with
It is corresponding with the module of program code (including related data), segment or part.Program code may include one or more
Instruction, the instruction can be performed by processor for the specific logic function or movement in implementation method or technology.Program generation
Code and/or related data can be stored on any kind of computer-readable medium for such as storing equipment, including disk drive,
Hard disk driver or other storaging mediums.
Computer-readable medium can also include non-transient computer readable medium, such as register memory, processing
Device cache and/or random access memory (RAM) short time period like that storing data computer readable medium.It calculates
Machine readable medium can also store the non-transient computer readable medium of program code and/data including longer period, all
As medium as such as read-only memory (ROM), CD or disk and/or compact disk read-only memory (CD-ROM) or
Permanent long-term storage apparatus.Computer readable medium can also be any other volatibility or Nonvolatile memory system.Meter
Calculation machine readable medium is considered such as computer readable storage medium or tangible storage device.
In addition, the step of representing the transmission of one or more information or box can be with the software moulds in same physical equipment
Information transmission between block and/or hardware module is corresponding.However, other information transmission may be in different physical equipments
Software module and/or hardware module between.
Although other aspect and embodiment are for art technology there has been disclosed various aspects and embodiment
Personnel will be apparent.Various aspects and embodiment disclosed herein be not intended to be for illustrative purposes it is restrictive,
True range is as indicated by following claim.
Claims (27)
1. a kind of method for the setting for adjusting first camera using second camera, comprising:
The first image capture system is operated with the first image data of capturing scenes by calculating equipment, wherein described first
Image capture system initially captures the first image data using the first value being arranged for the first image；And
While the first image capture systems are capturing the first image data:
After operating the second image capture system by the calculating equipment to determine the update for the first image setting
Value, wherein the first image capture systems and second image capture system be disposed in on locking equipment and by
Orientation is in substantially the same direction；And
Instruction is continued to the finger of capture the first image data using the updated value for being used for the first image setting
Order is sent to the first image capture system；
Wherein, the first image setting includes the time for exposure for the first image capture systems, and
Wherein, operating second image capture system to determine the updated value being arranged for the first image includes:
Operate multiple second images that second image capture system carrys out capturing scenes, wherein use the different time for exposure
To capture each image in multiple second images；
Pixel value histogram is constructed from multiple second images of scene；
Each of pixel value histogram and multiple reference pixel value histograms are compared, with for pixel value histogram and
Each of described reference pixel value histogram, determines corresponding similarity measurement；
Corresponding weight in each of the multiple reference pixel value histogram is determined based on corresponding similarity measurement；With
And
The time for exposure for the first image capture systems is determined based on the corresponding weight of reference pixel value histogram.
2. according to the method described in claim 1, wherein it is determined that the time for exposure includes determining for being captured by the first image
System uses the total exposure time (TET) to continue capture the first image data.
3. according to the method described in claim 1, wherein, by the pixel value histogram and the multiple reference pixel value histogram
Each of figure, which is compared, includes:
For each of the pixel value histogram and the multiple reference pixel value histogram, to determine corresponding similitude
Measurement；And
Inverse based on the similarity measurement determines corresponding weight.
4. according to the method described in claim 3, wherein, the multiple reference pixel value histogram and corresponding target are averaged picture
Element value is associated, and wherein, based on be applied to the sum of corresponding weight of corresponding target average pixel value come it is true
Surely it is used for the updated value of the time for exposure.
5. according to the method described in claim 1, wherein, operating second image capture system to determine for described first
The updated value of image setting further comprises:
(a) make second image capture system using the test value for the first image setting to capture the second image
Data；
(b) determine whether second image data meets one or more image criterion；
If (c) second image data meets one or more image criterion, the first image is set and is set
It is set to the test value；And
(d) otherwise, change for the first image setting the test value, and then repeat (a) to (d) until
(c) until determining that second image data meets one or more criterion.
6. according to the method described in claim 1, wherein, the first image data include video.
7. according to the method described in claim 1, wherein, the first image data include the figure for the display in view finder
As data.
8. according to the method described in claim 1, wherein, the first image setting includes focusing setting.
9. according to the method described in claim 1, wherein, the first image setting includes exposure setting.
10. according to the method described in claim 1, wherein, the first image setting includes that white balance is arranged.
11. according to the method described in claim 1, further comprising: perform claim require 1 described in method so as to described the
While one image capture system is capturing the first image data of the scene, update is caught for the first image
Obtain each of two or more images setting of system, wherein the setting of the two or more images includes described the
The setting of one image and the setting of the second image.
12. according to the method for claim 11, wherein the calculating equipment operates second image capture system at (a)
System operates second image capture system and it is arranged to update second image to update the first image setting and (b)
Between alternately.
13. according to the method for claim 11, wherein third image capture system is also disposed in described on locking equipment
And it is oriented at the direction being substantially the same with the first image capture systems and second image capture system
On, and wherein, the method further includes:
While the first image capture systems are capturing the first image data:
The third image capture system is operated by the calculating equipment to determine and be used for second image setting more
Value after new；And
Instruction is sent using the updated value being arranged for second image to the first image capture systems to continue
Capture the instruction of the first image data.
14. according to the method for claim 11, wherein the two or more image settings include focusing setting and expose
Light setting.
15. according to the method for claim 11, wherein the two or more image settings include near field focusing setting
It focuses and is arranged with far field.
16. according to the method for claim 11, wherein the first image setting includes focusing setting, and wherein, institute
Stating the setting of the second image includes exposure setting.
17. according to the method described in claim 1, wherein, second image capture system is initially configured as capturing quiet
State image, and wherein, the method further includes:
Before starting to capture the first image data using the first image capture systems, second figure is reconfigured
Update as capture systems to represent the first image capture systems to determine for the first image setting.
18. a kind of system for the setting for adjusting first camera using second camera, comprising:
Two or more image capture systems include at least the first image capture system and the second image capture system, wherein
The first image capture systems and second image capture system are oriented in substantially the same direction；
Control system is configured as:
Initially operation the first image capture systems capture the first figure to use the first value for the setting of the first image
As data；And
While the first image capture systems are capturing the first image data:
Second image capture system is operated to determine the updated value for the first image setting；And
So that the first image capture systems continue to catch using the updated image value being arranged for the first image
The first image data are obtained,
Wherein, the first image setting includes the time for exposure for the first image capture systems, and
Wherein, in order to operate second image capture system determine for the first image setting updated value,
The control system is configured as:
Operate multiple second images that second image capture system carrys out capturing scenes, wherein use the different time for exposure
To capture each image in multiple second images；
Pixel value histogram is constructed from multiple second images of scene；
Each of pixel value histogram and multiple reference pixel value histograms are compared, with for pixel value histogram and
Each of described reference pixel value histogram, determines corresponding similarity measurement；
Corresponding weight in each of the multiple reference pixel value histogram is determined based on corresponding similarity measurement；With
And
The time for exposure for the first image capture systems is determined based on the corresponding weight of reference pixel value histogram.
19. system according to claim 18, wherein the identified value for the time for exposure includes for by institute
State the first image capture system using come continue capture the first image data total exposure time (TET).
20. system according to claim 18, wherein in order to operate second image capture system to determine for institute
The updated value of the first image setting is stated, the control system is configured to:
(a) it operates second image capture system and captures the scene to use the test value for described image setting
Second image data；
(b) determine whether second image data meets one or more image criterion；
If (c) second image data meets one or more image criterion, described image is set and is set as
The test value；And
(d) otherwise, change the test value for described image setting, and repeat (a) to (d) until determining described second
Until image data meets one or more criterion.
21. system according to claim 18, wherein the first image data include video.
22. system according to claim 18, wherein described image setting includes one in following: (a) focusing is set
It sets, (b) exposure setting, and (c) white balance is arranged.
23. system according to claim 18, wherein the system is mobile device, and wherein, the first image
Capture systems and second image capture system are disposed in the corner of the first surface of the mobile device.
24. system according to claim 18, wherein the system is mobile device, and wherein, the mobile device
Including being directed four image capture systems in a same direction, wherein two in four image capture systems
It is disposed in the first corner of the first surface of the mobile device, and wherein, in four image capture systems
Third and the 4th the second corner for being respectively disposed in the first surface and third corner.
25. system according to claim 18, wherein the system is mobile device, and wherein, the mobile device
Including being directed four image capture systems in a same direction, wherein the four of the first surface of the mobile device
Each place in a corner arranges one in four image capture systems.
26. one kind is wherein stored with the non-transitory computer-readable medium of instruction, described instruction by calculate equipment it is executable with
So that the calculating equipment executes function, the function includes:
The first image capture system is operated with the first image data of capturing scenes, wherein at the beginning of the first image capture systems
Beginning, ground was using the first value for the setting of the first image to capture the first image data；And
While the first image capture systems are capturing the first image data:
The second image capture system is operated to determine the updated value for the first image setting, wherein described first
Image capture system and second image capture system are disposed in on locking equipment and being oriented at substantially the same
On direction；And
Instruction is continued to the finger of capture the first image data using the updated value for being used for the first image setting
Order is sent to the first image capture systems,
Wherein, the first image setting includes the time for exposure for the first image capture systems, and
Wherein, operating second image capture system to determine the updated value being arranged for the first image includes:
Operate multiple second images that second image capture system carrys out capturing scenes, wherein use the different time for exposure
To capture each image in multiple second images；
Pixel value histogram is constructed from multiple second images of scene；
Each of pixel value histogram and multiple reference pixel value histograms are compared, with for pixel value histogram and
Each of described reference pixel value histogram, determines corresponding similarity measurement；
Corresponding weight in each of the multiple reference pixel value histogram is determined based on corresponding similarity measurement；With
And
The time for exposure for the first image capture systems is determined based on the corresponding weight of reference pixel value histogram.
27. non-transitory computer-readable medium according to claim 26, wherein determine the value for being used for the time for exposure
Including determining for using the total exposure time to continue capture the first image data by the first image capture systems
(TET)。
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/041,905 US9615012B2 (en) | 2013-09-30 | 2013-09-30 | Using a second camera to adjust settings of first camera |
US14/041,905 | 2013-09-30 | ||
PCT/US2014/056413 WO2015047877A1 (en) | 2013-09-30 | 2014-09-18 | Using a second camera to adjust settings of first camera |
Publications (2)
Publication Number | Publication Date |
---|---|
CN105765967A CN105765967A (en) | 2016-07-13 |
CN105765967B true CN105765967B (en) | 2019-06-04 |
Family
ID=52739784
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201480065311.4A Active CN105765967B (en) | 2013-09-30 | 2014-09-18 | The method, system and medium of the setting of first camera are adjusted using second camera |
Country Status (4)
Country | Link |
---|---|
US (1) | US9615012B2 (en) |
EP (1) | EP3053332B1 (en) |
CN (1) | CN105765967B (en) |
WO (1) | WO2015047877A1 (en) |
Families Citing this family (161)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113472989A (en) | 2012-11-28 | 2021-10-01 | 核心光电有限公司 | Multi-aperture imaging system and method for acquiring images by multi-aperture imaging system |
CN108989647B (en) | 2013-06-13 | 2020-10-20 | 核心光电有限公司 | Double-aperture zooming digital camera |
EP3779565A3 (en) | 2013-07-04 | 2021-05-05 | Corephotonics Ltd. | Miniature telephoto lens assembly |
CN108718376B (en) | 2013-08-01 | 2020-08-14 | 核心光电有限公司 | Thin multi-aperture imaging system with auto-focus and method of use thereof |
KR20150042574A (en) * | 2013-10-11 | 2015-04-21 | 엘지전자 주식회사 | Mobile terminal and method for controlling thereof |
US9578252B2 (en) * | 2013-10-18 | 2017-02-21 | Light Labs Inc. | Methods and apparatus for capturing images using optical chains and/or for using captured images |
KR102154802B1 (en) * | 2014-02-11 | 2020-09-11 | 삼성전자주식회사 | Photogrpaphing method of electronic apparatus and electronic apparatus thereof |
US9344638B2 (en) | 2014-05-30 | 2016-05-17 | Apple Inc. | Constant bracket high dynamic range (cHDR) operations |
US9380218B2 (en) * | 2014-05-30 | 2016-06-28 | Apple Inc. | Highlight exposure metric and its applications |
WO2015188359A1 (en) * | 2014-06-12 | 2015-12-17 | 深圳市大疆创新科技有限公司 | Image processing method and apparatus |
US9392188B2 (en) | 2014-08-10 | 2016-07-12 | Corephotonics Ltd. | Zoom dual-aperture camera with folded lens |
WO2016025962A1 (en) * | 2014-08-15 | 2016-02-18 | The University Of Akron | Device and method for three-dimensional video communication |
TWI554098B (en) * | 2014-10-07 | 2016-10-11 | 緯創資通股份有限公司 | Video recording device and associated video recording method |
US10445895B2 (en) * | 2014-11-21 | 2019-10-15 | Apple Inc. | Method and system for determining spatial coordinates of a 3D reconstruction of at least part of a real object at absolute spatial scale |
US9396554B2 (en) | 2014-12-05 | 2016-07-19 | Symbol Technologies, Llc | Apparatus for and method of estimating dimensions of an object associated with a code in automatic response to reading the code |
CN107209404B (en) | 2015-01-03 | 2021-01-15 | 核心光电有限公司 | Miniature telephoto lens module and camera using the same |
US9729785B2 (en) * | 2015-01-19 | 2017-08-08 | Microsoft Technology Licensing, Llc | Profiles identifying camera capabilities that are usable concurrently |
KR101914894B1 (en) | 2015-04-02 | 2018-11-02 | 코어포토닉스 리미티드 | Dual voice coil motor structure of dual optical module camera |
KR101963546B1 (en) | 2015-04-16 | 2019-03-28 | 코어포토닉스 리미티드 | Auto focus and optical imagestabilization in a compact folded camera |
US9979890B2 (en) | 2015-04-23 | 2018-05-22 | Apple Inc. | Digital viewfinder user interface for multiple cameras |
CN110687655B (en) | 2015-05-28 | 2022-10-21 | 核心光电有限公司 | Bi-directional stiffness for optical image stabilization and auto-focus in dual aperture digital cameras |
KR102336447B1 (en) * | 2015-07-07 | 2021-12-07 | 삼성전자주식회사 | Image capturing apparatus and method for the same |
US10230888B2 (en) * | 2015-07-31 | 2019-03-12 | Qualcomm Incorporated | Sensor-based camera initialization |
KR102253997B1 (en) | 2015-08-13 | 2021-05-20 | 코어포토닉스 리미티드 | Dual aperture zoom camera with video support and switching/non-switching dynamic control |
CN106470336A (en) * | 2015-08-17 | 2017-03-01 | 深圳富泰宏精密工业有限公司 | Camera color temperature compensating system and intelligent terminal |
KR102339178B1 (en) * | 2015-08-27 | 2021-12-14 | 엘지전자 주식회사 | Mobile terminal and method for controlling the same |
KR102225727B1 (en) | 2015-09-06 | 2021-03-10 | 코어포토닉스 리미티드 | Auto focus and optical image stabilization with roll compensation in a compact folded camera |
USD998607S1 (en) * | 2015-10-16 | 2023-09-12 | Sondhi Amit | Mobile device with multiple camera lenses |
TW201719572A (en) * | 2015-11-19 | 2017-06-01 | 國立交通大學 | Method for analyzing and searching 3D models |
US10212341B2 (en) * | 2015-11-20 | 2019-02-19 | Amit SONDHI | Mobile electronic device with multiple cameras |
EP3342151B1 (en) * | 2015-11-30 | 2023-05-17 | Hewlett-Packard Development Company, L.P. | Parameter adjustments based on strength change |
US9762893B2 (en) * | 2015-12-07 | 2017-09-12 | Google Inc. | Systems and methods for multiscopic noise reduction and high-dynamic range |
US10511776B2 (en) * | 2015-12-28 | 2019-12-17 | Huawei Technologies Co., Ltd. | Image fusion method and apparatus, and terminal device |
KR102140882B1 (en) | 2015-12-29 | 2020-08-04 | 코어포토닉스 리미티드 | Dual-aperture zoom digital camera with automatic adjustable tele field of view |
US10352689B2 (en) | 2016-01-28 | 2019-07-16 | Symbol Technologies, Llc | Methods and systems for high precision locationing with depth values |
US10145955B2 (en) | 2016-02-04 | 2018-12-04 | Symbol Technologies, Llc | Methods and systems for processing point-cloud data with a line scanner |
US10721451B2 (en) | 2016-03-23 | 2020-07-21 | Symbol Technologies, Llc | Arrangement for, and method of, loading freight into a shipping container |
CN111784615A (en) * | 2016-03-25 | 2020-10-16 | 北京三星通信技术研究有限公司 | Method and device for processing multimedia information |
US9805240B1 (en) | 2016-04-18 | 2017-10-31 | Symbol Technologies, Llc | Barcode scanning and dimensioning |
CN111965919B (en) | 2016-05-30 | 2022-02-08 | 核心光电有限公司 | Rotary ball guided voice coil motor |
KR20170139408A (en) * | 2016-06-09 | 2017-12-19 | 엘지전자 주식회사 | Moving picture photographing apparatus having dual cameras |
US9854156B1 (en) | 2016-06-12 | 2017-12-26 | Apple Inc. | User interface for camera effects |
CN109639954B (en) | 2016-06-19 | 2020-10-23 | 核心光电有限公司 | Frame synchronization system and method in dual aperture camera |
KR102520225B1 (en) * | 2016-06-30 | 2023-04-11 | 삼성전자주식회사 | Electronic device and image capturing method thereof |
WO2018007981A1 (en) | 2016-07-07 | 2018-01-11 | Corephotonics Ltd. | Linear ball guided voice coil motor for folded optic |
US10706518B2 (en) | 2016-07-07 | 2020-07-07 | Corephotonics Ltd. | Dual camera system with improved video smooth transition by image blending |
KR102636272B1 (en) * | 2016-07-26 | 2024-02-14 | 삼성전자주식회사 | Image pickup device and electronic system including the same |
US10776661B2 (en) * | 2016-08-19 | 2020-09-15 | Symbol Technologies, Llc | Methods, systems and apparatus for segmenting and dimensioning objects |
CN106550230A (en) * | 2016-08-31 | 2017-03-29 | 深圳小辣椒虚拟现实技术有限责任公司 | A kind of 3D rendering filming apparatus and method |
GB2554669B (en) * | 2016-09-30 | 2022-04-06 | Apical Ltd | Image processing |
US11042161B2 (en) | 2016-11-16 | 2021-06-22 | Symbol Technologies, Llc | Navigation control method and apparatus in a mobile automation system |
US10284761B2 (en) * | 2016-11-17 | 2019-05-07 | Motorola Mobility Llc | Multi-camera capture of a high dynamic range image |
US10451405B2 (en) | 2016-11-22 | 2019-10-22 | Symbol Technologies, Llc | Dimensioning system for, and method of, dimensioning freight in motion along an unconstrained path in a venue |
TWI626620B (en) * | 2016-12-20 | 2018-06-11 | 廣東歐珀移動通訊有限公司 | Image processing method and device, electronic device, and computer readable storage medium |
US10354411B2 (en) | 2016-12-20 | 2019-07-16 | Symbol Technologies, Llc | Methods, systems and apparatus for segmenting objects |
EP4246993A3 (en) | 2016-12-28 | 2024-03-06 | Corephotonics Ltd. | Folded camera structure with an extended light-folding-element scanning range |
KR20180078596A (en) * | 2016-12-30 | 2018-07-10 | 삼성전자주식회사 | Method and electronic device for auto focus |
US10250794B2 (en) | 2017-01-04 | 2019-04-02 | Motorola Mobility Llc | Capturing an image using multi-camera automatic focus |
KR102365926B1 (en) | 2017-01-12 | 2022-02-23 | 코어포토닉스 리미티드 | Compact folded camera |
US10630888B2 (en) | 2017-02-09 | 2020-04-21 | Samsung Electronics Co., Ltd. | Method and apparatus for selecting capture configuration based on scene analysis |
US10169671B2 (en) | 2017-02-17 | 2019-01-01 | Motorola Mobility Llc | Face detection with temperature and distance validation |
US10057499B1 (en) | 2017-02-21 | 2018-08-21 | Motorola Mobility Llc | Automatic exposure control convergence procedure by auxiliary camera |
EP3579040B1 (en) | 2017-02-23 | 2021-06-23 | Corephotonics Ltd. | Folded camera lens designs |
US10645286B2 (en) | 2017-03-15 | 2020-05-05 | Corephotonics Ltd. | Camera with panoramic scanning range |
US10250795B2 (en) | 2017-03-15 | 2019-04-02 | Motorola Mobility Llc | Identifying a focus point in a scene utilizing a plurality of cameras |
EP3619600A4 (en) | 2017-05-01 | 2020-10-21 | Symbol Technologies, LLC | Method and apparatus for object status detection |
US10949798B2 (en) | 2017-05-01 | 2021-03-16 | Symbol Technologies, Llc | Multimodal localization and mapping for a mobile automation apparatus |
US10726273B2 (en) | 2017-05-01 | 2020-07-28 | Symbol Technologies, Llc | Method and apparatus for shelf feature and object placement detection from shelf images |
US11093896B2 (en) | 2017-05-01 | 2021-08-17 | Symbol Technologies, Llc | Product status detection system |
US10591918B2 (en) | 2017-05-01 | 2020-03-17 | Symbol Technologies, Llc | Fixed segmented lattice planning for a mobile automation apparatus |
US10663590B2 (en) | 2017-05-01 | 2020-05-26 | Symbol Technologies, Llc | Device and method for merging lidar data |
US11449059B2 (en) | 2017-05-01 | 2022-09-20 | Symbol Technologies, Llc | Obstacle detection for a mobile automation apparatus |
US11367092B2 (en) | 2017-05-01 | 2022-06-21 | Symbol Technologies, Llc | Method and apparatus for extracting and processing price text from an image set |
CN107426471B (en) * | 2017-05-03 | 2021-02-05 | Oppo广东移动通信有限公司 | Camera module and electronic device |
US10122943B1 (en) | 2017-05-05 | 2018-11-06 | Motorola Mobility Llc | High dynamic range sensor resolution using multiple image sensors |
WO2018201423A1 (en) | 2017-05-05 | 2018-11-08 | Symbol Technologies, Llc | Method and apparatus for detecting and interpreting price label text |
DK180859B1 (en) | 2017-06-04 | 2022-05-23 | Apple Inc | USER INTERFACE CAMERA EFFECTS |
CN107343158A (en) * | 2017-07-25 | 2017-11-10 | 广东欧珀移动通信有限公司 | Accelerate the convergent method and devices of AEC, terminal device |
WO2019048904A1 (en) | 2017-09-06 | 2019-03-14 | Corephotonics Ltd. | Combined stereoscopic and phase detection depth mapping in a dual aperture camera |
US10521914B2 (en) | 2017-09-07 | 2019-12-31 | Symbol Technologies, Llc | Multi-sensor object recognition system and method |
US10572763B2 (en) | 2017-09-07 | 2020-02-25 | Symbol Technologies, Llc | Method and apparatus for support surface edge detection |
US10891126B2 (en) * | 2017-09-11 | 2021-01-12 | Mx Technologies, Inc. | On-device feature and performance testing and adjustment |
US10951834B2 (en) | 2017-10-03 | 2021-03-16 | Corephotonics Ltd. | Synthetically enlarged camera aperture |
US10922551B2 (en) * | 2017-10-06 | 2021-02-16 | The Nielsen Company (Us), Llc | Scene frame matching for automatic content recognition |
US10200623B1 (en) * | 2017-10-17 | 2019-02-05 | Qualcomm Incorporated | Image capture setting determination in flash photography operations |
CN113075838B (en) | 2017-11-23 | 2022-11-29 | 核心光电有限公司 | Camera, manufacturing method thereof, mobile electronic equipment and method for reducing space occupied by bulges |
CN110352371B (en) | 2018-02-05 | 2022-05-13 | 核心光电有限公司 | Folding camera device capable of reducing height allowance |
US11112964B2 (en) | 2018-02-09 | 2021-09-07 | Apple Inc. | Media capture lock affordance for graphical user interface |
WO2019155289A1 (en) | 2018-02-12 | 2019-08-15 | Corephotonics Ltd. | Folded camera with optical image stabilization |
US10740911B2 (en) | 2018-04-05 | 2020-08-11 | Symbol Technologies, Llc | Method, system and apparatus for correcting translucency artifacts in data representing a support structure |
US10823572B2 (en) | 2018-04-05 | 2020-11-03 | Symbol Technologies, Llc | Method, system and apparatus for generating navigational data |
US10809078B2 (en) | 2018-04-05 | 2020-10-20 | Symbol Technologies, Llc | Method, system and apparatus for dynamic path generation |
US10832436B2 (en) | 2018-04-05 | 2020-11-10 | Symbol Technologies, Llc | Method, system and apparatus for recovering label positions |
US11327504B2 (en) | 2018-04-05 | 2022-05-10 | Symbol Technologies, Llc | Method, system and apparatus for mobile automation apparatus localization |
US10694168B2 (en) | 2018-04-22 | 2020-06-23 | Corephotonics Ltd. | System and method for mitigating or preventing eye damage from structured light IR/NIR projector systems |
US11268829B2 (en) | 2018-04-23 | 2022-03-08 | Corephotonics Ltd | Optical-path folding-element with an extended two degree of freedom rotation range |
US10594921B2 (en) * | 2018-04-26 | 2020-03-17 | Qualcomm Incorporated | Dual phase detection power optimizations |
US10375313B1 (en) | 2018-05-07 | 2019-08-06 | Apple Inc. | Creative camera |
US11722764B2 (en) | 2018-05-07 | 2023-08-08 | Apple Inc. | Creative camera |
JP2019197946A (en) * | 2018-05-07 | 2019-11-14 | シャープ株式会社 | Electronic apparatus, program, control apparatus, and control method |
CN111316346B (en) | 2018-08-04 | 2022-11-29 | 核心光电有限公司 | Switchable continuous display information system above camera |
CN109040589B (en) * | 2018-08-16 | 2020-06-30 | Oppo广东移动通信有限公司 | Image processing method, image processing device, storage medium and electronic equipment |
WO2020039302A1 (en) | 2018-08-22 | 2020-02-27 | Corephotonics Ltd. | Two-state zoom folded camera |
US11412154B2 (en) | 2018-09-07 | 2022-08-09 | Dolby Laboratories Licensing Corporation | Auto exposure of spatially-multiplexed-exposure high-dynamic-range image sensor metric and adjustment |
US10674072B1 (en) | 2019-05-06 | 2020-06-02 | Apple Inc. | User interfaces for capturing and managing visual media |
US11770601B2 (en) | 2019-05-06 | 2023-09-26 | Apple Inc. | User interfaces for capturing and managing visual media |
US11128792B2 (en) | 2018-09-28 | 2021-09-21 | Apple Inc. | Capturing and displaying images with multiple focal planes |
US11321857B2 (en) | 2018-09-28 | 2022-05-03 | Apple Inc. | Displaying and editing images with depth information |
US11010920B2 (en) | 2018-10-05 | 2021-05-18 | Zebra Technologies Corporation | Method, system and apparatus for object detection in point clouds |
US11506483B2 (en) | 2018-10-05 | 2022-11-22 | Zebra Technologies Corporation | Method, system and apparatus for support structure depth determination |
KR102569375B1 (en) | 2018-10-24 | 2023-08-22 | 삼성전자주식회사 | Electronic device and controlling method thereof |
KR20200053125A (en) | 2018-11-08 | 2020-05-18 | 삼성전자주식회사 | Electronic device and control method thereof |
US11003188B2 (en) | 2018-11-13 | 2021-05-11 | Zebra Technologies Corporation | Method, system and apparatus for obstacle handling in navigational path generation |
US11090811B2 (en) | 2018-11-13 | 2021-08-17 | Zebra Technologies Corporation | Method and apparatus for labeling of support structures |
JP7115253B2 (en) * | 2018-11-28 | 2022-08-09 | トヨタ自動車株式会社 | In-vehicle camera system |
US11079240B2 (en) | 2018-12-07 | 2021-08-03 | Zebra Technologies Corporation | Method, system and apparatus for adaptive particle filter localization |
US11416000B2 (en) | 2018-12-07 | 2022-08-16 | Zebra Technologies Corporation | Method and apparatus for navigational ray tracing |
US11100303B2 (en) | 2018-12-10 | 2021-08-24 | Zebra Technologies Corporation | Method, system and apparatus for auxiliary label detection and association |
US11015938B2 (en) | 2018-12-12 | 2021-05-25 | Zebra Technologies Corporation | Method, system and apparatus for navigational assistance |
US10731970B2 (en) | 2018-12-13 | 2020-08-04 | Zebra Technologies Corporation | Method, system and apparatus for support structure detection |
US11057558B2 (en) * | 2018-12-27 | 2021-07-06 | Microsoft Technology Licensing, Llc | Using change of scene to trigger automatic image capture |
CA3028708A1 (en) | 2018-12-28 | 2020-06-28 | Zih Corp. | Method, system and apparatus for dynamic loop closure in mapping trajectories |
WO2020144528A1 (en) | 2019-01-07 | 2020-07-16 | Corephotonics Ltd. | Rotation mechanism with sliding joint |
CN111971956B (en) | 2019-03-09 | 2021-12-03 | 核心光电有限公司 | Method and system for dynamic stereo calibration |
CN111901475A (en) * | 2019-05-06 | 2020-11-06 | 苹果公司 | User interface for capturing and managing visual media |
US11706521B2 (en) | 2019-05-06 | 2023-07-18 | Apple Inc. | User interfaces for capturing and managing visual media |
US11662739B2 (en) | 2019-06-03 | 2023-05-30 | Zebra Technologies Corporation | Method, system and apparatus for adaptive ceiling-based localization |
US11200677B2 (en) | 2019-06-03 | 2021-12-14 | Zebra Technologies Corporation | Method, system and apparatus for shelf edge detection |
US11402846B2 (en) | 2019-06-03 | 2022-08-02 | Zebra Technologies Corporation | Method, system and apparatus for mitigating data capture light leakage |
US11341663B2 (en) | 2019-06-03 | 2022-05-24 | Zebra Technologies Corporation | Method, system and apparatus for detecting support structure obstructions |
US11960286B2 (en) | 2019-06-03 | 2024-04-16 | Zebra Technologies Corporation | Method, system and apparatus for dynamic task sequencing |
US11151743B2 (en) | 2019-06-03 | 2021-10-19 | Zebra Technologies Corporation | Method, system and apparatus for end of aisle detection |
US11080566B2 (en) | 2019-06-03 | 2021-08-03 | Zebra Technologies Corporation | Method, system and apparatus for gap detection in support structures with peg regions |
CN110189277B (en) * | 2019-06-05 | 2023-03-31 | 电子科技大学 | High dynamic range image visualization method based on empirical mode decomposition |
US11575884B1 (en) | 2019-07-26 | 2023-02-07 | Apple Inc. | Display calibration system |
EP3837662A4 (en) | 2019-07-31 | 2021-12-15 | Corephotonics Ltd. | System and method for creating background blur in camera panning or motion |
US11659135B2 (en) | 2019-10-30 | 2023-05-23 | Corephotonics Ltd. | Slow or fast motion video using depth information |
US11507103B2 (en) | 2019-12-04 | 2022-11-22 | Zebra Technologies Corporation | Method, system and apparatus for localization-based historical obstacle handling |
US11770618B2 (en) | 2019-12-09 | 2023-09-26 | Corephotonics Ltd. | Systems and methods for obtaining a smart panoramic image |
US11949976B2 (en) | 2019-12-09 | 2024-04-02 | Corephotonics Ltd. | Systems and methods for obtaining a smart panoramic image |
US11107238B2 (en) | 2019-12-13 | 2021-08-31 | Zebra Technologies Corporation | Method, system and apparatus for detecting item facings |
US11341621B2 (en) * | 2020-03-04 | 2022-05-24 | GM Global Technology Operations LLC | Enhanced imaging system for a motor vehicle |
EP4115603A1 (en) * | 2020-03-06 | 2023-01-11 | Telefonaktiebolaget LM ERICSSON (PUBL) | Methods providing video conferencing with adjusted/modified video and related video conferencing nodes |
WO2021184341A1 (en) * | 2020-03-20 | 2021-09-23 | SZ DJI Technology Co., Ltd. | Autofocus method and camera system thereof |
US11822333B2 (en) | 2020-03-30 | 2023-11-21 | Zebra Technologies Corporation | Method, system and apparatus for data capture illumination control |
KR20220003550A (en) | 2020-04-26 | 2022-01-10 | 코어포토닉스 리미티드 | Temperature Control for Hall Bar Sensor Calibration |
WO2021234515A1 (en) | 2020-05-17 | 2021-11-25 | Corephotonics Ltd. | Image stitching in the presence of a full field of view reference image |
CN114080565B (en) | 2020-05-30 | 2024-01-19 | 核心光电有限公司 | System and method for obtaining ultra-macro images |
US11039074B1 (en) | 2020-06-01 | 2021-06-15 | Apple Inc. | User interfaces for managing media |
EP4202521A1 (en) | 2020-07-15 | 2023-06-28 | Corephotonics Ltd. | Point of view aberrations correction in a scanning folded camera |
US11637977B2 (en) | 2020-07-15 | 2023-04-25 | Corephotonics Ltd. | Image sensors and sensing methods to obtain time-of-flight and phase detection information |
US11450024B2 (en) | 2020-07-17 | 2022-09-20 | Zebra Technologies Corporation | Mixed depth object detection |
US11946775B2 (en) | 2020-07-31 | 2024-04-02 | Corephotonics Ltd. | Hall sensor—magnet geometry for large stroke linear position sensing |
KR102480820B1 (en) | 2020-08-12 | 2022-12-22 | 코어포토닉스 리미티드 | Optical Image Stabilization of Scanning Folded Cameras |
US11212449B1 (en) | 2020-09-25 | 2021-12-28 | Apple Inc. | User interfaces for media capture and management |
US11593915B2 (en) | 2020-10-21 | 2023-02-28 | Zebra Technologies Corporation | Parallax-tolerant panoramic image generation |
US11392891B2 (en) | 2020-11-03 | 2022-07-19 | Zebra Technologies Corporation | Item placement detection and optimization in material handling systems |
US11847832B2 (en) | 2020-11-11 | 2023-12-19 | Zebra Technologies Corporation | Object classification for autonomous navigation systems |
US11539876B2 (en) | 2021-04-30 | 2022-12-27 | Apple Inc. | User interfaces for altering visual media |
US11778339B2 (en) | 2021-04-30 | 2023-10-03 | Apple Inc. | User interfaces for altering visual media |
US11954882B2 (en) | 2021-06-17 | 2024-04-09 | Zebra Technologies Corporation | Feature-based georegistration for mobile computing devices |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1848913A (en) * | 2005-04-05 | 2006-10-18 | 奥林巴斯映像株式会社 | Digital camera |
CN102761690A (en) * | 2011-04-28 | 2012-10-31 | 佳能株式会社 | Imaging apparatus and method for controlling the same |
Family Cites Families (132)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPS59137942A (en) | 1983-01-28 | 1984-08-08 | Hitachi Ltd | Picture positioning system |
US6204881B1 (en) | 1993-10-10 | 2001-03-20 | Canon Kabushiki Kaisha | Image data processing apparatus which can combine a plurality of images at different exposures into an image with a wider dynamic range |
US5781308A (en) | 1996-03-04 | 1998-07-14 | Hewlett-Packard Company | High speed system for threshold matrix alignment and tiling, during creation of a binary half-tone image |
US5828793A (en) | 1996-05-06 | 1998-10-27 | Massachusetts Institute Of Technology | Method and apparatus for producing digital images having extended dynamic ranges |
US6061091A (en) | 1996-05-30 | 2000-05-09 | Agfa Gevaert N.V. | Detection of and correction for specular reflections in digital image acquisition |
US6075905A (en) | 1996-07-17 | 2000-06-13 | Sarnoff Corporation | Method and apparatus for mosaic image construction |
US5926190A (en) | 1996-08-21 | 1999-07-20 | Apple Computer, Inc. | Method and system for simulating motion in a computer graphics application using image registration and view interpolation |
US6466701B1 (en) | 1997-09-10 | 2002-10-15 | Ricoh Company, Ltd. | System and method for displaying an image indicating a positional relation between partially overlapping images |
JP3667105B2 (en) | 1997-10-01 | 2005-07-06 | 松下電器産業株式会社 | Motion vector detection method and apparatus for implementing the method |
JPH11120185A (en) | 1997-10-09 | 1999-04-30 | Canon Inc | Information processor and method therefor |
US6101285A (en) | 1998-03-23 | 2000-08-08 | Xerox Corporation | Filter for producing continuous tone images from halftone digital images data |
JPH11341292A (en) | 1998-05-29 | 1999-12-10 | Canon Inc | Device and method for processing image and storage medium |
JP4245699B2 (en) | 1998-09-16 | 2009-03-25 | オリンパス株式会社 | Imaging device |
US6864916B1 (en) | 1999-06-04 | 2005-03-08 | The Trustees Of Columbia University In The City Of New York | Apparatus and method for high dynamic range imaging using spatially varying exposures |
US6975755B1 (en) | 1999-11-25 | 2005-12-13 | Canon Kabushiki Kaisha | Image processing method and apparatus |
JP3750462B2 (en) | 2000-02-22 | 2006-03-01 | コニカミノルタフォトイメージング株式会社 | Digital camera and recording medium |
US7298402B2 (en) | 2000-10-26 | 2007-11-20 | Olympus Corporation | Image-pickup apparatus with expanded dynamic range capabilities |
US7053953B2 (en) * | 2001-12-21 | 2006-05-30 | Eastman Kodak Company | Method and camera system for blurring portions of a verification image to show out of focus areas in a captured archival image |
US7173666B1 (en) | 2002-08-22 | 2007-02-06 | Smal Camera Technologies | System and method for displaying a non-standard aspect ratio image on a standard aspect ratio monitor |
JP3898606B2 (en) | 2002-09-12 | 2007-03-28 | 株式会社東芝 | Motion vector detection method and apparatus, and frame interpolation image creation method and apparatus |
JP3804617B2 (en) | 2003-02-14 | 2006-08-02 | コニカミノルタフォトイメージング株式会社 | Image processing apparatus and method |
US6879731B2 (en) | 2003-04-29 | 2005-04-12 | Microsoft Corporation | System and process for generating high dynamic range video |
US7158168B2 (en) | 2003-05-13 | 2007-01-02 | The United States Of America As Represented By The Secretary Of The Navy | Auto Focus and Zoom Controller for controlling multiple cameras |
US7142723B2 (en) | 2003-07-18 | 2006-11-28 | Microsoft Corporation | System and process for generating high dynamic range images from multiple exposures of a moving scene |
CA2442603C (en) | 2003-10-01 | 2016-11-22 | Aryan Saed | Digital composition of a mosaic image |
WO2005047545A2 (en) | 2003-11-04 | 2005-05-26 | Applera Corporation | Microarray controls |
US7330592B2 (en) | 2004-01-27 | 2008-02-12 | Samsung Electronics Co., Ltd. | Method and apparatus for detecting the location and luminance transition range of slant image edges |
US20050243176A1 (en) | 2004-04-30 | 2005-11-03 | James Wu | Method of HDR image processing and manipulation |
US7667764B2 (en) | 2004-06-04 | 2010-02-23 | Konica Minolta Holdings, Inc. | Image sensing apparatus |
US7522779B2 (en) | 2004-06-30 | 2009-04-21 | Accuray, Inc. | Image enhancement method and system for fiducial-less tracking of treatment targets |
EP1829364A2 (en) | 2004-12-07 | 2007-09-05 | Bright Imaging Ltd. | Method and apparatus for imaging of scenes having large intensity variance |
US7239805B2 (en) | 2005-02-01 | 2007-07-03 | Microsoft Corporation | Method and system for combining multiple exposure images having scene and camera motion |
US7626614B1 (en) | 2005-02-15 | 2009-12-01 | Apple Inc. | Transfer function and high dynamic range images |
AU2006244201B2 (en) | 2005-05-09 | 2012-09-06 | Lockheed Martin Corporation | Continuous extended range image processing |
JP4420294B2 (en) | 2005-06-28 | 2010-02-24 | エルジー ディスプレイ カンパニー リミテッド | Midian filtering method and apparatus |
JP4530961B2 (en) | 2005-06-30 | 2010-08-25 | オリンパスイメージング株式会社 | Electronic image stabilization device |
EP1924966B1 (en) | 2005-08-08 | 2009-04-29 | MEP Imaging Technologies Ltd. | Adaptive exposure control |
WO2007031696A1 (en) | 2005-09-13 | 2007-03-22 | Arm Limited | Cache miss detection in a data processing apparatus |
JP4567593B2 (en) | 2005-12-27 | 2010-10-20 | 三星デジタルイメージング株式会社 | Imaging apparatus and imaging method |
WO2007075065A1 (en) | 2005-12-29 | 2007-07-05 | Mtekvision Co., Ltd. | Device of processing dead pixel |
US7903168B2 (en) | 2006-04-06 | 2011-03-08 | Eastman Kodak Company | Camera and method with additional evaluation image capture based on scene brightness changes |
US8189100B2 (en) * | 2006-07-25 | 2012-05-29 | Qualcomm Incorporated | Mobile device with dual digital camera sensors and methods of using the same |
US20080030592A1 (en) | 2006-08-01 | 2008-02-07 | Eastman Kodak Company | Producing digital image with different resolution portions |
US7944485B2 (en) | 2006-08-30 | 2011-05-17 | Micron Technology, Inc. | Method, apparatus and system for dynamic range estimation of imaged scenes |
TW200820123A (en) | 2006-10-20 | 2008-05-01 | Primax Electronics Ltd | Method and system of generating high dynamic range image corresponding to specific scene |
US7859588B2 (en) * | 2007-03-09 | 2010-12-28 | Eastman Kodak Company | Method and apparatus for operating a dual lens camera to augment an image |
US7548689B2 (en) | 2007-04-13 | 2009-06-16 | Hewlett-Packard Development Company, L.P. | Image processing method |
US20080278633A1 (en) | 2007-05-09 | 2008-11-13 | Mikhail Tsoupko-Sitnikov | Image processing method and image processing apparatus |
KR101341095B1 (en) | 2007-08-23 | 2013-12-13 | 삼성전기주식회사 | Apparatus and method for capturing images having optimized quality under night scene conditions |
US8026955B2 (en) * | 2007-08-30 | 2011-09-27 | Honda Motor Co., Ltd. | Camera exposure controller including imaging devices for capturing an image using stereo-imaging |
WO2009029810A1 (en) | 2007-08-31 | 2009-03-05 | Historx, Inc. | Automatic exposure time selection for imaging tissue |
JP4783461B2 (en) | 2007-09-25 | 2011-09-28 | 富士通株式会社 | Image composition apparatus and method |
JP4424402B2 (en) | 2007-09-28 | 2010-03-03 | ソニー株式会社 | Imaging apparatus, imaging control method, and imaging control program |
KR100911814B1 (en) | 2007-10-08 | 2009-08-12 | 성균관대학교산학협력단 | Stereo-image matching error removal apparatus and removal methord using the same |
US7777804B2 (en) | 2007-10-26 | 2010-08-17 | Omnivision Technologies, Inc. | High dynamic range sensor with reduced line memory for color interpolation |
US8600189B2 (en) | 2007-11-12 | 2013-12-03 | Qualcomm Incorporated | Block-based image stabilization |
US8059891B2 (en) | 2007-12-30 | 2011-11-15 | Intel Corporation | Markov stationary color descriptor |
KR101442610B1 (en) | 2008-02-18 | 2014-09-19 | 삼성전자주식회사 | Digital photographing apparatus, method for controlling the same, and recording medium storing program to implement the method |
US8482620B2 (en) | 2008-03-11 | 2013-07-09 | Csr Technology Inc. | Image enhancement based on multiple frames and motion estimation |
KR20090098197A (en) | 2008-03-13 | 2009-09-17 | 삼성디지털이미징 주식회사 | Digital photographing apparatus to control flash lighting, controlling method for the same, and recording medium which records the program for carrying the same method |
JP5054583B2 (en) | 2008-03-17 | 2012-10-24 | 株式会社リコー | Imaging device |
US20090244301A1 (en) | 2008-04-01 | 2009-10-01 | Border John N | Controlling multiple-image capture |
KR101599875B1 (en) | 2008-04-17 | 2016-03-14 | 삼성전자주식회사 | Method and apparatus for multimedia encoding based on attribute of multimedia content, method and apparatus for multimedia decoding based on attributes of multimedia content |
KR101257942B1 (en) | 2008-04-23 | 2013-04-23 | 고려대학교 산학협력단 | Pre-processing method and apparatus in Wide Dynamic Range image processing |
US8145015B2 (en) | 2008-04-25 | 2012-03-27 | Intel Corporation | Device, system, and method for indexing digital image frames |
US8724921B2 (en) | 2008-05-05 | 2014-05-13 | Aptina Imaging Corporation | Method of capturing high dynamic range images with objects in the scene |
FR2931288B1 (en) | 2008-05-19 | 2010-08-20 | Thierry Prigent | METHOD FOR RECORDING IMAGES AND RESET DATA FOR THESE IMAGES |
KR20090127602A (en) | 2008-06-09 | 2009-12-14 | 삼성전자주식회사 | Method and apparatus for obtaining images using entropy |
US8035728B2 (en) | 2008-06-27 | 2011-10-11 | Aptina Imaging Corporation | Method and apparatus providing rule-based auto exposure technique preserving scene dynamic range |
US8134589B2 (en) | 2008-07-17 | 2012-03-13 | Eastman Kodak Company | Zoom by multiple image capture |
JP4561912B2 (en) | 2008-09-12 | 2010-10-13 | ソニー株式会社 | Imaging apparatus, imaging method, and program |
CN101394487B (en) | 2008-10-27 | 2011-09-14 | 华为技术有限公司 | Image synthesizing method and system |
JP5230376B2 (en) | 2008-11-28 | 2013-07-10 | 三星電子株式会社 | Imaging apparatus and imaging method |
KR101520068B1 (en) | 2008-12-16 | 2015-05-13 | 삼성전자 주식회사 | Apparatus and method of blending multiple image |
US8339475B2 (en) | 2008-12-19 | 2012-12-25 | Qualcomm Incorporated | High dynamic range image combining |
TWI395051B (en) | 2008-12-31 | 2013-05-01 | Altek Corp | Panorama Image Automatic Shooting Method for Digital Camera |
US8675122B2 (en) * | 2009-01-16 | 2014-03-18 | Microsoft Corporation | Determining exposure time in a digital camera |
TWI393992B (en) | 2009-02-05 | 2013-04-21 | Nat Univ Chung Cheng | High dynamic range image synthesis method |
US8228400B2 (en) | 2009-04-17 | 2012-07-24 | Sony Corporation | Generation of simulated long exposure images in response to multiple short exposures |
US8525900B2 (en) | 2009-04-23 | 2013-09-03 | Csr Technology Inc. | Multiple exposure high dynamic range image capture |
US8237813B2 (en) | 2009-04-23 | 2012-08-07 | Csr Technology Inc. | Multiple exposure high dynamic range image capture |
JP2010279016A (en) | 2009-04-30 | 2010-12-09 | Sony Corp | Solid-state imaging device, driving method thereof, and imaging apparatus |
US8553106B2 (en) | 2009-05-04 | 2013-10-08 | Digitaloptics Corporation | Dual lens digital zoom |
KR101351100B1 (en) | 2009-06-16 | 2014-01-14 | 인텔 코오퍼레이션 | Camera applications in a handheld device |
JP5319415B2 (en) | 2009-06-22 | 2013-10-16 | キヤノン株式会社 | Image processing apparatus and image processing method |
JP2011010108A (en) | 2009-06-26 | 2011-01-13 | Seiko Epson Corp | Imaging control apparatus, imaging apparatus, and imaging control method |
US8644644B2 (en) | 2009-09-14 | 2014-02-04 | Adobe Systems Incorporation | Methods and apparatus for blending images |
KR101604068B1 (en) | 2009-09-22 | 2016-03-17 | 삼성전자주식회사 | High dynamic range imaging apparatus and method |
GB2486348B (en) | 2009-10-08 | 2014-11-12 | Ibm | Method and system for transforming a digital image from a low dynamic range (LDR) image to a high dynamic range (HDR) image |
US8582802B2 (en) | 2009-10-09 | 2013-11-12 | Edgenet, Inc. | Automatic method to generate product attributes based solely on product images |
US20110149111A1 (en) | 2009-12-22 | 2011-06-23 | Prentice Wayne E | Creating an image using still and preview |
JP4983905B2 (en) | 2009-12-25 | 2012-07-25 | カシオ計算機株式会社 | Imaging apparatus, 3D modeling data generation method, and program |
KR101575803B1 (en) | 2010-01-15 | 2015-12-09 | 삼성전자 주식회사 | Method and apparatus of generating high sensitivity image in dark environment |
WO2011093994A1 (en) | 2010-01-27 | 2011-08-04 | Thomson Licensing | High dynamic range (hdr) image synthesis with user input |
US8339508B2 (en) | 2010-02-22 | 2012-12-25 | Csr Technology Inc. | Method and apparatus for low-light imaging enhancement |
JP5445235B2 (en) | 2010-03-09 | 2014-03-19 | ソニー株式会社 | Image processing apparatus, image processing method, and program |
US20110255786A1 (en) * | 2010-04-20 | 2011-10-20 | Andrew Hunter | Method and apparatus for determining flicker in the illumination of a subject |
EP2569615A4 (en) | 2010-05-12 | 2014-01-15 | Li Cor Inc | Wide dynamic range imaging |
US8760537B2 (en) | 2010-07-05 | 2014-06-24 | Apple Inc. | Capturing and rendering high dynamic range images |
JP2012029029A (en) | 2010-07-23 | 2012-02-09 | Seiko Epson Corp | Image processing device, image processing method and imaging device |
US8625013B2 (en) | 2010-08-23 | 2014-01-07 | Red.Com, Inc. | Multi-exposure imaging |
US8994843B2 (en) | 2010-09-01 | 2015-03-31 | Qualcomm Incorporated | High dynamic range image sensor |
US9544498B2 (en) | 2010-09-20 | 2017-01-10 | Mobile Imaging In Sweden Ab | Method for forming images |
US20120075489A1 (en) | 2010-09-24 | 2012-03-29 | Nishihara H Keith | Zoom camera image blending technique |
US8493500B2 (en) * | 2010-10-27 | 2013-07-23 | Apple Inc. | Auto exposure blowout prevention |
US8462221B2 (en) | 2010-11-03 | 2013-06-11 | Eastman Kodak Company | Method for producing high dynamic range images |
EP2642746B1 (en) * | 2010-11-18 | 2015-06-10 | Panasonic Intellectual Property Corporation of America | Image capture device, image capture method |
TWI433533B (en) | 2010-11-22 | 2014-04-01 | Altek Corp | Image capturing device and image synthesis method thereof |
CN103026384B (en) | 2011-01-20 | 2016-08-03 | 松下知识产权经营株式会社 | Feature deriving means, feature extracting method and image processing apparatus |
US8428308B2 (en) | 2011-02-04 | 2013-04-23 | Apple Inc. | Estimating subject motion for capture setting determination |
US8611655B2 (en) | 2011-02-04 | 2013-12-17 | Apple Inc. | Hue-based color matching |
KR101226751B1 (en) | 2011-02-18 | 2013-01-25 | (주)앤드웍스 | Design code pattern representing information interpreted with a digital device, and operating system thereof |
WO2012110894A1 (en) | 2011-02-18 | 2012-08-23 | DigitalOptics Corporation Europe Limited | Dynamic range extension by combining differently exposed hand-held device-acquired images |
US8873882B2 (en) | 2011-02-28 | 2014-10-28 | Aptina Imaging Corporation | Blooming filter for multiple exposure high dynamic range image sensors |
US8593548B2 (en) | 2011-03-28 | 2013-11-26 | Aptina Imaging Corporation | Apparataus and method of automatic color shading removal in CMOS image sensors |
US9813632B2 (en) * | 2011-05-19 | 2017-11-07 | Foveon, Inc. | Method of adjusting digital camera image processing parameters |
US8620070B2 (en) | 2011-05-31 | 2013-12-31 | Korean Electronics Technology Institute | Corresponding image processing method for compensating colour |
US9077917B2 (en) | 2011-06-09 | 2015-07-07 | Apple Inc. | Image sensor having HDR capture capability |
US8754977B2 (en) * | 2011-07-28 | 2014-06-17 | Hewlett-Packard Development Company, L.P. | Second camera for finding focal target in poorly exposed region of frame taken by first camera |
JP2013038504A (en) | 2011-08-04 | 2013-02-21 | Sony Corp | Imaging device, image processing method and program |
JP5780885B2 (en) | 2011-08-26 | 2015-09-16 | キヤノン株式会社 | Imaging device, control method thereof, and control program |
US9106879B2 (en) | 2011-10-04 | 2015-08-11 | Samsung Electronics Co., Ltd. | Apparatus and method for automatic white balance with supplementary sensors |
US8913153B2 (en) | 2011-10-06 | 2014-12-16 | Aptina Imaging Corporation | Imaging systems and methods for generating motion-compensated high-dynamic-range images |
US8200020B1 (en) | 2011-11-28 | 2012-06-12 | Google Inc. | Robust image alignment using block sums |
KR101225482B1 (en) * | 2012-02-15 | 2013-01-23 | 인텔 코오퍼레이션 | Digital image processing method, apparatus, and computer-readable recording medium |
US9137456B2 (en) | 2012-06-06 | 2015-09-15 | Apple Inc. | Intelligent auto-exposure bracketing |
US8446481B1 (en) | 2012-09-11 | 2013-05-21 | Google Inc. | Interleaved capture for high dynamic range image acquisition and synthesis |
US8866927B2 (en) | 2012-12-13 | 2014-10-21 | Google Inc. | Determining an image capture payload burst structure based on a metering image capture sweep |
US8866928B2 (en) | 2012-12-18 | 2014-10-21 | Google Inc. | Determining exposure times using split paxels |
TWI511559B (en) | 2013-02-07 | 2015-12-01 | Altek Semiconductor Corp | Image processing method |
CN105474157A (en) * | 2013-05-09 | 2016-04-06 | 亚马逊技术股份有限公司 | Mobile device interfaces |
US8885976B1 (en) | 2013-06-20 | 2014-11-11 | Cyberlink Corp. | Systems and methods for performing image fusion |
-
2013
- 2013-09-30 US US14/041,905 patent/US9615012B2/en active Active
-
2014
- 2014-09-18 WO PCT/US2014/056413 patent/WO2015047877A1/en active Application Filing
- 2014-09-18 CN CN201480065311.4A patent/CN105765967B/en active Active
- 2014-09-18 EP EP14848412.4A patent/EP3053332B1/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1848913A (en) * | 2005-04-05 | 2006-10-18 | 奥林巴斯映像株式会社 | Digital camera |
CN102761690A (en) * | 2011-04-28 | 2012-10-31 | 佳能株式会社 | Imaging apparatus and method for controlling the same |
Also Published As
Publication number | Publication date |
---|---|
US20150092066A1 (en) | 2015-04-02 |
US9615012B2 (en) | 2017-04-04 |
WO2015047877A1 (en) | 2015-04-02 |
EP3053332B1 (en) | 2020-11-04 |
EP3053332A4 (en) | 2017-05-24 |
EP3053332A1 (en) | 2016-08-10 |
CN105765967A (en) | 2016-07-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN105765967B (en) | The method, system and medium of the setting of first camera are adjusted using second camera | |
US8964060B2 (en) | Determining an image capture payload burst structure based on a metering image capture sweep | |
US9172888B2 (en) | Determining exposure times using split paxels | |
Battiato et al. | Exposure correction for imaging devices: an overview | |
EP1924966B1 (en) | Adaptive exposure control | |
US7916181B2 (en) | Method and device for creating high dynamic range pictures from multiple exposures | |
CN106797453B (en) | Image processing apparatus, photographic device, image processing method and image processing program | |
JP6259185B2 (en) | IMAGING DEVICE, ITS CONTROL METHOD, PROGRAM, AND STORAGE MEDIUM | |
CN106358030B (en) | Image processing apparatus and image processing method | |
JP3873994B2 (en) | Imaging apparatus and image acquisition method | |
CN105814875A (en) | Selecting camera pairs for stereoscopic imaging | |
US9087391B2 (en) | Determining an image capture payload burst structure | |
WO2012098768A1 (en) | Image processing device, image processing method, image processing program, and photography device | |
US9961319B2 (en) | Image processing apparatus and control method thereof | |
CN105516611A (en) | An imaging device and a shooting method | |
CN103685928B (en) | Image processing apparatus and image processing method | |
JP7077100B2 (en) | Imaging device and its control method, and program | |
JP2013012124A (en) | Image processing device, imaging device, and program | |
CN104144286A (en) | Imaging apparatus and imaging method | |
WO2017071560A1 (en) | Picture processing method and device | |
CN113989387A (en) | Camera shooting parameter adjusting method and device and electronic equipment | |
Granados et al. | Contrast-use metrics for tone mapping images | |
Watson et al. | Documenting Archaeological Mortuary Features using High Dynamic Range (HDR) Imaging | |
Jantunen | Software Implementation of Contrast-Based Autofocus in Mobile Camera System | |
JP2018182678A (en) | Imaging apparatus |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
CB02 | Change of applicant information | ||
GR01 | Patent grant | ||
GR01 | Patent grant |