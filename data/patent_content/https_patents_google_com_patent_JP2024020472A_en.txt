JP2024020472A - Semi-delegated calls with automated assistants on behalf of human participants - Google Patents
Semi-delegated calls with automated assistants on behalf of human participants Download PDFInfo
- Publication number
- JP2024020472A JP2024020472A JP2023196592A JP2023196592A JP2024020472A JP 2024020472 A JP2024020472 A JP 2024020472A JP 2023196592 A JP2023196592 A JP 2023196592A JP 2023196592 A JP2023196592 A JP 2023196592A JP 2024020472 A JP2024020472 A JP 2024020472A
- Authority
- JP
- Japan
- Prior art keywords
- user
- call
- client device
- assisted
- given
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000004044 response Effects 0.000 claims abstract description 105
- 238000000034 method Methods 0.000 claims description 113
- 238000009877 rendering Methods 0.000 claims description 48
- 238000012545 processing Methods 0.000 claims description 27
- 230000000977 initiatory effect Effects 0.000 claims description 19
- 238000012544 monitoring process Methods 0.000 claims description 18
- 230000000007 visual effect Effects 0.000 claims description 13
- 230000008685 targeting Effects 0.000 claims 1
- 238000010586 diagram Methods 0.000 abstract description 7
- 230000000875 corresponding effect Effects 0.000 description 72
- 230000003993 interaction Effects 0.000 description 36
- 230000008569 process Effects 0.000 description 25
- 230000009471 action Effects 0.000 description 13
- 230000015572 biosynthetic process Effects 0.000 description 11
- 238000003786 synthesis reaction Methods 0.000 description 11
- 238000013518 transcription Methods 0.000 description 11
- 230000035897 transcription Effects 0.000 description 11
- 238000004891 communication Methods 0.000 description 8
- 230000008859 change Effects 0.000 description 5
- 238000000275 quality assurance Methods 0.000 description 5
- 230000008901 benefit Effects 0.000 description 3
- 238000012790 confirmation Methods 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 2
- 238000001914 filtration Methods 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 241000473391 Archosargus rhomboidalis Species 0.000 description 1
- 241001155433 Centrarchus macropterus Species 0.000 description 1
- 230000003213 activating effect Effects 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000008447 perception Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000026676 system process Effects 0.000 description 1
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/487—Arrangements for providing information services, e.g. recorded voice services or time announcements
- H04M3/493—Interactive information services, e.g. directory enquiries ; Arrangements therefor, e.g. interactive voice response [IVR] systems or voice portals
- H04M3/4936—Speech interaction details
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/42204—Arrangements at the exchange for service or number selection by voice
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
- G06F16/90332—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/04—Details of speech synthesis systems, e.g. synthesiser structure or memory management
- G10L13/047—Architecture of speech synthesisers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/2866—Architectures; Arrangements
- H04L67/30—Profiles
- H04L67/306—User profiles
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72403—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/50—Centralised arrangements for answering calls; Centralised arrangements for recording messages for absent or busy subscribers ; Centralised arrangements for recording messages
- H04M3/51—Centralised call answering arrangements requiring operator intervention, e.g. call or contact centers for telemarketing
- H04M3/5166—Centralised call answering arrangements requiring operator intervention, e.g. call or contact centers for telemarketing in combination with interactive voice response systems or voice portals, e.g. as front-ends
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2201/00—Electronic components, circuits, software, systems or apparatus used in telephone systems
- H04M2201/39—Electronic components, circuits, software, systems or apparatus used in telephone systems using speech synthesis
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2201/00—Electronic components, circuits, software, systems or apparatus used in telephone systems
- H04M2201/40—Electronic components, circuits, software, systems or apparatus used in telephone systems using speech recognition
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2250/00—Details of telephonic subscriber devices
- H04M2250/74—Details of telephonic subscriber devices with voice recognition means
Abstract
【課題】実装は、所与のユーザの代理として支援された通話を開始するために自動アシスタントを使用することを対象とする。【解決手段】アシスタントは、支援された通話中に、支援された通話のさらなるユーザから、アシスタントに知られていない情報の要求を受け取ることができる。それに応じて、アシスタントは、情報に関するプロンプトをレンダリングし、所与のユーザからの応答入力を待つ間、支援された通話のために既に解決された値を使用して、支援された通話を継続することができる。応答入力が閾値の継続時間内に受け取られる場合、応答入力に対応する合成音声が、支援された通話の一部としてレンダリングされる。実装は、追加的または代替的に、所与のユーザとさらなるユーザとの間の進行中の通話中に、進行中の通話中にさらなるユーザによって要求された値に基づく出力を提供するために自動アシスタントを使用することを対象とする。【選択図】図1An implementation is directed to using an automated assistant to initiate assisted calls on behalf of a given user. The assistant may receive requests for information unknown to the assistant from further users of the assisted call during an assisted call. In response, the assistant renders a prompt for information and continues the assisted call while waiting for a response input from the given user, using the values already resolved for the assisted call. be able to. If the response input is received within the threshold duration, synthetic speech corresponding to the response input is rendered as part of the assisted call. The implementation may additionally or alternatively be operable to automatically, during an ongoing call between a given user and a further user, provide output based on values requested by the further user during the ongoing call. Intended for use with assistants. [Selection diagram] Figure 1
Description
自動アシスタントは、スマートフォン、タブレットコンピュータ、ウェアラブルデバイス、自動車システム、スタンドアロンのパーソナルアシスタントデバイスなどの様々なコンピューティングデバイスを介してユーザによってインタラクションされ得る。自動アシスタントは、ユーザから(たとえば、発話された、タッチ、および/またはタイピングされた)入力を受け取り、(たとえば、視覚的および/または聴覚的(audible))応答出力によって応答する。 Automated assistants can be interacted with by users through a variety of computing devices, such as smartphones, tablet computers, wearable devices, automotive systems, and standalone personal assistant devices. An automated assistant receives input (eg, spoken, touched, and/or typed) from a user and responds with a responsive output (eg, visual and/or audible).
ユーザは、自動アシスタントとインタラクションして、自動アシスタントにユーザの代理としてアクションを実行させることができる。一例として、自動アシスタントは、所与のアクションを実行するためにユーザの代理として電話をかけることができ、アクションを実行するためにさらなるユーザと対話を行うことができる。たとえば、ユーザは、自動アシスタントがユーザの代理として電話でレストランの予約をすることを要求するユーザ入力を与えることができる。自動アシスタントは、特定のレストランとの電話を開始することができ、予約をするために、特定のレストランに関連するさらなるユーザに予約情報を提供することができる。そして、自動アシスタントは、ユーザの代理としてレストランの予約が成功裏に行われたかどうかをユーザに通知することができる。 A user can interact with an automated assistant to have the automated assistant perform actions on the user's behalf. As an example, an automated assistant can make a phone call on behalf of a user to perform a given action, and can interact with further users to perform the action. For example, a user may provide user input requesting that the automated assistant make a restaurant reservation over the phone on the user's behalf. The automated assistant may initiate a call with the particular restaurant and may provide reservation information to further users associated with the particular restaurant to make the reservation. The automated assistant can then notify the user whether the restaurant reservation was successfully made on the user's behalf.
しかし、ユーザの代理として自動アシスタントによって実行される何らかのアクションに関して、自動アシスタントは、アクションを完全に実行するのに十分な情報を知らない場合がある。一例として、自動アシスタントが上述のユーザの代理として電話でレストランの予約をすると仮定し、さらに、特定のレストランに関連するさらなるユーザが自動アシスタントに知られていない情報を要求すると仮定する。一部の自動アシスタントは、要求された情報が知られていないと判定し、レストランの予約を完了するために通話に能動的に参加するようにユーザに要求する通知をユーザに提供することができる。しかし、ユーザが能動的に通話に参加するのを待つことは、通話を長引かせ、通話中に使用される計算および/またはネットワークリソースの関連する使用も長引かせ得る。追加的または代替的に、ユーザが通話に参加することができない場合もある。これは、通話の失敗につながり、自動アシスタントおよび/またはユーザが後でアクションを実行することを要求し、それによって、最初の通話中に自動アシスタントによってアクションが成功裏に実行された場合よりも多くの計算および/またはネットワークリソースを消費し得る。 However, for any action performed by an automated assistant on behalf of a user, the automated assistant may not know enough information to fully perform the action. As an example, suppose an automated assistant makes a restaurant reservation over the phone on behalf of the user mentioned above, and further assume that a further user associated with a particular restaurant requests information unknown to the automated assistant. Some automated assistants may determine that the requested information is not known and provide the user with a notification requesting the user to actively participate in the call to complete the restaurant reservation. . However, waiting for a user to actively participate in a call may prolong the call and the associated use of computational and/or network resources used during the call. Additionally or alternatively, the user may not be able to participate in the call. This can lead to call failures and require the automated assistant and/or the user to perform an action later, thereby making the call more difficult than if the action had been successfully performed by the automated assistant during the initial call. may consume computational and/or network resources.
一部の実装は、所与のユーザの代理としてタスクを実行するためにエンティティ(entity)との支援された通話(assisted call)を実行するために自動アシスタントを使用することを対象とする。支援された通話は、自動アシスタントと、エンティティに関連するさらなるユーザとの間で行われる。自動アシスタントは、タスクおよび/またはエンティティに関連付けられるパラメータの解決された値を使用して、支援された通話を実行することができる。支援された通話を実行することは、自動アシスタントによって、支援された通話において、さらなるユーザが聴覚的に知覚可能な合成音声のインスタンスを聴覚的にレンダリングすることを含み得る。支援された通話において合成音声のインスタンスをレンダリングすることは、さらなるユーザ(ただし、所与のユーザとは限らない)が聴覚的に知覚可能である合成音声を支援された通話に差し込むことを含み得る。合成音声のインスタンスは、それぞれ、解決された値のうちの1つもしくは複数に基づいて生成されることが可能であり、および/または支援された通話中のさらなるユーザの発言に応じるように生成されることが可能である。また、支援された通話を実行することは、自動アシスタントによって、さらなるユーザの発言をキャプチャする支援された通話のオーディオデータの自動音声認識を実行して、発言の認識されたテキストを生成することと、支援された通話においてレンダリングするための、発言に応じる合成音声のインスタンスを決定する際に認識されたテキストを使用することとを含み得る。 Some implementations are directed to using automated assistants to perform assisted calls with entities to perform tasks on behalf of a given user. An assisted call is made between the automated assistant and a further user associated with the entity. The automated assistant may use the resolved values of parameters associated with the task and/or entity to perform the assisted call. Performing the assisted call may include audibly rendering, by the automated assistant, an instance of synthetic speech that is audibly perceptible to a further user in the assisted call. Rendering an instance of synthetic speech in an assisted call may include injecting into the assisted call synthetic speech that is audibly perceptible to a further user (but not necessarily a given user). . Each instance of synthesized speech may be generated based on one or more of the resolved values and/or generated in response to further user utterances during the assisted call. It is possible to Performing an assisted call may also include performing automatic speech recognition of the audio data of the assisted call to capture further user utterances by the automated assistant to generate recognized text of the utterances. , using the recognized text in determining an instance of synthetic speech responsive to the utterance for rendering in the assisted call.
それらの実装の一部は、支援された通話の実行中に、さらなるユーザの発言がさらなるパラメータに関連する情報の要求を含み、さらなるパラメータに関して値が自動的に決定可能でないと判定することをさらに対象とする。それに応じて、自動アシスタントは、所与のユーザに対して音声のおよび/または視覚的な通知(たとえば、プロンプト)をレンダリングさせることができ、通知は、さらなるパラメータの値を解決することに関連するさらなるユーザ入力を要求する。一部の実装において、通知は、進行中の通話外でレンダリングされる(すなわち、進行中の通話の一部として差し込まれない)が、所与のユーザが値を確認し、進行中の通話内で値を伝達することを可能にするために所与のユーザが知覚可能であることが可能である。通知に応じる何らかのさらなるユーザ入力を受け取るまで、自動アシスタントは、支援された通話を継続することができる。たとえば、自動アシスタントは、通知に応じるさらなるユーザ入力を待たずに、合成音声の前のインスタンスでまだ伝えられていない、既に解決された値に基づく合成音声のインスタンスを、通話中に先回りして提供し得る。通知に応じてさらなるユーザ入力が与えられ、さらなるパラメータの値がさらなるユーザ入力に基づいて解決可能である場合、自動アシスタントは、支援された通話を継続した後、さらなるパラメータの解決された値を伝えるさらなる合成音声を提供することができる。通知に応じるさらなるユーザ入力を待たずに支援された通話を継続することによって、タスクを完了するために必要な値が、さらなるパラメータのさらなる値を待つ間に、支援された通話中に伝えることが可能であり、さらなる値が、(受け取られる場合)後で提供されることが可能である。これらの方法およびその他の方法で、支援された通話は、より迅速に終了することができ、それによって、支援された通話を実行する際にコンピュータおよび/またはネットワークリソースが利用される全体の継続時間を減らす。 Some of those implementations further provide that, during the execution of an assisted call, further user utterances include requests for information related to further parameters and determine that for the further parameters a value is not automatically determinable. set to target. Accordingly, the automated assistant may cause an audio and/or visual notification (e.g., a prompt) to be rendered to a given user, the notification relating to resolving the value of the further parameter. Request further user input. In some implementations, notifications are rendered outside of an ongoing call (i.e., not injected as part of an ongoing call), but if a given user confirms the value and It is possible for a given user to be perceivable in order to be able to convey values. The automated assistant may continue making the assisted call until it receives some further user input in response to the notification. For example, an automated assistant can proactively provide instances of synthesized speech during a call based on already resolved values that have not yet been conveyed in a previous instance of synthesized speech, without waiting for further user input in response to a notification. It is possible. If further user input is given in response to the notification and the value of the further parameter is resolvable based on the further user input, the automated assistant communicates the resolved value of the further parameter after continuing the assisted call. Further synthetic speech can be provided. By continuing the assisted call without waiting for further user input in response to the notification, the values needed to complete the task can be communicated during the assisted call while waiting for further values for further parameters. Possible and further values (if received) can be provided later. With these and other methods, assisted calls can be completed more quickly, thereby reducing the overall duration of computer and/or network resources utilized in performing the assisted call. Reduce.
さらなるパラメータの値を解決することに関連するさらなるユーザ入力を要求する通知をレンダリングする一部の実装において、通知および/または通知をレンダリングするための1つもしくは複数のプロパティは、支援された通話の状態および/または支援された通話で使用されているクライアントデバイスの状態に基づいて動的に決定され得る。たとえば、クライアントデバイスの状態が、所与のユーザが支援された通話を能動的に監視していることを示す場合、通知は、視覚のみの通知であることが可能であり、および/またはより低いボリュームの聴覚的構成要素を含むことが可能である。一方、クライアントデバイスの状態が、所与のユーザが支援された通話を能動的に監視していないことを示す場合、通知は、少なくとも聴覚的構成要素を含むことが可能であり、および/または聴覚的構成要素は、より高い音量でレンダリングされることが可能である。クライアントデバイスの状態は、たとえば、クライアントデバイスのセンサ(たとえば、ジャイロスコープ、加速度計、存在センサ、および/もしくはその他のセンサ)ならびに/またはユーザに関連するその他のクライアントデバイスのセンサからのセンサデータに基づくことが可能である。さらに別の例として、支援された通話の状態が、通知を提供する時点で複数の解決された値がまだ伝えられていないことを示す場合、通知は、視覚のみの通知であることが可能であり、および/またはより低いボリュームの聴覚的構成要素を含むことが可能である。一方、支援された通話の状態が、通知を提供する時点で1つの解決された値のみがまだ伝えられていない(またはすべての解決された値が既に伝えられた)ことを示す場合、通知は、少なくとも聴覚的構成要素を含むことが可能であり、および/または聴覚的構成要素は、より高い音量でレンダリングされることが可能である。より広く、クライアントデバイスの状態が、ユーザが通話を能動的に監視していないことを示すとき、および/または会話の状態が、会話を有意義に継続する継続時間が比較的短いことを示すとき、実装は、より嵌入的(intrusive)な通知を提供しようとすることができる。一方、実装は、クライアントデバイスの状態が、ユーザが通話を能動に監視していることを示すとき、および/または会話の状態が、会話を有意義に継続する継続時間が比較的長いことを示すとき、より嵌入的でない通知を提供しようとすることができる。より嵌入的な通知は、レンダリングするためにより多くのリソースを必要とし得るが、実装は、より嵌入的な通知をやはり選択的にレンダリングして、より嵌入的な通知をレンダリングするための増加したリソースと、支援された通話をむやみに引き延ばし、および/またはタスクの完了なしに支援された通話を終了するために必要とされるであろう増加したリソースとのバランスをとろうとし得る。 In some implementations that render a notification that requests further user input related to resolving the value of further parameters, the notification and/or one or more properties for rendering the notification are It may be determined dynamically based on the state and/or the state of the client device being used in the assisted call. For example, if client device status indicates that a given user is actively monitoring an assisted call, the notification may be a visual-only notification and/or a lower It is possible to include an auditory component of volume. On the other hand, if the state of the client device indicates that a given user is not actively monitoring an assisted call, the notification may include at least an audible component and/or an audible components can be rendered at higher volumes. The client device state is based, for example, on sensor data from the client device's sensors (e.g., gyroscopes, accelerometers, presence sensors, and/or other sensors) and/or other client device sensors associated with the user. Is possible. As yet another example, a notification may be a visual-only notification if the state of the assisted call indicates that multiple resolved values have not yet been conveyed at the time of providing the notification. and/or may include lower volume auditory components. On the other hand, if the state of the assisted call indicates that only one resolved value has not yet been conveyed (or all resolved values have already been conveyed) at the time of providing the notification, then the notification is , may include at least an auditory component, and/or the auditory component may be rendered at a higher volume. More broadly, when the state of the client device indicates that the user is not actively monitoring the call, and/or when the state of the conversation indicates that the duration of meaningfully continuing the conversation is relatively short; Implementations may attempt to provide more intrusive notifications. On the other hand, when the state of the client device indicates that the user is actively monitoring the call, and/or the state of the conversation indicates that the duration of the conversation is relatively long enough to continue the conversation meaningfully. , may attempt to provide less intrusive notifications. More intrusive notifications may require more resources to render, but implementations may still selectively render more intrusive notifications and use increased resources to render more intrusive notifications. and may attempt to balance the increased resources that would be required to unnecessarily prolong the assisted call and/or terminate the assisted call without completing the task.
一部の実装は、追加的または代替的に、所与のユーザとさらなるユーザとの間の進行中の通話中に、進行中の通話中にさらなるユーザによって要求された値に基づく出力を提供するために自動アシスタントを使用することを対象とする。出力は、先回りして提供されることが可能であり、所与のユーザがアプリケーションを起動し、および/またはアプリケーション内をナビゲートして、独力で値を探すことを防止することが可能である。たとえば、所与のユーザが公益事業会社の担当者との進行中の通話に従事していると仮定し、さらに、公益事業会社の担当者が所与のユーザの住所情報および公益事業会社に関連する口座番号を要求すると仮定する。この例において、所与のユーザは、住所情報を提供する場合があるが、公益事業会社からの受信された電子メールもしくはメッセージを検索し、公益事業会社に関連するウェブサイトを検索し、および/または公益事業会社に関連する口座番号を見つけるためにその他のコンピューティングデバイスのインタラクションを実行しないと公益事業会社に関する口座番号を知らない場合がある。しかし、本明細書において説明される技術を使用することによって、自動アシスタントは、自動アシスタントが口座番号を特定することを要求する所与のユーザからのいかなるユーザ入力とも無関係に公益事業会社に関連する口座番号を容易に特定することができ、公益事業会社の担当者が口座番号を要求したことの特定に応じて、進行中の通話中に所与のユーザおよび/またはさらなるユーザに口座番号を視覚的および/または聴覚的に提供することができる。 Some implementations additionally or alternatively provide, during an ongoing call between a given user and a further user, an output based on a value requested by the further user during the ongoing call. Intended for use with automated assistants. The output can be provided proactively, preventing a given user from launching and/or navigating within the application to look for the value on their own. . For example, suppose a given user is engaged in an ongoing call with a representative from a utility company, and the utility company representative also provides address information for the given user and information related to the utility company. Suppose you want to request an account number. In this example, a given user may provide address information, search for received emails or messages from the utility company, search for websites related to the utility company, and/or or may not know the account number associated with the utility company without performing other computing device interactions to find the account number associated with the utility company. However, by using the techniques described herein, the automated assistant can connect to a utility company independently of any user input from a given user that requires the automated assistant to identify an account number. The account number can be easily identified and the account number can be made visible to a given user and/or further users during an ongoing call, depending on the identification of the utility company representative requesting the account number. can be provided visually and/or aurally.
これらのおよびその他の方法で、クライアントデバイスのリソースは、そのようなアプリケーションの起動および/またはそのようなアプリケーションとのインタラクションを防止することによって節約され得る。さらに、先回りして提供される出力によって示される値は、ユーザが独力で値を探さなければならないとした場合よりもより迅速に所与の通話において伝達されることが可能であり、それによって、進行中の通話の全体的な継続時間を短縮する。様々な実装において、自動アシスタントは、進行中の通話中に少なくとも1つの発話された発言をキャプチャするオーディオデータのストリームを処理して、認識されたテキストを生成することができ、少なくとも1つの発話された発言は、所与のユーザまたはさらなるユーザのものである。さらに、自動アシスタントは、認識されたテキストを処理することに基づいて、少なくとも1つの発話された発言がパラメータの情報を要求することを特定し、パラメータに関して、所与のユーザの個人的なアクセス制限されたデータを使用して、パラメータの値が解決可能であると判定することができる。値が解決可能であるとの判定に応じて、出力がレンダリングされ得る。一部の実装において、出力は、進行中の通話外でレンダリングされる(すなわち、進行中の通話の一部として差し込まれない)が、所与のユーザが値を確認し、進行中の通話内で値を伝達することを可能にするために所与のユーザが知覚可能であることが可能である。いくつかの追加的または代替的な実装において、出力は、進行中の通話の一部として、合成音声としてレンダリングされ得る。たとえば、合成音声は、進行中の通話内で自動的にまたは所与のユーザから肯定的なユーザインターフェース入力を受け取るとレンダリングされ得る。所与のユーザとさらなるユーザとの間の進行中の通話中に、進行中の通話中にさらなるユーザによって要求された値に基づく出力を提供する一部の実装において、出力は、値を含む音声入力が進行中の通話中に所与のユーザによって閾値の量の時間内に提供されないとの判定に応じてのみ提供される。これらのおよびその他の方法で、出力を不必要にレンダリングするインスタンスが削減される。 In these and other ways, client device resources may be conserved by preventing the launch of and/or interaction with such applications. Furthermore, the value indicated by the proactively provided output can be conveyed in a given call more quickly than if the user had to look for the value on his own, thereby Reduce the overall duration of ongoing calls. In various implementations, the automated assistant may process a stream of audio data that captures at least one uttered utterance during an ongoing call to generate recognized text, and may process at least one uttered utterance during an ongoing call. The posted utterances may be of a given user or further users. Furthermore, based on processing the recognized text, the automated assistant identifies that at least one spoken utterance requests information for the parameters, and with respect to the parameters, the automated assistant identifies the personal access restrictions of a given user. The data obtained can be used to determine that the value of the parameter is resolvable. Output may be rendered in response to determining that the value is resolvable. In some implementations, the output is rendered outside of an ongoing call (i.e. not injected as part of an ongoing call), but a given user can see the value and use it within an ongoing call. It is possible for a given user to be perceivable in order to be able to convey values. In some additional or alternative implementations, the output may be rendered as synthetic speech as part of an ongoing call. For example, synthetic speech may be rendered automatically within an ongoing call or upon receiving positive user interface input from a given user. In some implementations that provide an output during an ongoing call between a given user and a further user based on a value requested by the further user during the ongoing call, the output includes a voice that includes the value. It is provided only in response to a determination that input is not provided by a given user within a threshold amount of time during an ongoing call. In these and other ways, instances of rendering output unnecessarily are reduced.
上の説明は、本明細書において開示されるほんの一部の実装の概要として提供されている。それらの実装およびその他の実装が、本明細書においてさらに詳細に説明される。 The above description is provided as a summary of just some implementations disclosed herein. Those and other implementations are described in further detail herein.
図1は、本開示の様々な態様を示す例示的な環境のブロック図を示す。クライアントデバイス110が、図1に示され、様々な実装において、ユーザ入力エンジン111、デバイス状態エンジン112、レンダリングエンジン113、スケジューリングエンジン114、音声認識エンジン120A1、自然言語理解(「NLU」)エンジン130A1、および音声合成エンジン140A1を含む。 FIG. 1 depicts a block diagram of an example environment illustrating various aspects of the present disclosure. A client device 110 is shown in FIG. 1 and includes, in various implementations, a user input engine 111, a device state engine 112, a rendering engine 113, a scheduling engine 114, a speech recognition engine 120A1, a natural language understanding ("NLU") engine 130A1, and speech synthesis engine 140A1.
ユーザ入力エンジン111は、クライアントデバイス110における様々な種類のユーザ入力を検出することができる。クライアントデバイス110において検出されるユーザ入力は、クライアントデバイス110のマイクロフォンによって検出される発話された入力、および/もしくは(たとえば、支援された通話中におよび/もしくは支援された通話がまだ呼び出されていないときのその他の進行中の通話中に)さらなるユーザのさらなるクライアントデバイスからクライアントデバイス110に送信されたさらなる発話された入力、クライアントデバイス110のユーザインターフェース入力デバイス(たとえば、タッチスクリーン)によって検出されるタッチ入力、ならびに/またはクライアントデバイス110のユーザインターフェース入力デバイスによって(たとえば、タッチスクリーン上の仮想キーボードによって)検出されるタイピングされた入力を含み得る。エンティティとの(支援されたまたは支援されていない)進行中の通話中のエンティティに関連するさらなるユーザは、たとえば、人間、さらなるクライアントデバイスに関連するさらなる人間の参加者、さらなるユーザのさらなるクライアントデバイスに関連するさらなる自動アシスタント、および/またはその他のさらなるユーザであることが可能である。 User input engine 111 can detect various types of user input at client device 110. User input detected at client device 110 may include spoken input detected by a microphone of client device 110 and/or (e.g., during an assisted call and/or when the assisted call has not yet been called). further spoken input sent to client device 110 from a further client device of a further user (during another ongoing call), a touch detected by a user interface input device (e.g., a touch screen) of client device 110; and/or typed input detected by a user interface input device of client device 110 (eg, by a virtual keyboard on a touch screen). A further user associated with the entity in an ongoing call (assisted or unassisted) with the entity may be, for example, a human being, a further human participant associated with a further client device, a further client device of the further user. It is possible that there are further automated assistants involved and/or other further users.
本明細書において説明される支援された通話および/または進行中の通話は、様々な音声通信プロトコル(たとえば、ボイスオーバインターネットプロトコル(VoIP)、公衆交換電話網(PSTN)、および/またはその他の電話通信プロトコル)を使用して実行され得る。本明細書において説明されるように、合成音声が、支援された通話および/または進行中の通話の一部としてレンダリングされることが可能であり、これは、合成音声が進行中の通話の参加者のうちの少なくとも1人によって知覚可能であり、進行中の通話のオーディオデータの一部を形成するように、合成音声を通話に差し込むことを含み得る。合成音声は、通話のエンドポイント(endpoint)のうちの1つであるクライアントデバイスによって生成されるおよび/もしくは差し込まれることが可能であり、ならびに/またはクライアントデバイスと通信し、通話にも接続されているサーバによって生成されるおよび/もしくは差し込まれることが可能である。やはり本明細書において説明されるように、聴覚的出力が、支援された通話外でレンダリングされることも可能であり、これは、聴覚的出力が通話に接続されたクライアントデバイスのマイクロフォンによって検出され、その結果、通話において知覚可能になる可能性があるが、聴覚的出力を通話に差し込むことを含まない。一部の実装においては、通話外でレンダリングされた聴覚的出力の通話内での知覚を軽減するために、通話が任意でミュートされることが可能であり、および/またはフィルタリングが使用されることが可能である。 The assisted calls and/or ongoing calls described herein may be implemented using various voice communication protocols (e.g., Voice over Internet Protocol (VoIP), Public Switched Telephone Network (PSTN), and/or other telephone communications protocols). As described herein, the synthesized voice may be rendered as part of an assisted call and/or an ongoing call, which means that the synthesized voice participates in the ongoing call. The method may include injecting the synthesized voice into the call so as to be perceivable by at least one of the parties and to form part of the audio data of the call in progress. The synthesized voice can be generated and/or plugged in by a client device that is one of the endpoints of the call and/or is in communication with the client device and is also connected to the call. can be generated and/or plugged in by the server. As also described herein, it is also possible for the auditory output to be rendered outside of the assisted call, where the auditory output is detected by a microphone on a client device connected to the call. , so that it may be perceptible in a call, but does not involve plugging an auditory output into the call. In some implementations, the call may optionally be muted and/or filtering may be used to reduce the perception within the call of auditory output rendered outside the call. is possible.
様々な実装において、(概して、図1において破線によって示される)自動アシスタント115は、支援された通話システム180を使用して、ネットワーク190(たとえば、Wi-Fi、Bluetooth、近距離無線通信(near-field communication)、ローカルエリアネットワーク、広域ネットワーク、および/またはその他のネットワーク)を介してクライアントデバイス110において支援された通話を実行することができる。支援された通話システム180は、様々な実装において、音声認識エンジン120A2、NLUエンジン130A2、音声合成エンジン140A2、および支援された通話エンジン150を含む。自動アシスタント115は、支援された通話システム180を利用して、さらなるユーザとの通話中に、クライアントデバイス110の所与のユーザの代理としてタスクを実行することができる。 In various implementations, automated assistant 115 (generally indicated by dashed lines in FIG. 1) uses assisted calling system 180 to communicate with network 190 (e.g., Wi-Fi, Bluetooth, The assisted call may be performed at the client device 110 via a local area network, a wide area network, and/or other networks). Assisted calling system 180 includes a speech recognition engine 120A2, an NLU engine 130A2, a speech synthesis engine 140A2, and an assisted calling engine 150 in various implementations. Automated assistant 115 may utilize assisted calling system 180 to perform tasks on behalf of a given user of client device 110 during calls with additional users.
さらに、一部の実装においては、クライアントデバイス110の所与のユーザの代理として何らかのタスクを実行する前に、自動アシスタント115は、自動アシスタント115と対話をするために、さらなるユーザから同意を得ることが可能である。たとえば、自動アシスタント115は、支援された通話を開始すると、タスクを実行する前に同意を得ることができる。別の例として、自動アシスタント115は、たとえ進行中の通話が自動アシスタント115によって開始されなくても、所与のユーザが進行中の通話を開始すると、クライアントデバイス110の所与のユーザに対して同意を得ることができる。自動アシスタント115が関連するさらなるユーザから同意を得る場合、自動アシスタント115は、支援された通話システム180を使用してタスクを実行することができる。しかし、自動アシスタント115がさらなるユーザから同意を得ない場合、自動アシスタント115は、クライアントデバイス110に、クライアントデバイス110の所与のユーザに対して、所与のユーザがタスクを実行することおよび/または通話を終了することが必要とされることを示す通知を(たとえば、レンダリングエンジン113を使用して)レンダリングさせ、クライアントデバイス110の所与のユーザに対して、タスクが実行されなかったことを示す通知を(たとえば、レンダリングエンジン113を使用して)レンダリングさせることができる。 Further, in some implementations, before performing any task on behalf of a given user of client device 110, automated assistant 115 may obtain consent from further users to interact with automated assistant 115. is possible. For example, when the automated assistant 115 initiates an assisted call, it may obtain consent before performing a task. As another example, automated assistant 115 may respond to a given user at client device 110 when a given user initiates an ongoing call, even if the ongoing call is not initiated by automated assistant 115. Consent can be obtained. If the automated assistant 115 obtains consent from the associated additional user, the automated assistant 115 may use the assisted calling system 180 to perform the task. However, if the automated assistant 115 does not obtain consent from the additional user, the automated assistant 115 may request the client device 110 to perform the task and/or for the given user of the client device 110. cause a notification to be rendered (e.g., using rendering engine 113) indicating that the call is required to be terminated and indicating to a given user of client device 110 that the task was not performed; The notification may be rendered (eg, using rendering engine 113).
下でより詳細に説明されるように、自動アシスタント115は、支援された通話を使用して通話を開始するための、クライアントデバイス110の所与のユーザからのユーザ入力の検出に応じて、および/または進行中の通話中に(すなわち、支援された通話がまだ呼び出されていないときに)、支援された通話システム180を使用して支援された通話を実行することができる。一部の実装において、自動アシスタント115は、支援された通話中および/または進行中の通話中に、クライアントデバイス110の所与のユーザの代理として、タスクを実行する際に使用される候補パラメータの値を決定することができる。それらの実装のいくつかのバージョンにおいて、自動アシスタント115は、支援された通話の前におよび/または支援された通話中に、候補パラメータの値を求めるためにクライアントデバイス110の所与のユーザと対話することができる。それらの実装のいくつかの追加的および/または代替的なバージョンにおいて、自動アシスタント115は、支援された通話の前におよび/または支援された通話中に候補パラメータの値を求めることなく、クライアントデバイス110の所与のユーザに関連するユーザプロファイルに基づいて候補パラメータの値を決定することができる。それらの実装のいくつかのバージョンにおいて、自動アシスタント115は、ユーザ入力エンジン111によってクライアントデバイス110の所与のユーザからのいかなるユーザ入力も検出することなく、進行中の通話の対話に基づいて自動的に支援された通話システム180を使用して支援された通話を実施することができる。 As described in more detail below, automated assistant 115 is responsive to detecting user input from a given user of client device 110 to initiate a call using assisted calling; The assisted call system 180 may be used to perform an assisted call/or while a call is in progress (ie, when the assisted call has not yet been called). In some implementations, automated assistant 115 selects candidate parameters to be used in performing tasks on behalf of a given user of client device 110 during an assisted call and/or during an ongoing call. value can be determined. In some versions of their implementation, automated assistant 115 interacts with a given user of client device 110 to determine values for candidate parameters before and/or during the assisted call. can do. In some additional and/or alternative versions of those implementations, the automated assistant 115 performs a process on the client device without prompting for candidate parameter values prior to and/or during the assisted call. Values of candidate parameters may be determined based on a user profile associated with a given user of 110. In some versions of those implementations, automated assistant 115 automatically operates based on ongoing call interaction without detecting any user input from a given user of client device 110 by user input engine 111. Assisted calling system 180 may be used to conduct assisted calling.
図1に示されるように、支援された通話システム180は、遠隔に(たとえば、サーバおよび/またはその他の遠隔クライアントデバイスによって)実装され得る。支援された通話システム180は、ネットワーク190を介して遠隔に実装されるものとして図1に示されているが、それは例示のためであり、限定的であるように意図されていないことを理解されたい。たとえば、様々な実装において、支援された通話システム180は、クライアントデバイス110のローカルに実装され得る。さらに、自動アシスタント115は、ローカルのクライアントデバイス110と遠隔の支援された通話システム180との両方に実装されるものとして図1に示されているが、それも例示のためであり、限定的であるように意図されていないことを理解されたい。たとえば、様々な実装において、自動アシスタント115は、クライアントデバイス110のローカルに実装されることが可能であり、またはクライアントデバイス110のローカルに実装され、別個のクラウドに基づく自動アシスタントとインタラクションすることが可能である。 As shown in FIG. 1, assisted calling system 180 may be implemented remotely (eg, by a server and/or other remote client device). Although assisted calling system 180 is illustrated in FIG. 1 as being implemented remotely via network 190, it is understood that it is illustrative and not intended to be limiting. sea bream. For example, in various implementations, assisted calling system 180 may be implemented locally on client device 110. Further, although automated assistant 115 is shown in FIG. Please understand that it is not intended to be. For example, in various implementations, automated assistant 115 may be implemented locally on client device 110, or may be implemented locally on client device 110 and interact with a separate cloud-based automated assistant. It is.
実装においては、ユーザ入力エンジン111がクライアントデバイス110のマイクロフォンによって所与のユーザの発話された入力を検出する、ならびに/または(たとえば、支援された通話中および/もしくは進行中の通話中に)さらなるクライアントデバイスからクライアントデバイス110に送信された、さらなるユーザからのさらなる発話された入力をキャプチャするオーディオデータを受信するとき、クライアントデバイス110の音声認識エンジン120A1は、音声認識モデル120Aを使用して、発話された入力をキャプチャするおよび/またはさらなる発話された入力をキャプチャするオーディオデータを処理して、発話された入力および/またはさらなる発話された入力に対応する認識されたテキストを生成することができる。さらに、クライアントデバイス110のNLUエンジン130A1は、NLUモデル130Aを使用して、音声認識エンジン120A1によって生成された認識されたテキストを処理して、発話された入力および/またはさらなる発話された入力に含まれる意図を決定することができる。たとえば、クライアントデバイス110が所与のユーザから「Example Cafeに電話して今夜の予約をして」という発話された入力を検出する場合、クライアントデバイス110は、音声認識モデル120Aを使用して、発話された入力をキャプチャするオーディオデータを処理して、「Example Cafeに電話して今夜の予約をして」という発話された入力に対応する認識されたテキストを生成することができ、NLUモデル130Aを使用して、認識されたテキストを処理して、少なくとも、電話を開始するという第1の意図と、レストランの予約をするという第2の意図とを決定することができる。別の例として、クライアントデバイス110が、「お子様もいらっしゃいますか」というさらなる発話された入力を検出する場合、クライアントデバイス110は、音声認識モデル120Aを使用して、さらなる発話された入力をキャプチャするオーディオデータを処理して、「お子様もいらっしゃいますか」というさらなる発話された入力に対応する認識されたテキストを生成することができ、NLUモデル130Aを使用して、認識されたテキストを処理して、本明細書において説明されるように、追加パラメータに関連する情報の要求の意図を決定することができる。それらの実装のいくつかのバージョンにおいて、クライアントデバイス110は、オーディオデータ、認識されたテキスト、および/または意図をアシスタント通話システム180に送信することができる。 In implementations, the user input engine 111 detects a given user's spoken input by the microphone of the client device 110 and/or detects further input (e.g., during an assisted call and/or during an ongoing call). Upon receiving audio data transmitted from the client device to the client device 110 that captures further spoken input from a further user, the speech recognition engine 120A1 of the client device 110 uses the speech recognition model 120A to recognize the utterances. The audio data capturing the spoken input and/or capturing the additional spoken input may be processed to generate recognized text corresponding to the spoken input and/or the additional spoken input. Additionally, the NLU engine 130A1 of the client device 110 uses the NLU model 130A to process the recognized text generated by the speech recognition engine 120A1 for inclusion in the spoken input and/or further spoken input. can determine their intentions. For example, if client device 110 detects spoken input from a given user, "Call Example Cafe to make a reservation for tonight," client device 110 uses speech recognition model 120A to detect spoken input from a given user. The NLU Model 130A can process audio data that captures input to produce recognized text that corresponds to the spoken input "Call Example Cafe to make a reservation for tonight." may be used to process the recognized text to determine at least a first intent to initiate a phone call and a second intent to make a restaurant reservation. As another example, if client device 110 detects additional spoken input, "Do you have any children?", client device 110 captures the additional spoken input using speech recognition model 120A. The audio data can be processed to generate recognized text that corresponds to the further spoken input "Do you have any children?" and the recognized text can be processed using NLU Model 130A , the intent of the request for information related to additional parameters can be determined, as described herein. In some versions of those implementations, client device 110 may send audio data, recognized text, and/or intent to assistant calling system 180.
その他の実装においては、ユーザ入力エンジン111がクライアントデバイス110のマイクロフォンによって所与のユーザの発話された入力、ならびに/または(たとえば、支援された通話中および/もしくは進行中の通話中に)さらなるクライアントデバイスからクライアントデバイス110に送信された、さらなるユーザからのさらなる発話された入力をキャプチャするオーディオデータを検出するとき、自動アシスタント115は、クライアントデバイス110に、発話された入力をキャプチャするオーディオデータおよび/またはさらなる発話された入力をキャプチャするオーディオデータを支援された通話システム180に送信させることができる。支援された通話システム180の音声認識エンジン120A2および/またはNLUエンジン130A2は、クライアントデバイス110の音声認識エンジン120A1および/またはNLUエンジン130A1に関連して上で説明されたのと同様にして、発話された入力をキャプチャするオーディオデータおよび/またはさらなる発話された発言をキャプチャするオーディオデータを処理することができる。いくつかの追加的および/または代替的な実施態様において、クライアントデバイス110の音声認識エンジン120A1および/またはNLUエンジン130A1は、分散された方法で、支援された通話システム180の音声認識エンジン120A2および/またはNLUエンジン130A2と併せて使用され得る。さらに、音声認識モデル120Aおよび/またはNLUモデル130Aは、ローカルのクライアントデバイス110に、ならびに/またはネットワーク190を介してクライアントデバイス110および/もしくは支援された通話システム180と通信する遠隔のサーバに記憶され得る。 In other implementations, user input engine 111 receives input spoken by a microphone of client device 110 from a given user and/or from further clients (e.g., during an assisted call and/or during an ongoing call). Upon detecting audio data that captures further spoken input from further users transmitted from the device to client device 110, automated assistant 115 sends the client device 110 the audio data that captures further spoken input and/or Or audio data capturing additional spoken input may be sent to the assisted calling system 180. Speech recognition engine 120A2 and/or NLU engine 130A2 of assisted calling system 180 performs speech recognition in a manner similar to that described above in connection with speech recognition engine 120A1 and/or NLU engine 130A1 of client device 110. The audio data that captures the input input and/or the audio data that captures further spoken utterances may be processed. In some additional and/or alternative implementations, the speech recognition engine 120A1 and/or the NLU engine 130A1 of the client device 110 is connected to the speech recognition engine 120A2 and/or the NLU engine 130A1 of the assisted calling system 180 in a distributed manner. or may be used in conjunction with NLU engine 130A2. Additionally, speech recognition model 120A and/or NLU model 130A may be stored locally on client device 110 and/or on a remote server that communicates with client device 110 and/or assisted calling system 180 via network 190. obtain.
様々な実装において、音声認識モデル120Aは、音声認識エンジン120A1および/または120A2が、モデルを直接使用して、発話された入力に対応する認識されたテキストを生成することができるようなエンドツーエンドの音声認識モデルである。たとえば、音声認識モデル120Aは、認識されたテキストを一文字ずつ(またはその他の一トークンずつ)生成するために使用されるエンドツーエンドのモデルであることが可能である。認識されたテキストを一文字ずつ生成するために使用されるそのようなエンドツーエンドのモデルの1つの非限定的な例は、再帰型ニューラルネットワークトランスデューサ(RNN-T: recurrent neural network transducer)モデルである。RNN-Tモデルは、アテンションメカニズムを使用しないシーケンスツーシーケンス(sequence-to-sequence)モデルの形態である。また、たとえば、音声認識モデルがエンドツーエンドの音声認識モデルでないとき、音声認識エンジン120A1および/または120A2は、その代わりに、予測された音素(および/またはその他の表現)を生成し得る。たとえば、そのようなモデルを用いて、予測された音素(および/またはその他の表現)は、次に、予測された音素に適合する認識されたテキストを決定するために音声認識エンジン120A1および/または120A2によって利用される。そのようにする際、音声認識エンジン120A1および/または120A2は、任意で、復号グラフ(decoding graph)、辞書、および/またはその他のリソースを使用し得る。 In various implementations, the speech recognition model 120A is configured end-to-end such that the speech recognition engine 120A1 and/or 120A2 can directly use the model to generate recognized text corresponding to spoken input. This is a speech recognition model. For example, speech recognition model 120A may be an end-to-end model used to generate recognized text character by character (or other token by token). One non-limiting example of such an end-to-end model used to generate recognized text character by character is a recurrent neural network transducer (RNN-T) model. . The RNN-T model is a form of sequence-to-sequence model that does not use an attention mechanism. Also, for example, when the speech recognition model is not an end-to-end speech recognition model, speech recognition engines 120A1 and/or 120A2 may instead generate predicted phonemes (and/or other representations). For example, using such a model, the predicted phonemes (and/or other representations) are then passed to speech recognition engine 120A1 and/or to determine recognized text that matches the predicted phonemes. Utilized by 120A2. In doing so, speech recognition engines 120A1 and/or 120A2 may optionally use decoding graphs, dictionaries, and/or other resources.
実装においては、ユーザ入力エンジン111がクライアントデバイス110のユーザインターフェース入力デバイスによってタッチ入力および/またはタイピングされた入力を検出するとき、自動アシスタント115は、タッチ入力のインジケーションおよび/またはタイピングされた入力のインジケーションをクライアントデバイス110から支援された通話システム180に送信させることができる。それらの実装のいくつかのバージョンにおいて、タッチ入力のインジケーションおよび/またはタイピングされた入力のインジケーションは、タッチ入力の基礎となるテキストおよび/またはタイピングされた入力のテキストを含むことができ、基礎となるテキストおよび/またはテキストは、基礎となるテキストおよび/またはテキストの意図を決定するためにNLUモデル130Aを使用して処理され得る。 In implementations, when user input engine 111 detects touch input and/or typed input by a user interface input device of client device 110, automated assistant 115 provides an indication of the touch input and/or typed input. An indication can be caused to be sent from client device 110 to assisted calling system 180. In some versions of those implementations, the indication of touch input and/or the indication of typed input may include the underlying text of the touch input and/or the text of the typed input; The underlying text and/or the text may be processed using the NLU model 130A to determine the underlying text and/or the intent of the text.
本明細書において説明されるように、支援された通話システム180の支援された通話エンジン150は、音声認識エンジン120A1および/もしくは120A2によって生成された認識されたテキスト、クライアントデバイス110において検出されたタッチ入力の基礎となるテキスト、クライアントデバイス110において検出されたタイピングされた入力の基礎となるテキスト、ならびに/またはNLUエンジン130A1および/もしくは130A2によって決定された意図をさらに処理することができる。支援された通話エンジン150は、様々な実装において、エンティティ特定エンジン151、タスク決定エンジン152、パラメータエンジン153、タスク実行エンジン154、フィードバックエンジン155、および推薦エンジン156を含む。 As described herein, the assisted calling engine 150 of the assisted calling system 180 may recognize text generated by speech recognition engines 120A1 and/or 120A2, a touch detected at the client device 110, The text underlying the input, the text underlying the typed input detected at client device 110, and/or the intent determined by NLU engines 130A1 and/or 130A2 may be further processed. Assisted conversation engine 150 includes an entity identification engine 151, a task determination engine 152, a parameters engine 153, a task execution engine 154, a feedback engine 155, and a recommendation engine 156 in various implementations.
エンティティ特定エンジン151は、クライアントデバイス110の所与のユーザの代理として関わり合うエンティティを特定することができる。エンティティは、たとえば、人のエンティティ、企業のエンティティ、場所のエンティティ、および/またはその他のエンティティであることが可能である。一部の実装において、エンティティ特定エンジン151は、特定されたエンティティのエンティティの特定の種類を決定することもできる。たとえば、人のエンティティの種類は、友人のエンティティ、家族のエンティティ、同僚のエンティティ、および/または人のエンティティのその他の特定の種類であることが可能である。さらに、企業のエンティティの種類は、レストランのエンティティ、航空会社のエンティティ、ホテルのエンティティ、サロンのエンティティ、診療所のエンティティ、および/または企業のエンティティのその他の特定の種類であることが可能である。さらに、場所のエンティティの種類は、学校のエンティティ、博物館のエンティティ、図書館のエンティティ、公園のエンティティ、および/または場所のエンティティのその他の特定の種類であることが可能である。一部の実装において、エンティティ特定エンジン151は、特定されたエンティティに関する特定のエンティティを決定することもできる。たとえば、人のエンティティに関する特定のエンティティは、人の名前(たとえば、Jane Doeなど)であることが可能であり、企業のエンティティに関する特定のエンティティは、企業の名前(たとえば、Hypothetical Cafe、Example Cafe、Example Airlinesなど)であることが可能であり、場所のエンティティに関する特定のエンティティは、場所の名前(たとえば、Hypothetical University、Example National Parkなど)であることが可能である。本明細書において説明されるエンティティは様々な粒度で定義され得るが、それらのエンティティは、簡単にするために、本明細書においては集合的に「エンティティ」と呼ばれる。 Entity identification engine 151 can identify entities to engage on behalf of a given user of client device 110. An entity can be, for example, a person entity, a business entity, a place entity, and/or other entities. In some implementations, entity identification engine 151 may also determine the particular type of entity of the identified entity. For example, the person entity type may be a friend entity, a family member entity, a coworker entity, and/or some other specific type of person entity. Further, the type of corporate entity may be a restaurant entity, an airline entity, a hotel entity, a salon entity, a clinic entity, and/or other specific types of corporate entity. . Further, the location entity type may be a school entity, a museum entity, a library entity, a park entity, and/or other specific types of location entities. In some implementations, entity identification engine 151 may also determine the specific entity for the identified entity. For example, a specific entity with respect to a person entity may be a person's name (e.g., Jane Doe), and a specific entity with respect to a company entity may be a company name (e.g., Hypothetical Cafe, Example Cafe, Example Airlines, etc.), and the specific entity for the location entity can be a location name (eg, Hypothetical University, Example National Park, etc.). Although the entities described herein may be defined at various granularity, they are collectively referred to herein as "entities" for simplicity.
一部の実装において、エンティティ特定エンジン151は、自動アシスタント115を使用して支援された通話を開始する前に、クライアントデバイス110とのユーザのインタラクションに基づいて、クライアントデバイス110の所与のユーザの代理として関わり合うエンティティを特定することができる。それらの実装のいくつかのバージョンにおいて、エンティティは、支援された通話を開始するためのユーザ入力の受信に応じて特定され得る。たとえば、クライアントデバイス110の所与のユーザが、ソフトウェアアプリケーションの通話インターフェース要素(たとえば、連絡先アプリケーション(contacts application)の連絡先に関する、ブラウザアプリケーションの検索結果に関する、および/またはその他のソフトウェアアプリケーションに含まれるその他の通話可能なエンティティに関する)に対して(たとえば、発話されたまたはタッチ)入力を送る場合、エンティティ特定エンジン151は、通話インターフェース要素に関連するエンティティを特定することができる。たとえば、ユーザ入力がブラウザアプリケーション内の「Example Cafe」に関連する通話インターフェース要素を対象とする場合、エンティティ特定エンジン151は、支援された通話中に、クライアントデバイス110の所与のユーザの代理として関わり合うエンティティとして「Example Cafe」(またはより広く、企業のエンティティもしくはレストランのエンティティ)を特定することができる。 In some implementations, entity identification engine 151 identifies a given user of client device 110 based on the user's interaction with client device 110 before initiating an assisted call using automated assistant 115. Entities to interact on behalf of can be identified. In some versions of those implementations, an entity may be identified in response to receiving user input to initiate an assisted call. For example, a given user of client device 110 may be involved in a software application's call interface elements (e.g., regarding contacts in a contacts application, regarding search results in a browser application, and/or in other software applications). When sending input (eg, spoken or touched) to (relating to other talkable entities), entity identification engine 151 may identify the entity associated with the call interface element. For example, if the user input is directed to a call interface element associated with "Example Cafe" within a browser application, entity identification engine 151 may interact on behalf of a given user of client device 110 during an assisted call. "Example Cafe" (or more broadly, a corporate or restaurant entity) can be identified as a matching entity.
一部の実装において、エンティティ特定エンジン151は、進行中の通話に関連するメタデータに基づいて、クライアントデバイス110の所与のユーザの代理として関わり合うエンティティを特定することができる。メタデータは、たとえば、さらなるユーザに関連する電話番号、さらなるユーザに関連する場所、さらなるユーザおよび/もしくはさらなるユーザに関連するエンティティを特定する識別子、進行中の通話が始まった時間、進行中の通話の継続時間、ならびに/または通話に関連するその他のメタデータを含み得る。たとえば、クライアントデバイス110の所与のユーザがさらなるユーザとの進行中の通話に従事している場合、エンティティ特定エンジン151は、クライアントデバイス110の所与のユーザとさらなるユーザとの間の進行中の通話のメタデータを分析して、進行中の通話中に関わり合われるさらなるユーザに関連する電話番号を特定することができる。さらに、エンティティ特定エンジン151は、電話番号をデータベースと相互参照してさらなるユーザに関連するエンティティを特定し、電話番号の検索クエリを送って電話番号に関連する対応する検索結果を特定してさらなるユーザに関連するエンティティを特定し、および/またはその他のアクションを実行してさらなるユーザに関連するエンティティを特定することができる。たとえば、クライアントデバイス110の所与のユーザが進行中の通話に従事している場合、エンティティ特定エンジン151は、進行中の通話に関連するメタデータを分析して、「Example Airlines」のアイデンティティ(identity)を特定するができる。 In some implementations, entity identification engine 151 may identify entities to engage on behalf of a given user of client device 110 based on metadata associated with an ongoing call. The metadata may include, for example, a telephone number associated with the further user, a location associated with the further user, an identifier identifying the further user and/or an entity associated with the further user, the time at which the call in progress began, the call in progress and/or other metadata related to the call. For example, if a given user of client device 110 is engaged in an ongoing call with a further user, entity identification engine 151 may identify an ongoing call between the given user of client device 110 and the further user. Metadata of the call may be analyzed to identify phone numbers associated with additional users engaged during the ongoing call. Additionally, the entity identification engine 151 cross-references the phone number with a database to identify entities associated with additional users, and sends search queries for the phone number to identify corresponding search results associated with the phone number to identify additional users. and/or may perform other actions to identify entities associated with additional users. For example, if a given user of client device 110 is engaged in an ongoing call, entity identification engine 151 analyzes metadata related to the ongoing call to determine the identity of "Example Airlines". ) can be specified.
さらに、エンティティ特定エンジン151は、任意の特定されたエンティティをエンティティデータベース151Aに記憶させることができる。一部の実装において、エンティティデータベース151Aに記憶された特定されたエンティティは、エンティティおよび/またはエンティティの特定の種類によってインデックス付けされ得る。たとえば、エンティティ特定エンジン151が「Example Cafe」エンティティを特定する場合、「Example Cafe」は、エンティティデータベース151Aにおいて企業のエンティティとしてインデックス付けされることが可能であり、任意で、レストランのエンティティとしてさらにインデックス付けされることが可能である。さらに、エンティティ特定エンジン151が「Example Airlines」エンティティを特定する場合、「Example Airlines」も、エンティティデータベース151Aにおいて企業のエンティティとしてインデックス付けされることが可能であり、任意で、航空会社のエンティティとしてさらにインデックス付けされることが可能である。特定されたエンティティをエンティティデータベース151Aに記憶し、インデックス付けすることによって、エンティティ特定エンジン151は、エンティティを容易に特定し、取り出すことができ、それによって、将来の支援された通話および/または進行中の通話においてそれらのエンティティに遭遇するときに、エンティティを特定するためのその後の処理を減らす。さらに、様々な実装において、各エンティティは、エンティティデータベース151Aにおいてタスクに関連付けられ得る。 Additionally, entity identification engine 151 may store any identified entities in entity database 151A. In some implementations, identified entities stored in entity database 151A may be indexed by entity and/or specific type of entity. For example, if entity identification engine 151 identifies an "Example Cafe" entity, "Example Cafe" may be indexed as a company entity in entity database 151A, and optionally further indexed as a restaurant entity. It is possible to be attached. Additionally, if entity identification engine 151 identifies an "Example Airlines" entity, "Example Airlines" may also be indexed as a corporate entity in entity database 151A, and optionally further indexed as an airline entity. Can be indexed. By storing and indexing identified entities in the entity database 151A, the entity identification engine 151 can easily identify and retrieve the entities, thereby allowing future assisted calls and/or ongoing reduce subsequent processing to identify entities when they are encountered in a call. Further, in various implementations, each entity may be associated with a task in entity database 151A.
タスク決定エンジン152は、クライアントデバイス110の所与のユーザの代理として実行されるタスクを決定することができる。一部の実装において、タスク決定エンジン152は、自動アシスタント115を使用して支援された通話を開始する前に、タスクを決定することができる。それらの実装のいくつかのバージョンにおいて、タスク決定エンジン152は、支援された通話を開始するためのユーザ入力に基づいて、クライアントデバイス110の所与のユーザの代理として実行されるタスクを決定することができる。たとえば、クライアントデバイス110の所与のユーザが「Example Cafeに電話して今夜の予約をして」という発話された入力を与える場合、タスク決定エンジン152は、(たとえば、NLUモデル130Aを使用して決定された)通話を開始し、レストランの予約をするという意図を利用して、発話された入力に基づいて、レストラン予約を行うタスクを決定することができる。別の例として、クライアントデバイス110の所与のユーザが「Example Cafe」に関連する通話インターフェース要素を選択するタッチ入力を与え、通話インターフェースが所与のユーザがExample Cafeのレストラン予約を修正したいことを示す場合、タスク決定エンジン152は、タッチ入力に基づいて既存のレストラン予約を修正するタスクを決定することができる。 Task determination engine 152 can determine tasks to be performed on behalf of a given user of client device 110. In some implementations, task determination engine 152 may determine the task prior to initiating an assisted call using automated assistant 115. In some versions of those implementations, task determination engine 152 may determine tasks to be performed on behalf of a given user of client device 110 based on user input to initiate an assisted call. I can do it. For example, if a given user of client device 110 provides the spoken input "Call Example Cafe to make a reservation for tonight," task decision engine 152 (e.g., using NLU model 130A) The determined intent to initiate a call and make a restaurant reservation can be used to determine the task of making a restaurant reservation based on the spoken input. As another example, a given user of client device 110 provides touch input selecting a call interface element associated with "Example Cafe" and the call interface indicates that the given user wishes to modify a restaurant reservation for Example Cafe. If indicated, task determination engine 152 may determine a task to modify an existing restaurant reservation based on the touch input.
それらの実装のいくつかの追加的および/または代替的なバージョンにおいて、タスク決定エンジン152は、支援された通話中に関わり合う特定されたエンティティに基づいてタスクを決定することができる。たとえば、レストランのエンティティは、レストラン予約を行うタスク、レストラン予約を修正するタスク、レストラン予約をキャンセルするタスク、および/またはその他のタスクに関連付けられ得る。別の例として、学校のエンティティは、休校(closure)について問い合わせるタスク、生徒/職員がその日に登校しないことを報告するタスク、および/またはその他のタスクに関連付けられ得る。 In some additional and/or alternative versions of these implementations, task determination engine 152 may determine tasks based on identified entities involved during the assisted call. For example, a restaurant entity may be associated with the task of making a restaurant reservation, modifying a restaurant reservation, canceling a restaurant reservation, and/or other tasks. As another example, a school entity may be associated with tasks that inquire about school closures, report that students/staff will not be attending school that day, and/or other tasks.
その他の実装において、タスク決定エンジン152は、進行中の通話中に、クライアントデバイス110の所与のユーザの代理として実行されるタスクを決定することができる。それらの実装のいくつかのバージョンにおいては、クライアントデバイス110の所与のユーザとさらなるクライアントデバイスのさらなるユーザとの間の対話に対応するオーディオデータのストリームが、本明細書において説明されるように(たとえば、音声認識モデル120AおよびNLUモデル130Aに関連して)処理され得る。たとえば、クライアントデバイス110の所与のユーザとさらなるクライアントデバイスに関連するさらなるユーザとの間の進行中の通話中に、タスク決定エンジン152が「あなたのマイレージ会員(frequent flier)番号は何番ですか」という認識されたテキストを特定する場合、タスク決定エンジン152は、さらなるユーザにマイレージ会員番号を提供するタスクを決定することができる。それらの実装のいくつかのさらなるバージョンにおいて、タスク決定エンジン152は、エンティティデータベース151Aにタスクに関連付けて記憶されているエンティティに基づいてタスクを決定することもできる。たとえば、エンティティ特定エンジン151が、(たとえば、上述のように進行中の通話に関連するメタデータに基づいて)Example Airlinesエンティティに関連するさらなるユーザを特定する場合、タスク決定エンジン152は、航空会社のエンティティに関連付けて記憶されているマイレージ会員番号を提供するためのタスクに基づいて、さらなるユーザにExample Airlinesに関連するマイレージ会員番号を提供するタスクを決定することができる。 In other implementations, task determination engine 152 may determine tasks to be performed on behalf of a given user of client device 110 during an ongoing call. In some versions of those implementations, a stream of audio data corresponding to an interaction between a given user of client device 110 and a further user of a further client device, as described herein ( For example, in connection with speech recognition model 120A and NLU model 130A). For example, during an ongoing call between a given user of client device 110 and a further user associated with a further client device, task decision engine 152 may ask, ``What is your frequent flier number? ”, the task determination engine 152 may determine a task to provide the frequent flyer number to the additional user. In some further versions of these implementations, task determination engine 152 may also determine tasks based on entities stored in association with the tasks in entity database 151A. For example, if entity identification engine 151 identifies additional users associated with the Example Airlines entity (e.g., based on metadata related to calls in progress, as described above), task determination engine 152 identifies additional users associated with the Example Airlines entity. Based on the task for providing a frequent flyer number stored in association with an entity, a task for providing a frequent flyer number associated with Example Airlines to a further user may be determined.
パラメータエンジン153は、タスク決定エンジン152によって決定されたタスクに関連するパラメータを特定することができる。自動アシスタント115は、パラメータの値を使用してタスクを実行することができる。一部の実装においては、候補パラメータが、パラメータデータベース153Aにタスクに関連付けて記憶され得る。それらの実装のいくつかのバージョンにおいては、所与のタスクのための候補パラメータが、支援された通話中に、クライアントデバイス110の所与のユーザの代理として関わり合うエンティティの特定に応じて、パラメータデータベースから取り出され得る。たとえば、レストラン予約を行うタスクに関して、パラメータエンジン153は、名前パラメータ、日付/時間パラメータ、予約の人数パラメータ、電話番号パラメータ、様々な座席の種類パラメータ(たとえば、ボックス席もしくはテーブル席、屋内席もしくは屋外席など)、子供パラメータ(すなわち、子供が予約に参加するかどうか)、特別な機会パラメータ(たとえば、誕生日、記念日など)、および/またはその他の候補パラメータを含む1つまたは複数の候補パラメータを特定し、取り出すことができる。対照的に、レストラン予約を修正するタスクに関して、パラメータエンジン153は、名前パラメータ、元の予約の日付/時間パラメータ、修正された予約の日付/時間パラメータ、修正された予約の人数パラメータ、および/またはその他の候補パラメータを含む候補パラメータを特定し、取り出すことができる。 Parameter engine 153 may identify parameters associated with the task determined by task determination engine 152. Automated assistant 115 may use the values of the parameters to perform tasks. In some implementations, candidate parameters may be stored in association with the task in parameter database 153A. In some versions of those implementations, the candidate parameters for a given task are determined depending on the identity of the entity that engages on behalf of a given user of client device 110 during an assisted call. Can be retrieved from the database. For example, for the task of making a restaurant reservation, the parameter engine 153 may include a name parameter, a date/time parameter, a number of people in the reservation parameter, a phone number parameter, various seating type parameters (e.g., box or table seating, indoor or outdoor seating, etc.). one or more candidate parameters, including a child parameter (i.e., whether a child will participate in the reservation), a special occasion parameter (e.g., birthday, anniversary, etc.), and/or other candidate parameters can be identified and retrieved. In contrast, for the task of modifying a restaurant reservation, the parameter engine 153 includes the name parameter, the date/time parameter of the original reservation, the date/time parameter of the modified reservation, the number of people parameter of the modified reservation, and/or Candidate parameters, including other candidate parameters, can be identified and retrieved.
いくつかの追加的および/または代替的な実装において、パラメータエンジン153は、クライアントデバイス110の所与のユーザとさらなるクライアントデバイスのさらなるユーザとの間の進行中の通話中に所与のタスクのためのパラメータを特定することができる。それらの実装のいくつかのバージョンにおいて、進行中の通話中に所与のタスクのために特定されるパラメータは、パラメータデータベース153Aに所与のタスクに関連付けて記憶された候補パラメータである可能性があり、またはそうでない可能性がある。上述のように、所与のユーザとさらなるユーザとの間の対話に対応するオーディオデータのストリームが、進行中の通話中に実行されるタスクを決定するために処理され得る。パラメータエンジン153は、さらなるユーザが自動アシスタント115に知られていない所与のパラメータに関連する情報を要求しているかどうかを判定することができる。たとえば、進行中の通話中に、Example Airlinesの担当者がクライアントデバイスの所与のユーザにマイレージ会員番号を要求する場合、パラメータエンジン153は、会話の認識されたテキストに含まれる意図に基づいて、および/または特定された「Example Airlines」エンティティに基づいて、「Example Airlinesマイレージ会員番号」パラメータを特定することができる。 In some additional and/or alternative implementations, the parameter engine 153 provides information for a given task during an ongoing call between a given user of client device 110 and a further user of a further client device. parameters can be specified. In some versions of those implementations, the parameters identified for a given task during an ongoing call may be candidate parameters stored in association with the given task in parameter database 153A. It may or may not be true. As mentioned above, streams of audio data corresponding to interactions between a given user and a further user may be processed to determine tasks to be performed during an ongoing call. Parameter engine 153 may determine whether additional users request information related to a given parameter that is not known to automated assistant 115. For example, if, during an ongoing call, a representative from Example Airlines requests a frequent flyer number from a given user at a client device, the parameter engine 153 uses the intent contained in the recognized text of the conversation to and/or an "Example Airlines frequent flyer number" parameter may be identified based on the identified "Example Airlines" entity.
さらに、一部の実装においては、パラメータデータベース153Aに記憶された候補パラメータが、エンティティデータベース151Aに記憶された様々なエンティティにマッピングされ得る。パラメータデータベース153Aに記憶された候補パラメータをエンティティデータベース151Aに記憶された様々なエンティティにマッピングすることによって、支援された通話エンジン150は、所与のエンティティの特定に応じて、タスクのためのパラメータを容易に特定することができる。たとえば、「Example Cafe」エンティティ(またはより広く、レストランのエンティティ)の特定に応じて、支援された通話エンジン150は、1組の候補パラメータを決定するために、(たとえば、エンティティデータベース151Aに「Example Cafe」エンティティおよび/またはレストランのエンティティに関連付けて記憶された)予め定義されたタスクを特定することができ、(たとえば、特定されたエンティティおよび/またはユーザ入力に基づいた)特定のタスクの特定に応じて、支援された通話エンジン150は、特定されたエンティティのための特定のタスクに関連する候補パラメータを決定することができる。その他の実装において、エンティティデータベース151Aおよびパラメータデータベース153Aは、エンティティ、タスク、および候補パラメータがそれぞれ互いに関連付けて記憶され得るように、様々なインデックスを用いる(たとえば、エンティティによってインデックス付けされた、タスクによってインデックス付けされた、および/またはその他の方法でインデックス付けされた)1つのデータベースとして組み合わされることが可能である。 Further, in some implementations, candidate parameters stored in parameter database 153A may be mapped to various entities stored in entity database 151A. By mapping candidate parameters stored in parameter database 153A to various entities stored in entity database 151A, assisted calling engine 150 determines parameters for a task depending on the identification of a given entity. Can be easily identified. For example, in response to identifying an "Example Cafe" entity (or more broadly, a restaurant entity), assisted calling engine 150 may store "Example Cafe" in entity database 151A (e.g., can identify predefined tasks (stored in association with the "Cafe" entity and/or restaurant entity), and can identify a particular task (e.g., based on the identified entity and/or user input). Accordingly, assisted conversation engine 150 may determine candidate parameters related to the particular task for the identified entity. In other implementations, entity database 151A and parameter database 153A employ various indexes (e.g., indexed by entity, indexed by task, and/or otherwise indexed) as one database.
上述のように、自動アシスタント115は、パラメータの値を使用して、クライアントデバイス110の所与のユーザの代理としてタスクを実行することができる。パラメータエンジン153は、パラメータの値を決定することもできる。実装においては、クライアントデバイス110の所与のユーザが支援された通話を開始するためのユーザ入力を与えるとき、パラメータエンジン153は、候補パラメータに関する情報を要求するさらなるユーザ入力を求めるために、支援された通話を開始する前に、自動アシスタント115に(たとえば、クライアントデバイス110を介して視覚的および/または聴覚的に)所与のユーザと対話させることができる。本明細書において説明されるように、自動アシスタント115は、情報を要求するプロンプトを生成することができ、対応する値(またはそのサブセット)を聴覚的に(たとえば、クライアントデバイス110のスピーカを介して)および/または視覚的に(たとえば、クライアントデバイス110のディスプレイを介して)求めるためにプロンプトをレンダリングすることができる。たとえば、(たとえば、タッチ入力または発話された入力による)Example Cafeの予約をするために支援された通話を開始するためのユーザ入力の受け取り応じて、自動アシスタント115は、候補パラメータ(またはそのサブセット)に関して、予約のための日付/時間パラメータの値、予約のための人数パラメータの値などを含むさらなるユーザ入力を要求するプロンプトを生成することができる。 As mentioned above, the automated assistant 115 can use the values of the parameters to perform tasks on behalf of a given user of the client device 110. Parameter engine 153 may also determine values for parameters. In implementations, when a given user of client device 110 provides user input to initiate an assisted call, parameter engine 153 uses the assisted call to solicit further user input requesting information about candidate parameters. The automated assistant 115 may be allowed to interact with a given user (eg, visually and/or audibly via the client device 110) before initiating the call. As described herein, automated assistant 115 may generate a prompt requesting information and respond with a corresponding value (or a subset thereof) audibly (e.g., via a speaker of client device 110). ) and/or visually (eg, via the display of client device 110). For example, in response to receiving user input to initiate an assisted call to make a reservation for the Example Cafe (e.g., via touch input or spoken input), automated assistant 115 selects candidate parameters (or a subset thereof). Regarding the reservation, a prompt may be generated requesting further user input including values for date/time parameters for the reservation, values for the number of people parameters for the reservation, etc.
それらの実装のいくつかのバージョンにおいて、パラメータエンジン153は、ユーザプロファイルデータベース153Bに記憶されたクライアントデバイス110の所与のユーザのユーザプロファイルに基づいて、候補パラメータの値を決定することができる。これらの実装のいくつかのさらなるバージョンにおいて、パラメータエンジン153は、所与のユーザからのいかなるさらなるユーザ入力も要求することなく、候補パラメータの値を決定することができる。パラメータエンジン153は、ユーザプロファイルデータベース153Bにアクセスすることができ、ソフトウェアアプリケーション(たとえば、カレンダーアプリケーション、電子メールアプリケーション、連絡先アプリケーション、リマインダアプリケーション、メモアプリケーション、SMSもしくはテキストメッセージングアプリケーション、および/またはその他のソフトウェアアプリケーション)から名前パラメータ、電話番号パラメータ、日付/時間パラメータの値を取り出すことができる。ユーザプロファイルは、たとえば、所与のユーザから許可を得て、所与のユーザのリンクされたアカウント、所与のユーザの電子メールアカウント、所与のユーザのフォトアルバム、所与のユーザのソーシャルメディアプロファイル、所与のユーザの連絡先、ユーザプリファレンス、および/またはその他の情報を含み得る。たとえば、クライアントデバイス110の所与のユーザが、支援された通話を開始するためのユーザ入力を与える前に、友人とのテキストメッセージングの会話、および所与のエンティティにおけるレストラン予約のための日付/時間情報の検討に従事している場合、パラメータエンジン153は、日付/時間パラメータの値としてテキストメッセージングの会話からの日付/時間情報を利用することができ、自動アシスタントは、レストラン予約のための日付/時間情報に関して所与のユーザに促す必要がない。(たとえば、図4Bに関連して)本明細書においてより詳細に検討されるように、クライアントデバイス110の所与のユーザは、支援された通話が開始される前に、候補パラメータの値を修正することができる。 In some versions of these implementations, parameter engine 153 may determine values for candidate parameters based on a user profile for a given user of client device 110 stored in user profile database 153B. In some further versions of these implementations, parameter engine 153 may determine values for candidate parameters without requesting any further user input from a given user. Parameter engine 153 may access user profile database 153B and may include software applications (e.g., calendar applications, email applications, contacts applications, reminder applications, notes applications, SMS or text messaging applications, and/or other software). You can retrieve the values of name parameters, phone number parameters, and date/time parameters from applications). A user profile can be created, for example, with permission from a given user, a given user's linked accounts, a given user's email account, a given user's photo album, a given user's social media It may include a profile, a given user's contacts, user preferences, and/or other information. For example, before a given user of client device 110 provides user input to initiate an assisted call, a text messaging conversation with a friend, and a date/time for a restaurant reservation at a given entity. When engaged in reviewing information, the parameter engine 153 can utilize date/time information from a text messaging conversation as the value of a date/time parameter, and the automated assistant can utilize the date/time information for a restaurant reservation. There is no need to prompt a given user for time information. As discussed in more detail herein (e.g., in connection with FIG. 4B), a given user of client device 110 may modify the values of candidate parameters before the assisted call is initiated. can do.
さらに、様々な実装において、所与のタスクのための一部の候補パラメータは、必須のパラメータであってよく、一方、所与のタスクのためのその他の候補パラメータは任意のパラメータであってよい。それらの実装のいくつかのバージョンにおいて、所与のパラメータが必須のパラメータであるのかまたは任意のパラメータであるのかは、タスクに基づき得る。言い換えれば、所与のタスクのための必須のパラメータは、所与のタスクを実行するために知られている必要がある最小限の量の情報であることが可能である。たとえば、レストラン予約のタスクに関しては、名前パラメータおよび時間/日付パラメータのみが、必須のパラメータであってよい。しかし、任意のパラメータ(たとえば、予約によりしっかりと対応するための人数、何らかの追加の連絡が必要とされる場合にかけるための電話番号など)の値が知られている場合、レストラン予約のタスクは、所与のユーザとレストラン予約のタスクに関連するレストランとの両方により大きな恩恵をもたらす可能性がある。したがって、自動アシスタント115は、少なくとも必須のパラメータに関するプロンプトを生成することができ、任意で、任意のパラメータに関するプロンプトを生成することができる。 Additionally, in various implementations, some candidate parameters for a given task may be required parameters, while other candidate parameters for a given task may be optional parameters. . In some versions of those implementations, whether a given parameter is a required or optional parameter may be based on the task. In other words, mandatory parameters for a given task can be the minimum amount of information that needs to be known to perform the given task. For example, for a restaurant reservation task, the name and time/date parameters may be the only required parameters. However, if the value of any parameter (e.g. number of people to better accommodate the reservation, phone number to call if some additional contact is required, etc.) is known, then the task of making a restaurant reservation is , can provide greater benefits to both a given user and the restaurant associated with the restaurant reservation task. Thus, the automated assistant 115 can generate prompts for at least the required parameters, and optionally can generate prompts for any parameters.
実装においては、支援された通話システム180が、クライアントデバイス110の所与のユーザとの進行中の通話に従事しているさらなるユーザがパラメータの情報を要求すると判定するとき、パラメータエンジン153が、ユーザプロファイルデータベース153Bに記憶されたクライアントデバイス110の所与のユーザのユーザプロファイルに基づいて値を決定することができる。それらの実装のいくつかのバージョンにおいて、パラメータエンジン153は、さらなるユーザがパラメータの情報を要求していることの特定に応じて、パラメータの値を決定することができる。それらの実装のその他のバージョンにおいて、パラメータエンジン153は、パラメータの情報を要求するクライアントデバイス110の所与のユーザからのユーザ入力に応じて、パラメータの値を決定することができる。 In implementations, when assisted calling system 180 determines that a further user engaged in an ongoing call with a given user of client device 110 requests parameter information, parameter engine 153 The value may be determined based on a user profile for a given user of client device 110 stored in profile database 153B. In some versions of their implementation, the parameter engine 153 may determine the value of the parameter in response to identifying that a further user is requesting information for the parameter. In other versions of these implementations, the parameter engine 153 may determine the value of the parameter in response to user input from a given user of the client device 110 requesting information for the parameter.
様々な実装において、タスク実行エンジン154は、自動アシスタント115に、タスクを実行するための支援された通話中に、合成音声を使用して、特定されたエンティティに関連するさらなるユーザと対話させることができる。タスク実行エンジン154は、合成音声オーディオデータを生成するために、クライアントデバイス110の音声合成エンジン140A1および/または支援された通話システム180の音声合成エンジン140A2に少なくとも値を含むテキストおよび/または音素を提供することができる。合成音声オーディオデータは、さらなるクライアントデバイスにおいて聴覚的にレンダリングするために、さらなるユーザのさらなるクライアントデバイスに送信され得る。音声合成エンジン140A1および/または140A2は、音声合成モデル140Aを使用して、少なくともパラメータの値に対応する合成音声を含む合成音声オーディオデータを生成することができる。たとえば、音声合成エンジン140A1および/または140A2は、さらなるユーザによって要求されたパラメータの情報に対応すると判定された音素のシーケンスを決定することができ、音声合成モデル140Aを使用して音素のシーケンスを処理して、合成音声オーディオデータを生成することができる。合成音声オーディオデータは、たとえば、オーディオ波形の形態であることが可能である。少なくともパラメータの値に対応する音素のシーケンスを決定する際に、音声合成エンジン140A1および/または140A2は、クライアントデバイス110のローカルに記憶された、または(たとえば、ネットワーク190を介して)サーバに記憶されたトークンから音素へのマッピングにアクセスすることができる。 In various implementations, the task execution engine 154 may cause the automated assistant 115 to interact with additional users associated with the identified entity using synthesized speech during the assisted call to perform the task. can. Task execution engine 154 provides the text and/or phonemes including at least the values to speech synthesis engine 140A1 of client device 110 and/or speech synthesis engine 140A2 of assisted calling system 180 to generate synthesized speech audio data. can do. The synthesized speech audio data may be sent to a further client device of a further user for aural rendering at the further client device. Speech synthesis engine 140A1 and/or 140A2 may use speech synthesis model 140A to generate synthesized speech audio data that includes synthesized speech that corresponds to at least the value of the parameter. For example, speech synthesis engine 140A1 and/or 140A2 may further determine a sequence of phonemes that is determined to correspond to the parameter information requested by the user and process the sequence of phonemes using speech synthesis model 140A. The synthesized speech audio data can be generated using the synthesized speech audio data. The synthesized speech audio data may be in the form of an audio waveform, for example. In determining the sequence of phonemes corresponding to the values of at least the parameters, speech synthesis engine 140A1 and/or 140A2 may be configured to perform a sequence of phonemes stored locally on client device 110 or stored on a server (e.g., over network 190). You can access the token-to-phoneme mappings.
一部の実装いおいて、タスク実行エンジン154は、クライアントデバイス110に、クライアントデバイス110の所与のユーザの代理として関わり合うエンティティとの支援された通話を開始させ、クライアントデバイス110の所与のユーザの代理としてタスクを実行させることができる。さらに、タスク実行エンジン154は、少なくともパラメータの値を含む合成音声オーディオデータを利用して、クライアントデバイス110の所与のユーザの代理としてタスクを実行することができる。たとえば、レストラン予約を行うタスクに関して、自動アシスタント115は、さらなるユーザに関連するさらなるクライアントデバイスにおいて、クライアントデバイス110の所与のユーザの代理として自動アシスタント115を特定し、支援された通話中に所与のユーザの代理として実行されるタスクを述べる合成音声(たとえば、「こちらはJane Doeの代理として予約をするために電話しているJane Doeの自動アシスタントです」)をレンダリングさせることができる。 In some implementations, the task execution engine 154 causes the client device 110 to initiate an assisted call with an entity that engages on behalf of a given user of the client device 110. You can have tasks performed on your behalf. Additionally, task execution engine 154 may utilize synthesized speech audio data that includes values of at least the parameters to perform tasks on behalf of a given user of client device 110. For example, with respect to the task of making a restaurant reservation, the automated assistant 115 may identify the automated assistant 115 on behalf of a given user of the client device 110 at a further client device associated with the additional user and make a given request during an assisted call. A synthesized voice can be rendered that states a task to be performed on behalf of a user (for example, "This is Jane Doe's automated assistant calling to make a reservation on Jane Doe's behalf").
それらの実装のいくつかのバージョンにおいて、自動アシスタント115は、エンティティに関連するさらなるユーザによって明示的に要求されることなく、(たとえば、上述のようにパラメータエンジン153を使用して決定された)様々な候補パラメータの対応する値をレンダリングさせることができる。上述の例を続けると、自動アシスタント115は、予約のための日付/時間パラメータの値(たとえば、「今夜7時」など)、予約のための人数パラメータの値(たとえば、「2」、「3」、「4」など)、予約のための座席の種類パラメータの値(たとえば、「ボックス席」、「屋内席」など)、および/またはその他の候補パラメータのその他の値を対話の始めに提供することができる。これらの実装のその他のバージョンにおいて、自動アシスタント115は、さらなるコンピューティングデバイスのさらなるユーザと対話し、エンティティに関連するさらなるユーザによって明示的に要求されたパラメータの特定の値を提供することができる。上述の例を続けると、自動アシスタント115は、さらなるユーザのスピーチ(たとえば、「何時に、何名様でしょうか」など)をキャプチャするオーディオデータを処理することができ、さらなるユーザからの要求の受け取りに応じて、さらなるユーザによって要求されているパラメータの情報(たとえば、「今夜7時に5名」など)を決定することができる。 In some versions of those implementations, the automated assistant 115 can perform various changes (e.g., determined using the parameter engine 153 as described above) without being explicitly requested by a further user associated with the entity. The corresponding values of the candidate parameters can be rendered. Continuing with the example above, the automated assistant 115 determines the value of the date/time parameter for the reservation (e.g., "7 o'clock tonight"), the value of the number of people parameter for the reservation (e.g., "2", "3"), ", "4", etc.), the value of the seat type parameter for the reservation (e.g. "box seat", "indoor seat", etc.), and/or other values for other candidate parameters at the beginning of the interaction. can do. In other versions of these implementations, the automated assistant 115 may interact with additional users of additional computing devices and provide particular values for parameters explicitly requested by the additional users associated with the entity. Continuing with the example above, the automated assistant 115 may process audio data that captures additional user speech (e.g., "What time and how many people are you here?") and receives requests from additional users. Depending on the parameter information requested by the additional users (eg, "5 people tonight at 7 o'clock", etc.) can be determined.
さらに、それらの実装のいくつかのバージョンにおいて、タスク決定エンジン154は、エンティティに関連するさらなるユーザからの要求が、自動アシスタント115が対応するさらなる値を知らないさらなるパラメータに関連する情報の要求であると判定し得る。たとえば、レストラン予約のタスクに関して、自動アシスタント115は、日付/時間情報パラメータ、人数パラメータ、および座席の種類パラメータの値を知っているが、さらなるユーザが、知られていない子供パラメータの情報(すなわち、子供が予約に参加するかどうか)を要求すると仮定する。さらなるユーザが知られていない子供パラメータの情報を要求しているとの判定に応じて、自動アシスタント115は、(たとえば、レンダリングエンジン113を使用して)子供パラメータのさらなる値がさらなるユーザによって要求されたことを示し、クライアントデバイス110の所与のユーザに子供パラメータのさらなる値を提供するように促す通知をクライアントデバイス110にレンダリングさせることができる。 Furthermore, in some versions of their implementation, the task decision engine 154 determines that further user requests related to the entity are requests for information related to further parameters to which the automated assistant 115 does not know the corresponding further values. It can be determined that For example, with respect to the task of making a restaurant reservation, the automated assistant 115 knows the values of the date/time information parameter, the number of people parameter, and the seating type parameter, but the additional user has information about the unknown child parameter (i.e. Suppose you want to request whether or not the child will participate in the reservation. In response to determining that the additional user requests information for the unknown child parameter, the automated assistant 115 (e.g., using the rendering engine 113) determines whether additional values for the child parameter are requested by the additional user. A notification may be rendered on the client device 110 indicating that the child parameter has changed and prompting the given user of the client device 110 to provide further values for the child parameter.
それらの実装のいくつかのさらなるバージョンにおいて、クライアントデバイス110においてレンダリングされる通知の種類、および/または通知をレンダリングする1つもしくは複数のプロパティ(たとえば、ボリューム、明るさ、サイズ)は、(たとえば、デバイス状態エンジン112を使用して決定された)クライアントデバイス110の状態および/または進行中の通話の状態に基づき得る。進行中の通話の状態は、たとえば、進行中の通話においてどの値が伝達されたかおよび/もしくはまだ伝達されていないかを示すことができ、ならびに/または進行中の通話のタスクのどのコンポーネントが進行中の通話において完了されたかおよび/もしくはまだ完了されていないかを示すことができる。クライアントデバイス110の状態は、たとえば、クライアントデバイス110のフォアグラウンドで動作するソフトウェアアプリケーション、クライアントデバイス110のバックグラウンドで動作するソフトウェアアプリケーション、クライアントデバイス110がロック状態であるかどうか、クライアントデバイス110がスリープ状態であるかどうか、クライアントデバイス110がオフ状態であるかどうか、クライアントデバイス110のセンサからのセンサデータ、および/またはその他のデータに基づき得る。たとえば、クライアントデバイス110の状態が、支援された通話の文字起こしを表示するソフトウェアアプリケーション(たとえば、自動アシスタントアプリケーション、通話アプリケーション、支援された通話アプリケーション、および/またはその他のソフトウェアアプリケーション)がクライアントデバイス110のフォアグラウンドで動作していることを示す場合、通知の種類は、バナー通知、ポップアップ通知、および/またはその他の種類の視覚的通知であってよい。別の例として、クライアントデバイス110の状態が、クライアントデバイス110がスリープ状態またはロック状態であることを示す場合、通知の種類は、スピーカによる聴覚的インジケーションおよび/またはスピーカもしくはクライアントデバイス110のその他のハードウェア構成要素による振動であってよい。さらに別の例として、クライアントデバイスの存在センサ、加速度計、および/またはその他のセンサからのセンサデータが、所与のユーザがクライアントデバイスの近くに現在いないおよび/またはクライアントデバイスを現在保持していないことを示す場合、(たとえば、視覚的なおよび第1のボリュームレベルの聴覚的な)より嵌入的な通知が提供され得る。一方、そのようなセンサデータが、所与のユーザがクライアントデバイスの近くに現在いるおよび/またはクライアントデバイスを現在保持していることを示す場合、(たとえば、視覚のみの、または視覚的なおよび第1の音量レベルよりも小さい第2の音量レベルの聴覚的な)より嵌入的でない通知が提供され得る。さらに別の例として、対話の状態が、対話が終わりに近いことを示す場合、より嵌的な通知が提供され得る一方、対話の状態が、対話が終わりに近くないことを示す場合は、より嵌入的でない通知が提供され得る。 In some further versions of those implementations, the type of notification rendered at client device 110 and/or the one or more properties (e.g., volume, brightness, size) that render the notification (e.g., may be based on the state of the client device 110 (determined using device state engine 112) and/or the state of the call in progress. The status of an ongoing call may indicate, for example, which values have been and/or have not yet been communicated in the ongoing call, and/or which components of the ongoing call's tasks are in progress. It can indicate whether the current call has been completed and/or has not yet been completed. The state of client device 110 may include, for example, software applications running in the foreground of client device 110, software applications running in the background of client device 110, whether client device 110 is in a locked state, and whether client device 110 is in a sleep state. may be based on whether the client device 110 is in an off state, sensor data from sensors on the client device 110, and/or other data. For example, the state of client device 110 indicates that a software application (e.g., an automated assistant application, a calling application, an assisted calling application, and/or other software application) that displays a transcription of an assisted call is When indicating running in the foreground, the type of notification may be a banner notification, pop-up notification, and/or other type of visual notification. As another example, if the state of the client device 110 indicates that the client device 110 is asleep or locked, the type of notification may include an audible indication over the speaker and/or an audio indication over the speaker or other The vibrations may be due to hardware components. As yet another example, sensor data from a client device's presence sensors, accelerometers, and/or other sensors indicates that a given user is not currently in the vicinity of the client device and/or is not currently holding the client device. If so, more intrusive notifications (eg, visual and audible at the first volume level) may be provided. On the other hand, if such sensor data indicates that a given user is currently in the vicinity of and/or currently holding a client device (e.g., visual-only or visual and A less intrusive notification may be provided (audible at a second volume level that is less than the first volume level). As yet another example, more intrusive notifications may be provided if the state of the interaction indicates that the interaction is near the end, whereas more intrusive notifications may be provided if the state of the interaction indicates that the interaction is not near the end. Non-intrusive notifications may be provided.
それらの実装のいくつかのさらなるバージョンにおいては、たとえ自動アシスタント115がさらなるユーザによって要求されたさらなるパラメータの対応するさらなる値を知らなくても、タスク決定エンジン153が、自動アシスタント115にさらなるユーザとの対話を継続させることができる。そのとき、自動アシスタント115は、さらなるユーザからの要求に応答する更なるユーザ入力を受け取った後、対話の中で後でさらなる値を提供することができる。たとえば、レストラン予約のタスクに関して、自動アシスタント115が、日付/時間情報パラメータ、人数パラメータ、および座席の種類パラメータの値を知っていると仮定し、さらに、さらなるユーザが、日付/時間情報パラメータおよび人数パラメータの値を要求すると仮定する。さらに、さらなるユーザが、次に、自動アシスタント115が値を知らない子供パラメータの値を要求すると仮定する。この例において、タスク決定エンジン153は、自動アシスタント115に、さらなるユーザのさらなるクライアントデバイスにおいて、子供が参加するかどうかに関する要求された情報が現在知られておらず、子供パラメータの値を含む、通知に応答するさらなるユーザ入力がクライアントデバイス110において検出されるまで(たとえば、座席の種類の)その他の値を提供することによって自動アシスタント115が支援された通話を継続し得るというインジケーションを含む合成音声をレンダリングさせることができる。さらなるユーザ入力の受け取りに応じて、自動アシスタント115は、さらなる値を単独の値として(たとえば、「子供は来ません」)、または後続の値として(たとえば、「Jane Doeはボックス席を希望しています。なお、子供は来ません」)提供することができる。 In some further versions of those implementations, the task decision engine 153 may cause the automated assistant 115 to interact with a further user, even if the automated assistant 115 does not know the corresponding further values of the additional parameters requested by the additional user. It is possible to continue the dialogue. The automated assistant 115 may then provide further values later in the interaction after receiving further user input in response to further requests from the user. For example, assume that for the task of making a restaurant reservation, the automated assistant 115 knows the values of the date/time information parameter, the number of people parameter, and the seating type parameter, and that a further user knows the values of the date/time information parameter and the number of people parameter. Suppose you want to request the value of a parameter. Further assume that a further user then requests a value for a child parameter for which automatic assistant 115 does not know the value. In this example, the task decision engine 153 notifies the automated assistant 115 that, at the further client device of the further user, the requested information regarding whether the child will participate is not currently known and includes the value of the child parameter. a synthetic voice that includes an indication that the automated assistant 115 may continue the assisted call by providing other values (e.g., seat type) until further user input is detected at the client device 110 in response to the can be rendered. In response to receiving further user input, the automated assistant 115 may provide the additional value as a standalone value (e.g., "No children are coming") or as a subsequent value (e.g., "Jane Doe wants a box seat." Please note that children are not allowed to come.
さらに、実装において、さらなるユーザが知られていないさらなるパラメータの情報を要求する場合、自動アシスタント115は、情報を要求する通知に応答するさらなるユーザ入力が閾値の継続時間(たとえば、15秒、30秒、60秒、および/またはその他の継続時間)内に受け取られない場合、支援された通話を終了し得る。それらの実装のいくつかのバージョンにおいて、閾値の継続時間は、情報を要求する通知が所与のユーザのクライアントデバイス110においてレンダリングされるときに始まり得る。それらの実装のその他のバージョンにおいて、閾値の継続時間は、自動アシスタント115に知られている最後の値がさらなるユーザによって要求されるかまたは(さらなるユーザが要求することとは無関係に)自動アシスタント115によって先回りして提供されるときに始まり得る。 Additionally, in implementations, if an additional user requests information for an additional parameter that is not known, the automated assistant 115 will indicate that the additional user input in response to the notification requesting the information is of a threshold duration (e.g., 15 seconds, 30 seconds). , 60 seconds, and/or other duration), the assisted call may be terminated. In some versions of those implementations, the threshold duration may begin when a notification requesting information is rendered at a given user's client device 110. In other versions of those implementations, the duration of the threshold is determined by whether the last value known to the automated assistant 115 is requested by a further user or whether the automated assistant 115 It can begin when proactively provided by.
さらに、実装において、さらなるユーザが知られていないさらなるパラメータの情報を要求する場合、フィードバックエンジン155は、さらなるパラメータをタスクのための候補パラメータとしてパラメータデータベース153Aに記憶することができる。それらの実装のいくつかのバージョンにおいて、フィードバックエンジン155は、さらなるパラメータを、エンティティデータベース151Aに記憶された、さらなるユーザに関連するエンティティにマッピングすることができる。それらの実装のいくつかのさらなるバージョンにおいて、フィードバックエンジン155は、支援されたスピーチ(assisted speech)がアクティブである間に、エンティティに関連するさらなるユーザが複数のユーザにさらなるパラメータを閾値の回数要求する場合に、さらなるパラメータをエンティティにマッピングし得る。たとえば、レストランのエンティティが、それぞれのクライアントデバイスを介して複数のユーザによって開始された複数の支援された通話にわたって対話中に、レストラン予約が子供を含むかどうかを少なくとも閾値の回数(たとえば、100回、1000回、および/またはその他の数の閾値)尋ねる場合、フィードバックエンジン155は、エンティティデータベース151Aにおいて子供パラメータを様々なレストランのエンティティにマッピングし得る。この例において、子供パラメータは、様々なレストランのエンティティおよび/または子供パラメータの情報を頻繁に要求する特定のエンティティとのレストラン予約のタスクのための将来の支援された通話を開始する前に値を求める新しい候補パラメータと考えられ得る。これらの方法およびその他の方法で、将来の支援された通話を開始する前に値が求められることが可能であり、それによって、将来の支援された通話の継続時間を短縮し、および/または将来の支援された通話において値に関するプロンプトをレンダリングするのに計算リソースを利用する必要を阻止する。 Additionally, in implementations, if additional users request information for additional parameters that are not known, feedback engine 155 may store the additional parameters as candidate parameters for the task in parameter database 153A. In some versions of their implementation, feedback engine 155 may map additional parameters to additional user-related entities stored in entity database 151A. In some further versions of those implementations, the feedback engine 155 causes the additional user associated with the entity to request additional parameters from the multiple users a threshold number of times while assisted speech is active. If so, additional parameters may be mapped to the entity. For example, a restaurant entity may determine whether a restaurant reservation includes a child at least a threshold number of times (e.g., 100 times) during an interaction across multiple assisted calls initiated by multiple users via their respective client devices. , 1000 times, and/or other number threshold), the feedback engine 155 may map the child parameters to various restaurant entities in the entity database 151A. In this example, the child parameter is set to a value prior to initiating a future assisted call for a restaurant reservation task with various restaurant entities and/or certain entities that frequently request child parameter information. It can be considered as a new candidate parameter to seek. In these and other ways, values may be determined before initiating future assisted calls, thereby reducing the duration of future assisted calls and/or Prevents the need to utilize computational resources to render prompts for values in assisted calls.
一部の実装において、タスク実行エンジン154は、クライアントデバイス110の所与のユーザの代理としてタスクを実行するために、たとえ進行中の通話が支援されたスピーチを使用して開始されなかった場合(すなわち、支援されていない通話)でも、自動アシスタント115に、エンティティに関連するさらなるユーザとの進行中の通話に適した出力を提供させることができる。たとえば、自動アシスタント115は、進行中の通話を中断して、さらなるクライアントデバイス110においてレンダリングされる値を含む合成音声を生じさせ得る。それらの実装のいくつかのバージョンにおいて、自動アシスタント115は、(たとえば、図5Aに関連して説明されるように)クライアントデバイス110の所与のユーザからのユーザ入力に応じて、進行中の通話に適した出力を提供することができる。それらの実装のいくつかのさらなるバージョンにおいて、自動アシスタント115は、進行中の通話に対応するオーディオデータを処理しない場合があり、それによって、さらなるユーザから同意を得る必要をなくす。むしろ、自動アシスタント115は、進行中の通話に関連するメタデータを分析し、メタデータおよび/またはユーザ入力に基づいて、さらなるユーザによって要求されているパラメータの対応する値を決定することができる。たとえば、クライアントデバイス110は、支援された通話を作動させ、さらなるユーザによって要求された対応する値(たとえば、マイレージ会員番号パラメータの値)を取り出すように自動アシスタント115に求めるユーザ入力を検出することができ、たとえ特定のエンティティが要求の中で明示的に特定されなかったとしても、進行中の通話に関連するメタデータに基づいて、値が特定のエンティティ(たとえば、Example Airlines)に関連付けられると判定することができる。そして、ユーザのアクセス制限されたデータが、さらなるユーザの要求(たとえば、「マイレージ会員」)とメタデータ(たとえば、「Example Airlines」)との両方に基づく検索パラメータ(たとえば、語)を使用して、自動アシスタントによって検索され得る。さらに、それらの実装のいくつかのさらなるバージョンにおいて、自動アシスタント115が進行中の通話に適した出力を提供するためのユーザ入力は、(たとえば、図5Bに関連して説明されるように)自動アシスタント115によって生成され、クライアントデバイス110において(たとえば、レンダリングエンジン113を使用して)レンダリングされる、支援された通話システム180がさらなるユーザに値を提供することができるという通知に応じるものである。たとえば、自動アシスタント115は、支援された通話システム180がさらなるユーザによって要求されているパラメータの対応する値を提供することができることを、クライアントデバイス110の所与のユーザに先回りして通知し得る。 In some implementations, task execution engine 154 is configured to perform tasks on behalf of a given user of client device 110, even if the ongoing call was not initiated using assisted speech ( (i.e., unassisted calls) can also cause the automated assistant 115 to provide output suitable for ongoing calls with further users associated with the entity. For example, automated assistant 115 may interrupt an ongoing call and produce a synthesized voice that includes the value to be rendered at further client device 110. In some versions of those implementations, automated assistant 115 responds to user input from a given user of client device 110 (e.g., as described in connection with FIG. 5A) to can provide output suitable for In some further versions of their implementation, automated assistant 115 may not process audio data corresponding to an ongoing call, thereby obviating the need to obtain consent from further users. Rather, the automated assistant 115 may analyze metadata associated with the ongoing call and, based on the metadata and/or user input, determine corresponding values for the parameters being requested by the further user. For example, client device 110 may detect user input prompting automated assistant 115 to activate an assisted call and retrieve a corresponding value requested by a further user (e.g., a value for a frequent flyer number parameter). determine that a value is associated with a particular entity (for example, Example Airlines) based on metadata related to the call in progress, even if that particular entity was not explicitly identified in the request. can do. The user's restricted access data is then searched using search parameters (e.g., terms) based on both the user's request (e.g., "frequent flyer") and the metadata (e.g., "Example Airlines"). , can be searched by an automated assistant. Moreover, in some further versions of those implementations, the user input for automated assistant 115 to provide output appropriate to an ongoing call is automatically It is in response to a notification generated by assistant 115 and rendered at client device 110 (eg, using rendering engine 113) that assisted calling system 180 can provide value to additional users. For example, automated assistant 115 may proactively notify a given user of client device 110 that assisted calling system 180 can provide corresponding values for parameters requested by the additional user.
それらの実装のその他のバージョンにおいて、自動アシスタント115は、(たとえば、図5Cに関連して説明されるように)情報がさらなるユーザによって要求されているとの判定に応じて、クライアントデバイス110の所与のユーザからいかなるユーザ入力も受け取ることなく、進行中の通話に適した出力(たとえば、進行中の通話内の合成音声)を提供する。それらの実装のいくつかのバージョンにおいて、自動アシスタント115は、値に関する信頼性測定指標が信頼性の閾値を満たすことに基づいて値を自動的に提供し得る。信頼性測定指標は、たとえば、所与のユーザが、同じ値の以前の要求の受け取りに応じて、決定された値を以前提供したかどうか、進行中の通話中に特定されたパラメータが、進行中の通話中に特定されたタスクに関連付けて記憶された候補パラメータであるかどうか、値が決定されるソース(たとえば、電子メール/カレンダーアプリケーション対テキストメッセージングアプリケーション)、および/または信頼性測定指標を決定する方法に基づき得る。たとえば、クライアントデバイス110の所与のユーザと航空会社のエンティティに関連するさらなるユーザとの間の進行中の通話に関して、支援された通話システム180は、さらなるユーザによって要求されているマイレージ会員番号を決定することができ、支援された通話システム180は、自動アシスタント115に、進行中の通話の一部として、マイレージ会員番号を含む合成音声を自動的に提供させることができ--自動アシスタント115がマイレージ会員番号を提供することを要求するいかなるユーザ入力も受け取ることなく合成音声を自動的に提供することができる。これらの実装のいくつかのバージョンにおいて、クライアントデバイス110の所与のユーザは、支援された通話に関連する設定において、進行中の通話を自動的に中断する権限を支援された通話に与えなければならない場合がある。 In other versions of those implementations, automated assistant 115 locates client device 110 in response to determining that information is requested by a further user (e.g., as described in connection with FIG. 5C). Provide output appropriate to an ongoing call (eg, synthesized speech within an ongoing call) without receiving any user input from a given user. In some versions of their implementation, automated assistant 115 may automatically provide a value based on a confidence metric for the value meeting a confidence threshold. Confidence metrics may include, for example, whether a given user has previously provided the determined value in response to receipt of a previous request for the same value, whether an identified parameter during an ongoing call is whether the candidate parameters are stored in association with the task identified during the call, the source from which the value is determined (e.g., email/calendar application vs. text messaging application), and/or the reliability metric. may be based on the method of determination. For example, with respect to an ongoing call between a given user of client device 110 and a further user associated with an airline entity, assisted calling system 180 determines the frequent flyer number requested by the additional user. and the assisted calling system 180 can cause the automated assistant 115 to automatically provide a synthesized voice that includes the frequent flyer number as part of an ongoing call -- Synthesized speech can be automatically provided without receiving any user input that requires providing a membership number. In some versions of these implementations, a given user of client device 110 must grant an assisted call permission to automatically interrupt an ongoing call in the settings associated with the assisted call. There may be cases where this is not the case.
様々な実装において、推薦エンジン156は、対話において伝達される候補値を決定することができ、自動アシスタント115に、クライアントデバイス110の所与のユーザのための推薦として候補値を提供させることができる。一部の実装において、候補値は、ネットワーク190を介してクライアントデバイス110に送信され得る。さらに、候補値は、さらなるユーザからの要求に応じた推薦として、(たとえば、レンダリングエンジン113を使用して)クライアントデバイスのディスプレイ上に視覚的にレンダリングされ得る。それらの実装のいくつかのバージョンにおいては、(たとえば、ユーザ入力エンジン111によって判定されるように)ユーザ入力が所与の推薦を対象とするときに、所与の推薦が、さらなるユーザのさらなるクライアントデバイスにおいて聴覚的にレンダリングされる合成音声に組み込まれ得るように、推薦は、選択可能であり得る。 In various implementations, recommendation engine 156 can determine candidate values to be conveyed in the interaction and can cause automated assistant 115 to provide the candidate values as recommendations for a given user of client device 110. . In some implementations, candidate values may be sent to client device 110 via network 190. Further, the candidate values may be visually rendered (eg, using rendering engine 113) on a display of the client device as a recommendation upon request from a further user. In some versions of those implementations, when a user input is directed to a given recommendation (as determined by user input engine 111, for example), a given recommendation is sent to a further client of a further user. Recommendations may be selectable such that they may be incorporated into synthetic speech that is audibly rendered at the device.
一部の実装において、推薦エンジン156は、さらなるユーザからの情報の要求に基づいて候補値を決定することができる。たとえば、要求がはいかいいえかの質問(たとえば、「予約にお子様も含まれますか」)である場合、推薦エンジン156は、「はい」の値を含む第1の推薦と、「いいえ」の値を含む第2の推薦とを決定することができる。その他の実装において、推薦エンジン156は、ユーザプロファイルデータベース153Bに記憶されたクライアントデバイス110に関連する所与のユーザのユーザプロファイルに基づいて候補値を決定することができる。たとえば、要求が特定の情報を求める場合(たとえば、「お子様は何人ですか」)、推薦エンジン156は、所与のユーザからの許可を得て、所与のユーザのユーザプロファイルに基づいて、クライアントデバイス110の所与のユーザが3人の子供がいると判定することができ--「3」の値を含む第1の推薦、「2」の値を含む第2の推薦、「1」の値を含む第3の推薦、および/またはその他の値を含むその他の推薦を決定することができる。本明細書において説明される様々な推薦は、クライアントデバイスに関連する所与のユーザへの提示のために、クライアントデバイス110において視覚的および/または聴覚的にレンダリングされ得る。 In some implementations, recommendation engine 156 may determine candidate values based on requests for information from additional users. For example, if the request is a yes-or-no question (e.g., "Does your reservation include children?"), the recommendation engine 156 selects a first recommendation with a value of "Yes" and a "No" value. A second recommendation including a value may be determined. In other implementations, recommendation engine 156 may determine candidate values based on a given user's user profile associated with client device 110 stored in user profile database 153B. For example, if the request asks for specific information (e.g., "How many children do you have?"), the recommendation engine 156, with permission from the given user and based on the given user's user profile, may A given user of device 110 may determine that there are three children -- a first recommendation containing a value of "3," a second recommendation containing a value of "2," a second recommendation containing a value of "1," and so on. A third recommendation containing the value and/or other recommendations containing other values may be determined. The various recommendations described herein may be visually and/or audibly rendered at client device 110 for presentation to a given user associated with the client device.
本明細書において説明されるように、レンダリングエンジン113は、クライアントデバイス110において様々な通知またはその他の出力をレンダリングすることができる。レンダリングエンジン113は、本明細書において説明される様々な通知を聴覚的および/または視覚的にレンダリングすることができる。さらに、レンダリングエンジン113は、クライアントデバイス110のユーザインターフェース上に対話の文字起こしをレンダリングさせることができる。一部の実装において、文字起こしは、(たとえば、図4Bに関連して説明されるように)クライアントデバイス110の所与のユーザと自動アシスタント115との間の対話に対応し得る。その他の実装において、文字起こしは、(たとえば、図4Cおよび図4Dに関連して説明されるように)さらなるクライアントデバイスのさらなるユーザと自動アシスタント115との間の対話に対応し得る。さらにその他の実装において、文字起こしは、(たとえば、図5A～図5Cに関連して説明されるように)クライアントデバイス110の所与のユーザと、さらなるクライアントデバイスのさらなるユーザと、自動アシスタント115との間の対話に対応し得る。 As described herein, rendering engine 113 may render various notifications or other output at client device 110. Rendering engine 113 may audibly and/or visually render various notifications described herein. Additionally, rendering engine 113 may cause a transcription of the interaction to be rendered on the user interface of client device 110. In some implementations, a transcription may correspond to an interaction between a given user of client device 110 and automated assistant 115 (eg, as described in connection with FIG. 4B). In other implementations, the transcription may correspond to an interaction between a further user of a further client device and the automated assistant 115 (eg, as described in connection with FIGS. 4C and 4D). In still other implementations, transcription is performed between a given user of client device 110, a further user of a further client device, and an automated assistant 115 (e.g., as described in connection with FIGS. 5A-5C). It can correspond to the dialogue between.
一部の実装において、スケジューリングエンジン114は、タスクの実行の結果に基づいて自動アシスタントがさらなる(または後続の)タスクを実行する推薦を、タスクの実行の結果を示す通知とともにおよび/またはタスクの実行の結果を示す通知に含めて、自動アシスタント115に含めさせることができる。それらの実装のいくつかのバージョンにおいては、(たとえば、ユーザ入力エンジン111によって判定されるように)ユーザ入力が所与の推薦を対象とするときに、所与の推薦がさらなるタスクを実行させ得るように、推薦は、選択可能であり得る。たとえば、成功したレストラン予約のタスクに関して、自動アシスタント115は、クライアントデバイス110のディスプレイのユーザインターフェースによって、クライアントデバイス110の所与のユーザによって選択されるとスケジューリングエンジン114に成功したレストラン予約に関するカレンダー項目を作成させる選択可能な要素をレンダリングすることができる。別の例として、成功したレストラン予約のタスクに関して、自動アシスタント115は、レストラン予約のタスクが成功裏に実行されたことを示すSMSまたはテキストメッセージを、レストラン予約に参加しているその他のユーザに送信することができる。対照的に、失敗したレストラン予約のタスクに関して、自動アシスタント115は、クライアントデバイス110の所与のユーザによって選択されると、スケジューリングエンジン114に、レストラン予約のタスクを後でおよび試みられたレストラン予約のタスクの時間/日付の値の前に再び実行する(たとえば、後で自動アシスタント115によって自動的に実行されるか、またはリマインダおよび/もしくはカレンダー項目のユーザの選択に応じて自動アシスタント115によって実行される)ためのリマインダおよび/またはカレンダー項目を作成させる選択可能な要素をレンダリングすることができる。 In some implementations, the scheduling engine 114 provides recommendations for the automated assistant to perform further (or subsequent) tasks based on the results of the task execution, along with notifications indicating the results of the task execution and/or the task execution. may be included in a notification indicating the results of the automatic assistant 115. In some versions of those implementations, a given recommendation may cause further tasks to be performed when user input targets the given recommendation (e.g., as determined by user input engine 111). As such, recommendations may be selectable. For example, with respect to the task of making a successful restaurant reservation, the automated assistant 115 may cause the scheduling engine 114 to send a calendar entry regarding the successful restaurant reservation to the scheduling engine 114 when selected by a given user of the client device 110 via the user interface of the display of the client device 110. Allows you to render selectable elements to create. As another example, for a successful restaurant reservation task, the automated assistant 115 sends an SMS or text message to other users participating in the restaurant reservation indicating that the restaurant reservation task was successfully performed. can do. In contrast, with respect to the failed restaurant reservation task, the automated assistant 115, once selected by a given user of the client device 110, requests the scheduling engine 114 to request the restaurant reservation task later and for the attempted restaurant reservation. Run again before the time/date value of the task (e.g., automatically run later by the automatic assistant 115, or run by the automatic assistant 115 in response to user selection of reminders and/or calendar items) A selectable element can be rendered that allows the creation of reminders and/or calendar items for
その他の実装において、スケジューリングエンジン114は、自動アシスタント115に、タスクの実行の結果の決定に応じて、タスクの実行の結果に基づいてさらなる(または後続の)タスクを自動的に実行させることができる。たとえば、成功したレストラン予約のタスクに関して、自動アシスタント115は、成功したレストラン予約に関するカレンダー項目を自動的に作成し、レストラン予約のタスクが成功裏に実行されたことを示すSMSもしくはテキストメッセージをレストラン予約に参加しているその他のユーザに自動的に送信し、および/またはレストラン予約のタスクの成功裏の実行に応じて自動アシスタント115によって実行され得るその他のさらなるタスクを自動的に実行することができる。対照的に、失敗したレストラン予約のタスクに関して、自動アシスタント115は、レストラン予約のタスクを後でおよびレストラン予約のタスクの時間/日付の値の前に再び実行する(たとえば、後で自動アシスタント115によって自動的に実行されるか、またはリマインダおよび/もしくはカレンダー項目のユーザの選択に応じて自動アシスタント115によって実行される)ためのリマインダおよび/またはカレンダー項目を自動的に作成することができる。 In other implementations, the scheduling engine 114 may cause the automated assistant 115 to automatically perform further (or subsequent) tasks based on the results of the task execution, in response to determining the results of the task execution. . For example, with respect to a successful restaurant reservation task, the automated assistant 115 may automatically create a calendar entry regarding the successful restaurant reservation and send an SMS or text message indicating that the restaurant reservation task was successfully performed to the restaurant reservation. and/or automatically perform other further tasks that may be performed by the automated assistant 115 in response to successful performance of the restaurant reservation task. . In contrast, for a failed Restaurant Reservation task, the automated assistant 115 will run the Restaurant Reservation task again later and before the time/date value of the Restaurant Reservation task (e.g., later by the automated assistant 115). Reminders and/or calendar items can be automatically created (either automatically or by automated assistant 115 in response to a user's selection of reminders and/or calendar items).
本明細書において説明される技術を使用することによって、様々な技術的利点が、実現され得る。非限定的な一例として、自動アシスタント115は、自動アシスタント115に現在知られていない情報がさらなるユーザによって要求されるときに、さらなるパラメータの値を待つために支援された通話の対話が停止されないので、支援された通話をより迅速に終わらせることができる。本明細書において開示される技術を使用することによって、支援された通話の長さが短縮され得るので、ネットワークリソースと計算リソースとの両方が節約され得る。別の非限定的な例として、自動アシスタント115は、進行中の通話中の所与のユーザによるタスクの実行中に、パラメータの対応する値を提供することができる。上述のように対応する値を自動的にかまたは明示的なユーザ入力に応じてかのどちらかで提供することによって、ユーザが、対応する値を決定するためにまったく異なるユーザインターフェースを有する様々なアプリケーションにナビゲートする必要がないので、クライアントデバイス110は、クライアントデバイス110の所与のユーザからより少ない入力を受け取り、それによって、所与のクライアントデバイスの計算リソースを節約する。さらに、ユーザがこれらの様々なアプリケーションにナビゲートする必要がないので、システムは、進行中の通話をより迅速に終わらせることによって計算リソースとネットワークリソースとの両方を節約する。 Various technical advantages may be realized by using the techniques described herein. As a non-limiting example, the automated assistant 115 may not stop the assisted call interaction to wait for further parameter values when information not currently known to the automated assistant 115 is requested by a further user. , allowing assisted calls to end more quickly. By using the techniques disclosed herein, the length of assisted calls may be reduced, thereby saving both network and computational resources. As another non-limiting example, automated assistant 115 may provide corresponding values for parameters during the performance of a task by a given user during an ongoing call. By providing the corresponding values either automatically or in response to explicit user input as described above, the user can use various Because there is no need to navigate to an application, client device 110 receives less input from a given user of client device 110, thereby saving computational resources on a given client device. Furthermore, since the user does not have to navigate to these various applications, the system saves both computational and network resources by terminating ongoing calls more quickly.
図2は、様々な実装による、支援された通話を実行する例示的な方法200を示す流れ図を示す。便宜上、方法200の動作は、動作を実行するシステムに関連して説明される。方法200のこのシステムは、コンピューティングデバイス(たとえば、図1のクライアントデバイス110、図4A～図4Dのクライアントデバイス410、図5A～図5Cのクライアントデバイス510、図6のコンピューティングデバイス610、1つもしくは複数のサーバ、および/またはその他のコンピューティングデバイス)の1つもしくは複数のプロセッサおよび/またはその他の構成要素を含む。さらに、方法200の動作は特定の順序で示されるが、これは、限定的であるように意図されていない。1つまたは複数の動作が、順序を変えられるか、省略されるか、または追加される可能性がある。 FIG. 2 depicts a flow diagram illustrating an example method 200 of performing assisted calling, according to various implementations. For convenience, operations of method 200 are described with respect to a system that performs the operations. The system of method 200 includes one computing device (e.g., client device 110 of FIG. 1, client device 410 of FIGS. 4A-4D, client device 510 of FIGS. 5A-5C, computing device 610 of FIG. 6). or multiple servers and/or other computing devices). Additionally, although the operations of method 200 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, or added.
ブロック252において、システムは、所与のユーザに関連するクライアントデバイスを介して所与のユーザから、支援された通話を開始するためのユーザ入力を受け取る。一部の実装において、ユーザ入力は、クライアントデバイスのマイクロフォンによって検出される発話された入力である。たとえば、発話された入力は、「Example Cafeに電話して」、または特に「支援された通話を使ってExample Cafeに電話して」を含み得る。その他の実装において、ユーザ入力は、クライアントデバイスにおいて検出されたタッチ入力である。たとえば、タッチ入力は、クライアントデバイス上で様々なソフトウェアアプリケーション(たとえば、ブラウザアプリケーション、メッセージングアプリケーション、電子メールアプリケーション、メモアプリケーション、リマインダアプリケーション、および/またはその他のソフトウェアアプリケーション)が動作している間に検出され得る。 At block 252, the system receives user input from a given user via a client device associated with the given user to initiate an assisted call. In some implementations, the user input is spoken input detected by a microphone of the client device. For example, the spoken input may include "Call Example Cafe," or specifically, "Call Example Cafe using assisted calling." In other implementations, the user input is touch input detected at the client device. For example, touch input may be detected while various software applications (e.g., browser applications, messaging applications, email applications, notes applications, reminder applications, and/or other software applications) are running on the client device. obtain.
ブロック254において、システムは、支援された通話を開始するためのユーザ入力に応じて、支援された通話中に所与のユーザの代理として関わり合うエンティティを特定する。エンティティは、ユーザ入力に基づいて特定され得る。実装において、ユーザ入力が発話された入力であるとき、エンティティは、(たとえば、図1の音声認識モデル120Aおよび/またはNLUモデル130Aを使用して)発話された入力をキャプチャするオーディオデータを処理して、発話された入力に含まれるエンティティ(たとえば、企業のエンティティ、特定の企業のエンティティ、場所のエンティティ、および/またはその他のエンティティ)を特定することに基づいて特定され得る。実装において、ユーザ入力がタッチ入力であるとき、エンティティは、クライアントデバイスとのユーザのインタラクション(たとえば、エンティティに関連する連絡先のエントリ、エンティティに関連する検索結果、エンティティに関連する広告を選択するタッチ入力、および/またはその他のユーザのインタラクション)に基づいて特定され得る。 At block 254, the system identifies an entity to engage on behalf of a given user during the assisted call in response to user input to initiate the assisted call. Entities may be identified based on user input. In an implementation, when the user input is spoken input, the entity processes audio data that captures the spoken input (e.g., using speech recognition model 120A and/or NLU model 130A of FIG. 1). may be identified based on identifying entities included in the spoken input (eg, business entities, specific business entities, location entities, and/or other entities). In an implementation, when the user input is a touch input, the entity receives the user's interaction with the client device (e.g., the entry of a contact related to the entity, a search result related to the entity, a touch that selects an advertisement related to the entity). and/or other user interactions).
ブロック256において、システムは、ユーザ入力および/またはエンティティに基づいて、支援された通話中に所与のユーザの代理として実行される少なくとも1つのタスクを決定する。様々な実装において、予め定義されたタスクが、1つまたは複数のデータベース(たとえば、図1のエンティティデータベース151A)に複数の対応するエンティティに関連付けて記憶され得る。たとえば、フライトの予約、フライトの変更、フライトのキャンセル、ロストバゲージの問い合わせのタスク、および/またはその他のタスクが、複数の異なる航空会社のエンティティに関連付けて記憶され得る。一部の実装において、実行される少なくとも1つのタスクは、ユーザ入力に基づいて決定され得る。たとえば、「Example Cafeに電話して今夜7時に予約をして」という発話された入力がクライアントデバイスにおいて受け取られる場合、システムは、発話された入力がレストラン予約のタスクを含むと判定することができる。その他の実装において、実行される少なくとも1つのタスクは、ブロック254において特定されたエンティティに基づいて決定され得る。たとえば、「Example Cafeに電話して」という発話された入力がクライアントデバイスにおいて受け取られる場合(つまり、レストラン予約のタスクを明示していない場合)、システムは、それがレストランのエンティティに関連する予め定義されたタスクであることに基づいて、予約を行うタスクを推測することができる。 At block 256, the system determines at least one task to be performed on behalf of a given user during the assisted call based on the user input and/or entity. In various implementations, predefined tasks may be stored in one or more databases (eg, entity database 151A of FIG. 1) in association with multiple corresponding entities. For example, flight booking, flight change, flight cancellation, lost baggage inquiry tasks, and/or other tasks may be stored in association with multiple different airline entities. In some implementations, at least one task to perform may be determined based on user input. For example, if the spoken input "Call Example Cafe and make a reservation tonight at 7 o'clock" is received at the client device, the system may determine that the spoken input includes the task of making a restaurant reservation. . In other implementations, at least one task to perform may be determined based on the entity identified at block 254. For example, if the spoken input "Call Example Cafe" is received at a client device (i.e., does not specify the task of making a restaurant reservation), the system will determine if it is related to the restaurant entity's predefined The task to be reserved can be inferred based on the fact that the task has been reserved.
ブロック258において、システムは、少なくとも1つのタスクに関連する候補パラメータを特定する。様々な実装において、候補パラメータは、1つまたは複数のデータベース(たとえば、図1のパラメータデータベース153A)に、(ブロック256において決定された)少なくとも1つのタスクに関連付けて、および/または(ブロック254において決定された)少なくとも1つのエンティティに関連付けて記憶され得る。たとえば、フライトの予約、フライトの変更、フライトのキャンセル、ロストバゲージの問い合わせのタスク、および/またはその他のタスクが、対応する候補パラメータに関連付けて記憶され得る。また、たとえば、Airline Entity 1に関連するフライトの変更のタスクが、第1の対応するパラメータに関連付けて記憶されることが可能であり、Airline Entity 2に関連するフライトの変更のタスクが、第2の対応するパラメータに関連付けて記憶されることが可能である。別の例として、Restaurant Entity 1が、第1の対応するパラメータに関連付けて記憶されることが可能であり、Restaurant Entity 2が、第2の対応するパラメータに関連付けて記憶されることが可能である。 At block 258, the system identifies candidate parameters associated with the at least one task. In various implementations, the candidate parameters are stored in one or more databases (e.g., parameter database 153A of FIG. 1), associated with at least one task (as determined in block 256), and/or (as determined in block 254). (determined) may be stored in association with at least one entity. For example, flight booking, flight change, flight cancellation, lost baggage inquiry tasks, and/or other tasks may be stored in association with corresponding candidate parameters. Also, for example, a flight change task related to Airline Entity 1 can be stored in association with a first corresponding parameter, and a flight change task related to Airline Entity 2 can be stored in association with a second corresponding parameter. can be stored in association with the corresponding parameters of the parameters. As another example, Restaurant Entity 1 may be stored in association with a first corresponding parameter, and Restaurant Entity 2 may be stored in association with a second corresponding parameter. .
ブロック260において、システムは、候補パラメータに関して、少なくとも1つのタスクの実行の際に使用される対応する値を決定する。一部の実装において、候補パラメータの値は、1つまたは複数のデータベース(たとえば、図1のユーザプロファイルデータベース153B)に記憶されている所与のユーザのユーザプロファイルに基づいて決定され得る。ユーザプロファイルは、たとえば、所与のユーザのリンクされたアカウント、所与のユーザの電子メールアカウント、所与のユーザのフォトアルバム、所与のユーザのソーシャルメディアプロファイル、所与のユーザの連絡先、ユーザプリファレンス、および/またはその他の情報を含み得る。たとえば、サロン予約のタスクに関して、システムは、連絡先アプリケーションに基づいて所与のユーザの名前パラメータおよび電話番号パラメータを決定し、特定のスタイリストとの以前の通信(たとえば、電子メールメッセージ、テキストもしくはSMSメッセージ、電話、および/またはその他の通信)に基づいてサロンの好ましいスタイリストを決定することができる。いくつかの追加的および/または代替的な実装において、候補パラメータの値は、追加的または代替的に、クライアントデバイスにおいて視覚的および/または聴覚的にレンダリングされ、パラメータの情報を要求するプロンプトに応じるさらなるユーザ入力に基づいて決定され得る。それらの実装のいくつかのバージョンにおいて、システムは、システムがユーザプロファイルに基づいて決定することができなかった候補パラメータの対応する値に関してのみプロンプトを生成し得る。たとえば、上述のサロン予約のタスクに関して、システムは、名前パラメータ、電話番号パラメータ、および電話番号パラメータの値を既に知っており、したがって、システムは、日付/時間パラメータの値を要求するプロンプトを生成するだけでよい。(たとえば、図4Bに関連して)本明細書において説明されるように、システムは、支援された通話を開始する前に、クライアントデバイスの所与のユーザが候補パラメータの値を修正するための機会を提供することができる。 At block 260, the system determines corresponding values for the candidate parameters to be used in performing the at least one task. In some implementations, values for candidate parameters may be determined based on a given user's user profile stored in one or more databases (eg, user profile database 153B of FIG. 1). A user profile may include, for example, a given user's linked accounts, a given user's email account, a given user's photo album, a given user's social media profile, a given user's contacts, May include user preferences and/or other information. For example, for the task of making a salon reservation, the system may determine name and phone number parameters for a given user based on the contacts application and any previous communications with a particular stylist (e.g., email message, text or SMS). A preferred stylist at the salon may be determined based on messages, phone calls, and/or other communications). In some additional and/or alternative implementations, the value of the candidate parameter is additionally or alternatively rendered visually and/or audibly at the client device in response to a prompt requesting information for the parameter. may be determined based on further user input. In some versions of those implementations, the system may generate prompts only for corresponding values of candidate parameters that the system was unable to determine based on the user profile. For example, for the salon booking task mentioned above, the system already knows the values for the name parameter, phone number parameter, and phone number parameter, so the system generates a prompt asking for the value of the date/time parameter. Just that is enough. As described herein (e.g., with respect to FIG. 4B), the system provides a method for a given user of a client device to modify the values of candidate parameters before initiating an assisted call. can provide opportunities.
ブロック262において、システムは、所与のユーザに関連するクライアントデバイスを使用して、候補パラメータの値を用いて少なくとも1つのタスクを実行するために、所与のユーザの代理として、エンティティとの支援された通話を開始する。システムは、さらなるコンピューティングデバイスからクライアントデバイスにおいて受信されたオーディオデータを処理して、さらなるユーザによって要求されているパラメータの値を決定することができる。さらに、システムは、ブロック254において特定されたエンティティに関連するさらなるユーザのさらなるクライアントデバイスに送信され、少なくともパラメータの値を含む合成音声オーディオを(たとえば、先回りして、またはさらなるユーザによる要求に応じて)生成することができる。一部の実装において、システムは、さらなるユーザと対話することができ、さらなるユーザによって要求された情報に含まれる特定の値を含む合成音声オーディオを生成することができる。たとえば、レストラン予約のタスクに関して、さらなるユーザは、日付/時間パラメータの情報を要求することができ、システムは、さらなるユーザからの要求が日付/時間パラメータの情報を求めるものであるとの判定に応じて、日付/時間の値を含む合成音声オーディオデータを生成することができる。さらに、システムは、合成音声オーディオデータをさらなるユーザのさらなるクライアントデバイスに送信させ、合成音声オーディオデータに含まれる合成音声をさらなるクライアントデバイスにおいて聴覚的にレンダリングさせることができる。さらなるユーザは、様々なパラメータのさらなる情報を要求することができ、システムは、レストラン予約のタスクを実行するために、さらなるユーザに値を提供することができる。 At block 262, the system coordinates with an entity on behalf of a given user to perform at least one task with the values of the candidate parameters using a client device associated with the given user. start a call. The system may process audio data received at the client device from the additional computing device to determine values for parameters requested by the additional user. Additionally, the system transmits the synthesized speech audio (e.g., proactively or upon request by the further user) including the values of at least the parameters to a further client device of a further user associated with the identified entity at block 254. ) can be generated. In some implementations, the system can interact with the additional user and generate synthesized speech audio that includes specific values included in the information requested by the additional user. For example, with respect to the task of making a restaurant reservation, a further user may request date/time parameter information, and the system responds by determining that the request from the further user is for date/time parameter information. can generate synthesized speech audio data that includes date/time values. Further, the system can cause the synthesized speech audio data to be transmitted to a further client device of the further user and cause the synthesized speech included in the synthesized speech audio data to be audibly rendered at the further client device. Further users may request further information for various parameters and the system may provide values to the further users to perform the task of restaurant reservation.
一部の実装において、方法200は、任意の下位ブロック262Aを含み得る。含まれる場合、任意の下位ブロック262Aにおいて、システムは、エンティティに関連するさらなるユーザから、支援された通話を監視するための同意を得る。たとえば、システムは、支援された通話を開始すると、タスクを実行する前に同意を得ることができる。システムが関連するさらなるユーザからの同意を得る場合、システムは、タスクを実行することができる。しかし、システムがさらなるユーザから同意を得ない場合、システムは、クライアントデバイスに、所与のユーザに対して、所与のユーザがタスクを実行することおよび/または通話を終了することが必要とされることを示す通知をレンダリングさせ、所与のユーザに対して、タスクが実行されなかったことを示す通知をレンダリングさせることができる。 In some implementations, method 200 may include optional subblock 262A. If included, in optional subblock 262A, the system obtains consent from additional users associated with the entity to monitor assisted calls. For example, when the system initiates an assisted call, it can obtain consent before performing a task. If the system obtains consent from the relevant further users, the system can perform the task. However, if the system does not obtain consent from further users, the system sends a request to the client device for the given user that the given user is required to perform tasks and/or end the call. A notification may be rendered for a given user indicating that the task was not performed.
ブロック264において、システムは、支援された通話中に、エンティティに関連するさらなるユーザによって、さらなるパラメータに関連する何らかの情報が要求されるかどうかを判定する。上述のように、システムは、さらなるコンピューティングデバイスからクライアントデバイスにおいて受信されたオーディオデータを処理して、さらなるユーザによって要求されている値を決定することができる。さらに、システムは、さらなるユーザによって要求されている値がシステムに現在知られていないように、要求されている値がシステムが以前に解決していないさらなるパラメータのものであるかどうかを判定することがでる。たとえば、システムがレストラン予約のタスクのための座席の種類パラメータの値を以前に決定していない場合、座席の種類パラメータは、システムに現在知られていない値を有するさらなるパラメータと考えられ得る。ブロック264の反復において、システムが、支援された通話中に、エンティティに関連するさらなるユーザによって、さらなるパラメータに関連する情報が要求されないと判定する場合、システムは、以下でより詳細に検討されるブロック272に進んでよい。 At block 264, the system determines whether any information related to additional parameters is requested by additional users associated with the entity during the assisted call. As described above, the system may process audio data received at the client device from the additional computing device to determine the value being requested by the additional user. Additionally, the system may determine whether the requested value is for a further parameter that the system has not previously resolved, such that the value requested by the further user is not currently known to the system. comes out. For example, if the system has not previously determined a value for the seat type parameter for a restaurant reservation task, the seat type parameter may be considered an additional parameter with a value not currently known to the system. If, in an iteration of block 264, the system determines that no information related to additional parameters is requested by additional users associated with the entity during the assisted call, the system executes the block 264 discussed in more detail below. You may proceed to 272.
任意のブロック266を含むブロック264の反復において、システムが、支援された通話中に、エンティティに関連するさらなるユーザからの要求にさらなるパラメータの値が含まれていると判定する場合、システムは、任意のブロック266に進んでよい。任意のブロック266を含む実装において、システムは、ブロック264からブロック266に直接進んでよく、ブロック266は、以下でより詳細に検討される。 In an iteration of block 264, including optional block 266, if the system determines that requests from additional users associated with the entity during the assisted call include values for additional parameters, the system You may proceed to block 266. In implementations that include optional block 266, the system may proceed directly from block 264 to block 266, which is discussed in more detail below.
含まれる場合、任意のブロック266において、システムは、所与のユーザに関連するクライアントデバイスの状態を決定する。クライアントデバイスの状態は、たとえば、クライアントデバイスのフォアグラウンドで動作するソフトウェアアプリケーション、クライアントデバイスのバックグラウンドで動作するソフトウェアアプリケーション、クライアントデバイス110がロック状態であるかどうか、クライアントデバイスがスリープ状態であるかどうか、クライアントデバイス110がオフ状態であるかどうか、クライアントデバイスのセンサからのセンサデータ、および/またはその他のデータに基づき得る。一部の実装において、システムは、追加的または代替的に、ブロック266において進行中の通話の状態を決定する。 If included, at optional block 266, the system determines the state of the client device associated with the given user. The state of the client device may include, for example, a software application running in the foreground of the client device, a software application running in the background of the client device, whether the client device 110 is in a locked state, whether the client device 110 is in a sleep state, It may be based on whether client device 110 is in an off state, sensor data from a sensor on the client device, and/or other data. In some implementations, the system additionally or alternatively determines the status of the ongoing call at block 266.
ブロック268において、システムは、所与のユーザに関連するクライアントデバイスに、そのさらなるパラメータを特定する通知をレンダリングさせる。通知は、さらに、情報に含まれるさらなるパラメータの値を要求し得る。任意のブロック268を含む実装において、クライアントデバイスによってレンダリングされる通知の種類、および/またはレンダリングに関する1つもしくは複数のプロパティは、任意のブロック268において決定されたクライアントデバイスの状態および/または進行中の通話の状態に基づき得る。たとえば、クライアントデバイスの状態が、支援された通話の文字起こしを表示するソフトウェアアプリケーション(たとえば、自動アシスタントアプリケーション、通話アプリケーション、支援された通話アプリケーション、および/またはその他のソフトウェアアプリケーション)がクライアントデバイスのフォアグラウンドで動作していることを示す場合、通知の種類は、バナー通知、ポップアップ通知、および/またはその他の種類の視覚的通知であってよい。別の例として、クライアントデバイスの状態が、クライアントデバイスがスリープ状態またはロック状態であることを示す場合、通知の種類は、スピーカによる聴覚的インジケーションおよび/またはスピーカもしくはクライアントデバイスのその他のハードウェア構成要素による振動であってよい。 At block 268, the system causes the client device associated with the given user to render a notification identifying its additional parameters. The notification may further request values for additional parameters included in the information. In implementations that include optional block 268, the type of notification rendered by the client device and/or one or more properties regarding the rendering may be determined based on the state of the client device and/or the ongoing May be based on the state of the call. For example, if the state of the client device is such that a software application that displays a transcription of an assisted call (e.g., an automated assistant application, a calling application, an assisted calling application, and/or other software application) is in the foreground of the client device. When indicating that it is working, the type of notification may be a banner notification, pop-up notification, and/or other type of visual notification. As another example, if the state of the client device indicates that the client device is asleep or locked, the type of notification may include an audible indication through the speaker and/or the speaker or other hardware configuration of the client device. It may be vibration caused by an element.
ブロック270において、システムは、閾値の継続時間内に所与のユーザに関連するクライアントデバイスにおいて何らかのさらなるユーザ入力が受け取られるかどうかを判定する。さらなるユーザ入力は、たとえば、情報を要求する通知に応じるさらなる発話された入力、さらなるタイプ入力、および/またはさらなるタッチ入力であることが可能である。一部の実装において、閾値の継続時間は、情報を要求する通知が所与のユーザのクライアントデバイスにおいてレンダリングされるときに始まり得る。その他の実装において、閾値の継続時間は、最後の値がさらなるユーザによって求められるときに開始し得る。ブロック270の反復において、さらなるユーザ入力が閾値の継続時間内に受け取られるとシステムが判定する場合、システムは、ブロック272に進んでよい。さらなるユーザ入力は、さらなるユーザによって要求されている情報を示す通知に応じて受け取られることが可能であり、要求に応じた値のインジケーションを含み得る。 At block 270, the system determines whether any additional user input is received at a client device associated with a given user within a threshold duration. Additional user input may be, for example, additional spoken input, additional typed input, and/or additional touch input in response to a notification requesting information. In some implementations, the threshold duration may begin when a notification requesting information is rendered on a given user's client device. In other implementations, the threshold duration may begin when the last value is determined by a further user. If, in an iteration of block 270, the system determines that additional user input is received within the threshold duration, the system may proceed to block 272. Additional user input may be received in response to the notification indicating information being requested by the additional user, and may include an indication of the requested value.
ブロック272において、システムは、候補パラメータおよび/またはさらなるパラメータの値に基づいて、少なくとも1つのタスクを完了する。実装において、支援された通話中に、ブロック264において、さらなるユーザからの情報の要求にさらなる値が含まれていないとシステムが判定する場合、システムは、ブロック260において決定された候補パラメータの対応する値を使用して、少なくとも1つのタスクを完了することができる。これらの実装において、システムは、クライアントデバイスの所与のユーザを関与させる必要なしに、支援された通話を完了することができる。実装において、支援された通話中に、ブロック264において、さらなるパラメータに関連する情報がさらなるユーザによって要求されるとシステムが判定する場合、システムは、ブロック260において決定された候補パラメータの対応する値と、ブロック270において受け取られたさらなるパラメータの値とを使用して少なくとも1つのタスクを完了することができる。これらの実装において、システムは、クライアントデバイスの所与のユーザからの最小限の関与で、支援された通話を完了することができる。ブロック272から、システムは、以下でより詳細に検討されるブロック276に進んでよい。 At block 272, the system completes at least one task based on the values of the candidate parameters and/or additional parameters. In implementations, during an assisted call, if the system determines at block 264 that the request for information from the additional user does not include additional values, the system determines that the corresponding candidate parameter determined at block 260 The value can be used to complete at least one task. In these implementations, the system can complete assisted calls without the need to involve a given user of the client device. In implementations, if during an assisted call, the system determines in block 264 that information related to additional parameters is requested by the additional user, the system determines the corresponding values of the candidate parameters determined in block 260. , and the values of the additional parameters received at block 270 may be used to complete at least one task. In these implementations, the system can complete assisted calls with minimal involvement from a given user of a client device. From block 272, the system may proceed to block 276, which is discussed in more detail below.
特に、ブロック264において、さらなるユーザがシステムに現在知られていない情報を要求しているとシステムが判定する場合があるが、システムは、さらなるパラメータの値なしで少なくとも1つのタスクの実行を継続し得る。たとえば、システムは、合成音声を支援された通話の一部として提供させることができ、それによって、その合成音声をさらなるユーザのさらなるクライアントデバイスにおいてレンダリングさせる。さらに、合成音声は、システムがさらなるパラメータの値を現在知らないが、システムがユーザに情報に関連する値を要求することができ、システムがクライアントデバイスの所与のユーザにさらなるパラメータの値を求める間、ブロック260において決定された候補パラメータのその他の値を提供することができることを示し得る。このようにして、システムは、タスクを実行するためにさらなるユーザと対話を続けることができる。さらに、さらなるユーザからの情報の要求に応じた値を含むさらなるユーザ入力が受け取られる場合、システムは、知られている値のうちの1つの提供の続きとして、または対話に中断があるときに単独の値として値を提供することができる。このようにして、さらなるパラメータの値を待つために対話が停止されないので、システムは、より迅速でより効率的に、クライアントデバイスの所与のユーザの代理としてタスクを成功裏に実行することができる。より迅速でより効率的にタスクを実行することによって、本明細書において開示される技術を使用することによって、会話の長さが短縮され得るので、ネットワークリソースと計算リソースとの両方が節約され得る。 In particular, at block 264, the system may determine that an additional user requests information that is not currently known to the system, but the system continues executing the at least one task without the value of the additional parameter. obtain. For example, the system can cause the synthesized voice to be provided as part of an assisted call, thereby causing the synthesized voice to be rendered on a further client device of a further user. Additionally, synthesized speech allows the system to request values associated with information from the user, even though the system does not currently know the values for the additional parameters, and for the system to request values for additional parameters from a given user of the client device. In the meantime, it may be indicated that other values of the candidate parameters determined in block 260 may be provided. In this way, the system can continue to interact with further users to perform tasks. Additionally, if further user input is received, including a value in response to a request for information from a further user, the system may perform a A value can be provided as the value of . In this way, the system is able to successfully perform tasks on behalf of a given user of a client device faster and more efficiently because the interaction is not stopped to wait for the value of further parameters. . By performing tasks faster and more efficiently, both network and computational resources may be saved as the length of conversations may be reduced by using the techniques disclosed herein. .
ブロック270の反復において、さらなるユーザ入力が閾値の継続時間内に受け取られないとシステムが判定する場合、システムは、ブロック274に進んでよい。ブロック274において、システムは、少なくとも1つのタスクの実行を終了する。さらに、システムは、エンティティとの進行中の通話を終了する場合がある。閾値の継続時間を超えてさらなるユーザ入力を待つのとは対照的に、進行中の通話を終了させることによって、たとえタスクが完全に実行され得ない場合でも、上述の技術的利点を達成するために、対話が、やはりより迅速に終わらせられ得る。ブロック274から、システムは、ブロック276に進んでよい。 If, in an iteration of block 270, the system determines that no further user input is received within the threshold duration, the system may proceed to block 274. At block 274, the system finishes executing the at least one task. Additionally, the system may terminate an ongoing call with the entity. To achieve the above-mentioned technical advantages by terminating an in-progress call, as opposed to waiting for further user input beyond a threshold duration, even if the task cannot be completely executed. Additionally, dialogue can also be concluded more quickly. From block 274, the system may proceed to block 276.
ブロック276において、システムは、クライアントデバイスによって、少なくとも1つのタスクの実行の結果を示す通知をレンダリングする。実装において、システムがブロック272からタスクの実行を完了する場合、通知は、クライアントデバイスの所与のユーザの代理としてタスクが完了されたというインジケーションを含むことができ、タスクの完了に関連する確認情報(たとえば、日付/時間情報、タスクに関連する金銭的コスト、確認番号、エンティティに関連する情報、および/またはその他の確認情報)を含むことができる。実装において、システムがブロック274からタスクの実行を終了する場合、通知は、タスクが完了されなかったというインジケーションを含むことができ、タスクの終了に関連するタスク情報(たとえば、必要とされる特定のパラメータの値、エンティティがブロック260において決定された対応する値および/もしくはブロック270において受け取られた値に対応することができないこと、エンティティが閉まっていること、ならびに/またはその他のタスク情報)を含むことができる。様々な実装において、通知は、選択されるときにシステムに、タスクの結果に基づいてカレンダー項目を作成させる、タスクの結果に基づいてリマインダを作成させる、タスクの結果を含むメッセージ(たとえば、テキスト、SMS、電子メール、および/もしくはその他のメッセージ)を送信させる、ならびに/またはそのユーザの選択に応じたその他のさらなるタスクを実行させることができる選択可能なグラフィカル要素を含み得る。 At block 276, the system renders a notification indicating the results of the execution of the at least one task by the client device. In implementations, if the system completes execution of the task from block 272, the notification may include an indication that the task has been completed on behalf of a given user of the client device, and a confirmation associated with the completion of the task. Information (eg, date/time information, monetary costs associated with the task, confirmation numbers, information related to the entity, and/or other confirmation information) may be included. In an implementation, if the system finishes executing the task from block 274, the notification can include an indication that the task was not completed, and includes task information related to the task completion (e.g., required specific , the entity is unable to correspond to the corresponding value determined in block 260 and/or the value received in block 270, the entity is closed, and/or other task information). can be included. In various implementations, notifications, when selected, cause the system to create a calendar item based on the results of the task, create a reminder based on the results of the task, or send a message containing the results of the task (e.g., text, SMS, email, and/or other messages) and/or perform other further tasks according to the user's selections.
図3は、様々な実装による、進行中の支援されていない通話中の補助出力の例示的な方法300を示す流れ図を示す。便宜上、方法300の動作は、動作を実行するシステムに関連して説明される。方法300のこのシステムは、コンピューティングデバイス(たとえば、図1のクライアントデバイス110、図4A～図4Dのクライアントデバイス410、図5A～図5Cのクライアントデバイス510、図6のコンピューティングデバイス610、1つもしくは複数のサーバ、および/またはその他のコンピューティングデバイス)の1つもしくは複数のプロセッサおよび/またはその他の構成要素を含む。さらに、方法300の動作は特定の順序で示されるが、これは、限定的であるように意図されていない。1つまたは複数の動作が、順序を変えられるか、省略されるか、または追加される可能性がある。 FIG. 3 depicts a flow diagram illustrating an example method 300 of auxiliary output during an ongoing unassisted call, according to various implementations. For convenience, the operations of method 300 are described with respect to a system that performs the operations. The system of method 300 includes one computing device (e.g., client device 110 of FIG. 1, client device 410 of FIGS. 4A-4D, client device 510 of FIGS. 5A-5C, computing device 610 of FIG. 6). or multiple servers and/or other computing devices). Additionally, although the operations of method 300 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, or added.
ブロック352において、システムは、クライアントデバイスにおいて、クライアントデバイスに関連する所与のユーザと、さらなるクライアントデバイスに関連するさらなるユーザとの間の進行中の通話を検出する。システムは、任意で、さらなるユーザに関連するエンティティも特定し得る。システムは、進行中の通話に関連するメタデータに基づいてエンティティを特定することができる。一部の実装において、方法300は、任意の下位ブロック352Aを含み得る。含まれる場合、任意の下位ブロック352Aにおいて、システムは、エンティティに関連するさらなるユーザから、進行中の通話を監視するための同意を得る。システムは、図2の任意の下位ブロック260Aに関連して説明された同じ方法で、さらなるユーザから同意を得ることができる。 At block 352, the system detects an ongoing call at the client device between a given user associated with the client device and a further user associated with a further client device. The system may optionally also identify additional user-related entities. The system may identify entities based on metadata associated with ongoing calls. In some implementations, method 300 may include optional subblock 352A. If included, in optional subblock 352A, the system obtains consent from additional users associated with the entity to monitor ongoing calls. The system may obtain consent from additional users in the same manner as described in connection with optional subblock 260A of FIG.
ブロック354において、システムは、進行中の通話に対応するオーディオデータのストリームを処理して、認識されたテキストを生成する。進行中の通話に対応するオーディオデータのストリームは、少なくとも、所与のユーザのクライアントデバイスに送信される、さらなるユーザのさらなる発話された入力を含み得る。進行中の通話に対応するオーディオデータのストリームは、所与のユーザの発話された入力も含み得る。さらに、システムは、音声認識モデル(たとえば、図1の音声認識モデル120A)を使用してオーディオデータのストリームを処理して、認識されたテキストを生成することができる。さらなるユーザが通話の監視に同意したと仮定して、システムは、進行中の通話に対応するオーディオデータのストリームを継続的に処理することができることを理解されたい。 At block 354, the system processes the stream of audio data corresponding to the ongoing call to generate recognized text. The stream of audio data corresponding to the ongoing call may include at least additional spoken input of additional users transmitted to a given user's client device. The stream of audio data corresponding to an ongoing call may also include spoken input of a given user. Further, the system can process the stream of audio data using a speech recognition model (eg, speech recognition model 120A of FIG. 1) to generate recognized text. It should be appreciated that the system may continuously process a stream of audio data corresponding to an ongoing call, assuming the additional user consents to monitoring the call.
ブロック356において、システムは、認識されたテキストに基づいて、進行中の通話中に所与のユーザによって実行される少なくとも1つのタスクのためのパラメータを特定する。システムは、NLUモデル(たとえば、図1のNLUモデル130A)を使用して、ブロック354からの認識されたテキストを処理して、オーディオデータのストリームに含まれる意図を決定することができる。一部の実装において、システムは、さらなるユーザのさらなるユーザ入力が少なくとも1つのタスクのパラメータの情報の要求を含むと判定し得る。たとえば、「この件の品質保証ケース番号をお持ちですか」という、さらなるユーザからのさらなる発話された入力が受け取られる場合、システムは、品質保証ケース番号パラメータを特定することができ、ユーザ入力が品質保証ケース番号パラメータの値の要求を含むと判定することができる。この例において、タスクは、航空会社のエンティティに関連する任意のタスク、および/または航空会社のエンティティに関連付けられるその他のパラメータにまったく関係なく品質保証ケース番号を提供する特定のタスクであることが可能である。 At block 356, the system identifies parameters for at least one task to be performed by the given user during the ongoing call based on the recognized text. The system may process the recognized text from block 354 using an NLU model (eg, NLU model 130A of FIG. 1) to determine the intent contained in the stream of audio data. In some implementations, the system may determine that the additional user input of the additional user includes a request for information of at least one task parameter. For example, if further spoken input from the user is received, such as "Do you have a quality assurance case number for this matter?", the system can identify the quality assurance case number parameter and the user input It can be determined that a request for the value of the quality assurance case number parameter is included. In this example, the task can be any task related to the airline entity and/or the specific task that provides a quality assurance case number without regard to any other parameters associated with the airline entity. It is.
ブロック358において、システムは、パラメータに関して、少なくとも1つのタスクの実行の際に使用される対応する値を決定する。対応する値は、ブロック356において少なくとも1つのタスクのためのパラメータを特定した後に決定され得る。一部の実装において、対応する値は、クライアントデバイスの所与のユーザに関連するユーザプロファイルに基づいて、少なくとも1つのタスクのパラメータの特定に応じて自動的に決定され得る。たとえば、品質保証ケース番号パラメータの特定に応じて、システムは、所与のユーザからの許可(たとえば、事前の許可)を得て、所与のユーザに関連する電子メールアカウントにアクセスし、品質保証ケース番号パラメータの対応する値を含む電子メールを検索することができる。さらに、実装において、進行中の通話中に所与のユーザと関わり合うエンティティが特定される場合、システムは、特定されたエンティティに関連する電子メールのみに検索を制限することができる。その他の実装において、対応する値は、自動的に特定されるのとは対照的に、パラメータの対応する値を要求する情報を含むユーザ入力の受け取りに応じて決定され得る。対応する値は、上述の同じまたは同様の方法で、ユーザ入力の受け取りに応じて決定され得る。 At block 358, the system determines corresponding values for the parameters to be used in performing the at least one task. A corresponding value may be determined after identifying parameters for the at least one task at block 356. In some implementations, the corresponding value may be automatically determined in response to identifying at least one task parameter based on a user profile associated with a given user of the client device. For example, depending on the identification of the Quality Assurance Case Number parameter, the system may, with permission (e.g., prior permission) from the given user, access the email account associated with the given user and Emails containing the corresponding value of the case number parameter can be searched for. Additionally, in implementations, if an entity is identified that engages a given user during an ongoing call, the system can limit the search to only emails related to the identified entity. In other implementations, the corresponding value may be determined in response to receiving user input that includes information requesting the corresponding value of the parameter, as opposed to being automatically determined. The corresponding value may be determined in response to receiving user input in the same or similar manner as described above.
一部の実装において、方法300は、任意のブロック360、362、および/または364を含み得る。含まれる場合、任意のブロック360において、システムは、支援された通話を作動させるための任意のユーザ入力が所与のユーザに関連するクライアントデバイスにおいて受け取られるかどうかを判定することができる。システムは、本明細書において説明される任意の方法で、クライアントデバイスの所与のユーザとさらなるクライアントデバイスのさらなるユーザとの間の進行中の通話中に支援された通話を呼び出す発話された入力、タイピングされた入力、および/またはタッチ入力に基づいて、ユーザ入力が支援された通話を作動させるかどうかを判定することができる。任意のブロック360の反復において、支援された通話を作動させるためのユーザ入力が受け取られるとシステムが判定する場合、システムは、以下でより詳細に検討されるブロック366に進むことができる。任意のブロック360の反復において、支援された通話を作動させるためのユーザ入力が受け取られないとシステムが判定する場合、システムは、任意のブロック362に進むことができる。 In some implementations, method 300 may include optional blocks 360, 362, and/or 364. If included, at optional block 360, the system may determine whether any user input to activate an assisted call is received at a client device associated with a given user. The system includes: a spoken input invoking an assisted call during an ongoing call between a given user of a client device and a further user of a further client device, in any manner described herein; Based on typed input and/or touch input, it can be determined whether the user input activates an assisted call. If, in any iteration of block 360, the system determines that user input is received to activate an assisted call, the system may proceed to block 366, discussed in more detail below. If, in an iteration of optional block 360, the system determines that no user input is received to activate an assisted call, the system may proceed to optional block 362.
含まれる場合、任意のブロック362において、システムは、所与のユーザに関連するクライアントデバイスによって、支援された通話が少なくとも1つのタスクを実行することができることを示す通知をレンダリングすることができる。通知は、たとえば、ブロック356において特定されたパラメータの、ブロック358において決定された対応する値の情報をさらなるユーザが要求しているというインジケーションを含むことができ、システムが所与のユーザの代理としてさらなるユーザに対応する値を提供することができるというインジケーションも含むことができる。通知は、視覚的および/または聴覚的にレンダリングされ得る。実装において、通知が聴覚的にレンダリングされる場合、通知は、クライアントデバイスのさらなるユーザが通知を知覚しないように(すなわち、通話外)、クライアントデバイスにおいてのみ聴覚的にレンダリングされてよい。任意で、さらなるユーザが通知を知覚する可能性を少なくするために、進行中の通話は、通知の聴覚的なレンダリング中、一時的にミュートされることが可能であり、または音響エコーキャンセル(acoustic echo cancellation)もしくはその他のフィルタリングが、通知をフィルタリングし、通知が進行中の通話の一部として提供されるのを防止するために利用されることが可能である。その他の実装において、通知が聴覚的にレンダリングされる場合、通知は、通知が所与のユーザとさらなるユーザとの間の進行中の通話を中断させるように、所与のユーザのクライアントデバイスとさらなるユーザのさらなるクライアントデバイスとの両方において聴覚的にレンダリングされる場合がある。 If included, at optional block 362, the system may render a notification indicating that the assisted call can perform at least one task by a client device associated with a given user. The notification may include, for example, an indication that a further user is requesting information about the corresponding value determined in block 358 of the parameter identified in block 356, and the system It may also include an indication that the corresponding value may be provided to further users as a. Notifications may be rendered visually and/or audibly. In implementations, if the notification is rendered audibly, the notification may be rendered audibly only at the client device so that no further users of the client device perceive the notification (i.e., outside of the call). Optionally, to reduce the likelihood of further users perceiving the notification, an ongoing call can be temporarily muted during the audible rendering of the notification, or by acoustic echo cancellation (acoustic echo cancellation). echo cancellation) or other filtering can be used to filter notifications and prevent them from being provided as part of an ongoing call. In other implementations, if the notification is rendered aurally, the notification is rendered audibly between a given user's client device and an additional user such that the notification interrupts an ongoing call between the given user and the additional user. It may be rendered aurally both at the user's further client device.
含まれる場合、任意のブロック364において、システムは、支援された通話を作動させるための何らかのユーザ入力が所与のユーザに関連するクライアントデバイスにおいて受け取られるかどうかを判定することができる。ブロック364において受け取られるユーザ入力は、支援された通話が少なくとも1つのタスクを実行することができることを示す通知のレンダリングに応じるものであることが可能である。システムは、本明細書において説明される任意の方法で、クライアントデバイスの所与のユーザとさらなるクライアントデバイスのさらなるユーザとの間の進行中の通話中に支援された通話を呼び出す発話された入力、タイピングされた入力、および/またはタッチ入力に基づいて、ユーザ入力が支援された通話を作動させるかどうかを判定することができる。任意のブロック360の反復において、支援された通話を作動させるためのユーザ入力が受け取られないとシステムが判定する場合、システムは、ブロック354に戻って、進行中の通話に対応するさらなるオーディオデータを処理することができる。たとえば、システムは、クライアントデバイスの所与のユーザが、ブロック362において通知の中でレンダリングされた対応する値を含む発話された入力を与えたと判定する場合があり、システムは、ブロック354に戻り、さらなるユーザによって要求されている任意のさらなるパラメータを監視するためにオーディオデータのストリームを処理することを継続することができる。任意のブロック364の反復において、支援された通話を作動させるためのユーザ入力が受け取られるとシステムが判定する場合、システムは、ブロック366に進むことができる。 If included, at optional block 364, the system may determine whether any user input is received at a client device associated with a given user to activate an assisted call. The user input received at block 364 may be responsive to rendering a notification indicating that the assisted call is capable of performing at least one task. The system includes: a spoken input invoking an assisted call during an ongoing call between a given user of a client device and a further user of a further client device, in any manner described herein; Based on typed input and/or touch input, it can be determined whether the user input activates an assisted call. If, in any iteration of block 360, the system determines that no user input is received to activate an assisted call, the system returns to block 354 to receive additional audio data corresponding to the call in progress. can be processed. For example, the system may determine that a given user of the client device has provided spoken input that includes a corresponding value rendered in the notification at block 362, and the system returns to block 354 to Processing of the stream of audio data may continue to monitor any further parameters requested by further users. If, in any iteration of block 364, the system determines that user input is received to activate an assisted call, the system may proceed to block 366.
ブロック366において、システムは、さらなるユーザに対する提示のために、さらなるクライアントデバイスにおいて値をレンダリングさせる。システムは、対応する値をさらなるユーザに提供するために支援された通話を作動させるためのユーザ入力の受け取るに応じて、対応する値を含む合成音声をさらなるユーザのさらなるクライアントデバイスおよび/または所与のユーザのクライアントデバイスにおいてレンダリングさせることができる。 At block 366, the system causes the values to be rendered at further client devices for presentation to further users. In response to receiving user input for activating an assisted call to provide the corresponding value to the further user, the system transmits synthesized speech containing the corresponding value to a further client device of the further user and/or to a given user. can be rendered on the user's client device.
任意のブロック360、362、および/または364を含む実装において、システムは、支援された通話を呼び出す明示的なユーザ入力の受け取りに応じて、さらなるユーザのさらなるクライアントデバイスおよび/または所与のユーザのクライアントデバイスにおいて対応する値をレンダリングさせることができる。それらの実装のいくつかのバージョンにおいて、支援された通話を作動させ、進行中の通話を中断するためのユーザ入力は、先回りしたものであることが可能である。言い換えると、システムがブロック360でユーザ入力を受け取る場合、(たとえば、図5Aに関連して説明されるように)たとえシステムが少なくとも1つのタスクを実行できることを示すいかなる通知もシステムがレンダリングしなかった場合でも、タスクのための対応する値を提供するために、支援された通話が作動され得る。それらの実装のその他のバージョンにおいて、支援された通話を作動させるためのユーザ入力は、事後的であることが可能である。言い換えると、システムがブロック364でユーザ入力を受け取る場合、(たとえば、図5Bに関連して説明されるように)システムが少なくとも1つのタスクを実行できることを示す通知の、ブロック362におけるレンダリングの後に、タスクのための対応する値を提供するために、支援された通話が作動され得る。任意のブロック360、362、および/または364を含まない実装において、システムは、ブロック258からブロック366に直接進むことができる。それらの実装のいくつかのバージョンにおいて、システムは、(たとえば、図5Cに関連して説明されるように)ブロック358における対応する値の決定に応じて、進行中の通話を自動的に(すなわち、支援された通話を作動させるためのいかなる明示的なユーザ入力も受け取ることなく)中断し得る。 In implementations that include optional blocks 360, 362, and/or 364, the system, in response to receiving explicit user input to invoke an assisted call, may call additional client devices of additional users and/or a given user. A corresponding value can be rendered on the client device. In some versions of those implementations, user input to activate an assisted call and interrupt a call in progress may be proactive. In other words, if the system receives user input at block 360, even if the system does not render any notification indicating that the system is capable of performing at least one task (e.g., as described in connection with FIG. 5A) Even in this case, an assisted call can be activated to provide the corresponding value for the task. In other versions of these implementations, user input to activate assisted calling may be reactive. In other words, if the system receives user input at block 364, after the rendering at block 362 of a notification indicating that the system can perform at least one task (e.g., as described in connection with FIG. 5B), An assisted call may be activated to provide corresponding values for the task. In implementations that do not include any blocks 360, 362, and/or 364, the system can proceed directly from block 258 to block 366. In some versions of those implementations, the system automatically (i.e. , without receiving any explicit user input to activate the assisted call).
ブロック368において、システムは、支援された通話を継続するための何らかのユーザ入力が所与のユーザに関連するクライアントデバイスにおいて受け取られるかどうかを判定する。ブロック368の反復において、支援された通話を継続するためのユーザ入力が受け取られないとシステムが判定する場合、システムは、ブロック354に戻って、進行中の通話に対応するさらなるオーディオデータを処理することができる。ブロック368の反復において、支援された通話を継続するためのユーザ入力が受け取られるとシステムが判定する場合、システムは、図2のブロック264に進み、支援された通話中にさらなるパラメータのいずれかの値が必要とされるかどうかを判定することができる。このようにして、システムは、通話中の所与のユーザによるタスクの実行中に、パラメータの対応する値を提供することができる。上述のように対応する値を自動的にかまたは明示的なユーザ入力に応じてかのどちらかで提供することによって、ユーザが、対応する値を決定するためにまったく異なるユーザインターフェースを有する様々なアプリケーションにナビゲートする必要がないので、システムは、クライアントデバイスの所与のユーザからより少ない入力を受け取り、それによって、所与のクライアントデバイスの計算リソースを節約する。さらに、ユーザがこれらの様々なアプリケーションにナビゲートする必要がないので、システムは、進行中の通話をより迅速に終わらせることによって計算リソースとネットワークリソースとの両方を節約する。1つの非限定的な例として、本明細書において説明される技術を使用することによって、所与のユーザは、対応する値に関して電子メールアプリケーション、航空会社のアプリケーション、および/またはその他のアプリケーションを検索する間、対話を一時停止するかまたは対話を保留にする必要がない。 At block 368, the system determines whether any user input to continue the assisted call is received at the client device associated with the given user. If, in an iteration of block 368, the system determines that no user input is received to continue the assisted call, the system returns to block 354 to process further audio data corresponding to the call in progress. be able to. If, in an iteration of block 368, the system determines that user input is received to continue the assisted call, the system proceeds to block 264 of FIG. You can determine whether a value is required. In this way, the system can provide corresponding values of parameters during the performance of tasks by a given user during a call. By providing the corresponding values either automatically or in response to explicit user input as described above, the user can use various Because there is no need to navigate to an application, the system receives less input from a given user of a client device, thereby saving computational resources on a given client device. Additionally, since the user does not have to navigate to these various applications, the system saves both computational and network resources by terminating ongoing calls more quickly. As one non-limiting example, by using the techniques described herein, a given user can search email applications, airline applications, and/or other applications for corresponding values. There is no need to pause the interaction or put the interaction on hold while doing so.
ここで図4A～図4Dを参照して、支援された通話の実行に関連するユーザインターフェースの様々な非限定的な例が、説明される。図4A～図4Dは、それぞれ、クライアントデバイス410の所与のユーザのインタラクションの例を表示するグラフィカルユーザインターフェース480を有するクライアントデバイス410を示す。インタラクションは、たとえば、1つまたは複数のソフトウェアアプリケーション(たとえば、ウェブブラウザアプリケーション、自動アシスタントアプリケーション、連絡先アプリケーション、電子メールアプリケーション、カレンダーアプリケーション、および/またはクライアントデバイス410によってアクセス可能なその他のソフトウェアに基づくアプリケーション)とのインタラクション、ならびにさらなるユーザ(たとえば、さらなるクライアントデバイスに関連するさらなる人間の参加者、さらなるユーザのさらなるクライアントデバイスに関連するさらなる自動アシスタント、および/またはその他のさらなるユーザ)とのインタラクションを含み得る。クライアントデバイス410に関連する自動アシスタント(たとえば、図1の自動アシスタント115)の1つまたは複数の態様は、クライアントデバイス410のローカルに実装されてよく、および/またはクライアントデバイス410とネットワーク通信するその他のクライアントデバイスに(たとえば、図1のネットワーク190を介して)分散して実装されてよい。簡単にするために、図4A～図4Dの動作は、自動アシスタントによって実行されるものとして本明細書において説明される。図4A～図4Dのクライアントデバイス410は、モバイル電話として描かれているが、それは限定的であるように意図されていないことを理解されたい。クライアントデバイス410は、たとえば、(たとえば、スピーカおよび/もしくはディスプレイを有する)スタンドアロンのアシスタントデバイス、ラップトップ、デスクトップコンピュータ、ならびに/または電話をかけることができる任意のその他のクライアントデバイスであることが可能である。 Referring now to FIGS. 4A-4D, various non-limiting examples of user interfaces related to performing assisted calls will be described. 4A-4D each illustrate a client device 410 having a graphical user interface 480 that displays an example of a given user's interaction with the client device 410. The interaction may be, for example, an application based on one or more software applications (e.g., a web browser application, an automated assistant application, a contacts application, an email application, a calendar application, and/or other software accessible by the client device 410). ), as well as interactions with further users (e.g., further human participants associated with further client devices, further automated assistants associated with further client devices of the further user, and/or other further users). . One or more aspects of an automated assistant (e.g., automated assistant 115 in FIG. 1) associated with client device 410 may be implemented locally on client device 410 and/or on other devices in network communication with client device 410. It may be implemented in a distributed manner on client devices (eg, via network 190 in FIG. 1). For simplicity, the operations of FIGS. 4A-4D are described herein as being performed by an automated assistant. Although client device 410 in FIGS. 4A-4D is depicted as a mobile phone, it should be understood that this is not intended to be limiting. Client device 410 can be, for example, a standalone assistant device (e.g., with speakers and/or display), a laptop, a desktop computer, and/or any other client device capable of making phone calls. be.
図4A～図4Dのグラフィカルユーザインターフェース480は、仮想キーボードまたはその他のタッチおよび/もしくはタイピングされた入力によってユーザ入力を生成するためにユーザが選択してよいテキスト応答インターフェース要素484と、クライアントデバイス410のマイクロフォンによってユーザ入力を生成するためにユーザが選択してよい音声応答インターフェース要素485とをさらに含む。一部の実装において、ユーザは、音声応答インターフェース要素485を選択することなく、マイクロフォンによってユーザ入力を生成してよい。たとえば、マイクロフォンによる聴覚的ユーザ入力の能動的な監視が、ユーザが音声応答インターフェース要素485を選択する必要をなくすために行われてよい。それらの一部において、および/またはその他の実装において、音声応答インターフェース要素485は、省略されてよい。さらに、一部の実装において、テキスト応答インターフェース要素484は、追加的および/または代替的に、省略されてよい(たとえば、ユーザは、聴覚的ユーザ入力のみを提供してよい)。図4A～図4Dのグラフィカルユーザインターフェース480は、コンピューティングデバイス410に1つまたは複数のアクションを実行させるためにユーザによってインタラクションされてよいシステムインターフェース要素481、482、483も含む。 The graphical user interface 480 of FIGS. 4A-4D includes a text responsive interface element 484 that a user may select to generate user input via a virtual keyboard or other touch and/or typed input, and a text responsive interface element 484 of the client device 410. and a voice response interface element 485 that the user may select to generate user input via a microphone. In some implementations, a user may generate user input through a microphone without selecting voice response interface element 485. For example, active monitoring of audible user input via a microphone may be performed to eliminate the need for the user to select audio responsive interface element 485. In some of these and/or other implementations, voice response interface element 485 may be omitted. Furthermore, in some implementations, text response interface element 484 may additionally and/or alternatively be omitted (eg, the user may provide only auditory user input). Graphical user interface 480 of FIGS. 4A-4D also includes system interface elements 481, 482, 483 that may be interacted with by a user to cause computing device 410 to perform one or more actions.
本明細書において説明される様々な実装において、自動アシスタントを使用してエンティティとの通話(たとえば、支援された通話)を開始するためのユーザ入力が、受け取られ得る。ユーザ入力は、支援された通話を開始するインジケーションを含む発話された入力、タッチ入力、および/またはタイピングされた入力であることが可能である。さらに、自動アシスタントは、エンティティに関連して、クライアントデバイス410の所与のユーザの代理としてタスクを実行ことができる。図4Aに示されるように、ユーザインターフェース480は、(たとえば、「www.exampleurl0.com/」というURL 411によって示されるように)クライアントデバイス410においてアクセス可能なブラウザアプリケーションからのレストランのエンティティの検索結果を含む。さらに、検索結果は、「Hypothetical Cafe」の第1の検索結果420と、「Example Cafe」の第2の検索結果430とを含む。 In various implementations described herein, user input may be received to initiate a call (eg, an assisted call) with an entity using an automated assistant. The user input can be spoken, touch, and/or typed input, including an indication to initiate an assisted call. Further, the automated assistant can perform tasks on behalf of a given user of client device 410 in association with an entity. As shown in FIG. 4A, user interface 480 displays search results for restaurant entities from a browser application accessible at client device 410 (e.g., as indicated by URL 411 "www.exampleurl0.com/"). including. Furthermore, the search results include a first search result 420 for "Hypothetical Cafe" and a second search result 430 for "Example Cafe."
一部の実装において、検索結果420および/または430は、選択されるとき、クライアントデバイス410に対応するアクションを実行させる様々な選択可能なグラフィカル要素に関連付けられ得る。たとえば、検索結果420および/または430の所与の1つに関連する通話グラフィカル要素421および/または431が選択されるとき、ユーザ入力は、検索結果420および/または430に関連するレストランのエンティティへの電話アクションが実行されるべきであることを示し得る。別の例として、検索結果420および/または430の所与の1つに関連する道順グラフィカル要素422および/または432が選択されるとき、ユーザ入力は、検索結果420および/または430に関連するレストランのエンティティへのナビゲーションアクションが実行されるべきであることを示し得る。さらに別の例として、検索結果420および/または430の所与の1つに関連するメニューグラフィカル要素423および/または433が選択されるとき、ユーザ入力は、検索結果420および/または430に関連するレストランのエンティティのメニューを表示するブラウザベースのアクションが実行されるべきであることを示し得る。図4Aにおいては、支援された通話がブラウザアプリケーションから開始されるが、それは例示のためであり、限定的であるように意図されていないことを理解されたい。たとえば、支援された通話は、クライアントデバイス410においてアクセス可能な様々なソフトウェアアプリケーション(たとえば、連絡先アプリケーション、電子メールアプリケーション、テキストもしくはSMSメッセージングアプリケーション、および/またはその他のソフトウェアアプリケーション)から開始されることが可能であり、支援された通話が発話された入力を使用して開始される場合は、クライアントデバイス410のホーム画面から、クライアントデバイス410のロック画面から、および/またはクライアントデバイス410のその他の状態から開始されることが可能である。 In some implementations, search results 420 and/or 430 may be associated with various selectable graphical elements that, when selected, cause client device 410 to perform a corresponding action. For example, when call graphical elements 421 and/or 431 associated with a given one of search results 420 and/or 430 are selected, the user input is directed to the restaurant entity associated with search results 420 and/or 430. may indicate that a telephone action is to be performed. As another example, when directions graphical elements 422 and/or 432 associated with a given one of search results 420 and/or 430 are selected, the user input may include a restaurant associated with search results 420 and/or 430. may indicate that a navigation action to an entity is to be performed. As yet another example, when a menu graphical element 423 and/or 433 associated with a given one of search results 420 and/or 430 is selected, the user input It may indicate that a browser-based action to display a menu of a restaurant entity is to be performed. Although in FIG. 4A, the assisted call is initiated from a browser application, it should be understood that this is for illustrative purposes only and is not intended to be limiting. For example, assisted calls may be initiated from various software applications accessible at client device 410 (e.g., contacts application, email application, text or SMS messaging application, and/or other software applications). from the home screen of the client device 410, from the lock screen of the client device 410, and/or from other states of the client device 410 if possible and the assisted call is initiated using spoken input. It is possible to start.
例示のために、「Example Cafe」の第2の検索結果430を用いて通話を開始するためのユーザ入力が、クライアントデバイス410において検出されると仮定する。ユーザ入力は、たとえば、「Example Cafeに電話して」という発話された入力、または通話グラフィカル要素431を対象とするタッチ入力であることが可能である。一部の実装においては、「Example Cafe」との通話を開始するためのユーザ入力の受け取り応じて、クライアントデバイス410において、通話詳細インターフェース470がレンダリングされ得る。それらの実装のいくつかのバージョンにおいて、通話詳細インターフェース470は、ユーザインターフェース480の一部としてクライアントデバイス410においてレンダリングされ得る。それらの実装のいくつかのその他のバージョンにおいて、通話詳細インターフェース470は、ユーザインターフェースに覆い被さるユーザインターフェース480とは別個のインターフェースであることが可能であり、ユーザが(たとえば、通話詳細インターフェース要素486上で上にスワイプすることによって)さらなる通話の詳細を表示するために通話詳細インターフェース470を展開すること、および/または(たとえば、通話詳細インターフェース要素486上で下にスワイプすることによって)通話詳細インターフェース470を終わらせることを可能にする通話詳細インターフェース要素486を含み得る。通話詳細インターフェース470は、ユーザインターフェース480の下部にあるものとして描かれているが、それは例示のためであり、限定的であるように意図されていないことを理解されたい。たとえば、通話詳細インターフェース470は、ユーザインターフェース480の上部、ユーザインターフェース480の側部、またはユーザインターフェース480とは完全に別個のインターフェースにレンダリングされ得る。 For purposes of illustration, assume that user input to initiate a call using a second search result 430 for "Example Cafe" is detected at client device 410. The user input can be, for example, a spoken input of "Call Example Cafe" or a touch input directed to the call graphical element 431. In some implementations, a call details interface 470 may be rendered at the client device 410 in response to receiving user input to initiate a call with "Example Cafe." In some versions of those implementations, call details interface 470 may be rendered at client device 410 as part of user interface 480. In some other versions of those implementations, the call details interface 470 may be a separate interface from the user interface 480 that overlays the user interface (e.g., on the call details interface element 486). Expanding the call details interface 470 to view further call details (e.g., by swiping up on the call details interface element 486); may include a call details interface element 486 that allows the call to be terminated. Although call details interface 470 is depicted as being at the bottom of user interface 480, it should be understood that this is illustrative and not intended to be limiting. For example, call details interface 470 may be rendered on top of user interface 480, on the side of user interface 480, or in an entirely separate interface from user interface 480.
通話詳細インターフェース470は、様々な実装において、複数のグラフィカル要素を含み得る。一部の実装において、グラフィカル要素は、グラフィカル要素の所与の1つが選択されるとき、クライアントデバイス410が対応するアクションを実行することができるように、選択可能であり得る。図4Aに示されるように、通話詳細インターフェース470は、「支援された通話」の第1のグラフィカル要素471、「通常通話」の第2のグラフィカル要素472、および「連絡先『Example Cafe』を保存」の第3のグラフィカル要素473を含む。さらに、第1のグラフィカル要素471は、選択されるとき、自動アシスタントを使用して支援された通話を開始したいという希望のインジケーションを自動アシスタントに与えることができ、第2のグラフィカル要素472は、選択されるとき、自動アシスタントに、支援された通話を使用せずに通話を開始させることができ、第3のグラフィカル要素473は、選択されるとき、自動アシスタントに、Example Cafeに関連する連絡先を作成させることができる。特に、それらの実装のいくつかのバージョンにおいて、グラフィカル要素は、実行されるタスクのインジケーションを提供するための下位要素を含み得る。たとえば、「支援された通話」の第1のグラフィック要素471は、Example Cafeのレストラン予約を行うタスクに関連する「予約する」の第1の下位要素471A、Example Cafeのレストラン予約を修正するタスクに関連する「予約を修正する」の第2の下位要素471B、およびExample Cafeのレストラン予約をキャンセルするタスクに関連する「予約をキャンセルする」の第3の下位要素471Cを含み得る。 Call details interface 470 may include multiple graphical elements in various implementations. In some implementations, graphical elements may be selectable such that when a given one of the graphical elements is selected, client device 410 can perform a corresponding action. As shown in FIG. 4A, the call details interface 470 includes a first graphical element 471 for "Assisted Call", a second graphical element 472 for "Normal Call", and a "Save Contact 'Example Cafe'". ” includes a third graphical element 473. Additionally, the first graphical element 471, when selected, can provide an indication to the automated assistant of a desire to initiate an assisted call using the automated assistant, and the second graphical element 472 When selected, the automated assistant can initiate a call without using assisted calling, and the third graphical element 473, when selected, allows the automated assistant to initiate a call without using assisted calling. can be created. In particular, in some versions of their implementation, graphical elements may include subelements to provide an indication of the task to be performed. For example, the first graphic element 471 of "Assisted Call" is related to the task of making a restaurant reservation for Example Cafe, the first subelement 471A of "Make a Reservation" is related to the task of modifying a restaurant reservation for Example Cafe. It may include an associated second sub-element 471B of "Modify Reservation" and a third sub-element 471C of "Cancel Reservation" associated with the task of canceling the Example Cafe restaurant reservation.
例示のために、Example Cafeのレストラン予約を行うためにExample Cafeとの支援された通話を開始するためのユーザ入力が、クライアントデバイス410において検出されると仮定する。ユーザ入力は、たとえば、「Example Cafeに電話してレストラン予約をして」という発話された入力、または第1の下位要素471Aを対象とするタッチ入力であることが可能である。ユーザ入力の検出に応じて、自動アシスタントは、「Example Cafeのレストラン予約をする」というタスクを決定することができ、(たとえば、図1のパラメータエンジン153に関連して)本明細書において説明されたように、特定されたタスクに関連する候補パラメータを特定することができる。一部の実装においては、図4Bに示されるように、自動アシスタントは、候補パラメータの値を決定することができる。特に、図4Aから図4Bに進む際、通話詳細インターフェース470は、特定された候補パラメータを含むように更新され得る。それらの実装のいくつかのバージョンにおいて、自動アシスタントは、クライアントデバイス410の所与のユーザに関連するユーザプロファイルに基づいて、候補パラメータの値を決定することができる。たとえば、自動アシスタントは、自動アシスタントがクライアントデバイス410の所与のユーザのユーザプロファイルを介して値474Aおよび475Aにアクセスすることができることに基づいて、所与のユーザに値474Aおよび475Aを求める必要なしに、名前パラメータ474の値474A(たとえば、Jane Doe)、および電話番号パラメータ475の値475A(たとえば、(502)123-4567)を決定することができる。図4Bは、自動アシスタントによって特定された特定の候補パラメータに関連して本明細書において説明されるが、それは例示のためであることを理解されたい。 For purposes of illustration, assume that user input is detected at client device 410 to initiate an assisted call with Example Cafe to make a restaurant reservation at Example Cafe. The user input can be, for example, a spoken input, "Call Example Cafe to make a restaurant reservation," or a touch input directed to the first sub-element 471A. In response to detecting the user input, the automated assistant may determine the task of "making a restaurant reservation for Example Cafe," as described herein (e.g., with respect to parameter engine 153 of FIG. 1). As described above, candidate parameters related to the identified task can be identified. In some implementations, the automated assistant can determine values for candidate parameters, as shown in FIG. 4B. In particular, upon proceeding from FIG. 4A to FIG. 4B, the call details interface 470 may be updated to include the identified candidate parameters. In some versions of their implementation, the automated assistant may determine values for candidate parameters based on a user profile associated with a given user of client device 410. For example, the automated assistant does not need to ask a given user for the values 474A and 475A based on the automated assistant having access to the values 474A and 475A through the user profile of the given user on the client device 410. , a value 474A for name parameter 474 (eg, Jane Doe) and a value 475A for telephone number parameter 475 (eg, (502)123-4567) may be determined. Although FIG. 4B is described herein in connection with specific candidate parameters identified by the automated assistant, it should be understood that this is for illustrative purposes.
それらの実装のいくつかのバージョンにおいて、自動アシスタントは、クライアントデバイス410の所与のユーザのユーザプロファイルに基づいて特定されない候補パラメータの対応する値を求めるために、クライアントデバイス410の所与のユーザと(たとえば、聴覚的および/または視覚的に)対話することができる。それらの実装のいくつかのさらなるバージョンにおいて、自動アシスタントは、(たとえば、図1のパラメータエンジン153に関連して)本明細書において説明されたように必須のパラメータとみなされる候補パラメータの値のみを求めてよい。たとえば、レストラン予約を行うタスクに関して、自動アシスタントは、"何日の何時にExample Cafeの予約をしたいですか"というプロンプト452B1を生成し、「3月1日の午後7時にボックス席」という(たとえば、タイピングされたかまたは発話された)ユーザ入力454B1を受け取ることができる。したがって、通話詳細インターフェース470は、日付/時間パラメータ476の値476A(たとえば、2020年3月1日午後7時)を含むように更新され得る。特に、ユーザ入力454B1は、プロンプト452B1において求められなかった座席の種類パラメータ478の「ボックス席」という値478Aも含む。たとえ自動アシスタントがプロンプト452B1において値478Aを要求しなかったとしても、自動アシスタントは、値478Aが、(たとえば、図1のパラメータエンジン153に関連して)本明細書において説明されたように任意のパラメータとみなされ得る座席の種類パラメータ478に対応すると判定することができる。さらに、自動アシスタントは、さらなるパラメータ(たとえば、人数パラメータ477)に関するさらなるプロンプト(たとえば、プロンプト452B2)を生成して、さらなるパラメータの対応する値(たとえば、値5を有する477A)を決定することができる。このようにして、自動アシスタントは、支援された通話を開始する前に、クライアントデバイス410の所与のユーザの代理としてタスクを実行する際に使用される候補パラメータの値を決定することができる。 In some versions of those implementations, the automated assistant communicates with a given user of client device 410 to determine corresponding values for candidate parameters that are not specified based on the user profile of the given user of client device 410. be able to interact (e.g., audibly and/or visually); In some further versions of those implementations, the automated assistant only selects values for candidate parameters that are considered mandatory parameters as described herein (e.g., in connection with parameter engine 153 of FIG. 1). You can ask for it. For example, for the task of making a restaurant reservation, the automated assistant would generate a prompt 452B1 that says, "What day and time would you like to make a reservation for Example Cafe?" , typed or spoken) user input 454B1 can be received. Accordingly, call details interface 470 may be updated to include a value 476A for date/time parameter 476 (eg, March 1, 2020 at 7:00 PM). In particular, user input 454B1 also includes a value 478A of "box seat" for seat type parameter 478 that was not requested in prompt 452B1. Even if the automated assistant did not request the value 478A at prompt 452B1, the automated assistant may assume that the value 478A is any It can be determined that it corresponds to the seat type parameter 478, which can be considered as a parameter. Additionally, the automated assistant may generate further prompts (e.g., prompt 452B2) for further parameters (e.g., number of people parameter 477) to determine the corresponding value of the further parameter (e.g., 477A with value 5). . In this manner, the automated assistant can determine values for candidate parameters to be used in performing tasks on behalf of a given user of client device 410 before initiating an assisted call.
さらに、様々な実装において、候補パラメータの値を決定するとき、図4Bに示されるように、通話詳細インターフェース470は、様々なグラフィカル要素を含み得る。たとえば、通話詳細インターフェース470は、選択されるとき、支援された通話を開始する前にユーザが値474A～478Aを修正することを可能にする編集グラフィカル要素441Bと、選択されるとき、支援された通話を終了し、任意で、ユーザインターフェース480を、支援された通話を開始するためのユーザ入力を検出する前の状態(たとえば、図4Aのユーザインターフェース480)に戻すキャンセルグラフィカル要素442Bと、選択されるとき、(たとえば、図4Aのグラフィカル要素472の選択と同様に)ユーザが自動アシスタントの助けなしに通常通話を開始することを可能にする通話インターフェース要素443Bとを含み得る。 Additionally, in various implementations, when determining values for candidate parameters, call details interface 470 may include various graphical elements, as shown in FIG. 4B. For example, the call details interface 470 includes an edit graphical element 441B that, when selected, allows the user to modify values 474A-478A before initiating an assisted call; cancel graphical element 442B that ends the call and optionally returns user interface 480 to the state it was in prior to detecting the user input to initiate the assisted call (e.g., user interface 480 of FIG. 4A); call interface element 443B that allows the user to initiate a regular call without the assistance of an automated assistant (eg, similar to selection of graphical element 472 of FIG. 4A).
候補パラメータ474～478の対応する値474A～478Aを決定した後、自動アシスタントは、エンティティに関連して、クライアントデバイス410の所与のユーザの代理としてタスクを実行するために支援された通話を開始することができる。自動アシスタントは、クライアントデバイス410においてアクセス可能な通話アプリケーションを使用して支援された通話を開始することができる。様々な実装において、図4Cに示されるように、通話詳細インターフェース470は、自動アシスタントによる通話の開始に応じて、様々なグラフィカル要素を含むように更新され得る。たとえば、通話詳細インターフェース470は、選択されるとき、支援された通話を開始した後に自動アシスタントに支援された通話を終了させる通話終了グラフィカル要素441Cと、選択されるとき、クライアントデバイス410の所与のユーザが自動アシスタントから通話および/またはタスクの実行を引き継ぐことを可能にする通話参加グラフィカル要素442Cと、選択されるとき、クライアントデバイス410にさらなるユーザと自動アシスタントとの間の対話を聴覚的にレンダリングさせるスピーカインターフェース要素443Cとを含み得る。これらのグラフィカル要素442C、443C、および444Cは、支援された通話の継続時間全体を通じて選択され得る。 After determining the corresponding values 474A-478A of the candidate parameters 474-478, the automated assistant initiates an assisted call to perform a task on behalf of the given user of the client device 410 in association with the entity. can do. The automated assistant can initiate assisted calls using a calling application accessible at client device 410. In various implementations, as shown in FIG. 4C, the call details interface 470 may be updated to include various graphical elements in response to the initiation of a call by the automated assistant. For example, the call details interface 470 includes a call termination graphical element 441C that, when selected, causes the automated assistant to terminate the assisted call after initiating the assisted call; call participation graphical element 442C that allows the user to take over calls and/or task performance from the automated assistant; and, when selected, audibly renders further interaction between the user and the automated assistant to the client device 410; and a speaker interface element 443C. These graphical elements 442C, 443C, and 444C may be selected throughout the duration of the assisted call.
さらに、様々な実装において、自動アシスタントは、支援された通話を開始した後およびタスクを実行する前に、クライアントデバイスのさらなるユーザから同意を得ることができる。図4Cに示されるように、自動アシスタントは、自動アシスタントとインタラクションするための同意をExample Cafeの担当者が与えることを要求する合成音声452C1を、Example Cafeの担当者に関連するさらなるクライアントデバイスにおいてレンダリングさせることができる。自動アシスタントは、Example Cafeの担当者からのさらなる発話された入力に対応するオーディオデータ456C1を処理して、Example Cafeの担当者によって同意が与えられた(たとえば、オーディオデータ456C1中の「はい」)と判定することができる。さらに、自動アシスタントは、オーディオデータ456C1が日付/時間パラメータ476の値476Aの情報を要求するとの判定に応じて、日付/時間パラメータ476の値476Aを含むさらなる合成音声456C2をExample Cafeの担当者のさらなるクライアントデバイスにおいてレンダリングさせることができる。このようにして、自動アシスタントは、Example Cafeの担当者からの情報の要求に応じた値を含む合成音声を提供することによって、Example Cafeのレストラン予約を行うタスクを実行することができる。 Further, in various implementations, the automated assistant may obtain consent from further users of the client device after initiating an assisted call and before performing a task. As shown in Figure 4C, the automated assistant renders a synthesized voice 452C1 at a further client device associated with the Example Cafe representative requesting that the Example Cafe representative provide consent to interact with the automated assistant. can be done. The automated assistant processes audio data 456C1 corresponding to further spoken input from the Example Cafe representative such that consent has been given by the Example Cafe representative (e.g., "yes" in audio data 456C1) It can be determined that Additionally, the automated assistant sends a further synthesized voice 456C2 containing the value 476A of the date/time parameter 476 to the Example Cafe representative in response to determining that the audio data 456C1 requests information of the value 476A of the date/time parameter 476. Rendering may occur on additional client devices. In this way, the automated assistant can perform the task of making restaurant reservations for Example Cafe by providing a synthesized voice that includes values in response to requests for information from representatives of Example Cafe.
一部の実装において、自動アシスタントは、Example Cafeの担当者からのさらなる発話された入力に対応するオーディオデータを処理することができ、さらなる発話された入力が自動アシスタントに現在知られていないパラメータに関連するさらなる値の情報を要求していると判定することができる。図4Cに示されるように、オーディオデータ456C2は、自動アシスタントに現在知られていない子供パラメータの情報を要求するさらなる発話された入力をキャプチャする。Example Cafeの担当者からのさらなる発話された入力が自動アシスタントに現在知られていない情報を要求しているとの判定に応じて、自動アシスタントは、要求されている情報が現在知られていないことを示すさらに別の合成音声をさらなるクライアントデバイスにおいてレンダリングさせることができる。自動アシスタントは、要求されている情報を現在知らない場合があるが、自動アシスタントが現在知っているその他の値を用いてタスクを実行し続けることができる。たとえば、自動アシスタントは、オーディオデータ456C2が自動アシスタントに現在知られていないパラメータに関連するさらなる値の情報を要求するとの判定に応じて、「分かりません。Jane Doeに聞いてくる必要があります。答えをもらうまで予約を続けてよろしいですか」というさらに別の合成音声454C3を生じ得る。 In some implementations, the automated assistant may process audio data that corresponds to further spoken input from the Example Cafe representative, and the automated assistant may process audio data that corresponds to further spoken input from the Example Cafe personnel, such that the further spoken input corresponds to parameters currently unknown to the automated assistant. It may be determined that further related value information is requested. As shown in FIG. 4C, audio data 456C2 captures additional spoken input requesting information of child parameters not currently known to the automated assistant. In response to determining that further spoken input from the Example Cafe representative requests information that is not currently known to the automated assistant, the automated assistant may request information that is not currently known to the automated assistant. A further synthesized voice may be rendered at a further client device. The automated assistant may not currently know the requested information, but may continue to perform the task using other values that the automated assistant currently knows. For example, in response to determining that audio data 456C2 requests further value information related to a parameter that is not currently known to the automated assistant, the automated assistant may respond with, ``I don't know. You should ask Jane Doe. "Are you sure you want to continue with your reservation until you receive an answer?" may occur, resulting in yet another synthesized voice 454C3.
それらの実装のいくつかのバージョンにおいては、自動アシスタントが、さらなるユーザのオーディオデータが情報の要求を含むと判定するとき、自動アシスタントは、自動アシスタントが現在知らないパラメータに関連する情報をさらなるユーザが要求していることを示す通知をクライアントデバイス410においてレンダリングさせることができる。それらの実装のいくつかのさらなるバージョンにおいて、通知は、情報の要求に応じる推薦として提案される値をさらに含み得る。推薦の所与の1つを選択すると、自動アシスタントが推薦の所与の1つに含まれる値をさらなるユーザの要求に応じた値として利用することができるように、推薦は、選択可能であり得る。たとえば、図4Dに示されるように、自動アシスタントは、通知479を通話詳細インターフェース470において視覚的にレンダリングさせることができる。通知479は、「Example Cafeは、予約に子供が含まれているかどうかを知りたい」というインジケーションを含み、さらなるユーザの要求に応じた推薦される値として提供される「はい」の第1の提案479Aおよび「いいえ」の第2の提案479Bも含む。以下でより詳細に説明されるように、様々なその他の種類の通知が、クライアントデバイス410においてレンダリングされることが可能であり、通知の種類は、オーディオデータが自動アシスタントが現在知らない情報の要求を含むとシステムが判定するときのクライアントデバイス410の状態に基づき得る。 In some versions of those implementations, when the automated assistant determines that the additional user's audio data includes a request for information, the automated assistant requests that the additional user provide information related to parameters that the automated assistant currently does not know. A notification may be rendered at client device 410 indicating the request. In some further versions of those implementations, the notification may further include a suggested value as a recommendation in response to a request for information. The recommendations are selectable such that selecting a given one of the recommendations allows the automated assistant to utilize the value contained in the given one of the recommendations as a value in response to further user requests. obtain. For example, as shown in FIG. 4D, the automated assistant may cause a notification 479 to be visually rendered in the call details interface 470. Notification 479 includes an indication that "Example Cafe would like to know whether the reservation includes children," with the first of "Yes" provided as the recommended value in response to further user requests. Also includes Proposition 479A and a second Proposition 479B of "No". Various other types of notifications may be rendered at the client device 410, as described in more detail below, and notification types may include audio data requesting information that the automated assistant does not currently know. may be based on the state of the client device 410 at the time the system determines that the client device 410 includes a .
上述のように、自動アシスタントは、たとえさらなるユーザが自動アシスタントが現在知らない情報を要求していると自動アシスタントが判定する場合でも、自動アシスタントが現在知っているその他の値を用いてタスクを実行し続けることができる。さらなる値は、通知479に応じてクライアントデバイス410の所与のユーザからさらなるユーザ入力を受け取った後、対話の中で後で提供され得る。たとえば、図4Dに示されるように、子供パラメータの値が自動アシスタントに現在知られていないが、自動アシスタントがその他の値を知っており、それらのその他の値を使用してタスクを実行し続けたいことを示すさらに別の合成音声452C3のレンダリングに応じて。さらに、「かまいません。お座席の種類はどうしますか」というオーディオデータ456D1が受け取られると仮定する(たとえば、座席の種類パラメータ478の「ボックス席」という値478Aの情報の要求)。さらに、オーディオデータ456D1が自動アシスタントによって処理されている間に、クライアントデバイス410の所与のユーザが、第2の提案479Bを対象とする発話された入力および/またはタッチ入力を与えて、Example Cafeのレストラン予約に子供が参加しないことを示すと仮定する。この例において、自動アシスタントは、座席の種類パラメータ478の「ボックス席」という値478Aを含み、値(たとえば、第2の提案479Bのユーザの選択に基づいて「いいえ」)を含む合成音声452D1を、さらなるユーザのさらなるクライアントデバイスにおいて聴覚的にレンダリングさせることができる。自動アシスタントは、「完璧です。予約は完了しました」というさらなるオーディオデータ456D2を処理して、レストラン予約のタスクが完了されたと判定することができ、「Jane Doeに知らせます。良い一日を」というさらなる合成音声452D2を生じることができ、通話を終了することができる。 As mentioned above, the automated assistant may perform the task using other values that the automated assistant currently knows, even if the automated assistant determines that a further user requests information that the automated assistant currently does not know. can continue to do so. Further values may be provided later in the interaction after receiving further user input from a given user of client device 410 in response to notification 479. For example, as shown in Figure 4D, the value of the child parameter is currently unknown to the automated assistant, but the automated assistant knows the other values and continues to perform the task using those other values. According to the rendering of yet another synthesized voice 452C3 that shows what you want. Furthermore, it is assumed that audio data 456D1 saying "I don't mind. What kind of seat do you want?" is received (for example, a request for information on the value 478A of "box seat" in the seat type parameter 478). Additionally, while audio data 456D1 is being processed by the automated assistant, a given user of client device 410 may provide spoken and/or touch input directed to second suggestion 479B to Assume that you indicate that your child will not be participating in your restaurant reservation. In this example, the automated assistant includes the value 478A of "box seat" for the seat type parameter 478 and generates a synthesized voice 452D1 that includes a value (e.g., "No" based on the user's selection of the second suggestion 479B). , may be rendered aurally on a further client device of a further user. The automated assistant can determine that the restaurant reservation task is completed by processing further audio data 456D2 that says "Perfect, your reservation is complete" and "Notify Jane Doe. Have a nice day." A further synthesized voice 452D2 can be generated, and the call can be ended.
特に、一部の実装において、自動アシスタントは、さらなるユーザからの、その他の情報の要求に応じる合成音声に、さらなる値(すなわち、さらなるユーザによって要求されたときにはまだ知られていなかったが、さらなるユーザインターフェース入力に基づいて今は知られている)を含めることができる。言い換えれば、自動アシスタントは、たとえさらなるユーザからの直前の要求が情報の要求でない場合でも、合成音声にさらなる値を含めることができる。たとえば、図4Dに示されるように、合成音声452D1は、オーディオデータ456D1に含まれる座席の種類パラメータ478に関連する情報の直前の要求に応じる「ボックス席」の値478Aを含み、合成音声は、オーディオデータ456C2に含まれる子供パラメータに関連する情報の前の要求に応じる子供パラメータの「いいえ」の値も含む。このようにして、自動アシスタントは、さらなる値を含むさらなるユーザ入力を待つ間、タスクの実行を継続することができ、さらなる値が自動アシスタントに知られるようになるとき、論理的におよび会話形式で、さらなる値をさらなるユーザに提供することができる。さらなるユーザ入力を待つ間、タスクの実行を継続することによって、タスクは、さらなるユーザ入力が受け取られるまでタスクの実行が一時停止されないので、より迅速でより効率的に実行され得る。より迅速でより効率的にタスクを実行することによって、本明細書において説明される技術は、支援された通話を使用してタスクを実行する際の計算リソースおよびネットワークリソースを節約することができる。 In particular, in some implementations, the automated assistant may include an additional value (i.e., an additional value (i.e., not yet known at the time of the request by the additional user, (now known based on interface input). In other words, the automated assistant can include additional values in the synthesized speech even if the previous request from the additional user was not a request for information. For example, as shown in FIG. 4D, synthesized speech 452D1 includes a "box seat" value 478A in response to a previous request for information related to seat type parameter 478 included in audio data 456D1; It also includes a "no" value for the child parameter in response to a previous request for information related to the child parameter included in the audio data 456C2. In this way, the automated assistant can continue performing the task while waiting for further user input containing further values, and when further values become known to the automated assistant, it can logically and conversationally , further values can be provided to additional users. By continuing to execute a task while waiting for further user input, the task may be executed more quickly and more efficiently because execution of the task is not paused until further user input is received. By performing tasks faster and more efficiently, the techniques described herein can save computational and network resources when performing tasks using assisted calling.
様々な実装において、描かれていないが、自動アシスタントは、支援された通話中に、エンティティに関連するさらなるユーザからのさらなるオーディオデータが閾値の継続時間内に受信されないと判定する場合がある。それらの実装のいくつかのバージョンにおいて、自動アシスタントは、さらなるユーザからの情報の要求に含まれていなかった1つまたは複数の対応する値に基づくさらに別の合成音声をレンダリングし得る。たとえば、自動アシスタントが、さらなるユーザが10秒間何も言わなかったと判定し、自動アシスタントが、さらなるユーザによって要求されなかったレストラン予約のタスクの人数パラメータの値を知っている場合、アシスタントは、「念のためお伝えすると、5名が予約に参加します」という合成音声をレンダリングし得る。それらの実装のいくつかの追加的および/または代替的なバージョンにおいて、自動アシスタントは、さらなるユーザからの情報の要求に含まれていなかった1つまたは複数のパラメータに基づくさらに別の合成音声をレンダリングすることができる。たとえば、自動アシスタントが、さらなるユーザが10秒間何も言わなかったと判定し、自動アシスタントが、さらなるユーザによって要求されなかったレストラン予約のタスクのための人数パラメータの値を知っている場合、アシスタントは、「予約に参加する人数を知りたいですか」という合成音声をレンダリングすることができる。 In various implementations, although not depicted, the automated assistant may determine that no additional audio data from additional users associated with the entity is received within a threshold duration during the assisted call. In some versions of their implementation, the automated assistant may render yet another synthesized voice based on one or more corresponding values that were not included in the request for information from the further user. For example, if the automated assistant determines that the additional user did not say anything for 10 seconds, and the automated assistant knows the value of the number of people parameter for a restaurant reservation task that was not requested by the additional user, the assistant may "We would like to inform you that 5 people will be taking part in your reservation," a synthesized voice could be rendered. In some additional and/or alternative versions of their implementation, the automated assistant renders yet another synthesized voice based on one or more parameters that were not included in the request for further information from the user. can do. For example, if the automated assistant determines that the additional user did not say anything for 10 seconds, and the automated assistant knows the value of the number of people parameter for a restaurant reservation task that was not requested by the additional user, the assistant: A synthesized voice can be rendered that says, "Do you want to know how many people will be joining your reservation?"
さらに、様々な実装において、自動アシスタントは、様々な対話の文字起こしを、クライアントデバイス410のユーザインターフェース480において視覚的にレンダリングさせることができる。文字起こしは、たとえば、クライアントデバイス410のホームに、様々なソフトウェアアプリケーション(たとえば、アシスタントアプリケーション、通話アプリケーション、および/またはその他のアプリケーション)において表示され得る。一部の実装において、文字起こしは、(たとえば、図4Bに示されるように)自動アシスタントとクライアントデバイス410の所与のユーザとの間の対話を含み得る。いくつかの追加的および/または代替的な実装において、文字起こしは、(たとえば、図4Cおよび図4Dに示されるように)自動アシスタントとさらなるユーザとの間の対話を含み得る。 Additionally, in various implementations, the automated assistant may cause transcriptions of various interactions to be rendered visually at the user interface 480 of the client device 410. The transcription may be displayed in various software applications (eg, an assistant application, a calling application, and/or other applications), for example, on the home of the client device 410. In some implementations, transcription may include an interaction between an automated assistant and a given user of client device 410 (eg, as shown in FIG. 4B). In some additional and/or alternative implementations, transcription may include interaction between an automated assistant and a further user (eg, as shown in FIGS. 4C and 4D).
図4B～図4Dの各々は、対話の文字起こしを含むものとして示されているが、それは例示のためであり、限定的であるように意図されていないことに留意されたい。上述の支援された通話は、クライアントデバイス410がスリープ状態、ロック状態、その他のソフトウェアアプリケーションがフォアグラウンドで動作しているとき、および/またはその他の状態である間に実行され得ることを理解されたい。さらに、実装において、自動アシスタントがクライアントデバイス410において通知をレンダリングさせる場合、クライアントデバイスにおいてレンダリングされる通知の種類は、本明細書において説明されるように、クライアントデバイス410の状態に基づく。さらに、図4A～図4Dは、レストラン予約を行うタスクに関連して本明細書において説明されているが、それはやはり限定的であるように意図されておらず、本明細書において説明される技術は、複数の異なるエンティティに関連して実行され得る複数の異なるタスクのために利用されることが可能であることを理解されたい。 Note that while each of FIGS. 4B-4D is shown as including a transcription of dialogue, it is illustrative and not intended to be limiting. It should be appreciated that the assisted calling described above may be performed while the client device 410 is asleep, locked, other software applications running in the foreground, and/or other states. Further, in implementations, when the automated assistant causes a notification to be rendered at the client device 410, the type of notification rendered at the client device is based on the state of the client device 410, as described herein. Additionally, although FIGS. 4A-4D are described herein in connection with the task of making a restaurant reservation, that is again not intended to be limiting and the techniques described herein are not intended to be limiting. It should be understood that the can be utilized for multiple different tasks that may be performed in connection with multiple different entities.
さらに、様々な実装において、自動アシスタントは、支援された通話の始めにおよび/または支援された通話中に、さらなるユーザによって保留にされる場合がある。それらの実装のいくつかのバージョンにおいて、自動アシスタントは、自動アシスタントがさらなる人間参加者との会話に従事していないとき、保留中とみなされ得る。たとえば、自動アシスタントが、エンティティに関連する保留システム、エンティティに関連する音声自動応答(IVR: interactive voice response)システム、および/またはエンティティに関連するその他のシステムの対応をしている場合、自動アシスタントは、保留中とみなされ得る。さらに、自動アシスタントは、支援された通話が保留にされるときおよび/または支援された通話が保留にされた後で再開されるときに、クライアントデバイス410において通知をレンダリングさせることができる。通知は、たとえば、支援された通話が保留にされたこと、支援された通話が保留にされた後で再開したこと、保留にされた後で再開されるときにユーザが支援された通話に加わるように要求されること、および/または支援された通話に関連するその他の情報を示すことができる。さらに、自動アシスタントは、さらなるユーザのさらなるクライアントデバイスから所与のユーザのクライアントデバイス410に送信されたオーディオデータのストリームを処理することに基づいて、支援された通話が保留にされた後で再開したと判定することができる。 Additionally, in various implementations, the automated assistant may be placed on hold by additional users at the beginning and/or during the assisted call. In some versions of those implementations, the automated assistant may be considered on hold when the automated assistant is not engaged in a conversation with an additional human participant. For example, if the automated assistant interacts with a hold system associated with the entity, an interactive voice response (IVR) system associated with the entity, and/or other systems associated with the entity, the automated assistant , may be considered pending. Additionally, the automated assistant may cause a notification to be rendered at the client device 410 when the assisted call is placed on hold and/or when the assisted call is resumed after being placed on hold. Notifications can be sent, for example, when an assisted call is placed on hold, when an assisted call is placed on hold and then resumed, and when a user joins an assisted call when it is resumed after being placed on hold. and/or other information related to the assisted call. Further, the automated assistant resumes the assisted call after it has been placed on hold based on processing the stream of audio data sent from the further user's further client device to the given user's client device 410. It can be determined that
それらの実装のいくつかのバージョンにおいて、自動アシスタントは、自動アシスタントが知っているパラメータに関連する情報のすべてをさらなるユーザが既に要求したと判定することができる。それらの実装のいくつかのさらなるバージョンにおいて、自動アシスタントは、タスクの残りの部分がクライアントデバイス410の所与のユーザを必要とする場合、支援された通話が保留にされた後で再開されるときに、クライアントデバイス410の所与のユーザが支援された通話に加わることを先回りして要求することができる。たとえば、特定のタスクは、自動アシスタントではなく、クライアントデバイス410の所与のユーザを必要とする場合がある。たとえば、エンティティが銀行であり、さらなるユーザが銀行の担当者であり、タスクがデビットカードの請求に対する異議申し立てであると仮定する。この例において、自動アシスタントは、最初に銀行に電話をかけ、所与のユーザの名前を含む合成音声、ユーザの銀行口座番号を含むシミュレーションされたボタンの押下、電話の理由を提供する合成音声、および/または銀行に関連する電話を転送するための自動化されたシステム(たとえば、IVRシステム)をナビゲートするシミュレーションされたボタンの押下などの合成音声および/またはエミュレーションされたボタンの押下を提供することができる。しかし、自動アシスタントは、支援された通話が銀行の担当者に転送されるときに、所与のユーザが所与のユーザのアイデンティティを確認し、異議を申し立てられたデビットカードの請求を説明するために支援された通話に参加することを求められることを知る場合があり、支援された通話が銀行の担当者に転送されるときにユーザのクライアントデバイス410において通知をレンダリングさせることができる。したがって、自動アシスタントは、支援された通話の最初の部分を処理し、異議を申し立てられた請求について話し合うために銀行の担当者が対応可能であるとき、クライアントデバイス410の所与のユーザが支援された通話を引き継ぐことを要求することができる。 In some versions of their implementation, the automated assistant may determine that the additional user has already requested all of the information related to the parameters that the automated assistant knows. In some further versions of those implementations, the automated assistant will perform a call when the assisted call is resumed after being put on hold if the remainder of the task requires the given user of the client device 410. In addition, a given user of client device 410 may proactively request to join an assisted call. For example, a particular task may require a given user of client device 410 rather than an automated assistant. For example, suppose the entity is a bank, the further user is a bank representative, and the task is to dispute a debit card charge. In this example, the automated assistant first calls the bank, calls a synthetic voice that includes the given user's name, a simulated button press that includes the user's bank account number, and a synthesized voice that provides the reason for the call. and/or providing synthetic speech and/or emulated button presses, such as simulated button presses to navigate automated systems (e.g., IVR systems) for transferring bank-related calls. I can do it. However, the automated assistant helps a given user verify a given user's identity and account for disputed debit card charges when the assisted call is transferred to a bank representative. The user may learn that he or she is required to participate in an assisted call and may have a notification rendered on the user's client device 410 when the assisted call is transferred to a bank representative. Accordingly, the automated assistant handles the first part of the assisted call and determines when a given user of client device 410 is assisted when a bank representative is available to discuss the disputed charge. You can request that the call be taken over.
それらの実装のいくつかのその他のさらなるバージョンにおいて、自動アシスタントは、自動アシスタントがレストラン予約に関連するいかなるさらなる情報も知らないので、支援された通話が保留にされた後で再開されるときに、クライアントデバイス410の所与のユーザが支援された通話に加わることを先回りして要求し得る。たとえば、図4B～図4Dに示された支援された通話のさらなるユーザが、レストラン予約が完了したことを示すのではなく、オーディオデータ456D2において自動アシスタントを保留にしたと仮定する。この例において、自動アシスタントは、レストラン予約に関連して自動アシスタントに知られている情報のすべての値を既に提供しており、支援された通話が再開されるときにさらなるユーザによって要求されたいずれのさらなる情報も、自動アシスタントに現在知られていない情報であることになる。したがって、自動アシスタントは、支援された通話が再開されるときに、クライアントデバイス410の所与のユーザが支援された通話を引き継ぐことを先回りして要求し得る。 In some other further versions of those implementations, the automated assistant does not know any further information related to the restaurant reservation when the assisted call is resumed after being placed on hold. A given user of client device 410 may proactively request to join an assisted call. For example, assume that the further user of the assisted call shown in FIGS. 4B-4D puts the automated assistant on hold in the audio data 456D2, rather than indicating that the restaurant reservation is complete. In this example, the automated assistant has already provided all the values of information known to the automated assistant related to the restaurant reservation, and any further information requested by the user when the assisted call is resumed. The additional information will also be information not currently known to the automated assistant. Thus, the automated assistant may proactively request that a given user of client device 410 take over the assisted call when the assisted call is resumed.
それらの実装のさらにその他のさらなるバージョンにおいて、自動アシスタントは、支援された通話が保留にされたこと、および/または支援された通話が保留にされた後で再開したことを示す通知を差し控え、支援された通話が保留にされた後で再開すると支援された通話を継続し、さらなるユーザが自動アシスタントが現在知らないパラメータに関連する情報を要求していることを示す通知(たとえば、図4Dの通知479)と一緒にまたはその通知の代わりに所与のユーザのクライアントデバイス410において通知をレンダリングさせることができる。上の例について続けると、支援された通話が再開されるときにクライアントデバイス410の所与のユーザが支援された通話を引き継ぐことを先回りして要求するのではなく、自動アシスタントは、さらなるユーザのさらなる発話された入力に対応するさらなるオーディオデータを処理して、さらなるユーザが自動アシスタントが現在知らないパラメータに関連するさらなる情報を要求しているかどうかを判定することができる。この例において、自動アシスタントは、図4Dの通知479と同様の、さらなる情報の値を要求する通知と一緒にまたはその通知の代わりに、クライアントデバイス410の所与のユーザが支援された通話を引き継ぐことを要求する通知を提供することができる。このようにして、自動アシスタントは、図4B～図4Dにおいて説明されたように支援された通話を継続し、および/または支援された通話の制御をクライアントデバイス410の所与のユーザに受け渡すことができる。 In still other further versions of those implementations, the automated assistant withholds notifications indicating that an assisted call has been placed on hold and/or that an assisted call has been resumed after being placed on hold, and If a call is resumed after being placed on hold, the assisted call continues and further users receive a notification indicating that the automated assistant is requesting information related to a parameter that it currently does not know (e.g., the notification in Figure 4D). 479) may be rendered on a given user's client device 410 in conjunction with or in place of the notification. Continuing with the example above, rather than proactively requesting that a given user of client device 410 take over the assisted call when the assisted call is resumed, the automated assistant may Additional audio data corresponding to additional spoken input may be processed to determine whether additional users request additional information related to parameters that the automated assistant does not currently know. In this example, the automated assistant takes over the assisted call for a given user of client device 410, along with or in place of a notification requesting further information, similar to notification 479 in FIG. 4D. may provide notices requesting that In this manner, the automated assistant may continue the assisted call as described in FIGS. 4B-4D and/or pass control of the assisted call to the given user of the client device 410. I can do it.
ここで図5A～図5Cを参照して、進行中の支援されていない通話中の補助出力の提供に関連するユーザインターフェースの様々な非限定的な例が、説明される。図5A～図5Cは、それぞれ、クライアントデバイス510の所与のユーザのインタラクションの例を表示するグラフィカルユーザインターフェース580を有するクライアントデバイス510を示す。インタラクションは、たとえば、1つまたは複数のソフトウェアアプリケーション(たとえば、ウェブブラウザアプリケーション、自動アシスタントアプリケーション、連絡先アプリケーション、電子メールアプリケーション、カレンダーアプリケーション、および/またはクライアントデバイス510によってアクセス可能なその他のソフトウェアに基づくアプリケーション)とのインタラクション、ならびにさらなるユーザ(たとえば、クライアントデバイス510に関連する自動アシスタント、さらなるクライアントデバイスに関連するさらなる人間の参加者、さらなるユーザのさらなるクライアントデバイスに関連するさらなる自動アシスタント、および/またはその他のさらなるユーザ)とのインタラクションを含み得る。クライアントデバイス510に関連する自動アシスタント(たとえば、図1の自動アシスタント115)の1つまたは複数の態様は、クライアントデバイス510のローカルに実装されてよく、および/またはクライアントデバイス510とネットワーク通信するその他のクライアントデバイスに(たとえば、図1のネットワーク190を介して)分散して実装されてよい。簡単にするために、図5A～図5Cの動作は、自動アシスタントによって実行されるものとして本明細書において説明される。図5A～図5Cのクライアントデバイス510は、モバイル電話として描かれているが、それは限定的であるように意図されていないことを理解されたい。クライアントデバイス510は、たとえば、スタンドアロンのスピーカ、グラフィカルユーザインターフェースに接続されたスピーカ、ラップトップ、デスクトップコンピュータ、ならびに/または電話をかけることができる任意のその他のクライアントデバイスであることが可能である。 5A-5C, various non-limiting examples of user interfaces related to providing auxiliary output during an ongoing unassisted call are described. 5A-5C each illustrate a client device 510 having a graphical user interface 580 that displays an example of a given user's interaction with the client device 510. The interaction may be, for example, an application based on one or more software applications (e.g., a web browser application, an automated assistant application, a contacts application, an email application, a calendar application, and/or other software accessible by the client device 510). ), as well as interactions with further users (e.g., automated assistants associated with client device 510, additional human participants associated with additional client devices, further automated assistants associated with additional client devices of additional users, and/or other further user interactions). One or more aspects of an automated assistant (e.g., automated assistant 115 in FIG. 1) associated with client device 510 may be implemented locally on client device 510 and/or on other devices in network communication with client device 510. It may be implemented in a distributed manner on client devices (eg, via network 190 in FIG. 1). For simplicity, the operations of FIGS. 5A-5C are described herein as being performed by an automated assistant. Although client device 510 in FIGS. 5A-5C is depicted as a mobile phone, it should be understood that this is not intended to be limiting. Client device 510 can be, for example, a stand-alone speaker, a speaker connected to a graphical user interface, a laptop, a desktop computer, and/or any other client device capable of making phone calls.
図5A～図5Cのグラフィカルユーザインターフェース580は、仮想キーボードまたはその他のタッチおよび/もしくはタイピングされた入力によってユーザ入力を生成するためにユーザが選択してよいテキスト応答インターフェース要素584と、クライアントデバイス510のマイクロフォンによってユーザ入力を生成するためにユーザが選択してよい音声応答インターフェース要素585とをさらに含む。一部の実装において、ユーザは、音声応答インターフェース要素585を選択することなく、マイクロフォンによってユーザ入力を生成してよい。たとえば、マイクロフォンによる聴覚的ユーザ入力の能動的な監視が、ユーザが音声応答インターフェース要素585を選択する必要をなくすために行われてよい。それらの一部において、および/またはその他の実装において、音声応答インターフェース要素585は、省略されてよい。さらに、一部の実装において、テキスト応答インターフェース要素584は、追加的および/または代替的に、省略されてよい(たとえば、ユーザは、聴覚的ユーザインターフェース入力のみを提供してよい)。図5A～図5Cのグラフィカルユーザインターフェース580は、コンピューティングデバイス510に1つまたは複数のアクションを実行させるためにユーザによってインタラクションされてよいシステムインターフェース要素581、582、583も含む。一部の実装において、通話詳細インターフェース570は、クライアントデバイス510においてレンダリングされることが可能であり、通話詳細インターフェース570は、選択されるとき、進行中の通話を終了させることができるグラフィカル要素542を含むことが可能であり、選択されるとき、自動アシスタントに、支援された通話を使用してクライアントデバイス510の所与のユーザから通話を引き継がせることができるグラフィカル要素543も含むことが可能である。それらの実装のいくつかのバージョンにおいて、支援された通話インターフェース570は、ユーザが(たとえば、通話詳細インターフェース要素586上で上にスワイプすることによって)さらなる通話の詳細を表示するために通話詳細インターフェース570を展開すること、および/または(たとえば、通話詳細インターフェース要素586上で下にスワイプすることによって)通話詳細インターフェース570を終わらせることを可能にする通話詳細インターフェース要素586も含み得る。 The graphical user interface 580 of FIGS. 5A-5C includes a text responsive interface element 584 that a user may select to generate user input via a virtual keyboard or other touch and/or typed input, and a text responsive interface element 584 of the client device 510. and a voice response interface element 585 that the user may select to generate user input via a microphone. In some implementations, a user may generate user input through a microphone without selecting voice response interface element 585. For example, active monitoring of auditory user input via a microphone may be performed to eliminate the need for the user to select audio responsive interface element 585. In some of these and/or other implementations, voice response interface element 585 may be omitted. Furthermore, in some implementations, text response interface element 584 may additionally and/or alternatively be omitted (eg, the user may provide only auditory user interface input). Graphical user interface 580 of FIGS. 5A-5C also includes system interface elements 581, 582, 583 that may be interacted with by a user to cause computing device 510 to perform one or more actions. In some implementations, the call details interface 570 may be rendered at the client device 510, and the call details interface 570 may include a graphical element 542 that, when selected, can terminate the call in progress. A graphical element 543 can also be included that, when selected, can cause an automated assistant to take over a call from a given user of the client device 510 using assisted calling. . In some versions of those implementations, the assisted call interface 570 allows the user to view additional call details (e.g., by swiping up on the call details interface element 586) on the call details interface 570. It may also include a call details interface element 586 that allows the user to expand the call details interface 570 and/or exit the call details interface 570 (eg, by swiping down on the call details interface element 586).
様々な実装において、自動アシスタントは、クライアントデバイス410の所与のユーザとさらなるクライアントデバイスのさらなるユーザとの間の進行中の通話(すなわち、支援された通話ではない)を中断することができる。一部の実装において、自動アシスタントは、進行中の通話に対応するオーディオデータを処理して、さらなるユーザに関連するエンティティ、進行中の通話中に実行されるタスク、および/または進行中の通話中に実行されるタスクのためのパラメータを特定することができる。さらに、自動アシスタントは、特定されたパラメータの値を決定することができる。たとえば、図5A～図5Cに示されるように、さらなるユーザからの「Example Airlinesです。ご用件をどうぞ」という発話された入力をキャプチャするオーディオデータ552A1、552B1、および/または552C1が、クライアントデバイス510において受信されると仮定する。この例において、自動アシスタントは、オーディオデータ552A1、552B1、および/または552C1を処理することができ、処理に基づいて、さらなるユーザがExample Airlinesの担当者であると判定し、および/またはExample AirlinesをExample Airlinesの担当者に関連するエンティティとして特定することができる。いくつかの追加的および/または代替的な実装において、自動アシスタントは、追加的および/または代替的に、(たとえば、図1の支援された通話エンジン150に関連して)本明細書において説明されたように進行中の通話に関連するメタデータに基づいてエンティティを特定することができる。それらの実装のいくつかのバージョンにおいて、自動アシスタントは、進行中の通話に対応するオーディオデータのストリームを処理せず、進行中の通話に関連するメタデータのみに基づいてエンティティを特定してよく、それによって、進行中の通話のさらなるユーザから同意を得る必要をなくす。以下で説明されるように、自動アシスタントは、進行中の通話に対応するオーディオデータのストリームを処理することなく、クライアントデバイス510の所与のユーザの代理として、依然としてタスクを実行することができる。 In various implementations, the automated assistant may interrupt an ongoing call (i.e., not an assisted call) between a given user of client device 410 and a further user of a further client device. In some implementations, the automated assistant processes audio data corresponding to an ongoing call to identify further user-related entities, tasks to be performed during the ongoing call, and/or Parameters for tasks to be performed can be specified. Furthermore, the automated assistant can determine the values of the identified parameters. For example, as shown in FIGS. 5A-5C, audio data 552A1, 552B1, and/or 552C1 that captures the spoken input "This is Example Airlines. May I assist you" from a further user is transmitted to the client device. Assume that it is received at 510. In this example, the automated assistant may process audio data 552A1, 552B1, and/or 552C1 and, based on the processing, determine that the further user is a representative of Example Airlines and/or Can be identified as an entity related to the person responsible for Example Airlines. In some additional and/or alternative implementations, an automated assistant is additionally and/or alternatively described herein (e.g., in connection with assisted calling engine 150 of FIG. 1). Entities can be identified based on metadata related to ongoing calls, such as: In some versions of those implementations, the automated assistant may not process the stream of audio data corresponding to the ongoing call and may identify entities based solely on metadata related to the ongoing call; Thereby eliminating the need to obtain consent from further users of an ongoing call. As explained below, the automated assistant can still perform tasks on behalf of a given user of client device 510 without processing the stream of audio data corresponding to an ongoing call.
さらに、クライアントデバイス510の所与のユーザ(たとえば、Jane Doe)からの「もしもし、フライトの変更が必要なのですが」という発話された入力をキャプチャするオーディオデータ554A1、554B1、および/または554C1が、クライアントデバイス510において検出され、さらなるユーザのさらなるクライアントデバイスに送信されると仮定する。この例において、自動アシスタントは、オーディオデータ554A1、554B1、および/または552C1を処理することができ、処理に基づいてフライトを変更するタスクを決定することができる。その他の例において、自動アシスタントは、追加的および/または代替的に、(たとえば、図1の支援された通話エンジン150に関連して)本明細書において説明されたようにエンティティに関連するタスクを決定することができる。さらなるユーザからの「承知しました。マイレージ会員番号はお持ちですか」という発話された入力をキャプチャするオーディオデータ552A2、552B2、および/または552C2が、クライアントデバイス510において受信されるとさらに仮定する。この例において、自動アシスタントは、オーディオデータ554A2、554B2、および/または552C2を処理することができ、処理に基づいて、フライトを変更するタスクのためにマイレージ会員番号パラメータを特定する(またはマイレージ会員番号を提供するタスクを考える)ことができる。その他の例において、自動アシスタントは、追加的および/または代替的に、(たとえば、図1の支援された通話エンジン150に関連して)本明細書において説明されたように、パラメータがタスクおよび/またはエンティティに関連付けて記憶されていることに基づいてパラメータを特定することができる。さらに、自動アシスタントは、マイレージ会員番号パラメータの値を決定することができ、マイレージ会員番号パラメータの値をさらなるユーザに提供することができる。 Additionally, audio data 554A1, 554B1, and/or 554C1 that captures the spoken input "Hello, I need to change my flight" from a given user (e.g., Jane Doe) of client device 510 is Assume that it is detected at client device 510 and sent to a further client device of a further user. In this example, the automated assistant may process the audio data 554A1, 554B1, and/or 552C1 and may determine the task of changing the flight based on the processing. In other examples, the automated assistant additionally and/or alternatively performs tasks related to the entity as described herein (e.g., in connection with assisted calling engine 150 of FIG. 1). can be determined. Assume further that audio data 552A2, 552B2, and/or 552C2 is received at client device 510 capturing spoken input of "I understand. Do you have a frequent flyer number?" from a further user. In this example, the automated assistant may process the audio data 554A2, 554B2, and/or 552C2 and, based on the processing, identify the frequent flyer number parameter (or the frequent flyer number ). In other examples, the automated assistant additionally and/or alternatively has parameters as described herein (e.g., in connection with assisted calling engine 150 of FIG. Alternatively, parameters can be identified based on what is stored in association with the entity. Additionally, the automated assistant can determine a value for the frequent flyer number parameter and provide the value for the frequent flyer number parameter to further users.
一部の実装において、自動アシスタントは、クライアントデバイス510の所与のユーザからの、パラメータに関連する情報の要求を含むユーザ入力の受け取りに応じて、クライアントデバイス510の所与のユーザとさらなるユーザとの間の進行中の通話中に特定されたパラメータの値を決定することができる。自動アシスタントは、(たとえば、図1のユーザプロファイルデータベース153Bに記憶された)クライアントデバイス510の所与のユーザに関連するユーザプロファイルに基づいて、パラメータの値を決定することができる。それらの実装のいくつかのバージョンにおいて、自動アシスタントは、パラメータの値の決定に応じて、さらなるユーザのさらなるクライアントデバイスおよび/または所与のユーザのクライアントデバイス510においてパラメータの値をレンダリングさせることができる。たとえば、図5Aに示されるように、クライアントデバイス510の所与のユーザ(たとえば、Jane Doe)からの「アシスタント、私のマイレージ会員番号は何番」という発話された入力をキャプチャするオーディオデータ554A2が、クライアントデバイス510において検出されると仮定する。オーディオデータ554A2にキャプチャされた発話された入力の受け取りに応じて、自動アシスタントは、(たとえば、所与のユーザに関連するExample Airlinesのアカウントがユーザプロファイルに含まれることに基づいて)マイレージ会員パラメータの値を決定することができ、さらなるユーザのクライアントデバイスおよび/または所与のユーザのクライアントデバイス510において「Jane DoeのExample Airlinesマイレージ会員番号は0112358です」という合成音声556A1をレンダリングさせることができる。 In some implementations, the automated assistant interacts with a given user of client device 510 and additional users in response to receiving user input from the given user of client device 510, including requests for information related to parameters. The value of the identified parameter may be determined during an ongoing call during the call. The automated assistant may determine the value of the parameter based on a user profile associated with a given user of client device 510 (eg, stored in user profile database 153B of FIG. 1). In some versions of their implementation, the automated assistant may cause the value of the parameter to be rendered at further client devices of further users and/or client device 510 of a given user in response to determining the value of the parameter. . For example, as shown in FIG. 5A, audio data 554A2 captures the spoken input of "Assistant, what is my frequent flyer number" from a given user (e.g., Jane Doe) of client device 510. , is detected at client device 510. In response to receiving the spoken input captured in the audio data 554A2, the automated assistant determines whether the frequent flyer parameters of the A value can be determined and can cause a synthesized voice 556A1 to be rendered at the further user's client device and/or the given user's client device 510 that says "Jane Doe's Example Airlines frequent flyer number is 0112358."
それらの実装のいくつかの追加的および/または代替的なバージョンにおいては、オーディオデータ554A2に含まれる発話された入力を受け取るのではなく、自動アシスタントは、支援された通話を使用してクライアントデバイス510の所与のユーザから通話を引き継ぐためのグラフィカル要素543の選択を受け取り得る。たとえば、グラフィカル要素543の選択の受け取りに応じて、自動アシスタントは、(たとえば、所与のユーザに関連するExample Airlinesのアカウントがユーザプロファイルに含まれることに基づいて)マイレージ会員パラメータの値を決定することができ、合成音声556A1をさらなるユーザのさらなるクライアントデバイスおよび/または所与のユーザのクライアントデバイス510においてレンダリングさせることができる。 In some additional and/or alternative versions of those implementations, rather than receiving spoken input contained in audio data 554A2, the automated assistant communicates with client device 510 using an assisted call. may receive a selection of graphical element 543 to take over a call from a given user. For example, in response to receiving a selection of graphical element 543, the automated assistant determines a value for a frequent flyer parameter (e.g., based on the user profile including an Example Airlines account associated with the given user). The synthesized speech 556A1 may be rendered at additional client devices of additional users and/or client device 510 of a given user.
特に、オーディオデータ554A2は、マイレージ会員番号に関連するエンティティを特定しない「私のマイレージ会員番号は何番」を含む。実装において、自動アシスタントがさらなるユーザから同意を得ず、および/またはマイレージ会員番号パラメータに関連するエンティティのインジケーションを提供しない場合、自動アシスタントは、進行中の通話に関連するメタデータに基づいて、「私のマイレージ会員番号」がExample Airlinesに関連するマイレージ会員番号パラメータの値を参照すると判定することができる。このようにして、自動アシスタントは、進行中の通話に対応するオーディオデータのストリームを処理することなく、クライアントデバイス510の所与のユーザからの要求に応じた値を依然として提供することができる。 In particular, audio data 554A2 includes "What is my frequent flyer number" that does not specify an entity associated with the frequent flyer number. In implementation, if the automated assistant does not obtain consent from further users and/or provides an indication of the entity related to the frequent flyer number parameter, the automated assistant may, based on metadata related to the call in progress, It can be determined that "my frequent flyer number" refers to the value of the frequent flyer number parameter related to Example Airlines. In this way, the automated assistant can still provide value in response to a request from a given user of client device 510 without processing the stream of audio data corresponding to an ongoing call.
その他の実装において、自動アシスタントは、クライアントデバイス510の所与のユーザからの、パラメータに関連する情報の要求を含むいかなるユーザ入力も受け取ることなく、クライアントデバイス510の所与のユーザとさらなるユーザとの間の進行中の通話中に特定されたパラメータの値を先回りして決定することができる。自動アシスタントは、(たとえば、図1のユーザプロファイルデータベース153Bに記憶された)クライアントデバイス510の所与のユーザに関連するユーザプロファイルに基づいて、パラメータの値を決定することができる。 In other implementations, the automated assistant communicates between a given user of client device 510 and further users without receiving any user input from the given user of client device 510, including requests for information related to parameters. The values of parameters identified during an ongoing call can be determined proactively. The automated assistant may determine the value of the parameter based on a user profile associated with a given user of client device 510 (eg, stored in user profile database 153B of FIG. 1).
それらの実装のいくつかのバージョンにおいて、自動アシスタントは、支援された通話がタスクを実行することができ、および/またはパラメータの値をさらなるユーザに提供することができるというインジケーションを含む通知を、クライアントデバイス510においてレンダリングさせることができる。自動化された支援は、クライアントデバイス510の所与のユーザからの、支援されたスピーチを呼び出すユーザ入力の受け取りに応じて、さらなるユーザのさらなるクライアントデバイスにおいて、値を含む合成音声をレンダリングさせることができる。たとえば、図5Bに示されるように、自動アシスタントが、オーディオデータ552B2内のマイレージ会員番号パラメータの値の要求の特定に応じて、マイレージ会員番号パラメータの値を決定すると仮定する。さらに、自動アシスタントが、「あなたのExample Airlinesマイレージ会員番号は0112358です。Example Airlinesの担当者にマイレージ会員番号を提供してよろしいですか」という通知579をクライアントデバイス510の通話詳細インターフェース570においてレンダリングさせると仮定する。そのとき、自動アシスタントは、自動アシスタントがマイレージ会員番号パラメータの値をされなるユーザに提供すべきであることを示すグラフィカル要素579Aおよび/またはグラフィカル要素543の選択を受け取ることに応じて、さらなるユーザのさらなるクライアントデバイスおよび/または所与のユーザのクライアントデバイス510において「Jane DoeのExample Airlinesマイレージ会員番号は0112358です」という合成音声556B1をレンダリングさせることができる。いくつかの追加的および/または代替的な実装において、自動アシスタントは、進行中の対話中に特定されたパラメータの値を含むクライアントデバイス510の所与のユーザの発話された入力を検出することができる。それらの実装のいくつかのバージョンにおいて、自動アシスタントは、通話詳細インターフェース570に含まれる通知579を自動的に終わらせることができる。 In some versions of their implementation, the automated assistant sends a notification containing an indication that the assisted call can perform the task and/or provide the value of the parameter to a further user. Rendering may occur at client device 510. The automated assistance may, in response to receipt of user input from a given user of client device 510 that invokes assisted speech, cause a synthesized speech that includes the value to be rendered at a further client device of a further user. . For example, as shown in FIG. 5B, assume that the automated assistant determines the value of the frequent flyer number parameter in response to identifying the requested value of the frequent flyer number parameter in audio data 552B2. Additionally, the automated assistant causes a notification 579 to be rendered in the call details interface 570 of the client device 510 that says, "Your Example Airlines frequent flyer number is 0112358. Would you like to provide your frequent flyer number to an Example Airlines representative?" Assume that The automated assistant then receives the selection of graphical element 579A and/or graphical element 543 indicating that the automated assistant should provide the value of the frequent flyer number parameter to the additional user. A synthesized voice 556B1 may be rendered at a further client device and/or a given user's client device 510 that reads "Jane Doe's Example Airlines frequent flyer number is 0112358." In some additional and/or alternative implementations, the automated assistant may detect spoken input of a given user of the client device 510 that includes a value for a parameter identified during an ongoing interaction. can. In some versions of their implementation, the automated assistant may automatically terminate the notification 579 included in the call details interface 570.
それらの実装のいくつかのその他のバージョンにおいて、自動アシスタントは、パラメータに関連する情報の要求を含むいかなるユーザ入力も受け取ることなく、パラメータの値の決定に応じて、進行中の通話中に特定されたパラメータの値を先回りして提供することもできる。自動化された支援は、値の決定に応じて、さらなるユーザのさらなるクライアントデバイスにおいて、値を含む合成音声をレンダリングさせることができる。たとえば、図5Cに示されるように、自動アシスタントが、オーディオデータ552C2内の、さらなるユーザからの、マイレージ会員番号パラメータの値の要求の特定に応じて、マイレージ会員番号パラメータの値を決定すると仮定する。そのとき、自動アシスタントは、マイレージ会員番号パラメータに関連する情報の要求を含むいかなるユーザ入力も受け取ることなく、マイレージ会員番号パラメータの値の決定に応じて、さらなるユーザのさらなるクライアントデバイスおよび/または所与のユーザのクライアントデバイス510において「Jane DoeのExample Airlinesマイレージ会員番号は0112358です」という合成音声556C1をレンダリングさせることができる。それらの実装のいくつかのさらなるバージョンにおいて、自動アシスタントは、パラメータの決定された値に関連する信頼性測定指標が信頼性の閾値を満たす場合、進行中の通話中に特定されたパラメータの値を先回りして提供するだけでよい。信頼性測定指標は、たとえば、所与のユーザが、同じ情報の以前の要求の受け取りに応じて、決定された値を以前提供したかどうか、進行中の通話中に特定されたパラメータが、進行中の通話中に特定されたタスクに関連付けて記憶された候補パラメータであるかどうか、値が決定されるソース(たとえば、電子メール/カレンダーアプリケーション対テキストメッセージングアプリケーション)、および/または信頼性測定指標を決定する方法に基づき得る。 In some other versions of those implementations, the automated assistant is identified during an ongoing call in response to determining the value of a parameter without receiving any user input, including a request for information related to the parameter. It is also possible to provide the values of parameters in advance. The automated assistance may cause synthetic speech including the value to be rendered at a further client device of a further user in response to determining the value. For example, as shown in FIG. 5C, assume that the automated assistant determines the value of the frequent flyer number parameter in response to identifying a request for the value of the frequent flyer number parameter from a further user in audio data 552C2. . The automated assistant then determines the value of the further client device of the further user and/or the given The synthesized voice 556C1 that says "Jane Doe's Example Airlines frequent flyer number is 0112358" can be rendered on the user's client device 510. In some further versions of their implementation, the automated assistant determines the value of the parameter identified during an ongoing call if the reliability metric associated with the determined value of the parameter satisfies a reliability threshold. All you have to do is be proactive and offer it. Confidence metrics include, for example, whether a given user has previously provided the determined value in response to receipt of a previous request for the same information, whether the parameters identified during an ongoing call are whether the candidate parameters are stored in association with the task identified during the call, the source from which the value is determined (e.g., email/calendar application vs. text messaging application), and/or the reliability metric. may be based on the method of determination.
さらに、様々な実装においては、進行中の通話中に自動アシスタントが特定されたパラメータの値を提供した後、自動アシスタントは、進行中の通話の残りを引き継ぐことができる。それらの実装のいくつかのバージョンにおいて、自動アシスタントは、グラフィカル要素543のユーザの選択に応じて、進行中の通話の残りを引き継ぐことができる。さらに、自動アシスタントは、クライアントデバイス510に送信されたオーディオデータに基づいてタスクのためのパラメータを特定し、特定されたパラメータの値を決定することを継続することができる。さらに、所与の要求が自動アシスタントによって決定され得ない情報を求めるものであると自動アシスタントが判定する場合、(たとえば、図4Cおよび図4Dに関連して)上で説明されたように、自動アシスタントは、要求に応じるさらなるユーザ入力を求める通知を提供することができ、自動アシスタントは、次に、要求に応じるさらなるユーザ入力に基づいて値を提供することができる。 Additionally, in various implementations, after the automated assistant provides values for the specified parameters during an ongoing call, the automated assistant may take over the remainder of the ongoing call. In some versions of those implementations, the automated assistant may take over the remainder of the ongoing call depending on the user's selection of graphical element 543. Additionally, the automated assistant may continue to identify parameters for the task based on the audio data sent to client device 510 and determine values for the identified parameters. Additionally, if the automated assistant determines that a given request asks for information that cannot be determined by the automated assistant, the automated The assistant may provide a notification requesting further user input in response to the request, and the automated assistant may then provide a value based on the further user input in response to the request.
図5A～図5Cは、マイレージ会員番号を提供するタスクに関連して本明細書において説明されているが、それは限定的であるように意図されておらず、本明細書において説明される技術は、複数の異なるエンティティに関連して実行され得る複数の異なるタスクのために利用されることが可能であることを理解されたい。さらに、図5A～図5Cに示されていないが、たとえ進行中の通話が支援された通話を使用して自動アシスタントによって開始されない場合でも、進行中の通話が開始されると、自動アシスタントがさらなるユーザから同意を得ることができることを理解されたい。自動アシスタントは、本明細書において説明される任意の方法を使用して、さらなるユーザから同意を得ることができる。 Although FIGS. 5A-5C are described herein in connection with the task of providing frequent flyer numbers, it is not intended to be limiting and the techniques described herein are , can be utilized for multiple different tasks that may be performed in connection with multiple different entities. Additionally, although not shown in Figures 5A-5C, even if an ongoing call is not initiated by the automated assistant using assisted calling, once an ongoing call is initiated, the automated assistant may Please understand that consent may be obtained from the user. The automated assistant may obtain consent from further users using any of the methods described herein.
図6は、本明細書において説明される技術の1つまたは複数の態様を実行するために任意で利用されてよい例示的なコンピューティングデバイス610のブロック図である。一部の実装においては、クライアントデバイス、クラウドに基づく自動アシスタント構成要素、および/またはその他の構成要素のうちの1つまたは複数が、例示的なコンピューティングデバイス610の1つまたは複数の構成要素を含んでよい。 FIG. 6 is a block diagram of an example computing device 610 that may optionally be utilized to perform one or more aspects of the techniques described herein. In some implementations, one or more of a client device, a cloud-based automated assistant component, and/or other components connect one or more components of the example computing device 610. may be included.
概して、コンピューティングデバイス610は、バスサブシステム612を介していくつかの周辺デバイスと通信する少なくとも1つのプロセッサ614を含む。これらの周辺デバイスは、たとえば、メモリサブシステム625およびファイルストレージサブシステム626を含むストレージサブシステム624と、ユーザインターフェース出力デバイス620と、ユーザインターフェース入力デバイス622と、ネットワークインターフェースサブシステム616とを含んでよい。入力および出力デバイスは、コンピューティングデバイス610とのユーザのインタラクションを可能にする。ネットワークインターフェースサブシステム616は、外部ネットワークへのインターフェースを提供し、その他のコンピューティングデバイスの対応するインターフェースデバイスに結合される。 Generally, computing device 610 includes at least one processor 614 that communicates with a number of peripheral devices via bus subsystem 612. These peripheral devices may include, for example, a storage subsystem 624 that includes a memory subsystem 625 and a file storage subsystem 626, a user interface output device 620, a user interface input device 622, and a network interface subsystem 616. . Input and output devices enable user interaction with computing device 610. Network interface subsystem 616 provides an interface to external networks and is coupled to corresponding interface devices of other computing devices.
ユーザインターフェース入力デバイス622は、キーボード、マウス、トラックボール、タッチパッド、もしくはグラフィックスタブレットなどのポインティングデバイス、スキャナ、ディスプレイに組み込まれたタッチスクリーン、音声認識システムなどの音声入力デバイス、マイクロフォン、および/またはその他の種類の入力デバイスを含んでよい。概して、用語「入力デバイス」の使用は、コンピューティングデバイス610または通信ネットワークに情報を入力するためのすべての可能な種類のデバイスおよび方法を含むように意図される。 User interface input device 622 may include a pointing device such as a keyboard, mouse, trackball, touch pad, or graphics tablet, a scanner, a touch screen integrated into a display, an audio input device such as a voice recognition system, a microphone, and/or Other types of input devices may also be included. In general, use of the term "input device" is intended to include all possible types of devices and methods for entering information into computing device 610 or communication network.
ユーザインターフェース出力デバイス620は、ディスプレイサブシステム、プリンタ、ファックスマシン、または音声出力デバイスなどの非視覚的表示を含んでよい。ディスプレイサブシステムは、ブラウン管(CRT)、液晶ディスプレイ(LCD)などのフラットパネルデバイス、プロジェクションデバイス、または可視画像を生成するための何らかのその他のメカニズムを含んでよい。ディスプレイサブシステムは、音声出力デバイスを介するなどして非視覚的表示を提供する場合もある。概して、用語「出力デバイス」の使用は、コンピューティングデバイス610からユーザまたは別のマシンもしくはコンピューティングデバイスに情報を出力するすべての可能な種類のデバイスおよび方法を含むように意図される。 User interface output device 620 may include a non-visual display such as a display subsystem, printer, fax machine, or audio output device. The display subsystem may include a flat panel device such as a cathode ray tube (CRT), a liquid crystal display (LCD), a projection device, or some other mechanism for producing visible images. The display subsystem may also provide non-visual display, such as through an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and methods for outputting information from computing device 610 to a user or another machine or computing device.
ストレージサブシステム624は、本明細書において説明されるモジュールの一部またはすべての機能を提供するプログラミングおよびデータ構造を記憶する。たとえば、ストレージサブシステム624は、本明細書において開示される方法の選択された態様を実行するためおよび図1に示された様々な構成要素を実装するための論理を含んでよい。 Storage subsystem 624 stores programming and data structures that provide the functionality of some or all of the modules described herein. For example, storage subsystem 624 may include logic for performing selected aspects of the methods disclosed herein and for implementing the various components shown in FIG. 1.
これらのソフトウェアモジュールは、概して、プロセッサ614によって単独で、またはその他のプロセッサとの組合せで実行される。ストレージサブシステム624において使用されるメモリ625は、プログラムの実行中の命令およびデータの記憶のための主ランダムアクセスメモリ(RAM)630と、決まった命令が記憶される読み出し専用メモリ(ROM)632とを含むいくつかのメモリを含み得る。ファイルストレージサブシステム626は、プログラムおよびデータファイルのための永続的ストレージを提供することができ、ハードディスクドライブ、関連する取り外し可能な媒体をともなうフロッピーディスクドライブ、CD-ROMドライブ、光学式ドライブ、または取り外し可能なメディアカートリッジを含んでよい。特定の実装の機能を実装するモジュールは、ストレージサブシステム624内のファイルストレージサブシステム626によって、またはプロセッサ614によりアクセスされ得るその他のマシンに記憶される場合がある。 These software modules are generally executed by processor 614 alone or in combination with other processors. The memory 625 used in the storage subsystem 624 includes a main random access memory (RAM) 630 for storing instructions and data during program execution, and a read only memory (ROM) 632 in which fixed instructions are stored. may contain some memory, including The file storage subsystem 626 can provide persistent storage for program and data files and can be a hard disk drive, a floppy disk drive with associated removable media, a CD-ROM drive, an optical drive, or a removable may include a possible media cartridge. Modules implementing the functionality of a particular implementation may be stored by file storage subsystem 626 within storage subsystem 624 or on other machines that may be accessed by processor 614.
バスサブシステム612は、コンピューティングデバイス610の様々な構成要素およびサブシステムに意図されたように互いに通信させるためのメカニズムを提供する。バスサブシステム612は単一のバスとして概略的に示されているが、バスサブシステム612の代替的な実装は複数のバスを使用する場合がある。 Bus subsystem 612 provides a mechanism for allowing the various components and subsystems of computing device 610 to communicate with each other as intended. Although bus subsystem 612 is schematically shown as a single bus, alternative implementations of bus subsystem 612 may use multiple buses.
コンピューティングデバイス610は、ワークステーション、サーバ、コンピューティングクラスタ、ブレードサーバ、サーバファーム、または任意のその他のデータ処理システムもしくはコンピューティングデバイスを含む様々な種類であることが可能である。コンピュータおよびネットワークの変わり続ける性質が原因で、図6に示されたコンピューティングデバイス610の説明は、いくつかの実装を示すことを目的とする特定の例としてのみ意図される。図6に示されたコンピューティングデバイスよりも多くのまたは図6に示されたコンピューティングデバイスよりも少ない構成要素を有するコンピューティングデバイス610の多くのその他の構成が、可能である。 Computing device 610 can be of various types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 610 shown in FIG. 6 is intended only as a specific example for the purpose of illustrating some implementations. Many other configurations of computing device 610 having more or fewer components than the computing device illustrated in FIG. 6 are possible.
本明細書において説明されたシステムがユーザについての個人情報を収集するかもしくはそうでなければ監視するか、または個人情報および/もしくは監視された情報を利用する場合がある状況において、ユーザは、プログラムまたは特徴がユーザ情報(たとえば、ユーザのソーシャルネットワーク、ソーシャルな行為もしくは活動、職業、ユーザのプリファレンス、またはユーザの現在の地理的位置についての情報)を収集するかどうかを制御するか、あるいはユーザにより関連性がある可能性があるコンテンツをコンテンツサーバから受信するべきかどうかおよび/またはどのようにして受信するべきかを制御する機会を与えられてよい。また、特定のデータが、個人を特定することができる情報が削除されるように、記憶されるかまたは使用される前に1つまたは複数の方法で処理される場合がある。たとえば、ユーザのアイデンティティが、個人を特定することができる情報がユーザに関して決定され得ないか、または地理的位置情報が取得される場合にユーザの地理的位置が(都市、郵便番号、もしくは州のレベルまでになど)一般化される場合があり、したがって、ユーザの特定の地理的位置が決定され得ないように処理されてよい。したがって、ユーザは、情報がユーザについてどのようにして収集されるかおよび/または使用されるかを制御することができる場合がある。 In situations where the systems described herein may collect or otherwise monitor personal information about a user or make use of personal information and/or monitored information, the user may or control whether the feature collects user information (e.g., information about the user's social networks, social conduct or activities, occupation, user preferences, or user's current geographic location); The user may be given the opportunity to control whether and/or how potentially more relevant content should be received from the content server. Certain data may also be processed in one or more ways before being stored or used, such that personally identifiable information is removed. For example, if the user's identity is determined by the user's geographic location (city, zip code, or state), if no personally identifiable information can be determined about the user, or if geographic location information is obtained, etc.) and may therefore be treated such that the user's specific geographic location cannot be determined. Accordingly, a user may be able to control how information is collected and/or used about the user.
一部の実装においては、1つまたは複数のプロセッサによって実施される方法が、提供され、所与のユーザに関連するクライアントデバイスを介して所与のユーザから、支援された通話を開始するためのユーザ入力を受け取るステップと、ユーザ入力に基づいて、支援された通話中に所与のユーザの代理として関わり合うエンティティ、および支援された通話中に所与のユーザの代理として実行されるタスクを決定するステップとを含む。方法は、タスクおよび/またはエンティティに関連付けて記憶された1つまたは複数の候補パラメータに関して、支援された通話中にタスクを実行する際に合成音声を自動的に生成するのに使用される1つまたは複数の対応する値を決定するステップと、所与のユーザに関連するクライアントデバイスを使用して、支援された通話の実行を開始するステップと、支援された通話の実行中に、エンティティに関連するさらなるユーザの発言をキャプチャする、支援された通話のオーディオデータを処理することに基づいて、さらなるパラメータに関連する情報がさらなるユーザによって要求されると判定するステップとをさらに含む。方法は、さらなるパラメータに関連する情報が要求されるとの判定に応じて、クライアントデバイスに、支援された通話外で、さらなるパラメータを特定し、情報に関するさらなるユーザ入力を要求する通知をレンダリングさせるステップをさらに含む。方法は、通知に応じる何らかのさらなる入力を受け取るのに先立って、支援された通話を継続するステップをさらに含む。支援された通話を継続するステップは、候補パラメータの対応する値のうちの1つまたは複数に基づく合成音声の1つまたは複数のインスタンスをレンダリングすることを含む。方法は、支援された通話を継続する間に、通知に応じ、さらなるパラメータの特定の値を特定するさらなるユーザ入力が閾値の継続時間内に受け取られるかどうかを判定するステップと、さらなるユーザ入力が閾値の継続時間内に受け取られるとの判定に応じて、支援された通話の一部として、特定の値に基づくさらなる合成音声をレンダリングするステップとをさらに含む。 In some implementations, a method implemented by one or more processors is provided for initiating an assisted call from a given user via a client device associated with the given user. receiving user input and determining, based on the user input, entities to engage with on behalf of a given user during the assisted call and tasks to perform on behalf of the given user during the assisted call; and a step of doing so. A method is one used to automatically generate synthesized speech when performing a task during an assisted call with respect to one or more candidate parameters stored in association with the task and/or entity. or determining a plurality of corresponding values, and initiating execution of the assisted call using a client device associated with a given user; and during execution of the assisted call, associated with an entity. and determining that information related to the additional parameter is requested by the additional user based on processing the audio data of the assisted call, capturing the utterance of the additional user. In response to determining that information related to the additional parameters is requested, the method causes the client device to render, outside the assisted call, a notification identifying the additional parameters and requesting further user input regarding the information. further including. The method further includes continuing the assisted call prior to receiving any further input in response to the notification. Continuing the assisted call includes rendering one or more instances of synthetic speech based on one or more of the corresponding values of the candidate parameters. The method includes the steps of: determining whether further user input identifying a particular value of the further parameter is received within a threshold duration in response to the notification while continuing the assisted call; and rendering further synthesized speech based on the particular value as part of the assisted call in response to the determination that it is received within the threshold duration.
本明細書において開示されるテクノロジーのこれらのおよびその他の実装は、任意で、以下の特徴のうちの1つまたは複数を含み得る。 These and other implementations of the technology disclosed herein may optionally include one or more of the following features.
一部の実装において、対応する値のうちの所与の値を決定することは、支援された通話を開始する前に、候補パラメータのうちの所与の候補パラメータを特定し、所与の候補パラメータに関連するさらなる情報を要求するプロンプトを生成することと、クライアントデバイスにプロンプトをレンダリングさせることと、プロンプトに応じるさらなるユーザ入力に基づいて、所与の候補パラメータの所与の値を特定することとを含む。 In some implementations, determining a given value of the corresponding values involves identifying a given candidate parameter among candidate parameters and determining a given candidate parameter before initiating an assisted call. generating a prompt requesting further information related to the parameter; causing a client device to render the prompt; and determining a given value for a given candidate parameter based on further user input in response to the prompt. including.
一部の実装において、対応する値のうちのさらなる値を決定することは、支援された通話を開始する前に、所与のユーザに関連するユーザプロファイルに基づいてさらなる値を特定することを含む。 In some implementations, determining further values of the corresponding values includes identifying further values based on a user profile associated with the given user prior to initiating the assisted call. .
一部の実装において、支援された通話を継続するステップは、支援された通話のさらなるオーディオデータを処理して、さらなるユーザのさらなる発言が候補パラメータのうちの所与の候補パラメータの要求を含むと判定することと、さらなる発言が所与の候補パラメータの要求を含むとの判定に応じて、クライアントデバイスに、通話において、合成音声の1つまたは複数のインスタンスのうちの所与のインスタンスをレンダリングさせることとを含む。それらの実装のいくつかのバージョンにおいて、所与のインスタンスは、所与の候補パラメータに関して所与の値が決定されることに基づいて、対応する値のうちの所与の値を含み、所与のインスタンスは、所与のユーザからのいかなるさらなるユーザ入力も要求することなくレンダリングされる。 In some implementations, continuing the assisted call processes further audio data of the assisted call so that further utterances of the further user include a request for a given one of the candidate parameters. and causing the client device to render the given instance of the one or more instances of the synthesized speech in the call in response to determining that the further utterance includes a request for the given candidate parameter. Including things. In some versions of those implementations, a given instance contains a given value of the corresponding values based on which a given value is determined for a given candidate parameter, and a given is rendered without requiring any further user input from the given user.
一部の実装において、支援された通話を継続するステップは、さらなるユーザのさらなる発言がさらなる閾値の継続時間内に受け取られるかどうかを判定するために、さらなるオーディオデータを処理することと、さらなる発言がさらなる閾値の継続時間内にさらなるユーザから受け取られないとの判定に応じて、支援された通話中に、合成音声の1つまたは複数のインスタンスのうち、さらなるユーザによって要求されなかった対応する値のうちの1つまたは複数に基づく別のインスタンスをレンダリングすることとを含む。 In some implementations, continuing the assisted call includes processing the additional audio data to determine whether additional user utterances are received within an additional threshold duration; and further utterances. is not received from the further user within the further threshold duration, the corresponding value of the one or more instances of the synthesized speech that was not requested by the further user during the assisted call. and rendering another instance based on one or more of the.
一部の実装において、方法は、さらなるパラメータに関連する情報がさらなるユーザによって要求されるとの判定に応じて、エンティティに関連付けて記憶された1つまたは複数の候補パラメータを、さらなるパラメータを含むように更新するステップをさらに含む。 In some implementations, the method causes the one or more candidate parameters stored in association with the entity to be configured to include the additional parameter in response to determining that information related to the additional parameter is requested by the additional user. further including the step of updating to .
一部の実装において、方法は、さらなるパラメータに関連する情報がさらなるユーザによって要求されるときにクライアントデバイスの状態を決定するステップと、クライアントデバイスの状態に基づいて、通知および/または通知をレンダリングするための1つもしくは複数のプロパティを決定するステップとをさらに含む。 In some implementations, the method includes determining a state of the client device when information related to the further parameter is requested by the further user, and rendering the notification and/or notification based on the state of the client device. and determining one or more properties for.
それらの実装のいくつかのバージョンにおいて、クライアントデバイスの状態は、所与のユーザが支援された通話を能動的に監視していることを示し、クライアントデバイスの状態に基づいて通知を決定するステップは、所与のユーザが支援された通話を能動的に監視していることを示すクライアントデバイスの状態に基づいて、1つまたは複数の選択可能なグラフィカル要素と一緒にクライアントデバイスのディスプレイによって視覚的にレンダリングされる視覚的な構成要素を含むように通知を決定することを含む。それらの実装のいくつかのさらなるバージョンにおいて、通知に応じるさらなるユーザ入力は、1つまたは複数の選択可能なグラフィカル要素のうちの所与の1つの選択を含む。 In some versions of those implementations, the state of the client device indicates that the given user is actively monitoring the assisted call, and the step of determining the notification based on the state of the client device includes , visually by the client device display along with one or more selectable graphical elements based on the state of the client device that indicates that the given user is actively monitoring the assisted call. including determining a notification to include a visual component to be rendered. In some further versions of those implementations, further user input in response to the notification includes selection of a given one of one or more selectable graphical elements.
それらの実装のいくつかのバージョンにおいて、クライアントデバイスの状態は、所与のユーザが支援された通話を能動的に監視していないことを示し、クライアントデバイスの状態に基づいて通知を決定するステップは、所与のユーザが支援された通話を能動的に監視していないことを示すクライアントデバイスの状態に基づいて、クライアントデバイスの1つまたは複数のスピーカによって聴覚的にレンダリングされる聴覚的構成要素を含むように通知を決定することを含む。 In some versions of those implementations, the state of the client device indicates that the given user is not actively monitoring assisted calls, and the step of determining the notification based on the state of the client device is , an auditory component that is rendered aurally by one or more speakers of a client device based on a state of the client device that indicates that a given user is not actively monitoring the assisted call. Including determining the notice to include.
一部の実装において、方法は、支援された通話を終了させるステップと、支援された通話を終了させた後、クライアントデバイスに、支援された通話の結果のインジケーションを含むさらなる通知をレンダリングさせるステップとをさらに含む。それらの実装のいくつかのバージョンにおいて、支援された通話のインジケーションを含むさらなる通知は、支援された通話の終了に応じてユーザの代理として実行されるさらなるタスクのインジケーションを含むか、または選択されると、クライアントデバイスにユーザの代理としてさらなるタスクを実行させる1つもしくは複数の選択可能なグラフィカル要素を含む。 In some implementations, the method includes the steps of terminating the assisted call and, after terminating the assisted call, causing the client device to render a further notification including an indication of the outcome of the assisted call. further including. In some versions of those implementations, the further notification containing the indication of the assisted call includes an indication of further tasks to be performed on behalf of the user upon termination of the assisted call, or the selection includes one or more selectable graphical elements that, when selected, cause the client device to perform further tasks on behalf of the user.
一部の実装において、方法は、さらなるユーザ入力が閾値の継続時間内に受け取られないとの判定に応じて、支援された通話を終了させるステップと、支援された通話を終了させた後、クライアントデバイスに、支援された通話の結果のインジケーションを含むさらなる通知をレンダリングさせるステップとをさらに含む。 In some implementations, the method includes the steps of: terminating the assisted call in response to determining that no further user input is received within a threshold duration; causing the device to render further notifications including an indication of the outcome of the assisted call.
一部の実装において、閾値の継続時間は、さらなるパラメータを特定し、情報に関するさらなるユーザ入力を要求する通知がレンダリングされるときからの固定の継続時間、または対応する値のうちの1つもしくは複数の最後の1つがさらなるクライアントデバイスを介してさらなるユーザに提示するためにレンダリングされるときに基づく動的な持続時間である。 In some implementations, the threshold duration is a fixed duration from when a notification is rendered that specifies further parameters and requests further user input regarding information, or one or more of the corresponding values. The last one is a dynamic duration based on when it is rendered for presentation to further users via further client devices.
いくつかの実装では、方法は、支援された通話を開始した後、エンティティに関連するさらなるユーザから、支援された通話を監視することへの同意を得るステップをさらに含む。 In some implementations, the method further includes, after initiating the assisted call, obtaining consent from a further user associated with the entity to monitor the assisted call.
一部の実装においては、1つまたは複数のプロセッサによって実施される方法が、提供され、クライアントデバイスにおいて、クライアントデバイスの所与のユーザとさらなるクライアントデバイスのさらなるユーザとの間の進行中の通話を検出するステップ、進行中の通話中の少なくとも1つの発話された発言をキャプチャするオーディオデータのストリームを処理して、認識されたテキストを生成するステップを含む。少なくとも1つの発話された発言は、所与のユーザまたはさらなるユーザのものである。方法は、認識されたテキストを処理することに基づいて、少なくとも1つの発話された発言がパラメータの情報を要求することを特定するステップと、パラメータに関して、所与のユーザの個人的なアクセス制限されたデータを使用して、パラメータの値が解決可能であると判定するステップとをさらに含む。方法は、値が解決可能であるとの判定に応じて、進行中の通話中に、値に基づく出力をレンダリングするステップをさらに含む。 In some implementations, a method implemented by one or more processors is provided that provides a method for communicating, at a client device, an ongoing call between a given user of a client device and a further user of a further client device. detecting and processing a stream of audio data capturing at least one spoken utterance during an ongoing call to generate recognized text. The at least one spoken utterance is of the given user or a further user. The method includes the steps of: determining that at least one spoken utterance requests information of a parameter based on processing the recognized text; and determining that the value of the parameter is solvable using the determined data. The method further includes rendering output based on the value during the ongoing call in response to determining that the value is resolvable.
本明細書において開示されるテクノロジーのこれらのおよびその他の実装は、任意で、以下の特徴のうちの1つまたは複数を含み得る。 These and other implementations of the technology disclosed herein may optionally include one or more of the following features.
一部の実装において、方法は、パラメータの値を解決するステップをさらに含む。それらの実装のいくつかのバージョンにおいて、進行中の通話中に出力をレンダリングするステップは、さらに、パラメータの値の解決に応じる。それらの実装のいくつかのさらなるバージョンにおいて、パラメータの値を解決するステップは、所与のユーザとさらなるユーザとの間の進行中の通話のメタデータを分析することと、分析に基づいて、さらなるユーザに関連するエンティティを特定することと、値がエンティティおよびパラメータに関連付けて記憶されていることに基づいて、値を解決することとを含む。 In some implementations, the method further includes resolving values for the parameters. In some versions of those implementations, rendering output during an ongoing call is further responsive to resolving values of parameters. In some further versions of those implementations, the step of resolving the value of the parameter includes analyzing metadata of an ongoing call between a given user and a further user and, based on the analysis, further The method includes identifying an entity associated with the user and resolving the value based on the value being stored in association with the entity and the parameter.
一部の実装において、出力は、合成音声を含み、進行中の通話中に、値に基づく出力をレンダリングするステップは、合成音声を進行中の通話の一部としてレンダリングすることを含む。それらの実装のいくつかのバージョンにおいて、方法は、合成音声を進行中の通話の一部としてレンダリングする前に、所与のユーザから、進行中の通話中に支援を作動させるためのユーザ入力を受け取るステップをさらに含む。それらの実装のいくつかのバージョンにおいて、合成音声を進行中の通話の一部としてレンダリングすることは、さらに、支援を作動させるためのユーザ入力の受け取りに応じる。 In some implementations, the output includes synthesized speech, and during the ongoing call, rendering the value-based output includes rendering the synthesized speech as part of the ongoing call. In some versions of their implementation, the method receives user input from a given user to activate assistance during an ongoing call before rendering the synthesized speech as part of the ongoing call. further comprising the step of receiving. In some versions of those implementations, rendering the synthesized voice as part of an ongoing call is further responsive to receiving user input to activate the assistance.
一部の実装において、出力は、クライアントデバイスにおいて、進行中の通話外でレンダリングされる通知を含む。それらの実装のいくつかのバージョンにおいて、出力は、進行中の通話の一部としてレンダリングされる合成音声をさらに含み、方法は、通知をレンダリングした後に、通知に応じた肯定的なユーザ入力の受け取りに応じて、合成音声をレンダリングするステップをさらに含む。 In some implementations, the output includes a notification that is rendered at the client device outside of an ongoing call. In some versions of those implementations, the output further includes synthesized speech rendered as part of an ongoing call, and the method, after rendering the notification, includes receiving positive user input in response to the notification. The method further includes rendering the synthesized speech in accordance with the method.
一部の実装において、方法は、パラメータの情報を要求する少なくとも1つの発話された発言の後、閾値の継続時間の間、オーディオデータのストリームを処理することに基づいて、所与のユーザの、閾値の継続時間内に受け取られた任意のさらなる発話された発言が値を含むかどうかを判定するステップをさらに含む。それらの実装のいくつかのバージョンにおいて、出力を提供することは、所与のユーザの、閾値の継続時間内に受け取られたさらなる発話された発言が値を含まないと判定することを条件とする。 In some implementations, the method is based on processing a stream of audio data for a threshold duration after at least one spoken utterance requesting information of a given user. The method further includes determining whether any additional spoken utterances received within the threshold duration include the value. In some versions of those implementations, providing the output is conditional on determining that further spoken utterances of a given user received within a threshold duration do not contain a value. .
加えて、一部の実装は、1つまたは複数のコンピューティングデバイスの1つまたは複数のプロセッサ(たとえば、中央演算処理装置(CPU))、グラフィックス処理ユニット(GPU)、および/またはテンソルプロセッシングユニット(TPU)を含み、1つまたは複数のプロセッサは、関連するメモリに記憶された命令を実行するように動作可能であり、命令は、上述の方法のいずれかの実行を引き起こすように構成される。一部の実装は、上述の方法のいずれかを実行するために1つまたは複数のプロセッサによって実行され得るコンピュータ命令を記憶する1つまたは複数の非一時的コンピュータ可読ストレージ媒体も含む。一部の実装は、上述の方法のいずれかを実行するために1つまたは複数のプロセッサによって実行され得る命令を含むコンピュータプログラム製品も含む。 In addition, some implementations utilize one or more processors (e.g., central processing units (CPUs)), graphics processing units (GPUs), and/or tensor processing units of one or more computing devices. (TPU), the one or more processors are operable to execute instructions stored in associated memory, the instructions configured to cause execution of any of the methods described above; . Some implementations also include one or more non-transitory computer-readable storage media that store computer instructions that may be executed by one or more processors to perform any of the methods described above. Some implementations also include a computer program product that includes instructions that can be executed by one or more processors to perform any of the methods described above.
上述の概念および本明細書においてより詳細に説明された追加の概念のすべての組合せは、本明細書において開示される対象の一部であると考えられることを理解されたい。たとえば、本開示の最後に現れる特許請求の対象のすべての組合せは、本明細書において開示される対象の一部であると考えられる。 It is to be understood that all combinations of the concepts described above and additional concepts described in more detail herein are considered to be part of the subject matter disclosed herein. For example, all combinations of claimed subject matter that appear at the end of this disclosure are considered to be part of the subject matter disclosed herein.
110 クライアントデバイス
111 ユーザ入力エンジン
112 デバイス状態エンジン
113 レンダリングエンジン
114 スケジューリングエンジン
115 自動アシスタント
120A1 音声認識エンジン
120A2 音声認識エンジン
130A1 NLUエンジン
130A2 NLUエンジン
140A1 音声合成エンジン
140A2 音声合成エンジン
150 支援された通話エンジン
151 エンティティ特定エンジン
151A エンティティデータベース
152 タスク決定エンジン
153 パラメータエンジン
153A パラメータデータベース
153B ユーザプロファイルデータベース
154 タスク実行エンジン
155 フィードバックエンジン
156 推薦エンジン
180 支援された通話システム
190 ネットワーク
200 方法
410 クライアントデバイス
411 URL
420 第1の検索結果
421 通話グラフィカル要素
422 道順グラフィカル要素
423 メニューグラフィカル要素
430 第2の検索結果
431 通話グラフィカル要素
432 道順グラフィカル要素
433 メニューグラフィカル要素
441B 編集グラフィカル要素
441C 終了通話グラフィカル要素
442B キャンセルグラフィカル要素
442C 通話参加グラフィカル要素
443B 通話インターフェース要素
443C スピーカインターフェース要素
452B1 プロンプト
452B2 プロンプト
452C1 合成音声
452C3 合成音声
452D1 合成音声
452D2 合成音声
454B1 ユーザ入力
454C3 合成音声
456C1 オーディオデータ
456C2 オーディオデータ
456D1 オーディオデータ
456D2 オーディオデータ
470 通話詳細インターフェース
471 第1のグラフィカル要素
471A 第1の下位要素
471B 第2の下位要素
471C 第3の下位要素
472 第2のグラフィカル要素
473 第3のグラフィカル要素
474 名前パラメータ
474A 値
475 電話番号パラメータ
475A 値
476 日付/時間パラメータ
476A 値
477 人数パラメータ
値 477A
478 座席の種類パラメータ
478A 値
479 通知
479A 第1の提案
479B 第2の提案
480 グラフィカルユーザインターフェース
481 システムインターフェース要素
482 システムインターフェース要素
483 システムインターフェース要素
484 テキスト応答インターフェース要素
485 音声応答インターフェース要素
486 通話詳細インターフェース要素
510 クライアントデバイス
542 グラフィカル要素
543 グラフィカル要素
552A1 オーディオデータ
552A2 オーディオデータ
552B1 オーディオデータ
552B2 オーディオデータ
552C1 オーディオデータ
552C2 オーディオデータ
554A1 オーディオデータ
554A2 オーディオデータ
554B1 オーディオデータ
554C1 オーディオデータ
556A1 合成音声
556B1 合成音声
556C1 合成音声
570 通話詳細インターフェース
579 通知
579A グラフィカル要素
580 グラフィカルユーザインターフェース
581 システムインターフェース要素
582 システムインターフェース要素
583 システムインターフェース要素
584 テキスト応答インターフェース要素
585 音声応答インターフェース要素
586 通話詳細インターフェース要素
610 コンピューティングデバイス
612 バスサブシステム
614 プロセッサ
616 ネットワークインターフェースサブシステム
620 ユーザインターフェース出力デバイス
622 ユーザインターフェース入力デバイス
624 ストレージサブシステム
625 メモリサブシステム
626 ファイルストレージサブシステム
630 主ランダムアクセスメモリ(RAM)
632 読み出し専用メモリ(ROM)
110 Client device
111 User Input Engine
112 Device state engine
113 Rendering Engine
114 Scheduling Engine
115 Automated Assistant
120A1 speech recognition engine
120A2 speech recognition engine
130A1 NLU Engine
130A2 NLU Engine
140A1 Speech synthesis engine
140A2 Speech synthesis engine
150 assisted calling engines
151 Entity identification engine
151A Entity Database
152 Task decision engine
153 Parameter Engine
153A Parameter database
153B User Profile Database
154 Task Execution Engine
155 Feedback Engine
156 Recommendation Engine
180 assisted calling system
190 Network
200 ways
410 client device
411 URL
420 1st search result
421 Call Graphical Elements
422 Directions Graphical Elements
423 Menu Graphical Elements
430 Second search result
431 Call Graphical Elements
432 Directions Graphical Elements
433 Menu Graphical Elements
441B Edit Graphical Element
441C End Call Graphical Element
442B Cancel Graphical Element
442C Call Participation Graphical Elements
443B Call Interface Element
443C Speaker Interface Element
452B1 Prompt
452B2 Prompt
452C1 Synthetic voice
452C3 Synthetic voice
452D1 Synthetic voice
452D2 Synthetic voice
454B1 User input
454C3 Synthetic voice
456C1 Audio data
456C2 Audio data
456D1 Audio data
456D2 Audio data
470 Call Details Interface
471 First graphical element
471A First subelement
471B Second subelement
471C Third subelement
472 Second graphical element
473 Third graphical element
474 name parameter
474A value
475 Telephone Number Parameter
475A value
476 Date/Time Parameter
476A value
477 Number of people parameter value 477A
478 Seat type parameters
478A value
479 Notification
479A First proposal
479B Second proposal
480 Graphical User Interface
481 System Interface Elements
482 System Interface Elements
483 System Interface Elements
484 Text Response Interface Element
485 Voice Response Interface Elements
486 Call Details Interface Elements
510 client device
542 Graphical Elements
543 Graphical Elements
552A1 audio data
552A2 audio data
552B1 Audio data
552B2 Audio data
552C1 audio data
552C2 audio data
554A1 Audio data
554A2 Audio data
554B1 Audio data
554C1 Audio data
556A1 Synthetic voice
556B1 Synthetic voice
556C1 Synthetic voice
570 Call Details Interface
579 notifications
579A Graphical Element
580 Graphical User Interface
581 System Interface Elements
582 System Interface Elements
583 System Interface Elements
584 Text Response Interface Element
585 Voice Response Interface Elements
586 Call Details Interface Elements
610 computing device
612 Bus Subsystem
614 processor
616 Network Interface Subsystem
620 User Interface Output Device
622 User Interface Input Device
624 Storage Subsystem
625 Memory Subsystem
626 File Storage Subsystem
630 Main random access memory (RAM)
632 Read-only memory (ROM)
Claims (28)
所与のユーザに関連するクライアントデバイスを介して前記所与のユーザから、支援された通話を開始するためのユーザ入力を受け取るステップと、
前記ユーザ入力に基づいて、
前記支援された通話中に前記所与のユーザの代理として関わり合うエンティティ、および
前記支援された通話中に前記所与のユーザの代理として実行されるタスク
を決定するステップと、
前記タスクおよび/または前記エンティティに関連付けて記憶された1つまたは複数の候補パラメータに関して、前記支援された通話中に前記タスクを実行する際に合成音声を自動的に生成するのに使用される1つまたは複数の対応する値を決定するステップと、
前記所与のユーザに関連する前記クライアントデバイスを使用して、前記支援された通話の実行を開始するステップと、
前記支援された通話の実行中に、前記エンティティに関連するさらなるユーザの発言をキャプチャする、前記支援された通話のオーディオデータを処理することに基づいて、さらなるパラメータに関連する情報が前記さらなるユーザによって要求されると判定するステップと、
前記さらなるパラメータに関連する前記情報が要求されるとの判定に応じて、
前記クライアントデバイスに、前記支援された通話外で、前記さらなるパラメータを特定し、前記情報に関するさらなるユーザ入力を要求する通知をレンダリングさせるステップと、
前記通知に応じる何らかのさらなる入力を受け取るのに先立って、前記支援された通話を継続するステップであって、前記候補パラメータの前記対応する値のうちの1つまたは複数に基づいている合成音声の1つまたは複数のインスタンスをレンダリングすることを含む、ステップと、
前記支援された通話を継続する間に、前記通知に応じ、前記さらなるパラメータの特定の値を特定するさらなるユーザ入力が閾値の継続時間内に受け取られるかどうかを判定するステップと、
前記さらなるユーザ入力が前記閾値の継続時間内に受け取られるとの判定に応じて、
前記支援された通話の一部として、前記特定の値に基づいているさらなる合成音声をレンダリングするステップと
を含む、方法。 A method implemented by one or more processors, the method comprising:
receiving user input to initiate an assisted call from a given user via a client device associated with the given user;
Based on said user input,
determining entities to engage on behalf of the given user during the assisted call, and tasks to be performed on behalf of the given user during the assisted call;
1 used to automatically generate synthesized speech when performing the task during the assisted call, with respect to one or more candidate parameters stored in association with the task and/or the entity; determining one or more corresponding values;
initiating execution of the assisted call using the client device associated with the given user;
Based on processing audio data of the assisted call, capturing utterances of a further user related to the entity during the performance of the assisted call, information relating to further parameters is provided by the further user. a step of determining that it is requested;
in response to determining that said information relating to said further parameter is requested;
causing the client device to render a notification outside the assisted call identifying the additional parameters and requesting further user input regarding the information;
continuing the assisted call prior to receiving any further input responsive to the notification, one of the synthesized voices being based on one or more of the corresponding values of the candidate parameters; rendering one or more instances;
While continuing the assisted call, in response to the notification, determining whether further user input identifying a particular value of the further parameter is received within a threshold duration;
in response to determining that the further user input is received within the threshold duration;
rendering, as part of the assisted call, additional synthesized speech based on the particular value.
前記支援された通話を開始する前に、
前記候補パラメータのうちの所与の候補パラメータを特定し、前記所与の候補パラメータに関連するさらなる情報を要求するプロンプトを生成することと、
前記クライアントデバイスに前記プロンプトをレンダリングさせることと、
前記プロンプトに応じるさらなるユーザ入力に基づいて、前記所与の候補パラメータの前記所与の値を特定することと
を含む、請求項1に記載の方法。 Determining a given value of the corresponding values comprises:
Before starting the assisted call,
identifying a given candidate parameter of the candidate parameters and generating a prompt requesting further information related to the given candidate parameter;
causing the client device to render the prompt;
and determining the given value of the given candidate parameter based on further user input in response to the prompt.
前記支援された通話を開始する前に、
前記所与のユーザに関連するユーザプロファイルに基づいて前記さらなる値を特定することを含む、請求項1または2に記載の方法。 determining a further one of said corresponding values;
Before starting the assisted call,
3. A method according to claim 1 or 2, comprising identifying the further value based on a user profile associated with the given user.
前記支援された通話のさらなるオーディオデータを処理して、前記さらなるユーザのさらなる発言が前記候補パラメータのうちの所与の候補パラメータの要求を含むと判定することと、
前記さらなる発言が前記所与の候補パラメータの前記要求を含むとの判定に応じて、
前記クライアントデバイスに、前記通話において、合成音声の前記1つまたは複数のインスタンスのうちの所与のインスタンスをレンダリングさせることであって、
前記所与のインスタンスが、前記所与の候補パラメータに関して所与の値が決定されることに基づいて、前記対応する値のうちの前記所与の値を含み、
前記所与のインスタンスが、前記所与のユーザからのいかなるさらなるユーザ入力も要求することなくレンダリングされる、レンダリングさせることと
を含む、請求項1から3のいずれか一項に記載の方法。 Continuing the assisted call comprises:
processing further audio data of the assisted call to determine that a further utterance of the further user includes a request for a given one of the candidate parameters;
in response to determining that the further utterance includes the request for the given candidate parameter;
causing the client device to render a given instance of the one or more instances of synthesized speech in the call;
the given instance includes the given value of the corresponding values based on which the given value is determined for the given candidate parameter;
4. A method according to any one of claims 1 to 3, comprising causing the given instance to be rendered, wherein the given instance is rendered without requesting any further user input from the given user.
前記さらなるユーザのさらなる発言がさらなる閾値の継続時間内に受け取られるかどうかを判定するために、さらなるオーディオデータを処理することと、
前記さらなる発言が前記さらなる閾値の継続時間内に前記さらなるユーザから受け取られないとの判定に応じて、
前記支援された通話中に、合成音声の前記1つまたは複数のインスタンスのうち、前記さらなるユーザによって要求されなかった前記対応する値のうちの1つまたは複数に基づいている別のインスタンスをレンダリングすることと
を含む、請求項1から4のいずれか一項に記載の方法。 Continuing the assisted call comprises:
processing the further audio data to determine whether further utterances of the further user are received within a further threshold duration;
in response to determining that the further utterance is not received from the further user within the further threshold duration;
rendering another instance of the one or more instances of synthesized speech during the assisted call that is based on one or more of the corresponding values that were not requested by the further user; 5. The method according to any one of claims 1 to 4, comprising:
前記エンティティに関連付けて記憶された前記1つまたは複数の候補パラメータを、前記さらなるパラメータを含むように更新するステップをさらに含む、請求項1から5のいずれか一項に記載の方法。 in response to determining that the information related to the further parameter is requested by the further user;
6. The method of any one of claims 1 to 5, further comprising updating the one or more candidate parameters stored in association with the entity to include the further parameter.
前記クライアントデバイスの前記状態に基づいて、前記通知および/または前記通知をレンダリングするための1つもしくは複数のプロパティを決定するステップと
をさらに含む、請求項1から6のいずれか一項に記載の方法。 determining a state of the client device when the information related to the further parameter is requested by the further user;
determining the notification and/or one or more properties for rendering the notification based on the state of the client device. Method.
前記所与のユーザが前記支援された通話を能動的に監視していることを示す前記クライアントデバイスの前記状態に基づいて、1つまたは複数の選択可能なグラフィカル要素と一緒に前記クライアントデバイスのディスプレイによって視覚的にレンダリングされる視覚的な構成要素を含むように前記通知を決定することを含み、
前記通知に応じる前記さらなるユーザ入力が、前記1つまたは複数の選択可能なグラフィカル要素のうちの所与の1つの選択を含む、請求項7に記載の方法。 the state of the client device indicates that the given user is actively monitoring the assisted call, and determining the notification based on the state of the client device;
a display of the client device with one or more selectable graphical elements based on the state of the client device indicating that the given user is actively monitoring the assisted call; determining the notification to include a visual component that is visually rendered by
8. The method of claim 7, wherein the further user input in response to the notification includes selection of a given one of the one or more selectable graphical elements.
前記所与のユーザが前記支援された通話を能動的に監視していないことを示す前記クライアントデバイスの前記状態に基づいて、前記クライアントデバイスの1つまたは複数のスピーカによって聴覚的にレンダリングされる聴覚的構成要素を含むように前記通知を決定することを含む、請求項7に記載の方法。 the state of the client device indicates that the given user is not actively monitoring the assisted call, and determining the notification based on the state of the client device;
an audible sound rendered acoustically by one or more speakers of the client device based on the state of the client device indicating that the given user is not actively monitoring the assisted call; 8. The method of claim 7, comprising determining the notification to include a physical component.
前記支援された通話を終了させた後、
前記クライアントデバイスに、前記支援された通話の結果のインジケーションを含むさらなる通知をレンダリングさせるステップと
をさらに含む、請求項1から9のいずれか一項に記載の方法。 terminating the assisted call;
After ending said assisted call,
10. The method of any one of claims 1 to 9, further comprising causing the client device to render a further notification comprising an indication of the outcome of the assisted call.
前記支援された通話の終了に応じて前記ユーザの代理として実行されるさらなるタスクのインジケーションを含むか、または
選択されると、前記クライアントデバイスに前記ユーザの代理としてさらなるタスクを実行させる1つもしくは複数の選択可能なグラフィカル要素を含む、請求項10に記載の方法。 the further notification including the indication of the assisted call;
an indication of further tasks to be performed on behalf of the user upon termination of the assisted call, or one or more that, when selected, cause the client device to perform further tasks on behalf of the user. 11. The method of claim 10, comprising a plurality of selectable graphical elements.
前記支援された通話を終了させるステップと、
前記支援された通話を終了させた後、
前記クライアントデバイスに、前記支援された通話の結果のインジケーションを含むさらなる通知をレンダリングさせるステップと
をさらに含む、請求項1から11のいずれか一項に記載の方法。 in response to determining that the further user input is not received within the threshold duration;
terminating the assisted call;
After ending said assisted call,
12. The method of any one of claims 1 to 11, further comprising causing the client device to render a further notification comprising an indication of the outcome of the assisted call.
前記さらなるパラメータを特定し、前記情報に関するさらなるユーザ入力を要求する前記通知がレンダリングされるときからの固定の継続時間、または
前記対応する値のうちの1つもしくは複数の最後の1つが前記クライアントデバイスを介して前記さらなるユーザに提示するためにレンダリングされるときに基づいている動的な持続時間である、請求項1から12のいずれか一項に記載の方法。 The duration of the threshold value is
a fixed duration from when the notification is rendered specifying the further parameters and requesting further user input regarding the information, or a last one of the one or more of the corresponding values on the client device; 13. A method according to any one of claims 1 to 12, wherein the method is a dynamic duration based on when rendered for presentation to the further user via.
前記エンティティに関連する前記さらなるユーザから、前記支援された通話を監視することへの同意を得るステップをさらに含む、請求項1から13のいずれか一項に記載の方法。 After initiating the assisted call,
14. The method of any one of claims 1 to 13, further comprising obtaining consent from the further user associated with the entity to monitor the assisted call.
クライアントデバイスにおいて、前記クライアントデバイスの所与のユーザとさらなるクライアントデバイスのさらなるユーザとの間の進行中の通話を検出するステップと、
前記進行中の通話中の少なくとも1つの発話された発言をキャプチャするオーディオデータのストリームを処理して、認識されたテキストを生成するステップであって、前記少なくとも1つの発話された発言が、前記所与のユーザまたは前記さらなるユーザのものである、ステップと、
前記認識されたテキストを処理することに基づいて、前記少なくとも1つの発話された発言がパラメータの情報を要求することを特定するステップと、
前記パラメータに関して、前記所与のユーザの個人的なアクセス制限されたデータを使用して、前記パラメータの値が解決可能であると判定するステップと、
前記値が解決可能であるとの判定に応じて、
前記進行中の通話中に、前記値に基づいている出力をレンダリングするステップと
を含む、方法。 A method implemented by one or more processors, the method comprising:
detecting, at a client device, an ongoing call between a given user of said client device and a further user of a further client device;
processing a stream of audio data capturing at least one spoken utterance during the ongoing call to generate recognized text, the at least one spoken utterance being at the location; of the given user or said further user;
determining, based on processing the recognized text, that the at least one spoken utterance requires parameter information;
determining that the value of the parameter is resolvable using personal restricted access data of the given user with respect to the parameter;
Depending on the determination that the value is resolvable,
and rendering an output based on the value during the ongoing call.
前記進行中の通話中に前記出力をレンダリングするステップが、さらに、前記パラメータの前記値の解決に応じる、請求項15に記載の方法。 further comprising resolving the value of the parameter;
16. The method of claim 15, wherein rendering the output during the ongoing call is further responsive to resolving the value of the parameter.
前記所与のユーザと前記さらなるユーザとの間の前記進行中の通話のメタデータを分析することと、
分析に基づいて、前記さらなるユーザに関連するエンティティを特定することと、
前記値が前記エンティティおよび前記パラメータに関連付けて記憶されていることに基づいて、前記値を解決することと
を含む、請求項16に記載の方法。 resolving the value of the parameter comprises:
analyzing metadata of the ongoing call between the given user and the further user;
identifying an entity associated with the further user based on the analysis;
resolving the value based on the value being stored in association with the entity and the parameter.
前記合成音声を前記進行中の通話の一部としてレンダリングすることを含む、請求項15から17のいずれか一項に記載の方法。 the output includes synthesized speech, and rendering the output based on the value during the ongoing call;
18. A method according to any one of claims 15 to 17, comprising rendering the synthesized speech as part of the ongoing call.
前記所与のユーザから、前記進行中の通話中に支援を作動させるためのユーザ入力を受け取るステップをさらに含み、
前記合成音声を前記進行中の通話の一部としてレンダリングすることが、さらに、前記支援を作動させるための前記ユーザ入力の受け取りに応じる、請求項18に記載の方法。 before rendering the synthesized speech as part of the ongoing call;
further comprising receiving user input from the given user to activate assistance during the ongoing call;
19. The method of claim 18, wherein rendering the synthesized speech as part of the ongoing call is further responsive to receiving the user input to activate the assistance.
前記通知をレンダリングした後に、前記通知に応じた肯定的なユーザ入力の受け取りに応じて、前記合成音声をレンダリングするステップをさらに含む、請求項20に記載の方法。 the output further comprises synthesized speech rendered as part of the ongoing call, the method further comprising:
21. The method of claim 20, further comprising, after rendering the notification, rendering the synthesized speech in response to receiving positive user input in response to the notification.
前記出力を提供することが、前記所与のユーザの、前記閾値の継続時間内に受け取られたさらなる発話された発言が前記値を含まないと判定することを条件とする、請求項15から21のいずれか一項に記載の方法。 the threshold duration of the given user based on processing the stream of audio data for a threshold duration after the at least one spoken utterance requesting information of the parameter; further comprising determining whether any further spoken utterances received within include the value;
21 . Claims 15 to 21 , wherein providing the output is conditional on determining that further spoken utterances of the given user received within the threshold duration do not include the value. The method described in any one of the above.
所与のユーザに関連するクライアントデバイスを介して前記所与のユーザから、支援された通話を開始するためのユーザ入力を受け取るステップと、
前記ユーザ入力に基づいて、
前記支援された通話中に前記所与のユーザの代理として関わり合うエンティティ、および
前記支援された通話中に前記所与のユーザの代理として実行されるタスク
を決定するステップと、
前記所与のユーザに関連する前記クライアントデバイスを使用して、前記支援された通話の実行を開始するステップであって、前記支援された通話の前記実行が、前記支援された通話中に、実行される前記タスクに基づいている合成音声および/またはシミュレーションされたボタンの押下を自動的に生成することを含む、ステップと、
前記支援された通話の前記実行中に、前記支援された通話のオーディオデータを処理することに基づいて、人間の参加者が前記エンティティに代わって参加することを監視するステップと、
前記監視中に、前記人間の参加者が前記エンティティに代わって参加したとの判定に応じて、
前記クライアントデバイスに、前記支援された通話外で、前記支援された通話に能動的に参加するように前記ユーザに要求する通知をレンダリングさせるステップと、
前記ユーザによる、前記通知を対象とする肯定的なユーザ入力に応じて、
前記所与のユーザを前記支援された通話に能動的な参加者として参加させるステップであって、前記クライアントデバイスの1つまたは複数のマイクロフォンによって検出された前記所与のユーザの発話された入力に対応するオーディオデータを、前記支援された通話の一部としてレンダリングすることを含む、ステップと
を含む、方法。 A method implemented by one or more processors, the method comprising:
receiving user input to initiate an assisted call from a given user via a client device associated with the given user;
Based on said user input,
determining entities to engage on behalf of the given user during the assisted call, and tasks to be performed on behalf of the given user during the assisted call;
initiating execution of the assisted call using the client device associated with the given user, the execution of the assisted call being performed during the assisted call; automatically generating synthesized speech and/or simulated button presses based on the task to be performed;
during the execution of the assisted call, monitoring participation of a human participant on behalf of the entity based on processing audio data of the assisted call;
in response to determining that the human participant has participated on behalf of the entity during the monitoring;
causing the client device to render a notification requesting the user to actively participate in the assisted call outside of the assisted call;
In response to positive user input by the user targeting the notification;
Participating the given user as an active participant in the assisted call, the step of causing the given user to participate in the assisted call as an active participant, the method comprising: rendering corresponding audio data as part of the assisted call.
前記監視が、前記支援された通話が保留にされたとの判定に応じて実行される、請求項23に記載の方法。 further comprising determining that the assisted call is placed on hold after generating the synthesized voice and/or the simulated button press;
24. The method of claim 23, wherein the monitoring is performed in response to determining that the assisted call has been placed on hold.
前記支援された通話の先行するオーディオデータを処理することと、
前記オーディオデータが前記通話が保留にされたことを示す1つまたは複数の手がかりを含むことに基づいて、前記支援された通話が保留にされたと判定することと
を含む、請求項24に記載の方法。 determining that the assisted call is placed on hold;
processing preceding audio data of the assisted call;
and determining that the assisted call has been placed on hold based on the audio data including one or more cues indicating that the call has been placed on hold. Method.
実行されると前記少なくとも1つのプロセッサに請求項1から27のいずれか一項に記載の方法を実行させる命令を記憶した少なくとも1つのメモリと
を含む、少なくとも1つのコンピューティングデバイス。 at least one processor;
at least one memory storing instructions which, when executed, cause the at least one processor to perform the method of any one of claims 1 to 27.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202062992609P | 2020-03-20 | 2020-03-20 | |
US62/992,609 | 2020-03-20 | ||
JP2022520917A JP7392128B2 (en) | 2020-03-20 | 2020-04-22 | Semi-delegated calls with automated assistants on behalf of human participants |
PCT/US2020/029346 WO2021188126A1 (en) | 2020-03-20 | 2020-04-22 | Semi-delegated calling by an automated assistant on behalf of human participant |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022520917A Division JP7392128B2 (en) | 2020-03-20 | 2020-04-22 | Semi-delegated calls with automated assistants on behalf of human participants |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2024020472A true JP2024020472A (en) | 2024-02-14 |
Family
ID=70680658
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022520917A Active JP7392128B2 (en) | 2020-03-20 | 2020-04-22 | Semi-delegated calls with automated assistants on behalf of human participants |
JP2023196592A Pending JP2024020472A (en) | 2020-03-20 | 2023-11-20 | Semi-delegated calls with automated assistants on behalf of human participants |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022520917A Active JP7392128B2 (en) | 2020-03-20 | 2020-04-22 | Semi-delegated calls with automated assistants on behalf of human participants |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220051664A1 (en) |
EP (1) | EP3909230A1 (en) |
JP (2) | JP7392128B2 (en) |
KR (1) | KR102631797B1 (en) |
CN (1) | CN114631300A (en) |
WO (1) | WO2021188126A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN110719362A (en) * | 2019-09-10 | 2020-01-21 | 北京百度网讯科技有限公司 | Call processing method and device, electronic equipment and storage medium |
Family Cites Families (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2011000046A1 (en) * | 2009-07-01 | 2011-01-06 | Ozmota Inc. | Systems and methods for determining information and knowledge relevancy, relevant knowledge discovery and interactions, and knowledge creation |
US10134395B2 (en) * | 2013-09-25 | 2018-11-20 | Amazon Technologies, Inc. | In-call virtual assistants |
JP6736691B2 (en) | 2016-06-13 | 2020-08-05 | グーグル エルエルシー | Escalation to a human operator |
US10567579B2 (en) * | 2016-08-24 | 2020-02-18 | Vonage Business Inc. | Systems and methods for providing integrated computerized personal assistant services in telephony communications |
US10318096B2 (en) * | 2016-09-16 | 2019-06-11 | Microsoft Technology Licensing, Llc | Intelligent productivity monitoring with a digital assistant |
JP2018101847A (en) | 2016-12-19 | 2018-06-28 | シャープ株式会社 | Electronic device, control method of the same, control program, and recording medium |
US9865260B1 (en) * | 2017-05-03 | 2018-01-09 | Google Llc | Proactive incorporation of unsolicited content into human-to-computer dialogs |
US10498904B1 (en) * | 2017-11-22 | 2019-12-03 | Repnow, Inc. | Automated telephone host system interaction |
WO2019194925A1 (en) * | 2018-04-02 | 2019-10-10 | SecretLab, LLC | Reservation buying/selling systems and methods |
KR20200024511A (en) * | 2018-08-28 | 2020-03-09 | 삼성전자주식회사 | Operation method of dialog agent and apparatus thereof |
US10425533B1 (en) * | 2019-01-16 | 2019-09-24 | Capital One Services, Llc | Interacting with an interactive voice response system device or agent device of an organization |
US10944864B2 (en) * | 2019-03-26 | 2021-03-09 | Ribbon Communications Operating Company, Inc. | Methods and apparatus for identification and optimization of artificial intelligence calls |
US11134034B2 (en) * | 2019-09-09 | 2021-09-28 | Disney Enterprises, Inc. | Systems, methods, and storage media configured to integrate artificial intelligence chatbots into a communication between real-world users |
US11005988B1 (en) * | 2019-12-27 | 2021-05-11 | Qualcomm Incorporated | Smart notification system for voice calls |
-
2020
- 2020-04-22 CN CN202080073684.1A patent/CN114631300A/en active Pending
- 2020-04-22 JP JP2022520917A patent/JP7392128B2/en active Active
- 2020-04-22 US US16/973,597 patent/US20220051664A1/en active Pending
- 2020-04-22 EP EP20725347.7A patent/EP3909230A1/en active Pending
- 2020-04-22 KR KR1020227021946A patent/KR102631797B1/en active IP Right Grant
- 2020-04-22 WO PCT/US2020/029346 patent/WO2021188126A1/en unknown
-
2023
- 2023-11-20 JP JP2023196592A patent/JP2024020472A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
JP2023501059A (en) | 2023-01-18 |
JP7392128B2 (en) | 2023-12-05 |
US20220051664A1 (en) | 2022-02-17 |
KR102631797B1 (en) | 2024-02-01 |
WO2021188126A1 (en) | 2021-09-23 |
EP3909230A1 (en) | 2021-11-17 |
CN114631300A (en) | 2022-06-14 |
KR20220106183A (en) | 2022-07-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6690063B2 (en) | Conditional provision of access via an interactive assistant module | |
KR102313474B1 (en) | System, method and apparatus for resuming conversation session via automated assistant | |
US11355117B2 (en) | Dialog system with automatic reactivation of speech acquiring mode | |
KR102475719B1 (en) | Generating and transmitting invocation request to appropriate third-party agent | |
JP6827479B2 (en) | Non-deterministic task initiation with personal assistant module | |
US11436417B2 (en) | Providing access to user-controlled resources by automated assistants | |
KR102624148B1 (en) | Automatic navigation of interactive voice response (IVR) trees on behalf of human users | |
KR102414159B1 (en) | Methods and apparatus for managing holds | |
JP2024020472A (en) | Semi-delegated calls with automated assistants on behalf of human participants | |
JP2022539674A (en) | Speaker Recognition Using Speaker-Specific Speech Models | |
JP2024510698A (en) | Contextual suppression of assistant commands | |
US20220277745A1 (en) | Dialog system with automatic reactivation of speech acquiring mode | |
KR20240011841A (en) | Provide relevant queries to secondary automated assistants based on past interactions | |
JP2024505787A (en) | Interaction data transfer from the first called automatic assistant to the later called automatic assistant | |
CN117121100A (en) | Enabling natural conversations with soft endpoints for automated assistants |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20231124 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20231124 |