JP2023534889A - Conditional camera control with auto-assistant commands - Google Patents
Conditional camera control with auto-assistant commands Download PDFInfo
- Publication number
- JP2023534889A JP2023534889A JP2022571847A JP2022571847A JP2023534889A JP 2023534889 A JP2023534889 A JP 2023534889A JP 2022571847 A JP2022571847 A JP 2022571847A JP 2022571847 A JP2022571847 A JP 2022571847A JP 2023534889 A JP2023534889 A JP 2023534889A
- Authority
- JP
- Japan
- Prior art keywords
- camera
- conditions
- computing device
- automated assistant
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000007613 environmental effect Effects 0.000 claims abstract description 5
- 238000000034 method Methods 0.000 claims description 79
- 238000012545 processing Methods 0.000 claims description 60
- 238000010801 machine learning Methods 0.000 claims description 36
- 230000004044 response Effects 0.000 claims description 19
- 230000008569 process Effects 0.000 claims description 17
- 230000009471 action Effects 0.000 claims description 16
- 238000012549 training Methods 0.000 claims description 6
- 230000000007 visual effect Effects 0.000 claims description 6
- 238000013481 data capture Methods 0.000 claims description 2
- 238000010586 diagram Methods 0.000 description 9
- 238000005070 sampling Methods 0.000 description 6
- 239000000463 material Substances 0.000 description 5
- 241000593989 Scardinius erythrophthalmus Species 0.000 description 4
- 239000003795 chemical substances by application Substances 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 230000004313 glare Effects 0.000 description 4
- 201000005111 ocular hyperemia Diseases 0.000 description 4
- 238000004891 communication Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 230000002093 peripheral effect Effects 0.000 description 3
- 238000009877 rendering Methods 0.000 description 3
- 241000894007 species Species 0.000 description 3
- 230000003068 static effect Effects 0.000 description 3
- 230000001629 suppression Effects 0.000 description 3
- 238000013475 authorization Methods 0.000 description 2
- 230000001143 conditioned effect Effects 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 230000001747 exhibiting effect Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 238000010200 validation analysis Methods 0.000 description 2
- 230000001755 vocal effect Effects 0.000 description 2
- 239000002699 waste material Substances 0.000 description 2
- 241001465754 Metazoa Species 0.000 description 1
- 241000699670 Mus sp. Species 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000002457 bidirectional effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000007405 data analysis Methods 0.000 description 1
- 230000003111 delayed effect Effects 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 230000001815 facial effect Effects 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 230000009191 jumping Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 230000001052 transient effect Effects 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/64—Computer-aided capture of images, e.g. transfer from script file into camera, check of taken image quality, advice or proposal for image composition or decision on when to take image
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/61—Control of cameras or camera modules based on recognised objects
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/62—Control of parameters via user interfaces
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/66—Remote control of cameras or camera parts, e.g. by remote control devices
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/80—Camera processing pipelines; Components thereof
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/95—Computational photography systems, e.g. light-field imaging systems
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/91—Television signal processing therefor
- H04N5/92—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback
- H04N5/9201—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback involving the multiplexing of an additional signal and the video signal
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Abstract
本明細書に示す実装形態は、ユーザによって明らかにされた1つまたは複数の条件に従ってカメラを制御することができる自動アシスタントに関わるものである。例えば、特定の環境特徴が明らかであることを自動アシスタントが検出すると、ある条件を満たすことができる。このように、カメラの覗き窓を絶えずモニタすることをユーザに必ずしも求めずともある一定の瞬間を同定し、捉えるのに、ユーザは自動アシスタントを当てにすることができる。実装形態によっては、自動アシスタントがメディアデータを取り込む際の条件は、自動アシスタントに対応しているアプリケーションデータおよび/またはそれ以外のコンテキストデータに基づくものであってもよい。例えば、カメラ覗き窓における内容とアプリケーションインターフェースのそれ以外の内容との関係が、自動アシスタントがカメラを使用してある一定のメディアデータを取り込む条件になり得る。The implementations presented herein involve an automated assistant that can control a camera according to one or more conditions specified by a user. For example, certain conditions may be met when the automated assistant detects that certain environmental features are evident. In this way, the user can rely on the automated assistant to identify and capture certain moments without necessarily requiring the user to constantly monitor the camera's viewing window. In some implementations, the conditions under which the automated assistant captures media data may be based on application data and/or other contextual data that the automated assistant supports. For example, the relationship between the content in the camera viewport and the rest of the application interface may condition the automated assistant to capture certain media data using the camera.
Description
人は、本明細書では、「自動アシスタント」としている(また、「デジタルエージェント」、「チャットボット」、「対話型パーソナルアシスタント」、「インテリジェントパーソナルアシスタント」、「会話型エージェント」などともしている)対話型ソフトウェアアプリケーションにより人対コンピュータ会話に参加することができる。例えば、人(彼らが自動アシスタントと相互作用する場合、「ユーザ」としていることもある)は、場合によっては、テキストに変換されてから処理されてもよい口頭自然言語入力(すなわち、発話)を使用して、および/またはテキスト(例えば、型付き)自然言語入力を与えることによって、コマンドおよび/またはリクエストを与えることができる。 A person is herein referred to as an "automated assistant" (also referred to as a "digital agent", "chatbot", "conversational personal assistant", "intelligent personal assistant", "conversational agent", etc.). ) Interactive software applications allow you to participate in human-to-computer conversations. For example, a person (sometimes referred to as a "user" when they interact with an automated assistant) provides verbal natural language input (i.e., speech) that may optionally be converted to text and then processed. Commands and/or requests can be given using and/or by providing textual (eg, typed) natural language input.
場合によっては、自動アシスタントが、スクリーンショット、写真、またはそれ以外の類の画像などの画像を取り込むようユーザがリクエストすることができる。しかし、自動アシスタントが画像を取り込む時点は、ユーザしか制御することができないので、ある一定の画像を取り込もうとするとユーザの自律性が限られる可能性がある。ユーザが無二の瞬間の映像または画像を取り込みたいと思う場合、ユーザは、カメラが画像を取り込むか、または無二の瞬間を含んでもよい広い時間帯を捉えるために映像の記録を初期化するようにタイマーをセットすることができる。しかし、このような方策は、無二の瞬間を捉えるために様々な計算リソースを使い尽くす結果をもたらす可能性があり、実際にはそれほど多くのデータを取り込まなくて済む場合がある。例えば、映像において赤ちゃんの笑顔を捉えるには、2、3秒の映像データだけで済むことがあるが、なおユーザは、この比較的無二の瞬間を捉えるために様々な長さの複数の映像を記録することができる。 In some cases, a user may request that an automated assistant capture an image, such as a screenshot, photo, or other type of image. However, since only the user can control when the automated assistant captures images, the user's autonomy may be limited when trying to capture certain images. When a user wishes to capture a video or image of a unique moment, the user initiates a camera capture of the image or recording of video to capture a wide time period that may include the unique moment. You can set a timer like However, such measures may result in exhausting various computational resources in order to capture the unique moment, and may not actually capture that much data. For example, capturing a baby's smile in a video may only require a few seconds of video data, yet the user may need multiple videos of varying lengths to capture this relatively unique moment. can be recorded.
本明細書に示す実装形態の中には、自動アシスタントが画像データを取り込むようにカメラを制御するために満たされるべき1つまたは複数の条件をユーザに発話を通して指定させる自動アシスタントに関わるものがある。条件は、時間内のある場合におよび/またはある期間にわたって、見ておよび/または聞いて、また対応するセンサデータの処理を通して注意を向けることのできる環境の特徴とすることができるが、これに限るわけではない。このようなまたは他のやり方では、一回の発話を使用して、無二の瞬間を捉えるようデバイスのカメラを制御することができ、ユーザが直にデバイスを扱う必要がないようにすることができる。さらに、これにより、ユーザがカメラに画像データを取り込むよう明確に指示し、および/またはデバイスのインターフェースにタッチ入力を与えて、画像データが取り込まれるようにする必要をなくすべき時を判断するのに、ユーザがカメラの電子ビューファインダをモニタする必要をなくすることができる。さらに、これにより、ユーザが、画像データの取り込みをはっきりと指示するタイミングを判断するのにカメラの電子ビューファインダをモニタする必要をなくし、および/または画像データが取り込まれるようにするために、ユーザがデバイスのインターフェースでタッチ入力を行う必要をなくすることができる。したがって、実装形態では、環境条件が満たされたのに応じてカメラを制御する際の一回の発話など、カメラを制御する際のユーザ入力を簡略化することができる。このような実装形態は、器用さが限られたユーザにとっては特に有益であり得る。 Some of the implementations presented herein involve automated assistants that allow the user to specify, through speech, one or more conditions that must be met in order for the automated assistant to control the camera to capture image data. . Conditions, which can be features of the environment that can be seen and/or heard and attended to through processing of corresponding sensor data, at certain occasions in time and/or over a period of time, include: It is not limited. In this or other manner, a single utterance can be used to control a device's camera to capture a unique moment, avoiding the need for the user to directly handle the device. can. In addition, this helps determine when the user should explicitly instruct the camera to capture image data and/or provide touch input to the interface of the device to eliminate the need for image data to be captured. , eliminating the need for the user to monitor the camera's electronic viewfinder. In addition, this eliminates the need for the user to monitor the camera's electronic viewfinder to determine when to explicitly indicate the capture of image data and/or allows the user to wait until the image data is captured. eliminates the need for touch input on the device's interface. Thus, implementations can simplify user input in controlling the camera, such as a single utterance in controlling the camera in response to environmental conditions being met. Such implementations may be particularly beneficial to users with limited dexterity.
さらにまた、実装形態では、カメラを含み、また画像データを取り込むのに活かされるクライアントデバイスのコンピュータメモリ、ディスク空き容量、および/またはバッテリ寿命を保つことができる。例えば、実装形態では、ユーザが自分のカメラにより特定の無二の瞬間を捉えようとする際に起こされ、保存される誤った画像データの取り込みを防ぐことができる。したがって、誤った画像データがメモリおよび/またはディスク空き容量を占めることはなくなり、バッテリ寿命は、このような画像データを処理するのに不必要に使い尽くされることがなくなる。別の例として、いくつかの実施形態においては、条件が満たされたのに応じてカメラを制御する際のある一定のまたはすべての発話に応じて電子ビューファインダのレンダリングを抑制し、それにより、そうでなければこのようなレンダリングによって使い尽くされると思われるバッテリリソースを節約することができる。 Furthermore, implementations can conserve computer memory, disk space, and/or battery life of client devices that include cameras and are utilized to capture image data. For example, implementations can prevent the capture of erroneous image data that is awakened and saved when a user attempts to capture a particular unique moment with their camera. Thus, erroneous image data will not occupy memory and/or disk space and battery life will not be unnecessarily exhausted processing such image data. As another example, some embodiments suppress rendering of the electronic viewfinder in response to certain or all utterances in controlling the camera in response to a condition being met, thereby: Battery resources that would otherwise be exhausted by such rendering can be conserved.
ある例として、またいくつかの実装形態によれば、ユーザが、「アシスタント、私の犬がおもちゃを取ってくる映像を撮って」などの発話をすることにより、ペットがいたずらをする映像を自動アシスタントが撮り込むようリクエストすることができる。例えば、ユーザがペットの方向に向いている、カメラを含むポータブルコンピューティングデバイスにおいて発話を受け取ることができる。ポータブルコンピューティングデバイスは、発話に応じて、発話において具体化されたリクエストを同定するために、発話の自然言語内容を処理することができる自動アシスタントにアクセスすることができる。例えば、自動アシスタントは、様々な条件が満たされると、自動アシスタントが、ある一定の画像データを取り込むようカメラを制御することをユーザがリクエストしていると判断することができる。それにより、自動アシスタントは、発話に基づき、カメラによって取り込まれた画像データにおいて犬およびおもちゃに注意を向ける際に満たされる第1の条件を同定することができる。自動アシスタントは、犬がおもちゃを取り出し始めたらしいおよび/またはおもちゃを取り出す過程にあると判断されると満たされる第2の条件も同定することができる。 As an example, and according to some implementations, a video of a pet playing pranks is automatically generated by a user uttering an utterance such as "Assistant, take a video of my dog fetching a toy." You can request that an assistant take the picture. For example, speech can be received at a portable computing device that includes a camera, where the user is facing the pet. In response to the utterance, the portable computing device can access an automated assistant that can process the natural language content of the utterance to identify requests embodied in the utterance. For example, the automated assistant may determine that the user has requested that the automated assistant control the camera to capture certain image data when various conditions are met. Based on the utterance, the automated assistant can thereby identify the first condition that is met in bringing attention to the dog and toy in the image data captured by the camera. The automated assistant may also identify a second condition that is met once it is determined that the dog has likely started and/or is in the process of removing the toy.
いくつかの実施形態においては、自動アシスタントは、条件が満たされているかどうかを判断するようにデータを処理する際に使用され得る1つまたは複数の訓練済み機械学習モデルを同定することができる。例えば、カメラを使用して起こされた画像データを処理するのを助けるためのリクエストに応じて、自動アシスタントが、動物を同定する際に使用され得る特定の訓練済み機械学習モデルを選択することができる。その代わりにまたはさらに、特定の働き(例えば、犬がおもちゃを取ってくる)がなされたことを画像データが示すかどうかを判断するのを助けるのに別の訓練済み機械学習モデルを選択してもよい。その代わりにまたはさらに、自動アシスタントは、条件が満たされているかどうかを判断するのに使用され得るオーディオデータ、画像データ、アプリケーションデータ、および/またはそれ以外の如何なる適したデータでも含むことができる他のデータを処理するのに使用され得る1つまたは複数の訓練済み機械学習モデルを当てにすることができる。 In some embodiments, the automated assistant can identify one or more trained machine learning models that can be used in processing the data to determine whether the conditions are met. For example, an automated assistant can select a particular trained machine learning model that can be used in identifying animals upon request to help process image data generated using a camera. can. Alternatively or additionally, another trained machine learning model is selected to help determine whether the image data indicates that a particular task (e.g., a dog fetching a toy) was performed. good too. Alternatively or additionally, the automated assistant can include audio data, image data, application data, and/or any other suitable data that can be used to determine whether a condition has been met. can be relied on one or more trained machine learning models that can be used to process the data of
いくつかの実施形態においては、一定の条件が発生することと、それらの一定の条件が認識されることとの間の待ち時間を短縮するために、1つまたは複数の訓練済み機械学習モデルがクライアントデバイスにローカルに格納され得る。さらにまた、これによって、さらなる処理に別個のデバイスにオフロードされると考えられ得るデータ量を減らすことにより、ネットワークトラフィックを減らすことができる。いくつかの実施形態においては、特定の訓練済み機械学習モデルがクライアントデバイスにローカルに格納されていなければ、この特定の訓練済み機械学習モデルがリモートサーバなどのデバイスからダウンロードされ得る。場合によっては、ユーザからのリクエストに自動アシスタントが対処できるが、ネットワークアクティビティ(例えば、特定の訓練済み機械学習モデルをダウンロードすること)の結果として遅れが出るという表示を自動アシスタントが与えることができる。 In some embodiments, one or more trained machine learning models are used to reduce the latency between the occurrence of certain conditions and the recognition of those certain conditions. It may be stored locally on the client device. Furthermore, this can reduce network traffic by reducing the amount of data that can be considered offloaded to a separate device for further processing. In some embodiments, if a particular trained machine learning model is not stored locally on the client device, the particular trained machine learning model may be downloaded from a device such as a remote server. In some cases, the automated assistant can provide an indication that the request from the user can be addressed by the automated assistant, but is delayed as a result of network activity (e.g., downloading a particular trained machine learning model).
自動アシスタントがユーザからのリクエストを受信すると、自動アシスタントは、第1の条件および第2の条件が満たされているかどうかを判断するのを始めることができる。いくつかの実施形態においては、これは、カメラからの画像データおよび/またはコンピューティングデバイスに一体化しているマイクからのオーディオデータを処理することを伴う場合がある。例えば、犬とおもちゃとを含む画像が第1の条件を満たす場合があり、ユーザが犬から遠くにおもちゃを投げるのが分かる映像データの一部が第2の条件を満たす場合がある。いくつかの実施形態においては、ある一定の条件が満たされているかどうかを判断するのに使用されるデータは、一時的にメモリ(例えば、RAM)に格納されてもよいが、条件が満たされると取り込まれるメディアデータは、ユーザからのリクエストに従って格納されてもよい。このように、ユーザは、条件が満たされる前、また満たされた後にかなりの量の画像データを手作業で削除するのにカメラアプリケーションを離れなくても済む。むしろ、ユーザが自動アシスタントに起こすようリクエストしたファイルとして格納される(例えば、ハードディスク空き容量に)ことになるメディアデータを判断するのに、ユーザは、自動アシスタントを当てにすることができる。 Once the automated assistant receives the request from the user, the automated assistant can begin determining whether the first and second conditions are met. In some embodiments, this may involve processing image data from a camera and/or audio data from a microphone integrated into the computing device. For example, an image containing a dog and a toy may satisfy the first condition, and a portion of the video data in which the user throws the toy away from the dog may meet the second condition. In some embodiments, data used to determine whether certain conditions are met may be temporarily stored in memory (e.g., RAM), but only if the conditions are met. Media data captured with may be stored according to a request from a user. In this way, the user does not have to leave the camera application to manually delete significant amounts of image data before and after conditions are met. Rather, the user can rely on the automated assistant to determine which media data will be stored (eg, in hard disk space) as the file the user requested the automated assistant to wake up.
条件が満たされていると自動アシスタントが判断すると、自動アシスタントは、カメラに、この条件が満たされたときから始まるメディアデータを記録させることができる。いくつかの実施形態においては、ユーザからのリクエストが画像1枚だけを取り込むリクエストに相当する場合、自動アシスタントが、カメラに、1つまたは複数の条件が満たされている場合の1つまたは複数の画像を取り込ませることができる。その代わりにまたはさらに、ユーザからのリクエストが映像を取り込むリクエストに相当する場合、自動アシスタントが、カメラに、1つまたは複数の条件が満たされると始まり、1つまたは複数の条件がもう満たされなくなると終わる映像を取り込ませることができる。例えば、上に述べた例では、犬がおもちゃを取ってくるのを終え、それ故、第2の条件がもう満たされなくなったと自動アシスタントが判断することができる。この判断に基づき、自動アシスタントは、カメラに映像を記録することを止めさせ、取り込み映像を格納させることができる。 Once the automated assistant determines that a condition is met, the automated assistant can cause the camera to record media data beginning when this condition is met. In some embodiments, if the request from the user corresponds to a request to capture only one image, the automated assistant will prompt the camera to select one or more images if one or more conditions are met. Images can be captured. Alternatively or additionally, if the request from the user corresponds to a request to capture footage, the automated assistant will tell the camera to start when one or more conditions are met and one or more conditions are no longer met. You can capture the video that ends with . For example, in the example described above, the automated assistant may determine that the dog has finished fetching the toy and therefore the second condition is no longer met. Based on this determination, the automated assistant can cause the camera to stop recording video and store the captured video.
いくつかの実施形態においては、ユーザが、別の発話などの明確なリクエストを自動アシスタントに必ずしも与えずとも自分の犬と「取ってくる」遊びを続けることによって、別の映像を記録するのを自動アシスタントに初期化させることができる。例えば、自動アシスタントがカメラに犬がおもちゃを取ってくる第1の映像を取り込ませるのに続いて、自動アシスタントは、第1の条件および第2の条件が満たされたかどうかを判断するのを続けることができる。ユーザがおもちゃをまた取ってくるよう犬に指示したと自動アシスタントが判断すると、自動アシスタントは、第1の条件および第2の条件がまた満たされたと判断し、それにより、カメラに第2の映像を取り込ませることができる。このように、ユーザに自分のポータブルコンピューティングデバイスを直に扱うよう必ずしも求めずとも、ユーザは、カメラを介して無二の瞬間を捉えるのに自動アシスタントを当てにすることができる。さらにまた、これにより、そうでなければ、ユーザが、記録済み映像の持続時間全体よりもかなり短い瞬間を捉えることを目的としている長い映像を自分のカメラに記録させる場合に使い尽くされる可能性がある計算リソースの無駄を減らすことができる。 In some embodiments, the user is encouraged to record another video by continuing a "fetch" play with their dog without necessarily giving the automated assistant an explicit request, such as another utterance. It can be initialized by an automated assistant. For example, following the automated assistant having the camera capture the first footage of the dog fetching a toy, the automated assistant continues to determine whether the first and second conditions are met. be able to. When the automated assistant determines that the user instructed the dog to fetch the toy again, the automated assistant determines that the first and second conditions are also met, thereby causing the camera to display a second image. can be incorporated. In this way, without necessarily requiring users to handle their portable computing devices directly, users can rely on automated assistants to capture unique moments via cameras. Furthermore, this can otherwise lead to exhaustion if the user has his camera record a long video that is intended to capture a moment much shorter than the entire duration of the recorded video. Waste of certain computing resources can be reduced.
いくつかの実施形態においては、ユーザは、自動アシスタントがカメラに画像データを取り込ませる前、その間、および/またはその後に1つまたは複数のカメラ設定および/または画像特性を修正するよう自動アシスタントに指示することができる。例えば、ユーザが人の群れにカメラを向けている場合があり、またユーザが自撮りを取り込むためにその人の群れに加わった場合、ユーザは、カメラを制御しているコンピューティングデバイスにおいてカメラプレビューを見ることができる(例えば、レンダリングされた電子ビューファインダを通してカメラプレビューを見る)。ユーザがその人の群れと一緒にいて、その人の群れがユーザの顔から反射する眩しさを嫌っているとユーザが判断することができるのがこのカメラプレビューから分かる。この判断に基づき、ユーザは、「アシスタント、私の顔から反射している眩しさを和らげて」などの発話をすることができる。この発話に応じて、自動アシスタントが、カメラプレビューにおける眩しさを和らげるようにカメラ設定および/または画像特性を調整することができる。それにより、ユーザは、カメラプレビューが適切な画像を提供していると認めることができ、「アシスタント、よくやった。皆の目が開いているときに写真を撮って。」などの発話をすることができる。これに応じて、自動アシスタントは、カメラおよび/または画像設定が維持されるようにし(例えば、眩しさを和らげる際)、それにより、画像の全員の目が開いていると自動アシスタントが判断したときに画像をカメラに取り込ませることができる。その代わりに、自動アシスタントにより取り込まれる画像を修正するようにカメラなどの画像アプリケーションの設定を自動アシスタントに調整させるために、ユーザが、「アシスタント、背景を明るくして」などの別の発話をすることによって、カメラ設定および/または画像設定にさらに調整を行うことができる。 In some embodiments, the user directs the automated assistant to modify one or more camera settings and/or image characteristics before, during, and/or after the automated assistant causes the camera to capture image data. can do. For example, a user may point a camera at a crowd of people, and if the user joins the crowd to capture a selfie, the user can view the camera preview on the computing device controlling the camera. can be viewed (e.g. viewing a camera preview through a rendered electronic viewfinder). It can be seen from this camera preview that the user can determine that the user is with the crowd of people and that the crowd of people dislikes the glare reflected off the user's face. Based on this determination, the user can say something like, "Assistant, soften the glare that's reflecting off my face." In response to this utterance, the automated assistant can adjust camera settings and/or image characteristics to reduce glare in camera previews. This allows the user to acknowledge that the camera preview is providing a suitable image, and to say something like "Well done Assistant. Take a picture when everyone's eyes are open." be able to. In response, the automated assistant ensures that camera and/or image settings are maintained (e.g., when reducing glare) so that when the automated assistant determines that everyone's eyes in the image are open. can capture images into the camera. Instead, the user says another utterance, such as "Assistant, lighten the background," to have the automated assistant adjust the settings of an image application, such as the camera, to modify images captured by the automated assistant. This allows further adjustments to be made to camera settings and/or image settings.
いくつかの実施形態においては、ユーザが、手作業で画像を取り込む(例えば、スナップ写真を撮るのにGUI要素を選択することによって)と決めてもよいが、ある一定の調整を求めるリスエストを自動アシスタントに与えてもよい。例えば、ユーザは、「アシスタント、画像を白黒にして」などの発話をしてもよい。これに応じて、自動アシスタントは、カメラビューファインダに、ユーザが自分のカメラを向けている環境の白黒プレビューをレンダリングさせることができる。ユーザがこのプレビューに満足すると、ユーザは、GUI要素をタップして、画像を取り込むことができる。このように、ユーザは、カメラのある一定の設定を調整するためにカメラアプリケーションインターフェース間を行き来しなくとも済む。むしろ、ユーザは、ある一定の画像および/またはカメラ調整を行うと同時にカメラを特定の環境に向けるのに、自動アシスタントとの音声対話を当てにすることができる。 In some embodiments, a user may decide to manually capture an image (e.g., by selecting a GUI element to take a snapshot), but automatically request for certain adjustments. You can give it to your assistant. For example, the user may say something like "Assistant, make image black and white." In response, the automated assistant can cause the camera viewfinder to render a black-and-white preview of the environment the user is pointing his camera at. Once the user is satisfied with this preview, the user can tap the GUI element to capture the image. In this way, the user does not have to navigate back and forth between camera application interfaces to adjust certain settings of the camera. Rather, the user can rely on voice interaction with the automated assistant to point the camera at a particular environment while making certain image and/or camera adjustments.
いくつかの実施形態においては、ユーザは、調整を行うのに自動アシスタントを採用するコマンドを与えることができ、また、さらなる調整を行うのを止める前に調整が適切であるかどうかを判断することもできる。例えば、ユーザは、「アシスタント、画像の赤目を直して」などのコマンドを与えることができ、これに応じて、自動アシスタントは、「赤目」を含む画像プレビューのある一定の部分を同定し(例えば、境界ボックスにより)、赤目をなくすように色調整を行うことができる。赤目がなくされたと判断すると、ユーザおよび/または自動アシスタントは、画像が取り込まれるようにすることができる。 In some embodiments, the user can command the automated assistant to make the adjustments and determine if the adjustments are appropriate before stopping to make further adjustments. can also For example, a user can give a command such as "Assistant, fix red eye in image", and in response the automated assistant will identify certain parts of the image preview that contain "red eye" (e.g. , by the bounding box), color adjustments can be made to eliminate red-eye. Upon determining that red-eye has been eliminated, the user and/or automated assistant may cause an image to be captured.
上の概要は、本開示のいくつかの実装形態の全体像として提供している。これらの実装形態、またそれ以外の実装形態のさらなる概要は、以下により詳しく述べる。 The above summary is provided as an overview of some implementations of the disclosure. A further overview of these and other implementations is provided in more detail below.
それ以外の実装形態は、上に記載の方法および/または本明細書の他の個所に記載の方法のうちの1つまたは複数などの方法を行うように、1つまたは複数のプロセッサ(例えば、中央処理装置(CPU:Central Processing Device)、グラフィックス処理ユニット(GPU:Graphics Processing unit)、および/またはテンソル処理ユニット(TPU:Tensor Processing Unit))により実行可能な命令を格納する非一時的コンピュータ可読記憶媒体を含むことができる。また別の実装形態は、上に記載の方法および/または本明細書の他の個所に記載の方法のうちの1つまたは複数などの方法を行うように、格納された命令を実行するのに使用可能な1つまたは複数のプロセッサを含む1つまたは複数のコンピュータのシステムを含むことができる。 Other implementations comprise one or more processors (e.g., non-transient computer-readable storing instructions executable by a central processing device (CPU), graphics processing unit (GPU), and/or tensor processing unit (TPU) A storage medium can be included. Yet another implementation includes instructions for executing stored instructions to perform methods such as one or more of the methods described above and/or methods described elsewhere herein. It can include one or more computer systems with one or more processors enabled.
上述の概念と本明細書の以下でより詳細に説明されるさらなる概念とのあらゆる組み合わせは、本明細書に開示の主題の一部であることを理解されたい。例えば、本開示の末尾にある請求項に記載の主題のあらゆる組み合わせは、本明細書に開示される主題の一部であると考えられる。 It should be understood that any combination of the above concepts with further concepts described in more detail hereinbelow are part of the subject matter disclosed herein. For example, all combinations of the subject matter recited in the claims at the end of this disclosure are considered part of the subject matter disclosed herein.
図1Aおよび図1Bは、ユーザ102が、1つまたは複数の条件が満たされているときに画像データを自動アシスタントに取り込ませる図100および図200を図示している。ユニークな瞬間を捉えるためにカメラのタッチインターフェースに直に触れることをユーザ102が独占して担当しないように、画像を条件に応じて取り込むよう自動アシスタントに求めるリクエストをユーザ102が提供することができる。例えば、ユーザ102は、「アシスタント、鳥が鳴いているときの映像を撮って」などの発話118を提供することができる。この発話は、それぞれがカメラへのアクセスをそれぞれ提供することができる、コンピューティングデバイス104および/または他のコンピューティングデバイス106で受信され得る。例えば、コンピューティングデバイス104は、特定の画像および/または映像を取り込むために関心のある場所に手動で向けられ得る、および/または電気機械的に向けられ得るカメラを含むことができる。他のコンピューティングデバイス106は、通常、静的配置でポーズをとられるが、他のコンピューティングデバイス106の表示ウィンドウ内のあらゆる画像および/または映像を取り込むカメラを含む、アシスタントデバイスとすることができる。いくつかの実装形態では、ユーザ102にアクセス可能なそれぞれのアシスタントデバイスは、1つまたは複数の条件が満たされているかどうかを判断するのに最も適するように配置されるそれぞれのカメラをそれぞれが含む1つまたは複数の特定のアシスタントデバイスを協力して識別することができる。この決定は、条件の主題と、条件の主題(例えば、最高品質の画像、最高解像度、最短の待ち時間、最小の干渉など)を最も明確に捉える画像データを提供するそれぞれの特定のアシスタントデバイスの能力とに基づくものとすることができる。 Figures 1A and 1B illustrate diagrams 100 and 200 in which a user 102 causes an automated assistant to capture image data when one or more conditions are met. The user 102 can provide a request to the automated assistant to conditionally capture images so that the user 102 does not have the exclusive responsibility of directly touching the camera's touch interface to capture unique moments. . For example, the user 102 can provide an utterance 118 such as "Assistant, take a video of the birds singing." This utterance may be received at computing device 104 and/or other computing device 106, each of which may each provide access to a camera. For example, computing device 104 may include a camera that may be manually aimed and/or electromechanically aimed at a location of interest to capture a particular image and/or video. The other computing device 106 is typically posed in a static arrangement, but can be an assistant device, including a camera that captures any images and/or video within the display window of the other computing device 106. . In some implementations, each assistant device accessible to user 102 each includes a respective camera that is best positioned to determine whether one or more conditions are met. It can jointly identify one or more specific assistant devices. This determination is based on the subject matter of the conditions and of each particular assistant device that provides image data that most clearly captures the subject matter of the conditions (e.g. highest quality image, highest resolution, lowest latency, lowest interference, etc.). can be based on competence.
この発話118を受信することに応じて、コンピューティングデバイス104またはコンピューティングデバイス106は、発話118を特徴付けるオーディオデータを処理することができる。発話118が1つまたは複数の条件に従ってメディアデータを取り込むように自動アシスタントに求めるリクエストを含むと判断するように、オーディオデータを処理することができる。いくつかの実装形態において、オーディオデータは、コンピューティングデバイス104、コンピューティングデバイス106、および/またはコンピューティングデバイス110において、ユーザリクエストデータ112を生成するように処理されてもよい。ユーザリクエストデータ112は、ユーザ102からのリクエストを満たすための1つまたは複数の目的および/またはスロット値を特徴付けることができる。ユーザリクエストデータ112は、自動アシスタントがユーザリクエストデータ１１２において識別された目的を実行する前に満たされるべき１つまたは複数の条件を特徴づけることができる条件データ114を生成するために、さらに処理することができる。例えば、条件データは、鳥の存在を検出するための第1の条件、および鳥が鳴いていることを検出するための第2の条件を特徴付けることができる。 In response to receiving this utterance 118 , computing device 104 or computing device 106 may process audio data characterizing utterance 118 . The audio data can be processed to determine that the utterance 118 comprises a request for an automated assistant to capture media data according to one or more conditions. In some implementations, audio data may be processed at computing device 104 , computing device 106 , and/or computing device 110 to generate user-requested data 112 . User request data 112 may characterize one or more objectives and/or slot values for fulfilling a request from user 102 . The user request data 112 is further processed to generate condition data 114 that can characterize one or more conditions to be met before the automated assistant performs the purpose identified in the user request data 112. be able to. For example, condition data may characterize a first condition for detecting the presence of a bird and a second condition for detecting that the bird is singing.
いくつかの例では、第1の条件がオーディオデータを用いて満たされ、画像データを用いて第2の条件を満たすことができる場合、自動アシスタントは、第1の条件が満たされるまで処理される画像データ量を減らすことができる。例えば、マイクのオーディオフィードからのオーディオデータが特定の条件を満たすと自動アシスタントが判断するまで、カメラの画像フィードからの画像データの処理を制限してもよくおよび/または止めてもよい。このように、自動アシスタントは、ユーザ102からの条件付きリクエストに依然として応答しながら計算リソースの使用量を最適化することができる。 In some examples, if a first condition is met using audio data and a second condition can be met using image data, the automated assistant will process until the first condition is met. Image data volume can be reduced. For example, processing of image data from a camera's image feed may be restricted and/or prevented until the automated assistant determines that the audio data from the microphone's audio feed satisfies certain conditions. In this manner, the automated assistant can optimize usage of computing resources while still responding to conditional requests from user 102 .
いくつかの実装形態において、1つまたは複数の訓練済み機械学習モデルを識別することができるモデルデータ116を生成するように、ユーザ102によって指定された条件と関連するデータを処理することができる。訓練済み機械学習モデルは、1つまたは複数の条件が満たされているかどうかを判断するために、データを処理する際にコンピューティングデバイス110または他のコンピューティングデバイスによって使用されることができる。例えば、訓練済み機械学習モデルは、カメラの表示ウィンドウに鳥がいると判断するときに使用され得る第1の訓練済み機械学習モデルを含むことができる。その代わりに、またはさらに、訓練済み機械学習モデルは、鳥が鳴いていることに対応するオーディオが検出されていると判断する際に使用され得る第2の訓練済み機械学習モデルを含むことができる。 In some implementations, data associated with conditions specified by user 102 can be processed to generate model data 116 that can identify one or more trained machine learning models. A trained machine learning model can be used by computing device 110 or other computing device in processing data to determine whether one or more conditions are met. For example, the trained machine learning models can include a first trained machine learning model that can be used when determining that there is a bird in the viewing window of the camera. Alternatively, or in addition, the trained machine learning model may include a second trained machine learning model that may be used in determining that audio corresponding to birds singing is being detected. .
いくつかの実装形態では、自動アシスタントが、発話が1つまたは複数の条件に基づきカメラを制御するためのリクエストに対応すると判断すると、自動アシスタントは、コンピューティングデバイス104のカメラ用のビューファインダGUIを起動することを無視することができる。言い換えれば、カメラ作業が行われるようユーザ102がリクエストしたとしても、自動アシスタントは、バッテリおよび処理帯域幅を維持するために、ビューファインダGUIが、非アクティブにするかそうでなければレンダリングされないようにすることができる。自動アシスタントがこのようなリクエストに対してカメラを制御している場合、ユーザ102は、必ずしも任意の画像をプレビューする必要がないため、ビューファインダはこのように無視されることができる。 In some implementations, when the automated assistant determines that the utterance corresponds to a request to control the camera based on one or more conditions, the automated assistant displays a viewfinder GUI for the camera of computing device 104. You can ignore starting. In other words, even if the user 102 requests that camera work be done, the automated assistant will deactivate or otherwise prevent the viewfinder GUI from rendering in order to conserve battery and processing bandwidth. can do. The viewfinder can thus be ignored because the user 102 does not necessarily need to preview any images if the automated assistant is controlling the camera for such a request.
いくつかの実装形態では、自動アシスタントによって採用される音声処理にバイアスを掛けるかどうかを判断するために条件データ114を処理することができる。その代わりに、またはさらに、選択された1つまたは複数の訓練済み機械学習モデルを使用してデータが処理されると、データ処理の1つまたは複数の結果に従って、音声処理にバイアスを掛けることができる。例えば、発話118が自動アシスタントによって受信され、1つまたは複数の条件が満たされているどうかを判断するために自動アシスタントがコンピューティングデバイス104のカメラを初期化すると、カメラの表示ウィンドウ内の物を、さらに自動アシスタントに対して処理された音声にバイアスを掛けるための基礎とすることができる。例えば、発話118に応じて、自動アシスタントは、「少女」を「鳥」の語よりも高くランク付けし、それにより、自動アシスタントに、条件が、歌っている少女の存在に関連すると結論付けさせることができてもよい。しかしながら、選択された訓練済み機械学習モデルを使用してカメラからの画像データが処理される場合、自動アシスタントは、鳥小屋108がカメラの覗き窓に存在すると判断することができる。この判断に基づき、自動アシスタントは、「鳥」という語が「少女」という語よりも優先されるように1つまたは複数の候補語のそれぞれのスコアを変更することができる。この結果、条件が「少女」が歌っていることに代わって「鳥」が鳴いていることに関連することを示すように、条件データ114を修正することができる。 In some implementations, the conditional data 114 may be processed to determine whether to bias the speech processing employed by the automated assistant. Alternatively, or in addition, once the data has been processed using one or more selected trained machine learning models, the audio processing may be biased according to one or more outcomes of the data processing. can. For example, when utterance 118 is received by the automated assistant and the automated assistant initializes the camera of computing device 104 to determine if one or more conditions are met, objects in the camera's viewing window , and can also be the basis for biasing the processed speech for automated assistants. For example, in response to utterance 118, the automated assistant ranks "girl" higher than the word "bird," thereby causing the automated assistant to conclude that the condition is related to the presence of a singing girl. may be able to However, if the image data from the camera is processed using the selected trained machine learning model, the automated assistant can determine that the aviary 108 is present in the camera's viewing window. Based on this determination, the automated assistant can change the score of each of the one or more candidate words so that the word "bird" is preferred over the word "girl." As a result, the condition data 114 can be modified to indicate that the condition relates to "birds" singing instead of "girls" singing.
いくつかの実装形態では、画像データ、音声データ、および/またはそれ以外のリクエスト関連データのさらなる処理の双方向バイアシングを実行するために画像データおよび/または音声データを処理することができる。例えば、音声データの処理は、ユーザが自動アシスタントに履行させることを望む1つまたは複数のリクエストに関する仮説、および/または1つまたはリクエストの主題をもたらすことができる。さらに、画像データの処理により、特定の物の分類を特定することができる。次いで、物の分類に従って音声データの後続の任意の処理にバイアスを掛けるとともに、1つまたは複数のリクエストの仮説または主題に従って画像データの任意のさらなる処理にもバイアスを掛けるように、さらに作業を行うことができる。 In some implementations, image data and/or audio data may be processed to perform bidirectional biasing of further processing of image data, audio data, and/or other request-related data. For example, the processing of voice data can yield hypotheses about one or more requests that the user wants the automated assistant to fulfill, and/or the subject matter of one or more requests. In addition, processing the image data can identify classes of particular objects. Further work is then performed to bias any subsequent processing of the audio data according to the classification of the object, as well as to bias any further processing of the image data according to the hypothesis or theme of one or more of the requests. be able to.
いくつかの実装形態において、ユーザ102によって自動アシスタントに提供されたオーディオデータを処理する1つまたは複数の結果に従って、画像処理にバイアスを掛けることができる。例えば、カメラの覗き窓内の様々な物がコンピューティングデバイス130により生成された画像データにより特徴付けられ得る。画像データは、複数の物の周りのバウンディングボックスを識別するために処理を受けることができる。画像データの取り込み時間付近で取り込まれたオーディオデータは、特定の物に対する識別子にバイアスを掛けるために使用されることができる。例えば、自動アシスタントは、画像データにおいて鳥小屋108の周りのバウンディングボックスに割り当てるために最初に「小屋」という語を優先させてもよい。「小屋」という語は、「木」、「ボックス」、「開口」、および「鳥小屋」などのそれ以外の候補語よりも最初に優先され得る。しかしながら、ユーザ102が発話118において「鳥」という語を識別したため、「鳥小屋」がすべての他の候補語よりも優先されるように、候補語の優先順位にバイアスを掛けることができる。このように、ユーザ102からの特定の条件付きリクエストに対して、特定の条件が満たされているかどうかを判断する際、画像データをより正確に処理することができる。 In some implementations, image processing can be biased according to one or more results of processing audio data provided by user 102 to the automated assistant. For example, various objects within a camera viewing window may be characterized by image data generated by computing device 130 . The image data can be processed to identify bounding boxes around objects. Audio data captured around the capture time of the image data can be used to bias the identifier for a particular object. For example, the automated assistant may prioritize the word "hut" first for assignment to a bounding box around aviary 108 in the image data. The word "shed" may be prioritized first over other candidate words such as "tree", "box", "opening" and "aviary". However, because user 102 identified the word "bird" in utterance 118, the priority of the candidate words can be biased such that "aviary" is preferred over all other candidate words. In this way, for certain conditional requests from user 102, image data can be processed more accurately when determining whether certain conditions have been met.
いくつかの実装形態においては、自動アシスタントは、コンピューティングデバイス110、コンピューティングデバイス104、および/またはコンピューティングデバイス106に、リクエストに関連する1つまたは複数の条件が満たされているかどうかを判断するためにメディアデータ122を処理させることができる。いくつかの実装形態においては、1つまたは複数の条件が満たされているかどうかを判断するためにデータを処理するコンピューティングデバイスが、ユーザ102からのリクエストに応じて画像を取り込むために使用されるカメラを含む他のコンピューティングデバイスとは異なっていてもよい。例えば、メディアデータ122は、1つまたは複数のコンピューティングデバイスの1つまたは複数のインターフェースを介して生成されているデータを特徴付けすることができる。いくつかの実装形態においては、メディアデータ122は、コンピューティングデバイス104のグラフィカルユーザインターフェース(GUI:Graphical User Interface)130の内容を特徴付けることができる。その代わりに、またはさらに、メディアデータ122は、コンピューティングデバイス104のマイクで受信したオーディオおよび/またはコンピューティングデバイス104のカメラで受信した光を特徴付けることができる。いくつかの実装形態においては、コンピューティングデバイス110またはコンピューティングデバイス104が1つまたは複数の条件が満たされたと判断すると、条件検証データ124が生成され得る。条件検証データ124は、リクエストの1つまたは複数の条件が満たされたことを自動アシスタントに示すことができ、またこのデータに基づき、自動アシスタントがカメラ制御命令126を生成することができる。 In some implementations, the automated assistant tells computing device 110, computing device 104, and/or computing device 106 whether one or more conditions associated with the request are met. The media data 122 can be processed for the purpose. In some implementations, a computing device that processes data to determine whether one or more conditions are met is used to capture images upon request from user 102. It may be different than other computing devices, including cameras. For example, media data 122 may characterize data being generated via one or more interfaces of one or more computing devices. In some implementations, media data 122 may characterize the content of a graphical user interface (GUI) 130 of computing device 104 . Alternatively or additionally, media data 122 may characterize audio received by a microphone of computing device 104 and/or light received by a camera of computing device 104 . In some implementations, condition validation data 124 may be generated when computing device 110 or computing device 104 determines that one or more conditions have been met. Condition validation data 124 can indicate to the automated assistant that one or more conditions of the request have been met, and based on this data, the automated assistant can generate camera control instructions 126 .
いくつかの実装形態においては、メディアデータ122は、さもなければカメラを使用して画像または映像を取り込むために使用されるであろう低いサンプリングレートでカメラフィードをサンプリングすることによって生成され得る。その代わりに、またはさらに、メディアデータ122は、さもなければカメラを使用して画像または映像を取り込むために使用されるであろう低い解像度の画像を取り込むために、カメラフィードを介して取り込まれた画像の解像度を調整することによって、生成され得る。このように、条件付きリクエストの1つまたは複数の条件が満たされたことを検証すると、計算リソースを維持することができる。いくつかの実装形態においては、1つまたは複数の条件が満たされているかどうかを判断するための、データのサンプリングレートおよび/または解像度が、ユーザ102によって識別された1つまたは複数の条件に基づいて動的に選択され得る。例えば、条件がアクティブな動き(例えば、犬がジャンプしている)に関連する場合、第1のサンプリングレートおよび/または第1の解像度を選択することができ、条件がより静的な特徴(例えば、赤ちゃんが笑っている)に関連する場合、第1のサンプリングレートおよび/または第1の解像度よりも低い第2のサンプリングレートおよび/または第2の解像度を選択してもよい。 In some implementations, media data 122 may be generated by sampling a camera feed at a lower sampling rate that would otherwise be used to capture images or videos using a camera. Alternatively or additionally, the media data 122 was captured via a camera feed to capture lower resolution images that would otherwise be used to capture images or video using a camera. It can be generated by adjusting the resolution of the image. In this manner, verifying that one or more conditions of a conditional request have been met can conserve computational resources. In some implementations, the sampling rate and/or resolution of the data for determining whether one or more conditions are met is based on one or more conditions identified by user 102. can be dynamically selected by For example, if the condition relates to active motion (e.g. a dog is jumping), a first sampling rate and/or a first resolution can be selected and the condition is a more static feature (e.g. , baby laughing), a second sampling rate and/or second resolution that is lower than the first sampling rate and/or first resolution may be selected.
図1Bによれば、メディアデータ122は、画像が鳥128などの特徴を含むことを検証するために、1つまたは複数の訓練済み機械学習モデルを使用して処理され得る、鳥128の画像を特徴付けることができる。その代わりに、またはさらに、メディアデータ122は、鳥128を含むオーディオを特徴付けることができ、オーディオが鳥128が鳴いているなどの特徴を含むことを検証するために、1つまたは複数の他の訓練済み機械学習モデルを使用してオーディオを処理することができる。いくつかの実装形態においては、リクエストの条件が満たされたことをメディアデータ122が示すと、自動アシスタントが、1つもしくは複数の画像および/または1つもしくは複数の映像を取り込むなどの作業の実施を初期化することができる。例えば、第1の条件および第2の条件が満たされている間にメディアデータを作成するために、自動アシスタントが、カメラ制御命令126をコンピューティングデバイス104のカメラに提供することができる。 According to FIG. 1B, media data 122 may be processed using one or more trained machine learning models to verify that the image contains features such as bird 128. can be characterized. Alternatively, or in addition, media data 122 may characterize audio that includes bird 128, and one or more other parameters to verify that the audio includes features such as bird 128 singing. Audio can be processed using trained machine learning models. In some implementations, when the media data 122 indicates that the conditions of the request have been met, the automated assistant performs a task such as capturing one or more images and/or one or more videos. can be initialized. For example, an automated assistant can provide camera control instructions 126 to the camera of computing device 104 to create media data while the first and second conditions are met.
いくつかの実装形態においては、カメラ制御命令126は、1つまたは複数のカメラを制御するための1つまたは複数のパラメータを含むことができる。例えば、カメラ制御命令126は、カメラに、鳥128を含むカメラの覗き窓の一部にフォーカスさせるデータを含むことができる。その代わりに、またはさらに、カメラ制御命令126は、カメラに、鳥128がもはやカメラの覗き窓にいなくなるまで、および/または鳥128がもはや可聴音を発しないかおよび/またはもはや鳴かなくなるまで、鳥128の画像および/または映像を取り込ませるデータを含むことができる。その代わりに、またはさらに、カメラ制御命令126は、カメラに、ユーザからのリクエストに対応する1つまたは複数の条件に関連する環境の特徴にズームインさせるデータを含むことができる。 In some implementations, camera control instructions 126 may include one or more parameters for controlling one or more cameras. For example, camera control instructions 126 may include data that causes the camera to focus on a portion of the camera's viewing window that contains bird 128 . Alternatively, or in addition, camera control instructions 126 instruct the camera to wait until bird 128 is no longer in the camera's viewing window and/or until bird 128 no longer emits audible sounds and/or no longer sings. It may contain data that allows images and/or video of the bird 128 to be captured. Alternatively or additionally, camera control instructions 126 may include data that causes the camera to zoom in on features of the environment associated with one or more conditions corresponding to the request from the user.
いくつかの実装形態においては、自動アシスタントが1つまたは複数のアプリケーション機能にカメラ作業を条件付けるべきであることをユーザ102が指定することができる。例えば、ユーザ102は、「アシスタント、このウェブサイトに表示されている鳥が存在し、鳴いているときに映像を撮って」などの発話を提供することができる。このように、自動アシスタントは、コンピューティングデバイス104のGUI130またはコンピューティングデバイス106のGUIにレンダリングされる鳥と同じ種類の鳥を識別することにカメラ作業を条件付けすることができる。このように、条件データ114は、コンピューティングデバイス104を介してアクセスできるアプリケーションの1つまたは複数の機能を特徴付けることができる。例えば、GUI130が、ショウジョウコウカンチョウのグラフィックを含むウェブサイトをレンダリングすることができ、自動アシスタントが、ショウジョウコウカンチョウがコンピューティングデバイス104の表示ウィンドウに存在するときに第1の条件が満たされていると見なすことができる。第2の条件(例えば、ショウジョウコウカンチョウが鳴いている)が満たされると、自動アシスタントが、ユーザ102からの発話に従ってカメラ作業の実施を初期化することができる。その代わりに、またはさらに、コンピューティングデバイス104を介してアクセスできるアプリケーションの状態に基づいてカメラ作業を条件付けすることができる。したがって、特定のアプリケーションが特定の状態を示すと(例えば、特定の作業を完了する)、自動アシスタントは、特定のカメラ作業が行われるようにすることができる。 In some implementations, the user 102 can specify that the automated assistant should condition camera work on one or more application functions. For example, user 102 may provide an utterance such as "Assistant, take a picture of the bird displayed on this website when it is present and singing." In this way, the automated assistant can condition the camera work on identifying birds of the same species as those rendered in the GUI 130 of computing device 104 or the GUI of computing device 106 . In this manner, condition data 114 may characterize one or more functions of an application accessible via computing device 104 . For example, GUI 130 may render a website that includes a cardboard graphic, and an automated assistant may indicate that the first condition is met when the cardboard is present in the display window of computing device 104. can be assumed to be When a second condition (eg, a cardinal is singing) is met, the automated assistant can initiate performing camera work according to an utterance from user 102 . Alternatively, or in addition, camera activity can be conditioned based on the state of applications accessible via the computing device 104 . Thus, when a particular application indicates a particular state (eg, completes a particular task), the automated assistant can cause a particular camera task to occur.
いくつかの実装形態においては、カメラによって取り込まれる画像データの例は、1つまたは複数の時点で取り込まれ、および/または1つまたは複数の異なるカメラを使用して取り込まれた1つまたは複数の画像を含むことができる。その代わりに、またはさらに、カメラの1つまたは複数のセンサが有効であるときに、カメラは画像データを取り込むことができ、またカメラに接続されているコンピューティングデバイスが画像データから1つまたは複数の画像を生成する。その代わりに、またはさらに、ROMおよび/またはハードドライブ空間などのコンピューティングデバイスのメモリに画像データが保存されているときに画像データが取り込まれる。その代わりに、またはさらに、アプリケーションがカメラを使用して生成された画像を画像フィードから選択したとき、画像はカメラによって取り込まれたと見なすことができる。 In some implementations, examples of image data captured by a camera are one or more images captured at one or more points in time and/or captured using one or more different cameras. Can contain images. Alternatively, or in addition, the camera can capture image data when one or more sensors of the camera are enabled, and a computing device connected to the camera can extract one or more from the image data. to generate an image of Alternatively, or additionally, the image data is captured as it is stored in the computing device's memory, such as ROM and/or hard drive space. Alternatively, or in addition, an image may be considered captured by a camera when an application selects from an image feed an image generated using the camera.
いくつかの実施形態においては、1つまたは複数の画像が、1つまたは複数の条件が満たされているかどうかを判断するために使用され、および/または、ユーザがリクエストした特定の画像データとして保存するために使用されることができる。例えば、ユーザ102が発話118を提供すると、自動アシスタントは、カメラに、指定された条件が満たされるまで、50フレーム/秒(FPS:Frames Per Second)で画像データを取り込ませることができる。取り込み画像データからの特定の画像が指定された条件を満たすと判断されると、自動アシスタントは、特定の画像が保存されるようにし、他の取り込み画像データが削除されるようにすることができる。いくつかの実施形態においては、リクエストが映像を取り込むことに対応する場合、特定の条件が満たされるまで、第1のフレームレートで画像データが取り込まれ得る。特定の条件が満たされると、自動アシスタントは、第1のフレームレートよりも大きい第2のフレームレートで映像が取り込まれるようにすることができる。その代わりに、またはさらに、自動アシスタントは、コンピューティングデバイス104にメモリを維持するために、第1のフレームで取り込まれた映像データが削除されるようにすることができる。 In some embodiments, one or more images are used to determine whether one or more conditions are met and/or stored as specific image data requested by the user. can be used to For example, when user 102 provides speech 118, the automated assistant can cause the camera to capture image data at 50 frames per second (FPS) until specified conditions are met. Upon determining that a particular image from the captured image data meets specified criteria, the automated assistant can cause the particular image to be saved and other captured image data to be deleted. . In some embodiments, if the request corresponds to capturing video, image data may be captured at the first frame rate until certain conditions are met. When certain conditions are met, the automated assistant can cause video to be captured at a second frame rate that is greater than the first frame rate. Alternatively, or in addition, the automated assistant may cause the video data captured in the first frame to be deleted in order to preserve memory on the computing device 104 .
図2は、入力リクエストにより指定された1つまたは複数の条件に従ってカメラを制御することができる自動アシスタントを提供するシステム200を図示している。自動アシスタント204は、コンピューティングデバイス202などの1つまたは複数のコンピューグデバイス、および/またはサーバデバイスで提供されたアシスタントアプリケーションの一部として働くことができる。ユーザは、ユーザとアプリケーションとの間にインターフェースを提供することのできるマイク、カメラ、タッチスクリーンディスプレイ、ユーザインターフェース、および/またはそれ以外の如何なる装置であってもよい、アシスタントインターフェース220を介して自動アシスタント204と対話することができる。例えば、ユーザは、アシスタントインターフェース220に口頭、テキスト、および/またはグラフィカル入力を提供することにより、自動アシスタント204に、1つまたは複数の働き(例えば、データの提供、周辺機器の制御、エージェントへのアクセス、入力および/または出力を生成する、など)を初期化させることができる。その代わりに、1つまたは複数の訓練済み機械学習モデルを使用するコンテキストデータ236の処理に基づき、自動アシスタント204を初期化することができる。コンテキストデータ236は、自動アシスタント204がアクセスできる環境の1つもしくは複数の特徴および/またはその環境にある物、および/または自動アシスタント204と対話しようとしていることが予測されるユーザの1つまたは複数の特徴を特徴付けすることができる。コンピューティングデバイス202は、タッチインターフェースを介してコンピューティングデバイス204のアプリケーション234を制御することをユーザに可能にさせるために、タッチ入力および/またはジェスチャを受け取るためのタッチインターフェースを含む表示パネルであってもよい、表示デバイスを含むことができる。いくつかの実施形態においては、コンピューティングデバイス202は、表示デバイスを欠くことができ、それにより、グラフィカルユーザインターフェース出力を提供することなく、可聴ユーザインターフェース出力を提供することができる。さらに、コンピューティングデバイス202は、ユーザから口頭自然言語入力を受信するために、マイクなどのユーザインターフェースを提供することができる。いくつかの実施形態においては、コンピューティングデバイス202は、タッチインターフェースを含んでもよく、カメラがなくてもよいが、場合によっては、1つまたは複数の他のセンサを含んでもよい。 FIG. 2 illustrates a system 200 that provides an automated assistant capable of controlling a camera according to one or more conditions specified by input requests. Automated assistant 204 can work as part of an assistant application provided on one or more computing devices, such as computing device 202, and/or a server device. A user communicates with an automated assistant via assistant interface 220, which can be a microphone, camera, touch screen display, user interface, and/or any other device capable of providing an interface between the user and the application. 204 can be interacted with. For example, a user may provide verbal, textual, and/or graphical input to assistant interface 220 to instruct automated assistant 204 to perform one or more functions (e.g., provide data, control peripherals, provide feedback to agents, etc.). access, generate input and/or output, etc.). Instead, automated assistant 204 can be initialized based on processing context data 236 using one or more trained machine learning models. Context data 236 may include one or more features of and/or objects in the environment accessible to automated assistant 204 and/or one or more of the users expected to interact with automated assistant 204. can be characterized. Computing device 202 is a display panel that includes a touch interface for receiving touch input and/or gestures to allow a user to control applications 234 on computing device 204 via the touch interface. may also include a display device. In some embodiments, computing device 202 may lack a display device, thereby providing audible user interface output without providing graphical user interface output. Additionally, computing device 202 may provide a user interface, such as a microphone, to receive spoken natural language input from a user. In some embodiments, the computing device 202 may include a touch interface, may be cameraless, but may optionally include one or more other sensors.
コンピューティングデバイス202および/または他のサードパーティクライアントデバイスは、インターネットなどのネットワーク上でサーバデバイスと通信することができる。さらに、コンピューティングデバイス202および任意の他のコンピューティングデバイスは、Wi-Fiネットワークなどのローカルエリアネットワーク(LAN:Local Area Network)上で互いに通信することができる。コンピューティングデバイス202は、コンピューティングデバイス202における計算リソースを浪費しないために、計算タスクをサーバデバイスにオフロードすることができる。例えば、サーバデバイスは、自動アシスタント204をホストしてもよく、および/またはコンピューティングデバイス202が、1つまたは複数のアシスタントインターフェース220で受信した入力をサーバデバイスに送信してもよい。しかしながら、いくつかの実施形態においては、自動アシスタント204がコンピューティングデバイス202でホストされてもよく、自動アシスタント作業に関連付けられ得る様々な工程が、コンピューティングデバイス202で行われてもよい。 Computing device 202 and/or other third party client devices may communicate with the server device over a network such as the Internet. Additionally, computing device 202 and any other computing device may communicate with each other over a local area network (LAN), such as a Wi-Fi network. Computing device 202 can offload computing tasks to server devices in order not to waste computational resources on computing device 202 . For example, the server device may host the automated assistant 204 and/or the computing device 202 may send input received at one or more assistant interfaces 220 to the server device. However, in some embodiments, the automated assistant 204 may be hosted on the computing device 202, and various steps that may be associated with automated assistant work may occur on the computing device 202.
様々な実装形態において、自動アシスタント204の全態様またはすべてに満たない態様がコンピューティングデバイス202に実装されてもよい。このような実装形態のいくつかにおいては、自動アシスタント204の態様が、コンピューティングデバイス202を介して実装され、自動アシスタント204の他の態様を実装することのできる、サーバデバイスとインターフェースすることができる。サーバデバイスは、場合によっては、複数のユーザとそれらの関連するアプリケーションに複数のスレッドを介してサービスを提供することができる。自動アシスタント204の全態様またはすべてに満たない態様がコンピューティングデバイス202を介して実装されている実装形態では、自動アシスタント204がコンピューティングデバイス202のオペレーティングシステムとは別であるアプリケーケーションであってもよく(例えば、オペレーティングシステムの「オントップ」にインストールされた)、その代わりに、コンピューティングデバイス202のオペレーティングシステムによって直接実装されていてもよい(例えば、オペレーティングシステムのアプリケーションと見なされるが、それと一体化している)。 In various implementations, all or less than all aspects of automated assistant 204 may be implemented on computing device 202 . In some such implementations, aspects of automated assistant 204 are implemented via computing device 202 and can interface with a server device that can implement other aspects of automated assistant 204. . A server device may potentially serve multiple users and their associated applications via multiple threads. In implementations in which all or less than all aspects of automated assistant 204 are implemented via computing device 202, automated assistant 204 may be an application that is separate from the operating system of computing device 202. often (e.g., installed "on top" of the operating system), or alternatively may be implemented directly by the operating system of the computing device 202 (e.g., considered an application of the operating system, but integrated with it). are becoming).
いくつかの実施形態においては、自動アシスタント204は、コンピューティングデバイス202および/またはサーバデバイスに対して入力および/または出力を処理するための複数の異なるモジュールを採用してもよい入力処理エンジン206を含んでもよい。例えば、入力処理エンジン206は、オーディオデータにおいて具体化されたテキストを識別するようにアシスタントインターフェース220で受信したオーディオデータを処理することのできる、音声処理エンジン208を含んでもよい。コンピューティングデバイス202にある計算リソースを維持するために、例えばコンピューティングデバイス202からサーバデバイスにオーディオデータを送信することができる。さらに、またはその代わりに、オーディオデータがコンピューティングデバイス202において排他的に処理され得る。いくつかの実施形態においては、画像データにおいて識別された特徴は、オーディオデータ処理の正確さを促進するために使用され得るように、対応する画像対照物を考慮してオーディオデータ処理が行われてもよい。画像の特徴抽出は、サービスデバイスでおよび/またはコンピューティングデバイス202で行われてもよい。 In some embodiments, the automated assistant 204 has an input processing engine 206 that may employ multiple different modules for processing input and/or output to the computing device 202 and/or server device. may contain. For example, the input processing engine 206 may include a voice processing engine 208 capable of processing audio data received at the assistant interface 220 to identify text embodied in the audio data. To conserve computing resources at computing device 202, audio data may be transmitted from computing device 202 to a server device, for example. Additionally or alternatively, audio data may be processed exclusively at computing device 202 . In some embodiments, features identified in the image data are subjected to audio data processing in view of corresponding image contrasts so that they can be used to promote accuracy in audio data processing. good too. Image feature extraction may be performed at the service device and/or at the computing device 202 .
オーディオデータをテキストに変換する工程は、単語または語句に対応するオーディオデータ群を識別するためのニューラルネットワーク、および/または統計モデルを採用することができる音声認識アルゴリスムを含むことができる。オーディオデータから変換されたテキストは、データ解析エンジン210によって解析され、コマンドフレーズ、目的、アクション、スロット値、および/またはユーザによって指定された他の任意のコンテンツを生成および/または識別するために使用され得るテキストデータとして、自動アシスタント204が利用できるようにされる。いくつかの実施形態においては、データ解析エンジン210によって提供された出力データはパラメータエンジン212に提供され、自動アシスタント204および/または自動アシスタント204を介してアクセスすることができるアプリケーションもしくはエージェントによって行われることができる特定の目的、アクション、および/またはルーチンに対応する入力をユーザが提供したかどうかを判断することができる。例えば、アシスタントデータ238がサーバデバイスおよび/またはコンピューティングデバイス202に保存されてもよく、このデータ238は、自動アシスタント204が行うことができる1つまたは複数の働きを定義するデータとともに、アクションを行うために必要なパラメータも含むことができる。パラメータエンジン212は、目的、アクション、および/またはスロット値に対して1つまたは複数のパラメータを生成することができ、出力生成エンジン214に1つまたは複数のパラメータ提供することができる。出力生成エンジン214は、1つまたは複数のパラメータを使用して、ユーザに出力を提供するためにアシスタントインターフェース220と通信し、および/または1つまたは複数のアプリケーション234に出力を提供するために1つまたは複数のアプリケーション234と通信することができる。 Converting the audio data to text can include speech recognition algorithms that can employ neural networks and/or statistical models to identify groups of audio data that correspond to words or phrases. The text converted from the audio data is parsed by the data parsing engine 210 and used to generate and/or identify command phrases, objectives, actions, slot values, and/or any other content specified by the user. It is made available to the automated assistant 204 as text data that can be read. In some embodiments, the output data provided by the data analysis engine 210 is provided to the parameter engine 212 to be performed by the automated assistant 204 and/or an application or agent that can be accessed via the automated assistant 204. It can be determined whether the user has provided input corresponding to a particular purpose, action, and/or routine that can be performed. For example, assistant data 238 may be stored on the server device and/or computing device 202, which, along with data defining one or more actions that the automated assistant 204 may perform, performs an action. It can also contain the parameters necessary for Parameter engine 212 can generate one or more parameters for objectives, actions, and/or slot values, and can provide one or more parameters to output generation engine 214 . Output generation engine 214 uses one or more parameters to communicate with assistant interface 220 to provide output to a user and/or 1 to provide output to one or more applications 234 . It can communicate with one or more applications 234 .
いくつかの実施形態においては、自動アシスタント204は、コンピューティングデバイス202のオペレーティングシステムの「オントップ」にインストールされ得るアプリケーションであってもよく、および/またはそれ自体がコンピューティングデバイス202のオペレーティングシステムの一部(または全部)を形成してもよい。自動アシスタントアプリケーションは、オンデバイス音声認識、オンデバイス自然言語理解、およびオンデバイスフルフィルメントを含み、および/またはそれらにアクセスする。例えば、コンピューティングデバイス202にローカルに格納されたエンド－ツーエンド音声認識機械学習モデルを使用して(例えば、マイクによって検出された)オーディオデータを処理する、オンデバイス音声認識モジュールを使用してオンデバイス音声認識を行うことができる。オンデバイス音声認識は、オーディオデータにある発話(もしあれば)に対する認識されたテキストを生成する。また、例えば、オンデバイス自然言語理解(NLU:Natural Language Understanding)データを生成するために、オンデバイス音声認識、また場合によってはコンテキストデータを使用して生成された認識済みテキストを処理するオンデバイスNLUモジュールを使用して、NLUを行うことができる。 In some embodiments, the automated assistant 204 may be an application that may be installed "on top" of the operating system of the computing device 202 and/or may itself be installed within the operating system of the computing device 202. You may form a part (or all). Automated assistant applications include and/or access on-device speech recognition, on-device natural language understanding, and on-device fulfillment. For example, on-device using an on-device speech recognition module that processes audio data (e.g., detected by a microphone) using an end-to-end speech recognition machine learning model stored locally on the computing device 202 Can perform voice recognition. On-device speech recognition produces recognized text for utterances (if any) in the audio data. Also, for example, an on-device NLU that processes recognized text generated using on-device speech recognition, and possibly contextual data, to generate on-device Natural Language Understanding (NLU) data Modules can be used to do NLU.
NLUデータは、目的に対する発話、および場合によってはパラメータ(例えば、スロット値)に対応する目的を含むことができる。(オンデバイスNLUからの)NLUデータ、および場合によってはそれ以外のローカルデータを利用して、発話の目的(および場合によっては目的に対するパラメータ)を解決するために取るべきアクションを決めるオンデバイスフルフィルメントモジュールを使用して、オンデバイスフルフィルメントを行うことができる。これは、発話に対するローカルレスポンスおよび/またはリモートレスポンス(例えば、回答)、発話に基づき行うローカルにインストールされたアプリケーションとの対話、発話に基づくモノのインターネット(IoT:Internet-Of-Things)デバイスに(直接にまたは対応するリモートシステムを介して)送信すべきコマンド、および/または発話に基づき行うそれ以外の解決アクションを決めること含むことができる。次いで、オンデバイスフルフィルメントは、発話を解決するための決められたアクションの、ローカルおよび/またはリモートパフォーマンス/実行を初期化することができる。 NLU data can include utterances to objectives and possibly objectives corresponding to parameters (eg, slot values). On-device fulfillment using NLU data (from the on-device NLU) and possibly other local data to determine actions to be taken to resolve the purpose (and possibly parameters to the purpose) of the utterance Modules can be used for on-device fulfillment. This includes local and/or remote responses (e.g., answers) to utterances, interaction with locally installed applications based on utterances, Internet-Of-Things (IoT) devices based on utterances ( determining commands to send and/or other resolution actions to take based on the utterance (either directly or via a corresponding remote system). The on-device fulfillment can then initiate local and/or remote performance/execution of the determined actions to resolve the utterance.
様々な実装形態において、リモート音声処理、リモートNLU、および/またはリモートフルフィルメントを少なくとも選択的に利用され得る。リモートNLUおよび/またはリモートフルフィルメントのために、例えば、認識済みテキストを少なくとも選択的にリモート自動アシスタントコンポーネントに送信することができる。例えば、場合によっては、オンデバイスパフォーマンスと並行するリモートパフォーマンスにのために、またはオンデバイスNLUおよび/またはオンデバイスフルフィルメントの失敗に応じて、認識済みテキストを送信することができる。但し、オンデバイス音声処理、オンデバイスNLU、オンデバイスフルフィルメント、および/またはオンデバイス実行が、少なくとも発話を解決する際に提供する待ち時間短縮(発話を決するのに必要なクライアント－サーバラウンドトリップがないことを原因とする)のために優先され得る。さらに、オンデバイス機能性は、ネットワーク接続性がないか制限されている状況において使用可能な唯一の機能性であってもよい。 In various implementations, remote voice processing, remote NLU, and/or remote fulfillment may be at least selectively utilized. For remote NLU and/or remote fulfillment, for example, the recognized text can be at least selectively sent to a remote automated assistant component. For example, in some cases, recognized text can be sent for remote performance in parallel with on-device performance, or in response to on-device NLU and/or on-device fulfillment failures. However, on-device voice processing, on-device NLU, on-device fulfillment, and/or on-device execution provide at least the reduced latency in resolving utterances (the client-server round trip required to resolve utterances is due to non-existence) may take precedence. Additionally, on-device functionality may be the only functionality available in situations where network connectivity is absent or limited.
いくつかの実施形態においては、コンピューティングデバイス202は、コンピューデバイス202および/または自動アシスタント204を提供したエンティティとは異なるサードパーティエンティティによって提供され得る1つまたは複数のアプリケーション234を含むことができる。自動アシスタント204および/またはコンピューティングデバイス202のアプリケーション状態エンジンは、アプリケーションデータ230にアクセスして、1つまたは複数のアプリケーション234が行うことができる1つまたは複数のアクション、ならびに1つまたは複数のアプリケーション234のそれぞれのアプリケーションの状態および/またはコンピューティングデバイス202に関連しているそれぞれのデバイスの状態も決めることができる。自動アシスタント204および/またはコンピューティングデバイス202のデバイス状態エンジンは、デバイスデータ232にアクセスして、コンピューティングデバイス202、および/またはコンピューティングデバイス202に関連している1つまたは複数のデバイスが行うことができる1つまたは複数のアクションを決めることができる。さらに、アプリケーションデータ230および/またはそれ以外の任意のデータ(例えば、デバイスデータ232)は、自動アシスタント204によってアクセスされ得、特定のアプリケーション234および/またはデバイスが実行しているコンテキスト、および/または特定のユーザが、アプリケーション234、および/またはそれ以外の任意のデバイスもしくはモジュールにアクセスするコンピューティングデバイス202にアクセスしているコンテキストを特徴付けることができるコンテキストデータ236を生成することができる。 In some embodiments, computing device 202 may include one or more applications 234 that may be provided by a third party entity different from the entity that provided computing device 202 and/or automated assistant 204. . The application state engine of automated assistant 204 and/or computing device 202 accesses application data 230 to determine one or more actions that one or more applications 234 can take, as well as one or more actions that one or more applications can take. The state of each application 234 and/or the state of each device associated with the computing device 202 can also be determined. The automated assistant 204 and/or the device state engine of the computing device 202 access the device data 232 to determine what the computing device 202 and/or one or more devices associated with the computing device 202 are doing. You can decide which one or more actions you can take. Further, application data 230 and/or any other data (e.g., device data 232) may be accessed by automated assistant 204 to provide specific context in which application 234 and/or device is running, and/or specific context data 236 may be generated that may characterize the context in which the user is accessing the computing device 202 accessing the application 234 and/or any other device or module.
1つまたは複数のアプリケーション234がコンピューティングデバイス202で実行される間、デバイスデータ232は、コンピューティングデバイス202で実行される各アプリケーション234の現在のオペレーション状態を特徴付けることができる。さらに、アプリケーションデータ230は、1つまたは複数のアプケーション234の方向においてレンダリングされる1つまたは複数のグラフィカルユーザインターフェースの内容など、実行アプリケーション234の1つまたは複数の特徴を特徴付けることができる。その代わりに、またはさらに、アプリケーションデータ230は、それぞれのアプリケーションの現在の動作状態に基づき、それぞれのアプリケーションによっておよび/または自動アシスタント204によって更新され得るアクションスキーマを特徴付けることができる。その代わりに、またはさらに、1つまたは複数のアプリケーション234に対する1つまたは複数のアクションスキーマは静的なままであり得るが、自動アシスタント204を介して初期化するために適したアクションを判断するために、アプリケーション状態エンジンによってアクセスされ得る。 While one or more applications 234 are running on computing device 202 , device data 232 can characterize the current operational state of each application 234 running on computing device 202 . Additionally, application data 230 may characterize one or more characteristics of executing applications 234 , such as the content of one or more graphical user interfaces rendered in the direction of one or more applications 234 . Alternatively or additionally, application data 230 may feature action schemas that may be updated by respective applications and/or by automated assistant 204 based on the current operating state of each application. Alternatively, or in addition, one or more action schemas for one or more applications 234 may remain static, but to determine actions suitable for initialization via automated assistant 204. can be accessed by the application state engine.
コンピューティングデバイス202は、さらに、アプリケーションデータ230、デバイスデータ232、コンテキストデータ236、および/またはそれ以外のコンピューティングデバイス202にアクセスできる任意のデータを処理するために1つまたは複数の訓練済み機械学習モデルを使用することができるアシスタント呼び出しエンジン222を含むことができる。アシスタント呼び出しエンジン222は、自動アシスタント204を呼び出すための呼び出しフレーズをユーザがはっきり言うのを待つべきであるか否か、あるいはユーザが呼び出しフレーズをはっきり言うようユーザに求める代わりに、データが自動アシスタントを呼び出すというユーザによる目的を示すと見なすかどうかを判断するために、このデータを処理することができる。例えば、複数のデバイスおよび/またはアプリケーションが様々なオペレーティング状態を示している環境にユーザがいるシナリオに基づいている訓練データのインスタンスを使用して、1つまたは複数の訓練済み機械学習モデルを訓練することができる。訓練データのインスタンスは、ユーザが自動アシスタントを呼び出すコンテキストと、ユーザが自動アシスタントを呼び出さない他のコンテキストとを特徴付ける訓練データを取り込むために、生成するができる。 Computing device 202 may also employ one or more trained machine learning algorithms to process application data 230, device data 232, context data 236, and/or any other data accessible to computing device 202. An assistant calling engine 222 can be included that can use the model. The assistant calling engine 222 should wait for the user to articulate a call phrase to invoke the automated assistant 204, or instead of asking the user to articulate the call phrase, the data determines whether the automated assistant is called. This data can be processed to determine whether it is considered indicative of the user's intent to call. For example, train one or more pretrained machine learning models using instances of training data that are based on scenarios in which a user is in an environment with multiple devices and/or applications exhibiting different operating states. be able to. Training data instances can be generated to capture training data that characterize contexts in which the user invokes the automated assistant and other contexts in which the user does not invoke the automated assistant.
1つまたは複数の訓練済み機械学習モデルが訓練データのこれらのインスタンスに従って訓練されるとき、アシスタント呼び出しエンジン222が自動アシスタント204に、コンテキストおよび/または環境の、特徴もしくは性質、および/またはコンテキストおよび/または環境にある物に基づき、ユーザからの発話された呼び出しフレーズを検出させるか、または検出することを制限させることができる。さらに、またはその代わりに、アシスタント呼び出しエンジン222は、自動アシスタント204に、コンテキストおよび/または環境の特徴に基づき、ユーザからの1つまたは複数のアシスタントコマンドを検出させるか、または検出するのを制限させることができる。いくつかの実施形態においては、コンピューティングデバイス202が別のコンピューティングデバイスからのアシスタント抑制出力を検出するのに基づき、アシスタント呼び出しエンジン222が無効化または制限されてもよい。このように、コンピューティングデバイス202がアシスタント抑制出力を検出しているとき、自動アシスタント204は、コンテキストデータ236に基づき呼び出されない、これは、そうでなければ、アシスタント抑制出力が検出されていない場合に自動アシスタント204が呼び出される原因となる。 When the one or more trained machine learning models are trained according to these instances of the training data, the assistant invocation engine 222 tells the automated assistant 204 the context and/or environment, features or properties, and/or the context and/or Or, based on what is in the environment, it can detect or limit the detection of spoken call phrases from the user. Additionally or alternatively, the assistant invocation engine 222 causes the automated assistant 204 to detect or limit detection of one or more assistant commands from the user based on context and/or environmental characteristics. be able to. In some embodiments, assistant call engine 222 may be disabled or restricted based on computing device 202 detecting an assistant suppression output from another computing device. In this way, when the computing device 202 is detecting the assistant suppression output, the automated assistant 204 is not invoked based on the context data 236, which would otherwise be the case if the assistant suppression output had not been detected. cause the automated assistant 204 to be called.
いくつかの実施形態においては、システム200は、リクエストした作業が条件とすることができる1つまたは複数の条件を同定するのに使用され得る条件エンジン218を含むことができる。1つまたは複数の条件が満たされているときを判断するために、ユーザは、必ずしも自分だけを当てにせずに済むように、ユーザは、作業が1つまたは複数の条件に依存することをリクエストすることができる。このような条件付きリクエストは、1つまたは複数のアプリケーション、デバイス、および/またはコンピュータが制御することができる任意の他の装置もしくはモジュールを制御するために使用することができる。例えば、条件エンジン218を使用して、コンピューティングデバイス202と通信しているカメラを制御するための1つまたは複数の条件を同定することができる。 In some embodiments, system 200 may include condition engine 218 that may be used to identify one or more conditions on which requested work may be conditioned. Users can request that their work depend on one or more conditions so that they do not necessarily have to rely solely on themselves to determine when one or more conditions are met. can do. Such conditional requests can be used to control one or more applications, devices, and/or any other device or module that the computer can control. For example, condition engine 218 can be used to identify one or more conditions for controlling a camera in communication with computing device 202 .
いくつかの実施形態においては、条件が環境の特定の特徴に対応する場合、条件エンジン218が、対応する条件が満たされるために、検出された特徴が満たすべき閾値信頼スコアを明らかにすることができる。例えば、システム200は、識別された各それぞれの特徴にスコアを割り当てるためのデータを処理するために使用され得る特徴スコアエンジン224を含めてもよい。例えば、特定の作業に対する条件が、特定の物(例えば、ある種の熊)が識別されることを条件としている場合、特徴スコアエンジン224が、特定の物の存在の証拠であるデータの特徴を識別するためにデータを処理することができる。いくつかの実施形態においては、スコア(例えば、信頼スコア)を特定の特徴に割り当てるために、システム200は、画像データを処理する際に使用されてもよい特定の訓練済み機械学習モデルを識別するために使用され得る、モデル識別エンジン226を含んでもよい。例えば、条件がある種の熊の存在である場合、モデル識別エンジン226は、特定の画像が特定の種の熊を含むかどうかを判断するように画像を処理する際に使用され得る訓練済み機械学習モデルを識別することができる。画像を処理する際、1つまたは複数の画像に存在する可能性がある特定の特徴に対して特徴スコアエンジン224によりスコアを生成することができる。スコアが条件に対して閾値スコアを満たすと、自動アシスタント204は、その条件が満たされたと判断し、それに従って、対応するリクエストを進めることができる。 In some embodiments, if the condition corresponds to a particular feature of the environment, the condition engine 218 may identify a threshold confidence score that the detected feature must satisfy in order for the corresponding condition to be met. can. For example, system 200 may include feature score engine 224 that may be used to process data to assign a score to each respective feature identified. For example, if the conditions for a particular task are contingent on the identification of a particular object (eg, a bear of some kind), the feature score engine 224 identifies features in the data that are evidence of the existence of the particular object. Data can be processed for identification. In some embodiments, to assign scores (e.g., confidence scores) to particular features, system 200 identifies particular trained machine learning models that may be used in processing the image data. may include a model identification engine 226, which may be used for For example, if the condition is the presence of a certain species of bear, the model identification engine 226 is a trained machine that can be used in processing images to determine whether a particular image contains a particular species of bear. Can identify learning models. When processing images, scores can be generated by feature score engine 224 for particular features that may be present in one or more images. If the score meets the threshold score for a condition, automated assistant 204 can determine that the condition has been met and proceed with the corresponding request accordingly.
いくつかの実施形態においては、システム200は、コンテキストデータ236、デバイスデータ232、および/またはアプリケーションデータ230に従ってリクエストの特定の処理にバイアスを掛けるか否かを判断するために、ユーザからのリクエストに関連するデータを処理するリクエストバイアシングエンジン228を含んでもよい。例えば、ユーザは、カメラを制御するよう自動アシスタントに求める条件付きリクエストを提供することができ、この条件付きリクエストに基づき、リクエストバイアシングエンジン228がカメラの覗き窓において識別された特徴に従って、このリクエストの条件付き処理の処理にバイアスを掛けることができる。例えば、条件付きリクエストが「アシスタント、犬が横になっているときの映像を撮って」などの発話において具体化されると、最初に、「アシスタント、『ドック(dock)』が横になっているときの映像を撮って」と解釈されてもよい。しかしながら、カメラの可視領域に犬がいる可能性があり、それにより、リクエストバイアシングエンジン228は、「犬(dog)」という語が、「ドック(dock)」という語よりも優先されるか、そうでなければ「ドック(dock)」という語より上にランク付けされるように、発話の音声処理にバイアスを掛けることができる。その結果、自動アシスタント204は、「ドック(dock)」が存在するかどうかを判断しようとする代わりに、カメラの覗き窓において「犬(dog)」が横になっているときに映像が取り込まれるように動作することができる。 In some embodiments, system 200 processes requests from users to determine whether to bias a particular processing of the request according to context data 236, device data 232, and/or application data 230. A request biasing engine 228 may be included to process relevant data. For example, a user can provide a conditional request that asks the automated assistant to control a camera, and based on this conditional request, the request biasing engine 228 will apply this request according to the characteristics identified in the camera's viewing window. You can bias the processing of the conditional processing of . For example, if a conditional request is embodied in an utterance such as "Assistant, take a video of the dog lying down", it first says "Assistant, dock" lying down. It may be interpreted as "Take a video when you are there." However, there may be a dog in the camera's view area, which causes the request biasing engine 228 to prioritize the word "dog" over the word "dock" or Audio processing of utterances can be biased to otherwise rank above the word "dock". As a result, instead of trying to determine if a "dock" is present, the automated assistant 204 will capture video when the "dog" is lying down in the camera's viewing window. can work like
いくつかの実施形態においては、ある程度の修正コンテキストが利用できるようになり、それを提供することができるさらなるデータに鑑みて、データを続いて再度処理することができるようにするためにデータをキャッシュすることができる。例えば、さらなるオーディオデータがユーザによって提供され、初期オーディオデータの誤解釈を訂正するまで、発話に相当するある程度のオーディオデータがキャッシュに保存され得る。例えば、誤解釈を認識すると、ユーザは、「違う、『ドッグ(dog)』は、ドック(dock)じゃない」などの別の発話をすることができる。それにより、初期オーディオデータをキャッシュから取り出し、それ以外の発話に鑑みてある程度のバイアシングによりそれを再処理することができる。その代わりに、ユーザが発話をしたときに犬が最初にカメラの覗き窓にいなくてもよい。但し、初期画像データにおいて犬が捉えられ、犬として分類されると、「犬(dog)」という語に対して「ドック(dock)」という語に関するオーディオデータの初期誤解釈を修正するために、キャッシュからの初期オーディオデータを(例えば、その分類に基づき)ある程度のバイアシングにより再処理することができる。 In some embodiments, the data is cached so that it can subsequently be processed again in light of additional data that can provide some modification context as it becomes available. can do. For example, some audio data corresponding to speech may be cached until further audio data is provided by the user to correct a misinterpretation of the initial audio data. For example, upon recognizing a misinterpretation, the user can say another utterance such as "No, 'dog' is not a dock." This allows the initial audio data to be retrieved from the cache and reprocessed with some degree of biasing in view of the other utterances. Alternatively, the dog may not be initially in the camera's viewport when the user speaks. However, once a dog is captured in the initial image data and classified as dog, to correct the initial misinterpretation of the audio data for the word "dock" versus the word "dog": The initial audio data from the cache can be reprocessed with some degree of biasing (eg, based on its classification).
リクエストに対応している1つまたは複数の条件が満たされると、カメラ制御エンジン240が、カメラに対して1つまたは複数の命令を生成することができる。1つまたは複数の命令は、一人または複数のユーザからのリクエストに基づき1つもしくは複数の画像、映像、および/またはそれ以外のメディアデータを取り込むためのものとすることができる。いくつかの実施形態においては、1つまたは複数の命令は、カメラを制御する際に明らかにされ得る取り込み期間、取り込み時間、焦点、パン命令、ズーム命令、および/またはそれ以外の任意のパラメータなどのそれ以外のパラメータでも含むことができる。例えば、自動アシスタント204が特定の物の映像を取り込むようユーザがリクエストすると、カメラ制御エンジン240は、その特定の物の撮影を始めるよう、また写真アプリケーションにその特定の物に基づき映像をクロップさせるのを始めるよう、カメラに求める命令をユーザが生成することができる。このように、ユーザが手作業で、また直に、映像を取り込むのを始めるよう、映像を取り込むのを止めるよう、および/または映像を編集するようコンピューティングデバイスのタッチインターフェースと対話するのを余儀なくさせずとも、コマンドに応じて生成された映像は、予め特定の物に焦点を合わせているように見えてくる。 Camera control engine 240 may generate one or more instructions to the camera when one or more conditions corresponding to the request are met. The one or more instructions may be for capturing one or more images, videos, and/or other media data upon request from one or more users. In some embodiments, the one or more instructions may be capture duration, capture time, focus, pan instructions, zoom instructions, and/or any other parameter that may be manifested in controlling the camera. can also contain other parameters of For example, when the user requests that the automated assistant 204 capture an image of a particular object, the camera control engine 240 can initiate the capture of that particular object and have the photo application crop the image based on that particular object. A user can generate an instruction to ask the camera to start Thus, the user is forced to manually and directly interact with the computing device's touch interface to begin capturing video, stop capturing video, and/or edit video. Even without it, the video generated in response to the command will appear to be focused on a specific object in advance.
図3A、図3Bは、1つまたは複数の条件が満たされているときの画像データを取り込むよう自動アシスタントにカメラを条件付きで制御させる方法300、方法320を図示している。方法300は、自動アシスタントに対応することができる1つもしくは複数のコンピューティングデバイス、アプリケーション、および/またはそれ以外の任意の装置もしくはモジュールによっても行われてもよい。方法300は、カメラを制御するよう求める条件付きアシスタントリクエストをユーザが与えたかどうかを判断する作業302を含んでもよい。条件付きアシスタントリクエストは、自動アシスタントにアクセスする、コンピューティングデバイスにおいて受け取られた発話または他の入力において具体化されてもよい。発話は、例えば、「アシスタント、明日家で私の子どもがダンスするのを見たら、写真を撮って」であってもよく、自動アシスタントが影響を受けるあらゆる人からの事前の許可を得てこれに応答することができる。ユーザは、自分の家でパーティを催し、ある特定の瞬間を捉えるのに手作業で写真を撮る都合が付かない場合、この発話をすることがきる。自動アシスタントが条件付きアシスタントリクエストを受信したとき、方法300は作業304に進むことができる。そうでなければ、ユーザがカメラを制御するよう求める条件付きアシスタントリクエストを与えたかどうか、自動アシスタントが判断し続けてもよい。 Figures 3A, 3B illustrate methods 300, 320 of having an automated assistant conditionally control a camera to capture image data when one or more conditions are met. Method 300 may also be performed by one or more computing devices, applications, and/or any other device or module capable of supporting an automated assistant. Method 300 may include operation 302 of determining whether the user has provided a conditional assistance request to control the camera. A conditional assistant request may be embodied in speech or other input received at the computing device that accesses the automated assistant. The utterance may, for example, be "Assistant, if you see my kids dancing at home tomorrow, take a picture", and the automated assistant will do this with prior permission from anyone affected. can respond to A user can say this if he or she is having a party at his or her house and does not feel comfortable taking pictures manually to capture a particular moment. The method 300 may proceed to task 304 when the automated assistant receives the conditional assistance request. Otherwise, the automated assistant may continue to determine if the user has given a conditional assistant request to control the camera.
作業304には、リクエストに従ってカメラを制御するよう自動アシスタントに求めるために満たすべき1つまたは複数の条件を同定することを含めてもよい。1つまたは複数の条件は、1つまたは複数の条件を特徴付けるオーディオデータを処理することによって同定されてもよい。例えば、自動アシスタントが、自動アシスタントがアクセスできるカメラの覗き窓に見える影響を受けるあらゆる人からの事前許可で、ユーザの子どもが識別されると満たされ得る第1の条件を同定することができる。さらに、自動アシスタントが、その子どもが次の日にダンスをしていると満たされ得る第2の条件を同定することができる。 Task 304 may include identifying one or more conditions to be met to prompt the automated assistant to control the camera in accordance with the request. One or more conditions may be identified by processing audio data characterizing the one or more conditions. For example, an automated assistant can identify a first condition that can be met once a user's child is identified, with prior authorization from any affected person visible in a camera viewport that the automated assistant can access. Additionally, the automated assistant can identify a second condition that can be met if the child is dancing the next day.
方法300では、作業304から、カメラの覗き窓において捉えられた1つまたは複数の特徴および/または物に従って、1つまたは複数の条件の同定にバイアスを掛けることを含めてもよい、場合による作業306までを進むことができる。例えば、ユーザが、「姪」と言われる者に連絡することができ、自動システムが第1の条件を同定しているとき、最初に、「姪」という語が、「子ども」という語より高いスコアが付けられるか、そうでなければそれに優先されてもよい。但し、ユーザが発話をしたときにユーザとともにユーザの「子ども」がいたと判断するように、カメラの覗き窓内の特徴などのコンテキストデータを自動アシスタントが処理することができる。いくつかの実施形態においては、この判断は、1つまたは複数の顔認識モデルを使用して画像データを処理することに基づくものであってもよい。この判断に基づき、自動アシスタントは、「子ども」が「姪」という名前の代わりに第1の条件の一部になる、と言った「子ども」という語のスコアまたは優先度を修正することができる。 Method 300 may include, from operation 304, biasing the identification of one or more conditions according to one or more features and/or objects captured in the camera viewing window, an optional operation. You can go up to 306. For example, when a user can contact someone called "niece" and the automated system identifies the first condition, first the word "niece" is higher than the word "child". It may be scored or otherwise overridden. However, the automated assistant can process contextual data, such as features in the camera's viewport, to determine that the user's "children" were present with the user when the user spoke. In some embodiments, this determination may be based on processing the image data using one or more facial recognition models. Based on this determination, the automated assistant can modify the score or priority of the word "child" to say that "child" is part of the first condition instead of the name "niece". .
方法300では、1つまたは複数の条件が満たされているかどうかを判断するようにデータを処理することを含むことができる作業306または作業304から作業308まで進むことができる。データは、自動アシスタントがアクセスできる1つまたは複数のインターフェースを使用して起こされるデータを含むことができるがそれに限るわけではない。例えば、影響を受ける任意の人からの事前許可により、ユーザの子どもに対応する声紋が識別されているかどうかを判断するように、コンピューティングデバイスのマイクからのオーディオデータを処理することができる。その代わりにまたはさらに、ユーザの子どもがカメラの覗き窓に見えていておよび/またはカメラの覗き窓の範囲でダンスをしているかどうかを判断するように、コンピューティングデバイスと通信している1つまたは複数のカメラからの画像データを処理することができる。いくつかの実施形態においては、ユーザの背景に対応している時間、地理上の位置、および/またはそれ以外のあらゆる情報などのコンテキストデータを処理することができる。例えば、その時の日がユーザからの発話の後の日であるかどうかを判断するように、時間データを処理することができる。 Method 300 may proceed from operation 306 or operation 304 to operation 308, which may involve processing data to determine whether one or more conditions are met. The data can include, but is not limited to, data originated using one or more interfaces accessible by the automated assistant. For example, with prior authorization from any affected person, audio data from a microphone of a computing device can be processed to determine whether a voiceprint corresponding to a child of the user has been identified. Alternatively or additionally, one in communication with a computing device to determine whether a user's child is visible in the camera's viewport and/or dancing within the camera's viewport. Or image data from multiple cameras can be processed. In some embodiments, contextual data such as time corresponding to the user's background, geographic location, and/or any other information may be processed. For example, the time data can be processed to determine if the current day is the day after the utterance from the user.
方法300では、1つまたは複数の条件が満たされているかどうかを判断することを含むことができる作業308から作業310まで進むことができる。1つまたは複数の条件が満たされていないと判断されると、方法300では、作業308に戻ることができる。その代わりに、1つまたは複数の条件が満たされていると判断されると、図3Bの方法320で示す方法では、作業310から継続要素Aを経て作業312まで進むことができる。作業312には、カメラにユーザからのリクエストに従って画像データを取り込ませることを含めてもよい。例えば、リクエストが1枚の写真に特有のものであるならば、自動アシスタントが、カメラに、如何なる人からの事前許可により、画像において取り込まれ得る1つまたは複数の画像を取り込ませることができる。その代わりに、リクエストが映像に特有のものであるならば、自動アシスタントがカメラに1つまたは複数の映像を取り込ませることができる。写真または映像を取り込む前に、自動アシスタントがこのような人の写真または映像を取り込むのを許可されている、または許可されていないことを人が確認するために、一人または複数人(例えば、ユーザの子ども)を選択するためにプロンプトをレンダリングすることができる。いくつかの実施形態においては、方法320では、図3Bの継続要素「B」を経て図3Aの継続要素「B」まで進むことができる。その代わりに、場合によっては、方法320では、作業312から作業314までを進むことができる。 Method 300 may proceed from operation 308 to operation 310, which may include determining whether one or more conditions are met. Upon determining that one or more conditions are not met, method 300 may return to operation 308 . Alternatively, upon determining that one or more conditions are met, the method illustrated by method 320 in FIG. Task 312 may include causing the camera to capture image data as requested by the user. For example, if the request is specific to one picture, the automated assistant can cause the camera to capture one or more images that can be captured in the image with prior permission from any person. Alternatively, if the request is video-specific, the automated assistant can cause the camera to capture one or more videos. Before capturing a photo or video, one or more persons (e.g., user A prompt can be rendered to select a child of In some embodiments, method 320 may proceed through continuation element 'B' in FIG. 3B to continuation element 'B' in FIG. 3A. Alternatively, method 320 may proceed from operation 312 to operation 314 in some cases.
作業314は、1つまたは複数の条件がもう満たされていないかどうかを判断することを含むことができる。例えば、上に述べた例の第1の条件および第2の条件がもう満たされていないと、方法320では、場合による作業316まで進むことができる。作業316は、カメラに、ユーザからのリクエストに従って画像データを取り込むのを止めさせることを含むことができる。但し、第1の条件および第2の条件が満たされたままであると、方法320では、作業312に戻ることができる。このように、ユーザは、必ずしも絶えず特定のユニークな瞬間を探す必要はなく、ある一定の無二の瞬間を検出するのに自動アシスタントに依存することができる。 Operation 314 may include determining whether one or more conditions are no longer met. For example, method 320 may proceed to optional operation 316 if the first and second conditions of the example discussed above are no longer met. Task 316 may include causing the camera to stop capturing image data in accordance with a request from the user. However, if the first and second conditions remain satisfied, method 320 may return to operation 312 . In this way, the user does not necessarily have to constantly search for specific unique moments, but can rely on the automated assistant to detect certain unique moments.
図4は、コンピュータシステム例410のブロック図400である。コンピュータシステム410は、通常、いくつかの周辺機器とバスサブシステム412を介して通信する少なくとも1つのプロセッサ414を含む。これらの周辺機器は、例えば、メモリ425およびファイルストレージサブシステム426が挙げられるストレージサブシステム、ユーザインターフェース出力デバイス420、ユーザインターフェース入力デバイス422、およびネットワークインターフェースサブシステム416を挙げてもよい。入力デバイスおよび出力デバイスにより、ユーザはコンピュータシステム216と対話するのが可能になる。ネットワークインターフェースサブシステム416は、外側のネットワークへのインターフェースを提供し、その他のコンピュータシステムにある対応するインターフェースデバイスに結合される。 FIG. 4 is a block diagram 400 of an example computer system 410. As shown in FIG. Computer system 410 typically includes at least one processor 414 that communicates with a number of peripherals via bus subsystem 412 . These peripherals may include, for example, storage subsystems including memory 425 and file storage subsystem 426 , user interface output device 420 , user interface input device 422 , and network interface subsystem 416 . Input and output devices allow users to interact with computer system 216 . Network interface subsystem 416 provides an interface to an external network and couples to corresponding interface devices in other computer systems.
ユーザインターフェース入力デバイス422は、キーボード、マウス、トラックボール、タッチバッド、もしくはグラフィックスタブレットなどのポインティングデバイス、スキャナ、ディスプレイに組み込まれたタッチスクリーン、声認識システムなどのオーディオ入力デバイス、マイク、および/またはそれ以外のタイプの入力デバイスを挙げてもよい。大抵、「入力デバイス」という語を使うとき、それは、情報をコンピュータシステム410にまたは通信ネットワークに入力するすべての考えられるタイプのデバイスおよび方法を含むことを意図している。 User interface input devices 422 may include pointing devices such as keyboards, mice, trackballs, touchpads, or graphics tablets, scanners, touchscreens integrated into displays, audio input devices such as voice recognition systems, microphones, and/or Other types of input devices may be mentioned. Generally, when the term "input device" is used, it is intended to include all possible types of devices and methods for entering information into the computer system 410 or into a communication network.
ユーザインターフェース出力デバイス420は、ディスプレイサブシステム、プリンタ、ファックス機、またはオーディオ出力デバイスなどの非視覚ディスプレイを含んでもよい。ディスプレイサブシステムは、可視画像を生み出すための陰極線管(CRT:Cathode Ray Tube)、液晶ディスプレイ(LCD:Liquid Crystal Display)などのフラットパネルデバイス、投射装置、またはそれ以外の機構を含んでもよい。ディスプレイサブシステムはまた、オーディオ出力デバイスなどの非視覚ディスプレイを提供する。一般に、「出力デバイス」という語を使うとき、それは、コンピュータシステム410からの情報をユーザに、または別の機械もしくはコンピュータシステムに出力するすべての考えられるタイプのデバイスおよび方法を含むことを意図している。 User interface output devices 420 may include display subsystems, printers, fax machines, or non-visual displays such as audio output devices. The display subsystem may include a cathode ray tube (CRT), a flat panel device such as a liquid crystal display (LCD), a projection device, or other mechanism for producing a visible image. The display subsystem also provides non-visual displays such as audio output devices. Generally, when we use the term "output device" it is intended to include all conceivable types of devices and methods of outputting information from computer system 410 to a user or to another machine or computer system. there is
ストレージサブシステム424は、本明細書に記載のモジュールのうちのいくつかまたはすべての機能性を与えるプログラミングおよびデータ構造を格納する。例えば、ストレージサブシステム424は、方法330の選択態様を行い、および/または本明細書で述べる、システム200、コンピューティングデバイス104、コンピューティングデバイス110、コンピューティングデバイス106、ならびに/またはそれ以外の任意のアプリケーション、デバイス、装置、および/もしくはモジュールのうちの1つまたは複数を実装する論理回路を含んでもよい。 Storage subsystem 424 stores programming and data structures that provide the functionality of some or all of the modules described herein. For example, storage subsystem 424 may perform selected aspects of method 330 and/or may perform system 200, computing device 104, computing device 110, computing device 106, and/or any other method described herein. may include logic circuitry that implements one or more of the applications, devices, apparatuses, and/or modules of
このようなソフトウェアモジュールは、通常、プロセッサ414単独でまたはそれ以外のプロセッサとの組み合わせで実行される。ストレージサブシステム424に使用されるメモリ425は、プログラム実行中の命令およびデータの格納用のメインランダムアクセスメモリ(RAM:Random Access Memory)430と、固定命令が格納される読み取り専用メモリ(ROM:Read Only Memory)432と含むいくつかのメモリを含むことができる。ファイルストレージサブシステム426は、プログラムおよびデータファイルに持続ストレージをもたらすことができ、ファイルストレージサブシステム426は、ハードディスクドライブ、フロッピーディスクドライブと関連リムーバブルメディア、CD-ROMドライブ、光ドライブ、またはリムーバブルメディアカートリッジを挙げてもよい。特定の実装形態の機能性を実装するこのようなモジュールは、ストレージサブシステム242にファイルストレージサブシステム426によって格納されても、プロセッサ414がアクセスできるそれ以外の機械に格納されてもよい。 Such software modules are typically executed on processor 414 alone or in combination with other processors. The memory 425 used in the storage subsystem 424 includes main random access memory (RAM) 430 for storing instructions and data during program execution, and read only memory (ROM) where fixed instructions are stored. Only Memory) 432 can be included. File storage subsystem 426 can provide persistent storage for program and data files, and file storage subsystem 426 can include hard disk drives, floppy disk drives and associated removable media, CD-ROM drives, optical drives, or removable media cartridges. may be mentioned. Such modules implementing the functionality of a particular implementation may be stored in storage subsystem 242 by file storage subsystem 426 or on some other machine accessible to processor 414 .
バスサブシステム412は、コンピューティングシステム410の様々な構成要素およびサブシステムに目的通りに互いに通信させる機構を提供する。バスサブシステム412は、1つのみのバスとして概略的に示しているが、バスサブシステムの代替えの実装形態では、複数のバスを使用することができる。 Bus subsystem 412 provides a mechanism for allowing the various components and subsystems of computing system 410 to communicate with each other as desired. Although bus subsystem 412 is shown schematically as only one bus, alternate implementations of the bus subsystem may use multiple buses.
コンピュータシステム410は、ワークステーション、サーバ、コンピューティングクラスタ、ブレードサーバ、サーバファーム、または任意の他のデータ処理システムもしくはコンピューティングデバイスを含む、様々な種類であり得る。コンピュータおよびネットワークの絶え間なく変わる性質に起因して、図4に表すコンピューティングシステム410の説明は、いくつかの実装形態を説明するための単なる具体例として意図されている。図4に表すコンピュータシステムよりも構成要素が多いか少ない、コンピュータシステム410の多くの他の構成が考えられる。 Computer system 410 may be of various types, including workstations, servers, computing clusters, blade servers, server farms, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing system 410 depicted in FIG. 4 is intended only as an example to describe some implementations. Many other configurations of computer system 410 are possible, having more or fewer components than the computer system depicted in FIG.
本明細書に記載のシステムがユーザ(または本明細書では時には「参加者」としている)に関する個人情報を集めるか、または個人情報を利用することができる状況では、プログラムまたは特徴がユーザ情報(例えば、ユーザのソーシャルネットワーク、ソーシャルアクション、もしくは活動に関する情報、職業、ユーザの嗜好、またはユーザのその時の地理上の位置)を集めるかどうかを管理するか、またはユーザにとってより重要であると思われるコンテンツをコンテンツサーバから受信するかどうか、および/またはどのように受信するか、を管理する機会をユーザに与えることができる。また、個人識別情報が取り除かれるように、保存されるか使用される前に1つまたは複数の方法で特定のデータを処理することができる。例えば、ユーザに対して個人識別情報が判断できないようにユーザの身元を処理してもよく、ユーザの特定の地理上の位置が特定できないように地理上の位置情報が得られるユーザの地理上の位置を一般化してもよい(都市、ZIPコード、または州レベルに)。それにより、ユーザは、ユーザに関する情報がどのように集められおよび/または使用されるかを管理することができる。 In situations where the systems described herein collect or utilize personal information about users (or, as we sometimes refer to herein, "participants"), the programs or features may include user information (e.g., , information about your social networks, social actions or activities, your occupation, your preferences, or your current geographic location), or content that we believe is more important to you are received from the content server and/or the opportunity to control how they are received. We may also process certain data in one or more ways before it is stored or used such that personally identifiable information is removed. For example, a user's identity may be processed such that no personally identifiable information can be determined for the user, and a user's geographic location information can be obtained such that the user's specific geographic location cannot be determined. Locations may be generalized (to city, zip code, or state level). It allows the user to control how information about the user is collected and/or used.
本明細書では、いくつかの実装形態について述べ、図示してきたが、機能を果たしおよび/またはその結果を得る様々なそれ以外の手段および/または構造、および/または本明細に記載の利点のうちの1つまたは複数を利用することができ、変形形態および/または修正形態のそれぞれは、本明細書に記載の実装形態の範囲にあると見なす。より一般的には、本明細書に記載のすべてのパラメータ、寸法、材料、および構成は、例示的なものであるつもりであり、実際のパラメータ、寸法、材料、および/または構成が1つまたは複数の教示が使用される1つまたは複数の具体的な用途によって決まる。当業者であれば、本明細書に記載の具体的な実装形態に相当する多くを、決まりきった実験しか使わずに認識するか、または解明することができるであろう。それ故、これまで述べた実装形態は単に例として提示し、添付の特許請求の範囲および均等物の範囲にあり、実装形態が、具体的に説明し、請求項に記載した以外のやり方で実施されてもよい、ということが分かるはずである。本開示の実装形態は、それぞれが本明細書に記載の個々の特徴、システム、物品、材料、キット、および/または方法を対象とする。また、2つ以上のこのような特徴、システム、物品、材料、キット、および/または方法のどのような組み合わせも、このような特徴、システム、物品、材料、キット、および/または方法が互いに矛盾しなければ、本開示の範囲に含まれる。 Although several implementations have been described and illustrated herein, various other means and/or structures for performing and/or obtaining the functions and/or advantages described herein may be realized. may be utilized, and each of the variations and/or modifications is considered within the scope of the implementations described herein. More generally, all parameters, dimensions, materials and configurations described herein are intended to be exemplary and the actual parameters, dimensions, materials and/or configurations may be one or more. It depends on the specific application or applications in which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is therefore intended that the implementations described above be presented as examples only and fall within the scope of the following claims and equivalents, and that implementations may be practiced otherwise than as specifically described and claimed. It should be understood that it may be done. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. Also, any combination of two or more such features, systems, articles, materials, kits, and/or methods is not intended to contradict each other such features, systems, articles, materials, kits, and/or methods. otherwise, it falls within the scope of this disclosure.
いくつかの実施形態においては、1つまたは複数のプロセッサによって実施される方法を、コンピューティングデバイスによって、ユーザからの発話を受け取るなどの作業を含むとして示している。コンピューティングデバイスは、自動アシスタントおよびカメラへのアクセスを提供することができる。この方法にはさらに、発話に基づき、発話が画像および/またはカメラ設定を修正するよう自動アシスタントに求めるリクエストを含むと判断する作業を含めてもよい。発話によって、ユーザが画像を取り込むべきGUI要素を選択するのに先立ち、調整すべき画像の1つまたは複数の性質を明らかにすることができる。方法にはさらに、1つまたは複数の特徴に基づき、ユーザからのリクエストに従って画像データが修正されるようにする作業を含めてもよい。例えば、画像データが、1つまたは複数の画像がカメラに向けてビューフィンダーおよび/またはプレビューウィンドウにレンダリングされるのに対応してもよい。画像データがリクエストに従って修正されていると、ユーザは、適切な画像を取り込むためにGUI要素を選択することができる。 In some embodiments, one or more processor-implemented methods are presented as including tasks such as receiving utterances from a user by a computing device. Computing devices can provide access to automated assistants and cameras. The method may further include determining, based on the utterance, that the utterance includes a request for the automated assistant to modify the image and/or camera settings. Speech may reveal one or more characteristics of the image to be adjusted prior to the user selecting a GUI element to capture the image. The method may further include causing the image data to be modified according to a request from the user based on one or more characteristics. For example, the image data may correspond to one or more images being rendered in the viewfinder and/or preview window facing the camera. Once the image data has been modified according to the request, the user can select GUI elements to capture the appropriate image.
いくつかの実施形態においては、1つまたは複数のプロセッサによって実施される方法を、コンピューティングデバイスにおいて、コンピューティングデバイスを経てアクセスできる自動アシスタントに向けられている発話を受け取るなどの作業を含むとして示している。いくつかの実施形態においては、コンピューティングデバイスはまた、カメラへのアクセスも提供する。方法にはさらに、発話に基づき、1つまたは複数の条件が満たされているかどうかに従ってカメラを制御するよう、ユーザが自動アシスタントに指示していると判断する作業を含んでもよい。いくつかの実施形態においては、1つまたは複数の条件は、発話の自然言語内容において言い表される。方法にはさらに、自動アシスタントが利用できるデータに基づき、1つまたは複数の条件が満たされているかどうかを判断する作業を含んでもよい。いくつかの実施形態においては、方法にはさらに、1つまたは複数の条件が満たされると、カメラに画像データを取り込ませる作業を含んでもよい。 In some embodiments, one or more processor-implemented methods are presented at a computing device as including operations such as receiving an utterance directed at an automated assistant accessible via the computing device. ing. In some embodiments, the computing device also provides access to the camera. The method may further include determining, based on the utterance, that the user is instructing the automated assistant to control the camera according to whether one or more conditions are met. In some embodiments, the one or more conditions are expressed in the natural language content of the utterance. The method may further include determining whether one or more conditions have been met based on data available to the automated assistant. In some embodiments, the method may further include causing the camera to capture image data once one or more conditions are met.
いくつかの実施形態においては、1つまたは複数の条件が満たされているかどうかに従ってカメラを制御するよう自動アシスタントにユーザが指示していると判断することが、発話の自然言語内容に基づき、カメラの作業に基づいている現在の画像データにアクセスすることと、現在の画像データにある1つまたは複数の物に基づき、発話に相当する、オーディオデータの処理にバイアスを掛けることと、を含む。いくつかの実施形態においては、1つまたは複数の条件が満たされているかどうかを判断することが、発話を受け取るのに応じて、コンピューティングデバイスまたは別のコンピューティングデバイスの環境においてオーディオを捉えるそれ以外のオーディオデータを処理することと、1つまたは複数の条件を満たす1つまたは複数の特徴をそれ以外のオーディオデータが含むかどうかを判断することと、を含む。いくつかの実施形態においては、1つまたは複数の条件が満たされているかどうかを判断することが、発話を受け取るのに応じて、コンピューティングデバイスまたは別のコンピューティングデバイスの環境の1つまたは複数の視覚的特徴を取り込むそれ以外の画像データを処理することと、1つまたは複数の視覚特徴が1つまたは複数の条件を満たすかどうかを判断することと、を含む。 In some embodiments, determining that the user has instructed the automated assistant to control the camera according to whether one or more conditions are met is based on the natural language content of the utterance, and biasing processing of the audio data, corresponding to speech, based on one or more objects in the current image data. In some embodiments, determining whether one or more conditions are met includes capturing audio in the environment of a computing device or another computing device in response to receiving speech. and determining whether the other audio data includes one or more features that satisfy the one or more conditions. In some embodiments, determining whether the one or more conditions are satisfied involves one or more of the environments of the computing device or another computing device in response to receiving the utterance. and processing other image data to capture the visual features of and determining whether the one or more visual features satisfy one or more conditions.
いくつかの実施形態においては、カメラに画像データを取り込ませることが、発話の自然言語内容に基づき、カメラの1つまたは複数の設定を修正することを含む。いくつかの実施形態においては、カメラが1つまたは複数の設定に従って動作しているときに画像データが取り込まれる。いくつかの実施形態においては、1つまたは複数の条件が満たされているかどうかを判断することが、発話を受け取るのに応じて、コンピューティングデバイスまたは別のコンピューティングデバイスがアクセスできるアプリケーションの状態を示すアプリケーションデータを処理することと、アプリケーションの状態が1つまたは複数の条件を満たすかどうかを判断することと、を含む。いくつかの実施形態においては、コンピューティングデバイスがポータブルコンピューティングデバイスであり、発話が、ユーザがポータブルコンピューティングデバイスを扱っている間に受け取られる。いくつかの実施形態においては、カメラに画像データを取り込ませることが、発話をすることに続いて、ユーザがコンピューティングデバイスの任意のプログラマブルタッチインターフェースにも直接接触せずに行われる。 In some embodiments, having the camera capture image data includes modifying one or more settings of the camera based on the natural language content of the speech. In some embodiments, image data is captured while the camera is operating according to one or more settings. In some embodiments, determining whether the one or more conditions are met determines the state of an application that the computing device or another computing device can access in response to receiving the utterance. processing the indicated application data; and determining whether the state of the application satisfies one or more conditions. In some embodiments, the computing device is a portable computing device and the speech is received while the user is manipulating the portable computing device. In some embodiments, having the camera capture image data occurs without the user directly touching any programmable touch interface of the computing device following speaking.
他の実装形態において、1つまたは複数のプロセッサによって実施される方法を、コンピューティングデバイスにおいて、ユーザから入力を受信することであって、コンピューティングデバイスは自動アシスタントおよびカメラにアクセスを提供する、受信することなどの作業を含むとして規定している。方法にはさらに、入力に基づき、入力が、1つまたは複数の条件に従ってカメラを操作するよう自動アシスタントに求めるリクエストであると判断する作業を含めてもよい。いくつかの実施形態においては、1つまたは複数の条件が入力の自然言語内容において指定されている。方法はさらに、1つまたは複数の条件に基づき、1つまたは複数の訓練済み機械学習モデルにアクセスする動作であって、自動アシスタントが1つまたは複数の訓練済み機械学習モデルにアクセスし、コンピューティングデバイスまたは別のコンピューティングデバイスの環境の1つまたは複数の特徴を識別するのを助ける、アクセスする作業を含んでもよい。方法にはさらに、1つまたは複数の訓練済み機械学習モデルを使用して、コンピューティングデバイスまたは別のコンピューティングデバイスの環境の1つまたは複数の現在の特徴を特徴付けるデータを処理する作業を含めてもよい。方法にはさらに、データに基づき、環境の1つまたは複数のその時の特徴が1つまたは複数の条件を満たすかどうかを判断する作業であって、コンピューティングデバイスまたは別のコンピューティングデバイスの環境が1つまたは複数の明示の特徴を示しているときに1つまたは複数の条件が満たされる、判断する作業を含めてもよい。方法にはさらに、1つまたは複数の条件が満たされていると判断されると、カメラに画像データを取り込ませる作業を含めてもよい。 In other implementations, the method implemented by one or more processors is receiving input from a user at a computing device, the computing device providing access to an automated assistant and a camera. It is stipulated that it includes work such as The method may further include, based on the input, determining that the input is a request for an automated assistant to operate the camera according to one or more conditions. In some embodiments, one or more conditions are specified in the natural language content of the input. The method further comprises an act of accessing the one or more trained machine learning models based on the one or more conditions, wherein the automated assistant accesses the one or more trained machine learning models and computes the It may involve accessing to help identify one or more characteristics of the environment of the device or another computing device. The method further includes using one or more trained machine learning models to process data characterizing one or more current features of the environment of the computing device or another computing device. good too. The method further includes the act of determining, based on the data, whether one or more current characteristics of the environment satisfy one or more conditions, wherein the computing device or the environment of another computing device It may include the task of determining that one or more conditions are satisfied when exhibiting one or more explicit characteristics. The method may further include causing the camera to capture image data upon determining that one or more conditions are met.
いくつかの実施形態においては、環境の現在の特徴が特定の性質を呈しているときに1つまたは複数の条件のうちのある条件が満たされ、1つまたは複数の現在の特徴を特徴付けるデータを処理することが、環境の現在の特徴の性質に対して信頼スコアを割り当ることであって、信頼スコアが閾値スコアを満たすと、条件が満たされる、割り当てることを含む。いくつかの実施形態においては、入力が、1つまたは複数の条件に従ってカメラを働かせるよう自動アシスタントに求める前記リクエストであると判断することが、環境の現在の特徴に基づき、入力の自然言語理解にバイアスを掛けることを含む。いくつかの実施形態においては、方法にはさらに、入力または別の入力が、画像データが修正されるようにするよう自動アシスタントに求める別のリクエストを含むと判断することと、1つまたは複数の条件が満たされていると判断すると、カメラによって取り込まれた画像データが入力またはそれ以外の入力に従って修正されるようにすることと、を含めてもよい。いくつかの実施形態においては、その他のリクエストがユーザによって与えられたそれ以外の入力において具体化され、カメラが画像データを取り込んでいるときに、それ以外の入力が受け取られる。いくつかの実施形態においては、カメラに画像データを取り込ませることが、画像データを取り込むのを始めるのにユーザがコンピューティングデバイスのタッチインターフェースに直接接触せずに行われる。 In some embodiments, a condition of the one or more conditions is met when current features of the environment exhibit a particular property, and data characterizing the one or more current features is generated. Processing includes assigning a confidence score to a property of the current feature of the environment, wherein the condition is satisfied if the confidence score meets the threshold score. In some embodiments, determining that the input is said request to an automated assistant to activate the camera according to one or more conditions is based on current characteristics of the environment and relying on natural language understanding of the input. Including biasing. In some embodiments, the method further includes determining that the input or another input comprises another request to the automated assistant to cause the image data to be modified; and causing image data captured by the camera to be modified according to the input or otherwise upon determining that the conditions are met. In some embodiments, the other requests are embodied in other inputs provided by the user, and the other inputs are received when the camera is capturing image data. In some embodiments, causing the camera to capture image data occurs without the user directly touching the touch interface of the computing device to initiate image data capture.
さらに他の実装形態において、1つまたは複数のプロセッサによって実施される方法を、コンピューティングデバイスによって、ユーザから発話を受け取ることであって、コンピューティングデバイスが自動アシスタントおよびカメラにアクセスする、受け取ることなどの作業を含むとして示している。方法にはさらに、発話に基づき、発話が、カメラを制御するよう自動アシスタントに求めるリクエストを含むと判断する作業であって、満たされると、自動アシスタントに、カメラを利用する作業の遂行を初期化させる1つまたは複数の条件を発話が指定する、判断する作業を含めてもよい。方法にはさらに、1つまたは複数の条件に基づき、1つまたは複数の条件が満たされているかどうかを判断するのを推し進めるのに、カメラを使用して起こされた画像データを処理する作業を含めてもよい。方法にはさらに、1つまたは複数の条件が満たされていると判断すると、自動アシスタントに、カメラを使用する作業の遂行を初期化させる作業であって、作業を初期化することがカメラにさらなる画像データを取り込ませる、初期化させる作業を含めてもよい。 In still other implementations, a method implemented by one or more processors is receiving, by a computing device, an utterance from a user, the computing device accessing, receiving, etc. an automated assistant and a camera. shown as including the work of The method further includes, based on the utterance, the task of determining that the utterance includes a request for the automated assistant to control the camera, and, when satisfied, initiating the automated assistant to perform the task utilizing the camera. It may include the task of determining that the utterance specifies one or more conditions to cause. The method further includes processing image data generated using the camera to facilitate determining whether the one or more conditions are satisfied based on the one or more conditions. may be included. The method further includes, upon determining that the one or more conditions are met, causing the automated assistant to initiate performance of a task using the camera, wherein initializing the task further comprises: It may include the work of importing image data and initializing it.
いくつかの実施形態においては、さらなる画像データは、映像データが挙げられ、自動アシスタントに、カメラを使用した作業の遂行を初期化させることが、カメラに、1つまたは複数の条件が満たされている期間に映像データを取り込ませることを含む。いくつかの実施形態においては、方法にはさらに、1つまたは複数の条件に基づき、1つまたは複数の訓練済み機械学習モデルを識別する作業であって、画像データを処理することが、1つまたは複数の訓練済み機械学習モデルを使用して行われ、1つまたは複数の機械学習が、1つまたは複数の条件を満たす環境特徴を特徴付ける訓練データを使用して訓練されている、識別する作業を含めてもよい。いくつかの実施形態においては、方法にはさらに、1つまたは複数の条件が満たされているかどうかを判断するのに続いて、1つまたは複数の条件がもう満たされていないかどうかを判断するのを推し進めるのに別個の画像データを処理する作業であって、別個の画像データがカメラを使用して取り込まれる、処理する作業を含めてもよい。いくつかの実施形態においては、方法にはさらに、1つまたは複数の条件が満たされていると判断するのに続いて、1つまたは複数の条件がもう満たされていないと判断する作業と、1つまたは複数の条件がもう満たされていないのに基づき、コンピューティングデバイスに、さらなる画像データおよび別個の画像データの少なくとも一部を画像ファイルとして保存させる作業とを含めてもよい。いくつかの実施形態においては、自動アシスタントに、カメラを使用した作業の遂行を初期化させることが、ユーザが直にコンピューティングデバイスのタッチインターフェースを触って、さらなる画像データを取り込むのを始めることも、さらなる画像データを取り込むのを止めることもせずとも行われる。 In some embodiments, the additional image data includes video data, causing the automated assistant to initiate the performance of a task using the camera when the camera detects that one or more conditions are met. including capturing video data during a period of time. In some embodiments, the method further comprises identifying one or more trained machine learning models based on one or more conditions, and processing the image data. or performed using multiple trained machine learning models, one or more machine learning being trained using training data characterizing environmental features that satisfy one or more conditions; may be included. In some embodiments, the method further includes determining whether the one or more conditions are satisfied followed by determining whether the one or more conditions are no longer satisfied. may include processing separate image data to further the processing where the separate image data is captured using a camera. In some embodiments, the method further comprises determining that the one or more conditions are met, followed by determining that the one or more conditions are no longer met; An act of causing the computing device to save at least a portion of the additional image data and the separate image data as an image file based on the one or more conditions no longer being met may also be included. In some embodiments, having the automated assistant initiate the performance of a task using the camera may cause the user to directly touch the touch interface of the computing device to begin capturing further image data. , without stopping the acquisition of further image data.
102 ユーザ
104 コンピューティングデバイス
106 コンピューティングデバイス
108 鳥小屋
110 コンピューティングデバイス
112 ユーザリクエストデータ
114 条件データ
116 モデルデータ
118 発話
122 メディアデータ
124 条件検証データ
126 カメラ制御命令
128 鳥
130 コンピューティングデバイス、グラフィカルユーザインターフェース(GUI)
200 システム
202 コンピューティングデバイス
204 自動アシスタント
206 入力処理エンジン
208 音声処理エンジン
210 データパージングエンジン
212 パラメータエンジン
214 出力生成エンジン
218 条件エンジン
220 アシスタントインターフェース
222 アシスタント呼び出しエンジン
224 特徴スコアエンジン
226 モデル同定エンジン
228 リクエストバイアシングエンジン
230 アプリケーションデータ
232 デバイスデータ
234 アプリケーション、特定のアプリケーション、実行アプリケーション
236 コンテキストデータ
238 アシスタントデータ
240 カメラ制御エンジン
242 ストレージサブシステム
300 方法
320 方法
400 ブロック図
410 コンピュータシステム
412 バスサブシステム
414 プロセッサ
416 ネットワークインターフェースサブシステム
420 ユーザインターフェース出力デバイス
422 ユーザインターフェース入力デバイス
424 ストレージサブシステム
425 メモリ
426 ファイルストレージサブシステム
430 メインランダムアクセスメモリ(RAM)
432 読み取り専用メモリ(ROM)
102 users
104 Computing Devices
106 Computing Devices
108 Aviary
110 Computing Devices
112 User request data
114 Condition data
116 model data
118 Utterance
122 media data
124 condition verification data
126 camera control instructions
128 bird
130 Computing Devices, Graphical User Interfaces (GUI)
200 systems
202 Computing Devices
204 Automated Assistant
206 Input Processing Engine
208 Audio Processing Engine
210 Data Parsing Engine
212 parameter engine
214 Output Generation Engine
218 Condition Engine
220 assistant interface
222 Assistant Call Engine
224 Feature Score Engine
226 model identification engine
228 Request Biasing Engine
230 Application data
232 device data
234 applications, specific applications, running applications
236 Context Data
238 Assistant Data
240 camera control engine
242 storage subsystem
300 ways
320 ways
400 block diagram
410 computer system
412 Bus Subsystem
414 processor
416 Network Interface Subsystem
420 User Interface Output Device
422 User Interface Input Device
424 storage subsystem
425 memory
426 File Storage Subsystem
430 Main Random Access Memory (RAM)
432 Read Only Memory (ROM)
Claims (22)
コンピューティングデバイスにおいて、前記コンピューティングデバイスを介してアクセスできる自動アシスタントに向けられている発話を受信するステップであって、
前記コンピューティングデバイスがカメラにもアクセスを提供する、ステップと、
前記発話に基づき、1つまたは複数の条件が満たされているかどうかに従って、ユーザが前記自動アシスタントに前記カメラを制御するよう指示していると判断するステップであって、
前記1つまたは複数の条件が前記発話の自然言語内容において言い表されている、ステップと、
前記自動アシスタントが利用できるデータに基づき、前記1つまたは複数の条件が満たされているかどうかを判断するステップと、
前記1つまたは複数の条件が満たされると、
前記カメラに画像データを取り込ませるステップと、を含む、方法。 A method, performed by one or more processors, comprising:
receiving, at a computing device, an utterance directed to an automated assistant accessible via the computing device, comprising:
said computing device also providing access to a camera;
determining that the user is instructing the automated assistant to control the camera based on the utterance and according to whether one or more conditions are met;
wherein said one or more conditions are expressed in the natural language content of said utterance;
determining whether the one or more conditions are met based on data available to the automated assistant;
If said one or more conditions are met,
and c. causing the camera to capture image data.
前記発話の前記自然言語内容に基づき、前記カメラの動作に基づいている現在の画像データにアクセスするステップと、
前記現在の画像データにある1つまたは複数の物に基づき、前記発話に対応するオーディオデータの処理にバイアスを掛けるステップと、を含む、請求項1に記載の方法。 determining that the user instructs the automated assistant to control the camera according to whether one or more conditions are met;
accessing current image data based on motion of the camera based on the natural language content of the utterance;
biasing processing of audio data corresponding to said speech based on one or more things in said current image data.
前記発話を受け取ることに応じて、前記コンピューティングデバイスまたは他のコンピューティングデバイスの環境においてオーディオを捉える他のオーディオデータを処理するステップと、
前記他のオーディオデータが前記1つまたは複数の条件を満たす1つまたは複数のオーディオ特徴を含むかどうかを判断するステップと、を含む、請求項1または2に記載の方法。 determining whether the one or more conditions are met,
processing other audio data capturing audio in the environment of the computing device or other computing device in response to receiving the utterance;
and determining whether said other audio data includes one or more audio features satisfying said one or more conditions.
前記発話を受け取ることに応じて、前記コンピューティングデバイスまたは他のコンピューティングデバイスの環境の1つまたは複数の視覚特徴を捉える他の画像データを処理するステップと、
前記1つまたは複数の視覚特徴が前記1つまたは複数の条件を満たすかどうかを判断するステップと、を含む、請求項1または2に記載の方法。 determining whether the one or more conditions are met,
processing other image data capturing one or more visual features of an environment of the computing device or other computing device in response to receiving the utterance;
and determining whether said one or more visual features satisfy said one or more conditions.
前記発話の前記自然言語内容に基づき、前記カメラの1つまたは複数の設定を修正するステップであって、
前記カメラが前記1つまたは複数の設定に従って作動しているときに前記画像データが取り込まれる、ステップを含む、
請求項1から4のいずれか一項に記載の方法。 causing the camera to capture the image data,
modifying one or more settings of the camera based on the natural language content of the utterance, comprising:
wherein said image data is captured when said camera is operating according to said one or more settings;
5. A method according to any one of claims 1-4.
前記発話を受信することに応じて、前記コンピューティングデバイスまたは他のコンピューティングデバイスを介してアクセスできるアプリケーションの状態を示すアプリケーションデータを処理するステップと、
前記アプリケーションの前記状態が前記1つまたは複数の条件を満たすかどうかを判断するステップと、を含む、
請求項1または2に記載の方法。 determining whether the one or more conditions are met,
processing application data indicating the state of applications accessible via the computing device or other computing devices in response to receiving the utterance;
determining whether the state of the application satisfies the one or more conditions;
3. A method according to claim 1 or 2.
コンピューティングデバイスにおいて、ユーザから入力を受信するステップであって、
前記コンピューティングデバイスが自動アシスタントおよびカメラにアクセスを提供する、ステップと、
前記入力に基づき、前記入力が、1つまたは複数の条件に従って、前記カメラを動作するための、前記自動アシスタントに対するリクエストであると判断するステップであって、
前記1つまたは複数の条件が前記入力の自然言語内容において指定される、ステップと、
前記1つまたは複数の条件に基づき、1つまたは複数の訓練済み機械学習モデルにアクセスするステップであって、
前記自動アシスタントが前記1つまたは複数の訓練済み機械学習モデルにアクセスし、前記コンピューティングデバイスまたは他のコンピューティングデバイスの環境の1つまたは複数の特徴を識別することを助ける、ステップと、
前記1つまたは複数の訓練済み機械学習モデルを使用して、前記コンピューティングデバイスまたは他のコンピューティングデバイスの前記環境の1つまたは複数の現在の特徴を特徴付けするデータを処理するステップと、
前記データに基づき、前記環境の前記1つまたは複数の現在の特徴が前記1つまたは複数の条件を満たすかどうかを判断するステップであって、
前記コンピューティングデバイスまたは前記他のコンピューティングデバイスの前記環境が1つまたは複数の特定された特徴を示しているときに前記1つまたは複数の条件が満たされる、ステップと、
前記1つまたは複数の条件が満たされていると判断されると、
前記カメラに画像データを取り込ませるステップと、を含む、方法。 A method performed by one or more processors comprising:
At a computing device, receiving input from a user, comprising:
said computing device providing access to an automated assistant and a camera;
determining, based on the input, that the input is a request to the automated assistant to operate the camera according to one or more conditions;
wherein said one or more conditions are specified in natural language content of said input;
accessing one or more trained machine learning models based on the one or more conditions, comprising:
the automated assistant accessing the one or more trained machine learning models to assist in identifying one or more characteristics of the environment of the computing device or other computing device;
using the one or more trained machine learning models to process data characterizing one or more current features of the environment of the computing device or other computing device;
based on the data, determining whether the one or more current characteristics of the environment satisfy the one or more conditions;
said one or more conditions are met when said environment of said computing device or said other computing device exhibits one or more identified characteristics;
If it is determined that said one or more conditions are satisfied,
and c. causing the camera to capture image data.
前記1つまたは複数の現在の特徴を特徴付ける前記データを処理するステップが、
前記環境の前記現在の特徴の性質に対して信頼スコアを割り当てるステップであって、
前記信頼スコアが閾値スコアを満たすと、前記条件が満たされる、ステップを含む、請求項9に記載の方法。 a condition of said one or more conditions is met when current characteristics of said environment exhibit a particular property;
processing the data characterizing the one or more current features,
assigning a confidence score to a property of the current feature of the environment, comprising:
10. The method of claim 9, comprising: if the confidence score meets a threshold score, then the condition is met.
前記環境の前記現在の特徴に基づき、前記入力の自然言語理解にバイアスを掛けるステップを含む、請求項10に記載の方法。 determining that the input is the request to the automated assistant to operate the camera according to the one or more conditions;
11. The method of claim 10, comprising biasing the natural language understanding of the input based on the current characteristics of the environment.
前記1つまたは複数の条件が満たされていると判断すると、
前記カメラによって取り込まれた前記画像データが前記入力または前記他の入力に従って修正されるようにするステップと、をさらに含む、請求項9から11のいずれか一項に記載の方法。 determining that the input or other input comprises another request to the automated assistant to cause the image data to be modified;
Upon determining that one or more of the foregoing conditions are satisfied,
12. A method according to any one of claims 9 to 11, further comprising causing said image data captured by said camera to be modified according to said input or said other input.
前記カメラが前記画像データを取り込んでいるときに前記他の入力が受信される、請求項9から12のいずれか一項に記載の方法。 said other request is embodied in said other input provided by said user;
13. The method of any one of claims 9-12, wherein the other input is received when the camera is capturing the image data.
コンピューティングデバイスによって、ユーザから発話を受け取るステップであって、
前記コンピューティングデバイスが自動アシスタントおよびカメラにアクセスを提供する、ステップと、
前記発話に基づき、前記発話が、前記カメラを制御するための前記自動アシスタントに対するリクエストを含むと判断するステップであって、
前記発話は、満たされたときに前記自動アシスタントに、前記カメラを利用する動作の実行を初期化させる1つまたは複数の条件を特定する、ステップと、
前記1つまたは複数の条件に基づき、前記1つまたは複数の条件が満たされているかどうかを判断することを推進するのに、前記カメラを使用して生成された画像データを処理するステップと、
前記1つまたは複数の条件が満たされていると判断すると、
前記自動アシスタントに、前記カメラを使用する前記作業の実行を初期化させるステップであって、
前記作業を初期化するステップが、前記カメラにさらなる画像データを取り込ませる、ステップと、を含む、方法。 A method performed by one or more processors comprising:
receiving, by a computing device, an utterance from a user, comprising:
said computing device providing access to an automated assistant and a camera;
based on the utterance, determining that the utterance comprises a request to the automated assistant to control the camera;
said utterance identifies one or more conditions that, when met, cause said automated assistant to initiate execution of an action utilizing said camera;
Based on the one or more conditions, processing image data generated using the camera to facilitate determining whether the one or more conditions are met;
Upon determining that one or more of the foregoing conditions are satisfied,
causing the automated assistant to initiate execution of the task using the camera, comprising:
and c. initializing the operation includes causing the camera to capture additional image data.
前記自動アシスタントに、前記カメラを使用する前記作業の遂行を初期化させるステップが、
前記カメラに、前記1つまたは複数の条件が満たされた時点で前記映像データを取り込ませるステップを含む、請求項15に記載の方法。 The further image data includes video data,
causing the automated assistant to initiate performance of the task using the camera;
16. The method of claim 15, comprising causing the camera to capture the video data when the one or more conditions are met.
前記画像データを処理するステップが、前記1つまたは複数の訓練済み機械学習モデルを使用して実行され、
前記1つまたは複数の訓練済み機械学習モデルが、前記1つまたは複数の条件を満たす環境特徴を特徴付ける訓練データを使用して訓練される、請求項15または16に記載の方法。 further comprising identifying one or more trained machine learning models based on the one or more conditions;
processing the image data is performed using the one or more trained machine learning models;
17. The method of claim 15 or 16, wherein the one or more trained machine learning models are trained using training data characterizing environmental features that satisfy the one or more conditions.
前記1つまたは複数の条件がもう満たされていないかどうかを判断することを推進するために別個の画像データを処理するステップであって、
前記別個の画像データが前記カメラを使用して取り込まれる、ステップをさらに含む、請求項15から17のいずれか一項に記載の方法。 following the step of determining that the one or more conditions are met;
processing separate image data to facilitate determining whether the one or more conditions are no longer satisfied, comprising:
18. The method of any one of claims 15-17, further comprising the step of capturing said separate image data using said camera.
前記1つまたは複数の条件がもう満たされていないと判断するステップと、
前記1つまたは複数の条件がもう満たされていないことに基づき、前記コンピューティングデバイスに、画像ファイルとして、前記さらなる画像データおよび前記別個の画像データの少なくとも一部を保存させるステップとをさらに含む、請求項18に記載の方法。 following the step of determining that the one or more conditions are met;
determining that the one or more conditions are no longer met;
and causing the computing device to save at least a portion of the additional image data and the separate image data as an image file based on the one or more conditions no longer being met. 19. The method of claim 18.
実行されると、前記少なくとも1つのプロセッサに、請求項1から20のいずれか一項に対応する作業を行わせる命令を格納するメモリと、を含むシステム。 at least one processor;
a memory storing instructions which, when executed, cause the at least one processor to perform the work corresponding to any one of claims 1 to 20.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/103,805 US11558546B2 (en) | 2020-11-24 | 2020-11-24 | Conditional camera control via automated assistant commands |
US17/103,805 | 2020-11-24 | ||
PCT/US2021/055748 WO2022115181A1 (en) | 2020-11-24 | 2021-10-20 | Conditional camera control via automated assistant commands |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2023534889A true JP2023534889A (en) | 2023-08-15 |
Family
ID=78622044
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022571847A Pending JP2023534889A (en) | 2020-11-24 | 2021-10-20 | Conditional camera control with auto-assistant commands |
Country Status (6)
Country | Link |
---|---|
US (3) | US11558546B2 (en) |
EP (1) | EP4143675A1 (en) |
JP (1) | JP2023534889A (en) |
KR (1) | KR20230008172A (en) |
CN (1) | CN115720655A (en) |
WO (1) | WO2022115181A1 (en) |
Family Cites Families (43)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030117505A1 (en) * | 2001-12-20 | 2003-06-26 | Sasaki Gary David | Intermediate memory for a digital camera |
US7751628B1 (en) * | 2001-12-26 | 2010-07-06 | Reisman Richard R | Method and apparatus for progressively deleting media objects from storage |
US8292433B2 (en) * | 2003-03-21 | 2012-10-23 | Queen's University At Kingston | Method and apparatus for communication between humans and devices |
US7697827B2 (en) * | 2005-10-17 | 2010-04-13 | Konicek Jeffrey C | User-friendlier interfaces for a camera |
US11287847B2 (en) * | 2006-02-15 | 2022-03-29 | Virtual Video Reality by Ritchey, LLC (VVRR, LLC) | Human-like emulation enterprise system and method |
JP5413034B2 (en) * | 2009-08-03 | 2014-02-12 | 船井電機株式会社 | Mobile phone, imaging system using mobile phone, and imaging method using mobile phone |
US9179104B2 (en) * | 2011-10-13 | 2015-11-03 | At&T Intellectual Property I, Lp | Method and apparatus for managing a camera network |
US9104467B2 (en) * | 2012-10-14 | 2015-08-11 | Ari M Frank | Utilizing eye tracking to reduce power consumption involved in measuring affective response |
TWI527449B (en) * | 2013-10-29 | 2016-03-21 | 信泰光學（深圳）有限公司 | Wireless control systems for cameras, cameras which are wirelessly controlled by control devices, and operational methods thereof |
KR20150112337A (en) * | 2014-03-27 | 2015-10-07 | 삼성전자주식회사 | display apparatus and user interaction method thereof |
US9942583B2 (en) * | 2014-11-18 | 2018-04-10 | The Invention Science Fund Ii, Llc | Devices, methods and systems for multi-user capable visual imaging arrays |
US11231826B2 (en) | 2015-03-08 | 2022-01-25 | Google Llc | Annotations in software applications for invoking dialog system functions |
US9769367B2 (en) * | 2015-08-07 | 2017-09-19 | Google Inc. | Speech and computer vision-based control |
US10127906B1 (en) * | 2015-12-28 | 2018-11-13 | Amazon Technologies, Inc. | Naming devices via voice commands |
US9922648B2 (en) | 2016-03-01 | 2018-03-20 | Google Llc | Developer voice actions system |
US10049670B2 (en) | 2016-06-06 | 2018-08-14 | Google Llc | Providing voice action discoverability example for trigger term |
US10726836B2 (en) * | 2016-08-12 | 2020-07-28 | Kt Corporation | Providing audio and video feedback with character based on voice command |
US10360910B2 (en) * | 2016-08-29 | 2019-07-23 | Garmin Switzerland Gmbh | Automatic speech recognition (ASR) utilizing GPS and sensor data |
US10726835B2 (en) * | 2016-12-23 | 2020-07-28 | Amazon Technologies, Inc. | Voice activated modular controller |
KR101889279B1 (en) * | 2017-01-16 | 2018-08-21 | 주식회사 케이티 | System and method for provining sercive in response to voice command |
US10467509B2 (en) | 2017-02-14 | 2019-11-05 | Microsoft Technology Licensing, Llc | Computationally-efficient human-identifying smart assistant computer |
US10902855B2 (en) * | 2017-05-08 | 2021-01-26 | Motorola Mobility Llc | Methods and devices for negotiating performance of control operations with acoustic signals |
US11436417B2 (en) * | 2017-05-15 | 2022-09-06 | Google Llc | Providing access to user-controlled resources by automated assistants |
US10275651B2 (en) | 2017-05-16 | 2019-04-30 | Google Llc | Resolving automated assistant requests that are based on image(s) and/or other sensor data |
US9973732B1 (en) | 2017-07-27 | 2018-05-15 | Amazon Technologies, Inc. | Device selection for video based communications |
US11169772B2 (en) * | 2018-03-19 | 2021-11-09 | Gopro, Inc. | Image capture device control using mobile platform voice recognition |
US10902626B2 (en) * | 2018-04-11 | 2021-01-26 | International Business Machines Corporation | Preventing intrusion during video recording or streaming |
WO2019203795A1 (en) * | 2018-04-16 | 2019-10-24 | Google Llc | Automatically determining language for speech recognition of spoken utterance received via an automated assistant interface |
US11922934B2 (en) * | 2018-04-19 | 2024-03-05 | Microsoft Technology Licensing, Llc | Generating response in conversation |
JP7263505B2 (en) | 2018-05-04 | 2023-04-24 | グーグル エルエルシー | Adaptation of automatic assistant functions without hotwords |
US11119725B2 (en) * | 2018-09-27 | 2021-09-14 | Abl Ip Holding Llc | Customizable embedded vocal command sets for a lighting and/or other environmental controller |
US11158004B2 (en) * | 2018-11-05 | 2021-10-26 | EIG Technology, Inc. | Property assessment using a virtual assistant |
US11269804B1 (en) * | 2019-02-25 | 2022-03-08 | Amazon Technologies, Inc. | Hardware adapter to connect with a distributed network service |
US11687316B2 (en) | 2019-02-28 | 2023-06-27 | Qualcomm Incorporated | Audio based image capture settings |
US11240560B2 (en) * | 2019-05-06 | 2022-02-01 | Google Llc | Assigning priority for an automated assistant according to a dynamic user queue and/or multi-modality presence detection |
JP7226073B2 (en) * | 2019-05-07 | 2023-02-21 | 富士フイルムビジネスイノベーション株式会社 | Information processing device, information processing system and program |
WO2021033889A1 (en) * | 2019-08-20 | 2021-02-25 | Samsung Electronics Co., Ltd. | Electronic device and method for controlling the electronic device |
KR102201858B1 (en) * | 2019-08-26 | 2021-01-12 | 엘지전자 주식회사 | Method for editing image based on artificial intelligence and artificial device |
US11553887B2 (en) * | 2020-03-05 | 2023-01-17 | Shenzhen Mindray Bio-Medical Electronics Co., Ltd. | Limited data persistence in a medical imaging workflow |
CN111722824B (en) * | 2020-05-29 | 2024-04-30 | 北京小米松果电子有限公司 | Voice control method, device and computer storage medium |
US11455996B2 (en) * | 2020-07-27 | 2022-09-27 | Google Llc | Automated assistant adaptation of a response to an utterance and/or of processing of the utterance, based on determined interaction measure |
US11488597B2 (en) * | 2020-09-08 | 2022-11-01 | Google Llc | Document creation and editing via automated assistant interactions |
US11748660B2 (en) * | 2020-09-17 | 2023-09-05 | Google Llc | Automated assistant training and/or execution of inter-user procedures |
-
2020
- 2020-11-24 US US17/103,805 patent/US11558546B2/en active Active
-
2021
- 2021-10-20 JP JP2022571847A patent/JP2023534889A/en active Pending
- 2021-10-20 EP EP21807392.2A patent/EP4143675A1/en active Pending
- 2021-10-20 CN CN202180039193.XA patent/CN115720655A/en active Pending
- 2021-10-20 KR KR1020227042613A patent/KR20230008172A/en unknown
- 2021-10-20 WO PCT/US2021/055748 patent/WO2022115181A1/en unknown
-
2023
- 2023-01-13 US US18/097,150 patent/US11765452B2/en active Active
- 2023-08-08 US US18/446,381 patent/US20240022809A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR20230008172A (en) | 2023-01-13 |
US20220166919A1 (en) | 2022-05-26 |
CN115720655A (en) | 2023-02-28 |
US20230156322A1 (en) | 2023-05-18 |
EP4143675A1 (en) | 2023-03-08 |
US11765452B2 (en) | 2023-09-19 |
US20240022809A1 (en) | 2024-01-18 |
WO2022115181A1 (en) | 2022-06-02 |
US11558546B2 (en) | 2023-01-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7418563B2 (en) | Using automated assistant feature correction for on-device machine learning model training | |
JP2021524975A (en) | Invoking automation assistant features based on detected gestures and gaze | |
JP2023089115A (en) | Hot-word free adaptation of automated assistant function | |
JP2022119878A (en) | Generating and/or adapting contents of automated assistant according to distance between user and automated assistant interface | |
JP2023014167A (en) | Adaptation to automated assistant based on detected mouth motion and/or gaze | |
CN114041283A (en) | Automated assistant engaged with pre-event and post-event input streams | |
US11076091B1 (en) | Image capturing assistant | |
JP7371135B2 (en) | Speaker recognition using speaker specific speech models | |
JP2024506778A (en) | Passive disambiguation of assistant commands | |
US20220172727A1 (en) | Detecting and suppressing commands in media that may trigger another automated assistant | |
CN111506183A (en) | Intelligent terminal and user interaction method | |
EP3939033B1 (en) | Automated assistant control of external applications lacking automated assistant application programming interface functionality | |
US20220310089A1 (en) | Selectively invoking an automated assistant based on detected environmental conditions without necessitating voice-based invocation of the automated assistant | |
JP2023534889A (en) | Conditional camera control with auto-assistant commands | |
US11743588B1 (en) | Object selection in computer vision | |
US20230252995A1 (en) | Altering a candidate text representation, of spoken input, based on further spoken input | |
US20230197071A1 (en) | Accelerometer-based endpointing measure(s) and /or gaze-based endpointing measure(s) for speech processing | |
US20230035713A1 (en) | Dynamic adaptation of graphical user interface elements by an automated assistant as a user iteratively provides a spoken utterance, or sequence of spoken utterances | |
US20240038246A1 (en) | Non-wake word invocation of an automated assistant from certain utterances related to display content | |
CN112236739B (en) | Adaptive automatic assistant based on detected mouth movement and/or gaze | |
KR20230153450A (en) | Device arbitration for local implementation of automatic speech recognition | |
CN116670637A (en) | Dynamic adaptation of graphical user interface elements by an automated assistant when a user iteratively provides a spoken utterance or sequence of spoken utterances | |
JP2024505794A (en) | Dynamic adaptation of graphical user interface elements by an automated assistant as the user repeatedly provides an audio utterance or sequence of audio utterances | |
JP2024519261A (en) | Providing a secondary automated assistant with relevant queries based on past interactions |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20230123 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20230123 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20240129 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20240430 |