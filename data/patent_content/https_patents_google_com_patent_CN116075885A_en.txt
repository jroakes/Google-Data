CN116075885A - Bit vector based content matching for third party digital assistant actions - Google Patents
Bit vector based content matching for third party digital assistant actions Download PDFInfo
- Publication number
- CN116075885A CN116075885A CN202180017772.4A CN202180017772A CN116075885A CN 116075885 A CN116075885 A CN 116075885A CN 202180017772 A CN202180017772 A CN 202180017772A CN 116075885 A CN116075885 A CN 116075885A
- Authority
- CN
- China
- Prior art keywords
- data processing
- processing system
- action
- content
- application
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3329—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
- G06F16/90332—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/225—Feedback of the input speech
Abstract
Matching content to third party digital assistant actions using bit vectors is provided. A system receives an application having voice assistant compatible actions. The system identifies operations in the application. The system identifies content items provided by a third party computing device. The system generates a bit vector or action from the machine learning model and the performance data of the content item. The system selects a content item based on a bit vector corresponding to an action in response to a content request from a client device executing the action of the application.
Description
Background
The computing device may run an application. The computing device may receive a query from a user, process the query, and provide a response to the query. However, due to limited availability or functionality in the input or output interfaces of a computing device, providing a useful response to queries with limited information in a manner that is efficient and free of excessive input/output processes via the available interfaces can be challenging.
Disclosure of Invention
The present disclosure is generally directed to bit vector based content matching for digital assistant actions. In particular, an application running on a computing device may be configured with voice actions that may be invoked by a user. The data processing system of this aspect may identify invocation of a voice action in an application, retrieve a bit vector generated for the action in the application, and select a content item to be provided in response to or in conjunction with execution of the voice action.
For example, it can be challenging to efficiently and accurately match content items (digital component objects or supplemental content items) with third party developed assistant actions due to the large number of assistant actions developed by third party developers of applications and the various session states that these actions can enter during human interaction. Efficient and accurate matching of content items can be challenging because action responses may not be revealed until the user begins a conversation with an action, and based on the user utterance, the action can take many different paths. Furthermore, efficient and accurate matching of content items can be challenging due to the difficulty in generating relevant signals from actions to match with content items.
The system and method of the present technology provides bit vector based content matching for digital assistant actions. For example, the data processing system of the present solution may include an action crawler (action crawler) that may simulate a conversation with a digital assistant-based application. The action crawler may grasp various conversation paths in the third party action. The data processing system may obtain the output of the action crawler and feed the output into a machine learning model to extract the embeddings. The data processing system may use the embedding to retrieve the most recently matching content item. The data processing system may score the content item according to the action crawler output. The data processing system may create a bit vector for an action that includes the relevant content item. The data processing system may then retrieve the bit vector to perform real-time content selection in response to the content request or invocation of the corresponding action.
At least one aspect is directed to a system that includes a data processing system having a memory and one or more processors. The data processing system can receive an application configured with voice assistant compatible actions from an application developer. The data processing system can identify actions that the application is configured to perform in response to the voice input. The data processing system may identify a content item provided by a third party computing device. The data processing system may generate a bit vector corresponding to the action through the machine learning model and the performance data of the content item. The bit vector may indicate candidate content items for each of the actions. The data processing system may select, in response to a content request to a computing device executing an action of the application, a content item based on a bit vector of the bit vectors that corresponds to the action.
At least one aspect is directed to a method performed by a data processing system having a memory and one or more processors. The method may include the data processing system receiving an application configured with voice assistant compatible actions from an application developer. The method may include an act of the data processing system identifying that the application is configured to perform in response to a voice input. The method may include the data processing system identifying a content item provided by a third party computing device. The method may include the data processing system generating a bit vector corresponding to the action through the machine learning model and the performance data of the content item. The bit vector may indicate candidate content items for each of the actions. The method may include the data processing system selecting, in response to a content request to a computing device executing an action of the application, a content item based on a bit vector of the bit vectors corresponding to the action.
These and other aspects and embodiments are discussed in detail below. The foregoing information and the following detailed description include illustrative examples of various aspects and embodiments, and provide an overview or framework for understanding the nature and character of the claimed aspects and embodiments. The accompanying drawings provide a description and a further understanding of various aspects and embodiments, and are incorporated in and constitute a part of this specification.
Drawings
The drawings are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing. In the drawings:
FIG. 1 is an illustration of an example system for vector-based content matching for third-party digital assistant actions according to an embodiment;
FIG. 2 is an illustration of an example method of generating a bit vector for content matching for third party digital assistant actions, according to an embodiment;
FIG. 3 is an illustration of an example method of selecting content using a bit vector in response to a call to a third party digital assistant action, according to an embodiment;
FIG. 4 is a block diagram illustrating an architecture of a computer system that may be used to implement elements of the systems and methods described and illustrated herein, including, for example, the systems depicted in FIG. 1 and the methods depicted in FIGS. 2 and 3.
Detailed Description
The following is a more detailed description of various concepts related to and implementations of the methods, devices, and systems for vector-based content matching for third-party digital assistant actions. The various concepts introduced above and discussed in more detail below may be implemented in any of a variety of ways.
The technical solution is generally directed to bit vector based content matching for third party digital assistant actions. An application developer may develop an application for installation and execution on a computing device. The applications may be referred to as third party application developers because they are independent of or separate entities from the provider, owner, or manufacturer of the computing device. An application developer may provide an application to a data processing system or cloud computing system or environment. For example, an application developer may provide an application to an online marketplace or an application store. The computing device may download the application from an online application store or marketplace.
The application may include a digital assistant action. The application may include a voice action. An application may interface with a digital assistant running on a computing device. The application may be configured with voice actions to provide a conversational user interface. For example, a user of a computing device may interface with an application through a dialog interface. A dialog interface may refer to or include conducting a voice-based session with an application. During the session, the user may invoke one or more voice actions in the application.
In an illustrative example, the application may be for a theme park. The application may include references to various roles associated with the theme park, such as a cartoon role. Applications may provide various content associated with a character, such as multimedia content, video, movies, programs, music, songs, stories, poetry, or other content. The application may provide entertainment-related content in response to invocation of the voice content.
However, when invoking a voice-based action, it can be challenging to identify or select a supplemental content item provided by a supplemental content provider to provide with or along with the performance of the voice-based action. For example, the content selector component may not be aware of the conversation path or dialog path that resulted in the invocation of a particular voice action. The content selector may not be aware of the dialog branches that resulted in the utterance invoking the voice action. This may be because the voice action is unlike a web page, because the response from the application to the voice action may take into account one or more previous branches associated with the dialog or conversation flow. The content selector may not have access to those previous branches or information associated with the previous branches. The content selector may have little or insufficient information. The content selector may attempt to select the content item based on the voice action itself because the content selector may not have access to the previous branches in the dialog flow. Thus, the content selector may select an irrelevant content item, or may perform a resource-intensive selection process to attempt to select a content item, but with a low accuracy or low relevance score. This may lead to waste of resources or incorrect or inaccurate selection of content items. Providing inaccurate or irrelevant content items can result in excessive remote procedure calls or network bandwidth usage because multiple additional content items may be selected and transmitted over the network to the computing device. Furthermore, providing inaccurate or irrelevant content items to a computing device can lead to a poor user experience. The content item may perform poorly on the computing device because the user may not be able to interact with or participate in the content item. In some cases, the user may skip or skip inaccurate content items, resulting in the user providing additional input or interaction with the computing device than presenting the accurate or related content items.
To address these and other technical challenges and problems, the present solution may generate and use a bit vector to match a content item with a third party voice action invoked in a third party application running on a computing device. To this end, the data processing system of the solution may receive an application from an application developer. The data processing system may crawl the application to obtain actions. The data processing system may simulate a dialog with an application to identify individual branches in the dialog or conversation flow, and corresponding actions that may be invoked at each branch. The same speech action theme in the application may be invoked by multiple conversational flows in the application, which may result in different responses to the speech action. For example, a session flow may include "talking to role a," which may be divided into three branches: 1) play a game, 2) read a story, or 3) sing a song. Each of these branches may correspond to a different voice action.
The data processing system may crawl the application to identify various conversational flows and corresponding voice actions and generate transcripts (transcriptions). The transcript may include a voice action and a conversation flow that resulted in a voice action or may result in invoking a voice action. The transcript may include network reference ("webref") entities, internal classification information, or vertical classifications. The data processing system may receive a supplemental content item provided by a supplemental content provider, such as an advertiser. The data processing system may input the transcript into a deep learning model along with the content item to generate a bit vector. A bit vector may be generated for each voice action. For each voice action, the bit vector may indicate candidate content items deemed to be relevant to the voice action by the deep learning model. The deep learning model may include a multi-layer cross-attention model. The training data may include a representation of the content item when presented in a speech action. The performance may include whether the user skipped the content item when presenting the content item in a particular voice action, or whether the user perceived, viewed, or listened to the complete content item while continuing to play the content item. The bit vector may provide a concise representation of all inputs into the deep learning model. The data processing system may match and score the content items with respect to the voice action.
The data processing system may provide the bit vector to the content selector. Storing the bit vector may be more efficient than storing the conversation transcript of the voice action alone and storing the content item alone. For example, the bit vector may consume less memory or storage in a data repository of the data processing system. In another example, performing a lookup in a bit vector to identify candidate content items for a voice action during a real-time content selection process may be more efficient than scoring a larger set of content items relative to a copy. The content selector may use the bit vector to perform real-time content selection during a content service time in response to a request for content or in response to a call for a voice action.
In some cases, the data processing system may generate content item bit vectors separately for each set of content items in order to match the most relevant content items with the voice action. To this end, the data processing system may use a combination of k-dimensional trees or data structures.
When a third party application is running on a computing device, a user may invoke voice actions in the application through a conversational flow or conversation. The data processing system may receive an indication of a voice action. The data processing system may perform a lookup in the bitvector data store with a voice action to identify a corresponding set of candidate content items. The data processing system may retrieve the top-ranked candidate content item from the bit vector. The data processing system may score the top ranked candidate content items using additional signals associated with the computing device or its electronic account to select the highest scoring content item. The data processing system may then provide the highest scoring content item to the computing device for presentation. By scoring top-ranked candidate content items, rather than all available content items, the data processing system may reduce computing resource utilization without sacrificing accuracy or relevance of selecting content items.
The data processing system may receive performance information when presenting the content item. The performance information may include, for example, whether the user has skipped the content item, the time the user has listened to the content item, or the total duration of whether the user has listened to the content item. For example, if the content item includes 15 seconds of audio output and the user skips the audio output after 3 seconds, the data processing system may determine that the content item is behaving poorly. If the user listens to the content item for the full 15 seconds duration, the data processing system may determine that the content item performs well for voice actions. The data processing system may update a deep learning model for generating bit vectors based on the performance of the content items. The data processing system may determine to retrain or otherwise update the model. The data processing system may determine to regenerate or update the bit vector based on the received performance feedback.
FIG. 1 illustrates an example system 100 for vector-based content matching for third-party digital assistant actions according to an embodiment. The system 100 may include a content selection infrastructure. System 100 may include a data processing system 102. The data processing system 102 may communicate with one or more of a client computing device 136 or a supplemental digital content provider device 132 over the network 105. Network 105 may include computer networks (such as the internet, local area network, wide area network, metropolitan area network, or other area network), intranets, satellite networks, and other communication networks (such as voice or data mobile telephone networks). The network 105 may be used to access information resources, such as web pages, websites, domain names, or uniform resource locators, that may be provided, output, presented, or displayed on the client computing device 136.
The network 105 may include or constitute a display network, such as a subset of information resources available on the Internet, associated with a content placement or search engine results system or otherwise eligible to include a third-party digital component as part of a digital component placement advertising campaign. Network 105 may be used by data processing system 102 to access information resources, such as web pages, websites, domain names, or uniform resource locators, that may be provided, output, presented, or displayed by client computing devices 136. For example, via network 105, a user of client computing device 136 may access information or data provided by supplemental digital content provider device 132.
The network 105 may be any type or form of network and may include any of the following: point-to-point networks, broadcast networks, wide area networks, local area networks, telecommunication networks, data communication networks, computer networks, ATM (asynchronous transfer mode) networks, SONET (synchronous optical network) networks, SDH (synchronous digital hierarchy) networks, wireless networks, and wired networks. The network 105 may include wireless links such as infrared channels or satellite bands. The topology of the network 105 may include a bus, star, or ring network topology. The network may include a mobile telephone network that uses any one or more protocols for communicating between mobile devices, including advanced mobile phone protocol ("AMPS"), time division multiple access ("TDMA"), code division multiple access ("CDMA"), global system for mobile communications ("GSM"), general packet radio service ("GPRS"), or universal mobile telecommunications system ("UMTS"). Different types of data may be transmitted through different protocols, or the same type of data may be transmitted through different protocols.
The client computing device 136 may include, for example, a laptop computer, desktop computer, tablet computer, digital assistant device, smart phone, mobile telecommunications device, portable computer, smart watch, wearable device, headset, speaker, television, smart display, or automotive unit. For example, through network 105, a user of client computing device 136 may access information or data provided by supplemental digital content provider device 132. In some cases, the client computing device 136 may or may not include a display; for example, a computing device may include a limited type of user interface, such as a microphone and speaker. In some cases, the primary user interface of the client computing device 136 may be a microphone and speaker, or a voice interface. In some cases, the client computing device 136 includes a display device coupled to the client computing device 136, and the primary user interface of the client computing device 136 may utilize the display device.
The client computing device 136 may include at least one input device 138. The input device 138 may include at least one microphone. The input device 138 may include a transducer or other hardware configured to detect sound waves (such as voice input from a user) and convert the sound waves into another format that the client computing device 136 may process. For example, the input device 138 may detect sound waves and convert the sound waves to analog or digital signals. The client computing device 136 may convert analog or digital signals into data packets corresponding to voice inputs or other detected audio inputs using hardware or software. The client computing device 136 may send data packets with voice input to the data processing system 102 for further processing. The input device 138 may include a touch input device, keyboard, mouse, gesture input, sensor input, or other type of input interface.
The client computing device 136 may include at least one output device 140. The output device 140 may include a speaker. The output device 140 may output audio or sound. The output device 140 may be driven by an audio driver to produce an audio output. Output device 140 may output speech or other audio generated by data processing system 102 and provided to client computing device 136 for output. For example, a user may engage in a conversation with the digital assistant 108 via the input device 138 and the output device 140 of the client computing device 136.
In some cases, client computing device 136 may include one or more components or functions of data processing system 102, such as NLP 106, digital assistant 108, interface 104, or data repository 120. For example, the client computing device 136 may include a local digital assistant 144 or digital assistant proxy having one or more components or functions of the server digital assistant 108 or NLP 106. The client computing device 136 may include a data repository, memory, or storage device that stores one or more of applications, content data, account information, or profile information. The client computing device 136 may include one or more components or functions of the computing system 400 depicted in fig. 4.
Servers in a machine group may be stored in a high-density rack system along with associated storage systems and located at an enterprise data center. Integrating servers in this manner may improve system manageability, data security, physical security of the system, and system performance by placing the servers and high-performance storage systems on a local high-performance network, for example. Centralizing all or some of the data processing system 102 components (including servers and storage systems) and coupling them with advanced system management tools allows for more efficient use of server resources, which saves power and processing requirements and reduces bandwidth usage.
The system 100 may include, access, or otherwise interact with at least one third party device, such as a supplemental digital content provider device 132, a service provider device, or an application developer device 134. The supplemental digital content provider device 132, service provider device, or application developer device 134 may include at least one logic device (such as a computing device with a processor) to communicate with, for example, a client computing device 136 or the data processing system 102 over the network 105.
The supplemental digital content provider device 132 may provide content items to the data processing system 102 to cause the data processing system 102 to provide the client computing device 136 for presentation. The content items may include visual content, audio content, text-based content, multimedia content, or other types of content. The content items may include audio-based digital components for presentation by the client computing device 136 as audio output digital components. The digital component may be referred to as a sponsored digital component in that it is provided by a third party sponsor, such as an advertisement provided by an advertiser. The digital components may include offers for goods or services, such as the statement "do you wish me to taxi for you? "voice-based messages. For example, the supplemental digital content provider device 132 may include a memory to store a series of audio digital components that may be provided in response to a voice-based query. The supplemental digital content provider device 132 may also provide audio-based digital components (or other digital components) to the data processing system 102, which may be stored in a data repository of the data processing system 102 in the data processing system 102. The data processing system 102 can select an audio digital component and provide (or instruct the supplemental digital content provider device 132 to provide) the audio digital component to the client computing device 136. The audio-based digital component may be exclusively audio or may be combined with text, image or video data. The content item may have a duration, such as the length of audio or video. When presented by the client computing device 136, the content items may be presented with user interface elements that allow interaction with or participation in the content items. For example, the user interface element may allow for input or commands that cause the client computing device 136 to pause presentation or playback of the content item, skip playback of the content item, fast forward the content item, rewind the content item, close or hide the content item, participate in the content item, or otherwise interact with the content item. The type of interaction may indicate a performance associated with the content item. For example, rewinding or replaying a content item may indicate a positive performance, while skipping a content item or fast forwarding a content item may indicate a negative performance.
The data processing system 102 may interface with an application, script, or program (such as application 142 or local digital assistant 144) installed at the client computing device 136 to communicate input audio signals to the interface 104 of the data processing system 102 and to drive components of the client computing device 136 to present, render, or otherwise output visual or audio signals. The data processing system 102 may receive data packets, or other signals that include or identify audio input signals, from the application 142 or the local digital assistant 144.
The data processing system 102 may include a natural language processor ("NLP") 106. For example, the data processing system 102 may run or operate the NLP 106 to parse received input audio signals or queries. For example, the NLP 106 may provide interaction between a person and a computer. NLP 106 may be configured with techniques for understanding natural language and allowing data processing system 102 to obtain meaning from human or natural language input. NLP 106 may include or be configured with techniques based on machine learning, such as statistical machine learning. The NLP 106 may parse the input audio signal using a decision tree, statistical model, or probabilistic model. NLP 106 can perform functions such as the following: the term "text" may include, but is not limited to, named entity recognition (e.g., determining which items in text map to proper names, such as people or places, and what the type of each such name is, such as people, locations, or organizations), natural language generation (e.g., converting information from a computer database or semantic intent to a human language that is understandable), natural language understanding (e.g., converting text to a more formal representation, such as a first order logical structure that a computer module can manipulate), machine translation (e.g., automatically translating text from one human language to another), morpheme segmentation (e.g., separating words into individual morphemes and identifying classes of morphemes, which may be challenging based on the lexical or structural complexity of the language words considered), question answer (e.g., determining answers to human language questions, which may be specific or open), semantic processing (e.g., processing that may occur after recognizing and encoding their meanings to associate the recognized words with other words having similar meanings).
The audio input signals may be detected by an input device 138 (e.g., microphone, sensor, transducer) of the client computing device 136. Through transducers, audio drivers, or other components, client computing device 136 may provide audio input signals to data processing system 102 (e.g., through network 105), where audio input signals may be received (e.g., through interface 104) and provided to NLP 106 or stored in a data repository.
The NLP 106 may obtain an input audio signal. In response to the local digital assistant on the client computing device 136 detecting the trigger key, the NLP 106 of the data processing system 102 may receive a data packet with a voice input or an input audio signal. The trigger key may be a wake signal or hotword that instructs the client computing device 136 to convert subsequent audio input into text and send the text to the data processing system 102 for further processing.
Upon receiving the input audio signal, the NLP 106 may identify at least one query or request or at least one keyword corresponding to the request. The request may indicate an intent or topic of the input audio signal. The keywords may indicate the type of action that may be taken. For example, the NLP 106 may parse the input audio signal to identify at least one request to leave home at night to eat dinner and watch a movie. The trigger key may include at least one word, phrase, root or part of word, or derivative indicating an action to be taken. For example, a trigger keyword "go" or "go" from an input audio signal may indicate a need for shipment. In this example, the input audio signal (or identified request) does not directly express the intent of the shipment, but the trigger key indicates that the shipment is an affiliated action requesting at least one other action indicated. In another example, the voice input may include a query or request to perform an action in the application.
The NLP 106 may parse the input audio signal to identify, determine, retrieve, or otherwise obtain the request and one or more keywords associated with the request. For example, the NLP 106 may apply semantic processing techniques to the input audio signal to identify keywords or requests. The NLP 106 may apply semantic processing techniques to the input audio signal to identify keywords or phrases that include one or more keywords (such as a first keyword and a second keyword). For example, the input audio signal may include the word "I want to purchase a voice book". NLP 106 can apply semantic processing techniques or other natural language processing techniques to the data packet that includes the sentence to identify keywords or phrases "want to purchase" and "voice book". The NLP 106 may also identify a plurality of keywords, such as purchases and audio books. For example, the NLP 106 may determine that the phrase includes first and second keywords.
The NLP 106 may filter the input audio signal to identify trigger keywords. For example, a data packet carrying an input audio signal may include "It would be great if I could get someone that could help me go to the airport", in which case NLP 106 may filter out one or more terms as follows: "it", "would", "be", "great", "if", "I", "could", "get", "somerone", "that", "could" or "hellp". By filtering out these terms, the NLP 106 can more accurately and reliably identify trigger keywords, such as "go to the airport," and determine that this is a request for taxi or carpool services.
The server digital assistant 108 or the local digital assistant 144 may interface with the application 142 running on the client computing device 136. The application 142 may include voice-based actions or a voice interface. The application may receive voice input from a user of the client computing device 136. The client computing device 136 may forward the voice input to the data processing system 102 (e.g., NLP 106 or server digital assistant 108) for further processing. In some cases, the application 142 may forward the voice action to the application developer device 134 for processing. The application 142 may forward the voice action to the data processing system 102, and the data processing system 102 may pre-process the voice action and then forward the voice action to the application developer device 134. In some cases, the application 142 running on the client computing device 136 may be configured to process voice actions and perform corresponding actions. In some cases, the application 142 forwards the voice input to the data processing system 102 for processing, and the data processing system 102 may identify an action or intent associated with the voice input and forward a command to the application 142 to perform the requested action or intent.
The data processing system 102 can determine a supplemental content item that selects a voice action of the application 142. However, when invoking a voice-based action, it can be challenging to identify or select a supplemental content item provided by a supplemental content provider to provide with or along with the performance of the voice-based action. For example, the content selector 114 may not be aware of the conversation path or conversation path that resulted in the invocation of the particular voice action. The content selector 114 may not be aware of the conversation branch of the utterance that resulted in the invoked voice action. This may be because the voice action is unlike a web page, because the response from the application to the voice action may take into account one or more previous branches associated with a conversation or conversation flow. The content selector may not have access to those previous branches or information associated with the previous branches. The content selector may have little or insufficient information. The content selector may attempt to select the content item based on the voice action itself because the content selector may not have access to the previous branches in the dialog flow. Thus, the content selector may select an irrelevant content item, or may perform a resource-intensive selection process to attempt to select a content item, but with a low accuracy or low relevance score. This may lead to waste of resources consumption or incorrect or inaccurate selection of content items. Providing inaccurate or irrelevant content items can result in excessive remote procedure calls or network bandwidth usage because multiple additional content items can be selected and transmitted over the network to the computing device. Furthermore, providing inaccurate or irrelevant content items to a computing device can lead to a poor user experience. Content items may perform poorly on computing devices because users may not be able to interact with or participate in the content items. In some cases, a user may skip or skip inaccurate content items, resulting in additional input provided by the user or interaction with the computing device compared to presenting the accurate or related content items.
To address these and other technical challenges, data processing system 102 for this technical solution may include an action crawler 110, where action crawler 110 is designed, constructed, and operative to generate transcripts of actions that may be performed by applications. The action crawler 110 may receive an application configured with voice assistant compatible actions from the application developer 134. The action crawler 110 may identify actions that the application is configured to perform in response to voice input. The action crawler 110 may crawl the application to identify actions. The action crawler 110 may simulate conversations with the application to identify various conversation paths that a user may conduct with the application. Depending on the intent associated with the conversation path, different conversation paths may result in the same voice action or different voice actions being performed.
For example, the NLP 106 or server digital assistant 108 of the data processing system 102 can simulate a dialog with an application to identify a plurality of actions. The server digital assistant 108 may simulate one or more conversations with the application. In some cases, data processing system 102 can receive an application package file including example dialog or transcript information from application developer device 134. In some cases, data processing system 102 can automatically determine possible dialogs that an application can engage in and identify voice actions that the application is configured to perform.
For example, an application may involve providing entertainment content related to cartoon characters at a theme park. The action crawler 110 may simulate a conversation with an application. The action crawler 110 may launch or invoke an application. Applications can talk with, for example, "what character you want to talk with? "inquiry or prompt begins. The action crawler 110 may select a character from the list of options by generating a simulated voice input that includes the statement "talk to character a". The application may respond to action crawler 110 with responses and prompts or queries such as "do you want to play a game with character a, read a story with character a, or sing a song with character a. The action crawler 110 may select one of the voice actions that corresponds to the voice assistant-enabled action. The action crawler 110 may store a transcript of each voice action that includes a dialog flow that results in invoking or selecting the voice action. Action crawler 110 may repeat this flow for each role in the application.
In another example, the action crawler 110 may invoke an application and then enter a prompt or query asking the application what tasks the application may perform. The action crawler 110 may then select a task and initiate one or more conversational flows that result in the invocation of the voice action. Thus, the action crawler 110 may crawl the application to generate a transcript that includes actions the application is configured to perform according to one or more sessions. Data processing system 102 can generate a transcript to store information about the action and one or more sessions for invoking the action. The information may include conversations, keywords, webref entities, vertical categories, intents, voice actions, or other information that facilitates content selection. The webref entity may correspond to an entity classified in taxonomy. The network reference entity helps understand the text and extend the knowledge repository. The entity may be a single person, place or thing and the repository may include millions of entities, each having a unique identifier to distinguish between entities having similar names (e.g., a leopard car and a leopard animal). The data processing system may access the referencing entity and scan any text segments (e.g., text in a web page, keyword text, content text, advertisement text) to identify entities from various sources. For example, one such source may be a manually created entity taxonomy, such as an entity diagram of people, places, and things built by a community of users.
The data processing system 102 can receive content items from the supplemental digital content provider device 132. The data processing system 102 can identify content items provided by a third party of the computing device, such as the supplemental digital content provider device 132. For example, the content selector 114 of the data processing system 102 may receive content items from the supplemental content provider device 132 via the network 105. The data processing system 102 may receive content items (or sponsored content items) through an interface of the data processing system, such as a graphical user interface. The data processing system 102 can associate the content item with an account identifier that is associated with the supplemental content provider device 132. The content item may be part of a content placement advertising campaign. The content item may be part of a content group. The content items may be associated with content selection criteria, such as keywords, locations, or other signals used by the content selector 114 to perform content selection in real-time. The data processing system 102 can identify concepts, topics, entities, or verticals associated with a content item or group of content. The data processing system 102 may also identify or generate performance data for the content item. The performance data may include or indicate performance of the content item as it is presented or provided for presentation on one or more client computing devices 136. The performance may refer to, include, or be based on a level of interaction or type of interaction of the client computing device 136 with the content item. For example, the performance information may include or be based on selecting a content item, clicking on a content item, converting, or completing a reminder or survey associated with the content item. The performance information may include, for example, whether the user closed a window that presented the content item, skipped playback of the content item, fast-forwarded playback of the content item, repeated playback of the content item, or allowed the content item to play for the full duration of the content item. Thus, the performance data for the content items may indicate the type of interaction with each content item in response to previous presentations of each content item by one or more computing devices 136. The type of interaction may include at least one of skip, fast forward, or duration of presentation. The performance data may be stored in a content data 126 data structure in the data repository 120.
The performance data may be associated with or related to keywords or other signals associated with the content item, or signals associated with the location at which the content item is presented. The signal may include the type of the client computing device 136, the location of the client computing device 136 when the content item was presented, information associated with the companion content item or primary content presented on the client computing device 136 with the content item, or other account information or profile information associated with the client computing device 136.
However, performing real-time selection of content items in an efficient and accurate manner can be challenging due to the large number of content items received from the large number of supplemental digital content provider devices 132, as well as the large number of voice-based assistant actions and large number of applications. The data processing system 102 of the present solution may include an embedded extractor 112 designed, constructed, and operative to generate a bit vector that indicates for each speech action the associated candidate content item. The bit vector may be used by the content selector 114 to perform real-time content selection in response to voice action calls in an application running on the client computing device 136.
The embedding extractor 112 may generate a bit vector corresponding to the action through the machine learning model and the performance data of the content item. The bit vector may indicate candidate content items for the action. The embedding extractor 112 may take as input the transcript generated by the action crawler 110 along with the content item information or performance data and generate a bit vector indicating which content items are relevant to each action in the transcript. For example, a transcript may include multiple voice-based actions in a particular application. Each action in the transcript may be associated with a conversation stream that includes keywords, intents, concepts, topics, or other information associated with the simulated dialog that resulted in the voice action. The embedded extractor 112 may use the deep learning model 128 to identify which content items are relevant to each action in the transcript. The embedded extractor 112 may generate one or more bit vectors for an application that identifies a plurality of candidate content items for each action in a transcript generated by the action crawler 110 for the application.
A bit vector may refer to or include a data structure that may be efficiently processed by data processing system 102 or one or more processors and memories thereof. The bit vector may effectively allow data processing system 102 to perform bit-level parallelism in hardware to perform operations quickly. The bit vector may be a number of bits. The bit vector may include a plurality of bits. The bit vector may include a mapping to a value 0 1, such as indicating which content items are relevant for a particular action. For example, a bit vector of the voice action "read together with CharacterA" may include indicating which candidate content items from the set of candidate content items are relevant by indicating a 1 or 0 in the bit vector, where the position in the bit vector corresponds to a particular candidate content item. Table 1 shows example bit vectors for different actions in an application and a data structure listing candidate content items.
Candidate content item | Bit vector of action 1 | Bit vector of action 2 | Bit vector of action 3 |
Content item 1 | 0 | 0 | 1 |
Content item 2 | 0 | 1 | 1 |
Content item 3 | 1 | 0 | 0 |
Content item 4 | 1 | 1 | 0 |
Content item 5 | 1 | 0 | 1 |
Table 1: illustrative examples of bit vectors for voice actions
In the illustrative example of table 1, the candidate content item data structure for a particular application may list 5 different content items in an ordered list. The order may be based on any order, such as numerical order, alphabetical order, or application-dependent order. The embedded extractor 112 may generate a bit vector for each action in the transcript generated by the action crawler 110. For example, for action 1, the bit vector may be [0 0 11 1]; for action 2, the bit vector may be [0 10 10 ], and for action 3, the bit vector may be [ 110 0 1]. The bit vector may indicate which content items are related to or candidates for a particular voice action. For example, for voice action 1, the relevant candidate content items may be content items 3, 4, and 5. For voice action 2, the candidate content items may be 2 and 4. For voice action 3, the candidate content items may be 1, 2, and 5.
The different voice actions shown in table 1 may refer to different actions, such as playing a song, reading a book, or playing music with character a. In another example, different actions may refer to the same type of action (such as playing a song), but have different roles as follows: voice action 1 may play a song with character a; voice action 2 may play a song with character B; voice action 3 may play a song with character C. By simulating a conversation with an application to generate transcript files, the action crawler 110 may generate different voice action identifiers for the same type of action (e.g., play a song) based on the conversation flow that caused the application to invoke. Thus, table 1 may include different bit vectors for each action based on the conversational flow that resulted in the invocation of the voice action, thereby improving the accuracy of content selection while maintaining efficiency through the use of bit vector formats.
To generate a bit vector for each action, data processing system 102 may use the training data to train a deep learning model or a machine learning model. The training data may include historical data associated with previous presentations of content items on the plurality of client computing devices 136. The historical training data may be based on the performance of the content item and a signal associated with the performance. The historical training data may include information about the content item (e.g., keywords, topics, entities), signals associated with the presentation of the content item (e.g., type of computing device, location of computing device, concept, keywords or topics associated with the primary content with which sponsored content is presented, or profile information), performance data (e.g., whether the user interacted with or skipped the content item).
The data processing system 102 may train the model 128 with training data using machine learning techniques, such as deep learning techniques. Deep learning techniques may refer to deep structured learning. Deep learning techniques may include or be based on artificial neural networks with representation learning. Deep learning techniques may be supervised, semi-supervised, or unsupervised. Deep learning techniques may include recurrent neural networks, convolutional neural networks, deep neural networks, or deep reinforcement learning. Deep learning techniques may use multiple layers to progressively extract advanced features from an input. For example, using content data (e.g., content item information and performance data) and transcripts (e.g., voice actions and information associated with a session that resulted in the invocation of the voice action), deep learning techniques may determine which content items are relevant or predicted to be presented with, spatially or temporally proximate to, or otherwise perform well with or for a particular voice action. The embedded extractor 112 may predict which content items are likely to perform well (e.g., have a likelihood of interactions above a threshold (such as 1%, 2%, 3%, 5%, 10%, 15%, 20% or more relative to the number of presentations of the content item) using a deep learning model. The embedded extractor 112 may predict which content items are the highest performing content relative to the set of available content items received from the one or more supplemental digital content provider devices 132 using a deep learning model. Thus, the embedding extractor 112 may take as input the transcript and the content item performance data and output a bit vector for each action in the transcript. The output bit vector may indicate candidate content items for a particular action that are candidates for presentation by the client computing device 136 in response to the action call in the application 142.
The data processing system 102 may include a content selector 114 designed, constructed, or operative to select a supplemental content item (or sponsored content item or digital member object). In response to a request for content of the computing device 136 executing an action of the application 142, the content selector 114 may select a content item based on a bit vector corresponding to the action. To avoid excessive computing resource utilization due to selection from a large number of available content items, the content selector 114 may include a content retriever 116, the content retriever 116 being designed, constructed and operative to retrieve candidate content items for actions invoked in the application using the bit vector generated by the embedded extractor 112. The bit vector may store an indication of a subset of all available content items provided by the various supplemental digital content provider device 132 content items. The subset of all available content items may be those content items that correspond to the highest ranked content item or content item that is most relevant to the action of the application. The most relevant content items may be determined by the embedded extractor 112 using the deep learning model 128. This subset of content items may be listed in a bit vector or otherwise identified as candidates for presentation in response to an action call in application 142. However, the content selector 114 may further refine the subset of content items to select the highest scoring content item of the subset of content items based on signals or other information associated with the particular client computing device 136 invoking the action in the application 142.
The content retriever 116 may identify the action and the application 142 in which the action is invoked. For example, an application may be associated with a unique identifier (such as a name, alphanumeric identifier, or other unique identifier of the application). The content retriever 116 may further identify actions invoked in the application 142. The action may be associated with a unique identifier or an identifier (such as a name, alphanumeric identifier, or other valid identifier) that is unique to the application 142.
The content retriever 116 may perform a lookup in the bit vector 130 data structure using the identifier of the application and the identifier of the action to access, retrieve, obtain, or otherwise identify the bit vector established for the action of the application. The content retriever 116 may identify one or more candidate content items indicated in the bit vector of the action. The content retriever 116 may identify a unique identifier of the content item, a uniform resource locator of the content item, a pointer to the content item, or other indicator of the content item that may be used to retrieve or provide the content item to the computing device 136.
The bit vector may indicate a number of candidate content items for a particular action invoked in the application 142. Candidate content items associated with the bit vector of actions may include content data 126, such as content selection criteria, performance data, or other information that may facilitate content selection. The content retriever 116 may provide the candidate content items indicated in the bit vector to the content scorer 118 to select a content item from the candidate content items for provision to the computing device 136.
The content selector 114 may include a content scorer 118, the content scorer 118 being designed, constructed and operative to select a content item from candidate content items indicated in the bit vector. The content selector 114 may score candidate content items to identify the highest scoring or highest ranking content item. The content selector 114 may score candidate content items using any content scoring technique that may facilitate identifying content items that are most likely to perform well on the computing device 136. The content scorer 118 may use signals associated with the particular client computing device 136 that invoked the action to score candidate content items. The signal may include, for example, a type of computing device, a geographic location of the computing device, a language used with the client computing device (e.g., english, french, or spanish), or profile information associated with an electronic account linked to the client computing device 136. For example, the content scorer 118 may predict which of the candidate content items is most likely to be relevant to the action invoked in the application 142 based on signals associated with the client computing device 136. The content scorer 118 may predict which of the candidate content items is most likely to result in a favorable interaction by the user of the computing device 136. The content scorer 118 may predict which of the candidate content items is most likely to play for the full duration of the content item without being skipped or turned off by the user of the computing device 136.
To select sponsored content items or digital components, content scorer 118 may use content selection criteria to select matched sponsored content items based on broad matches, exact matches, or phrase matches. For example, the content selector 114 may analyze, parse, or otherwise process the topics of the candidate sponsored content item to determine whether the topics of the candidate sponsored content item correspond to topics of intent, keywords, or phrases associated with the action invoked in the application 142. Content ranker 118 may use image processing techniques, character recognition techniques, natural language processing techniques, or database lookup to identify, analyze, or recognize speech, audio, terms, characters, text, symbols, or images of candidate digital components. The candidate sponsored content item may include metadata indicating a topic of the candidate digital component, in which case the content ranker 118 may process the metadata to determine whether the topic of the candidate digital component corresponds to an action invoked in the application 142. The content advertising campaign provided by the supplemental digital content provider device 132 may include content selection criteria that the content scorer 118 may match with criteria indicated in the second profile layer or the first profile layer.
The content selector 114 may perform a real-time content selection process in response to the request. Real-time content selection may refer to or include performing content selection in response to a request. Real-time may refer to or include selecting content within 0.2 seconds, 0.3 seconds, 0.4 seconds, 0.5, 0.6 seconds, or 1 second of receiving a request. Real-time may refer to selecting content in response to receiving an input audio signal from a client computing device 136. The real-time content selection process may be expedited by identifying bit vectors for actions invoked in application 142 and inputting candidate content items identified in the bit vectors into content scorer 118 to output the highest ranked or scoring content items. The content scorer 118 may determine a score or ranking for each of the plurality of candidate supplemental content items in order to select the highest ranked supplemental content item for provision to the client computing device 136.
In some cases, the content scorer 118 may be configured with a machine learning model that may determine the highest scoring or most relevant content items. The machine learning model may be trained based on historical performance of content items associated with the various signals. The model may output predictions or scores indicating the likelihood that the content item is behaving well in the presence of certain signals. Thus, the content selector 114 may score content items from candidate content items retrieved by the content retriever 116 or select content items from among the candidate content items using machine learning techniques.
Accordingly, the data processing system may receive an indication of an action call from the computing device 136. Data processing system 102 may perform a lookup with an action in data repository 120 storing bit vectors 130 to retrieve the bit vector corresponding to the action. The data processing system 102 may identify top-ranked candidate content items from the bit vectors of the actions. The top-ranked candidate content items may be identified by the embedded extractor 112 using the deep learning model 128 and stored in a bit vector of the applied action. The data processing system 102 may select the highest scoring content item from the top ranked candidate content items that identify the automatic bit vector. The data processing system 102 can use one or more signals associated with the computing device 136 performing the action to select a content item from the top ranked candidate content items through a real-time content selection process.
The data processing system 102 can provide the selected content item to the client computing device 136 to cause the client computing device 136 to present the content item to a user of the computing device 136. The data processing system 102 can provide the content item to the local digital assistant 144 to cause the local digital assistant 144 running on the client computing device 136 to play the content item through an audio output (such as through a voice output). In some cases, the data processing system 102 can provide the content item to the application 142 to cause the application 142 to present the content item. For example, the application 142 may output the content item by playing the content item using an audio output or a visual output. The application 142 or the local digital assistant 144 may present or output the content item during execution of an action invoked in the application, before execution of the action, or after execution of the action. In some cases, the computing device 136 may pause execution of the action in order to play or output the content item. In some cases, the computing device 136 may wait to play the content item until the action is completed in the application 142. In some cases, the computing device 136 may wait a predetermined time, such as 1 second, 2 seconds, 5 seconds, 10 seconds, 15 seconds, 30 seconds, or other time interval, for playing the content item after the execution of the action is completed.
The data processing system 102 may receive an indication of a level of interaction with the content item when the content item is presented. Data processing system 102 may store the interaction level as performance data in a content data 126 data structure. The data processing system 102 may retrain the machine learning model with the indication of the level of interaction, such as by embedding the extractor 112, to update the bit vector corresponding to the action. For example, if the selected content does not perform well, the embedded extractor 112 may update the bit vector of the action to remove the content item from the bit vector so that the content item may not be selected in response to future invocations of the action in the application. In another example, the embedded extractor 112 may update the bit vector to decrease the rank or score of the content item, thereby decreasing the likelihood that the content item is selected in response to future invocations of actions in the application. Accordingly, data processing system 102 may improve deep learning model 128 in response to real-time performance feedback, and embedded extractor 112 may update the bit vector based on the retrained model.
FIG. 2 is an illustration of an example method of generating a bit vector for content matching for third party digital assistant actions, according to an embodiment. Method 200 may be performed by one or more systems or components depicted in fig. 1, including, for example, a data processing system. At 202, a data processing system may receive an application. The data processing system may receive an application from an application developer. The developer of the application may provide the application to the data processing system. The developer of the application may upload the application to the data processing system or otherwise send or provide the application to the data processing system. For example, the application may be stored as a binary file, an executable file, an application package file, or in another format that facilitates installation and execution of the application on a data processing system or client computing device.
At 204, the data processing system may identify actions in the application and generate a transcript. The data processing system may crawl the application to identify actions. The data processing system may recognize actions by installing and running an application. The data processing system may simulate a dialog with the application to generate one or more session flows in the application that may result in invocation or execution of actions in the application. The action may be referred to as a voice assistant compatible action because the action may be invoked by a voice-based dialog with the application user. The action may be invoked by an utterance from the user. The actions may be identified or selected through a dialog facilitated by a digital assistant running on the data processing system or the client computing device, or both.
The data processing system may generate transcripts for the identified actions. A transcript may be generated for a particular application. The transcript may indicate various actions identified in the application, as well as information associated with the conversation flow or dialog that caused the action to invoke. For example, the transcript may include information about the simulated utterance that caused the action invocation. The transcript may include an utterance of the simulated user and an application that interacts with the simulated user.
At 206, the data processing system may receive content data. The content data may refer to supplemental content items. The data processing system may receive content data from one or more supplemental content providers, such as advertisers. The data processing system may receive content data from a data repository accessible to the data processing system that stores the content data. The content data may include content items, such as digital component objects, and performance data associated with historical or previous presentations of the content items.
At 208, the data processing system may generate a bit vector for one or more actions indicated in the transcript. To this end, the data processing system may use a deep learning model trained based on historical content performance data. The data processing system may input the transcript of the application and the content data into a deep learning model to generate an embedded or bit vector or bit array for each action that indicates a subset of the content items as candidate content items to be presented in response to the invocation of the action in the application. By generating bit vectors indicative of subsets of content items, the data processing system may improve the efficiency and accuracy of the real-time content selection process.
FIG. 3 is an illustration of an example method of selecting content using a bit vector in response to a call to a third party digital assistant action, according to an embodiment. Method 300 may be performed by one or more systems or components depicted in fig. 1, including, for example, a data processing system. At 302, the data processing system may receive an indication of a call for an action within an application. In-application actions may refer to or include actions compatible with the voice assistant. For example, an action may be triggered, invoked, or initiated in response to voice input from a user of the client computing device. The action may be invoked by a local digital assistant running on the client computing device. The actions may be invoked or parsed by an NLP component running on a client computing device or data processing system.
In response to the invocation of the action, the client computing device may send an indication to the data processing system. The indication sent to the data processing system may include information about an application, action, or signal associated with the computing device. For example, the information sent to the data processing system may include an identifier of the action or an identifier of the application. The signal may include, for example, a geographic location of the computing device, a type of the computing device, or information associated with a profile or electronic account linked to the computing device. In some cases, the signal may include information regarding preferences associated with the computing device (such as preferences for content types).
The client computing device may send the indication to the data processing system through a local digital assistant, application, or other component running on the client computing device. For example, an application may send an indication to a data processing system in response to a call for an action. The local digital assistant may detect the invocation of the action and package the transmission to the data processing system including the identifier of the action or application and other signals associated with the computing device.
At 304, the data processing system may perform a lookup in the data repository to identify a bit vector for an action in the application. The data processing system may use the identifiers of the actions and applications to perform a lookup. At 306, the data processing system may identify candidate content items from the bit vector. The data processing system may use information received from the client computing device to perform a lookup in the data repository to retrieve or access the bit vector configured for the action. The bit vector may include candidate content items associated with the action.
At 308, the data processing system may select the highest scoring candidate content item from the bit vector. The data processing system may use any scoring technique to determine the score for each content item. The data processing system may rank the content items using the scores to determine the highest ranked content item. The data processing system may determine a score for each content item based on or using the signals received from the computing device. For example, if a content item is associated with the same geographic location as the geographic location of the client computing device, the data processing system may score the content item higher than another content item that may not be associated with the geographic location of the client computing device. In another example, the client computing device may be configured to use english and french content items may be scored lower by the data processing system than english content items.
At 310, the data processing system may send the selected content item to the client computing device to cause the client computing device to present the content item. The data processing system may include instructions on how to present the content item. The data processing system may provide instructions as to when to present the content item, what output device to use to present the content item, or allow interactive features of the content item. The data processing system may provide the content item to the component of the client computing device that sent the action call indication. For example, the data processing system may return the content item to an application or local digital assistant running on the client computing device.
At 312, the data processing system may determine the performance of the content item. The data processing system may determine the performance of the content item based on whether the user interacted with the content item or the type of interaction with the content item. The performance may be based on whether the user allowed full playback of the content item, whether the user skipped playback of the content item, whether the user turned off the content item, cancelled the content item, fast forwarded the content item, or otherwise interacted with the content item. For example, if the content item is an audio clip and the computing device has played the full duration of the audio clip, the data processing system may determine that the content item is performing well. However, if the user skips playback of the content item, the data processing system may determine that the content item is underperforming.
At 314, the data processing system may update the machine learning model based on the performance of the content item. The data processing system may update a machine learning model or a deep learning model for generating bit vectors based on the performance of the content items. For example, if the content item does not perform well, the data processing system may remove the content item from the bit vector. If the content item performs well, the data processing system may retain the content item as a candidate content item in the bit vector or increase a score or weight associated with the candidate content item.
Fig. 4 is a block diagram of an example computer system 400. Computer system or computing device 400 may include or be used to implement system 100 or components thereof, such as data processing system 102 or client computing device 136. The data processing system 102 or the client computing device 136 may include an intelligent personal assistant or a voice-based digital assistant. Computing system 400 includes a bus 405 or other communication component for communicating information, and a processor 410 or processing circuit coupled to bus 405 for processing information. Computing system 400 may also include one or more processors 410 or processing circuits coupled to the bus for processing information. Computing system 400 also includes a main memory 415, such as a Random Access Memory (RAM) or other dynamic storage device, coupled to bus 405 for storing information and instructions to be executed by processor 410. Main memory 415 may be or include data repository 120. Main memory 415 also may be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 410. Computing system 400 may also include a Read Only Memory (ROM) 420 or other static storage device coupled to bus 405 to store static information and instructions for processor 410. A storage device 425, such as a solid state device, magnetic disk, or optical disk, may be coupled to bus 405 to permanently store information and instructions. Storage device 425 may comprise or be part of data repository 120.
The computing system 400 may be coupled via bus 405 to a display 435, such as a liquid crystal display or an active matrix display, for displaying information to a user. An input device 430, such as a keyboard including alphanumeric and other keys, may be coupled to bus 405 for communicating information and command selections to processor 410. The input device 430 may include a touch screen display 435. The input device 430 may also include a cursor control, such as a mouse, a trackball, or cursor direction keys for communicating direction information and command selections to the processor 410 and for controlling cursor movement on the display 435. For example, display 435 may be part of data processing system 102 or client computing device 136 or other component of fig. 1.
The processes, systems, and methods described herein may be implemented by computing system 400 in response to processor 410 executing an arrangement of instructions contained in main memory 415. Such instructions may be read into main memory 415 from another computer-readable medium, such as storage device 425. Execution of the arrangement of instructions contained in main memory 415 causes computing system 400 to perform the illustrative processes described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 415. Hardwired circuitry may be used in place of or in combination with software instructions in systems and methods described herein. The systems and methods described herein are not limited to any specific combination of hardware circuitry and software.
Although an example computing system has been described in FIG. 4, the subject matter including the operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
For the case where the system discussed herein collects personal information about a user or may utilize personal information, the user may be provided with an opportunity to control whether a program or feature may collect personal information (e.g., information about the user's social network, social actions or activities, the user's preferences, or the user's location) or whether or how to receive content from a content server or other data processing system that may be more relevant to the user. Furthermore, certain data may be anonymized in one or more ways prior to storage or use such that personal identity information is deleted upon generation of parameters. For example, the identity of the user may be anonymized so that personal identity information cannot be determined for the user, or the geographic location of the user may be summarized (e.g., to a city, zip code, or state level) in the event that location information is obtained so that a particular location of the user cannot be determined. Thus, the user can control how information about him or her is collected and used by the content server.
The subject matter and operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs, e.g., one or more computer program instruction circuits, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal) that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium may be a computer readable storage device, a computer readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them, or may be included in a computer readable storage device, a computer readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Although the computer storage medium is not a propagated signal, the computer storage medium may be a source or destination of computer program instructions encoded in an artificially generated propagated signal. Computer storage media may also be, or be included in, one or more separate components or media (e.g., multiple CDs, disks, or other storage devices). The operations described in this specification may be implemented as operations performed by a data processing apparatus on data stored on one or more computer readable storage devices or received from other sources.
The terms "data processing system," "computing device," "component," or "data processing apparatus" encompass a variety of devices, apparatuses, and machines for processing data, including, for example, one or more programmable processors, one or more computers, one or more systems-on-a-chip, or a combination of the foregoing. The apparatus may comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, an apparatus may include code that creates a runtime environment for the computer program in question, such as code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and the operating environment may implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures. For example, the action crawler 110, the embedded extractor 112, or other components may include or share one or more data processing apparatuses, systems, computing devices, or processors.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. The computer program may correspond to a file in a file system. A computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors (e.g., components of data processing system 102) executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices including by way of example: example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM disks and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
The subject matter described herein may be implemented in a computing system that includes a back-end component (e.g., as a data server) or that includes a middleware component (e.g., an application server) or that includes a front-end component (e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification), or a combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), internets (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
A computing system such as system 100 or system 400 may include clients and servers. The client and server are generally remote from each other and typically interact through a communication network (e.g., network 105). The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, the server sends data (e.g., data packets representing the digital components) to the client device (e.g., to display data to and receive user input from a user interacting with the client device). Data generated at the client device (e.g., results of user interactions) may be received at the server from the client device (e.g., by the data processing system 102 from the client computing device 136 or the supplemental digital content provider device 132).
Although operations are depicted in the drawings in a particular order, such operations do not require execution in the particular order shown or in sequential order, and not all of the illustrated operations need be performed. The acts described herein may be performed in a different order.
The separation of various system components does not require separation in all embodiments, and the described program components can be included in a single hardware or software product. For example, natural language processor 106 and interface 104 may be part of a single component, an application or program, or a logic device having one or more processing circuits, or one or more servers of data processing system 102.
Having now described a few illustrative embodiments, it should be apparent that the foregoing is illustrative and not limiting and has been provided by way of example. In particular, although many of the examples presented herein involve specific combinations of method acts or system elements, those acts and those elements may be combined in other ways to achieve the same objectives. Acts, elements and features discussed in connection with one embodiment are not intended to be excluded from other embodiments or similar roles in an embodiment.
The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of "including," "comprising," "having," "containing," "involving," "characterized by" and variations thereof herein is meant to encompass the items listed thereafter and equivalents thereof as well as additional items as well as alternative embodiments consisting of items specifically listed thereafter. In one embodiment, the systems and methods described herein consist of one, each combination of more than one, or all of the elements, acts, or components described.
Any reference to an embodiment or element or act of a system and method recited in the singular can also include embodiments containing a plurality of such elements, and any plural reference to any embodiment or element or act herein can also include embodiments containing only a single element. Singular or plural references are not intended to limit the presently disclosed systems or methods, their components, acts, or elements to either a single or plural configuration. References to any action or element based on any information, action, or element may include implementations in which the action or element is based at least in part on any information, action, or element.
Any embodiment disclosed herein may be combined with any other embodiment or example, and references to "an embodiment," "some embodiments," "one embodiment," etc., are not necessarily mutually exclusive, and are intended to indicate that a particular feature, structure, or characteristic described in connection with the embodiment may be included in at least one embodiment or example. These terms are not necessarily all referring to the same embodiment. Any embodiment may be combined with any other embodiment, either implicitly or exclusively, in any manner consistent with aspects and embodiments disclosed herein.
Reference to "or" may be construed as inclusive such that any term described using "or" may indicate any one of a single, more than one, and all of the described terms. Reference to at least one of the combined list of terms may be construed as inclusive or to indicate any one of the terms described singly, more than one, and all. For example, a reference to "at least one of a 'and B' may include only 'a', only 'B', and both 'a' and 'B'. Such references used in connection with "comprising" or other open terms may include additional items.
Where technical features in the figures, detailed description, or any claim are followed by reference signs, the reference signs have been included for the purpose of improving the intelligibility of the figures, detailed description, and claims. Accordingly, neither the reference numerals nor their absence have any limiting effect on the scope of any claim elements.
The systems and methods described herein may be embodied in other specific forms without departing from the characteristics thereof. The foregoing embodiments are illustrative and not limiting of the described systems and methods. The scope of the systems and methods described herein is, therefore, indicated by the appended claims rather than by the foregoing description, and all changes which come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein.
Claims (20)
1. A system, comprising:
a data processing system comprising a memory and one or more processors, which:
receiving an application configured with voice assistant compatible actions from an application developer;
identifying a plurality of actions the application is configured to perform in response to voice input;
identifying a plurality of content items provided by a plurality of third party computing devices;
generating a plurality of bit vectors corresponding to the plurality of actions by a machine learning model and performance data of the plurality of content items, the plurality of bit vectors indicating a plurality of candidate content items for each of the plurality of actions; and
In response to a request for content of a computing device executing an action of the application, a content item is selected based on a bit vector of the plurality of bit vectors that corresponds to the action.
2. The system of claim 1, comprising:
a data processing system crawling the application to identify the plurality of actions the application is configured to perform in response to voice input.
3. The system according to claim 1 or 2, comprising:
a data processing system that simulates one or more dialogs with the application to identify the plurality of actions.
4. The system of any preceding claim, comprising:
a data processing system crawling the application to generate a transcript comprising the plurality of actions the application is configured to perform according to one or more sessions, the transcript storing information about the plurality of actions and the one or more sessions for invoking the plurality of actions.
5. The system of any preceding claim, wherein the performance data of the plurality of content items indicates a type of interaction with each of the plurality of content items in response to previous presentation of each of the plurality of content items by one or more computing devices.
6. The system of claim 5, wherein the type of interaction comprises at least one of skip, fast forward, or duration of presentation.
7. A system according to any preceding claim, comprising a data processing system to:
receiving an indication of an action call from a computing device;
performing a lookup with the action in a data repository storing the plurality of bit vectors to retrieve a bit vector corresponding to the action;
identifying a plurality of top-ranked candidate content items from a bit vector of the action; and
a content item is selected from the plurality of top ranked candidate content items identified from the bit vector of the action.
8. The system of claim 7, comprising a data processing system to:
one or more signals associated with a computing device performing the action are used to select a content item from the plurality of top ranked candidate content items through a real-time content selection process.
9. A system according to any preceding claim, comprising a data processing system to:
receiving an indication of a level of interaction with a content item selected in response to the action and presented by the computing device; and
the machine learning model is retrained with the indication of the interaction level to update the bit vector corresponding to the action.
10. The system of any preceding claim, wherein each of the plurality of bit vectors stores a subset of the plurality of content items, the subset corresponding to a highest ranked content item.
11. A method, comprising:
receiving, by a data processing system including a memory and one or more processors, an application configured with voice assistant compatible actions from an application developer;
identifying, by the data processing system, a plurality of actions the application is configured to perform in response to the voice input;
identifying, by the data processing system, a plurality of content items provided by a plurality of third party computing devices;
generating, by the data processing system, a plurality of bit vectors corresponding to the plurality of actions through the machine learning model and the performance data of the plurality of content items, the plurality of bit vectors indicating a plurality of candidate content items for each of the plurality of actions; and
in response to a request for content of a computing device executing an action of the application, a content item is selected by the data processing system based on a bit vector of the plurality of bit vectors corresponding to the action.
12. The method of claim 11, comprising:
the application is crawled by a data processing system to identify the plurality of actions the application is configured to perform in response to a voice input.
13. The method according to claim 11 or 12, comprising:
one or more dialogs with the application are simulated by the data processing system to identify the plurality of actions.
14. The method according to any one of claims 11 to 13, comprising:
the application is crawled by a data processing system to generate a transcript including the plurality of actions the application is configured to perform according to one or more sessions, the transcript storing information about the plurality of actions and the one or more sessions for invoking the plurality of actions.
15. The method of any of claims 11-14, wherein the performance data of the plurality of content items indicates a type of interaction with each of the plurality of content items in response to a previous presentation of each of the plurality of content items by one or more computing devices.
16. The method of claim 15, wherein the type of interaction comprises at least one of skip, fast forward, or duration of presentation.
17. The method according to any one of claims 11 to 16, comprising:
receiving, by the data processing system, an indication of an action call from the computing device;
Performing, by the data processing system, a lookup with the action in a data repository storing the plurality of bit vectors to retrieve a bit vector corresponding to the action;
identifying, by the data processing system, a plurality of top-ranked candidate content items from the bit vector of actions; and
a content item is selected by the data processing system from the plurality of top ranked candidate content items identified from the bit vector of the action.
18. The method of claim 17, comprising:
one or more signals associated with the computing device performing the action are used by the data processing system to select a content item from the plurality of top ranked candidate content items through a real-time content selection process.
19. The method according to any one of claims 11 to 18, comprising:
receiving, by the data processing system, an indication of a level of interaction with a content item selected in response to the action and presented by the computing device; and
the machine learning model is retrained by the data processing system with the indication of the level of interaction to update the bit vector corresponding to the action.
20. The method of any of claims 11-19, wherein each of the plurality of bit vectors stores a subset of the plurality of content items, the subset corresponding to a highest ranked content item.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/042231 WO2023003537A1 (en) | 2021-07-19 | 2021-07-19 | Bit vector-based content matching for third-party digital assistant actions |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116075885A true CN116075885A (en) | 2023-05-05 |
Family
ID=77301010
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180017772.4A Pending CN116075885A (en) | 2021-07-19 | 2021-07-19 | Bit vector based content matching for third party digital assistant actions |
Country Status (5)
Country | Link |
---|---|
EP (1) | EP4143674A1 (en) |
JP (1) | JP2023535102A (en) |
KR (1) | KR20230014680A (en) |
CN (1) | CN116075885A (en) |
WO (1) | WO2023003537A1 (en) |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6499013B1 (en) | 1998-09-09 | 2002-12-24 | One Voice Technologies, Inc. | Interactive user interface using speech recognition and natural language processing |
US10573298B2 (en) | 2018-04-16 | 2020-02-25 | Google Llc | Automated assistants that accommodate multiple age groups and/or vocabulary levels |
EP4361777A2 (en) * | 2018-05-04 | 2024-05-01 | Google LLC | Generating and/or adapting automated assistant content according to a distance between user(s) and an automated assistant interface |
-
2021
- 2021-07-19 EP EP21752821.5A patent/EP4143674A1/en active Pending
- 2021-07-19 KR KR1020227029138A patent/KR20230014680A/en unknown
- 2021-07-19 WO PCT/US2021/042231 patent/WO2023003537A1/en active Application Filing
- 2021-07-19 JP JP2022552218A patent/JP2023535102A/en active Pending
- 2021-07-19 CN CN202180017772.4A patent/CN116075885A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
JP2023535102A (en) | 2023-08-16 |
WO2023003537A1 (en) | 2023-01-26 |
KR20230014680A (en) | 2023-01-30 |
EP4143674A1 (en) | 2023-03-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11929069B2 (en) | Proactive incorporation of unsolicited content into human-to-computer dialogs | |
CN110869969A (en) | Virtual assistant for generating personalized responses within a communication session | |
US20190347118A1 (en) | Identifying parameter values and determining features for boosting rankings of relevant distributable digital assistant operations | |
JP7171911B2 (en) | Generate interactive audio tracks from visual content | |
US11861316B2 (en) | Detection of relational language in human-computer conversation | |
US11514896B2 (en) | Interfacing with applications via dynamically updating natural language processing | |
Yoshino et al. | News navigation system based on proactive dialogue strategy | |
US11817093B2 (en) | Method and system for processing user spoken utterance | |
US11416229B2 (en) | Debugging applications for delivery via an application delivery server | |
KR102637614B1 (en) | Assessing applications for delivery via an application delivery server | |
CN116075885A (en) | Bit vector based content matching for third party digital assistant actions | |
US20230316940A1 (en) | Methods and systems for automatic speech signal processing using natural language processing (nlp) algorithms to provide personalized learning activities during real-time spoken conversations | |
US11900926B2 (en) | Dynamic expansion of acronyms in audio content | |
Patel et al. | My Buddy App: Communications between Smart Devices through Voice Assist | |
Sun et al. | Learning user intentions spanning multiple domains |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |