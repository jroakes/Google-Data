EP2836987B1 - Determining three-dimensional (3d) object data models based on object movement - Google Patents
Determining three-dimensional (3d) object data models based on object movement Download PDFInfo
- Publication number
- EP2836987B1 EP2836987B1 EP13728652.2A EP13728652A EP2836987B1 EP 2836987 B1 EP2836987 B1 EP 2836987B1 EP 13728652 A EP13728652 A EP 13728652A EP 2836987 B1 EP2836987 B1 EP 2836987B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- mesh image
- dimensional mesh
- movement
- view
- scan data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T17/00—Three dimensional [3D] modelling, e.g. data description of 3D objects
- G06T17/20—Finite element generation, e.g. wire-frame surface description, tesselation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/20—Editing of 3D images, e.g. changing shapes or colours, aligning objects or positioning parts
Definitions
- the present disclosure relates to three-dimensional (3D) object data modeling, and for example, to methods for determining a 3D object data model.
- Three-dimensional (3D) scanning and digitization of objects is commonly used in many industries and services and their applications are numerous.
- a few examples include 3D inspection and measurement of shape conformity in industrial production systems, digitization of clay models for industrial design and styling applications, reverse engineering of existing parts with complex geometry, interactive visualization of objects in multimedia applications, or three-dimensional documentation of artwork and artifacts.
- This disclosure may disclose, inter alia, methods and systems for determining three-dimensional (3D) object data models based on the movement of the 3D object.
- a method in one example, includes receiving by a processor first scan data of an object within a space.
- the first scan data comprises: (i) a first position of the object in the space and (ii) a first mesh image defining a first view of a shape of the object at the first position.
- the method also includes receiving second scan data of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space and (ii) a second mesh image defining a second view of the shape of the object at the second position.
- the method additionally includes aligning the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model of the object based on (i) the first mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, and (iii) the second mesh image that defines the second view of the shape of the object at the second position.
- 3D three-dimensional
- a system in another example, includes a computing device, a non-transitory computer-readable medium, and program instructions stored on the non-transitory computer-readable medium executable by the computing device to perform the functions of: receiving first scan data of an object within a space.
- the first scan data comprises: (i) a first position of the object in the space and (ii) a first mesh image defining a first view of a shape of the object at the first position.
- the functions also include receiving second scan data of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space and (ii) a second mesh image defining a second view of the shape of the object at the second position.
- the functions additionally include aligning the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model of the object based on (i) the first mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, and (iii) the second mesh image that defines the second view of the shape of the object at the second position.
- 3D three-dimensional
- a non-transitory computer readable medium having stored therein instructions executable by a computing device to cause the computing device to perform functions.
- the functions include receiving first scan data of an object within a space.
- the first scan data comprises: (i) a first position of the object in the space and (ii) a first mesh image defining a first view of a shape of the object at the first position.
- the functions also include receiving second scan data of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space and (ii) a second mesh image defining a second view of the shape of the object at the second position.
- the functions additionally include aligning the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model of the object based on (i) the first mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, and (iii) the second mesh image that defines the second view of the shape of the object at the second position.
- 3D three-dimensional
- This disclosure may disclose, inter alia, methods and systems for determining three-dimensional (3D) object data models of an object based on the movement of the object.
- Known methods and techniques may exist for determining and creating 3D models of objects based on data representing the objects.
- different mesh images of an object may be positioned and aligned to create the 3D model of the object.
- different scans of the object may be positioned and aligned to create the 3D model.
- to complete the alignment requires sufficient data (e.g., imagery) to allow for appropriate positioning and aligning of the data to create the model. At times, however, sufficient data may be unobtainable.
- the data may have been scanned incomplete, or stored in an unrelated manner (e.g., stored without providing information as to how each image relates or correlates to the other stored images). Under both scenarios, the positioning and alignment of the data may be less accurate thereby inhibiting the creation of the 3D model. Accordingly, it may be useful to track the movement of an object in attempt to determine how different scan data of the object is related, as well as using the movement information to interpolate any missing scan data.
- the example methods and systems disclosed herein relate to tracking the movement of an object in attempt to help with the initial positioning of data used to create a 3D object model.
- a processor may receive first scan data of an object within in a space.
- the first scan data may comprise: (i) a first position of the object in the space and (ii) a first mesh image defining a first view of a shape of the object at the first position.
- the processor may also receive second scan data of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space and (ii) a second mesh image defining a second view of the shape of the object at the second position.
- the processor may additionally align the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model of the object based on (i) the first mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, and (iii) the second mesh image that defines the second view of the shape of the object at the second position.
- 3D three-dimensional
- Figure 1 illustrates an example system 100 for determining 3D object data models based on the movement of an object.
- the system 100 includes an input source 102 coupled to a server 104 and a database 106.
- the server 104 is also shown coupled to the database 106 and an output target 108.
- the system 100 may include more or fewer components, and each of the input source 102, the server 104, the database 106, and the output target 108 may comprise multiple elements as well, or each of the input source 102, the server 104, the database 106, and the output target 108 may be interconnected as swell.
- one or more of the described functions of the system 100 may be divided up into additional functional or physical components, or combined into fewer functional or physical components.
- additional functional and/or physical components may be added to the examples illustrated by Figure 1 .
- Components of the system 100 may be coupled to or configured to be capable of communicating via a network (not shown), such as a local area network (LAN), wide area network (WAN), wireless network (Wi-Fi), or Internet, for example.
- a network such as a local area network (LAN), wide area network (WAN), wireless network (Wi-Fi), or Internet, for example.
- any of the components of the system 100 may be coupled to each other using wired or wireless communications.
- communication links between the input source 102 and the server 104 may include wired connections, such as a serial or parallel bus, or wireless links, such as Bluetooth, IEEE 802.11 (IEEE 802.11 may refer to IEEE 802.11-2007, IEEE 802.11n-2009, or any other IEEE 802.11 revision), or other wireless based communication links.
- the input source 102 may be any source from which 3D object scan data may be received.
- the 3D object scan data acquisition may be achieved by working with venders or manufacturers to scan an object in 3D at various angles and different positions.
- structured light scanners may capture images of the object and a shape of the object may be recovered using monochrome stereo cameras and a pattern projector.
- a video camera may be mounted on the scanner to capture structure-form motion data of the object as the object is moved by a user, for example.
- a high-resolution digital single-lens reflex (DSLR) camera may be used to capture images as the object moves around on a turntable.
- DSLR digital single-lens reflex
- a raw computer-aided drafting (CAD) set of drawings may be received for each object along with translation and/or rotation matrices and view information that defines the movement of the object and the view of the object associated with each drawing.
- the input source 102 may provide 3D object data, in various forms, to the server 104.
- multiple scans of the object from varying viewpoints may be processed into individual mesh images, and provided, along with a rotation matrix of the object, to the server 104 in that form.
- the server 104 includes a model processor 110, an object data model builder 112, a semantics and search index 114, and a graphics library 116. Any of the components of the server 104 may be coupled to each other. In addition, any components of the server 104 may alternatively be a separate component coupled to the server 104.
- the server 104 may further include a processor and memory including instructions executable by the processor to perform functions of the components of the server 104, for example.
- the object data model processor 110 receives the 3D object scan data for each object from the input source 102 and generates triangulated mesh images, when necessary. For instance, the 3D object scan data that is received may be decimated (e.g., from 5 million to 120,000 surfaces) utilizing texture-preserving decimation.
- the model builder 112 may also receive the 3D object scan data for each object from the input source 102, which may include a data set defining a mesh image of the object, and may align and generate an animated model of the object in 3D. In other examples, the model builder 112 may receive the processed 3D object scan data from the data model processer 112 and generate a 3D model of the object using the triangulated mesh images.
- the view and shape image index 114 may receive captured data or processed data that represent or are related to the object on which the 3D object data model is based. For example, for each 3D object data model, the view and shape image index 114 may receive one or multiple mesh images of the object from different viewpoints and may index the mesh images as such. The view and shape image index 114 may also receive movement information for the corresponding 3D object scan data.
- the graphics library 116 may include a WebGL or OpenGL mesh compression to reduce a mesh file size, for example.
- the graphics library 116 may provide the 3D object data model in a form for display on a browser, for example.
- a 3D object data model viewer may be used to display images of the 3D objects data models.
- the 3D object data model viewer may be implemented using WebGL within a web browser, or OpenGL, for example.
- the database 106 may store all data sets for a 3D object data model in any number of various forms from raw data captured to processed data for display.
- the database 106 may store various models of the 3D object data (shown in Figure 3D ).
- the output target 108 may include a number of different targets, such as a webpage on the Internet, a search engine, a database, etc.
- the output target 108 may include a 3D object data model viewer that enables product advertisements or product searches based on the 3D object data model.
- the output target 108 may include an HTML webpage that displays multiple rendered images of the 3D object data model in the form of advertisements.
- the output target 108 may receive from the database 106 the 3D object data model for display in a 3D object data model viewer.
- system 100 may be used to acquire 3D object scan data of an object as it moves and process the data to generate an accurate 3D object data model for display in a 3D object data model viewer.
- Figure 2 is a block diagram of an example method for determining 3D object data models based on object movement.
- Method 200 shown in Figure 2 presents an embodiment of a method that, for example, could be used with the system 100, and may be performed by a device such as any of the components illustrated in Figure 1 .
- Method 200 may include one or more operations, functions, or actions as illustrated by one or more of blocks 202-206. Although the blocks are illustrated in a sequential order, these blocks may also be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation.
- each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor or computing device for implementing specific logical functions or steps in the process.
- the program code may be stored on any type of computer readable medium or memory, for example, such as a storage device including a disk or hard drive.
- the computer readable medium may include non-transitory computer readable medium, for example, such as computer-readable media that stores data for short periods of time like register memory, processor cache and Random Access Memory (RAM).
- the computer readable medium may also include non-transitory media, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- the computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
- each block in Figure 2 may represent circuitry that is wired to perform the specific logical functions in the process.
- the method 200 includes receiving first scan data of an object within a space.
- the space may comprise a 3D coordinate system.
- a server or other computing device may receive the first scan data of the object from a number of sources, such as input by a user, from a scanner, from another server, or database, for example.
- the first scan data may comprise: (i) a first position of the object in the space and (ii) a first mesh image defining a first view of a shape of the object at the first position.
- the first position of the object may, for example, be at the top of a coordinate system defining the space, at the bottom of the coordinate system, or in the middle of the coordinate system, and may be a coordinate defining a point in the center of the object.
- the first position may be any other coordinate defining a point on the object used to indicate the first position of the object in the coordinate system.
- the first mesh image may comprise a collection of vertices, edges, and faces that define the shape of the object and may depict the object from varying angles.
- the mesh image may depict a view from the bottom of the coordinate system directed upwards toward the object.
- the mesh image may depict a different view (e.g., shown in Figure 3A ).
- the first scan data may also comprise other data representations of the object such as a DSLR image of the object depicting the object from varying angles, all of which are contemplated herein.
- the first scan data may comprise non-rigid movement information defining a deformable surface area of the object.
- the deformable surface area may be deformed independent of the movement of the object.
- the non-rigid movement information may comprise coordinates in the coordinate system relative to the first position of the object in the space.
- the first scan data may comprise a mesh image of a bear with two arms and two legs.
- the arms and legs of the bear may be deformed without moving the bear in its entirety.
- the bear may be held still and the arm of the bear may be stretched upward.
- the legs may be squashed without movement of the arms or the bear in its entirety.
- the non-rigid movement information may comprise, for example, coordinates that define the location of the deformable arm on the mesh image of the bear.
- the method 200 includes receiving second scan data of the object.
- the second scan data may comprise: (i) movement information defining a movement of the object from the first position to a second position within the space and (ii) a second mesh image defining a second view of the shape of the object at the second position.
- the movement of the object may occur by user control of the object or automatically.
- system 100 may automatically operate a motorized arm to move the object to a desired position or elevation.
- the object may be moved using a turntable.
- the movement of the object may comprise a translation, a rotation, or a warp of the object, for example.
- the movement of the object may comprise an angulation of the object.
- the server 104 may receive movement information defining one or more of the translation, the rotation, the angulation, and the warp from the input 102. Similar to the first mesh image, the second mesh image may comprise a collection of vertices, edges, and faces that define the shape of the object and may depict the object from varying angles.
- the movement information may comprise a movement coordinate that defines the second position of the object.
- a translation matrix and/or a rotation matrix may be determined.
- a technique can be implemented to track the movement of a coordinate on the object in real-time.
- a particular coordinate on the object may be selected by a user, for example, for tracking.
- the movement information may be used to determine a feature point motion map defining the movement of the particular coordinate on the object.
- One or more coordinates may be selected for tracking.
- the second scan data may comprise non-rigid movement information that defines the deformation of the deformable surface area of the object within the space.
- the second scan data may comprise a deformation matrix that defines the deformation of the arm of the bear.
- the method 200 includes aligning the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model of the object.
- the first scan data and the second scan data may be aligned based on (i) the first mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, and (iii) the second mesh image that defines the second view of the shape of the object at the second position.
- the 3D object data model may be generated, for example, by aligning the first mesh image and the second mesh image (e.g., shown in Fig. 3B ) using the translation matrix and/or rotation matrix to position the mesh images together for alignment.
- the 3D object data model may be generated by aligning the first mesh image and the second mesh image using a feature point motion map (e.g., shown in Fig. 3C ) of the object by tracking the movement of the object in real time so as to position the first mesh image and the second mesh image together for alignment.
- the 3D object data model may be generated, for example, by aligning the first mesh image and the second mesh image using a deformation matrix so as to further position the first mesh image of the data representation and the second mesh image of the second data representation for alignment.
- the deformation matrix may be determined using the non-rigid movement information and/or the deformation information.
- first scan data and the second scan data in this manner may (1) allow the data (e.g., the first mesh image and the second mesh image) to be better positioned for alignment to create the 3D object model, and (2) allow any missing data to be interpolated based on the correctly positioned data.
- creation of a 3D object model representing a shoe can be performed by tracking movement of the shoe, and using the movement information to position mesh images of the shoe together for alignment.
- the movement information may be captured along with alternate views of the object (e.g., a top view and side view and capturing rotation or translation matrices representing movement from the top view to the side view), and any incomplete data may be interpolated based on the captured data. This positioning may help ensure the aligning process is more accurate.
- alignment of the data may be performed.
- the alignment of the 3D object data model may be performed manually.
- the alignment can be refined automatically using techniques such as the Iterative Closest Point (ICP) algorithm of Besl and McKay (See, “ A method for registration of 3-D shapes”; by Besl, P. J. and McKay, H. D. in Patten Analysis and Machine Intelligence, IEEE Transactions, Vol. 14, Issue 2, February 1992, pp. 239-256 ) and Chen and Medioni (See, " object modeling by registration of multiple range image”; Y. Chen and G. Medioni, Proc. IEEE Conf.
- ICP Iterative Closest Point
- the ICP algorithm iteratively establishes point correspondences given the current alignment of scanned data and computes a rigid transformation. More specifically, the ICP algorithm is employed to minimize the difference between two clouds of points.
- the clouds of points may be obtained from multiple scans of an object, for example.
- the ICP algorithm iteratively revises, in real time, the rigid transformation (i.e., translation and rotation) needed to minimize the distance between the points of the different scans.
- the ICP algorithm repeatedly establishes points on one or both scans (e.g., mesh images of a 3D object) finding closest points on the other respective scan, and computes the rigid body transformation that minimizes the least squares error between the two point sets.
- the Non-rigid range-scan alignment algorithm is a variant of the ICP algorithm and is employed to reconstruct non-rigid deformations. Such deformations may include deformations applied to a deformable object, or deformations caused by scanning device nonlinearities or calibration error.
- the algorithm first obtains sparse correspondences between scan views using a locally weighted, stability-guaranteeing variant of the ICP. Global positions for feature points are found (a common set of features on all scans) using a relaxation method, and the scans are warped to their final positions using thin-plate spines. Thin-plate spine warp is a widely known smoothing function used for modeling coordinate transformations.
- the object data model processor 110 may execute instructions to run an application that performs the ICP technique on the mesh images of the first scan data and the mesh images of the second scan data after the mesh images are positioned.
- the generated 3D object model may then be stored in the database 106 and displayed at the output target 108.
- a plurality of generated 3D object data models may be provided to the database 106. Any structuring technique may be used by the processor 110 to render the 3D data object model for display at the output target 108.
- Figures 3A-3D conceptually illustrate examples of receiving first scan data of an object within a space, receiving second scan data of the object that comprises movement information defining a movement of the object, and processing the first scan data and the second scan data so as to generate a 3D object data model of the object.
- Figure 3A represents first scan data 310 of a car (i.e., object) within a space 300.
- the space 300 comprises a coordinate system 312, and the car is depicted from a first view.
- Figure 3B illustrates second scan data 314-318 of the car within the space 300.
- the data comprises mesh images of the car based on its movement throughout the space 300.
- the second scan data comprises a second view (e.g., 314-318) of the car 310.
- the second scan data includes movement data.
- each data set might have a position coordinate, a rotation matrix and a translation matrix defining the movement of the car.
- the car may be moved straight up from its original position (i.e., translation) and rotated ninety degrees to the right (rotation) in space 300 and obtained as second scan data 314.
- Figure 3C illustrates another embodiment of second scan data 320 of the car within the space 300.
- the data comprises images of the car based on its movement throughout the space 300.
- the scan data also includes feature points 322 that track the movement of the car as it moves throughout the space.
- Figure 3D illustrates an example of a 3D object data model 324 of the car 310 being displayed in a 3D object model viewer or HTML web page, for example. As shown a complete 3D object model may be created based on the first scan data 310 and the second scan data 314-322 of the car, for example.
- FIG 4 is a functional block diagram illustrating an example computing device used in a computing system that is arranged in accordance with at least some embodiments described herein.
- the computing device may be a personal computer, mobile device, cellular phone, touch-sensitive wristwatch, tablet computer, video game system, or global positioning system, and may be implemented to provide a system for interacting with 3D object data models as described in Figures 1-3 .
- computing device 400 may typically include one or more processors 410 and system memory 420.
- a memory bus 430 can be used for communicating between the processor 410 and the system memory 420.
- processor 410 can be of any type including but not limited to a microprocessor ( ⁇ P), a microcontroller ( ⁇ C), a digital signal processor (DSP), or any combination thereof.
- ⁇ P microprocessor
- ⁇ C microcontroller
- DSP digital signal processor
- a memory controller 415 can also be used with the processor 410, or in some implementations, the memory controller 415 can be an internal part of the processor 410.
- system memory 420 can be of any type including but not limited to volatile memory (such as RAM), non-volatile memory (such as ROM, flash memory, etc.) or any combination thereof.
- System memory 420 may include one or more applications 422, and program data 424.
- Application 422 may include an index algorithm 423 that is arranged to provide inputs to the electronic circuits, in accordance with the present disclosure.
- Program data 424 may include content information 425 that could be directed to any number of types of data.
- application 422 can be arranged to operate with program data 424 on an operating system.
- Computing device 400 can have additional features or functionality, and additional interfaces to facilitate communications between the basic configuration 402 and any devices and interfaces.
- data storage devices 440 can be provided including removable storage devices 442, non-removable storage devices 444, or a combination thereof.
- removable storage and non-removable storage devices include magnetic disk devices such as flexible disk drives and hard-disk drives (HDD), optical disk drives such as compact disk (CD) drives or digital versatile disk (DVD) drives, solid state drives (SSD), and tape drives to name a few.
- Computer storage media can include volatile and nonvolatile, non-transitory, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data.
- System memory 420 and storage devices 440 are examples of computer storage media.
- Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing device 400. Any such computer storage media can be part of device 400.
- Computing device 400 can also include output interfaces 450 that may include a graphics processing unit 452, which can be configured to communicate to various external devices such as display devices 490 or speakers via one or more A/V ports or a communication interface 470.
- the communication interface 470 may include a network controller 472, which can be arranged to facilitate communications with one or more other computing devices 480 over a network communication via one or more communication ports 474.
- the communication connection is one example of a communication media. Communication media may be embodied by computer readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and includes any information delivery media.
- a modulated data signal can be a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.
- communication media can include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared (IR) and other wireless media.
- RF radio frequency
- IR infrared
- Computing device 400 can be implemented as a portion of a small-form factor portable (or mobile) electronic device such as a cell phone, a personal data assistant (PDA), a personal media player device, a wireless web-watch device, a personal headset device, an application specific device, or a hybrid device that include any of the above functions.
- a small-form factor portable (or mobile) electronic device such as a cell phone, a personal data assistant (PDA), a personal media player device, a wireless web-watch device, a personal headset device, an application specific device, or a hybrid device that include any of the above functions.
- PDA personal data assistant
- Computing device 400 can also be implemented as a personal computer including both laptop computer and non-laptop computer configuration.
- the disclosed methods may be implemented as computer program instructions encoded on a non-transitory computer-readable storage media in a machine-readable format, or on other non-transitory media or articles of manufacture.
- Figure 5 is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device, arranged according to at least some embodiments presented herein.
- the example computer program product 500 is provided using a signal bearing medium 501.
- the signal bearing medium 501 may include one or more programming instructions 502 that, when executed by one or more processors may provide functionality or portions of the functionality described above with respect to Figures 1-3 .
- the signal bearing medium 501 may encompass a computer-readable medium 503, such as, but not limited to, a hard disk drive, a Compact Disc (CD), a Digital Video Disk (DVD), a digital tape, memory, etc.
- the signal bearing medium 501 may encompass a computer recordable medium 504, such as, but not limited to, memory, read/write (R/W) CDs, R/W DVDs, etc.
- the signal bearing medium 501 may encompass a communications medium 505, such as, but not limited to, a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).
- a communications medium 505 such as, but not limited to, a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).
- the signal bearing medium 501 may be conveyed by a wireless form of the communications medium 505 (e.g., a wireless communications medium conforming with the IEEE 802.11 standard or other transmission protocol).
- the one or more programming instructions 502 may be, for example, computer executable and/or logic implemented instructions.
- a computing device such as the computing device 400 of Figure 4 may be configured to provide various operations, functions, or actions in response to the programming instructions 502 conveyed to the computing device 500 by one or more of the computer readable medium 503, the computer recordable medium 504, and/or the communications medium 505.
Description
- The present non-provisional utility application claims priority under 35 U.S.C. § 119(e) to co-pending provisional application number
US 61/674,140 filed on July 20, 2012 - The present disclosure relates to three-dimensional (3D) object data modeling, and for example, to methods for determining a 3D object data model.
- Three-dimensional (3D) scanning and digitization of objects is commonly used in many industries and services and their applications are numerous. A few examples include 3D inspection and measurement of shape conformity in industrial production systems, digitization of clay models for industrial design and styling applications, reverse engineering of existing parts with complex geometry, interactive visualization of objects in multimedia applications, or three-dimensional documentation of artwork and artifacts.
- A wide range of techniques exist for acquiring 3D data from an object. These techniques include using structured laser illumination or other controlled stimulus (such as x-rays, ultrasound, or magnetic resonance) to techniques that operate directly on video data captured from one or more cameras. Regardless of the particular sensing technology, a scanning process can be divided into abstract steps of incremental data capture, derivation of 3D data, and registration of the incremental data to a common 3D coordinate system. The final registration step brings the incremental data together into a single 3D model of a scan object.
- SYNAVE R ET AL: "Automated Trimmed Iterative Closest Point Algorithm", 26 November 2007 (2007-11-26), ADVANCES IN VISUAL COMPUTING; [LECTURE NOTES IN COMPUTER SCIENCE], SPRINGER BERLIN HEIDELBERG, BERLIN, HEIDELBERG, PAGE(S) 489 - 498, XP019083447,ISBN: 978-3-540-76855-5, describes a method for automatic registration based on an Iterative Closest Point (ICP) approach.
-
US2003/063086 describes a 3D computer model processing apparatus. - This disclosure may disclose, inter alia, methods and systems for determining three-dimensional (3D) object data models based on the movement of the 3D object.
- In one example, a method is provided that includes receiving by a processor first scan data of an object within a space. The first scan data comprises: (i) a first position of the object in the space and (ii) a first mesh image defining a first view of a shape of the object at the first position. The method also includes receiving second scan data of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space and (ii) a second mesh image defining a second view of the shape of the object at the second position. The method additionally includes aligning the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model of the object based on (i) the first mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, and (iii) the second mesh image that defines the second view of the shape of the object at the second position.
- In another example, a system is provided that includes a computing device, a non-transitory computer-readable medium, and program instructions stored on the non-transitory computer-readable medium executable by the computing device to perform the functions of: receiving first scan data of an object within a space. The first scan data comprises: (i) a first position of the object in the space and (ii) a first mesh image defining a first view of a shape of the object at the first position. The functions also include receiving second scan data of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space and (ii) a second mesh image defining a second view of the shape of the object at the second position. The functions additionally include aligning the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model of the object based on (i) the first mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, and (iii) the second mesh image that defines the second view of the shape of the object at the second position.
- In another example, a non-transitory computer readable medium having stored therein instructions executable by a computing device to cause the computing device to perform functions is provided. The functions include receiving first scan data of an object within a space. The first scan data comprises: (i) a first position of the object in the space and (ii) a first mesh image defining a first view of a shape of the object at the first position. The functions also include receiving second scan data of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space and (ii) a second mesh image defining a second view of the shape of the object at the second position. The functions additionally include aligning the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model of the object based on (i) the first mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, and (iii) the second mesh image that defines the second view of the shape of the object at the second position.
- The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects, embodiments, and features described above, further aspects, embodiments, and features will become apparent by reference to the figures and the following detailed description.
-
-
Figure 1 is a diagram illustrating an example system for determining three-dimensional (3D) object data models based on the movement of an object, in accordance with embodiments described herein. -
Figure 2 is a block diagram of an example method for determining 3D object data models based on the movement of the object, in accordance with embodiments described herein. -
Figures 3A-3D conceptually illustrate examples of determining a 3D object data model based on the movement of the object, in accordance with embodiments described herein. -
Figure 4 is a functional block diagram illustrating an example computing device used in a computing system, in accordance with embodiments described herein. -
Figure 5 is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device, in accordance with embodiments described herein. - The following detailed description includes references to the accompanying figures. In the figures, similar symbols typically identify similar components, unless context dictates otherwise. The example embodiments described in the detailed description, figures, and claims are not meant to be limiting. Other embodiments may be utilized, and other changes may be made, without departing from the scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure, as generally described herein and illustrated in the figures can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are contemplated herein.
- This disclosure may disclose, inter alia, methods and systems for determining three-dimensional (3D) object data models of an object based on the movement of the object. Known methods and techniques may exist for determining and creating 3D models of objects based on data representing the objects. In one example, different mesh images of an object may be positioned and aligned to create the 3D model of the object. In other examples, different scans of the object may be positioned and aligned to create the 3D model. Regardless of the specific type of data that may be used to create the 3D model, to complete the alignment requires sufficient data (e.g., imagery) to allow for appropriate positioning and aligning of the data to create the model. At times, however, sufficient data may be unobtainable. The data may have been scanned incomplete, or stored in an unrelated manner (e.g., stored without providing information as to how each image relates or correlates to the other stored images). Under both scenarios, the positioning and alignment of the data may be less accurate thereby inhibiting the creation of the 3D model. Accordingly, it may be useful to track the movement of an object in attempt to determine how different scan data of the object is related, as well as using the movement information to interpolate any missing scan data. The example methods and systems disclosed herein relate to tracking the movement of an object in attempt to help with the initial positioning of data used to create a 3D object model.
- In one example a processor may receive first scan data of an object within in a space. The first scan data may comprise: (i) a first position of the object in the space and (ii) a first mesh image defining a first view of a shape of the object at the first position. The processor may also receive second scan data of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space and (ii) a second mesh image defining a second view of the shape of the object at the second position. The processor may additionally align the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model of the object based on (i) the first mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, and (iii) the second mesh image that defines the second view of the shape of the object at the second position.
- Referring now to the figures,
Figure 1 illustrates anexample system 100 for determining 3D object data models based on the movement of an object. Thesystem 100 includes aninput source 102 coupled to aserver 104 and adatabase 106. Theserver 104 is also shown coupled to thedatabase 106 and an output target 108. Thesystem 100 may include more or fewer components, and each of theinput source 102, theserver 104, thedatabase 106, and the output target 108 may comprise multiple elements as well, or each of theinput source 102, theserver 104, thedatabase 106, and the output target 108 may be interconnected as swell. Thus, one or more of the described functions of thesystem 100 may be divided up into additional functional or physical components, or combined into fewer functional or physical components. In some further examples, additional functional and/or physical components may be added to the examples illustrated byFigure 1 . - Components of the
system 100 may be coupled to or configured to be capable of communicating via a network (not shown), such as a local area network (LAN), wide area network (WAN), wireless network (Wi-Fi), or Internet, for example. In addition, any of the components of thesystem 100 may be coupled to each other using wired or wireless communications. For example, communication links between theinput source 102 and theserver 104 may include wired connections, such as a serial or parallel bus, or wireless links, such as Bluetooth, IEEE 802.11 (IEEE 802.11 may refer to IEEE 802.11-2007, IEEE 802.11n-2009, or any other IEEE 802.11 revision), or other wireless based communication links. - The
input source 102 may be any source from which 3D object scan data may be received. In some examples, the 3D object scan data acquisition (movement, shape, appearance, and view) may be achieved by working with venders or manufacturers to scan an object in 3D at various angles and different positions. For instance, structured light scanners may capture images of the object and a shape of the object may be recovered using monochrome stereo cameras and a pattern projector. A video camera may be mounted on the scanner to capture structure-form motion data of the object as the object is moved by a user, for example. In further examples, a high-resolution digital single-lens reflex (DSLR) camera may be used to capture images as the object moves around on a turntable. In still other examples, a raw computer-aided drafting (CAD) set of drawings may be received for each object along with translation and/or rotation matrices and view information that defines the movement of the object and the view of the object associated with each drawing. Thus, theinput source 102 may provide 3D object data, in various forms, to theserver 104. As one example, multiple scans of the object from varying viewpoints may be processed into individual mesh images, and provided, along with a rotation matrix of the object, to theserver 104 in that form. - The
server 104 includes a model processor 110, an objectdata model builder 112, a semantics andsearch index 114, and agraphics library 116. Any of the components of theserver 104 may be coupled to each other. In addition, any components of theserver 104 may alternatively be a separate component coupled to theserver 104. Theserver 104 may further include a processor and memory including instructions executable by the processor to perform functions of the components of theserver 104, for example. - The object data model processor 110 receives the 3D object scan data for each object from the
input source 102 and generates triangulated mesh images, when necessary. For instance, the 3D object scan data that is received may be decimated (e.g., from 5 million to 120,000 surfaces) utilizing texture-preserving decimation. - The
model builder 112 may also receive the 3D object scan data for each object from theinput source 102, which may include a data set defining a mesh image of the object, and may align and generate an animated model of the object in 3D. In other examples, themodel builder 112 may receive the processed 3D object scan data from thedata model processer 112 and generate a 3D model of the object using the triangulated mesh images. - The view and
shape image index 114 may receive captured data or processed data that represent or are related to the object on which the 3D object data model is based. For example, for each 3D object data model, the view andshape image index 114 may receive one or multiple mesh images of the object from different viewpoints and may index the mesh images as such. The view andshape image index 114 may also receive movement information for the corresponding 3D object scan data. - The
graphics library 116 may include a WebGL or OpenGL mesh compression to reduce a mesh file size, for example. Thegraphics library 116 may provide the 3D object data model in a form for display on a browser, for example. In some examples, a 3D object data model viewer may be used to display images of the 3D objects data models. The 3D object data model viewer may be implemented using WebGL within a web browser, or OpenGL, for example. - The
database 106 may store all data sets for a 3D object data model in any number of various forms from raw data captured to processed data for display. For example, thedatabase 106 may store various models of the 3D object data (shown inFigure 3D ). - The output target 108 may include a number of different targets, such as a webpage on the Internet, a search engine, a database, etc. The output target 108 may include a 3D object data model viewer that enables product advertisements or product searches based on the 3D object data model. In a further example, the output target 108 may include an HTML webpage that displays multiple rendered images of the 3D object data model in the form of advertisements. In operation, for example, the output target 108 may receive from the
database 106 the 3D object data model for display in a 3D object data model viewer. - In examples herein, the
system 100 may be used to acquire 3D object scan data of an object as it moves and process the data to generate an accurate 3D object data model for display in a 3D object data model viewer. -
Figure 2 is a block diagram of an example method for determining 3D object data models based on object movement.Method 200 shown inFigure 2 presents an embodiment of a method that, for example, could be used with thesystem 100, and may be performed by a device such as any of the components illustrated inFigure 1 .Method 200 may include one or more operations, functions, or actions as illustrated by one or more of blocks 202-206. Although the blocks are illustrated in a sequential order, these blocks may also be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation. - In addition, for the
method 200 and other processes and methods disclosed herein, the flowchart shows functionality and operation of one possible implementation of present embodiments. In this regard, each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor or computing device for implementing specific logical functions or steps in the process. The program code may be stored on any type of computer readable medium or memory, for example, such as a storage device including a disk or hard drive. The computer readable medium may include non-transitory computer readable medium, for example, such as computer-readable media that stores data for short periods of time like register memory, processor cache and Random Access Memory (RAM). The computer readable medium may also include non-transitory media, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example. The computer readable media may also be any other volatile or non-volatile storage systems. The computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device. - In addition, for the
method 200 and other processes and methods disclosed herein, each block inFigure 2 may represent circuitry that is wired to perform the specific logical functions in the process. - Initially, at
block 202, themethod 200 includes receiving first scan data of an object within a space. The space may comprise a 3D coordinate system. A server or other computing device may receive the first scan data of the object from a number of sources, such as input by a user, from a scanner, from another server, or database, for example. - The first scan data may comprise: (i) a first position of the object in the space and (ii) a first mesh image defining a first view of a shape of the object at the first position. The first position of the object may, for example, be at the top of a coordinate system defining the space, at the bottom of the coordinate system, or in the middle of the coordinate system, and may be a coordinate defining a point in the center of the object. In other examples, the first position may be any other coordinate defining a point on the object used to indicate the first position of the object in the coordinate system. The first mesh image may comprise a collection of vertices, edges, and faces that define the shape of the object and may depict the object from varying angles. For instance, the mesh image may depict a view from the bottom of the coordinate system directed upwards toward the object. In other examples, the mesh image may depict a different view (e.g., shown in
Figure 3A ). The first scan data may also comprise other data representations of the object such as a DSLR image of the object depicting the object from varying angles, all of which are contemplated herein. - In further examples, the first scan data may comprise non-rigid movement information defining a deformable surface area of the object. The deformable surface area may be deformed independent of the movement of the object. The non-rigid movement information may comprise coordinates in the coordinate system relative to the first position of the object in the space.
- For example, the first scan data may comprise a mesh image of a bear with two arms and two legs. The arms and legs of the bear may be deformed without moving the bear in its entirety. For instance, the bear may be held still and the arm of the bear may be stretched upward. In other examples, the legs may be squashed without movement of the arms or the bear in its entirety. The non-rigid movement information may comprise, for example, coordinates that define the location of the deformable arm on the mesh image of the bear.
- At
block 204, themethod 200 includes receiving second scan data of the object. The second scan data may comprise: (i) movement information defining a movement of the object from the first position to a second position within the space and (ii) a second mesh image defining a second view of the shape of the object at the second position. The movement of the object may occur by user control of the object or automatically. For example,system 100 may automatically operate a motorized arm to move the object to a desired position or elevation. In other examples, the object may be moved using a turntable. The movement of the object may comprise a translation, a rotation, or a warp of the object, for example. In other examples, the movement of the object may comprise an angulation of the object. Theserver 104 may receive movement information defining one or more of the translation, the rotation, the angulation, and the warp from theinput 102. Similar to the first mesh image, the second mesh image may comprise a collection of vertices, edges, and faces that define the shape of the object and may depict the object from varying angles. - The movement information may comprise a movement coordinate that defines the second position of the object. Using the first position coordinate of the first scan data, the movement coordinate of the second scan data, the angle of the first mesh image, and the angle of the second mesh image, a translation matrix and/or a rotation matrix may be determined. In other examples, a technique can be implemented to track the movement of a coordinate on the object in real-time. A particular coordinate on the object may be selected by a user, for example, for tracking. As the object is moved the object may be tracked and, the movement information may be used to determine a feature point motion map defining the movement of the particular coordinate on the object. One or more coordinates may be selected for tracking.
- In further examples, the second scan data may comprise non-rigid movement information that defines the deformation of the deformable surface area of the object within the space. Continuing with the bear example above, the second scan data may comprise a deformation matrix that defines the deformation of the arm of the bear.
- At
block 206, themethod 200 includes aligning the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model of the object. The first scan data and the second scan data may be aligned based on (i) the first mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, and (iii) the second mesh image that defines the second view of the shape of the object at the second position. The 3D object data model may be generated, for example, by aligning the first mesh image and the second mesh image (e.g., shown inFig. 3B ) using the translation matrix and/or rotation matrix to position the mesh images together for alignment. In other examples, the 3D object data model may be generated by aligning the first mesh image and the second mesh image using a feature point motion map (e.g., shown inFig. 3C ) of the object by tracking the movement of the object in real time so as to position the first mesh image and the second mesh image together for alignment. In further examples, the 3D object data model may be generated, for example, by aligning the first mesh image and the second mesh image using a deformation matrix so as to further position the first mesh image of the data representation and the second mesh image of the second data representation for alignment. The deformation matrix may be determined using the non-rigid movement information and/or the deformation information. - Using the first scan data and the second scan data in this manner may (1) allow the data (e.g., the first mesh image and the second mesh image) to be better positioned for alignment to create the 3D object model, and (2) allow any missing data to be interpolated based on the correctly positioned data. For example, creation of a 3D object model representing a shoe can be performed by tracking movement of the shoe, and using the movement information to position mesh images of the shoe together for alignment. The movement information may be captured along with alternate views of the object (e.g., a top view and side view and capturing rotation or translation matrices representing movement from the top view to the side view), and any incomplete data may be interpolated based on the captured data. This positioning may help ensure the aligning process is more accurate.
- Once the scan data has been positioned (e.g., the first mesh image and the second mesh image) using the movement information, alignment of the data may be performed. In many systems the alignment of the 3D object data model may be performed manually. The alignment can be refined automatically using techniques such as the Iterative Closest Point (ICP) algorithm of Besl and McKay (See, "A method for registration of 3-D shapes"; by Besl, P. J. and McKay, H. D. in Patten Analysis and Machine Intelligence, IEEE Transactions, Vol. 14, Issue 2, February 1992, pp. 239-256) and Chen and Medioni (See, "object modeling by registration of multiple range image"; Y. Chen and G. Medioni, Proc. IEEE Conf. on Robotics and Automation, 1991), or the Non-rigid range-scan alignment of Brown and Rusinkiewiez (See " Non-rigid range-scan alignment using thin-plate splines"; by Brown, B. J. and Rusinkiewicz, S. in ), which are both known the art.
- The ICP algorithm iteratively establishes point correspondences given the current alignment of scanned data and computes a rigid transformation. More specifically, the ICP algorithm is employed to minimize the difference between two clouds of points. The clouds of points may be obtained from multiple scans of an object, for example. In operation, the ICP algorithm iteratively revises, in real time, the rigid transformation (i.e., translation and rotation) needed to minimize the distance between the points of the different scans. In other words, the ICP algorithm repeatedly establishes points on one or both scans (e.g., mesh images of a 3D object) finding closest points on the other respective scan, and computes the rigid body transformation that minimizes the least squares error between the two point sets.
- The Non-rigid range-scan alignment algorithm is a variant of the ICP algorithm and is employed to reconstruct non-rigid deformations. Such deformations may include deformations applied to a deformable object, or deformations caused by scanning device nonlinearities or calibration error. To achieve this, the algorithm first obtains sparse correspondences between scan views using a locally weighted, stability-guaranteeing variant of the ICP. Global positions for feature points are found (a common set of features on all scans) using a relaxation method, and the scans are warped to their final positions using thin-plate spines. Thin-plate spine warp is a widely known smoothing function used for modeling coordinate transformations.
- In one example, the object data model processor 110 may execute instructions to run an application that performs the ICP technique on the mesh images of the first scan data and the mesh images of the second scan data after the mesh images are positioned. The generated 3D object model may then be stored in the
database 106 and displayed at the output target 108. In another example, a plurality of generated 3D object data models may be provided to thedatabase 106. Any structuring technique may be used by the processor 110 to render the 3D data object model for display at the output target 108. -
Figures 3A-3D conceptually illustrate examples of receiving first scan data of an object within a space, receiving second scan data of the object that comprises movement information defining a movement of the object, and processing the first scan data and the second scan data so as to generate a 3D object data model of the object.Figure 3A representsfirst scan data 310 of a car (i.e., object) within aspace 300. Thespace 300 comprises a coordinatesystem 312, and the car is depicted from a first view. -
Figure 3B illustrates second scan data 314-318 of the car within thespace 300. As shown the data comprises mesh images of the car based on its movement throughout thespace 300. The second scan data comprises a second view (e.g., 314-318) of thecar 310. Moreover, the second scan data includes movement data. For example, each data set might have a position coordinate, a rotation matrix and a translation matrix defining the movement of the car. For example, the car may be moved straight up from its original position (i.e., translation) and rotated ninety degrees to the right (rotation) inspace 300 and obtained assecond scan data 314. -
Figure 3C illustrates another embodiment ofsecond scan data 320 of the car within thespace 300. As shown the data comprises images of the car based on its movement throughout thespace 300. The scan data also includes feature points 322 that track the movement of the car as it moves throughout the space. -
Figure 3D illustrates an example of a 3Dobject data model 324 of thecar 310 being displayed in a 3D object model viewer or HTML web page, for example. As shown a complete 3D object model may be created based on thefirst scan data 310 and the second scan data 314-322 of the car, for example. -
Figure 4 is a functional block diagram illustrating an example computing device used in a computing system that is arranged in accordance with at least some embodiments described herein. The computing device may be a personal computer, mobile device, cellular phone, touch-sensitive wristwatch, tablet computer, video game system, or global positioning system, and may be implemented to provide a system for interacting with 3D object data models as described inFigures 1-3 . In a basic configuration 402,computing device 400 may typically include one ormore processors 410 andsystem memory 420. A memory bus 430 can be used for communicating between theprocessor 410 and thesystem memory 420. Depending on the desired configuration,processor 410 can be of any type including but not limited to a microprocessor (µP), a microcontroller (µC), a digital signal processor (DSP), or any combination thereof. Amemory controller 415 can also be used with theprocessor 410, or in some implementations, thememory controller 415 can be an internal part of theprocessor 410. - Depending on the desired configuration, the
system memory 420 can be of any type including but not limited to volatile memory (such as RAM), non-volatile memory (such as ROM, flash memory, etc.) or any combination thereof.System memory 420 may include one ormore applications 422, andprogram data 424.Application 422 may include anindex algorithm 423 that is arranged to provide inputs to the electronic circuits, in accordance with the present disclosure.Program data 424 may includecontent information 425 that could be directed to any number of types of data. In some example embodiments,application 422 can be arranged to operate withprogram data 424 on an operating system. -
Computing device 400 can have additional features or functionality, and additional interfaces to facilitate communications between the basic configuration 402 and any devices and interfaces. For example,data storage devices 440 can be provided includingremovable storage devices 442, non-removable storage devices 444, or a combination thereof. Examples of removable storage and non-removable storage devices include magnetic disk devices such as flexible disk drives and hard-disk drives (HDD), optical disk drives such as compact disk (CD) drives or digital versatile disk (DVD) drives, solid state drives (SSD), and tape drives to name a few. Computer storage media can include volatile and nonvolatile, non-transitory, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. -
System memory 420 andstorage devices 440 are examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computingdevice 400. Any such computer storage media can be part ofdevice 400. -
Computing device 400 can also includeoutput interfaces 450 that may include agraphics processing unit 452, which can be configured to communicate to various external devices such asdisplay devices 490 or speakers via one or more A/V ports or acommunication interface 470. Thecommunication interface 470 may include anetwork controller 472, which can be arranged to facilitate communications with one or moreother computing devices 480 over a network communication via one ormore communication ports 474. The communication connection is one example of a communication media. Communication media may be embodied by computer readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and includes any information delivery media. A modulated data signal can be a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media can include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared (IR) and other wireless media. -
Computing device 400 can be implemented as a portion of a small-form factor portable (or mobile) electronic device such as a cell phone, a personal data assistant (PDA), a personal media player device, a wireless web-watch device, a personal headset device, an application specific device, or a hybrid device that include any of the above functions.Computing device 400 can also be implemented as a personal computer including both laptop computer and non-laptop computer configuration. - In some embodiments, the disclosed methods may be implemented as computer program instructions encoded on a non-transitory computer-readable storage media in a machine-readable format, or on other non-transitory media or articles of manufacture.
Figure 5 is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device, arranged according to at least some embodiments presented herein. - In one embodiment, the example
computer program product 500 is provided using a signal bearing medium 501. The signal bearing medium 501 may include one ormore programming instructions 502 that, when executed by one or more processors may provide functionality or portions of the functionality described above with respect toFigures 1-3 . In some examples, the signal bearing medium 501 may encompass a computer-readable medium 503, such as, but not limited to, a hard disk drive, a Compact Disc (CD), a Digital Video Disk (DVD), a digital tape, memory, etc. In some implementations, the signal bearing medium 501 may encompass acomputer recordable medium 504, such as, but not limited to, memory, read/write (R/W) CDs, R/W DVDs, etc. In some implementations, the signal bearing medium 501 may encompass acommunications medium 505, such as, but not limited to, a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.). Thus, for example, the signal bearing medium 501 may be conveyed by a wireless form of the communications medium 505 (e.g., a wireless communications medium conforming with the IEEE 802.11 standard or other transmission protocol). - The one or
more programming instructions 502 may be, for example, computer executable and/or logic implemented instructions. In some examples, a computing device such as thecomputing device 400 ofFigure 4 may be configured to provide various operations, functions, or actions in response to theprogramming instructions 502 conveyed to thecomputing device 500 by one or more of the computerreadable medium 503, thecomputer recordable medium 504, and/or thecommunications medium 505. - It should be understood that arrangements described herein are for purposes of example only. As such, those skilled in the art will appreciate that other arrangements and other elements (e.g. machines, interfaces, functions, orders, and groupings of functions, etc.) can be used instead, and some elements may be omitted altogether according to the desired results. Further, many of the elements that are described are functional entities that may be implemented as discrete or distributed components or in conjunction with other components, in any suitable combination and location.
- While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting, with the true scope being indicated by the following claims, along with the full scope of equivalents to which such claims are entitled, It is also to be understood that the terminology used herein is for the purpose of describing particular embodiments only, and is not intended to be limiting.
Claims (15)
- A method comprising:receiving (202), by a processor (410), first scan data (310) of an object within a space (300), wherein the first scan data comprises: (i) a first position of the object in the space (ii) a first two-dimensional mesh image defining a first view of a shape of the object at the first position, and (iii) non-rigid movement information defining a deformable surface area of the object, wherein the non-rigid movement information comprises coordinates in a coordinate system that define a location of the deformable surface area on the object and are relative to the first position of the object in the space;receiving (204) second scan data (314, 316, 318) of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space (ii) a second two-dimensional mesh image defining a second view of the shape of the object at the second position, and (iii) a deformation matrix that defines a deformation of the deformable surface area of the object at the second position within the space, wherein the deformable surface area is deformable independent of the movement of the object; and
aligning (206) the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model (324) of the object based on (i) the first two-dimensional mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, (iii) the second two-dimensional mesh image that defines the second view of the shape of the object at the second position, and (iv) using the deformation matrix. - The method of claim 1, wherein the movement comprises one or more of a translation, a rotation, an angulation, and a warp of the object.
- The method of claim 1, wherein the first position of the object comprises a position coordinate in a coordinate system that defines the first position of the object and the movement information comprises a movement coordinate in the coordinate system that defines the second position of the object.
- The method of claim 3, wherein:the first two-dimensional mesh image is at a first angle;the first two-dimensional mesh image comprises a collection of vertices, edges, and faces that define the first view of the shape of the object at the first position;the second two-dimensional mesh image is at a second angle; andthe second two-dimensional mesh image comprises a collection of vertices, edges, and faces that define the second view of the shape of the object at the second position.
- The method of claim 4, wherein aligning the first scan data (310) and the second scan data (314, 316, 318) so as to generate a three-dimensional (3D) object data model (324) of the object based on (i) the first two-dimensional mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, (iii) the second two-dimensional mesh image that defines the second view of the shape of the object at the second position, and (iv) the deformation matrix, comprises:aligning the first two-dimensional mesh image and the second two-dimensional mesh image using one or more of a translation matrix, a rotation matrix and the deformation matrix, wherein the translation matrix and the rotation matrix are determined using one or more of the position coordinate, the movement coordinate, the first angle, and the second angle.
- The method of claim 4, wherein aligning the first scan data (310) and the second scan data (314, 316, 318) so as to generate a three-dimensional (3D) object data model (324) of the object based on (i) the first two-dimensional mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, (iii) the second two-dimensional mesh image that defines the second view of the shape of the object at the second position, and (iv) the deformation matrix, comprises:aligning the first two-dimensional mesh image and the second two-dimensional mesh image using a feature point motion map of the object, wherein the feature point motion map is determined by tracking the movement of the object in real time.
- A system comprising:a computing device (400);a non-transitory computer-readable medium (420); andprogram instructions (422) stored on the non-transitory computer-readable medium and executable by the computing device to perform the functions of:receiving (202) first scan data (310) of an object within a space, wherein the first scan data comprises: (i) a first position of the object in the space (ii) a first two-dimensional mesh image defining a first view of a shape of the object at the first position, and (iii) non-rigid movement information defining a deformable surface area of the object, wherein the non-rigid movement information comprises coordinates in a coordinate system that define a location of the deformable surface area on the object and are relative to the first position of the object in the space;receiving (204) second scan data (314, 316, 318) of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space, (ii) a second two-dimensional mesh image defining a second view of the shape of the object at the second position, and (iii) deformation matrix that defines a deformation of the deformable surface area of the object at the second position within the space, wherein the deformable surface area is deformable independent of the movement of the object; andaligning (206) the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model (324) of the object based on (i) the first two-dimensional mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, (iii) the second two-dimensional mesh image that defines the second view of the shape of the object at the second position, and (iv) using the deformation matrix.
- The system of claim 7, wherein the movement comprises one or more of a translation, a rotation, an angulation, and a warp of the object.
- The system of claim 7, wherein:the first position of the object comprises a position coordinate in a coordinate system that defines the first position of the object;the movement information comprises a movement coordinate in the coordinate system that defines the second position of the object;the first two-dimensional mesh image is at a first angle;the second two-dimensional mesh image is at a second angle;the first two-dimensional mesh image comprises a collection of vertices, edges, and faces that define the first view of the shape of the object at the first position; andthe second two-dimensional mesh image comprises a collection of vertices, edges, and faces that define the second view of the shape of the object at the second position.
- The system of claim 9, wherein the function of aligning the first scan data (310) and the second scan data (314, 316, 318) so as to generate a three-dimensional (3D) object data model (324) of the object based on (i) the first two-dimensional mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, (iii) the second two-dimensional mesh image that defines the second view of the shape of the object at the second position, and (iv) the deformation matrix comprises:aligning the first two-dimensional mesh image and the second two-dimensional mesh image using one or more of a translation matrix, a rotation matrix and the deformation matrix, wherein the translation matrix and the rotation matrix are determined using one or more of the position coordinate, the movement coordinate, the first angle, and the second angle.
- The system of claim 9, wherein the function of aligning the first scan data (310) and the second scan data (314, 316, 318) so as to generate a three-dimensional (3D) object data model (324) of the object based on (i) the first two-dimensional mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, (iii) the second two-dimensional mesh image that defines the second view of the shape of the object at the second position, and (iv) the deformation matrix comprises:aligning the first two-dimensional mesh image and the second two-dimensional mesh image using a feature point motion map, wherein the feature point motion map is determined by tracking the movement of the object in real time.
- A non-transitory computer-readable medium (503) having stored therein instructions (502) executable by a computing device to cause the computing device to perform functions comprising:receiving (202) first scan data (310) of an object within a space, wherein the first scan data comprises: (i) a first position of the object in the space (ii) a first two-dimensional mesh image defining a first view of a shape of the object at the first position, and (iii) non-rigid movement information defining a deformable surface area of the object, wherein the non-rigid movement information comprises coordinates in a coordinate system that define a location of the deformable surface area on the object and are relative to the first position of the object in the space;receiving (204) second scan data (314, 316, 318) of the object that comprises: (i) movement information defining a movement of the object from the first position to a second position within the space (ii) a second two-dimensional mesh image defining a second view of the shape of the object at the second position, and (iii) deformation matrix that defines a deformation of the deformable surface area of the object at the second position within the space, wherein the deformable surface area is deformable independent of the movement of the object; andaligning (206) the first scan data and the second scan data so as to generate a three-dimensional (3D) object data model (324) of the object based on (i) the first two-dimensional mesh image that defines the first view of the shape of the object at the first position, (ii) the movement information, (iii) the second two-dimensional mesh image that defines the second view of the shape of the object at the second position, and (iv) using the deformation matrix.
- The non-transitory computer readable medium (503) of claim 12, wherein the movement comprises one or more of a translation, a rotation, an angulation, and a warp of the object.
- The non-transitory computer readable medium (503) of claim 12, wherein:the first position of the object comprises a position coordinate in a coordinate system that defines the first position of the object;the movement information comprises a movement coordinate in the coordinate system that defines the second position of the object;the first two-dimensional mesh image is at a first angle;the second two-dimensional mesh image is at a second angle;the first mesh image comprises a collection of vertices, edges, and faces that define the shape of the object at the first view; andthe second mesh image comprises a collection of vertices, edges, and faces
- The non-transitory computer readable medium (503) of claim 12, wherein the instructions (502) are further executable by the computing device to cause the computing device to perform functions comprising:aligning the first two-dimensional mesh image and the second two-dimensional mesh image using one or more of a translation matrix, a rotation matrix and a deformation matrix, wherein the translation matrix and the rotation matrix are determined using one or more of the position coordinate, the movement coordinate, the first angle, and the second angle; andaligning the first two-dimensional mesh image and the second two-dimensional mesh image using a feature point motion map of the object, wherein the feature point motion map of the object is created by tracking the movement of the object in real time.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261674140P | 2012-07-20 | 2012-07-20 | |
US13/595,766 US8754887B2 (en) | 2012-07-20 | 2012-08-27 | Determining three-dimensional (3D) object data models based on object movement |
PCT/US2013/043844 WO2014014562A1 (en) | 2012-07-20 | 2013-06-03 | Determining three-dimensional (3d) object data models based on object movement |
Publications (2)
Publication Number | Publication Date |
---|---|
EP2836987A1 EP2836987A1 (en) | 2015-02-18 |
EP2836987B1 true EP2836987B1 (en) | 2016-08-10 |
Family
ID=49946156
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP13728652.2A Active EP2836987B1 (en) | 2012-07-20 | 2013-06-03 | Determining three-dimensional (3d) object data models based on object movement |
Country Status (3)
Country | Link |
---|---|
US (1) | US8754887B2 (en) |
EP (1) | EP2836987B1 (en) |
WO (1) | WO2014014562A1 (en) |
Families Citing this family (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140195963A1 (en) * | 2011-12-16 | 2014-07-10 | Gehry Technologies | Method and apparatus for representing 3d thumbnails |
US9489472B2 (en) | 2011-12-16 | 2016-11-08 | Trimble Navigation Limited | Method and apparatus for detecting interference in design environment |
US9152743B2 (en) | 2012-02-02 | 2015-10-06 | Gehry Technologies, Inc. | Computer process for determining best-fitting materials for constructing architectural surfaces |
EP2904584B1 (en) * | 2012-10-05 | 2017-08-09 | Universidade De Coimbra | Method for aligning and tracking point regions in images with radial distortion that outputs motion model parameters, distortion calibration, and variation in zoom |
TWI510052B (en) * | 2013-12-13 | 2015-11-21 | Xyzprinting Inc | Scanner |
US10592929B2 (en) | 2014-02-19 | 2020-03-17 | VP Holdings, Inc. | Systems and methods for delivering content |
CN103927784B (en) * | 2014-04-17 | 2017-07-18 | 中国科学院深圳先进技术研究院 | A kind of active 3-D scanning method |
US9827714B1 (en) | 2014-05-16 | 2017-11-28 | Google Llc | Method and system for 3-D printing of 3-D object models in interactive content items |
WO2016018413A1 (en) * | 2014-07-31 | 2016-02-04 | Hewlett-Packard Development Company, L.P. | Object capture and illumination |
USRE49930E1 (en) | 2015-03-26 | 2024-04-23 | Universidade De Coimbra | Methods and systems for computer-aided surgery using intra-operative video acquired by a free moving camera |
WO2016168307A1 (en) | 2015-04-13 | 2016-10-20 | Universidade De Coimbra | Methods and systems for camera characterization in terms of response function, color, and vignetting under non-uniform illumination |
WO2016200383A1 (en) * | 2015-06-10 | 2016-12-15 | Hewlett-Packard Development Company, L.P. | 3d scan tuning |
GB2544263A (en) * | 2015-11-03 | 2017-05-17 | Fuel 3D Tech Ltd | Systems and methods for imaging three-dimensional objects |
US10019837B2 (en) | 2016-05-31 | 2018-07-10 | Microsoft Technology Licensing, Llc | Visualization alignment for three-dimensional scanning |
US11132728B2 (en) * | 2017-02-06 | 2021-09-28 | Lego A/S | Electronic ordering system and method |
US10796499B2 (en) | 2017-03-14 | 2020-10-06 | Universidade De Coimbra | Systems and methods for 3D registration of curves and surfaces using local differential information |
US10365650B2 (en) * | 2017-05-25 | 2019-07-30 | GM Global Technology Operations LLC | Methods and systems for moving object velocity determination |
US10825259B2 (en) * | 2019-01-02 | 2020-11-03 | The Boeing Company | Three-dimensional point data alignment with pre-alignment |
WO2021067888A1 (en) | 2019-10-03 | 2021-04-08 | Cornell University | Optimizing bra sizing according to the 3d shape of breasts |
CN112867136B (en) * | 2020-12-31 | 2022-11-15 | 思看科技(杭州)股份有限公司 | Three-dimensional scanning system and three-dimensional scanning method based on wireless peer-to-peer network |
Family Cites Families (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5633995A (en) * | 1991-06-19 | 1997-05-27 | Martin Marietta Corporation | Camera system and methods for extracting 3D model of viewed object |
WO1994001829A1 (en) * | 1992-07-10 | 1994-01-20 | Fujitsu Limited | Three-dimensional graphic editing machine |
US5598515A (en) * | 1994-01-10 | 1997-01-28 | Gen Tech Corp. | System and method for reconstructing surface elements of solid objects in a three-dimensional scene from a plurality of two dimensional images of the scene |
US5850469A (en) * | 1996-07-09 | 1998-12-15 | General Electric Company | Real time tracking of camera pose |
US6201541B1 (en) * | 1997-12-11 | 2001-03-13 | Cognitens, Ltd. | System and method for “Stitching” a plurality of reconstructions of three-dimensional surface features of object(s) in a scene defined relative to respective coordinate systems to relate them to a common coordinate system |
US6970591B1 (en) * | 1999-11-25 | 2005-11-29 | Canon Kabushiki Kaisha | Image processing apparatus |
JP4635392B2 (en) * | 2001-08-09 | 2011-02-23 | コニカミノルタホールディングス株式会社 | 3D object surface shape modeling apparatus and program |
GB2381429B (en) | 2001-09-28 | 2005-07-27 | Canon Europa Nv | 3D computer model processing apparatus |
US7221809B2 (en) * | 2001-12-17 | 2007-05-22 | Genex Technologies, Inc. | Face recognition system and method |
US7605817B2 (en) * | 2005-11-09 | 2009-10-20 | 3M Innovative Properties Company | Determining camera motion |
DE102006048578B4 (en) * | 2006-10-13 | 2010-06-17 | Gerhard Witte | Method and apparatus for determining the change in the shape of a three-dimensional object |
US20080112610A1 (en) | 2006-11-14 | 2008-05-15 | S2, Inc. | System and method for 3d model generation |
US8265376B2 (en) * | 2008-07-21 | 2012-09-11 | Cognitens Ltd. | Method and system for providing a digital model of an object |
US20120176478A1 (en) | 2011-01-11 | 2012-07-12 | Sen Wang | Forming range maps using periodic illumination patterns |
US8447099B2 (en) * | 2011-01-11 | 2013-05-21 | Eastman Kodak Company | Forming 3D models using two images |
-
2012
- 2012-08-27 US US13/595,766 patent/US8754887B2/en active Active
-
2013
- 2013-06-03 EP EP13728652.2A patent/EP2836987B1/en active Active
- 2013-06-03 WO PCT/US2013/043844 patent/WO2014014562A1/en active Application Filing
Also Published As
Publication number | Publication date |
---|---|
US20140022248A1 (en) | 2014-01-23 |
WO2014014562A1 (en) | 2014-01-23 |
US8754887B2 (en) | 2014-06-17 |
EP2836987A1 (en) | 2015-02-18 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP2836987B1 (en) | Determining three-dimensional (3d) object data models based on object movement | |
US8848201B1 (en) | Multi-modal three-dimensional scanning of objects | |
Gomes et al. | 3D reconstruction methods for digital preservation of cultural heritage: A survey | |
CN110490916B (en) | Three-dimensional object modeling method and apparatus, image processing device, and medium | |
US8416236B1 (en) | Calibration of devices used to generate images of a three-dimensional object data model | |
EP3742113A1 (en) | Systems and methods for marking images for three-dimensional image generation | |
US9163938B2 (en) | Systems and methods for image acquisition | |
US8436853B1 (en) | Methods and systems for acquiring and ranking image sets | |
US9529826B2 (en) | Methods and systems for use of a database of three-dimensional (3D) object data models for search queries | |
US9361665B2 (en) | Methods and systems for viewing a three-dimensional (3D) virtual object | |
US8854362B1 (en) | Systems and methods for collecting data | |
US9224238B2 (en) | Seamless texturing of 3D meshes of objects from multiple views | |
WO2012096747A1 (en) | Forming range maps using periodic illumination patterns | |
CN113689578B (en) | Human body data set generation method and device | |
CN115345822A (en) | Automatic three-dimensional detection method for surface structure light of aviation complex part | |
CN113841384A (en) | Calibration device, chart for calibration and calibration method | |
CN112055192A (en) | Image processing method, image processing apparatus, electronic device, and storage medium | |
Zhang et al. | Advances and prospects of vision-based 3D shape measurement methods | |
CN111311728A (en) | High-precision morphology reconstruction method, equipment and device based on optical flow method | |
US10891780B2 (en) | Methods and systems for viewing a three-dimensional (3D) virtual object | |
Ayaz et al. | Multiview registration-based handheld 3D profiling system using visual navigation and structured light | |
US9870435B1 (en) | Use of physical deformation during scanning of an object to generate views of the object | |
US9237329B1 (en) | Systems and methods for capturing data of an object | |
Kumara et al. | Real-time 3D human objects rendering based on multiple camera details | |
Pribanić et al. | An efficient surface registration using smartphone |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20141111 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
17Q | First examination report despatched |
Effective date: 20150706 |
|
DAX | Request for extension of the european patent (deleted) | ||
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
INTG | Intention to grant announced |
Effective date: 20160209 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EPRef country code: ATRef legal event code: REFRef document number: 819646Country of ref document: ATKind code of ref document: TEffective date: 20160815 |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602013010322Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG4D |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20160810 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 819646Country of ref document: ATKind code of ref document: TEffective date: 20160810 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20161210Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20161110Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20161111Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20161212Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602013010322Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: BEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20161110Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
REG | Reference to a national code |
Ref country code: FRRef legal event code: PLFPYear of fee payment: 5 |
|
26N | No opposition filed |
Effective date: 20170511 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R082Ref document number: 602013010322Country of ref document: DERepresentative=s name: MARKS & CLERK (LUXEMBOURG) LLP, LURef country code: DERef legal event code: R081Ref document number: 602013010322Country of ref document: DEOwner name: GOOGLE LLC (N.D.GES.D. STAATES DELAWARE), MOUN, USFree format text: FORMER OWNER: GOOGLE, INC., MOUNTAIN VIEW, CALIF., US |
|
REG | Reference to a national code |
Ref country code: FRRef legal event code: CDOwner name: GOOGLE INC., USEffective date: 20180213Ref country code: FRRef legal event code: CJEffective date: 20180213 |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: MM4A |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20170603Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20170630Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20170630Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20170603 |
|
REG | Reference to a national code |
Ref country code: FRRef legal event code: PLFPYear of fee payment: 6 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MTFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20170603 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20130603 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: TRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20160810 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230505 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20230626Year of fee payment: 11Ref country code: DEPayment date: 20230626Year of fee payment: 11 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20230627Year of fee payment: 11 |