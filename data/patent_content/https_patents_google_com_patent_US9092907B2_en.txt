US9092907B2 - Image shader using two-tiered lookup table for implementing style attribute references - Google Patents
Image shader using two-tiered lookup table for implementing style attribute references Download PDFInfo
- Publication number
- US9092907B2 US9092907B2 US13/174,351 US201113174351A US9092907B2 US 9092907 B2 US9092907 B2 US 9092907B2 US 201113174351 A US201113174351 A US 201113174351A US 9092907 B2 US9092907 B2 US 9092907B2
- Authority
- US
- United States
- Prior art keywords
- image
- vertex
- map
- lookup table
- attribute
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 238000009877 rendering Methods 0.000 claims abstract description 109
- 238000000034 method Methods 0.000 claims abstract description 80
- 230000008569 process Effects 0.000 claims abstract description 32
- 230000015654 memory Effects 0.000 claims description 74
- 238000004891 communication Methods 0.000 claims description 48
- 239000012634 fragment Substances 0.000 claims description 37
- 238000012545 processing Methods 0.000 claims description 28
- 238000003384 imaging method Methods 0.000 claims description 21
- 230000001172 regenerating effect Effects 0.000 claims description 2
- 230000008859 change Effects 0.000 description 18
- 239000003086 colorant Substances 0.000 description 15
- 230000009471 action Effects 0.000 description 10
- 238000010586 diagram Methods 0.000 description 5
- 230000006870 function Effects 0.000 description 5
- 230000000694 effects Effects 0.000 description 4
- 230000004044 response Effects 0.000 description 4
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000007667 floating Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000000547 structure data Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 235000013290 Sagittaria latifolia Nutrition 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 235000015246 common arrowhead Nutrition 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000000463 material Substances 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 230000008054 signal transmission Effects 0.000 description 1
- 239000000126 substance Substances 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/50—Lighting effects
- G06T15/506—Illumination models
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G5/00—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/005—General purpose rendering architectures
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/14—Digital output to display device ; Cooperation and interconnection of the display device with other functional units
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2340/00—Aspects of display data processing
- G09G2340/14—Solving problems related to the presentation of information to be displayed
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2350/00—Solving problems of bandwidth in display systems
Definitions
- the present disclosure relates to image rendering systems, such as electronic map display systems, and more specifically to an image rendering engine that uses one or more lookup tables to determine image attributes to be used in generating an image on a display screen using an image shader.
- Digital maps are found in and may be displayed by a wide variety of devices, including mobile phones, car navigation systems, hand-held GPS units, computers, and many websites. Although digital maps are easy to view and to use from an end-user's perspective, creating a digital map is a difficult task and can be a time-consuming process.
- every digital map begins with storing, in a map database, a set of raw data corresponding to millions of streets and intersections and other features to be displayed as part of a map.
- the raw map data that is stored in the map database and that is used to generate digital map images is derived from a variety of sources, with each source typically providing different amounts and types of information. This map data must therefore be compiled and stored in the map database before being accessed by map display or map rendering applications and hardware.
- map images there are, of course, different manners of digitally rendering map images (referred to as digital map images) based on map data stored in a map database.
- One method of rendering a map image is to store map images within the map database as sets of raster or pixelated images made up of numerous pixel data points, with each pixel data point including properties defining how a particular pixel in an image is to be displayed on an electronic display device. While this type of map data is relatively easy to create and store, the map rendering technique using this data typically requires a large amount of storage space for comprehensive digital map images, and it is difficult to manipulate the digital map images as displayed on a display device in very many useful manners.
- vector image data is typically used in high-resolution and fast-moving imaging systems, such as those associated with gaming systems, and in particular three-dimensional gaming systems.
- vector image data includes data that defines specific image objects or elements (also referred to as primitives) to be displayed as part of an image via an image display device.
- image elements or primitives may be, for example, individual roads, text labels, areas, text boxes, buildings, points of interest markers, terrain features, bike paths, map or street labels, etc.
- Each image element is generally made up or drawn as a set of one or more triangles (of different sizes, shapes, colors, fill patterns, etc.), with each triangle including three vertices interconnected by lines.
- the image database stores a set of vertex data points, with each vertex data point defining a particular vertex of one of the triangles making up the image element.
- each vertex data point includes data pertaining to a two-dimensional or a three-dimensional position of the vertex (in an X, Y or and X, Y, Z coordinate system, for example) and various vertex attributes defining properties of the vertex, such as color properties, fill properties, line width properties for lines emanating from the vertex, etc.
- an image shader is a set of software instructions used primarily to calculate rendering effects on graphics hardware with a high degree of flexibility.
- Image shaders are well known in the art and various types of image shaders are available in various application programming interfaces (APIs) provided by, for example, OpenGL and Direct3D, to define special shading functions.
- APIs application programming interfaces
- image shaders are simple programs in a high level programming language that describe or determine the traits of either a vertex or a pixel.
- Vertex shaders for example, define the traits (e.g., position, texture coordinates, colors, etc.) of a vertex, while pixel or fragment shaders define the traits (color, z-depth and alpha value) of a pixel.
- a vertex shader is called for each vertex in an image element or primitive so that, for each vertex input into the vertex shader, the vertex shader produces one (updated) vertex output.
- Each vertex output by the vertex shader is then rendered as a series of pixels onto a block of memory that will eventually be sent to a display screen.
- Vertex shaders are run once for each vertex given to the graphics processor.
- the purpose of a vertex shader is to transform a position of a vertex in a virtual space to the two-dimensional coordinate at which it appears on the display screen (as well as a depth value for the z-buffer of the graphics processor).
- Vertex shaders can manipulate properties such as position, color, and texture coordinates, but cannot create new vertices.
- the output of the vertex shader is provided to the next stage in the processing pipeline, which is either a geometry shader if present or the rasterizer.
- Geometry shaders can add and remove vertices from a mesh of vertices and can be used to generate image geometry procedurally or to add volumetric detail to existing images that would be too costly to process on a central processing unit (CPU). If geometry shaders are being used, the output is then sent to the rasterizer.
- Pixel shaders which are also known as fragment shaders, calculate the color and light properties of individual pixels in an image. The input to this stage comes from the rasterizer, and the fragment shaders operate to fill in the pixel values of the polygons being sent through the graphics pipeline.
- Fragment shaders are typically used for scene lighting and related effects such as color toning. There is not a one-to-one relationship between calls to the fragment shader and pixels on the screen as fragment shaders are often called many times per pixel because they are called for every image element or object that is in the corresponding space, even if that image object is occluded. However, if the occluding object is drawn first, the occluded pixels of other objects will generally not be processed in the fragment shader.
- vector graphics can be particularly advantageous in a mobile map system in which image data is sent from a centralized map database via a communications network (such as the Internet, a wireless network, etc.) to one or more mobile or remote devices for display.
- a communications network such as the Internet, a wireless network, etc.
- vector data once sent to the receiving device, may be more easily scaled and manipulated (e.g., rotated, etc.) than pixelated raster image data.
- the processing of vector data is typically much more time consuming and processor intensive on the image rendering system that receives the data.
- vector image data that provides a higher level of detail or information to be displayed in a map leads to a higher amount of vector data or vertices that need to be sent to the map rendering system from the map database that stores this information, which can result in higher bandwidth requirements or downloading time in some cases.
- a computer-implemented method for rendering an image on a display device includes obtaining, at a computer device, vector data having a set of vertex data points, each vertex data point including location information data and a value for an attribute reference pointer variable.
- the method also includes storing a first lookup table having an attribute reference value stored for each of a set of different values of the attribute reference pointer variable and storing a second lookup table having a set of vertex attribute values for each of a plurality of attribute reference values, wherein the set of vertex attribute values for each of the attribute reference values includes a value for each of a multiplicity of different vertex attributes.
- the method renders an image by first executing an image shader on the computer device to use the first and second lookup tables to determine values for each of the multiplicity of vertex attributes of one or more of the vertex data points based on the value for the attribute reference pointer variable of the vertex data point.
- the image shader operates to produce image shader outputs based on the location information data and the vertex attribute values of each of the set of vertex data points. Thereafter, the method processes each of the image shader outputs to render an image on the display device based on the received vector data.
- obtaining the vertex data points may include downloading the vertex data points from an image database via an electronic communications network with or without the attribute reference pointer variable values and/or may include associating a value for the attribute reference pointer variable for the vertex data points at the computer device.
- the first lookup table and the second lookup table may be received, via the electronic communications network, from a server connected to the image database and/or may be generated at the computer device.
- each vertex data point may include location information data in the form of two-dimensional coordinate values specifying a two-dimensional location of a vertex in virtual space and the lookup table may be configured as a two-dimensional texture map encoded with vertex attribute data.
- the multiplicity of vertex attributes may include one or more of a color, a fill pattern, and a line width.
- processing each of the image shader outputs in the rasterizer may include using a vertex or a fragment shader to render the image on the display device.
- an image rendering engine includes a communications network interface, one or more processors, one or more memories coupled to the one or more processors, a display device coupled to the one or more processors and a communications routine stored on memory that executes on one of the one or more processors to obtain, via the communications network interface, vector data having a plurality of vertex data points, each vertex data point including location information.
- the image rendering engine includes a routine stored on the memory that executes on one of the one or more processors to determine an attribute reference pointer variable value for each vertex data point, a first lookup table stored in the one or more memories having an attribute reference value stored for each of a set of different values of the attribute reference pointer variable and a second lookup table stored in the one or more memories having a set of vertex attribute values for each of a plurality of attribute reference values, wherein the set of vertex attribute values for each of the plurality of attribute reference values includes a value for each of a multiplicity of different vertex attributes.
- An image shader routine stored in the one or more memories executes on one of the one or more processors to use the first and the second lookup table to determine vertex attribute values for each of the multiplicity of attribute values of the received vertex data points based on the attribute references of the vertex data points.
- the image shader routine uses the vertex location information and the vertex attribute values for each of the received vertex data points to produce image shader outputs.
- a rasterizer renders an image on the display device using the image shader outputs.
- an imaging system to be used with a processor includes a first routine stored on a computer-readable memory that, when executed on the processor, (1) obtains vector data via a communications network, the vector data having a set of vertex data points, each vertex data point including location information data and an attribute reference pointer variable value, (2) obtains a first lookup table, the first lookup table having an attribute reference value stored for each of a set of different values of the attribute reference pointer variable, and (3) obtains a second lookup table, the second lookup table having a set of vertex attribute values for each of a plurality of attribute references, wherein the set of vertex attribute values for each of the plurality of attribute references includes a value for each of a multiplicity of different vertex attributes.
- An image shader routine stored on the memory executes on the processor to use the first and the second lookup tables to determine values for each of the multiplicity of attribute values of one or more of the vertex data points based on the attribute reference pointer variable values of the one or more vertex data points and produces image shader outputs based on the location information data and the vertex attribute values of the vertex data points.
- a processing device processes the image shader outputs to render an image on a display device based on the received vector data.
- an image shader for use in an imaging system having a processor and a rasterizer includes a first data access routine stored on a memory that, when executed on the processor, accesses vector data, the vector data having a set of vertex data points, each vertex data point including location information data and an attribute reference pointer variable value.
- the image shader also includes a second data access routine stored on a memory that, when executed on the processor, accesses a first lookup table having an attribute reference value stored for each of a set of different values of the attribute reference pointer variable to determine a particular attribute reference value for a vertex data point, and that accesses a second lookup table having a set of vertex attribute values for each of a plurality of attribute reference values, wherein the set of vertex attribute values for each of the attribute reference values includes a value for each of a multiplicity of different vertex attributes.
- the image shader uses the first and second lookup tables to obtain a particular set of vertex attribute values associated with the vertex data point.
- One or more image shader processing routines then process the location information data and the vertex attribute values for each of the vertex data points to produce image shader outputs that are adapted to be used by the rasterizer to render an image on an image display device.
- a map image rendering system includes a map storage that stores map-related vector image data, the vector image data including a plurality of vertex points, each vertex point including location data specifying a location of a vertex in a virtual space and an attribute reference pointer variable value.
- a map image rendering device is communicatively coupled to the map storage to receive the map-related vector image data and to render a map image using the map-related vector image data.
- the map image rendering device includes a communications network interface, a processor, a memory coupled to the processor, a display device coupled to the processor, and a communications routine stored on the memory that executes on the processor to receive, via the communications network interface, vector data having a plurality of map vertex data points, each map vertex data point including location information data.
- the attribute reference pointer variable values may also be obtained via the communications network or may be obtained directly from the map storage if the map storage is within the map image rendering device.
- an attribute reference pointer determination routine determines an attribute reference pointer variable value for each of the vertex data points
- a first lookup table decoding routine executes on the processor to access a first lookup table having an attribute reference value stored for each of a set of different values of the attribute reference pointer variable values to determine a particular attribute reference value for a vertex data point
- a second lookup table decoding routine executes on the processor to access a second lookup table having a set of vertex attribute values for each of a plurality of attribute reference values, wherein the set of vertex attribute values for each of the plurality of attribute references includes a value for each of a multiplicity of different vertex attributes.
- the second lookup table decoding routine determines a set of vertex attribute values for each vertex data point based on the attribute reference value for each vertex data point.
- an image shader routine stored on the memory executes on the processor and uses the vertex location information and the vertex attribute values for each of the received map vertex data points to produce image shader outputs.
- a rasterizer renders the map image on the display device using the image shader outputs.
- FIG. 1 is a high-level block diagram of a map imaging system that implements communications between a map database stored in a server and one or more map image rendering devices.
- FIG. 2 is a high level block diagram of an image rendering engine used to render map images using map vector data.
- FIG. 3A is a data diagram illustrating a set of vector data in the form of vertex data points encoded using a vertex attribute reference.
- FIG. 3B is a first texture map in the form of a style lookup table that defines vertex attributes values for each of a number of different styles and which is used in the image rendering engine of FIG. 2 to resolve vertex attributes based on a style reference.
- FIG. 3C is a second texture map in the form of a feature lookup table that defines a style reference associated with each of a number of different feature references and which is used within the image rendering engine of FIG. 2 to resolve vertex point attribute values for vertex data points based on both a feature reference and a style reference.
- FIG. 4 illustrates an example routine or process flow diagram that uses one or more of the lookup tables of FIGS. 3B and 3C to render map images using the rendering engine of FIG. 2 .
- FIG. 5 illustrates an example routine or process flow diagram of an image shader that uses one or more of the lookup tables of FIGS. 3B and 3C to process vertex data points having either a style attribute reference or a feature attribute reference.
- a graphics or image rendering system receives image data from an image database, such as a map database, in the form of vector data that defines various image objects to be displayed in an image. More particularly, the vector data defines image objects using one or more lookup tables and sets of vertex data points defining image objects, with each vertex data point including position or location data as well as an attribute reference pointer variable value that ultimately points to vertex attribute data values stored in the lookup tables.
- the attribute reference for a particular vertex data point is used by an image shader, such as a vertex shader or a fragment shader, to determine specific values for one or more vertex attributes of the particular vertex data point, as stored in one of the lookup tables.
- the image rendering system thus processes the vector data in a manner that allows for minimal data to be sent to the image rendering device from the image database or other application creating this image, as this system enables multiple vertex attribute properties to be specified by a single attribute reference pointer variable value associated with each vertex data point. Moreover, this system enables re-rendering of the image at the image rendering device to change certain look and feel properties of the image objects, such as colors, line widths, etc., by simply downloading or otherwise creating a new lookup table with changed vertex attribute values or with changed attribute reference pointer variable values, and then resubmitting or reusing the originally received vertex data points with the original attribute reference pointer variable values to the image shader, and using the new lookup table(s) to re-render the image.
- the image rendering device merely needs to receive the new lookup table data to re-render the image with changed properties, and does not need to receive additional or updated vertex data points from the image database.
- this system enables manipulation of the look and feel properties of the image at the image rendering device without a download of a significant amount of additional vector data.
- a map-related imaging system 10 includes a map database 12 stored in a server 14 or in multiple servers located at, for example, a central site or at various different spaced apart sites, and also includes multiple map client devices 16 , 18 , 20 , and 22 , each of which stores and implements a map rendering device or a map rendering engine.
- the map client devices 16 - 22 may be connected to the server 14 via any hardwired or wireless communication network 25 , including for example a hardwired or wireless LAN, MAN or WAN, the Internet, or any combination thereof.
- the map client devices 16 - 22 may be, for example, mobile phone devices ( 18 ), computers such a laptop, desktop or other types of computers ( 16 , 20 ) or components of other imaging systems such components of automobile navigation systems ( 22 ), etc. Moreover, the client devices 16 - 22 may be communicatively connected to the server 14 via any suitable communication system, such as any publicly available or privately owned communication network, including those that use hardwired based communication structure, such as telephone and cable hardware, and/or wireless communication structure, such as wireless communication networks, including for example, wireless LANs and WANs, satellite and cellular phone communication systems, etc.
- any suitable communication system such as any publicly available or privately owned communication network, including those that use hardwired based communication structure, such as telephone and cable hardware, and/or wireless communication structure, such as wireless communication networks, including for example, wireless LANs and WANs, satellite and cellular phone communication systems, etc.
- the map database 12 may store any desired types or kinds of map data including raster image map data and vector image map data.
- the image rendering systems described herein are best suited for use with vector image data which defines or includes a series of vertices or vertex data points for each of numerous sets of image objects, elements or primitives within an image to be displayed.
- each of the image objects defined by the vector data will have a plurality of vertices associated therewith and these vertices will be used to display a map related image object to a user via one or more of the client devices 16 - 22 .
- each of the client devices 16 - 22 includes an image rendering engine having one or more processors 30 , one or more memories 32 , a display device 34 , and in many cases a rasterizer or graphics card 36 which are generally programmed and interconnected in known manners to implement or to render graphics (images) on the associated display device 34 .
- the display device 34 for any particular client device 16 - 22 may be any type of electronic display device such as a liquid crystal display (LCD), a light emitting diode (LED) display, a plasma display, a cathode ray tube (CRT) display, or any other type of known or suitable electronic display.
- the map-related imaging system 10 of FIG. 1 operates such that a user, at one of the client devices 16 - 22 , opens or executes a map application (not shown in FIG. 1 ) that operates to communicate with and obtain map information or map related data from the map database 12 via the server 14 , and that then displays or renders a map image based on the received map data.
- the map application may allow the user to view different geographical portions of the map data stored in the map database 12 , to zoom in or zoom out on a particular geographical location, to rotate, spin or change the two-dimensional or three-dimensional viewing angle of the map being displayed, etc.
- each of the client devices 16 - 22 downloads map data in the form of vector data from the map database 12 and processes that vector data using one or more image shaders to render an image on the associated display device 34 .
- the image rendering system 40 of FIG. 2 includes two processors 30 a and 30 b , two memories 32 a and 32 b , a user interface 34 and a rasterizer 36 .
- the processor 30 b , the memory 32 b and the rasterizer 36 are disposed on a separate graphics card (denoted below the horizontal line), although this need not be the case in all embodiments.
- a single processor may be used instead.
- the image rendering system 40 includes a network interface 42 , a communications and storage routine 43 and one or more map applications 48 having map display logic therein stored on the memory 32 a , which may be executed on the processor 30 a .
- one or more image shaders in the form of, for example, vertex shaders 44 and fragment shaders 46 are stored on the memory 32 b and are executed on the processor 30 b .
- the memories 32 a and 32 b may include either or both volatile and non-volatile memory and the routines and shaders are executed on the processors 30 a and 30 b to provide the functionality described below.
- the network interface 42 includes any well known software and/or hardware components that operate to communicate with, for example, the server 14 of FIG.
- the image rendering device 40 also includes a data memory 49 , which may be a buffer or volatile memory for example, that stores vector data received from the map database 12 , the vector data including any number of vertex data points and one or more lookup tables as will be described in more detail.
- the map logic of the map application 48 executes on the processor 30 to determine the particular image data needed for display to a user via the display device 34 using, for example, user input, GPS signals, prestored logic or programming, etc.
- the display or map logic of the application 48 interacts with the map database 12 , using the communications routine 43 , by communicating with the server 14 through the network interface 42 to obtain map data, preferably in the form of vector data or compressed vector data from the map database 12 .
- This vector data is returned via the network interface 42 and may be decompressed and stored in the data memory 49 by the routine 43 .
- the data downloaded from the map database 12 may be a compact, structured, or otherwise optimized version of the ultimate vector data to be used, and the map application 48 may operate to transform the downloaded vector data into specific vertex data points using the processor 30 a .
- the image data sent from the server 14 includes vector data generally defining data for each of a set of vertices associated with a number of different image elements or image objects to be displayed on the screen 34 and possibly one or more lookup tables which will be described in more detail below.
- the lookup tables may be sent in, or may be decoded to be in, or may be generated by the map application 48 to be in the form of texture maps which are known types of data typically defining a particular texture or color field (pixel values) to be displayed as part of an image created using vector graphics.
- the vector data for each image element or image object may include multiple vertices associated with one or more triangles making up the particular element or object of an image. Each such triangle includes three vertices (defined by vertex data points) and each vertex data point has vertex data associated therewith.
- each vertex data point includes vertex location data defining a two-dimensional or a three-dimensional position or location of the vertex in a reference or virtual space, as well as an attribute reference.
- Each vertex data point may additionally include other information, such as an object type identifier that identifies the type of image object with which the vertex data point is associated.
- the attribute reference referred to herein as a style reference or as a feature reference, references or points to a location or a set of locations in one or more of the lookup tables created or downloaded and stored in the data memory 49 .
- FIG. 3A illustrates an exemplary set of vertex data points that may be, for example, downloaded or obtained from the map database 12 or that may be generated by the map application 48 and stored in the memory 49 for an image object prior to being processed by the rendering system 40 and being used to create an image containing the image object on the display device 34 .
- the vector data for an image object may include a data set for each vertex of the image object, with the example image object of FIG. 3A having 99 vertex data points (numbered 0-98).
- each vertex data point includes two location values (defined in the columns labeled X, Y) and a depth value (defined in the column Z) wherein each location value indicates a value or position of the associated vector data point in one of two coordinate axes of a virtual or reference two-dimensional space (e.g., an X, Y coordinate system).
- the depth value indicates a depth of the vertex with respect to other image objects in the virtual coordinate space.
- each vertex data point also includes an object type property (in the second column of FIG. 3A ) and an attribute reference (in the first column of the table of FIG. 3A ).
- the object type property defines the type of image object to which the vertex data point pertains and may be used to select the appropriate image shader to use to process the vertex data when rendering an image on a display screen.
- any different numbers and types of image objects may be used and the numbers and types of image objects used in any particular image rendering system will be determined by the type of image being rendered.
- vector map images may have image objects in the form of road objects, text objects, area objects, text box objects, and arrow objects, to name but a few.
- other types and kinds of image objects could be used in map images and in other types of images, such as three-dimensional images and the system described herein is not limited to the image objects described above.
- the attribute references of the vertex data points in FIG. 3A are, in this case, style attribute references or style references.
- Each style reference indicates one of a number of predetermined styles associated with the vertex data points. All of the vertex data points for a particular image object may be associated with the same style reference or, alternately, different vertex data points of a particular image object may be associated with different style references (also referred to as style numbers).
- style references also referred to as style numbers.
- different image objects may be sent having vertex data points that reference the same or different style numbers as those referenced in the vertex data points for other image objects.
- vertex data for the same image objects may reference the same or different style numbers and vertex data for different image objects may use the same set of style numbers so that, in some cases, the vertex data points for different image objects reference the same style number.
- the style references may be assigned at the server 12 or may be assigned or generated by the map application 48 based on logic therein generally dependent on user interactions with the map application 48 .
- FIG. 3B illustrates an example lookup table 50 that may be used to resolve the style attribute references or style references sent or associated with the vector data for one or more image objects.
- the lookup table 50 includes a set of rows and columns, with each row defining or associated with a different vertex data point attribute and each column defining a different style number.
- the vertex attributes provided in the lookup table 50 may include, for example, a fill color (e.g., for area objects), an outline color, an outline width, an outline dashing pattern and an indication of whether to use rounded end caps (e.g., for road objects), an interior color, an interior width, an interior dashing pattern, and interior rounded end caps (e.g., for road objects), a text color and a text outline color (e.g., for text objects), an arrow color, an arrow width, an arrow dashing pattern (e.g., for arrow objects), a text box fill color and a set of text box outline properties (e.g., for text box objects) to name but a few.
- a fill color e.g., for area objects
- an outline color e.g., an outline width, an outline dashing pattern and an indication of whether to use rounded end caps
- an interior color, an interior width, an interior dashing pattern, and interior rounded end caps e.g., for road objects
- each style reference (defined by any particular column of the lookup table 50 ) includes a particular value for each of the vertex attributes. Any number of different style references may be defined and any number of different vertex attributes may be stored or provided for each style number in the lookup table 50 . Generally speaking, however, a value for each of the possible vertex attributes will be stored for each style number.
- the data values actually stored within the lookup table 50 indicate the value of a particular vertex attribute (as defined by a row) for a particular style number (as defined by a particular column) of a vertex data point that references that style number.
- the lookup table 50 may be downloaded or otherwise generated (e.g., by the map application 48 ) and may be stored as a texture or a texture map (also referred to herein as a texture).
- textures are used in vector based imaging systems to define a particular field of pixel values to be displayed at a location or at a field within an image being rendered.
- textures were used in fragment shaders to define pixel colors on a one to one basis, and have not been used as lookup tables for other attributes of vector data points.
- the encoding technique described herein basically takes vertex attribute properties that would normally be sent through the communications network to and stored in data memory within the image rendering engine 40 or that would be sent from the processor 30 a (running the map application 48 ) to the processor 30 b (running the shaders) as raw data values, and instead encodes these vertex attribute values as colors in a texture map.
- the texture map is then sent to the processor 30 b of the image rendering engine 40 and is used as a lookup table to define vertex attribute values for different vertex data points based on an attribute reference sent with each of the vertex data points.
- This encoding technique can be advantageously used when the same combinations of vertex attribute values are repeated for multiple sets of vertices, either for the same image object or for different image objects. For example, all vertices in a particular image object may share a color and other material properties such as line width, fill properties, etc., and this set of vertex attribute values will be sent as a style within the lookup table 50 and be decoded in a vertex shader 44 or a fragment shader 46 based on an attribute reference sent with each of the vertex data points of the image object, the attribute reference pointing to the vertex attributes of a particular style as stored in the texture map.
- This encoding technique generally reduces the amount of data that needs to be sent through the communications network or between the processors 30 A and 30 b and stored within the graphics card of the image rendering engine.
- a texture may be encoded with vertex attribute values using a single column of the texture for each unique set of vertex properties or attribute values (i.e., for each style). Colors can be directly stored in the texture and the values of these colors can be encoded or determined as the values of the corresponding vertex attribute.
- Floating point values can be encoded into color values of the texture with fixed precision by multiplying the floating point number by different factors for each color component and taking the result modulo 256, where the factors for each component differ by a magnitude of 256. The exact factors used can be based on the expected range of the value. Integers can be encoded into colors in the same manner, but with factors of, for example, 1, 2 8 , 2 16 , and 2 24 .
- the texture map can be extended to support more entries than the maximum width of a typical image texture by wrapping onto a second, third, etc. row of properties.
- the resulting size of the texture should be sent to the image shader which uses the texture map to decode vertex attribute values.
- an encoded texture may be created with nearest pixel minification and magnification filters to prevent interpolation.
- the coordinates for the vertex properties can be computed in an image shader using the integer index (the style number) passed in as a vertex attribute reference, and the texture size, which can be a uniform variable.
- the coordinates to use for a particular attribute value will be the index modulo the texture width for X (the texture column dimension), and the floor of the index divided by the texture width for Y (the texture row dimension).
- the Y value should then be multiplied by the number of pixels it takes to encode one set of properties.
- the vertex shader 44 can then read the colors corresponding to each vertex attribute from the texture.
- the inverse of this encoding procedure can be performed to create the texture at, for example, the server 14 .
- the decoded attribute values can then be used as if they were vertex attribute values.
- map application 48 may initially generate and/or may change the style lookup table to generate new styles or to change styles as stored in the lookup table.
- the map application 48 may perform style data generation and changes, either partially or completely, using structure data from the map database or data in the client device such as preprogrammed style information, user inputs, etc.
- the logic of the map application 48 may implement an image rendering procedure using this data.
- one or more of the image shaders 44 and 46 decode the attribute values for one or more of the vertex data points based on the attribute reference of each vertex data point and the attribute values stored for vertex references within the lookup table 50 .
- the image rendering procedure thereafter uses these attribute values to render the image on the display device 34 .
- vertex attribute value decoding While it is preferable to perform vertex attribute value decoding within the vertex shaders 44 (as the decoding is then only performed once per vertex data point), it is also possible in some cases to perform the vertex attribute value decoding in the fragment shaders 46 (which generally use the decoded attribute values to render an image.) However, performing vertex attribute value decoding in the fragment shaders 46 generally involves more processing, as the fragment shaders 46 may need to decode a particular vertex data point more than once if that vertex data point is relevant to more than one pixel being rendered by the fragment shaders 46 . As a result, in the following discussion, it will be assumed that the vertex attribute value decoding is performed in the vertex shaders 44 , although as noted above, this need not be the case in all instances.
- each vertex shader 44 processes the vector data to produce an output for each input data point, each output indicating the two-dimensional location for each of the vector data points on the display screen 34 along with, in some cases, vertex attributes for those points.
- each vertex shader 44 implements a graphics processing function that may be used to add special effects to objects and generally operates to convert or transform the positions or locations of vertex data points from the virtual or reference space (as stored in the map database 12 ) to a two-dimensional point for display on an image display screen and a depth value associated with that point (if needed).
- each vertex shader 44 is run once for each vertex data point given to the processor 30 and, in the map imaging system being described as an example herein, the purpose of each vertex shader 44 is transform the two-dimensional position of each vertex in the virtual space to the two-dimensional coordinate at which it appears on the screen.
- the vertex shaders 44 may also use the attribute reference of the vertex data points and the associated texture to determine the values of the vertex attributes for each vertex from the texture or lookup table 50 , and the image rendering system 40 then uses these values to manipulate properties such as position, color, and texture coordinates.
- the outputs of the vertex shaders 44 are provided to the next stage in the graphics pipeline, which in this case is a rasterizer 36 .
- vertex shaders 44 there may be various different types of vertex shaders 44 that may operate or execute on different types of image objects.
- the vertex shaders 44 may include area shaders (to process area object types and text box objects), road shaders (to process road objects, line meshes, and arrow objects), building shaders (to process building objects which may be three-dimensional objects), and image shaders which use traditional textures to create an image on a display.
- the vertex shaders 44 may use or be programmed in any of a set of different languages depending on which API is used by the graphics engine or client device 40 .
- the vertex shaders 44 can be written in various forms of assembly language, they can also use or be programmed using higher-level languages such as DirectX's HLSL and OpenGL's GLSL, along with Cg, which can compile to both OpenGL and DirectX shader instructions.
- the vertex shaders 44 are programmed to convert the attribute reference provided as part of each vertex data point to a plurality of vertex attribute values by obtaining the corresponding vertex attribute values from the texture based lookup table 50 based on the value of the vertex attribute reference.
- the vertex shaders 44 will render the vector data based on the viewpoint or view currently associated with the image displayed or being displayed, as determined by the display logic of the map application 48 .
- the vector data can be downloaded to the device 40 or may be sent from the processor 30 a to the processor 30 b such that each vertex data point includes a single attribute reference value, instead of multiple vertex attribute values, one for each of a set of vertex attributes, as would be required by typical imaging systems.
- the only additional information that needs to be downloaded or sent between processors is the lookup table 50 which can be easily sent as a texture map and which, furthermore, only needs to be sent once and stored in the memory 49 or in the memory of the graphics card.
- the lookup table 50 can be used by the vertex shaders 44 any number of times to thereafter to resolve the attributes for the vector data points for any number of image features.
- the rasterizer 36 uses the outputs of the vertex shaders 44 to render an image on the display screen 34 .
- the rasterizer 36 may call or use the fragment shaders 46 which fill in or determine the values of pixels in the image being displayed on the display device 34 by interpolating between the values of the vector attributes as defined by or output by the vertex shader 44 (or as determined in the fragment shaders 46 ), so as draw lines, outlines and fill in areas of the image being displayed.
- the fill colors, line widths, light properties, etc. of the image at each point are defined in some manner by the vertex attribute values output by the vertex shader 44 , which in turn are determined using the lookup table 50 of, for example, FIG. 3B .
- the fragment shaders 46 also referred to as pixel shaders, perform complex per-pixel effects.
- the fragment shaders 46 are essentially a computation kernel function that computes color and other attributes of each pixel.
- the fragment shaders 46 can range from always outputting the same color, to applying a lighting value, to doing bump mapping, shadows, specular highlights, translucency and other phenomena.
- the fragment shaders 46 can alter the depth of a pixel (for z-buffering), or output more than one color if multiple rendering targets are active. However, this is not always the case, such as in OpenGL ES 2.0.
- the fragment shaders 46 may use any of a set of different languages depending on which API the graphics rendering engine 40 uses. However, the fragment shaders 46 will typically use the same language used by the vertex shaders 44 .
- the fragment shaders 46 can also or instead perform vertex attribute decoding using the attribute reference of each vertex data point and the lookup table 50 .
- downloading or generating the lookup table 50 in the form of a texture map in a single instance to be stored in the memory 49 , enables future vector data sent from the map database 12 or between the processors 30 a and 30 b including an attribute reference to the lookup table 50 to be more limited in scope. More particularly, using this communication and processing technique requires less bandwidth and/or time as compared to providing each vertex point with values for each of a set of attributes during the communication procedure. This technique also reduces the amount of storage space needed to store vertex data. This technique is very useful when, as typically the case with maps, certain combinations of vertex attributes are shared by many vertices sent as part of the vector data.
- each future set of data sent from the map database 12 which may be sent on a map tile by map tile basis (wherein each map tile is a predetermined amount of data or data covering a predetermined geographical region for a map), can use the same lookup table 50 to resolve vertex attributes.
- lookup table 50 to decode or resolve vertex attribute values based on a vertex attribute reference sent as part of the data for each vertex data point essentially reduces the amount of data that needs to be sent from the server 14 or that needs to be sent between processors 30 a and 30 b in further part because the texture map or lookup table 50 does not need to be sent for each map tile but can, instead or in some cases, be sent down or generated once per download.
- the lookup table 50 can be stored in the memory 32 and used for each subsequent downloaded tile to provide consistent styles or a consistent look and feel of the image as rendered on the image display device 34 (i.e., such that roads, labels or other image objects of the same type have the same style or look and feel because the vertices of these image objects use the same vertex attribute values as defined by a particular style in the lookup table 50 ).
- a map image once downloaded and rendered using a particular style or set of styles, can be easily changed or altered to have a different look and feel or a different set of styles by simply changing the values of the lookup table 50 for the styles defined therein and then re-rendering the image using the original vertex data.
- This technique allows re-rendering the image to be displayed in a manner that changes the look and feel of the image, such as by changing color properties, line width properties, fill properties, etc. by simply changing the vertex attribute values for a particular style within the lookup table 50 and then re-rendering the image using the new or changed lookup table 50 .
- this technique does not necessitate downloading a whole new set of vertex data points from the map database 12 (or regenerating new vertex data points with new attributes within the processor 30 a ), as the originally downloaded vertex data points can be used to re-render the map image using a new set of style attribute values. At most, this technique only requires downloading or generating a new lookup table or texture map to define the new attribute values for the styles defined therein.
- multiple different lookup tables 50 can be downloaded or generated and stored in the memory 49 of the image rendering device in the first instance, and the logic of the map application 48 can select or determine which lookup table 50 to use based on predetermined logic and/or actions performed by the user (e.g., the zoom level at which the user is viewing the map, highlighting or selections performed by the user, etc.) If, on the other hand, the vertex data was downloaded with actual values for each of the vertex attributes for each vertex point, it would be necessary to download new vertex data points with different attribute values to change the style properties of the image being rendered.
- using the lookup table 50 to resolve or decode vertex attribute values allows the client device 40 to re-render an image to change certain visual attributes thereof (such as colors, line widths, light properties, etc.) using the originally downloaded vector data.
- the imaging system only needs to download, generate, use or select a different lookup table 50 defining the styles differently (e.g., changing the values of the attributes associated with a particular style) to re-render the image using the new style.
- Different styles may be used, for example, to change or alter the manner in which lines are created (colors, widths, etc.) and to change colors or other light attributes of an image.
- the styles used to produce an image may be changed by the map application 48 downloading or generating a new lookup table in response to a request for a changed image view, or may be changed by originally providing multiple different predetermined lookup tables to the rendering engine 40 and allowing the logic within the map application 48 to switch between different lookup tables 50 to resolve the vertex attribute data and to re-render the image based on actions of the user or other factors.
- This image rendering technique enables image styles to be effectively changed with minimal data downloading (either between the server and the client or between the processors 30 a and 30 b ), and thus enables an image such as a map image, to be rerendered using different styles to create a different look and feel of the image, in response to user actions, without a significant amount of data downloading or communication.
- This technique thereby greatly reduces the amount of data that needs to be sent from the server 14 to the rendering engine 40 or between processors 30 a and 30 b when simple style changes to the image are needed.
- FIG. 4 illustrates an example process or routine 60 that may be implemented by a graphics or image rendering engine, such as the rendering engine 40 of FIG. 2 to render an image using the technique described above.
- a block 62 contacts and downloads vector data from the map database 12 via the network interface 42 .
- This vector data includes vertex data points, wherein each vertex data point includes spatial location values and at least one attribute reference value, and can include one or more lookup tables to be used in resolving the attribute reference values within the vertex data points.
- the block 62 can generate lookup tables based on user actions, logic in the device 40 , other data from the database 12 , etc.
- the block 62 then stores the downloaded or generated data, including the vector data points and the lookup table(s) in the memory 49 of the image rendering device 40 .
- the block 62 will use the communications routine 43 of FIG. 2 to communicate via the network interface 42 with the server 14 .
- a block 64 provides the vertex data and the lookup tables to the processor 30 b and executes the vertex shaders 44 of, for example, the image rendering engine 40 FIG. 2 .
- the particular vertex shaders to be used on each set of vertex data will depend or be determined by the object type of the particular vertex data points. That is, all of the vertex data points of one type (such as all roads, all text boxes, all areas, etc.) may be provided at the same time to a vertex shader that is designed to process those types of vertices and may be processed together (i.e., in one call) within the appropriate vertex shader.
- the vertex shaders 44 may decode the vertex attribute values for the vector data points provided thereto, and does so using one of the stored lookup tables, e.g., one of the textures, to resolve or decode the vertex attribute reference provided within each of the vertex data points to determine multiple vertex attribute values for each vertex data point.
- one of the stored lookup tables e.g., one of the textures
- a particular vertex shader will obtain or use the attribute reference in the form of, for example, a style number to obtain the column within the lookup table 50 to access for each of the vertex attributes provided in the different rows of the lookup table 50 .
- all of the vertex attribute values for a particular style may not be relevant to every type of image object and the different types of vertex shaders 44 may obtain only a subset of the vertex attribute values stored for each style.
- the vertex shader 44 also operates in a traditional or typical manner to determine the two-dimensional location of the vertices provided thereto on the display device 34 , which locations are provided to the rasterizer 36 .
- the rasterizer 36 renders the image on the display device 34 using the output of the vertex shader 44 .
- the rasterizer 36 may call the fragment shaders 46 to determine pixel values for each of the pixels for the image being displayed.
- the rasterizer 36 will use the fragment shaders 46 in known manners to produce an image based on the outputs of the vertex shader 44 , including the vertex attribute values.
- a block 70 obtains or selects a new lookup table to use to resolve the new styles within, for example, the vertex shader (or changes the values of the current lookup table to alter the values of at least one style).
- this new reference lookup table may be downloaded from the map database 12 via the network interface 42 , may be chosen as a different one of a number of lookup tables originally downloaded from the map database 12 and stored in the data memory 49 in the anticipation that the styles might need to be changed in a predetermined or expected manner, may be an altered or changed version of the lookup table, as changed by the logic of the map application 48 or may be generated partially or completely by the map application 48 using other structure data from the map database 12 or data in the client device such as preprogrammed style information or user inputs as examples only.
- the attribute values of a particular style number may be changed for any desired reason.
- the vector data i.e., the vertex data points may be encoded with the same or different style numbers in any desired manner.
- all of the vertex points of an image tile may use the same style number
- the vertex points of each image object of the same object type may use the same style number or the vertex points of each image object may use different style numbers, to name but a few.
- the same style number may be used for vertex points of various different tiles or image objects in different tiles as well.
- vertices associated with common types of objects may all use the same style number so that all of the objects of a particular type (primary roads, labels, arterial roads, terrain, etc.), are rendered using the same style and therefore have the same look and feel.
- the block 64 determines the new style attribute lookup table to use either as stored in memory 49 or as obtained from the map database 12 , the block 64 then re-executes the vertex shaders 44 , which then resolve or decode the vertex attributes using the new lookup table and which thus causes the block 66 to re-render the image using the new vertex attribute values for the changed style.
- this process can be repeated any number of times based on user actions or other inputs to the logic of the map application 48 .
- a two-tiered lookup table determines attribute values for vertex data points.
- the use of a two-tiered lookup table enables styles to be changed on a feature by feature basis to thereby be able to change the look and feel of some features (referencing a particular set of style attributes) without changing the look and feel of other features (which may originally have referenced the same set of style attributes).
- a feature may be a grouping of a number of different image objects (either of the same type or of different types).
- all arterial roads, or all secondary streets, or all labels or all text boxes within a map tile may be defined as a common feature.
- each of the image objects of the feature are of the same object type.
- a university campus may be defined as a feature, and this campus may include image objects of various different types.
- this feature may include various building objects, may include various roads or paths traversing the campus, may include text boxes or area boxes associated with the campus. These image objects of different types may be grouped together as a common feature.
- the system in addition to using a style attribute lookup table 50 , the system also uses a feature lookup table such as the lookup table 70 of FIG. 3C .
- the additional feature lookup table 70 is a single row of values with multiple columns, in which each column is associated with a different feature number and the value within the row indicates a style number for the associated feature number.
- the style number refers to one of the style references or columns in the lookup table 50 of FIG. 3B .
- the vertex data downloaded from the map database 12 or generated by the map application 48 will, instead of having an attribute reference in the form of a style reference, will have an attribute reference that refers to a particular feature number (referred to herein as a feature reference) defining the feature number in the lookup table 70 with which the vertex data point is associated.
- a feature reference refers to a particular feature number (referred to herein as a feature reference) defining the feature number in the lookup table 70 with which the vertex data point is associated.
- the feature lookup table 70 may also be encoded as a texture map (albeit of a single row) and may be download from the map database 12 or generated within the client device 40 , in addition to the style lookup table 50 that defines the values of each of the set of styles to be used for different features.
- the vertex shaders 44 or other shaders that perform the vertex attribute decoding function, when decoding each vertex data point, will access the texture map 70 using the feature reference sent or stored with a particular vertex data point to obtain a style number currently associated with that feature.
- the vertex shaders 44 (or other shaders as appropriate) will then use the style number as a style attribute reference to decode or determine the vertex attribute values for each of the data points based on that style number as stored in the lookup table 50 of FIG. 3B .
- the rendering engine 40 can download and/or generate a predetermined or fixed style reference lookup table, such as the table 50 , once and use that style reference lookup table in all future operations. That is, the style reference lookup table 50 may, in this case, define styles in a fixed manner to be used thereafter when rendering an image. However, the style used to render particular image objects may be changed by making changes to the feature texture map 70 .
- Changing the look and feel of the displayed image on a feature by feature basis may be desirable when, for example, a user wishes to highlight or view common types of image elements, such as all arterial roads, within a displayed image or when a user wishes to highlight or change properties of a logical group of different types of image objects, such as all image objects associated with a university campus. To do so, the user merely needs to highlight or select an element within an area or group of objects that he or she wishes to highlight, and specify one of the image objects of the feature type that the user wishes to highlight.
- the map application 48 may then determine the feature number of the selected image object and change the style number associated with that feature number (within the texture map 70 ) to a different style number that, for example, changes the color of the feature, to thereby highlight all of the image objects of that feature or all image elements encoded using that same feature number.
- style lookup table 50 new feature lookup tables 70 may be downloaded from the database 12 in response to user actions, may be downloaded as one of the series of different lookup tables at an initial download or may be changed by the logic within the map application 48 to alter styles on a feature by feature basis. In any event, this use of the feature lookup table 70 of FIG.
- the imaging system while enhancing the flexibility of the imaging system by enabling the imaging system to alter the look and feel of a displayed image on a feature by feature basis, also reduces the amount of downloading or communicating data between the processors 30 a and 30 b .
- the only downloading or inter-processor communication that needs to be performed to change the look and feel of an image already displayed is the downloading of a new or altered feature lookup table, which is relatively small in nature and is typically smaller than the style lookup table 50 .
- an example process or routine 80 that may be implemented by one of the image shaders, such as one of the vertex shaders 44 or one of the fragment shaders 46 , to decode the vertex attribute values using the style lookup table 50 alone or in combination with the reference lookup table 70 is described in more detail.
- the image shader which may be for example a particular one of the vertex shaders 44 , first obtains the appropriate style lookup table 50 and, if needed the appropriate feature lookup table 70 , as stored in the data memory 49 (which may additionally be provided on the graphics card if so desired).
- the particular lookup tables to use may change or vary from time to time based on input from the map application 48 , new downloads, etc.
- the image shader obtains the next vertex data point to be processed from the data memory 49 or other graphics card memory.
- the vertex data points received from the map database 12 may be categorized or segregated into lists of vertex data points associated with different object types or shader types, so that the vertex data points to be processed by a particular image shader, such as a particular vertex shader, are predefined or are located together.
- the block 84 may determine whether the next obtained vertex data point is of the appropriate object type for this particular image shader and, if not, may discard that vertex data point and obtain the next vertex data point, until the block 84 locates a vertex data point having the correct object type for processing by this image shader.
- a block 86 obtains the value of attribute reference of the obtained vertex data point to determine the style reference number to be used to decode the vertex attribute values to be used for this vertex data point.
- the value of the attribute reference of the vertex data point determines or identifies the appropriate style column in the lookup table 50 and thus the appropriate X value for use for obtaining the vertex attribute value from the two-dimensional lookup table 50 (which in this case may be a texture).
- the attribute reference of the vertex data point will be a feature reference and the block 86 uses the value of this feature reference to determine the specific style reference number to use from lookup table 70 (which again may be a texture).
- the feature reference of the vertex data point determines the X location in the single row of the lookup table 70 , and this value can be multiplied by the size of each memory location within the lookup table 70 to determine the exact memory location in the lookup table 70 from which to obtain the style reference number.
- the block 86 uses the obtained style reference number to determine the appropriate column of the lookup table 50 from which to determine vertex attribute values for the vertex data point being processed.
- the image shader determines the next vertex attribute to decode.
- a vertex attribute value will generally be stored for all of the possible vertex attributes for each style number
- a particular vertex data point may not need or use all of the different vertex attributes stored for each style number.
- the attribute values that are relevant to the vertex data point being decoded may be and usually are a subset of (that is less than) the total number of attribute values stored for each style number.
- the vertex shader and the fragment shader
- the image shader will know, beforehand, which particular vertex attributes it needs and the relative location (e.g. the row) of these vertex attributes within the style lookup table 50 .
- a block 90 obtains the vertex attribute value for the particular vertex attribute from the lookup table 50 based on the determined column and row locations of the vertex attribute value.
- this procedure may involve making a texture call to read data or values from the lookup table 50 as a texture map.
- a block 92 determines whether the last vertex attribute for this particular type of vector data point has been decoded and if not, returns controls back to the block 88 to determine the next vertex attribute value to decode for this vertex data point.
- a block 94 determines whether the last vertex data point within the data memory 49 for processing by this image shader has been analyzed, and if not, returns control to the block 84 which obtains the next vertex data point from the data memory 49 for processing.
- a block 96 processes the location information within the vertex data points as well as the decoded vertex attribute values for the complete set of vertex data points to produce image shader outputs.
- the image shader is a vertex shader
- the image shader outputs may include two-dimensional location points (and a z-depth if desired) as well as values for appropriate vertex data point attributes, and these outputs are provided to the rasterizer 36 for processing.
- some or all of the vertex attribute values for vertex data points might be used in the fragment shaders 46 to render an image based on those decoded vertex attribute values.
- fragment shader may use the decoded attribute values as well as the location of the vertex data point provided from the rasterizer 36 to pixelate or otherwise render an image on the display screen 34 using the rasterizer 36 .
- the outputs of the fragment shaders 46 are provided to the rasterizer 36 to be rendered on the image display device 34 .
- the network 25 may include but is not limited to any combination of a LAN, a MAN, a WAN, a mobile, a wired or wireless network, a private network, or a virtual private network.
- client devices While only four client devices are illustrated in FIG. 1 to simplify and clarify the description, it is understood that any number of client computers or display devices are supported and can be in communication with the server 14 .
- Modules may constitute either software modules (e.g., code embodied on a machine-readable medium or in a transmission signal) or hardware modules.
- a hardware module is tangible unit capable of performing certain operations and may be configured or arranged in a certain manner.
- one or more computer systems e.g., a standalone, client or server computer system
- one or more hardware modules of a computer system e.g., a processor or a group of processors
- software e.g., an application or application portion
- a hardware module may be implemented mechanically or electronically.
- a hardware module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a special-purpose processor, such as a field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC)) to perform certain operations.
- a hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.
- the term hardware should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in a certain manner or to perform certain operations described herein.
- hardware modules are temporarily configured (e.g., programmed)
- each of the hardware modules need not be configured or instantiated at any one instance in time.
- the hardware modules comprise a general-purpose processor configured using software
- the general-purpose processor may be configured as respective different hardware modules at different times.
- Software may accordingly configure a processor, for example, to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.
- Hardware and software modules can provide information to, and receive information from, other hardware and/or software modules. Accordingly, the described hardware modules may be regarded as being communicatively coupled. Where multiple of such hardware or software modules exist contemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) that connect the hardware or software modules. In embodiments in which multiple hardware modules or software are configured or instantiated at different times, communications between such hardware or software modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware or software modules have access. For example, one hardware or software module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware or software module may then, at a later time, access the memory device to retrieve and process the stored output. Hardware and software modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).
- a resource e.g., a collection of information
- processors may be temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions.
- the modules referred to herein may, in some example embodiments, comprise processor-implemented modules.
- the methods or routines described herein may be at least partially processor-implemented. For example, at least some of the operations of a method may be performed by one or processors or processor-implemented hardware modules. The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the processor or processors may be located in a single location (e.g., within a home environment, an office environment or as a server farm), while in other embodiments the processors may be distributed across a number of locations.
- the one or more processors may also operate to support performance of the relevant operations in a “cloud computing” environment or as a “software as a service” (SaaS). For example, at least some of the operations may be performed by a group of computers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., application program interfaces (APIs).)
- SaaS software as a service
- the performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines.
- the one or more processors or processor-implemented modules may be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm). In other example embodiments, the one or more processors or processor-implemented modules may be distributed across a number of geographic locations.
- an “algorithm” or a “routine” is a self-consistent sequence of operations or similar processing leading to a desired result.
- algorithms, routines and operations involve physical manipulation of physical quantities. Typically, but not necessarily, such quantities may take the form of electrical, magnetic, or optical signals capable of being stored, accessed, transferred, combined, compared, or otherwise manipulated by a machine.
- any reference to “one embodiment” or “an embodiment” means that a particular element, feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment.
- the appearances of the phrase “in one embodiment” in various places in the specification are not necessarily all referring to the same embodiment.
- Coupled and “connected” along with their derivatives.
- some embodiments may be described using the term “coupled” to indicate that two or more elements are in direct physical or electrical contact.
- the term “coupled,” however, may also mean that two or more elements are not in direct contact with each other, but yet still co-operate or interact with each other.
- the embodiments are not limited in this context.
- the terms “comprises,” “comprising,” “includes,” “including,” “has,” “having” or any other variation thereof, are intended to cover a non-exclusive inclusion.
- a process, method, article, or apparatus that comprises a list of elements is not necessarily limited to only those elements but may include other elements not expressly listed or inherent to such process, method, article, or apparatus.
- “or” refers to an inclusive or and not to an exclusive or. For example, a condition A or B is satisfied by any one of the following: A is true (or present) and B is false (or not present), A is false (or not present) and B is true (or present), and both A and B are true (or present).
Abstract
Description
Claims (32)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/174,351 US9092907B2 (en) | 2011-06-30 | 2011-06-30 | Image shader using two-tiered lookup table for implementing style attribute references |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/174,351 US9092907B2 (en) | 2011-06-30 | 2011-06-30 | Image shader using two-tiered lookup table for implementing style attribute references |
Publications (2)
Publication Number | Publication Date |
---|---|
US20150054842A1 US20150054842A1 (en) | 2015-02-26 |
US9092907B2 true US9092907B2 (en) | 2015-07-28 |
Family
ID=52479949
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/174,351 Active 2033-07-01 US9092907B2 (en) | 2011-06-30 | 2011-06-30 | Image shader using two-tiered lookup table for implementing style attribute references |
Country Status (1)
Country | Link |
---|---|
US (1) | US9092907B2 (en) |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10089291B2 (en) * | 2015-02-27 | 2018-10-02 | Microsoft Technology Licensing, Llc | Ink stroke editing and manipulation |
US20170357414A1 (en) * | 2016-06-12 | 2017-12-14 | Apple Inc. | Map Application with Novel Search, Browsing and Planning Tools |
CN110489083B (en) * | 2018-05-14 | 2022-06-21 | 武汉斗鱼网络科技有限公司 | Texture sharing method, computer equipment and storage medium |
US11423828B2 (en) * | 2020-12-28 | 2022-08-23 | Texas Instruments Incorporated | Light-emitting diode (LED) brightness non-uniformity correction for LED display driver circuit |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040208370A1 (en) * | 2003-04-17 | 2004-10-21 | Ken Whatmough | System and method of converting edge record based graphics to polygon based graphics |
US6917878B2 (en) * | 2002-04-30 | 2005-07-12 | Telmap Ltd. | Dynamic navigation system |
US20060069790A1 (en) * | 2004-09-30 | 2006-03-30 | Surender Surana | Content presentation adaptation |
US7136077B2 (en) * | 2004-06-09 | 2006-11-14 | International Business Machines Corporation | System, method, and article of manufacture for shading computer graphics |
US20110316854A1 (en) * | 2010-06-23 | 2011-12-29 | Bryan Vandrovec | Global Visualization Process Terrain Database Builder |
-
2011
- 2011-06-30 US US13/174,351 patent/US9092907B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6917878B2 (en) * | 2002-04-30 | 2005-07-12 | Telmap Ltd. | Dynamic navigation system |
US20040208370A1 (en) * | 2003-04-17 | 2004-10-21 | Ken Whatmough | System and method of converting edge record based graphics to polygon based graphics |
US7136077B2 (en) * | 2004-06-09 | 2006-11-14 | International Business Machines Corporation | System, method, and article of manufacture for shading computer graphics |
US20060069790A1 (en) * | 2004-09-30 | 2006-03-30 | Surender Surana | Content presentation adaptation |
US20110316854A1 (en) * | 2010-06-23 | 2011-12-29 | Bryan Vandrovec | Global Visualization Process Terrain Database Builder |
Also Published As
Publication number | Publication date |
---|---|
US20150054842A1 (en) | 2015-02-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8803901B1 (en) | Map rendering using interpolation of style parameters across zoom levels | |
US8400453B2 (en) | Rendering a text image following a line | |
US9093006B2 (en) | Image shader using style attribute references | |
US8237745B1 (en) | Label positioning technique to reduce crawling during zoom activities | |
KR101925292B1 (en) | Gradient adjustment for texture mapping to non-orthonormal grid | |
KR102001191B1 (en) | Rendering a text image following a line | |
US8730258B1 (en) | Anti-aliasing of straight lines within a map image | |
US20230053462A1 (en) | Image rendering method and apparatus, device, medium, and computer program product | |
US7626591B2 (en) | System and method for asynchronous continuous-level-of-detail texture mapping for large-scale terrain rendering | |
US9135743B2 (en) | Visualize the obscure object in 3D space | |
US8970583B1 (en) | Image space stylization of level of detail artifacts in a real-time rendering engine | |
US9965886B2 (en) | Method of and apparatus for processing graphics | |
US9495767B2 (en) | Indexed uniform styles for stroke rendering | |
US20090195555A1 (en) | Methods of and apparatus for processing computer graphics | |
US8760451B2 (en) | Rendering a text image using texture map character center encoding with character reference encoding | |
US9679349B2 (en) | Method for visualizing three-dimensional data | |
US9092907B2 (en) | Image shader using two-tiered lookup table for implementing style attribute references | |
US9542724B1 (en) | Systems and methods for stroke rendering on digital maps | |
GB2444628A (en) | Sorting graphics data for processing | |
US9911205B1 (en) | Visual continuity for arbitrary length stipple patterns | |
Scheibel et al. | Attributed vertex clouds | |
US20130002679A1 (en) | Rendering a text image using texture map character center encoding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:CORNELL, BRIAN;REEL/FRAME:026564/0022Effective date: 20110630 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044334/0466Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |