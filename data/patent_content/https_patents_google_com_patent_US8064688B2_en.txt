US8064688B2 - Object recognizer and detector for two-dimensional images using Bayesian network based classifier - Google Patents
Object recognizer and detector for two-dimensional images using Bayesian network based classifier Download PDFInfo
- Publication number
- US8064688B2 US8064688B2 US12/259,371 US25937108A US8064688B2 US 8064688 B2 US8064688 B2 US 8064688B2 US 25937108 A US25937108 A US 25937108A US 8064688 B2 US8064688 B2 US 8064688B2
- Authority
- US
- United States
- Prior art keywords
- image
- classifier
- classifiers
- sub
- variables
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
- G06V40/164—Detection; Localisation; Normalisation using holistic features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2415—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on parametric or probabilistic models, e.g. based on likelihood ratio or false acceptance rate versus a false rejection rate
- G06F18/24155—Bayesian classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/243—Classification techniques relating to the number of classes
- G06F18/24323—Tree-organised classifiers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/285—Selection of pattern recognition techniques, e.g. of classifiers in a multi-classifier system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/29—Graphical models, e.g. Bayesian networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/172—Classification, e.g. identification
Definitions
- the present disclosure generally relates to image processing and image recognition, and more particularly, to a system and method for recognizing and detecting 3D (three-dimensional) objects in 2D (two-dimensional) images using Bayesian network based classifiers.
- Object detection is the technique of using computers to automatically locate objects in images, where an object can be any type of a three dimensional physical entity such as a human face, an automobile, an airplane, etc. Object detection involves locating any object that belongs to a category such as the class of human faces, automobiles, etc. For example, a face detector would attempt to find all human faces in a photograph.
- FIG. 1A illustrates a picture slide 10 showing some variations in appearance for human faces.
- the class of human faces may contain human feces for males and females, young and old, bespectacled with plain eyeglasses or with sunglasses, etc.
- another class of objects may contain cars that vary in shape, size, coloring, and in small details such as the headlights, grill, and tires.
- a person's race, age, gender, ethnicity, etc. may play a dominant role in defining the person's facial features.
- the visual expression of a face may be different from human to human.
- FIG. 1C shows a picture slide 14 illustrating geometric variation among human faces.
- Various human facial, geometry variations are outlined by rectangular boxes superimposed on the human, faces in the slide 14 in FIG. 1C .
- a computer-based object detector must accommodate all these variations and still distinguish the object from any other pattern that may occur in the visual world.
- a human face detector must be able to find faces regardless of facial expression, variations in the geometrical relationship between the camera and the person, or variation in lighting and shadowing.
- Most methods for object detection use statistical modeling to represent this variability. Statistics is a natural way to describe a quantity that is not fixed or deterministic, such as a human face. The statistical approach is also versatile. The same statistical modeling techniques can potentially be used to build object detectors for different objects without re-programming.
- One known method represents object appearance by several prototypes consisting of a mean and a covariance about the mean.
- Another known technique consists of a quadratic classifier. Such a classifier is mathematically equivalent to the representation of each class by its mean and covariance.
- These and other known techniques emphasize statistical relationships over the full extent of the object. As a consequence, they compromise the ability to represent small areas in a rich and detailed way.
- Other known techniques address this limitation by decomposing the model in terms of smaller regions. These methods can represent appearance in terms of a series of inner products with portions of the image.
- another known technique decomposes appearance further into a sum of independent models for each pixel.
- Another known technique allows for geometric flexibility with a more powerful representation by using richer features (each takes on a large set of values) sampled at regular positions across the full extent of the object. Each feature measurement is treated as statistically independent of all others.
- the disadvantage of this approach is that any relationship not explicitly represented by one of the features is not represented in the statistical model. Therefore, performance depends critically on the quality of the feature choices.
- object detection may be implemented by forming a statistically based classifier to discriminate the object from other visual scenery.
- a statistically based classifier to discriminate the object from other visual scenery.
- Such a scheme requires choosing the form of the statistical representation and estimating the statistics from labeled training data.
- the overall accuracy of the detection program can be dependent on the skill and intuition of the human programmer. It is therefore desirable to design as much of the classifier as possible using automatic methods that infer a design based on actual labeled data in a manner that is not dependant on human intuition.
- the present disclosure is directed to a system and a method for detecting an object in a 2D (two-dimensional) image.
- the method of detection may include, for each of a plurality of view-based detectors, computing a transform of a digitized version of the 2D image containing a representation of an object, wherein the transform is a representation of the spatial frequency content of the image as a function of position in the image.
- Computing the transform generates a plurality of transform coefficients, wherein each transform coefficient represents corresponding visual information from the 2D image that is localized in space, frequency, and orientation.
- the method may also include applying the plurality of view-based detectors to the plurality of transform coefficients, wherein each view-based detector is configured to detect a specific orientation of the object in the 2D image based on visual information received from corresponding transform coefficients.
- Each of the plurality of view-based detectors includes a plurality of stages ordered sequentially where each stage is a classifier.
- the cascaded stages may be arranged in ascending order of computational complexity.
- the classifier forming each cascaded stage may be organized as a ratio of two Bayesian networks over relevant features, where each feature is computed from the transform coefficients.
- the cascaded stages may also be arranged in order of coarse to fine resolution of the image sites at which they evaluate the detector.
- the method includes combining results of the application of the plurality view-based detectors, and determining a pose (i.e., position and orientation) of the object from the combination of results of the application of the plurality view-based detectors.
- the present disclosure is directed to a system for determining a classifier to discriminate between two classes.
- the system is used by an object detection program, where the classifier detects the presence of a 3D (three dimensional) object in a 2D (two-dimensional) image.
- the classifier includes a cascade of sub-classifiers where each sub-classifier is based on a ratio of Bayesian networks.
- each sub-classifier involves a candidate coefficient-subset creation module, a feature creation module for use in representation of unconditional distributions, a probability estimation module, an evaluation module, a coefficient-subset selection module, a Bayesian network connectivity creation module, a feature creation module for use in representation of conditional distributions, a conditional probability estimation module, a detection threshold determination module, and a non-object example selection module.
- the transform coefficients are the result of a wavelet transform operation performed on a two-dimensional (2D) digitized image where the 2D image may be subject to lighting correction and normalization.
- Computing the transform generates a plurality of transform coefficients, wherein each transform coefficient represents corresponding visual information from the 2D image that is localized in space, frequency, and orientation.
- the candidate coefficient-subset creation module may create a plurality of candidate subsets of coefficients.
- the feature creation module for unconditional distributions may assign a function mapping the values of each subset to a discrete valued variable.
- the probability estimation module will estimate probability distributions over each feature and coefficient for each class.
- the evaluation module evaluates the probability of each of a set of images on each probability distribution.
- the coefficient subset selection module will select a set of the candidate subsets.
- the Bayesian network connectivity creation module creates a Bayesian network graph used by the Bayesian networks for each of the two classes (object and non-object). This graph entails the dependencies and independencies resulting from the selected subsets.
- Another feature selection module may represent the variables at each Bayesian network node by a pair of discrete valued functions.
- a probability estimation module may estimate the probability distribution for each conditional probability distribution in each Bayesian Network.
- a non-object example selection module actively selects non-object examples for the next stage from a large database of images.
- the system according to the present disclosure automatically learns the following aspects of the classifier from labeled training data: the Bayesian network graph, the features computed at each node in the Bayesian network, and the conditional probability distributions over each node in the network, thereby eliminating the need for a human to select these parameters, which, as previously described, is highly subject to error.
- the present disclosure is directed to a method for designing a discrete-valued variable representation at each node in a Bayesian network by a two-stage process of tree-structure vector quantization where the first stage constructs a tree over the conditioning variables and the second stage continues construction of the tree over the conditioned variables.
- the present disclosure is directed to a method for estimating the conditional probability distribution at each node in a Bayesian network by a process whereby a classifier is formed as the ratio of the classifiers for the two classes (object and non-object) and the probabilities of this classifier are estimated through, an iterative AdaBoost procedure.
- the present disclosure is directed to a method of finding a Bayesian Network Graph by first finding a restricted Bayesian Network of two layers, where the parents of the second layer nodes are modeled as statistically independent and where the restricted Bayesian Network is chosen using criterion of area underneath ROC curve.
- the present disclosure is directed to a method of classifying two images as either belonging to the same class or to different classes.
- a classifier according to the present disclosure may be employed on the two input images to determine whether the two given images are of the same person or not.
- the present disclosure contemplates a method, which comprises: receiving a digitized version of a two-dimensional (2D) image containing a 2D representation of a three-dimensional (3D) object; obtaining visual information from the digitized version of the 2D image; and classifying the 2D image based on a ratio of a plurality of graphical probability models using the visual information.
- the present disclosure also contemplates a computer-based system that implements this method, and a computer readable data storage medium that stores the necessary program code to enable a computer to perform this method.
- the present disclosure contemplates a method of providing assistance in detecting the presence of a 3D object in a 2D image containing a 2D representation of the 3D object.
- the method comprises: receiving a digitized version of the 2D image from a client site and over a communication network; determining a location of the 3D object in the 2D image using a Bayesian network-based classifier, wherein the classifier is configured to analyze the 2D image based on a ratio of a plurality of Bayesian networks; and sending a notification of the location of the 3D object to the client site over the communication network.
- the present disclosure also contemplates a computer system configured to perform such a method.
- the present disclosure contemplates a method of generating a classifier.
- the method comprises: computing a wavelet transform of each of a plurality of 2D images, wherein each the wavelet transform generates a corresponding plurality of transform coefficients; creating a plurality of candidate subsets of the transform coefficients; selecting a group of candidate subsets from the plurality of candidate subsets; and constructing the classifier based on a ratio of a plurality of Bayesian networks using the group of candidate subsets.
- the present disclosure also contemplates a computer-based system that implements such a method, and a computer readable data storage medium that stores the necessary program code to enable a computer to perform this method.
- FIGS. 1A-1C illustrate difference challenges in object detection
- FIG. 2 illustrates a generalized operational flow for an object finder program according to one embodiment of the present disclosure
- FIG. 3 depicts an exemplary setup to utilize the object finder program according to an embodiment of the present disclosure
- FIGS. 4A and 4B illustrate the classification decision process involving a fixed object size, orientation, and alignment according to one embodiment of the present disclosure
- FIG. 5 is a real-life illustration of the object classification approach outlined in FIG. 6 ;
- FIG. 6 shows an exemplary view-based classification approach utilized by the object finder program according to the present disclosure to detect object locations and orientations
- FIG. 7 shows an example of different orientations for human faces and cars that the object finder program according to the present disclosure may be configured to model
- FIG. 8A depicts the general object detection approach used by the object finder program according to one embodiment of the present disclosure involving an exhaustive search over position and scale;
- FIG. 8B illustrates the positional step size used in a search over position
- FIG. 8C illustrates the step size in scale used in a search over scale
- FIG. 8D illustrates the positional and scale invariance that a classifier must tolerate given a positional step size and a scale step size
- FIG. 9 depicts one embodiment of the object detection process according to the present disclosure.
- FIG. 10 shows a set of subbands produced by a wavelet transform based on a two-level decomposition of an input image using a filter-bank according to one embodiment of the present disclosure
- FIG. 11 depicts an input image and its wavelet transform representation using a symmetric 4/4 filter bank according to one embodiment of the present disclosure
- FIGS. 12A through 12C illustrate a wavelet decomposition, a partially overcomplete wavelet decomposition, and a fully overcomplete wavelet decomposition, respectively, for a two level wavelet transform
- FIGS. 13A and 13B illustrate the positional correspondence between a window sampled directly on the image and the same image window sampled with respect to the wavelet transform of the image according to one embodiment of the present disclosure
- FIGS. 14A through 14E illustrate the process of propagating probability through the wavelet pyramid representation
- FIG. 15 illustrates an image scaling process and corresponding wavelet transform computation according to one embodiment of the present disclosure
- FIG. 16 shows the details of the image scaling process as part of the overall object detection process illustrated in FIG. 9 according to one embodiment of the present disclosure
- FIGS. 17A and 17B are diagrams of a system for automatically constructing a Bayesian network based classifier system according to one embodiment of the present disclosure
- FIG. 18 outlines the major steps involved in preparing object training examples
- FIG. 19 illustrates a process of training a sequence of classifiers using bootstrapping to actively select training examples according to one embodiment of the present disclosure
- FIG. 20 is a flowchart illustrating the process flow through the candidate coefficient subset creation module according to one embodiment of the present disclosure
- FIG. 21 is a flowchart illustrating the process flow through the candidate coefficient subset selection module according to one embodiment of the present disclosure
- FIG. 22 illustrates a quantization tree generated by a tree-structured vector quantization (TSVQ);
- FIG. 23 is a flowchart illustrating the process flow for constructing log-likelihood tables indexed by feature values according to one embodiment of the present disclosure
- FIG. 24 shows an exemplary histogram
- FIG. 25 illustrates an example of how histograms are collected off-line using a set of training images according to one embodiment of the present disclosure
- FIG. 26 depicts a process of selecting the final set of candidate coefficient subsets
- FIG. 27 illustrates how classifiers are estimated using the AdaBoost algorithm according to one embodiment of the present disclosure
- FIG. 28 shows a process for evaluating one feature in one scale in the search across position according to one embodiment of the present disclosure
- FIG. 29 illustrates an embodiment of a candidate-based evaluation of the search in position
- FIG. 30 shows an alternative embodiment of the search in position called a feature-based evaluation
- FIG. 31 depicts various images of humans with the object markers placed on the human faces as detected by the object finder program according to one embodiment of the present disclosure
- FIG. 32 shows various images of teapots with the object markers placed on the teapots detected by the object finder program according to one embodiment of the present disclosure
- FIG. 33 illustrates various images of stop signs with the object markers placed on the stop signs detected by the object finder according to one embodiment of the present disclosure.
- FIG. 34 depicts a process of face recognition according to one embodiment of the present disclosure.
- FIG. 2 illustrates an embodiment of a generalized operational flow for the object detection program according to an embodiment of the present disclosure.
- the object detection program (simply, the “object detector” or “object finder”) is represented by the block 18 .
- a digital image 16 is a typical input to the object detector 18 , which operates on the image 16 and generates a list of object locations and orientations (block 20 ) for the 3D objects represented in the 2D image 16 .
- image and “digital image” are used interchangeably hereinbelow. However, both of these terms are used to refer to a 2D image (e.g., a photograph) containing two-dimensional representations of one or more 3D objects (e.g., human, faces, cars, etc.).
- the object finder 18 may place object markers 52 ( FIG. 5 ) on each object detected in the input image 16 by the object finder 18 .
- the input image may be an image file digitized in one of many possible formats including, for example, a BMP (bitmap) file format, a PGM (Portable Grayscale bitMap graphics) file format, a JPG (Joint Photographic Experts Group) file format, or any other suitable graphic file format.
- each pixel is represented as a set of one or more bytes corresponding to a numerical representation (e.g., a floating point number) of the light intensity measured by a camera at the sensing site.
- the input image may be gray-scale, i.e., measuring light intensity over one range of wavelength, or color, making multiple measurements of light intensity over separate ranges of wavelength.
- FIG. 3 depicts an exemplary setup to utilize the object detector program 18 according to one embodiment of the present disclosure.
- An object finder terminal or computer 22 may execute or “run” the object finder program application 18 when instructed by a user.
- the digitized image 16 may first be displayed on the computer terminal or monitor display screen and, after application of the object finder program 18 , a marked-up version of the input image (e.g., picture slide 50 in FIG. 5 ) may be displayed on the display screen of the object finder terminal 22 .
- a marked-up version of the input image e.g., picture slide 50 in FIG. 5
- the program code for the object finder program application 18 may be stored on a portable data storage medium, e.g., a floppy diskette 24 , a compact disc 26 , a data cartridge tape (not shown) or any other magnetic, solid state, or optical data storage medium.
- the object finder terminal 22 may include appropriate disk drives to receive the portable data storage medium and to read the program code stored thereon, thereby facilitating execution of the object tinder software.
- the object finder software 18 upon execution by a processor of the computer 22 , may cause the computer 22 to perform a variety of data processing and display tasks including, for example, analysis and processing of the input image 16 , display of a marked-up version of the input image 16 (e.g., slide 50 in FIG.
- the object finder terminal 22 may be remotely accessible from a client computer site 28 via a communication network 30 .
- the communication network 30 may be an Ethernet LAN (local area network) connecting all the computers within a facility, e.g., a university research laboratory or a corporate data processing center.
- the object finder terminal 22 and the client computer 28 may be physically located at the same site, e.g., a university research laboratory or a photo processing facility.
- the communication network 30 may include, independently or in combination, any of the present or future wireline or wireless data communication networks, e.g., the Internet, the PSTN (public switched telephone network), a cellular telephone network, a WAN (wide area network), a satellite-based communication link, a MAN (metropolitan area network) etc.
- the object finder terminal 22 may be, e.g., a personal computer (PC), a laptop computer, a workstation, a minicomputer, a mainframe, a handheld computer, a small computing device, a graphics workstation, or a computer chip embedded as part of a machine or mechanism (e.g., a computer chip embedded in a digital camera, in a traffic control device, etc.).
- the computer (not shown) at the remote client site 28 may also be capable of viewing and manipulating digital image files and digital lists of object identities, locations and, orientations for the 3D objects represented in the 2D image transmitted by the object finder terminal 22 .
- the client computer site 28 may also include the object finder terminal 22 , which can function as a server computer and can be accessed by other computers at the client site 28 via a LAN.
- the memory modules may include RAM (random access memory), ROM (read only memory) and HDD (hard disk drive) storage. Memory storage is desirable in view of sophisticated image processing and statistical analysis performed by the object finder terminal 22 as part of the object detection process.
- the arrangement depicted in FIG. 3 may be used to provide a commercial, network-based object detection service that may perform customer-requested object detection in real time or near real time.
- the object finder program 18 at the computer 22 may be configured to detect human faces and then human eyes in photographs or pictures remotely submitted to it over the communication network 30 (e.g., the Internet) by an operator at the client site 28 .
- the client site 28 may be a photo processing facility specializing in removal of “red eyes” from photographs or in color balancing of color photographs.
- the object finder terminal 22 may first automatically detect all human faces and then all human eyes in the photographs submitted and send the detection results to the client computer site 28 , which can then automatically remove the red spots on the faces pointed, out by the object finder program 18 .
- the whole process can be automated.
- the object finder terminal 22 may be a web server running the object finder software application 18 .
- the client site 28 may be in the business of providing commercial image databases.
- the client site 28 may automatically search and index images on the World Wide Web as requested by its customers.
- the computer at the client site 28 may “surf” the web and automatically send a set of images or photographs to the object finder terminal 22 for further processing.
- the object finder terminal 22 may process the received images or photographs and automatically generate a description of the content of each received image or photograph.
- the depth of image content analysis may depend on the capacity of the object finder software 18 , i.e., the types of 3D objects (e.g., human faces, cars, trees, etc.) the object finder 18 is capable of detecting.
- the results of image analysis may then be transmitted back to the sender computer at the client site 28 .
- a face detector may be used as a system to track attention and gaze of customers in a retail setting whereby the face detector automatically determines the locations and direction of each person's head and can infer what items the person is looking at. Such behavior can then be automatically logged to construct a record of how often items are viewed by customers and related to records of purchase for the same items.
- the owner or operator of the object finder terminal 22 may commercially offer a network-based object finding service, as illustrated by the arrangement in FIG. 3 , to various individuals, corporations, or other facilities on a fixed-fee basis, on a per-operation basis or on any other payment plan mutually convenient to the service provider and the service recipient.
- FIG. 4A illustrates an “overall” classifier (or detector) 34 according to one embodiment of the present disclosure. It is discussed hereinbelow that this “overall” classifier 34 is constructed of a sequence of classifiers, where each such classifier in the sequence is referred to hereinbelow interchangeably as a “sub-classifier” or simply a “classifier”. Thus, although the same term “classifier” is used below to refer to the “overall” classifier 34 and one of its constituent parts (or a “sub-classifier”), it is observed that which “classifier” is referred to at any given point in discussion will be evident from the context of discussion.
- the input to the classifier 34 is a fixed-size window 32 sampled from an input image 16 .
- the classifier 34 operates on the fixed size image input 32 and makes a decision whether the object is present in the input window 32 .
- the decision can be a binary one in the sense that the output of the classifier 34 represents only two values—either the object is present or the object is not present, or a probabilistic one indicating a probability from 0 to 1 (or over another scale) indicating the probability that the object is present.
- the classifier only identifies the object's presence when it occurs at a pre-specified range of size and alignment within the window. It is noted that lighting correction (discussed in detail later hereinbelow) may be necessary to compensate for differences in lighting.
- a lighting correction process 36 precedes evaluation by the classifier 34 as illustrated in FIG. 4B .
- a challenge in object detection is the amount of variation in visual appearance, e.g., faces vary from person to person, with varying facial expression, lighting, position and size within the classification window, etc., as shown in FIGS. 1A-1C .
- the classifier 34 in FIGS. 4A-4B may use statistical modeling to account for this variation.
- two statistical distributions are part of each classifier—the statistics of the appearance of the given object in the image window 32 , P(image-window
- ⁇ 1 ) where ⁇ 1 object, and the statistics of the visual appearance of the rest of the visual world, which are identified by the “non-object” class, P(image-window
- ⁇ 2 ), where ⁇ 2 non-object.
- the specification of these distributions will be described hereinbelow under the “Classifier Design” section.
- the classifier 34 may combine these two conditional probability distributions in a likelihood ratio test.
- the classifier 34 may compute the classification decision by retrieving the probabilities associated with the given input image window 32 , P(image-window
- ⁇ can be viewed as a threshold controlling the sensitivity of a classifier (e.g., the classifier 34 ).
- a classifier can miss the object (a false negative) or it can mistake something else for the object (a false positive) (such as a cloud pattern for a human face). These two types of errors are not mutually exclusive.
- the “ ⁇ ” controls the trade-off between these forms of error. Setting “ ⁇ ” to a low value makes the classifier more sensitive and reduces the number of false negatives, but increases the number of false positives. Conversely, increasing the value of “ ⁇ ” reduces the number of false positives, but increases the number of false negatives. Therefore, depending on the needs of a given application, a designer can choose “ ⁇ ” empirically to achieve a desirable compromise between the rates of false positives and false negatives.
- log likelihood ratio test given in equation-1 is equivalent to Bayes decision rule (i.e., the maximum a posteriori (MAP) decision rule) and will be optimal if the representations for P(image-window
- MAP maximum a posteriori
- FIG. 8A depicts the general object detection approach used by the object detector program 18 according to one embodiment of the present disclosure.
- the object detector must apply the classifier 34 repeatedly to original image 16 at regularly spaced (and, usually overlapping) positions of the rectangular image window 32 as shown in FIG. 8A .
- the process makes it possible for the object detector 18 to detect instances of the object at any position within an image.
- the object detector program 18 may iteratively resize the input image and re-apply the classifier in the same fashion to each resized image 62 and 64 , as illustrated in FIG. 8A .
- FIG. 8A depicts the general object detection approach used by the object detector program 18 according to one embodiment of the present disclosure.
- the object detector must apply the classifier 34 repeatedly to original image 16 at regularly spaced (and, usually overlapping) positions of the rectangular image window 32 as shown in FIG. 8A .
- the process makes it possible for the object detector 18 to detect instances of the object at any position within an image.
- the object detector program 18 may iter
- the size of the rectangular image window 32 may remain fixed throughout the whole detection process.
- the size of the image window 32 may be empirically selected based on a number of factors including, for example, object shape, desired accuracy or resolution, resulting computational complexity, efficiency of program execution, etc.
- the size of the rectangular window is 32 ⁇ 24 pixels.
- a classifier may be specialized not only in object size and alignment, but also object orientation.
- the object detector 18 uses a view-based approach with multiple classifiers that are each specialized to a specific orientation of the object as described and illustrated with respect to FIG. 6 .
- a predetermined number of view-based classifiers may be applied in parallel to the input image 16 to find corresponding object orientations.
- there are “m” view-based classifiers three of which 37 , 38 , and 40 are shown in FIG. 6 ).
- Each of the view-based classifiers is designed to detect one orientation of a particular object (e.g., a human face).
- Blocks 37 , 38 , and 40 represent view-based classifiers designed to detect object orientations 1 , 2 , . . . , m.
- the results of the application of the view-based classifiers are then combined at block 42 .
- the combined output indicates specific 3D objects (e.g., human faces) present in the input 2D image.
- object detector program 18 may be trained or modified to detect different other objects (e.g., shopping carts, faces of cats, helicopters, etc.) as well.
- FIG. 5 is a real-life illustration, of the object classification approach outlined in FIG. 6 .
- each view-based classifier e.g., the classifiers 37 , 38 , and 40 in FIG. 6
- the object e.g., a human face, or a car
- the view-based classifier is trained to detect.
- one classifier may be specialized to detect right profile views of faces as shown by the object marker 52 (generated by the classifier upon detecting the corresponding object orientation) in the picture slide 44 .
- the picture slide 44 may thus represent the result obtained at block 37 in FIG. 6 .
- a different view-based classifier may be specialized to detect frontal views as illustrated by the marked-up version of the picture slide 46 in FIG. 5 (corresponding, for example, to block 38 in FIG. 5 ).
- another classifier may be configured to detect left profile views of human faces as illustrated by the marked-up version of the picture slide 48 in FIG. 5 (corresponding, for example, to block 40 in FIG. 5 ).
- Each picture slide shows object markers 52 placed at appropriate orientations of human faces detected by corresponding classifiers.
- the object detector 18 may choose the strongest detection.
- picture slides 46 and 48 represent multiple detections (frontal face detection and left-hand side view detection) for the female face illustrated therein.
- the final combined output in slide 50
- the object detector 18 may thus find orientations and locations of multiple objects (e.g., several human faces) in an image.
- FIG. 7 shows an example of different orientations for human faces and cars that the object detector program 18 may be configured to model.
- the number of orientations to model for each object may be empirically determined.
- two view-based classifiers frontal ( 54 A) and right profile ( 54 B)—are used to detect front and right profiles of human faces.
- the right profile classifier 54 B may be applied to a mirror-reversed, input image.
- eight classifiers one frontal classifier 56 A and seven right-side classifiers 56 B- 56 H—may be used as shown in FIG. 7 .
- the left side views (of cars) may be detected by running the seven right-side classifiers ( 56 B- 56 H) on mirror-reversed images. It is noted that the numerals 54 A- 54 B and 56 A- 56 H are used hereinbelow to represent corresponding view-based classifiers for ease of description.
- the detector 18 has to scan this classifier 34 across the image in position and scale in order to find instances of an object as shown in FIG. 8A .
- This process applied directly can consume a great amount of computational time.
- several computational techniques and heuristic strategies, described hereinbelow, are employed to reduce the amount of computation.
- the detector 18 may be applied in a cascade of sequential stages of partial evaluation, where each stage performs a partial evaluation of the classifier.
- Each of these stages of evaluation can be considered a sub-classifier.
- Each such sub-classifier (in equation (2) below) forms a representation of the log-likelihood ratio given hereinbefore by equation (1):
- the set of image window 32 locations to be evaluated by each classifier stage may be determined at block 84 .
- the set of image window 32 locations to be evaluated can initially be a set of all possible image window 32 locations.
- the criterion for updating the set of image window 32 locations to be evaluated may be the current total log-likelihood ratio corresponding to each location. Locations with a current total-log-likelihood ratio below a predetermined threshold may be dropped from the set of image window 32 locations to be evaluated. Thus the number of candidate locations to be evaluated may be reduced at each successive iteration of this process.
- a cascade evaluation strategy using equations (2) and (3) can be a many step process, where a partial evaluation of equation (3) can be done incrementally.
- the classifier may compute a partial sum of terms in equation (3). After the computation of the k th sub-classifier, this sum would be given by:
- the classifier may apply a threshold, ⁇ k , to the partial sum in equation (4) and remove additional candidates (i.e. parts of the image being searched) from further consideration as shown and discussed hereinbelow.
- ⁇ k the threshold
- the threshold for the log-likelihood at each stage of classification may be pre-determined by a process of evaluating the current stages of the classifier on the cross-validation images (block 165 in FIG. 17B discussed later hereinbelow).
- Block 165 may set this threshold to match a specified performance metric such as, for example, correct detection of 95% of the pre-labeled instances of the object (e.g., a human face, or a car, etc.).
- the object detector 18 may apply the classifier 34 repeatedly to original image 16 at regularly spaced (and, usually overlapping) positions of this rectangular image window 32 and iteratively re-apply the classifier in the same fashion to each resized image 62 and 64 .
- the step size, ⁇ , for moving this window in position ( FIG. 8B ) and the step size, ⁇ , for scaling the window in size ( FIG. 8C ) may influence the speed of this process.
- the original image 16 and its scaled version 16 ′ obtained by scaling the original image 16 by the step size “ ⁇ ”) are illustrated in FIG. 8C . Bigger step sizes ( ⁇ , ⁇ ) may reduce the amount of times the classifier must be evaluated and reduce overall computation time.
- FIGS. 4A , 4 B the classifier 34 ( FIGS. 4A , 4 B) must be able to accurately classify the presence of the object over a greater range of variation in the size and position of the object as illustrated in FIG. 8D , where variations in object position (step size “ ⁇ ”) and scale (step size “ ⁇ ”) are illustrated for a given object image window 32 of 24 ⁇ 32 pixels.
- the early stages of evaluation differ from the later stages in the positional spacing (“ ⁇ ” in FIG. 8B ) of the evaluation sites.
- Early stages may use a coarse spacing, for example, 4 pixels in both the horizontal and vertical dimensions, between evaluation sites.
- the later stages may use a finer spacing, e.g., spacing of 2 pixels, and the last stages may use a spacing of 1 pixel.
- This evaluation strategy could be thought of as a “coarse-to-fine” strategy whereby the early stages evaluate the image at coarse resolution and the later stages evaluate it at a finer resolution.
- the early stages can quickly remove much of the image from consideration and do so quickly by evaluating the image at a coarse spacing.
- the later stages can then evaluate the remaining sites by using a more discriminative classifier where it does not represent as much invariance in object position (see, for example, the finer evaluation illustrated in FIG. 8D ).
- an evaluation strategy may use a multi-resolution representation of the input image based on a wavelet transform to achieve a coarse-to-fine search method.
- the input variables are wavelet coefficients, generated by applying 5/3 or 4/4 symmetric filter banks to the input pixels.
- the coefficients of a symmetric 4/4 filter bank are (1.0, 3.0, 3.0, 3.0) for the low-pass filter and ( ⁇ 1.0, ⁇ 3.0, 3.0, 1.0) for the high pass filter.
- the coefficients of 5/3 filter bank are ( ⁇ 1.0, 2.0, 6.0, 2.0, ⁇ 1.0) for the low pass filter and (2.0, ⁇ 4.0, 2.0) for the high-pass filter.
- Filter-bank implementation for any filter-pair is discussed in G. Strang and T. Nguyen, “Wavelets and Filter Banks”, Wellesley-Cambridge Press, 1997, the disclosure of which is incorporated herein by reference.
- FIG. 10 shows an exemplary set of subbands 60 A- 60 D and 62 A- 62 C produced by a wavelet transform using a 2 (two) level decomposition of an input image.
- the wavelet transform organizes the input image into subbands that are localized in orientation and frequency.
- a 4/4 filter-bank (not shown) may produce 7 (seven) subbands—four subbands 60 A- 60 D at level- 2 and three subbands 62 A- 62 C at level- 1 .
- each wavelet transform coefficient 66 FIG. 11
- LH in terms of orientation
- HL may denote high-pass filtering in the horizontal direction and low-pass filtering in the vertical direction, i.e., representation of horizontal features, such as, for example, human lips, eyelids, etc.
- HL may denote high-pass filtering in the horizontal direction and low-pass filtering in the vertical direction, i.e., representation of vertical features, such as, for example, a human nose.
- vertical subbands (LH) may be considered as representing horizontal features of an object
- HL horizontal subbands
- HH may denote high-pass filtering in both horizontal and vertical directions
- LL may denote low-pass filtering in both horizontal and vertical directions.
- FIG. 11 depicts an input window 32 and its two level wavelet transform representation 64 generated by a symmetric 4/4 filter bank (not shown). Seven subbands (similar to those shown in FIG. 10 ) are visible on the wavelet decomposition 64 .
- the wavelet transform coefficients 66 are also shown in FIG. 11 .
- the coefficients 66 are spatially localized within appropriate subbands. It is noted that each level in a wavelet transform 64 represents a lower octave of frequencies.
- a coefficient in level- 2 i.e., subbands 60 A- 60 D in FIG. 10
- describes 4 (four) times the area of a coefficient in level- 1 i.e., subbands 62 A- 62 C in FIG. 10 ).
- the wavelet representation 64 shows a gradation of details (of the input image) with level- 2 coefficients representing coarse details and level- 1 coefficients representing finer details.
- a wavelet transform is generated by a polyphase filter bank (not shown) or other suitable computational method (e.g., “lifting”) applied to the input image 16 and each scaled version of the input image (e.g., versions 62 and 64 shown in FIG. 8A ), as described in the Strang and Nguyen article mentioned above.
- each wavelet trans form may consist of two (2) or more levels (see, for example. FIG. 10 for a two level representation).
- the wavelet transform representation may be generated by the process shown in FIG. 12A .
- an input image 16 (or its scaled version) is first transformed to a level- 1 transform.
- the LL subband of level- 1 is then expanded to generate the level- 2 .
- each level is generated from the LL subband of the level that preceded it.
- This embodiment of FIG. 12A is referred to hereinbelow as a “critically sampled” wavelet transform.
- a fully overcomplete wavelet transform is a redundant version of the ordinary (critically sampled) wavelet transform.
- four (4) redundant “phases” are generated at each stage in a process illustrated in FIG. 12C as compared with one phase in the critically sampled case of FIG. 12A .
- the odd-odd phase of level- 1 corresponds to the wavelet transform of the input image shifted by one pixel in both the horizontal and vertical directions. If this transform in FIG.
- any branch (or path) starting at the input image 16 and terminating in level- 2 could be thought of as a redundant wavelet transform.
- the combination of level- 1 OO and the level- 2 OO (generated from level- 1 OO) would be one redundant wavelet transform corresponding to the critically sampled wavelet transform of the input shifted by 3 pixels in both directions.
- a critically-sampled two level wavelet transform ( FIG. 12A ) is shift-invariant only in shifts that are multiples of “4”.
- a fully overcomplete wavelet transform ( FIG. 12C ) is shift invariant for shifts that are a multiple of “1”.
- An intermediate representation that may be referred to as a “partially overcomplete” wavelet transform of the scaled input image computes a representation that is shift-invariant in multiples of “2”.
- this representation computes the EE phase (critically sampled and not overcomplete generation of level- 1 ) at level- 1 and then generates an overcomplete representation of level- 2 from the level- 1 EE phase producing level- 2 EE, level- 2 EO, level- 2 OE, and level- 2 OO phases as shown in FIG. 12B .
- FIG. 12B A partially overcomplete ( FIG. 12B ) and a critically sampled ( FIG. 12A ) transforms are contained within a fully overcomplete transform ( FIG. 12C ).
- the critically sampled transform is given by the left most branch consisting of level- 1 EE and level- 2 EE phases.
- the partially overcomplete transform has 4 redundant phases corresponding to the 4 left most branches given by (level- 1 EE, level- 2 EE), (level- 1 EE, level- 2 OE), (level- 1 EE, level- 2 EO), (level- 1 EE, level- 2 OO).
- the classifier should preferably be evaluated with respect to fully overcomplete wavelet transform during detection. This amount of shift invariance corresponds to evaluation sites that are spaced by increments of 1 pixels apart in both the horizontal and vertical directions. Likewise, a classifier with a shift-invariance of 2 pixels should preferably be evaluated with respect to a partially overcomplete or fully complete wavelet transform. This amount of shift-invariance corresponds to spacing the evaluation sites 2 pixels apart. A classifier with shift-invariance of 4 pixels can be evaluated with respect to critically-sampled, partially overcomplete, or fully overcomplete transform.
- each phase of a partially or fully overcomplete transform is evaluated separately as if it were a separate image.
- the set of windows to be evaluated for all phases is determined at block 74 (by, for example, a process of comparing partial evaluation of equation-3 to a threshold ⁇ k as discussed hereinbefore) and then the input windows are evaluated for each of these phases sequentially (block 84 ). For example, for the partially overcomplete expansion, block 84 in FIG. 9 is repeated for the 4 phases, whereas for the fully overcomplete transform, block 84 in FIG. 9 is repeated for 16 phases.
- the first two stages use a critically sampled wavelet transform and a classifier designed with shift invariance of 4 pixels and the last three stages use a partially overcomplete wavelet transform and a classifier designed with a shift-invariance of 2 pixels.
- This evaluation strategy could be thought of as a “coarse-to-fine” strategy whereby the early stages evaluate the image at coarse resolution and the later stages evaluate it at a finer resolution.
- a 5 th stage may use a fully overcomplete wavelet transform and a classifier designed with a shift-invariance of 1 giving a more pronounced progression from coarse to fine
- the image window 32 may not directly sample the scaled image as implied by FIG. 4A , but rather it may sample wavelet coefficients generated from the input image (e.g., at block 74 in FIG. 9 ).
- the window 32 may select coefficients from one wavelet transform derived from the overcomplete (or semi-overcomplete) wavelet transform.
- the image window 32 may select the coefficients within this transform that spatially correspond to the position of the image window within the image or its scaled version 16 ′ as shown in FIG. 13A .
- one image window corresponds to a collection of blocks of wavelet coefficients from each wavelet subband. This choosing of coefficients is equivalent to first selecting the window in the original image 16 (or its scaled version 16 ′) and then taking the wavelet transform of the image window.
- Evaluation sites may be specified by the center of the window 32 with respect to the chosen wavelet transform's second level.
- Each coefficient in the level- 2 LL, band corresponds to a center of a potential evaluation site as shown in FIG. 13A .
- the set of all possible evaluation sites for this phase of the wavelet transform corresponds to the set of windows placed at each such wavelet coefficient. It is noted that such image window 32 locations may be partially overlapping as shown in FIG. 13B .
- the object defector program 18 may evaluate a single stage, h i , for each member of a set of image window 32 locations to be evaluated.
- the object detector 18 can keep, for each of the set of image window 32 locations to be evaluated, a partial calculation of equation (4) that may be referred to as a total log-likelihood. It is noted that each term of equation (4) may correspond to the log-likelihood generated by the application of a sub-classifier to an image-window location.
- the partial calculation of equation (4), or total log-likelihood contains terms resulting from sub-classifiers already applied to the location.
- the system assigns log-likelihood to each evaluation site where each site corresponds to a window centered at a coefficient location in level 2 of the wavelet transform. Since evaluation may be repeated over different phases of the wavelet transform (i.e., small positional shifts), these evaluations must be combined (summed) and resolved with respect to a common coordinate frame at a higher resolution. In one embodiment, these log-likelihoods are resolved by re-expressing them in a representation at the resolution of the original image.
- Block 76 in FIG. 9 denotes propagation of the log-likelihood from level 2 of the wavelet transform to the resolution of the original image. Note that level 2 of the wavelet transform has 1 ⁇ 4th the resolution of the original image.
- FIGS. 14A-14E illustrate propagation for different amounts of shift invariance and different phases in one dimension of wavelet transform. For example, shift invariance of 4 leads to a propagation of 4 sites in the original image as shown in the diagrams 210 and 212 in FIGS.
- shift invariance of 2 leads to propagation of 2 sites as illustrated by diagrams 214 and 216 in FIGS. 14C and 14D , respectively; and shift-invariance of 1 leads to propagation of one site as shown in the diagram 218 in FIG. 14E .
- differences in phase correspond to positional offsets of the propagation as shown by the difference between FIG. 14A and FIG. 14B , and the difference between FIGS. 14C and 14D .
- Block 72 in FIG. 9 initializes the log likelihood for the current phase of the current stage to zero for all candidate object locations for each process iteration at block 78 .
- the process in FIG. 16 may increment the stage pointer “i” (in ⁇ i ) to evaluate other stages for each member of the set of image window locations as illustrated by the process loop including blocks 78 , 82 , and 84 in FIG. 9 .
- the input image 16 is repeatedly reduced in size (blocks 86 , 88 in FIG. 9 ) by a factor “f” given by:
- Extensive object search is continued for each scale until the scaled input image is smaller than the size of the image window 32 .
- the object detector 18 “re-uses” parts of the transform in the search across scale. FIGS. 18 and 19 illustrate this process.
- each level of the transform is recomputed in its entirety at each scaling.
- the object detector 18 can “reuse” parts of the transform computed at previous scalings as illustrated in FIG. 15 .
- the transform for scale (i- 4 ) may be utilized because the transform for scale-i (i ⁇ 4) is related to the transform at scale (i- 4 ) by a shift in levels.
- the wavelet transform at scale- 4 is related to that at scale- 0
- the transform at scale- 5 is related to that at scale- 1 , and so on as illustrated in FIG. 15 in case of scales 0 and 4 .
- the object detector 18 shifts the transform for scale (i- 4 ) by one level. That is, level- 2 at scale (i- 4 ) becomes level- 1 at scale-i (for i ⁇ 4) as shown in FIG. 15 with reference to scales 0 and 4 . Therefore, the object detector 18 needs to compute only the top level (i.e., level- 2 ) for each scale-i (i ⁇ 4), rather than computing the entire transform (i.e., levels 1 through 2 ) for the new scale-i.
- Blocks 126 and 128 in FIG. 16 illustrate the scaling and partial re-use of the transform discussed hereinabove. As shown by the feed-back arrow at block 128 in FIG.
- the transform re-use and extensive object search (as given by blocks 120 , 122 , 124 , and 130 in FIG. 16 ) is continued for each scale until the scaled input image is smaller than the size of the image window 32 as also discussed hereinbefore with reference to FIG. 9 .
- the object detector 18 may search the object at scales (i+k*4, where k ⁇ 1).
- Such an extensive scanning for each image size and image window location improves accuracy of detection by making it possible to find instances of the object over this range in position and size.
- the wavelet transform can be re-computed from scratch for each scaled version of the image.
- the object detector program 18 may not give one single large response (for the left side of equation-1) at one location that is greater than the threshold “ ⁇ ” (i.e., the right side of equation-1). It may instead give a number of large responses at adjacent locations all corresponding to the same face that all exceed the detection threshold.
- multiple view-based classifiers may detect the same object at the same location. For example, FIG. 5 shows a case where the same face (the female in the foreground of the photograph) is initially detected by two of the view-based classifiers (the face detectors #2 and #3 in FIG. 5 ).
- the following approach-which may be called “arbitration” may be used.
- the detection value i.e., the left side of equation-1 or equivalently equation-3
- a list composed of all the locations for which the detection value is greater than the detection threshold is formed. This list is then put in order from the detection of the greatest magnitude (i.e., for which the value of the left side of equation-1 is largest) to that of the smallest magnitude.
- the first entry in this list is declared a real detection. Then this entry may be compared to all others in the list.
- the distance may be half the radius of the image window because as the image window size is fixed, its radius is also fixed
- this process repeats for the entry in the remaining list that has the largest magnitude (after the first entry and all entries matching to it have been removed); that is, this second entry is now declared to be another real detection and is compared to all other entries in the remaining list whereby any entries that are close in distance to it are discarded. This process continues until the list is exhausted.
- the object detector according to the present disclosure may provide computational advantages over the existing state of the art. In particular, it is observed that although it may take many sub-classifier evaluations to confirm the presence of the object, it can often take only a few evaluations to confirm that an object is not present. It is therefore wasteful to defer a detection decision until all the sub-classifiers have been evaluated. According to one embodiment, the object detector 18 may thus discard non-object candidates after as few sub-classifier evaluations as possible.
- the coarse-to-fine strategy implemented by the object detector 18 involves a sequential evaluation whereby after each sub-classifier evaluation, the object detector makes a decision about whether to conduct further evaluations or to decide that the object is not present.
- This strategy may be applied to the multi-resolution representation provided by the wavelet transform whereby the sub-classifier evaluations are ordered from low-resolution, computationally quick features to high-resolution, computationally intensive features.
- the object detector may efficiently rule out large regions first and thereby it only has to use the more computationally intensive sub-classifiers on a much smaller number of candidates.
- Each sub-classifier (equation (2)) can be viewed as a representation of the ratio of the full joint distributions, P(image-window
- the image window 32 may encompass several hundreds or even thousands of pixel variables. In that case, it may not be computationally feasible to represent the joint distribution of such a large number of variables without reducing the complexity of this distribution.
- Bayesian networks may also be suitably employed by one skilled in the art to construct sub-classifiers as per the methodology discussed in the present disclosure.
- each probability distribution in h i is represented by a Bayesian Network.
- a Bayesian network structure is a directed acyclic graph whose nodes represent random variables as described in more detail in Richard E. Neapolitan, “Learning Bayesian Networks”, published by Pearson Prentice Hall (2004), the discussion of which is incorporated herein by reference in its entirety.
- a Bayesian network may take the following form: P (image_window
- ⁇ ) P ( x 1 , . . . , x r
- ⁇ ) P ( x 1
- the image-window 32 consists of the variables (consisting of pixels or coefficients generated by a transformation (e.g., a wavelet transform) on the pixels) ⁇ x 1 . . . x r ⁇ and where pa xi denotes the parents of x i in the graph, which are a subset of the variables.
- pa x1 may be ⁇ X 5 , x 9 , x 12 ⁇ .
- the classifier 34 can be represented as the ratio of two Bayesian network graphs (one representing P(image_window
- the conditional probability distributions forming each Bayesian Network can take many functional forms. For example, they could be Gaussian models, mixture models, kernel-based non-parametric representations, etc. Also, each conditional probability may have to be of the same functional form. As described in more detail hereinafter, in one embodiment, each probability distribution in equation (6) is represented by a table.
- a challenge in building a Bayesian network representation is that learning the connectivity of a Bayesian network is known to be NP complete; that is, the only guaranteed optimal solution is to construct classifiers over every possible network connectivity and explicitly compare their performance. Moreover, the number of possible connectivities is super-exponential in the number of variables. Heuristic search may be the only possible means of finding a solution. In one embodiment, the selection of the Bayesian Network connectivity is achieved by the statistical methods as described hereinbelow.
- FIG. 17A and FIG. 17B show a block diagram of a system 142 for automatically creating a Bayesian Network-based classifier for one embodiment of the present disclosure.
- Various blocks in FIG. 17A are illustrated in more detail in FIG. 17B with their corresponding constituent sub-blocks.
- the system 142 can use statistical methods to choose the Bayesian Network (BN) composing each sub-classifier stage of the object detector program.
- the system 142 may be implemented using a computing device such as, for example, a personal computer, a laptop computer, a workstation, a minicomputer, a mainframe, a handheld computer, a small computer device, or a supercomputer, depending upon the application requirements.
- the system 142 may include training images for the object class 150 , training images for the non-object class 154 , and cross-validation images (testing images in which the locations of the object are labeled) 152 .
- a method to prepare training images of an object is illustrated in FIG. 18 and discussed later hereinbelow.
- the system modules 144 , 146 , 148 may be implemented as software code to be executed by a processor (not shown) of the system 142 using any suitable computer language such as, for example, Java, Ada, C or C++ using, for example, conventional or object-oriented techniques.
- the software code may be stored as a series of instructions or commands on a computer-readable medium, such as a random access memory (RAM), a read only memory (ROM), a magnetic medium such as a hard-drive or a floppy disk, or an optical medium such as a CD-ROM.
- a computer-readable medium such as a random access memory (RAM), a read only memory (ROM), a magnetic medium such as a hard-drive or a floppy disk, or an optical medium such as a CD-ROM.
- the candidate subset creation module 144 may form a set of subsets (not shown), where each subset is chosen to represent a group of input variables that depend upon each other. These subsets may not be mutually exclusive. Thus, a variable can be a member of multiple subsets.
- the subset creation module 144 may determine the grouping into subsets by optimizing over “local” error in the log likelihood ratio function. This function assumes that every pair of variables is independent from the remaining variables. In particular, the ratio function may organize the variables into a large number of “candidate” subsets such that the “local” error measure is minimized as discussed later hereinbelow in conjunction with equation (11) and discussion of FIG. 20 . This organization may restrict the final network to representing dependencies that occur only within subsets. The module 144 computes this error function using labeled object and non-object training images as inputs.
- the candidate subset selection module 146 in FIGS. 17A-17B chooses a small set of the candidate subsets created by block 144 .
- the module 146 makes this choice by comparing “restricted Bayesian network” structures formed from combinations of the candidate subsets.
- the restricted network may consist of two layers of nodes. Each node in the second layer corresponds to a subset and consists of the variables from this subset that do not occur in any other subsets. Any variables that co-occur among subsets become nodes in the top layer. Each such variable becomes a separate node and is parent to multiple second layer nodes representing all its subset memberships.
- P ⁇ ( X 1 , ... ⁇ , X n ) P ⁇ ( S 1 ) ⁇ P ⁇ ( S 2 ) ⁇ ⁇ ... ⁇ ⁇ P ⁇ ( S r ) [ P ⁇ ( X 1 ) ] a 1 ⁇ [ P ⁇ ( X 2 ) ] a 2 ⁇ ⁇ ... ⁇ [ P ⁇ ( X n ) ] a n ( 7 )
- the variables X 1 , X 2 , etc. may be either pixels or transform coefficients. In one embodiment, they are wavelet transform coefficients.
- the structure in equation-7 assumes that the parents of any node in the second layer are statistically independent where S 1 . . .
- S r are some collection of subsets of the input variables generated by the subset creation module 144 .
- Each ⁇ k corresponds to the number of occurrences of the given variable in the r subsets. For example, if variable X k occurs in 3 subsets, then ⁇ k would equal 2 (to represent the occurrence of X k in 2 additional subsets).
- the denominator in equation (7) could be thought of as a term that corrects for “over-counting”; that is, accounting for the occurrence of a variable in more than one subset.
- the subset selection module 146 may search in this space of restricted networks and choose one (of the restricted networks) that minimizes global classification error as measured by the area under the receiver operating characteristic (ROC) curve.
- the restricted network is estimated using labeled examples of the object ( 150 ) and non-object ( 154 ).
- Global classification error is computed with respect both to the training images ( 150 , 154 ) and independent cross-validation images ( 152 ). It is observed here that all sets of images (either at block 150 , or 152 , or 154 ) are labeled.
- the Bayesian Network creation module 148 removes the independence assumption from the chosen restricted network and find a full Bayesian Network that entails all the direct dependence and independence relationships implied by the chosen set of subsets. In one embodiment, this full Bayesian network is constructed using a heuristic search designed to minimize the dimensionality of the resulting conditional probability distributions.
- the Bayesian Network creation module 148 may estimate the conditional probability distributions forming each Bayesian network from the labeled training images of the object and non-object.
- the construction of a full BN (given by blocks 144 , 146 , and 148 in FIG. 17A ) may continue for each stage of the classifier 34 as indicated by the process loop at block 156 .
- the object training images 150 are representative example images of the object (e.g., human faces or cars) that the object detector 18 is built to detect.
- FIG. 18 describes an embodiment of how these examples are generated.
- For each face viewpoint about 2,000 original images are used; and for each car viewpoint, between 300 and 500 original images are used.
- Each of these original training images can be sized, aligned, and rotated to give the best correspondence with a prototype representing the image class (e.g., frontal faces) as indicated at block 104 in FIG. 18 .
- the size of each such training image may be identical, to the size of the classification window 32 .
- each original training image approximately 400 synthetic variations of the image are generated (block 106 ) by altering background scenery and making small changes in aspect ratio, orientation, frequency content, and position using a random number generator. Then, at block 108 , the wavelet transform of each such example (including the original image or its synthetic variation) is computed and, in block 109 , a lighting correction is applied which will be discussed later hereinbelow. It is noted that increasing the number of original images and synthetic variations may increase the computational time required for the modules in the system 142 of FIGS. 17A-17B , but may also increase the accuracy of the resulting classifier.
- the number of original training images and the number of synthetic variations of each original image may be determined by the desired accuracy of detection, the availability of suitable training images, and limitations on the amount of time and computer resources that can be devoted to the computations among the modules in the system 142 in FIGS. 17A-17B .
- the object training images are re-used for each stage of classifier.
- the synthetic variations of these may be different in that different random numbers may be used.
- Non-object examples 154 may be taken from images that do not contain the object.
- some non-object examples may include backgrounds in a public place, office architecture, etc. In one embodiment, approximately 600,000 non-object examples are used. The selection of non-object examples is described in more detail hereinafter.
- each classifier stage may use a different set of training examples for the non-object class. It may be desirable to choose “non-object” examples that are most likely to be mistaken for the object to be detected (e.g., a human face or a car) as discussed in B. D. Ripley, Pattern Recognition and Neural Networks , Cambridge University Press, 1996. This concept is similar to the way support vector machines work by selecting samples near the decision boundary as discussed in V. N. Vapnik, The Nature of Statistical Learning Theory , Sprinter, 1995. The relevant disclosures of both of these publications are incorporated herein by reference in their entireties.
- non-object training examples ( 154 ) may be acquired at block 167 in FIG. 17B by a bootstrapping method designed to determine such samples (i.e., non-object samples that are most likely to be mistaken for the object).
- Bootstrapping requires that the overall classifier is trained as a sequence of classifiers, where each stage is a full classifier (or “sub-classifier”) (and the overall classifier is a sum of these classifiers given by equation-3).
- Bootstrapping works in the following way illustrated in FIG. 19 .
- 17A may be trained by estimating P(Image_window
- This classifier is then run over a set of images (for example, 2,500 images) that do not contain the object (block 176 , FIG. 19 ).
- Block 178 in FIG. 19 selects the image windows ( 32 ) within that collection of images which give high log-likelihood (false detections or near false detections). These image windows will then be ones that “resemble” the object in the sense of stage- 1 of the classifier.
- the wavelet transform of each training example is computed (block 108 in FIG. 18 ) prior to compiling the set of object training images (block 150 in FIGS. 15 and 17A ).
- the sampling of the wavelet transform depends on the shift-invariance desired as described hereinbefore. For a shift-invariance of 4, the fully overcomplete transform may be computed; for a shift-invariance of 2, the semi-overcomplete transform may be computed; and for a shift-invariance of 1, the critically-sampled transform may be computed.
- the fully overcomplete transform may effectively give 16 synthetic variations of the original training example where the variations correspond to positional shifts of the input as described hereinabove.
- Each of these 16 variants may each be treated henceforth as separate training examples and the wavelet coefficients forming each are the variables which may be decomposed into subsets. Similarly, the semi-overcomplete transform gives 4 synthetic variations, and the critically sampled transform gives one synthetic variation.
- the cross-validation images 152 are images of realistic scenes that often contain the object.
- the locations of the object in these scenes are known (usually entered by hand) and used to select coefficient subsets (block 161 , FIG. 17B ), determine the number of adaboost iterations (block 165 , FIG. 17B ), determine the detection threshold (block 161 , FIG. 17B ), and the weight of the stage (block 161 , FIG. 17B ) as described hereinbelow.
- the candidate subset creation module 144 may create a set of candidate subsets of input variables.
- FIG. 20 is a flowchart illustrating a process flow for the candidate coefficient-subset creation module 144 according to one embodiment of the present disclosure.
- the input variables to the module 144 may be pixels or variables derived from a transformation of the input pixels such as wavelet coefficients as described hereinbelow.
- the subset creation module 144 may form the subsets by considering two types of modeling error: not modeling the dependency between, two variables or not modeling a variable altogether. The cost of these modeling errors may be evaluated in terms of their impact on the log-likelihood ratio function.
- these errors are evaluated only over pairs of input variables, (X i , X j ) by assuming that each pair, (X i , X j ), is independent from the remaining input variables.
- each error is the difference between the true log-likelihood ratio, i.e., log (P(X i , X j
- An exemplary set of three possible cases may be considered with the following costs:
- the “abs[ ]” denotes an operator that gives absolute value of the parameter supplied as its operand.
- the error measures C 1 , C 2 , and C 3 may be obtained by empirically estimating the probability distributions, P(X i , X j
- each of the random variables is assumed to be discrete-valued.
- upper case notation is used to denote the random variable and lower case notation is used to denote a particular instantiation of the variable; that is, each sum is over all possible values of the given random variable.
- E I ⁇ ( G ) ⁇ ( X i , X j ) , ⁇ i , j , i ⁇ j X i
- ⁇ 2 ), may preferably be represented over pairs of variables, (x i ,x j ).
- the estimate_coefficient_mean_variance_object module (block 184 , FIG. 20 ) estimates the mean and variance of each coefficient, x i , over training examples of the object class, ⁇ 1 .
- the estimate_coefficient_pair_probability_object module 186 may use training examples from each object class, ⁇ 1 .
- the estimate_coefficient_pair_probability_non_object module 188 may use training examples from the object class, ⁇ 2 .
- each variable is quantized to five levels, where the quantization levels are chosen as a function of the mean ( ⁇ ) and standard deviation ( ⁇ ) of the variable with respect to the ⁇ 1 class with thresholds at ⁇ 0.7 ⁇ , ⁇ 0.15 ⁇ , ⁇ +0.15 ⁇ , ⁇ +0.7 ⁇ .
- Each joint distribution is represented by a histogram with 25 bins and each distribution of one variable is represented by a histogram of 5 bins. A detailed discussion of histograms is given hereinbelow with reference to FIG. 24 .
- the select_best_groups_with_each_coefficient module (block 190 , FIG. 20 ) assigns the variables to n subsets using n greedy searches, where each input variable, X i , is a seed for one search.
- n greedy searches may be found in Stuart J. Russell, Peter Norvig, “Artificial Intelligence: A Modern Approach” (second edition), Prentice-Hall, 2003, and the relevant portion of this book discussing a “greedy search” is incorporated herein by reference.
- the greedy searches at block 190 may guarantee that every variable is initially represented in at least one subset and, therefore, there are no errors of the form C 2 or C 3 .
- Each of these greedy searches may add new variables by choosing the one that has the largest sum of C 1 values formed by its pairing with all current members of the subset. Such a selection process may guarantee that the variables within any subset will have strong statistical dependency with each other. This may be a fairly reasonable way to initially optimize the error function of equation (11) because the errors due to removing a variable tend to be greater than those of removing a dependency.
- the select_final_candidate_subsets module 192 reduces the number of subsets to a smaller collection.
- the search performed by module 192 sequentially removes subsets until some desirable number, q, are remaining. At each step the search removes the subset that will lead to the smallest increase in modeling error. In particular, it follows from equation (11) that the error in removing a given subset, S k , is:
- the number of selected candidate subsets, q was 200.
- computational cost is linear in the number of candidate subsets and, therefore, does not prohibit consideration of a larger number of subsets, e.g., 1,000.
- Subset size is a flexible parameter that can be specified manually. Larger subsets have the potential to capture greater dependency. Larger subsets, however lead to greater dimensionality of the conditional probability distributions within the Bayesian network, and, therefore, size must be balanced against practical limits in representational power and limited training data.
- multiple subset sizes of 4, 8, and 16 are used where an equal number of subsets of each size are chosen by the process described hereinabove.
- a set of candidate subsets are selected (block 146 in FIG.
- FIG. 21 illustrates a detailed view of the sub-processes or sub-modules ( 158 , and 194 - 198 ) that are part of the candidate subset selection module 146 in FIGS. 17A-17B .
- equation (7) requires representation of probability distributions over all the candidate subsets: P(S 1
- the create_candidate_features_from_subsets module 158 ( FIGS.
- ⁇ i (S i ) forms a representation of each subset by a function, ⁇ i (S i ), that maps values for each subset (S i ) of variables to a discrete-valued variable.
- Computational considerations may govern the complexity of the feature functions, ⁇ i (S i ).
- the feature functions may be designed to be computationally efficient and restricted to a relatively small range of discrete values (for example, 10 2 values).
- the feature functions may be chosen to more discriminative and spread over a larger set of values (for example, 10 4 ).
- the early stages use features based on linear projection of the subset's wavelet coefficients and the later stages use features that do not use linear projection. It is noted here that, for the sake of convenience, the term “feature” is used herein interchangeably with the term “feature function.”
- the linear projection vectors, v k may be determined by several methods: principal components vectors computed from the object training set, principal components computed over the non-object training set, principal components computed over the combined object and non-object training sets. It is noted that one skilled in the art of statistics and linear algebra would recognize the process of principal component analysis or the Foley-Sammon discriminant vectors (multidimensional extension of Fisher Linear discriminant) between, both classes as discussed in Foley, D. H. and Sammon, J. W., An Optimal Set of Discriminant Vectors , IEEE Transactions Computers (1975), vol. C-24, pp. 281-289, the disclosure of which is incorporated herein by reference in its entirety. In one embodiment, the various subbands (see, e.g., FIG.
- scalar constants prior to this computation (principal component analysis or the Foley-Sammon discriminate vectors determination) and correspondingly prior to projection on these linear vectors.
- all 5/3 filter bank coefficients in all level- 1 subbands are multiplied by 0.25 and all coefficients in all level- 2 subbands are multiplied by 0.0625.
- all 4/4 filter bank coefficients in level- 1 are multiplied by 0.0156 and all coefficients in level- 2 are multiplied by 0.00024414.
- Each subset of projection coefficients can be represented by one discrete value that takes on a finite range of values.
- the one discrete value may be referred to herein as the “quantized feature value”, which is represented herein by the variable, f.
- This transformation is achieved by quantization of the projection coefficients.
- quantization thresholds may be determined by the following procedure.
- each variable is first separately quantized.
- the quantization boundaries may be set in terms of the mean ( ⁇ ) and standard deviation ( ⁇ ) of the variable computed over the object training images. For example, a variable could be quantized to 5 values with the following quantization boundaries: d ⁇ ⁇ d ⁇ 0.5 ⁇ ⁇ 0.5 ⁇ d ⁇ + 0.5 ⁇ ⁇ +0.5 ⁇ d ⁇ + ⁇ ⁇ + ⁇ d (14)
- the quantized feature value, f can then be uniquely computed from this conglomerate of the quantized projection coefficient values.
- the projection coefficients can be quantized by a form of vector quantization.
- vector quantization #2 modifies vector quantization #1 by considering up to 3 values whose magnitude exceeds some pre-specified threshold.
- this threshold is chosen as twice the mean of the coefficients corresponding to the top 3 projection vectors (in the case of projection vectors derived from principal components analysis) computed from the object training images.
- “f” takes on 571 values given by 480 values (if three coefficients exceed the threshold) plus 80 values (if two coefficients exceed the threshold) plus 10 values (if one coefficient value exceeds the threshold) plus 1 value (if no coefficient values exceed the threshold).
- vector quantization #3 is a modification of vector quantization #2.
- the vector quantization #3 method quantizes the coefficient due to the first principal component separately. In one embodiment, it quantizes this coefficient into 5 levels where the thresholds for these levels are given by: d ⁇ ⁇ d ⁇ 0.5 ⁇ ⁇ 0.5 ⁇ d ⁇ + 0.5 ⁇ ⁇ +0.5 ⁇ d ⁇ + ⁇ ⁇ + ⁇ d (16)
- This method then applies the vector quantization scheme #2 by ordering the top 3 of the 4 remaining coefficients, resulting in 249 possible values. In this method, “f” overall then has 1245 possible values corresponding to the product of these 249 values with 5 possible values for the quantized first coefficient.
- vector quantization #4 is also a modification of vector quantization #2. This method initially applies vector quantization #2. Then it applies a second threshold to the ordered coefficients. In one embodiment, this second threshold is four times the mean of the coefficients corresponding to the top 3 projection vectors (in the case of projection vectors derived from principal components analysis) computed from the object training images. The vector quantization #4 method then counts the number of coefficients that exceed this second threshold. This number can range from 0 to 3. Therefore, in this method, “f” has four times as many possible values as it does for vector quantization #2.
- N is the number of wavelet coefficients in a subset and Q is the number of projection vectors.
- Each “p j ” is a projection coefficient given by equation-13 and each “w i ” is a wavelet coefficient.
- the value of “g” in equation (17) can be quantized to 4 levels.
- the quantization thresholds may be 0.5g ave , g ave , and 2.0 g ave , where g ave is the average value of g computed over the object training image set. Combining this measurement with any other quantized measurement increases the total number of quantization bins by a factor of 4.
- Q is the number of projection vectors.
- the variable “h” in equation (18) is quantized to 4 levels.
- the quantization thresholds are 0.5h ave , 2.0h ave , and 4.0h ave , where h ave is the average value of h computed over the object training image set. Combining this measurement with any other quantized measurement increases the total number of quantization bins by a factor of 4.
- Some additional examples of various other quantization methods include the following: (1) A “scalar quantization 1 -A” method that combines scalar quantization #1 with the energy of the projection vectors measurement. (2) A “vector quantization 1 -A” method that combines vector quantization #1 with the energy of the projection vectors measurement. (3) A “vector quantization 2 -A” method that combines vector quantization #2 with the energy of the projection vectors measurement. (4) A “vector quantization 3 -A” method that combines vector quantization. #3 with the energy of the projection vectors measurement. (5) A “vector quantization 4 -A” method that combines vector quantization #4 with the energy orthogonal of the projection vectors measurement.
- a “scalar quantization 1 -B” method combines scalar quantization #1 with the energy orthogonal to the projection vectors measurement.
- a “vector quantization 1 -B” method that combines vector quantization #1 with the energy orthogonal to the projection vectors measurement.
- a “vector quantization 2 -B” method that combines vector quantization #2 with the energy orthogonal to the projection vectors measurement.
- a “vector quantization 4 -B” method that combines vector quantization #4 with the energy orthogonal to the projection vectors measurement.
- candidate subsets are represented by a form of vector quantization on each entire subset giving a function ⁇ i (S i ).
- these functions, ⁇ i (S i ) are learned from the combined object training examples 150 and non-object training examples 154 using tree-structured vector quantization (TSVQ) over the entire set of training examples.
- TSVQ tree-structured vector quantization
- FIG. 22 An example tree 220 is shown in FIG. 22 , which is adapted from FIG. 9.1 in the Gersho and Gray article mentioned above.
- Each node, 222 A through 222 M, in the tree 220 is associated with a “test-vector”.
- a method forms each quantization tree by repeatedly splitting the training data (the combination of blocks 154 and 150 in FIG. 17B ) into clusters.
- the algorithm may begin by splitting the entire data into “r” clusters using the k-means algorithm over the variables represented by S i .
- a description of the k-means algorithm can be found in R. Duda, P. Hart, D. Stork, “Pattern Classification,” second edition, John Wiley & Sons (2001), and a relevant portion of this book describing the k-means algorithm is incorporated herein by reference.
- Other suitable clustering methods known in the art may be employed in place of the k-means approach. This split forms the first branches in the tree where each of these clusters becomes a node.
- the means of these “r” clusters become the “test vectors” for these nodes.
- the algorithm then recursively splits each resulting cluster until reaching a desired tree depth or some other suitable termination condition.
- the terminal nodes each may have a unique code. However, the actual codes used may not be important, so long as each terminal node has a unique code. Typically, the codes are integer values from 1 to “n”, which are arbitrarily assigned to “n” terminal nodes.
- the TSVQ approach may provide savings over ordinary VQ (vector quantization) in the number of comparisons that need to be performed.
- VQ vector quantization
- the amount of computation is O(n) or “order of n”—i.e., the input has to be compared with all the nodes.
- the number of comparisons is a function of the depth of the tree and the branching factor, and leads to a computation cost that are O(log(n)) (order of log(n)) for a balanced tree.
- any probability distribution can be represented by many functional forms (e.g., neural network, mixture of Gaussians, kernel-density function, etc.).
- the functional form of each probability distribution in equation (7) is represented by a table of values where each entry in the table is indexed by a different value of “f”, the quantized feature value described hereinabove. For example, consider a hypothetical probability distribution P( ⁇ 4 ) where ⁇ 4 takes on 5 possible values (1, 2, 3, 4, 5). Such a table would have five entries corresponding to each of these possible values. Each entry would be a probability and all 5 probabilities would sum to 1.0. Under this representation, the restricted Bayesian network takes the following form:
- P ⁇ ( X 1 , ... ⁇ , X n ) P ⁇ ( f 1 ) ⁇ P ⁇ ( f 2 ) ⁇ ⁇ ... ⁇ ⁇ P ⁇ ( f r ) [ P ⁇ ( X 1 ) ] a 1 ⁇ [ P ⁇ ( X 2 ) ] a 2 ⁇ ⁇ ... ⁇ [ P ⁇ ( X n ) ] a n ( 19 )
- the estimate_candidate_feature_probability_object module 195 and the estimate_candidate_feature_probability_non_object module 194 in FIG. 21 estimate probability tables from the training data. Both of these modules constitute the estimate probability distribution module 159 in FIG. 17B . As illustrated in FIG.
- this training process may include, gathering histograms over object and non-object training images (blocks 200 and 204 , respectively), normalizing the histograms to compute P( ⁇ n
- the estimate_candidate_feature_probability_object module 195 and the estimate_candidate_feature_probability_non_object module 194 in FIG. 21 may also estimate marginal probability distributions, P(X i
- each variable is quantized to 13 levels given by; d ⁇ 3 ⁇ ⁇ 3 ⁇ d ⁇ 1.9 ⁇ ⁇ 1.9 ⁇ d ⁇ 1.2 ⁇ ⁇ 1.2 ⁇ d ⁇ 0.7 ⁇ ⁇ 0.7 ⁇ d ⁇ 0.35 ⁇ ⁇ 0.35 ⁇ d ⁇ 0.1 ⁇ ⁇ 0.1 ⁇ d ⁇ + 0.1 ⁇ ⁇ +0.1 ⁇ d ⁇ + 0.35 ⁇ ⁇ +0.35 ⁇ d ⁇ + 0.7 ⁇ ⁇ +0.7 ⁇ d ⁇ + 1.2 ⁇ ⁇ +1.2 ⁇ d ⁇ + 1.9 ⁇ ⁇ +1.9 ⁇ d ⁇ + 3 ⁇ ⁇ +3 ⁇ d (20)
- FIG. 24 illustrates an exemplary histogram 58 .
- the histogram 58 shown in FIG. 24 is for illustrative purposes only. In actual operation, the set of histograms for the classifier 34 may contain many more histograms with many different values.
- a histogram counts the frequency of occurrence of values of a given function.
- Each column in a histogram is called a “bin.”
- the bin is a count of the number of times a specific value of the given function has occurred.
- the statistics for the object training images may be gathered in steps 200 and 204 in FIG. 23 .
- a discrete value, f (the quantized feature value) is computed for each training image.
- the quantizer for subset-1, ⁇ 1 takes the value “567”
- the quantizer for subset-k, ⁇ k takes the value “350”.
- subsets refers to the candidate subsets of coefficients generated by block 144 in FIG. 17B or equivalently block 192 in FIG. 20 . It is noted that because of quantization there may be more than one combination of input values producing the same discrete value.
- Histograms can be collected by counting the number of occurrences of each quantized value across the set of training images. Then, a table of probabilities (not shown) can be generated for each probability distribution P( ⁇ k
- the bin count H 1 (#567) is divided by the count of all bins for histogram H 1 (i.e., ⁇ H 1 (#i)).
- Other probabilities may be similarly calculated.
- This process represents the normalization steps in blocks 202 and 206 in FIG. 23 .
- FIG. 25 shows human faces constituting the set of training images 118
- the method outlined in FIG. 25 can also be used to collect histograms corresponding to a sub-classifier for training images of any object including, for example, cars, telephones, shopping carts, etc., and also for the “non-object” class.
- the evaluate_training_examples_object module 196 and the evaluate_training examples_non_object module 197 in FIG. 21 evaluate the probability of each labeled example with respect to all subset probability distributions, P(F 1
- the evaluation may be done by table look-up; that is, for evaluating a given training example, X, against a particular distribution, P(F k
- the modules 196 and 197 constitute the module 160 in FIG. 17B .
- the select_best_subset_combination module 198 in FIG. 21 chooses a classifier in the form of equation (19) by using r of the q candidate subsets. This module 198 form the module 161 in FIG. 17B .
- a goal of classifier selection is to make the choice that minimizes empirical classification error over a set of labeled examples.
- ROC receiver operating characteristic
- This measure accounts for the classifier's full operating range over values for the threshold, ⁇ , in equation (1). It is noted that a description of the ROC curve can be found in R. Duda, P. Hart, D. Stork, “Pattern Classification,” second edition, John Wiley & Sons (2001), and a relevant portion, of this book describing the ROC curve is incorporated herein by reference.
- a restricted Bayesian Network in the form of equation (19) is formed for a given combination of candidate subsets through the following process. It is noted that disjoint subsets are treated as statistically independent. For example, the network formed by two disjoint subsets, S 1 and S 2 , becomes two disjoint nodes representing P(S 1 ) and P(S 2 ), respectively, with overall probability P(S 1 )P(S 2 ). On the other hand, two intersecting subsets are treated as conditionally independent on the common variables.
- R). The overall probability for this network is then: P ( S 3 ,S 4 ) P ( S 3
- P ⁇ ( X 1 , ... ⁇ , X n ) P ⁇ ( F j ⁇ ( 1 ) ) ⁇ P ⁇ ( F j ⁇ ( 2 ) ) ⁇ ⁇ ... ⁇ ⁇ P ⁇ ( F j ⁇ ( r ) ) [ P ⁇ ( X 1 ) ] a 1 ⁇ [ P ⁇ ( X 2 ) ] a 2 ⁇ ⁇ ... ⁇ [ P ⁇ ( X n ) ] a n ( 25 )
- ⁇ 2 ) for that training example are retrieved by table look-up. These values may then be substituted in equation (26) to get the log-likelihood ratio for a given labeled example.
- the select_best_candidate_feature_combination or select_best_subset_combinations module 198 in FIG. 21 greedily searches for structure of equation (26) by incrementally combining subsets such that ROC area characteristic is maximized over all training examples.
- this greedy search involves the process 230 illustrated in FIG. 26 .
- the area under the ROC curve is calculated for each of the Q sets of sub-classifiers selected in step 236 in FIG. 26 .
- the best M candidate sets of “K” sub-classifiers can then be chosen in step 238 , where these are the M candidates that give the largest area under the ROC curve.
- step 240 the value of K can be incremented by one.
- candidate combinations of size K can be created by adding another sub-classifier to each of the M candidates of size K ⁇ 1.
- the selection of best M combinations with “K” members is again performed at block 241 .
- the process 230 of FIG. 26 can begin again at step 236 for sets of sub-classifiers of size K.
- the ROC curve of these final M candidate sub-classifiers can be evaluated in step 244 on the cross-validation data (block 152 in FIGS. 17A-17B ). The best set of K sub-classifiers can then be chosen according to this criterion.
- an ROC curve is a plot of the number of objects classified correctly versus number of false positives for a given classifier evaluated on a given set of test images. Each point on the plot (not shown) may represent evaluation for a different value of ⁇ (in equation (1)).
- the area under the ROC curve is related to the performance of the classifier. Thus, a greater area indicates better performance of the algorithm. Hence, for example, in face detection, a high area means that a high number of faces classified correctly can be achieved with a low number of false positives.
- the search illustrated in the process 230 of FIG. 26 may be performed multiple times, (e.g., 10 times in one embodiment) to select multiple structures by restricting successive searches from making identical choices to previous searches.
- the module 198 in FIG. 21 compares the structures determined (using, for example, the process 230 of FIG. 26 ) by making more costly performance comparisons on cross-validation data (images that are separate from other aspects of training). The module 198 then chooses the structure that performs best on these cross-validation images based on the ROC criterion.
- the construct_Bayes_Network module or the construction_full_Bayestan_Network block 148 in FIG. 17A may be configured to derive a final representation for the probability distribution that does not involve making the independence assumption in equation (26) (the independence of a node's parents).
- the module 148 forms a full Bayesian network given by equation (6) from the independencies and dependencies implied by the subset grouping where if any two subsets contain overlapping variables, the two subsets are represented as conditionally independent given the overlapping variables.
- the Bayesian Network Connectivity module 162 (which is part of the module 148 in FIG. 17A ) in FIG. 17B may derive the graphical structure of the Bayesian network through the following process.
- the module 162 may identify all groups of variables that act in unison; that is, a group of variables that always co-occur together when they occur in a subset. Each such set of variables will form a node in the final network. For example, consider subsets (X1,X7), (X1,X2,X3), (X1,X2,X4,X5), (X4,X5,X6). In these subsets, only the combination (X4,X5) acts in unison. Hence, the combination (X1,X2) would not form a node because only one of them (X1) occurs in the first group (X1,X7)—i.e., X1 does not always occur in unison with X2 as seen from the first group (X1,X7).
- the module 162 may then form the network by placing the nodes in some ordering. The ordering may be determined by a process described hereinbelow.
- the module 162 may add directed arcs to indicate parent-child relationships within the ordering.
- a node becomes a parent to another node if it precedes the second node in the ordering and if both nodes occur in a subset; that is, if the variables from these two nodes occur together within at least one subset.
- the branches associated with a particular node are determined by which other nodes belong to the same subsets—i.e., the ordering process may take every subset that the node belongs to, and may also take form the total set of other nodes that belong to any of the same subsets.
- the node must be connected to each of these other nodes by a branch. The ordering of the nodes, however, determines the direction of these connections.
- the ordering of nodes in the network may be assigned by the following process. First, the node with the maximum branch count may be identified and assigned as the first node.
- nodes may then be added, one by one, in order of branch count, but considering only those nodes which would become children of nodes already in the network. If a point is reached where none of the unassigned nodes has a parent among the network nodes, then another node with the largest branch count may be added and the process may continue adding nodes in the same fashion.
- each conditional probability distribution is represented by a table (not shown) over discrete valued variables as noted hereinbefore.
- this representation in equations (27) and (28)) is rewritten in terms of these quantized variables, it may no longer necessarily be a conditional probability distribution: P(Z 1,2 )/P(Z 2 ), and one can end up representing something different than one intended.
- pathological cases such as when the dynamic range of X 2 is far larger than that of X 1 .
- module 163 in FIG. 17B combines aspects of both approaches described hereinabove in conjunction with equations (27) through (30).
- This approach in module 163 quantizes the variables in the numerator and denominator in the ratio P(X 1 ⁇ X 2 )/P(X 2 ) sequentially using Tree-Structured Vector Quantization (TSVQ) (described hereinbefore) through the following process.
- TSVQ Tree-Structured Vector Quantization
- a representation of Z 1,2 ⁇ 1 (X 1 ⁇ X 2 ) is obtained that is more efficient and concise than performing the quantization separately for X 1 and X 2 and taking the Cartesian product of their quantized values, Z 1 ⁇ Z 2 .
- the second step forms a sub-tree that reflects quantized values of X 1 that occur given this value of X 2 .
- Such a quantization method therefore can capture dependencies between X 1 and X 2 .
- the quantization functions in the numerator and denominator are consistent in their representation of X 2 , such a representation may preserve the conditional probability relationship.
- the block 164 computes the conditional probability distributions, P(N 1
- Pa Nk is the set of variables of all the parent nodes of node k.
- the AdaBoost algorithm is a general method for training pattern classifiers.
- the AdaBoost algorithm minimizes the classification error on the training set and maximizes the margin between the two classes on the training set as discussed in Shapire & Singer. It is rioted here that AdaBoost is a general method that can be applied to any type of classification algorithm. Given any classifier, AdaBoost works by sequentially re-training multiple instances of the classifier, where, for example, each instance corresponds to a different set of values for the look-up fables (not shown) comprising the terms in equation (6); P(N 1
- AdaBoost applies all of such instances of the classifier in combination and computes the weighted sum of their output to make a classification decision. This approach may result in the increased computational cost of applying all the classifiers in the combination.
- the method described hereinbelow is a process for overcoming this increased computational cost of the AdaBoost approach whereby the combination of classifiers is reduced back into one classifier.
- the AdaBoost algorithm works in an iterative fashion.
- the (i ⁇ 1) th instance is evaluated on the training data (block 96 , FIG. 27 ). Then, the classifier is iteratively retrained where more weight is given to training examples that were incorrectly classified by the classifier trained in the previous iteration (block 98 , FIG. 27 ). This process repeats for p iterations, where “p” represents a predetermined number of iterations. Each iteration produces another instances of the classifier.
- p represents a predetermined number of iterations.
- Each iteration produces another instances of the classifier.
- the values for “p” are in the range of 15 to 20. It is shown in Shapire & Singer that through this iteration process the classification error can be decreased.
- the AdaBoost algorithm re-computes the histograms for each conditional probability distribution over the object and non-object training samples (block 100 ) using the weights determined at block 98 .
- Histograms can be collected by counting the number of occurrences of each quantized feature value, ⁇ i (S i ), across the set of training images.
- Block 102 in FIG. 27 computes a log-likelihood look-up table (not shown) for each pair of conditional probability distributions P(N k
- the overall Bayesian network is then given by equation (6), which when used for both classes (object and non-object) gives the overall sub-classifier given by equation (2).
- G(X) h i (X) from equation (2).
- AdaBoost AdaBoost
- weighted sum of these values where the weights are given by a i :
- each G i (X) is represented by equation (2).
- equation (3) the right-hand side of the classifier in equation (31) can be expressed as:
- AdaBoost AdaBoost iterations are computed.
- the determine_detection_threshold module 165 in FIG. 17B measures performance of the classifier on the cross-validation test set (block 152 in FIG. 17B ).
- the number of iterations (say “k”) that gives the best performance is chosen and the sum in equation (35) is pre-computed up to k rather than p.
- each stage may be “weighted” differently.
- the weight C k ⁇ 1 may be chosen in the equation (37) by empirically trying a range of values, e.g., (0.1, 0.25, 1.0, 4.0, 10.0), over the set of cross-validation images and choosing the weight that gives the best accuracy as measured with respect to area under the ROC curve.
- the performance over these p AdaBoost iterations is computed where, for each iteration, performance is evaluated over several different weightings of the current stage with respect to the previous stages. In one embodiment, these weightings are (0.625, 0.25, 0.6, 1.5, 4.0). The trial that gives the best performance is chosen. The chosen trial indicates the AdaBoost iteration and the weighting ⁇ . The sum in equation (34) is computed up top, and then the sum is weighted by ⁇ .
- block 165 in FIG. 17B determines the threshold ⁇ k applied to equation (4) to achieve a desired performance by measuring accuracy of classifier up to stage “k” for different settings of ⁇ k on the cross-validation images at block 152 in FIG. 17B .
- Lighting correction may be necessary to compensate for differences in lighting of various objects (especially when a classifier is to be used for object detection in real life conditions where object lighting may be less then optimal).
- a lighting correction process 36 precedes evaluation of the classifier as illustrated in FIG. 4B and by block 70 in FIG. 9 .
- lighting correction may be performed on each training example immediately after evaluation of the wavelet transform.
- evaluation means “computing the wavelet transform.” Such lighting correction is indicated at block 109 in FIG. 18 .
- the classifier 34 applies multiple methods of compensation, where each method provides its input to a different group of sub-classifiers. Such an approach may be less susceptible to the failures of an individual method of correction.
- a “localized lighting correction” method adjusts the value of each wavelet coefficient as a function of its neighboring coefficients from within its subband and from other subbands.
- Each coefficient in each band may be normalized as follows.
- Each LL coefficient is normalized by its 3 ⁇ 3 (pixels) neighborhood in the LL band.
- the normalization process computes the average absolute value of the neighborhood. If this average is less than a pre-specified threshold (described hereinbelow), the coefficient is assigned a value of “1.0”. Otherwise, the normalized LL coefficient is computed as the ratio of the original coefficient divided by this neighborhood average.
- Each coefficient in the LH and HL bands is normalized by the combined average of its 3 ⁇ 3 neighborhoods in the LH and HL bands.
- the normalization process assigns value “0.0” to the normalized coefficient. If the average is greater than the threshold, the normalized coefficient is given by the ratio of the original coefficient divided by this average. Each coefficient in the HH band is normalized by the average of its 3 ⁇ 3 neighborhood in the HH band. If this average is less than a threshold, the normalization process assigns value “0.0” to the normalized coefficient. If the average is greater than the threshold, the normalization process divides the original coefficient by the average to give the normalized coefficient. In one embodiment, these thresholds are “1.0” for all LL bands, “2.5” for LH and HL sub bands, and “1.25” for HH subbands. These thresholds may be selected empirically.
- a “variance normalization” method linearly scales all the wavelet coefficients in the image window or some selected portion of it (described hereinbelow), such that the intensities in the region or a selected portion of it, have a pre-specified variance value.
- a “brightest point normalization” method may be used to scale all wavelet coefficients such that the brightest spot in the candidate region or some selected portion of it (described hereinbelow) has a fixed value.
- FIG. 28 illustrates an exemplary process 248 that explains blocks 70 , 72 , 74 , and 76 from FIG. 9 in more detail evaluating a single feature from a sub-classifier at the specified candidate locations. This process is repeated for all features comprising the sub-classifier as seen from the process loop at block 266 in FIG. 28 .
- the image window 32 is placed at a first location in the set of image window locations to be evaluated.
- the wavelet coefficients used by the feature ⁇ k are sampled. It is observed here that, at block 252 , the wavelet coefficient are chosen from the corresponding image window 32 within the selected components of the wavelet transform of the image (i.e., the input image 16 or a scaled version of the input image).
- Blocks 256 , 258 and 260 in FIG. 28 involve computing feature value ⁇ k (for example, by TSVQ or projecting the wavelet coefficients on the linear projection vectors and computing a quantized feature value from projection coefficients), and retrieving the log-likelihood associated with the quantized value.
- the operations at blocks 256 , 258 , and 260 may be identical to their counterparts in the classifier training procedure described hereinbefore.
- the log-likelihood for the candidate within this phase and stage is updated by adding the retrieved log-likelihood value to the corresponding log-likelihood sum for the candidate.
- the image window 32 can then be shifted to a second location (as shown, for example, in FIG. 13B ) and the log-likelihood ratio for the sub-classifier at the second location can be found by the calculation of blocks 252 , 254 , 256 , 258 , 260 , and 262 . This process can be repeated for a third and a fourth location and so on until the sub-classifier has been evaluated at all specified locations as indicated at blocks 264 and 266 .
- a classifier (e.g., the classifier 34 in FIGS. 4A-4B ) designed according to the present disclosure may compute “variance normalization” and “brightest point normalization” over various pre-specified extents of the object.
- the extent of some object does not necessarily occupy the full extent of the classification window 32 .
- the face training examples shown in FIG. 25 do not occupy the complete area of the window 32 .
- the average extent of the object may be entered by hand and used for normalization. This extent may be split into two horizontal halves that are normalized separately.
- FIGS. 20-21 illustrate a conceivable orderings of computation.
- the actual organization of the aforementioned computations may depend on the particular classifier stage.
- each stage can be evaluated in a “candidate-wise” manner ( FIG. 29 ) or a “feature-wise” manner ( FIG. 30 ).
- the “candidate-wise” evaluation performs all feature evaluations separately for each candidate window. This approach involves a total of N 2 ⁇ M 2 feature evaluations for M 2 sub-classifiers and N 2 candidates as shown in FIG. 29 .
- the “feature-wise” evaluation of FIG. 30 attempts to reduce computation cost by sharing feature evaluations among overlapping candidates.
- This strategy performs approximately N 2 +M 2 +2MN feature evaluations over the entire scaled image (assuming all candidates are to be evaluated). Each candidate then samples the M 2 evaluations that overlap its extent and supplies them to the corresponding M 2 sub-classifier log-likelihood look-up tables as illustrated in FIG. 30 .
- N is much greater than M
- this strategy greatly reduces the amount of computation.
- the M 2 sub-classifiers can differ only in their log-likelihood tables (as a function of position within the candidate), but must all share the same type of feature computation. Whereas, in the candidate-wise evaluation, the M 2 sub-classifiers can be completely different.
- the lighting correction must also preferably be applied in the feature-wise manner.
- the “feature-wise” correction assigns the correction at each wavelet coefficient as a function of a localized neighborhood about that point as described by “localized lighting correction” method hereinabove; that is, the correction is independent of the spatial location of the coefficient within the candidate image window 32 .
- the candidate-wise correction considers the whole candidate or a specified portion and can be accomplished by “variance normalization” or “brightest point normalization” methods described hereinabove.
- the early stages in a classifier may use the “feature-wise” evaluation for both lighting correction and feature evaluation.
- the later stages, in which the remaining candidates are sparser may use the “candidate-wise” evaluation.
- One embodiment using four stages uses the feature-wise evaluation for the first three stages and the candidate-wise evaluation for the fourth stage.
- the first two of the four stages use feature-wise lighting correction using the “localized lighting correction” method described hereinabove.
- the first stage uses 20 sub-classifiers which share the same feature computation.
- the second stage uses 24 sub-classifiers that share a feature computation and 42 other sub-classifiers that share another feature computation.
- the third stage uses the candidate-wise evaluation with 19 sub-classifiers, where 5 sub-classifiers share one feature computation, another 5 share a different feature computation, another 5 share a third feature computation, another 3 share a feature computation, and the last one has a unique feature computation.
- the fourth stage involves 9 sub-classifiers wherein each sub-classifier has a unique feature computation.
- features are designed in ascending order of complexity.
- features use small subsets of wavelet coefficients, small numbers of linear projections, and simple quantization. These feature evaluations are designed to be as quick as possible because they have to be applied to many candidates.
- the first two stages in the classifier use subsets of size 3-8 coefficients and use two linear projections with the “scalar quantization 1 -B” scheme described hereinabove.
- features use larger subset sizes, more linear projections, and more complicated quantization schemes such as Tree-Structured Vector Quantization (TSVQ).
- TSVQ Tree-Structured Vector Quantization
- a third stage can contain sub-classifiers that considers between four and twenty input variables, 5 linear projections, the “vector quantization 2 -A” scheme (discussed hereinbefore) for 4 feature computations, and the “vector quantization 3 -A” scheme (discussed hereinbefore) for one feature.
- a fourth and possibly a fifth stage may contain features derived by TSVQ.
- FIG. 31 depicts various images ( 133 through 136 ) of humans with the object markers 52 placed on the human faces detected by the object detector 18 ( FIG. 1 ) according to an embodiment of the present disclosure.
- FIG. 32 illustrates various images ( 137 , 138 ) of teapots with the object markers 52 placed on the teapots detected by the object detector 18 according to another embodiment of the present disclosure.
- FIG. 33 shows results of object detection for slop signs using the object detector 18 according to an embodiment of the present disclosure over three images 140 - 142 of various stop signs.
- a classifier (e.g., the classifier 34 in FIGS. 4A-4B ) designed according to the teachings of the present disclosure can be used for making other kinds of determinations besides object detection. For example, it can be used to determine if two observations (e.g., two images) belong to the same category or class of images. It could also be used for face recognition as shown in FIG. 34 , i.e., to determine if two photographs are of the same person.
- Such a classifier may model two probability distributions: the probability distribution over observations that come from the same category (person)—P(Im 1 ,Im 2
- different), modeling probabilities over the difference between the two images could be constructed by the method described in FIGS. 17A and 17B .
- Same) may be trained using image pairs of examples with both members of each pair come from the same face (but each pair could come from a different face).
- image pairs 270 a and 270 b are examples of such pairs.
- different) may be trained by pairs of examples where for each pair the members come from different faces.
- image pairs 272 a through 272 c are examples of such pairs.
- the (raining images may be constrained to a specific size and the faces to a specific orientation and size within this image, as indicated at block 104 in FIG. 18 .
- the two (non-training) face images may have to be scaled, aligned, and cropped to the same specifications as those applied at block 104 in FIG. 30 and then evaluated by retrieving the probabilities associated with this input effaces, computing the ratio of the retrieved probabilities, and comparing the ratio to a threshold in the same fashion as when the classifier is applied to an input window for face detection.
- the foregoing describes a system and method for detecting presence of a 3D object in a 2D image containing a 2D representation of the 3D object.
- the object detector according to the present disclosure may improve upon existing techniques for object detection both in accuracy and computational properties.
- a pre-selected number of view-based classifiers may be trained on sample images prior to performing the detection on an unknown image.
- the object detection program may then operate on the given input image and compute its wavelet transform for the entire input image.
- the object detection program may then proceed with sampling of the wavelet coefficients at different image window locations on the input image, and apply each classifier involving linear projection of selected subsets of coefficients, quantization of linear projection coefficients and efficient look-up of pre-computed log-likelihood tables to determine object presence.
- the object detector's coarse-to-fine object detection strategy coupled with exhaustive object search across different positions and scales may result in an efficient and accurate object detection scheme.
- the object detector may detect a 3D object over a wide range in angular variation (e.g., 180 degrees) through the combination of a small number of classifiers each specialized to a small range within this range of angular variation.
- Each classifier according to the present disclosure may contain one or more sub-classifiers, wherein each sub-classifier may be based on a ratio of two Bayesian networks.
- the object detector may be trained to detect many different types of objects (e.g., airplanes, cat faces, telephones, etc.) besides human faces and cars discussed hereinabove.
- Some of the applications where the object detector may be used include: commercial image databases (e.g., stock photography) for automatically labeling and indexing of images; an Internet-based image searching and indexing service; finding biological structures in various types of biological images (e.g., MRI, X-rays, microscope images, etc.); finding objects of military interest (e.g., mines, tanks, etc.) in satellite, radar, or visible imagery; finding objects of interest to scientists (e.g., craters, volcanoes, etc.) in astronomical images; as a tool for automatic description of the image content of an image database; to achieve accurate color balancing on human faces and remove “red-eye” from human faces in a digital photo development; for automatic adjustment of focus, contrast, and centering on human faces during digital photography; to automatically point, focus, and center cameras on human faces during video
Abstract
Description
If the log likelihood ratio (the left side in equation (1)) is greater than the right side (λ), the
It is noted that the functional forms chosen to approximate the distributions in equation (2) are discussed hereinbelow under the “Classifier Design” section. Together, the sum of the sub-classifiers from all of the stages of a classifier may constitute the “overall” classifier as given by equation (1) and also represented below in equation (3).
Turning to
After each sub-classifier evaluation is completed, the classifier may apply a threshold, λk, to the partial sum in equation (4) and remove additional candidates (i.e. parts of the image being searched) from further consideration as shown and discussed hereinbelow. Thus, the cascade evaluation strategy focuses on sub-classifier-by-sub-classifier analysis and builds on earlier computations to generate the final result.
Thus, for example, for i=2, f=1.41. Hence, the image is reduced by factor “f”. In other words, the new scaled image (for i=2) is 0.71 (1/f) in size as compared to the original image (for i=0). Thus, the size (along any dimension, e.g., horizontal) of the image to be evaluated can be expressed by N=(1/f)*S, where N is the size of the input image currently evaluated by the corresponding classifier, and S is the original size of the input image. Extensive object search (as given by
P(image_window|ω)=P(x 1 , . . . , x r|ω)=P(x 1 |pa x1)P(x 2 |pa x2) . . . P(x n |pa xn) (6)
where the image-
The variables X1, X2, etc. may be either pixels or transform coefficients. In one embodiment, they are wavelet transform coefficients. The structure in equation-7 assumes that the parents of any node in the second layer are statistically independent where S1 . . . Sr are some collection of subsets of the input variables generated by the
where, C1 is the error in modeling the two variables, Xi and Xj, as independent; C2 is the error of removing one variable, Xj, from the pair; and C3 is the error of removing both variables from the pair. Also, in the foregoing equations 8-10, the “abs[ ]” denotes an operator that gives absolute value of the parameter supplied as its operand. The error measures C1, C2, and C3 may be obtained by empirically estimating the probability distributions, P(Xi, Xj|ω1) and P(X, Xj|ω2), for every pairings of variables, Xi, Xj.
where each Sk is a subset chosen from the set of input variables, X1 . . . Xn. A set of candidate subsets, G, is then sought to minimize this localized error function in equation (11).
pk=vk Tw
w=(w 1 . . . , w 12)T (13)
d<μ−σ
μ−σ≦d<μ−0.5σ
μ−0.5σ≦d<μ+0.5σ
μ+0.5σ≦d<μ+σ
μ+σ≦d (14)
The quantized feature value, f, can then be uniquely computed from this conglomerate of the quantized projection coefficient values. For example if there are three quantized projection values, e1, e2, and e3, and if each of them takes on 5 possible values from 0 to 4, then f takes a value from 0 to 124 given by:
f=e 1+5e 2+52 e 3 (15)
d<μ−σ
μ−σ≦d<μ−0.5σ
μ−0.5σ≦d<μ+0.5σ
μ+0.5σ≦d<μ+σ
μ+σ≦d (16)
This method then applies the vector
Where N is the number of wavelet coefficients in a subset and Q is the number of projection vectors. Each “pj” is a projection coefficient given by equation-13 and each “wi” is a wavelet coefficient. In one embodiment, the value of “g” in equation (17) can be quantized to 4 levels. The quantization thresholds may be 0.5gave, gave, and 2.0 gave, where gave is the average value of g computed over the object training image set. Combining this measurement with any other quantized measurement increases the total number of quantization bins by a factor of 4.
Where Q is the number of projection vectors. In one embodiment, the variable “h” in equation (18) is quantized to 4 levels. The quantization thresholds are 0.5have, 2.0have, and 4.0have, where have is the average value of h computed over the object training image set. Combining this measurement with any other quantized measurement increases the total number of quantization bins by a factor of 4.
d<μ−3σ
μ−3σ≦d<μ−1.9σ
μ−1.9σ≦d<μ−1.2σ
μ−1.2σ≦d<μ−0.7σ
μ−0.7σ≦d<μ−0.35σ
μ−0.35σ≦d<μ−0.1σ
μ−0.1σ≦d<μ+0.1σ
μ+0.1σ≦d<μ+0.35σ
μ+0.35σ≦d<μ+0.7σ
μ+0.7σ≦d<μ+1.2σ
μ+1.2σ≦d<μ+1.9σ
μ+1.9σ≦d<μ+3σ
μ+3σ≦d (20)
R=S3∩S4={X17, X33, X92} (21)
has a structure with a third node representing P(R). This node is then parent to two nodes with probabilities P(S3|R) and P(S4|R). The overall probability for this network is then:
P(S 3 ,S 4)=P(S 3 |R)P(S 4 |R)P(R) (22)
Under feature representation for each subset, the relationship in equation (22) becomes:
where the members of the parent R, are represented as statistically independent. In this reduced parameterization, there are pre-computed distributions for every term, whereas, in the form of equation (23), one may have to estimate P(R) in the very least; that is, one cannot obtain P(R) by marginalizing P(F3) or P(F4) since they involve dimensionality reduction on the original variables. It also may not be possible to originally estimate the full distributions, P(S3) and P(S4), because S3 and S4 are usually of too high dimension. It is noted that the α's may be computed from the number of occurrences of the variable, ei (see, e.g., equation-15), over all the chosen subsets. It is noted that ei greater than “1” indicates some redundancy in representing the variable across these subsets. Therefore, it may be preferable to choose the term in the denominator in equation (24) to correct for this redundancy and choose αt=ei−1. Otherwise, the contribution of the variable would be “over-counted.”
Z 1,2=ƒ1(X 1 ×X 2) (27)
Z 2=ƒ2(X 2) (28)
Alternatively, in the second approach, X1 and X2 are quantized separately as given below:
Z 1=ƒ1(X 1) (29)
Z 2=ƒ2(X 2) (30)
In the above equations 27-30, “fi( )” denotes the quantized feature value function.
This involves “p” times the amount of computation of just evaluating a single instance of the classifier, where a single instance of the classifier could be trained using uniform weighting for all training examples.
The equation (33) can be re-written in a mathematically equivalent form as:
where each gj(X) represents a single log-likelihood table pre-computed by the sum:
The resulting classifier in equation (34) has the same computational cost as the original classifiers in equation (32) or equivalently, equation (2).
H=c 1 H 1 + . . . +c N H N (36)
or equivalently as a cascade of weights:
H=H N +C N−1(H N−1 +C N−2(H N−2 . . . +c 1 H 1) . . . ) (37)
In one embodiment, at
Claims (53)
Priority Applications (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/259,371 US8064688B2 (en) | 2004-10-22 | 2008-10-28 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
US13/300,884 US8472706B2 (en) | 2004-10-22 | 2011-11-21 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
US13/901,803 US9213885B1 (en) | 2004-10-22 | 2013-05-24 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
US14/937,851 US9978002B2 (en) | 2004-10-22 | 2015-11-10 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/971,868 US7848566B2 (en) | 2004-10-22 | 2004-10-22 | Object recognizer and detector for two-dimensional images using bayesian network based classifier |
US12/259,371 US8064688B2 (en) | 2004-10-22 | 2008-10-28 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US10/971,868 Division US7848566B2 (en) | 2004-10-22 | 2004-10-22 | Object recognizer and detector for two-dimensional images using bayesian network based classifier |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/300,884 Continuation US8472706B2 (en) | 2004-10-22 | 2011-11-21 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
Publications (2)
Publication Number | Publication Date |
---|---|
US20090067730A1 US20090067730A1 (en) | 2009-03-12 |
US8064688B2 true US8064688B2 (en) | 2011-11-22 |
Family
ID=35697137
Family Applications (5)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US10/971,868 Active 2028-07-20 US7848566B2 (en) | 2004-10-22 | 2004-10-22 | Object recognizer and detector for two-dimensional images using bayesian network based classifier |
US12/259,371 Active 2026-05-30 US8064688B2 (en) | 2004-10-22 | 2008-10-28 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
US13/300,884 Expired - Fee Related US8472706B2 (en) | 2004-10-22 | 2011-11-21 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
US13/901,803 Active 2026-06-04 US9213885B1 (en) | 2004-10-22 | 2013-05-24 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
US14/937,851 Active US9978002B2 (en) | 2004-10-22 | 2015-11-10 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US10/971,868 Active 2028-07-20 US7848566B2 (en) | 2004-10-22 | 2004-10-22 | Object recognizer and detector for two-dimensional images using bayesian network based classifier |
Family Applications After (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/300,884 Expired - Fee Related US8472706B2 (en) | 2004-10-22 | 2011-11-21 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
US13/901,803 Active 2026-06-04 US9213885B1 (en) | 2004-10-22 | 2013-05-24 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
US14/937,851 Active US9978002B2 (en) | 2004-10-22 | 2015-11-10 | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
Country Status (2)
Country | Link |
---|---|
US (5) | US7848566B2 (en) |
WO (1) | WO2006047253A1 (en) |
Cited By (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110141219A1 (en) * | 2009-12-10 | 2011-06-16 | Apple Inc. | Face detection as a metric to stabilize video during video chat session |
US20120050259A1 (en) * | 2010-08-31 | 2012-03-01 | Apple Inc. | Systems, methods, and computer-readable media for efficiently processing graphical data |
US8411909B1 (en) | 2012-06-26 | 2013-04-02 | Google Inc. | Facial recognition |
US8457367B1 (en) | 2012-06-26 | 2013-06-04 | Google Inc. | Facial recognition |
US8542879B1 (en) | 2012-06-26 | 2013-09-24 | Google Inc. | Facial recognition |
US20140270482A1 (en) * | 2013-03-15 | 2014-09-18 | Sri International | Recognizing Entity Interactions in Visual Media |
US8856541B1 (en) | 2013-01-10 | 2014-10-07 | Google Inc. | Liveness detection |
CN104778670A (en) * | 2015-04-17 | 2015-07-15 | 广西科技大学 | Fractal-wavelet self-adaption image denoising method based on multivariate statistical model |
US9128737B2 (en) | 2011-10-18 | 2015-09-08 | Google Inc. | Dynamic profile switching based on user identification |
US20160125272A1 (en) * | 2004-10-22 | 2016-05-05 | Carnegie Mellon University | Object recognizer and detector for two-dimensional images using bayesian network based classifier |
US9355088B2 (en) | 2013-07-12 | 2016-05-31 | Microsoft Technology Licensing, Llc | Feature completion in computer-human interactive learning |
US20180053044A1 (en) * | 2015-03-13 | 2018-02-22 | Nec Corporation | Living body detection device, living body detection method, and recording medium |
CN108257079A (en) * | 2016-12-29 | 2018-07-06 | 北京国双科技有限公司 | Graphic change method and device |
US10121059B2 (en) | 2016-08-23 | 2018-11-06 | Samsung Electronics Co., Ltd. | Liveness test method and apparatus |
US10664727B2 (en) * | 2016-04-01 | 2020-05-26 | Fuji Xerox Co., Ltd. | Image pattern recognition device and recording medium |
US11176392B2 (en) | 2017-03-27 | 2021-11-16 | Samsung Electronics Co., Ltd. | Liveness test method and apparatus |
Families Citing this family (152)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7508979B2 (en) * | 2003-11-21 | 2009-03-24 | Siemens Corporate Research, Inc. | System and method for detecting an occupant and head pose using stereo detectors |
JP4581924B2 (en) * | 2004-09-29 | 2010-11-17 | 株式会社ニコン | Image reproducing apparatus and image reproducing program |
JP4824411B2 (en) * | 2005-01-20 | 2011-11-30 | パナソニック株式会社 | Face extraction device, semiconductor integrated circuit |
US20060204107A1 (en) * | 2005-03-04 | 2006-09-14 | Lockheed Martin Corporation | Object recognition system using dynamic length genetic training |
GB2432064B (en) * | 2005-10-31 | 2011-01-19 | Hewlett Packard Development Co | Method of triggering a detector to detect a moving feature within a video stream |
US7724962B2 (en) * | 2006-07-07 | 2010-05-25 | Siemens Corporation | Context adaptive approach in vehicle detection under various visibility conditions |
US20080036593A1 (en) * | 2006-08-04 | 2008-02-14 | The Government Of The Us, As Represented By The Secretary Of The Navy | Volume sensor: data fusion-based, multi-sensor system for advanced damage control |
US7747079B2 (en) | 2006-09-27 | 2010-06-29 | Tandent Vision Science, Inc. | Method and system for learning spatio-spectral features in an image |
US8009900B2 (en) * | 2006-09-28 | 2011-08-30 | Siemens Medical Solutions Usa, Inc. | System and method for detecting an object in a high dimensional space |
US7881505B2 (en) * | 2006-09-29 | 2011-02-01 | Pittsburgh Pattern Recognition, Inc. | Video retrieval system for human face content |
US20080112593A1 (en) * | 2006-11-03 | 2008-05-15 | Ratner Edward R | Automated method and apparatus for robust image object recognition and/or classification using multiple temporal views |
US8150101B2 (en) | 2006-11-13 | 2012-04-03 | Cybernet Systems Corporation | Orientation invariant object identification using model-based image processing |
WO2008067479A1 (en) * | 2006-11-29 | 2008-06-05 | President And Fellows Of Harvard College | A framework for wavelet-based analysis and processing of color filter array images with applications to denoising and demosaicing |
WO2008067472A2 (en) * | 2006-11-29 | 2008-06-05 | President And Fellows Of Harvard College | A new spatio-spectral sampling paradigm for imaging and a novel color filter array design |
US20080199084A1 (en) * | 2007-02-19 | 2008-08-21 | Seiko Epson Corporation | Category Classification Apparatus and Category Classification Method |
JP2008234623A (en) * | 2007-02-19 | 2008-10-02 | Seiko Epson Corp | Category classification apparatus and method, and program |
US8027513B2 (en) * | 2007-03-23 | 2011-09-27 | Technion Research And Development Foundation Ltd. | Bitmap tracker for visual tracking under very general conditions |
US8934717B2 (en) * | 2007-06-05 | 2015-01-13 | Intellectual Ventures Fund 83 Llc | Automatic story creation using semantic classifiers for digital assets and associated metadata |
US7693806B2 (en) * | 2007-06-21 | 2010-04-06 | Microsoft Corporation | Classification using a cascade approach |
US7991199B2 (en) * | 2007-06-29 | 2011-08-02 | Microsoft Corporation | Object identification and verification using transform vector quantization |
US8010471B2 (en) * | 2007-07-13 | 2011-08-30 | Microsoft Corporation | Multiple-instance pruning for learning efficient cascade detectors |
US7949621B2 (en) * | 2007-10-12 | 2011-05-24 | Microsoft Corporation | Object detection and recognition with bayesian boosting |
US8099373B2 (en) * | 2008-02-14 | 2012-01-17 | Microsoft Corporation | Object detector trained using a working set of training data |
JP5538684B2 (en) * | 2008-03-13 | 2014-07-02 | キヤノン株式会社 | Image processing apparatus, image processing method, program, and storage medium |
US8175992B2 (en) * | 2008-03-17 | 2012-05-08 | Intelliscience Corporation | Methods and systems for compound feature creation, processing, and identification in conjunction with a data analysis and feature recognition system wherein hit weights are summed |
US9576217B2 (en) | 2008-04-11 | 2017-02-21 | Recognition Robotics | System and method for visual recognition |
US8150165B2 (en) * | 2008-04-11 | 2012-04-03 | Recognition Robotics, Inc. | System and method for visual recognition |
US8249142B2 (en) * | 2008-04-24 | 2012-08-21 | Motorola Mobility Llc | Method and apparatus for encoding and decoding video using redundant encoding and decoding techniques |
US8244044B2 (en) * | 2008-04-25 | 2012-08-14 | Microsoft Corporation | Feature selection and extraction |
GB0808586D0 (en) * | 2008-05-12 | 2008-06-18 | Qinetiq Ltd | Method and apparatus for vehicle classification |
JP5127583B2 (en) * | 2008-06-20 | 2013-01-23 | 株式会社豊田中央研究所 | Object determination apparatus and program |
CN101635763A (en) * | 2008-07-23 | 2010-01-27 | 深圳富泰宏精密工业有限公司 | Picture classification system and method |
JP4791598B2 (en) | 2008-09-17 | 2011-10-12 | 富士通株式会社 | Image processing apparatus and image processing method |
US8704832B2 (en) | 2008-09-20 | 2014-04-22 | Mixamo, Inc. | Interactive design, synthesis and delivery of 3D character motion data through the web |
US8379937B1 (en) | 2008-09-29 | 2013-02-19 | Videomining Corporation | Method and system for robust human ethnicity recognition using image feature-based probabilistic graphical models |
US8982122B2 (en) | 2008-11-24 | 2015-03-17 | Mixamo, Inc. | Real time concurrent design of shape, texture, and motion for 3D character animation |
US8659596B2 (en) | 2008-11-24 | 2014-02-25 | Mixamo, Inc. | Real time generation of animation-ready 3D character models |
US8144945B2 (en) * | 2008-12-04 | 2012-03-27 | Nokia Corporation | Method, apparatus and computer program product for providing an orientation independent face detector |
US8180917B1 (en) * | 2009-01-28 | 2012-05-15 | Trend Micro, Inc. | Packet threshold-mix batching dispatcher to counter traffic analysis |
US20100259547A1 (en) | 2009-02-12 | 2010-10-14 | Mixamo, Inc. | Web platform for interactive design, synthesis and delivery of 3d character motion data |
CN101872477B (en) * | 2009-04-24 | 2014-07-16 | 索尼株式会社 | Method and device for detecting object in image and system containing device |
JP5227888B2 (en) * | 2009-05-21 | 2013-07-03 | 富士フイルム株式会社 | Person tracking method, person tracking apparatus, and person tracking program |
JP5214533B2 (en) * | 2009-05-21 | 2013-06-19 | 富士フイルム株式会社 | Person tracking method, person tracking apparatus, and person tracking program |
JP2011013732A (en) * | 2009-06-30 | 2011-01-20 | Sony Corp | Information processing apparatus, information processing method, and program |
US8170332B2 (en) * | 2009-10-07 | 2012-05-01 | Seiko Epson Corporation | Automatic red-eye object classification in digital images using a boosting-based framework |
WO2011053328A1 (en) * | 2009-11-02 | 2011-05-05 | Archaio, Llc | System and method employing three-dimensional and two-dimensional digital images |
US8213709B1 (en) * | 2009-11-03 | 2012-07-03 | Hrl Laboratories, Llc | Method and system for directed area search using cognitive swarm vision and cognitive Bayesian reasoning |
CN102110122B (en) * | 2009-12-24 | 2013-04-03 | 阿里巴巴集团控股有限公司 | Method and device for establishing sample picture index table, method and device for filtering pictures and method and device for searching pictures |
RU2427911C1 (en) * | 2010-02-05 | 2011-08-27 | Фирма "С1 Ко., Лтд." | Method to detect faces on image using classifiers cascade |
US8605969B2 (en) * | 2010-04-06 | 2013-12-10 | Siemens Corporation | Method and system for multiple object detection by sequential Monte Carlo and hierarchical detection network |
US8928672B2 (en) | 2010-04-28 | 2015-01-06 | Mixamo, Inc. | Real-time automatic concatenation of 3D animation sequences |
KR101633377B1 (en) * | 2010-06-18 | 2016-07-08 | 삼성전자주식회사 | Method and Apparatus for Processing Frames Obtained by Multi-Exposure |
US8586955B2 (en) * | 2010-09-22 | 2013-11-19 | Ko Khee Tay | Apparatus and method for attenuating high energy radiation based on detected vehicle type |
US9760896B2 (en) * | 2010-10-18 | 2017-09-12 | Entit Software Llc | Acquiring customer insight in a retail environment |
US8705811B1 (en) * | 2010-10-26 | 2014-04-22 | Apple Inc. | Luminance adjusted face detection |
US20120213426A1 (en) * | 2011-02-22 | 2012-08-23 | The Board Of Trustees Of The Leland Stanford Junior University | Method for Implementing a High-Level Image Representation for Image Analysis |
US8605998B2 (en) | 2011-05-06 | 2013-12-10 | Toyota Motor Engineering & Manufacturing North America, Inc. | Real-time 3D point cloud obstacle discriminator apparatus and associated methodology for training a classifier via bootstrapping |
US8510166B2 (en) * | 2011-05-11 | 2013-08-13 | Google Inc. | Gaze tracking system |
US8860787B1 (en) | 2011-05-11 | 2014-10-14 | Google Inc. | Method and apparatus for telepresence sharing |
WO2012159109A2 (en) * | 2011-05-19 | 2012-11-22 | Bir Bhanu | Dynamic bayesian networks for vehicle classification in video |
US10049482B2 (en) | 2011-07-22 | 2018-08-14 | Adobe Systems Incorporated | Systems and methods for animation recommendations |
US8799201B2 (en) | 2011-07-25 | 2014-08-05 | Toyota Motor Engineering & Manufacturing North America, Inc. | Method and system for tracking objects |
WO2013033790A1 (en) * | 2011-09-09 | 2013-03-14 | Newsouth Innovations Pty Limited | A method and apparatus for communicating and recovering motion information |
CN102509284B (en) * | 2011-09-30 | 2013-12-25 | 北京航空航天大学 | Method for automatically evaluating portrait lighting artistry |
US10748325B2 (en) | 2011-11-17 | 2020-08-18 | Adobe Inc. | System and method for automatic rigging of three dimensional characters for facial animation |
JP5906071B2 (en) * | 2011-12-01 | 2016-04-20 | キヤノン株式会社 | Information processing method, information processing apparatus, and storage medium |
US20130215113A1 (en) * | 2012-02-21 | 2013-08-22 | Mixamo, Inc. | Systems and methods for animating the faces of 3d characters using images of human faces |
US9747495B2 (en) | 2012-03-06 | 2017-08-29 | Adobe Systems Incorporated | Systems and methods for creating and distributing modifiable animated video messages |
US8862764B1 (en) | 2012-03-16 | 2014-10-14 | Google Inc. | Method and Apparatus for providing Media Information to Mobile Devices |
US8948500B2 (en) * | 2012-05-31 | 2015-02-03 | Seiko Epson Corporation | Method of automatically training a classifier hierarchy by dynamic grouping the training samples |
US8396265B1 (en) | 2012-06-26 | 2013-03-12 | Google Inc. | Facial recognition |
US9262869B2 (en) | 2012-07-12 | 2016-02-16 | UL See Inc. | Method of 3D model morphing driven by facial tracking and electronic device using the method the same |
US9600711B2 (en) * | 2012-08-29 | 2017-03-21 | Conduent Business Services, Llc | Method and system for automatically recognizing facial expressions via algorithmic periocular localization |
US8886953B1 (en) | 2012-09-14 | 2014-11-11 | Google Inc. | Image processing |
JP6003492B2 (en) * | 2012-10-01 | 2016-10-05 | 富士ゼロックス株式会社 | Character recognition device and program |
US9449284B2 (en) * | 2012-10-04 | 2016-09-20 | Nec Corporation | Methods and systems for dependency network analysis using a multitask learning graphical lasso objective function |
CN104718559B (en) | 2012-10-22 | 2018-05-01 | 诺基亚技术有限公司 | Method and apparatus for image procossing |
US10445415B1 (en) * | 2013-03-14 | 2019-10-15 | Ca, Inc. | Graphical system for creating text classifier to match text in a document by combining existing classifiers |
US9152860B2 (en) * | 2013-05-10 | 2015-10-06 | Tantrum Street LLC | Methods and apparatus for capturing, processing, training, and detecting patterns using pattern recognition classifiers |
US9235781B2 (en) * | 2013-08-09 | 2016-01-12 | Kabushiki Kaisha Toshiba | Method of, and apparatus for, landmark location |
US9053392B2 (en) * | 2013-08-28 | 2015-06-09 | Adobe Systems Incorporated | Generating a hierarchy of visual pattern classes |
BR112016006566B1 (en) * | 2013-09-25 | 2022-06-28 | Sicpa Holding Sa | METHOD AND DEVICE FOR AUTHENTICATION OF A BRAND |
US20150278635A1 (en) * | 2014-03-31 | 2015-10-01 | Massachusetts Institute Of Technology | Methods and apparatus for learning representations |
CN105005798B (en) * | 2014-04-24 | 2018-09-04 | 南京理工大学 | One kind is based on the similar matched target identification method of structures statistics in part |
US10474949B2 (en) | 2014-08-19 | 2019-11-12 | Qualcomm Incorporated | Knowledge-graph biased classification for data |
US9541507B2 (en) | 2014-08-26 | 2017-01-10 | Northrop Grumman Systems Corporation | Color-based foreign object detection system |
US9773155B2 (en) * | 2014-10-14 | 2017-09-26 | Microsoft Technology Licensing, Llc | Depth from time of flight camera |
US9760809B2 (en) * | 2014-10-20 | 2017-09-12 | Bae Systems Information And Electronic Systems Integration Inc. | Systems and methods for multi-factor image recognition |
SG11201704076YA (en) * | 2014-11-18 | 2017-06-29 | Agency Science Tech & Res | Method and device for traffic sign recognition |
EP3076337B1 (en) * | 2015-03-31 | 2020-01-08 | ChannelSight Limited | Method and system for product recognition |
GB2538095B (en) * | 2015-05-07 | 2019-07-03 | Thales Holdings Uk Plc | Recognition of Objects from Shadow and Layover Effects in Synthetic Aperture Radar Images |
US10657362B2 (en) * | 2015-06-30 | 2020-05-19 | Nec Corporation Of America | Facial recognition system |
CN105139422B (en) * | 2015-08-14 | 2018-05-18 | 中国联合网络通信集团有限公司 | A kind of self-explanatory method for tracking target and device |
US9767564B2 (en) | 2015-08-14 | 2017-09-19 | International Business Machines Corporation | Monitoring of object impressions and viewing patterns |
EP3365838A4 (en) | 2015-10-21 | 2019-08-28 | 15 Seconds Of Fame, Inc. | Methods and apparatus for false positive minimization in facial recognition applications |
CN106803055B (en) * | 2015-11-26 | 2019-10-25 | 腾讯科技（深圳）有限公司 | Face identification method and device |
US20170154246A1 (en) * | 2015-11-30 | 2017-06-01 | Seematics Systems Ltd | Apparatus and method for inferring information using a computer network |
CN105740781B (en) * | 2016-01-25 | 2020-05-19 | 北京眼神智能科技有限公司 | Three-dimensional human face living body detection method and device |
CN105550677B (en) * | 2016-02-02 | 2018-08-24 | 河北大学 | A kind of 3D palmprint authentications method |
ES2909047T3 (en) * | 2016-03-14 | 2022-05-05 | Siemens Mobility GmbH | Method and system to efficiently mine essential elements of a dataset with a resampling strategy with repositioning in pose estimation with 6DOF of 3D objects |
US9996772B2 (en) * | 2016-04-28 | 2018-06-12 | International Business Machines Corporation | Detection of objects in images using region-based convolutional neural networks |
CN106056141B (en) * | 2016-05-27 | 2019-04-19 | 哈尔滨工程大学 | A kind of target identification of use space sparse coding and angle rough estimate calculating method |
US10559111B2 (en) | 2016-06-23 | 2020-02-11 | LoomAi, Inc. | Systems and methods for generating computer ready animation models of a human head from captured data images |
WO2017223530A1 (en) | 2016-06-23 | 2017-12-28 | LoomAi, Inc. | Systems and methods for generating computer ready animation models of a human head from captured data images |
US9904844B1 (en) | 2016-08-04 | 2018-02-27 | International Business Machines Corporation | Clustering large database of images using multilevel clustering approach for optimized face recognition process |
US10019655B2 (en) * | 2016-08-31 | 2018-07-10 | Adobe Systems Incorporated | Deep-learning network architecture for object detection |
US10445576B2 (en) * | 2016-09-23 | 2019-10-15 | Cox Automotive, Inc. | Automated vehicle recognition systems |
CN106528770B (en) * | 2016-11-07 | 2019-10-29 | 大连工业大学 | CAD semantic model search method based on design idea |
KR20180051838A (en) | 2016-11-09 | 2018-05-17 | 삼성전자주식회사 | Informing method and apparatus of approaching of the other party to pedestrian and passenger |
US10572570B2 (en) * | 2016-11-14 | 2020-02-25 | Blackberry Limited | Determining a load status of a platform using a likelihood ratio test |
KR20180060784A (en) | 2016-11-29 | 2018-06-07 | 삼성전자주식회사 | Method and apparatus for determining abnormal object |
US10733506B1 (en) | 2016-12-14 | 2020-08-04 | Waymo Llc | Object detection neural network |
CN106780503A (en) * | 2016-12-30 | 2017-05-31 | 北京师范大学 | Remote sensing images optimum segmentation yardstick based on posterior probability information entropy determines method |
CN107194464B (en) * | 2017-04-25 | 2021-06-01 | 北京小米移动软件有限公司 | Training method and device of convolutional neural network model |
EP3580690B1 (en) | 2017-05-24 | 2020-09-02 | Google LLC | Bayesian methodology for geospatial object/characteristic detection |
CN107274401B (en) * | 2017-06-22 | 2020-09-04 | 中国人民解放军海军航空大学 | High-resolution SAR image ship detection method based on visual attention mechanism |
US10713485B2 (en) | 2017-06-30 | 2020-07-14 | International Business Machines Corporation | Object storage and retrieval based upon context |
CN107341519B (en) * | 2017-07-10 | 2021-01-26 | 电子科技大学 | Support vector machine identification optimization method based on multi-resolution analysis |
US10572963B1 (en) * | 2017-07-14 | 2020-02-25 | Synapse Technology Corporation | Detection of items |
US10210631B1 (en) | 2017-08-18 | 2019-02-19 | Synapse Technology Corporation | Generating synthetic image data |
CN107609535A (en) * | 2017-09-28 | 2018-01-19 | 天津大学 | Face datection, Attitude estimation and localization method based on shared pool hybrid coordination tree model |
CN107748869B (en) * | 2017-10-26 | 2021-01-22 | 奥比中光科技集团股份有限公司 | 3D face identity authentication method and device |
CN107609383B (en) * | 2017-10-26 | 2021-01-26 | 奥比中光科技集团股份有限公司 | 3D face identity authentication method and device |
CN108520210A (en) * | 2018-03-26 | 2018-09-11 | 河南工程学院 | Based on wavelet transformation and the face identification method being locally linear embedding into |
US10839266B2 (en) * | 2018-03-30 | 2020-11-17 | Intel Corporation | Distributed object detection processing |
US10997462B2 (en) * | 2018-04-04 | 2021-05-04 | Canon Virginia, Inc. | Devices, systems, and methods for clustering reference images for non-destructive testing |
US10198845B1 (en) | 2018-05-29 | 2019-02-05 | LoomAi, Inc. | Methods and systems for animating facial expressions |
US10452959B1 (en) | 2018-07-20 | 2019-10-22 | Synapse Tehnology Corporation | Multi-perspective detection of objects |
CN110751670B (en) * | 2018-07-23 | 2022-10-25 | 中国科学院长春光学精密机械与物理研究所 | Target tracking method based on fusion |
US11755937B2 (en) | 2018-08-24 | 2023-09-12 | General Electric Company | Multi-source modeling with legacy data |
US10936856B2 (en) * | 2018-08-31 | 2021-03-02 | 15 Seconds of Fame, Inc. | Methods and apparatus for reducing false positives in facial recognition |
CN109598213B (en) * | 2018-11-20 | 2021-04-06 | 图普科技（广州）有限公司 | Face orientation aggregation method and device |
US11468275B1 (en) * | 2019-03-05 | 2022-10-11 | Apple Inc. | Computer vision using a prior probability distribution selected based on an image capture condition |
US11010596B2 (en) | 2019-03-07 | 2021-05-18 | 15 Seconds of Fame, Inc. | Apparatus and methods for facial recognition systems to identify proximity-based connections |
KR20200144658A (en) * | 2019-06-19 | 2020-12-30 | 삼성전자주식회사 | Classification apparatus and operating method thereof and training method |
US11551393B2 (en) | 2019-07-23 | 2023-01-10 | LoomAi, Inc. | Systems and methods for animation generation |
US11010605B2 (en) | 2019-07-30 | 2021-05-18 | Rapiscan Laboratories, Inc. | Multi-model detection of objects |
CN110428007B (en) * | 2019-08-01 | 2020-11-24 | 科大讯飞(苏州)科技有限公司 | X-ray image target detection method, device and equipment |
CN110555386A (en) * | 2019-08-02 | 2019-12-10 | 天津理工大学 | Face recognition identity authentication method based on dynamic Bayes |
CN110580680B (en) * | 2019-09-09 | 2022-07-05 | 武汉工程大学 | Face super-resolution method and device based on combined learning |
US11440194B2 (en) * | 2019-09-13 | 2022-09-13 | Honda Motor Co., Ltd. | Physical human-robot interaction (pHRI) |
US11727721B2 (en) * | 2019-09-30 | 2023-08-15 | Mcafee, Llc | Methods and apparatus to detect deepfake content |
CN111027390B (en) * | 2019-11-11 | 2023-10-10 | 北京三快在线科技有限公司 | Object class detection method and device, electronic equipment and storage medium |
US11341351B2 (en) | 2020-01-03 | 2022-05-24 | 15 Seconds of Fame, Inc. | Methods and apparatus for facial recognition on a user device |
CN111178340B (en) * | 2020-04-10 | 2020-07-21 | 支付宝(杭州)信息技术有限公司 | Image recognition method and training method of image recognition model |
US20220375204A1 (en) * | 2020-05-11 | 2022-11-24 | Nec Corporation | Learning device, learning method, and recording medium |
CN114386412B (en) * | 2020-10-22 | 2023-10-13 | 四川大学 | Multi-mode named entity recognition method based on uncertainty perception |
CN112115443B (en) * | 2020-11-19 | 2021-02-12 | 索信达(北京)数据技术有限公司 | Terminal user authentication method and system |
CN112215228B (en) * | 2020-12-14 | 2021-03-16 | 佛山市南海区广工大数控装备协同创新研究院 | Method for building efficient framework by directly simulating two-stage characteristics |
CN112926479A (en) * | 2021-03-08 | 2021-06-08 | 新疆爱华盈通信息技术有限公司 | Cat face identification method and system, electronic device and storage medium |
ES2948062A1 (en) * | 2022-02-10 | 2023-08-30 | Neurogenesis Ia Tech Sl | Method for creating a model based on deep learning and device for implementing the model created by such method (Machine-translation by Google Translate, not legally binding) |
WO2023192666A1 (en) * | 2022-04-01 | 2023-10-05 | Carnegie Mellon University | System and method for multiview product detection and recognition |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030194132A1 (en) * | 2002-04-10 | 2003-10-16 | Nec Corporation | Picture region extraction method and device |
US20040066966A1 (en) | 2002-10-07 | 2004-04-08 | Henry Schneiderman | Object finder for two-dimensional images, and system for determining a set of sub-classifiers composing an object finder |
US6829384B2 (en) | 2001-02-28 | 2004-12-07 | Carnegie Mellon University | Object finder for photographic images |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP1436611A4 (en) * | 2001-09-26 | 2007-04-11 | Gni Kk | Biological discovery using gene regulatory networks generated from multiple-disruption expression libraries |
US6937745B2 (en) * | 2001-12-31 | 2005-08-30 | Microsoft Corporation | Machine vision system and method for estimating and tracking facial pose |
US7035467B2 (en) * | 2002-01-09 | 2006-04-25 | Eastman Kodak Company | Method and system for processing images for themed imaging services |
US7003158B1 (en) * | 2002-02-14 | 2006-02-21 | Microsoft Corporation | Handwriting recognition with mixtures of Bayesian networks |
US7113185B2 (en) * | 2002-11-14 | 2006-09-26 | Microsoft Corporation | System and method for automatically learning flexible sprites in video layers |
US7451210B2 (en) * | 2003-11-24 | 2008-11-11 | International Business Machines Corporation | Hybrid method for event prediction and system control |
US7848566B2 (en) * | 2004-10-22 | 2010-12-07 | Carnegie Mellon University | Object recognizer and detector for two-dimensional images using bayesian network based classifier |
GB0607143D0 (en) * | 2006-04-08 | 2006-05-17 | Univ Manchester | Method of locating features of an object |
US8379937B1 (en) * | 2008-09-29 | 2013-02-19 | Videomining Corporation | Method and system for robust human ethnicity recognition using image feature-based probabilistic graphical models |
-
2004
- 2004-10-22 US US10/971,868 patent/US7848566B2/en active Active
-
2005
- 2005-10-21 WO PCT/US2005/037825 patent/WO2006047253A1/en active Application Filing
-
2008
- 2008-10-28 US US12/259,371 patent/US8064688B2/en active Active
-
2011
- 2011-11-21 US US13/300,884 patent/US8472706B2/en not_active Expired - Fee Related
-
2013
- 2013-05-24 US US13/901,803 patent/US9213885B1/en active Active
-
2015
- 2015-11-10 US US14/937,851 patent/US9978002B2/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6829384B2 (en) | 2001-02-28 | 2004-12-07 | Carnegie Mellon University | Object finder for photographic images |
US20030194132A1 (en) * | 2002-04-10 | 2003-10-16 | Nec Corporation | Picture region extraction method and device |
US20040066966A1 (en) | 2002-10-07 | 2004-04-08 | Henry Schneiderman | Object finder for two-dimensional images, and system for determining a set of sub-classifiers composing an object finder |
US7194114B2 (en) | 2002-10-07 | 2007-03-20 | Carnegie Mellon University | Object finder for two-dimensional images, and system for determining a set of sub-classifiers composing an object finder |
Non-Patent Citations (13)
Title |
---|
A. Gersho and R.M. Gray, "Vector Quantization and Signal Compression", 1992, Springer, Cover page and pp. 410-423. |
B.D. Ripley, "Pattern Recognition and Neural Networks", 1996, Cambridge University Press, Cover page and pp. 1-89. |
D.H. Foley and J.W. Sammon. Jr., "An Optimal Set of Discriminant Vectors", Jan.-Jun. 1975, IEEE Transactions on Computers , pp. 281-289, vol. C-24, No. 3. |
G. Strang and T. Nguyen, "Wavelets and Filter Banks", 1997, Wellesley-Cambridge Press, pp. 1-35, pp. 103-142, pp. 216-218. |
Pham, Thang V. et al., "Face Detection by Aggregated Bayesian Network Classifiers," Pattern Recognition Letters 24, pp. 451-461, 2002. |
R. Duda, P. Hart, D. Stork, "Pattern Classification", Second Edition, 2001, John Wiley & Sons, Cover page and pp. 1-83. |
R.E. Neapolitan, "Learning Bayesian Networks", 2004, Pearson Prentice Hall, Cover page and pp. 433-502. |
R.E. Schapire and Y. Singer, "Improved Boosting Algorithms Using Confidence-rated Predictions", Dec. 1999, Machine Learning, pp. 1-40. |
Schneiderman, Henry, "Feature-Centric Evaluation for Efficient Cascaded Object Detection," Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1-8, Jun. 27, 2004. |
Schneiderman, Henry, "Learning a Restricted Bayesian Network for Object Detection," Computer Society Conference on Computer Vision and Patern Recognition, pp. 1-8, Jun. 27-Jul. 2, 2004. |
Schneiderman, Henry, "Learning Statistical Structure for Object Detection," Computer Analysis of Images and Patterns, pp. 434-441, Aug. 2003. |
Sung, Kah-Kay et al., "Example-based Learning for View-based Human Face Detection," Image Understanding Workshop, pp. 843-850, Nov. 13, 2004. |
V.N. Vapnik, "The Nature of Statistical Learning Theory", 1995, Springer, Cover page, pp. viii-xv, pp. 1-64. |
Cited By (41)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9978002B2 (en) * | 2004-10-22 | 2018-05-22 | Carnegie Mellon University | Object recognizer and detector for two-dimensional images using Bayesian network based classifier |
US20160125272A1 (en) * | 2004-10-22 | 2016-05-05 | Carnegie Mellon University | Object recognizer and detector for two-dimensional images using bayesian network based classifier |
US8416277B2 (en) * | 2009-12-10 | 2013-04-09 | Apple Inc. | Face detection as a metric to stabilize video during video chat session |
US20110141219A1 (en) * | 2009-12-10 | 2011-06-16 | Apple Inc. | Face detection as a metric to stabilize video during video chat session |
US20120050259A1 (en) * | 2010-08-31 | 2012-03-01 | Apple Inc. | Systems, methods, and computer-readable media for efficiently processing graphical data |
US9582920B2 (en) * | 2010-08-31 | 2017-02-28 | Apple Inc. | Systems, methods, and computer-readable media for efficiently processing graphical data |
US9128737B2 (en) | 2011-10-18 | 2015-09-08 | Google Inc. | Dynamic profile switching based on user identification |
US9690601B2 (en) | 2011-10-18 | 2017-06-27 | Google Inc. | Dynamic profile switching based on user identification |
US8542879B1 (en) | 2012-06-26 | 2013-09-24 | Google Inc. | Facial recognition |
US9117109B2 (en) | 2012-06-26 | 2015-08-25 | Google Inc. | Facial recognition |
US8798336B2 (en) | 2012-06-26 | 2014-08-05 | Google Inc. | Facial recognition |
EP2680192A2 (en) | 2012-06-26 | 2014-01-01 | Google Inc. | Facial recognition |
EP2680191A2 (en) | 2012-06-26 | 2014-01-01 | Google Inc. | Facial recognition |
US8457367B1 (en) | 2012-06-26 | 2013-06-04 | Google Inc. | Facial recognition |
US8411909B1 (en) | 2012-06-26 | 2013-04-02 | Google Inc. | Facial recognition |
US8856541B1 (en) | 2013-01-10 | 2014-10-07 | Google Inc. | Liveness detection |
US20140270482A1 (en) * | 2013-03-15 | 2014-09-18 | Sri International | Recognizing Entity Interactions in Visual Media |
US10121076B2 (en) * | 2013-03-15 | 2018-11-06 | Sri International | Recognizing entity interactions in visual media |
US9330296B2 (en) * | 2013-03-15 | 2016-05-03 | Sri International | Recognizing entity interactions in visual media |
US20160247023A1 (en) * | 2013-03-15 | 2016-08-25 | Sri International | Recognizing entity interactions in visual media |
US9582490B2 (en) | 2013-07-12 | 2017-02-28 | Microsoft Technolog Licensing, LLC | Active labeling for computer-human interactive learning |
US10372815B2 (en) | 2013-07-12 | 2019-08-06 | Microsoft Technology Licensing, Llc | Interactive concept editing in computer-human interactive learning |
US9430460B2 (en) | 2013-07-12 | 2016-08-30 | Microsoft Technology Licensing, Llc | Active featuring in computer-human interactive learning |
US9779081B2 (en) | 2013-07-12 | 2017-10-03 | Microsoft Technology Licensing, Llc | Feature completion in computer-human interactive learning |
US11023677B2 (en) | 2013-07-12 | 2021-06-01 | Microsoft Technology Licensing, Llc | Interactive feature selection for training a machine learning system and displaying discrepancies within the context of the document |
US9355088B2 (en) | 2013-07-12 | 2016-05-31 | Microsoft Technology Licensing, Llc | Feature completion in computer-human interactive learning |
US9489373B2 (en) | 2013-07-12 | 2016-11-08 | Microsoft Technology Licensing, Llc | Interactive segment extraction in computer-human interactive learning |
US10528803B2 (en) | 2015-03-13 | 2020-01-07 | Nec Corporation | Living body detection device, living body detection method, and recording medium |
US20180053044A1 (en) * | 2015-03-13 | 2018-02-22 | Nec Corporation | Living body detection device, living body detection method, and recording medium |
US10509955B2 (en) | 2015-03-13 | 2019-12-17 | Nec Corporation | Living body detection device, living body detection method, and recording medium |
US11132536B2 (en) | 2015-03-13 | 2021-09-28 | Nec Corporation | Living body detection device, living body detection method, and recording medium |
US10546190B2 (en) | 2015-03-13 | 2020-01-28 | Nec Corporation | Living body detection device, living body detection method, and recording medium |
US10546189B2 (en) * | 2015-03-13 | 2020-01-28 | Nec Corporation | Living body detection device, living body detection method, and recording medium |
CN104778670A (en) * | 2015-04-17 | 2015-07-15 | 广西科技大学 | Fractal-wavelet self-adaption image denoising method based on multivariate statistical model |
US10664727B2 (en) * | 2016-04-01 | 2020-05-26 | Fuji Xerox Co., Ltd. | Image pattern recognition device and recording medium |
US10789455B2 (en) | 2016-08-23 | 2020-09-29 | Samsung Electronics Co., Ltd. | Liveness test method and apparatus |
US10121059B2 (en) | 2016-08-23 | 2018-11-06 | Samsung Electronics Co., Ltd. | Liveness test method and apparatus |
US11783639B2 (en) | 2016-08-23 | 2023-10-10 | Samsung Electronics Co., Ltd. | Liveness test method and apparatus |
CN108257079A (en) * | 2016-12-29 | 2018-07-06 | 北京国双科技有限公司 | Graphic change method and device |
US11176392B2 (en) | 2017-03-27 | 2021-11-16 | Samsung Electronics Co., Ltd. | Liveness test method and apparatus |
US11721131B2 (en) | 2017-03-27 | 2023-08-08 | Samsung Electronics Co., Ltd. | Liveness test method and apparatus |
Also Published As
Publication number | Publication date |
---|---|
US20160125272A1 (en) | 2016-05-05 |
US20060088207A1 (en) | 2006-04-27 |
US9978002B2 (en) | 2018-05-22 |
US9213885B1 (en) | 2015-12-15 |
US20120106857A1 (en) | 2012-05-03 |
US7848566B2 (en) | 2010-12-07 |
US8472706B2 (en) | 2013-06-25 |
US20090067730A1 (en) | 2009-03-12 |
WO2006047253A1 (en) | 2006-05-04 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9978002B2 (en) | Object recognizer and detector for two-dimensional images using Bayesian network based classifier | |
US7194114B2 (en) | Object finder for two-dimensional images, and system for determining a set of sub-classifiers composing an object finder | |
US6829384B2 (en) | Object finder for photographic images | |
Schneiderman et al. | Object detection using the statistics of parts | |
Garcia et al. | Convolutional face finder: A neural architecture for fast and robust face detection | |
Amit et al. | Pop: Patchwork of parts models for object recognition | |
US8107726B2 (en) | System and method for class-specific object segmentation of image data | |
US7840059B2 (en) | Object recognition using textons and shape filters | |
US7058209B2 (en) | Method and computer program product for locating facial features | |
US7903883B2 (en) | Local bi-gram model for object recognition | |
US7369682B2 (en) | Adaptive discriminative generative model and application to visual tracking | |
US20160140425A1 (en) | Method and apparatus for image classification with joint feature adaptation and classifier learning | |
US20040086185A1 (en) | Method and system for multiple cue integration | |
US20080025568A1 (en) | System and method for detecting still objects in images | |
US20080025596A1 (en) | System and Method for Machine Learning using a Similarity Inverse Matrix | |
US20050105795A1 (en) | Classification in likelihood spaces | |
US20110293173A1 (en) | Object Detection Using Combinations of Relational Features in Images | |
Kenduiywo et al. | Detection of built-up area in optical and synthetic aperture radar images using conditional random fields | |
Ghosh et al. | A survey on remote sensing scene classification algorithms | |
Dalara et al. | Entity Recognition in Indian Sculpture using CLAHE and machine learning | |
Crandall et al. | Object recognition by combining appearance and geometry | |
Hegerath et al. | Patch-based object recognition | |
Maryan | Detecting Rip Currents from Images | |
Bhatia | Hierarchical Charged Particle Filter for Multiple Target Tracking | |
Todorovic | Statistical modeling and segmentation of sky/ground images |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
FEPP | Fee payment procedure |
Free format text: PAT HOLDER NO LONGER CLAIMS SMALL ENTITY STATUS, ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: STOL); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044695/0115Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 12 |