JP2020507857A - Agent navigation using visual input - Google Patents
Agent navigation using visual input Download PDFInfo
- Publication number
- JP2020507857A JP2020507857A JP2019543104A JP2019543104A JP2020507857A JP 2020507857 A JP2020507857 A JP 2020507857A JP 2019543104 A JP2019543104 A JP 2019543104A JP 2019543104 A JP2019543104 A JP 2019543104A JP 2020507857 A JP2020507857 A JP 2020507857A
- Authority
- JP
- Japan
- Prior art keywords
- characterization
- time step
- environment
- map
- value
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000000007 visual effect Effects 0.000 title abstract description 5
- 238000012512 characterization method Methods 0.000 claims abstract description 141
- 238000012545 processing Methods 0.000 claims abstract description 41
- 238000000034 method Methods 0.000 claims abstract description 27
- 238000013507 mapping Methods 0.000 claims abstract description 13
- 230000009471 action Effects 0.000 claims description 74
- 238000013528 artificial neural network Methods 0.000 claims description 30
- 230000008569 process Effects 0.000 claims description 19
- 230000000306 recurrent effect Effects 0.000 claims description 9
- 238000012549 training Methods 0.000 claims description 6
- 230000033001 locomotion Effects 0.000 claims description 5
- 238000005070 sampling Methods 0.000 claims description 5
- 238000011156 evaluation Methods 0.000 claims description 4
- 230000001902 propagating effect Effects 0.000 claims 1
- 238000004590 computer program Methods 0.000 abstract description 13
- 239000003795 chemical substances by application Substances 0.000 description 67
- 230000006870 function Effects 0.000 description 23
- 230000015654 memory Effects 0.000 description 6
- 230000004044 response Effects 0.000 description 6
- 238000004891 communication Methods 0.000 description 5
- 238000010801 machine learning Methods 0.000 description 5
- 230000003993 interaction Effects 0.000 description 3
- 230000004913 activation Effects 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 241000009334 Singa Species 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 238000011176 pooling Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 230000006886 spatial memory Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
- G06T11/60—Editing figures and text; Combining figures or text
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05B—CONTROL OR REGULATING SYSTEMS IN GENERAL; FUNCTIONAL ELEMENTS OF SUCH SYSTEMS; MONITORING OR TESTING ARRANGEMENTS FOR SUCH SYSTEMS OR ELEMENTS
- G05B13/00—Adaptive control systems, i.e. systems automatically adjusting themselves to have a performance which is optimum according to some preassigned criterion
- G05B13/02—Adaptive control systems, i.e. systems automatically adjusting themselves to have a performance which is optimum according to some preassigned criterion electric
- G05B13/0265—Adaptive control systems, i.e. systems automatically adjusting themselves to have a performance which is optimum according to some preassigned criterion electric the criterion being a learning criterion
- G05B13/027—Adaptive control systems, i.e. systems automatically adjusting themselves to have a performance which is optimum according to some preassigned criterion electric the criterion being a learning criterion using neural networks only
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/38—Electronic maps specially adapted for navigation; Updating thereof
- G01C21/3863—Structures of map data
- G01C21/387—Organisation of map data, e.g. version management or database structures
- G01C21/3878—Hierarchical structures, e.g. layering
-
- G06T3/18—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/55—Depth or shape recovery from multiple images
- G06T7/579—Depth or shape recovery from multiple images from motion
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/10—Terrestrial scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20084—Artificial neural networks [ANN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30244—Camera pose
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30248—Vehicle exterior or interior
- G06T2207/30252—Vehicle exterior; Vicinity of vehicle
Abstract
視覚入力を使用したナビゲーションのための、コンピュータ記憶媒体上に符号化されたコンピュータプログラムを含む方法、システム、および装置。システムのうちの１つは、複数の時間ステップの各時間ステップにおいて、その時間ステップにおける環境の画像から環境の特徴付けを生成するように構成されたマッピングサブシステムを含み、特徴付けは、特定の特徴を有する環境内の場所を識別する環境マップを含み、特徴付けを生成することは、時間ステップごとに、その時間ステップにおける環境の画像を取得することと、その時間ステップについての第１の初期特徴付けを生成するために画像を処理することと、先行する時間ステップについての第１の特徴付けを取得することと、その時間ステップについての第２の初期特徴付けを生成するために、先行する時間ステップについての特徴付けを処理することと、第１の初期特徴付けと第２の初期特徴付けとを結合して、その時間ステップについての最終的な特徴付けを生成することとを含む。Methods, systems, and devices, including computer programs encoded on computer storage media, for navigation using visual input. One of the systems includes, at each time step of the plurality of time steps, a mapping subsystem configured to generate an environment characterization from an image of the environment at that time step, wherein the characterization comprises a specific characterization. Generating a characterization, including an environment map identifying locations within the environment having features, comprises, for each time step, obtaining an image of the environment at that time step and a first initial value for the time step. Processing the image to generate a characterization, obtaining a first characterization for the preceding time step, and leading to generate a second initial characterization for the time step Processing the characterization for the time step and combining the first and second initial characterizations to combine the time And generating a final characterization of steps.
Description
本明細書は、環境内のエージェントナビゲーションに関する。 This specification relates to agent navigation in an environment.
視覚入力を使用して、環境内で、たとえばロボットなどのエージェントをナビゲートすることは、環境に関する情報を抽出するために視覚入力を処理し、環境に関する抽出された情報が与えられると適切なアクションを選択することを必要とする。いくつかのシステムは、ニューラルネットワークを使用して、エージェントによって実行されるべきアクションを選択する。 Navigating an agent, such as a robot, in an environment using visual input involves processing the visual input to extract information about the environment, and taking appropriate action given the extracted information about the environment. Need to choose. Some systems use neural networks to select actions to be performed by agents.
ニューラルネットワークは、非線形ユニットの１つまたは複数の層を使用して、受信された入力の出力を予測する機械学習モデルである。いくつかのニューラルネットワークは、出力層に加えて、１つまたは複数の隠れ層を含む。各隠れ層の出力は、ネットワーク内の次の層、すなわち、次の隠れ層または出力層への入力として使用される。ネットワークの各層は、パラメータのそれぞれのセットの現在の値に従って、受信された入力から出力を生成する。 Neural networks are machine learning models that use one or more layers of nonlinear units to predict the output of a received input. Some neural networks include one or more hidden layers in addition to the output layer. The output of each hidden layer is used as input to the next layer in the network, the next hidden or output layer. Each layer of the network produces an output from the received input according to the current value of the respective set of parameters.
いくつかのニューラルネットワークは、リカレントニューラルネットワークである。リカレントニューラルネットワークは、入力シーケンスを受信し、その入力シーケンスから出力シーケンスを生成するニューラルネットワークである。特に、リカレントニューラルネットワークは、現在の時間ステップで出力を計算する際に、先行する時間ステップからのネットワークの内部状態の一部または全部を使用することができる。リカレントニューラルネットワークの一例は、１つまたは複数の長短期（ＬＳＴＭ）メモリブロックを含むＬＳＴＭニューラルネットワークである。各ＬＳＴＭメモリブロックは、たとえば、現在のアクティブ化を生成する際に使用するために、またはＬＳＴＭニューラルネットワークの他の構成要素に提供されるように、セルがセルの以前の状態を記憶することを可能にする入力ゲート、忘却ゲート、および出力ゲートを各々含む１つまたは複数のセルを含むことができる。 Some neural networks are recurrent neural networks. A recurrent neural network is a neural network that receives an input sequence and generates an output sequence from the input sequence. In particular, the recurrent neural network can use some or all of the internal state of the network from the preceding time step in calculating the output at the current time step. One example of a recurrent neural network is an LSTM neural network that includes one or more long and short term (LSTM) memory blocks. Each LSTM memory block indicates that the cell stores the previous state of the cell, for example, for use in generating a current activation or as provided to other components of the LSTM neural network. It may include one or more cells, each including an enabling gate, a forgetting gate, and an output gate.
本明細書は、一般に、環境内をナビゲートするエージェントを制御する、すなわち、エージェントによって実行されるべきアクションを選択し、次いで、選択されたアクションをエージェントに実行させるナビゲーションシステムを記述する。 This specification generally describes a navigation system that controls an agent navigating in an environment, ie, selects an action to be performed by an agent, and then causes the agent to perform the selected action.
記述された主題の発明的一態様では、１つまたは複数のコンピュータと１つまたは複数の記憶デバイスとを備えるシステムは、実行されると、１つまたは複数のコンピュータにマッピングサブシステムを実装させる命令を記憶し、マッピングサブシステムは、複数の時間ステップの各時間ステップにおいて、環境の特徴付けをその時間ステップにおける環境の画像から生成するように構成され、特徴付けは、特定の特徴を有する環境内の場所を識別する環境マップを含み、特徴付けを生成することは、時間ステップごとに、その時間ステップにおける環境の画像を取得することと、その時間ステップについての環境の第１の初期特徴付けを生成するために画像を処理することと、その時間ステップに先立つ先行する時間ステップについての環境の最終的な特徴付けを取得することと、その時間ステップについての環境の第２の初期特徴付けを生成するために、先行する時間ステップについての環境の特徴付けを処理することと、第１の初期特徴付けと第２の初期特徴付けとを結合して、その時間ステップについての環境の最終的な特徴付けを生成することとを含む。 In one inventive aspect of the described subject matter, a system comprising one or more computers and one or more storage devices, when executed, causes the one or more computers to implement a mapping subsystem. Wherein the mapping subsystem is configured to generate, at each time step of the plurality of time steps, a characterization of the environment from an image of the environment at that time step, wherein the characterization is performed within the environment having particular characteristics. Generating an characterization including, for each time step, obtaining an image of the environment at that time step and a first initial characterization of the environment for that time step. Processing the image to produce it and a loop over the preceding time step that precedes it Obtaining a final characterization of the environment, processing a characterization of the environment for a preceding time step to generate a second initial characterization of the environment for that time step, Combining the initial characterization and the second initial characterization to produce a final characterization of the environment for that time step.
いくつかの実装形態では、第１の初期特徴付けを生成するために画像を処理することは、第１の初期特徴付けを生成するために、ニューラルネットワークを使用して画像を処理することを含む。 In some implementations, processing the image to generate a first initial characterization includes processing the image using a neural network to generate the first initial characterization. .
いくつかの実装形態では、画像は、環境内を移動するエージェントによってキャプチャされた画像であり、その時間ステップについての環境の第２の初期特徴付けを生成するために、先行する時間ステップについての環境の最終的な特徴付けを処理することは、先行する時間ステップとその時間ステップとの間のエージェントの動きの尺度を取得することと、第２の初期特徴付けを生成するために、先行する時間ステップについての環境の最終的な特徴付けおよび動きの尺度にワーピング関数を適用することとを含む。 In some implementations, the image is an image captured by an agent traveling in the environment and the environment for a preceding time step is generated to generate a second initial characterization of the environment for that time step. Processing the final characterization of the first time step comprises obtaining a measure of the agent's movement between the preceding time step and the previous time step, and generating the second initial characterization by the preceding time step. Final characterization of the environment for the step and applying a warping function to the measure of motion.
いくつかの実装形態では、ワーピング関数は、双線形サンプリングを使用して補間を実行する関数である。 In some implementations, the warping function is a function that performs interpolation using bilinear sampling.
いくつかの実装形態では、画像は、環境内を移動するエージェントによってキャプチャされ、画像は、エージェントの自己中心的視点からのものであり、第１の初期特徴付け、第２の初期特徴付け、先行する時間ステップについての最終的な特徴付け、およびその時間ステップについての最終的な特徴付けは、トップダウンの視点からのものである。 In some implementations, the image is captured by an agent moving in the environment, and the image is from an agent's self-centered perspective and includes a first initial characterization, a second initial characterization, The final characterization of the time step to perform, and the final characterization of that time step, are from a top-down perspective.
いくつかの実装形態では、第１の初期特徴付けと第２の初期特徴付けとを結合して、その時間ステップについての最終的な特徴付けを生成することは、第１の初期特徴付けおよび第２の初期特徴付けに更新関数を適用して、最終的な特徴付けを生成することを含む。 In some implementations, combining the first initial characterization and the second initial characterization to produce a final characterization for the time step comprises: Applying an update function to the two initial characterizations to produce a final characterization.
いくつかの実装形態では、各特徴付けは、（ｉ）環境内の複数の場所が特定の特徴を有するかどうかを表すスコアのセット、および（ｉｉ）スコアのセットにおける信頼の尺度のセットを含み、更新関数は、以下の式の演算を実行することを含む。 In some implementations, each characterization includes (i) a set of scores that indicate whether multiple locations in the environment have particular characteristics, and (ii) a set of measures of confidence in the set of scores. , The update function includes performing the operation of the following equation:
Ｃｔ＝Ｃｔ−１＋Ｃｔ’
ただし、ｆｔは、時間ステップについての最終的な特徴付けに対するスコアのセットであり、ｃｔは、時間ステップについての最終的な特徴付けに対するスコアのセットにおける信頼の尺度のセットであり、ｆｔ’は、第１の初期特徴付けに対するスコアのセットであり、ｃｔ’は、第１の初期特徴付けに対するスコアのセットにおける信頼の尺度のセットであり、ｆｔ−１は、第２の初期特徴付けに対するスコアのセットであり、ｃｔ−１は、第２の初期特徴付けに対するスコアのセットにおける信頼の尺度のセットである。
Ct = Ct-1 + Ct '
However, f t is a set of scores for the final characterization for time step, c t is a set of final measure of confidence in the set of scores for the characterization of the time step, f t 'Is the set of scores for the first initial characterization, ct ' is the set of measures of confidence in the set of scores for the first initial characterization, and ft-1 is the second initial A set of scores for the characterization, ct-1 is a set of measures of confidence in the set of scores for the second initial characterization.
いくつかの実装形態では、更新関数は、リカレントニューラルネットワークによって実行され、リカレントニューラルネットワークは、複数の時間ステップの時間ステップごとに、その時間ステップについての第１の初期特徴付けおよび第２の初期特徴付けを処理して、その時間ステップについての最終的な特徴付けを生成するように構成される。 In some implementations, the update function is performed by a recurrent neural network, wherein the recurrent neural network includes, for each time step of the plurality of time steps, a first initial characterization and a second initial characteristic for that time step. Configured to process the tagging to generate a final characterization for that time step.
いくつかの実装形態では、時間ステップについての環境マップは、環境内の複数の場所の各々について、その場所が特定の特徴を有するかどうかを表すスコアを含み、特徴付けは、場所ごとに、場所についてのスコアにおける信頼の尺度をさらに含む。 In some implementations, the environment map for the time step includes, for each of the plurality of locations in the environment, a score that indicates whether the location has particular characteristics, and the characterization includes, for each location, Further includes a measure of confidence in the score for.
いくつかの実装形態では、システムは、複数の時間ステップの各々について、マッピングサブシステムから時間ステップについての最終的な特徴付けを取得することと、時間ステップにおいて環境と対話するエージェントによって実行されるべき提案されたアクションを選択するために、最終的な特徴付けを処理することとをさらに含む。 In some implementations, for each of a plurality of time steps, the system should obtain the final characterization of the time step from the mapping subsystem and be performed by an agent that interacts with the environment at the time step Processing the final characterization to select the proposed action.
別の発明的態様では、システムは、計画サブシステムを含み、計画サブシステムは、複数の時間ステップの時間ステップごとに、その時間ステップについての目標を達成するためにアクションを実行するエージェントの環境の最終的な特徴付けを取得し、エージェントの環境の最終的な特徴付けは、特定の特徴を有する環境内の場所を識別する環境マップを含み、最終的な特徴付けから空間的にスケーリングされた環境マップのシーケンスを生成し、シーケンス内の各空間的にスケーリングされた環境マップは、シーケンス内の任意の後続の空間的にスケーリングされた環境マップに対してダウンサンプリングされ、最もダウンサンプリングされた空間的にスケーリングされた環境マップから始まる、シーケンス内の空間的にスケーリングされた環境マップごとに、シーケンス内の空間的にスケーリングされた環境マップについての１つまたは複数の値マップを生成するために、空間的にスケーリングされた環境マップおよび空間的にスケーリングされた環境マップに関連付けられた１つまたは複数の入力を処理し、値マップは、空間的にスケーリングされた環境マップ内の複数の領域の領域ごとに、目標を達成するためにその領域内にいるエージェントの値の評価を含み、最後の空間的にスケーリングされた環境マップ以外のシーケンス内の空間的にスケーリングされた環境マップごとに、シーケンス内の後続の空間的にスケーリングされた環境マップに関連付けられた入力として、空間的にスケーリングされた環境マップについての１つまたは複数の更新された値マップを提供し、シーケンス内の最後の空間的にスケーリングされた環境マップについての更新された値マップに基づいて、提案されたアクションを選択するように構成される。 In another inventive aspect, a system includes a planning subsystem, wherein for each time step of a plurality of time steps, the planning subsystem includes an agent environment that performs an action to achieve a goal for that time step. Obtaining the final characterization, the final characterization of the agent's environment includes an environment map identifying locations within the environment with particular characteristics, and a spatially scaled environment from the final characterization Generate a sequence of maps, each spatially scaled environment map in the sequence is downsampled to any subsequent spatially scaled environment map in the sequence, and the most downsampled spatial map Spatially scaled in the sequence, starting with the scaled environment map For each spatial environment map, the spatially scaled environment map and the spatially scaled environment map to generate one or more value maps for the spatially scaled environment map in the sequence. Processing the associated one or more inputs, the value map provides, for each region of the plurality of regions in the spatially scaled environment map, a value of an agent's value within that region to achieve a goal. For each spatially scaled environment map in the sequence that includes an evaluation and other than the last spatially scaled environment map, as input associated with a subsequent spatially scaled environment map in the sequence, Provides one or more updated value maps for a spatially scaled environment map , Based on the updated value map for the last spatially scaled environment map in the sequence, configured to select the proposed actions.
いくつかの実装形態では、シーケンス内の各空間的にスケーリングされた環境マップに関連付けられた入力は、エージェントの目標を識別するデータを含む。 In some implementations, the input associated with each spatially scaled environment map in the sequence includes data identifying an agent goal.
いくつかの実装形態では、目標は、エージェントが到達しようと試みる環境内の場所である。 In some implementations, the goal is a location in the environment that the agent attempts to reach.
いくつかの実装形態では、目標は、エージェントが見つけようと試みる環境内のオブジェクトである。 In some implementations, the goal is an object in the environment that the agent attempts to find.
いくつかの実装形態では、最もダウンサンプリングされた空間的にスケーリングされた環境マップへの入力は、その時間ステップについての最終的な特徴付けを含む。 In some implementations, the input to the most downsampled spatially scaled environment map includes a final characterization for that time step.
いくつかの実装形態では、空間的にスケーリングされた環境マップについての１つまたは複数の値マップを生成するために、空間的にスケーリングされた環境マップに関連付けられた入力を処理することは、空間的にスケーリングされた環境マップについての１つまたは複数の初期値マップを生成するために、空間的にスケーリングされた環境マップに関連付けられた入力を処理することと、空間的にスケーリングされた環境マップについての１つまたは複数の値マップを生成するために、空間的にスケーリングされた環境マップについての１つまたは複数の初期値マップに対して１つまたは複数の値反復（ｖａｌｕｅ ｉｔｅｒａｔｉｏｎ）を実行することとを含む。 In some implementations, processing the input associated with the spatially scaled environment map to generate one or more value maps for the spatially scaled environment map comprises: Processing inputs associated with the spatially scaled environment map to generate one or more initial value maps for the spatially scaled environment map; and a spatially scaled environment map. Perform one or more value iterations on one or more initial value maps for the spatially scaled environment map to generate one or more value maps for. Including.
いくつかの実装形態では、シーケンス内の最後の空間的にスケーリングされた環境マップについての値マップに基づいて、提案されたアクションを選択することは、シーケンス内の最後の空間的にスケーリングされた環境マップについての更新された値マップから、値の最高の評価を有する更新された値マップの特定の領域を決定することと、その時間ステップについての提案されたアクションとして、特定の領域にエージェントを連れて行くアクションを選択することとを含む。 In some implementations, selecting a proposed action based on a value map for the last spatially scaled environment map in the sequence comprises selecting the last spatially scaled environment map in the sequence. From the updated value map for the map, determine the specific area of the updated value map with the highest rating of the value and take the agent to the specific area as a suggested action for that time step Selecting the action to take.
いくつかの実装形態では、シーケンス内の最後の空間的にスケーリングされた環境マップについての値マップに基づいて、提案されたアクションを選択することは、提案されたアクションを選択するために１つまたは複数の修正された線形ユニットを含むニューラルネットワークを介して値マップを処理することを含む。 In some implementations, selecting a suggested action based on a value map for the last spatially scaled environment map in the sequence comprises selecting one or more to select the suggested action. Processing the value map via a neural network including a plurality of modified linear units.
本明細書に記載される主題の特定の実施形態は、以下の利点のうちの１つまたは複数を実現するように実装することができる。 Certain embodiments of the subject matter described herein can be implemented to achieve one or more of the following advantages.
マッピングサブシステムおよび計画サブシステムを含む環境内でエージェントをナビゲートするためのシステムは、エンドツーエンドでトレーニングすることができ、それによって、トレーニングされた後のそのようなシステムの効率および精度を高めることができる。すなわち、マッパおよびプランナーによって実行される動作は完全に微分可能であるため、マッパは、プランナーからの誤差を逆伝搬することによってトレーニングできる。たとえば、計画サブシステムは、マッピングサブシステムがより良い環境マップを生成する方法を学習している間に、マッピングサブシステムによって生成された環境マップの使用方法を学習することができる。環境マップを空間的にスケーリングして、よりダウンサンプリングされた空間的にスケーリングされた環境マップの値反復精度を最小化しながら、高精度のアクション計画を依然として生成することによって、エージェントのアクションの計画は、より計算効率的にすることができる。 A system for navigating agents in an environment that includes a mapping subsystem and a planning subsystem can be trained end-to-end, thereby increasing the efficiency and accuracy of such a system after being trained be able to. That is, since the operations performed by the mapper and the planner are fully differentiable, the mapper can be trained by backpropagating errors from the planner. For example, the planning subsystem can learn how to use the environment map generated by the mapping subsystem while the mapping subsystem is learning how to generate a better environment map. By spatially scaling the environment map and minimizing the value repetition accuracy of the more downsampled spatially scaled environment map, the agent's action plan is still generated by generating a highly accurate action plan. , Can be more computationally efficient.
本明細書に記載の主題の１つまたは複数の実施形態の詳細は、添付の図面および以下の説明に記載されている。主題の他の特徴、態様、および利点は、説明、図面、および特許請求の範囲から明らかになるであろう。 The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will be apparent from the description, drawings, and claims.
様々な図面における同様の参照番号および名称は、同様の要素を示す。 Like reference numbers and designations in the various figures indicate like elements.
図１は、例示的なナビゲーションシステム１００を示す。ナビゲーションシステム１００は、以下に説明するシステム、構成要素、および技法を実装することができる１つまたは複数の場所にある１つまたは複数のコンピュータ上のコンピュータプログラムとして実装されるシステムの一例である。 FIG. 1 shows an exemplary navigation system 100. Navigation system 100 is one example of a system implemented as a computer program on one or more computers at one or more locations where the systems, components, and techniques described below can be implemented.
ナビゲーションシステム１００は、環境の画像を受信し、受信された画像に応答してエージェントによって実行されるべきアクションを選択することによって、エージェントが環境内を移動するときにエージェントを制御する。 Navigation system 100 controls the agent as it moves through the environment by receiving images of the environment and selecting actions to be performed by the agent in response to the received images.
一般に、環境は、現実世界環境であり、エージェントは、目標を達成するために現実世界環境と対話する機械エージェントである。たとえば、エージェントは、特定の目標を達成するために、たとえば環境内の特定の場所に移動するために、または環境内の特定のオブジェクトを見つけるために、環境と対話するロボットとすることができる。別の例として、エージェントは、目標を達成するために環境内をナビゲートする、すなわち特定の場所にナビゲートする自律型または半自律型車両でもよい。 Generally, the environment is a real-world environment, and the agents are machine agents that interact with the real-world environment to achieve goals. For example, an agent may be a robot that interacts with an environment to achieve a particular goal, for example, to move to a particular location in the environment or to find a particular object in the environment. As another example, an agent may be an autonomous or semi-autonomous vehicle that navigates within the environment to achieve a goal, ie, navigates to a particular location.
特に、ナビゲーションシステム１００は、環境の画像を繰り返し受信し、各画像に応答して、可能なアクションのセットからエージェントによって実行されるべきアクションを選択し、エージェントに、選択されたアクションを実行させる。画像を受信し、次いで、画像に応答してアクションを実行することの各反復は、本明細書では時間ステップと呼ばれる。このようにしてアクションを繰り返し選択することによって、システム１００は、エージェントに目標を達成させる。 In particular, the navigation system 100 repeatedly receives images of the environment and, in response to each image, selects an action to be performed by the agent from a set of possible actions, and causes the agent to perform the selected action. Each iteration of receiving an image and then performing an action in response to the image is referred to herein as a time step. By repeatedly selecting an action in this manner, the system 100 causes the agent to achieve the goal.
一般に、受信された画像は、エージェントが環境内を移動するにつれてエージェントによってキャプチャされた画像、すなわち、カメラまたはエージェントの他のセンサーによってキャプチャされた画像である。したがって、画像は、エージェントに対する環境の１人称画像である。 Generally, the received image is an image captured by the agent as the agent moves through the environment, ie, an image captured by a camera or other sensor of the agent. Thus, the image is a first person image of the environment for the agent.
たとえば、所与の時間ステップにおいて、システム１００は、環境の画像１０２と、先行する時間ステップに対するエージェントのエゴモーション１０４を識別するデータとを受信する。エゴモーション１０４は、先行する時間ステップとその時間ステップとの間のエージェントの動きの尺度、すなわち、エージェントの場所が先行する時間ステップに対してどの程度変化したかを示す尺度である。 For example, at a given time step, the system 100 receives an image 102 of the environment and data identifying the ego-motion 104 of the agent for the preceding time step. The egomotion 104 is a measure of the agent's movement between the preceding time step and that time step, ie, a measure of how much the agent's location has changed relative to the preceding time step.
次いで、システム１００は、画像１０２、エゴモーション１０４、およびエージェントが到達しようとしている目標１２２を識別するデータを使用して、その時間ステップについてのアクション１３２を選択し、すなわち、アクション１３２に対応する特定の制御入力を提出するようエージェントの制御システムに命令することによって、エージェントにアクション１３２を実行させる。すなわち、アクションは、エージェントに環境内を移動させるエージェントへの制御入力である。 The system 100 then selects an action 132 for that time step using the image 102, the egomotion 104, and data identifying the goal 122 that the agent is trying to reach, i.e., identifying the action 132 By instructing the agent's control system to submit its control input. That is, an action is a control input to an agent that causes the agent to move through the environment.
特に、システム１００は、微分可能マッパ１１０と微分可能階層プランナー１２０とを含む。 In particular, the system 100 includes a differentiable mapper 110 and a differentiable hierarchy planner 120.
以下の説明からわかるように、マッパ１１０およびプランナー１２０は、所与の画像に応答してアクションを選択する間にこれらのサブシステムの各々によって実行される動作が完全に微分可能であるので、「微分可能」と呼ばれる。これによって、システム１００は、マッパ１１０とプランナー１２０とをエンドツーエンドでトレーニングすることができ、トレーニングされたシステムの性能が向上する。マッパ１１０およびプランナー１２０のトレーニングについては、図４を参照しながら以下でより詳細に説明する。 As can be seen from the description below, the mapper 110 and the planner 120 are fully differentiable because the actions performed by each of these subsystems while selecting an action in response to a given image are " It is called "differentiable." This allows the system 100 to train the mapper 110 and the planner 120 end-to-end, improving the performance of the trained system. Training of the mapper 110 and the planner 120 will be described in more detail below with reference to FIG.
各時間ステップにおいて、マッパ１１０は、その時間ステップについての画像１０２およびエゴモーション１０４を受信し、環境の特徴付け１１２を生成する。 At each time step, mapper 110 receives image 102 and egomotion 104 for that time step and generates an environment characterization 112.
特徴付け１１２は、一般に、確信度データ（ｂｅｌｉｅｆ ｄａｔａ）および信頼度データ（ｃｏｎｆｉｄｅｎｃｅ ｄａｔａ）を含む。 The characterization 112 generally includes belief data and confidence data.
確信度データは、プランナー１２０にとって有用となるであろう何らかの特徴を有する環境内の場所を識別する環境のマップである。特に、マッパ１１０がトレーニングされる方法のために、特定の特徴は所定のものではなく学習される。すなわち、マッパはプランナー１２０からの誤差を逆伝搬することによってトレーニングされるので、マップは、環境の潜在的表現になる。以下で説明するように、マップは、学習されたプランナー１２０に直接供給されるので、マップは、環境の所定の特徴を符号化する必要はなく、代わりに、プランナー１２０によって使用される一般的な空間メモリとし、すなわち、環境内の様々な場所のいくつかの特徴を符号化するメモリとして機能することができる。 Confidence data is a map of the environment that identifies locations within the environment that have some characteristics that will be useful to planner 120. In particular, because of the way the mapper 110 is trained, certain features are learned rather than predetermined. That is, because the mapper is trained by backpropagating the error from the planner 120, the map becomes a potential representation of the environment. As described below, since the map is provided directly to the trained planner 120, the map does not need to encode certain features of the environment, instead the general It can be a spatial memory, that is, a memory that encodes some features at various locations in the environment.
より具体的には、マップは、環境のトップダウンビューからのものであり、トップダウンビュー内の複数の場所の各々について、その場所が特定の特徴を有する可能性がどの程度高いかを示すそれぞれのスコアを含む。 More specifically, the map is from a top-down view of the environment, and for each of a plurality of locations in the top-down view, each indicates how likely that location is to have a particular characteristic. Including the score of.
信頼度データは、確信度データが正確であることにマッパ１１０がどの程度自信を持っているかを識別するデータである。たとえば、信頼度データは、マップ内の場所の各々について、マッパ１１０がその場所についてどの程度自信を持っているかを表す信頼度スコアを含むことができる。 The reliability data is data that identifies how confident the mapper 110 is that the confidence data is accurate. For example, the confidence data may include, for each location in the map, a confidence score that indicates how confident the mapper 110 is about that location.
マッパ１１０および特徴付けを生成することについては、図２を参照しながら以下でより詳細に説明する。 Generating the mapper 110 and characterization is described in more detail below with reference to FIG.
プランナー１２０は、特徴付け１１２を受信し、エージェントが特徴付け１１２に基づいて目標１２２を達成するために、エージェントによって実行されるべきアクション１３２を選択する。一般に、プランナー１２０は、エージェントによって実行されるべきアクションを選択するために、特徴付け１１２を使用して複数のスケールで値反復を実行する。プランナー１２０、および特徴付けに基づいてアクションを選択することについては、図３を参照しながら以下でより詳細に説明する。 The planner 120 receives the characterization 112 and selects an action 132 to be performed by the agent in order for the agent to achieve the goal 122 based on the characterization 112. Generally, planner 120 performs value iterations on multiple scales using characterization 112 to select an action to be performed by the agent. The selection of actions based on the planner 120 and the characterization is described in more detail below with reference to FIG.
システム１００は、次いで、エージェントに、選択されたアクション１３２を実行させる。一般に、選択されたアクション１３２は、エージェントについての制御入力であり、システムは、制御入力を直接エージェントに提出することによって、またはそれに応じてエージェントに指示するようにエージェントの制御システムに指示することによって、エージェントに選択されたアクション１３２を実行させる。 The system 100 then causes the agent to perform the selected action 132. In general, the selected action 132 is a control input for the agent, and the system may submit the control input directly to the agent or by instructing the agent's control system to instruct the agent accordingly. , Cause the agent to execute the selected action 132.
図２は、微分可能プランナー１１０の例示的なアーキテクチャを示す。 FIG. 2 illustrates an exemplary architecture of the differentiable planner 110.
所与の時間ステップについての最終的な特徴付け１１２を生成するために、プランナー１１０は、時間ステップについての２つの初期特徴付けを生成するために、画像１０２およびエゴモーション１０４を処理する。最終的な特徴付けと同様に、２つの初期特徴付けには各々、確信度データと信頼度データの両方を含むが、最終的な特徴付けとはスコアおよび信頼度が異なる可能性がある。次いで、プランナー１１０は、２つの初期特徴付けを結合して、最終的な特徴付け１１２を生成する。 To generate a final characterization 112 for a given time step, the planner 110 processes the image 102 and the egomotion 104 to generate two initial characterizations for the time step. As with the final characterization, each of the two initial characterizations includes both confidence and confidence data, but may have different scores and confidence from the final characterization. The planner 110 then combines the two initial characterizations to produce a final characterization 112.
特に、プランナーは、時間ステップについての環境の第１の初期特徴付け２２２を生成するために、ニューラルネットワーク２２０を使用して画像１０２を処理する。図２からわかるように、ニューラルネットワーク２２０は、残差接続を使用し、２次元（２Ｄ）画像空間内の画像１０２内のシーンの表現を生成する畳み込みエンコーダを含む。この表現は、完全に接続されたレイヤを介して、自己中心的な２Ｄトップダウン図内にあるものに変換される。このトップダウンビュー表現は、第１の初期特徴付けを取得するために、アップコンボリューションレイヤ（および残差接続も）を使用してアップサンプリングされる。 In particular, the planner processes the image 102 using the neural network 220 to generate a first initial characterization 222 of the environment for the time step. As can be seen from FIG. 2, neural network 220 includes a convolutional encoder that uses a residual connection to generate a representation of the scene in image 102 in two-dimensional (2D) image space. This representation is transformed into a self-centered 2D top-down view through fully connected layers. This top-down view representation is upsampled using the upconvolution layer (and also the residual connection) to obtain a first initial characterization.
プランナー１１０はまた、時間ステップについての第２の初期特徴付け２１２を生成するために、エゴモーション１０４を使用して、先行する時間ステップからの最終的な特徴付け２０２を処理する。 The planner 110 also processes the final characterization 202 from the preceding time step using the egomotion 104 to generate a second initial characterization 212 for the time step.
より具体的には、プランナー１１０は、先行する時間ステップおよびエゴモーション１０４からの最終的な特徴付け２０２に微分可能ワーピング関数２１２を適用して第２の初期特徴付け２１２を生成する。ワーピング関数は、最終的な特徴付け２０２を先行する時間ステップから微分可能な方法でワープする任意の関数とすることができる。 More specifically, planner 110 applies a differentiable warping function 212 to a preceding time step and final characterization 202 from egomotion 104 to generate a second initial characterization 212. The warping function can be any function that warps the final characterization 202 in a manner differentiable from the preceding time step.
たとえば、ワーピング関数は、双線形サンプリングを使用して実現される関数であり得る。特に、エゴモーションが与えられると、システムは、第２の初期特徴付け２１２内の各ピクセルを、それが由来する最終的な特徴付け２０２内の場所にマッピングする逆方向の流れ場を計算することができる。この逆方向の流れは、エゴモーションから分析的に計算することができる。次いで、関数は、双線形サンプリングを使用してこの流れ場を最終的な特徴付け評価２０２に適用して第２の初期特徴付け２１２を生成することができる。 For example, the warping function may be a function implemented using bilinear sampling. In particular, given the ego-motion, the system calculates a reverse flow field that maps each pixel in the second initial characterization 212 to the location in the final characterization 202 from which it originated. Can be. This reverse flow can be calculated analytically from the egomotion. The function can then apply this flow field to the final characterization evaluation 202 using bilinear sampling to produce a second initial characterization 212.
次いで、プランナー１１０は、第１の初期特徴付け２２２と第２の初期特徴付け２１２とを結合して、その時間ステップについての環境の最終的な特徴付け１１２を生成する。 The planner 110 then combines the first initial characterization 222 and the second initial characterization 212 to produce a final characterization 112 of the environment for that time step.
より具体的には、プランナー１１０は、結合関数２３０を適用して、第１の初期特徴付け２２２と第２の特徴付け２１２とを結合して、時間ステップについての最終的な特徴付け１１２を生成する。 More specifically, planner 110 applies combination function 230 to combine first initial characterization 222 and second characterization 212 to produce final characterization 112 for a time step. I do.
いくつかの実装形態では、結合関数２３０は、解析関数である。特に、更新関数は、次のように表すことができる。 In some implementations, combining function 230 is an analytic function. In particular, the update function can be expressed as:
Ｃｔ＝Ｃｔ−１＋Ｃｔ’
ここで、ｆｔは、時間ステップについての最終的な特徴付けの確信度データにおけるスコアのセット、ｃｔは、時間ステップについての最終的な特徴付けの信頼度データにおける信頼の尺度のセット、ｆｔ’は、第１の初期特徴付けについてのスコアのセット、ｃｔ’は、第１の初期特徴付けにおける信頼の尺度のセット、ｆｔ−１は、第２の初期特徴付けについてのスコアのセット、ｃｔ−１は、第２の初期特徴付けについてのスコアのセットにおける信頼の尺度のセットである。
Ct = Ct-1 + Ct '
Here, f t is the set of final score in the confidence data characterization for time step, c t, the final set of confidence measure in the reliability data of characterization for time step, f t 'is a set of scores for the first initial characterization, c t' is set in the measure of confidence in the first initial characterization, f t - 1 is the score for the second initial characterization The set, ct-1, is a set of measures of confidence in the set of scores for the second initial characterization.
いくつかの他の実装形態では、結合関数２３０は、ニューラルネットワーク、たとえば、長短期記憶（ＬＳＴＭ）ニューラルネットワークのようなリカレントニューラルネットワークによって実現される。 In some other implementations, the combining function 230 is implemented by a neural network, for example, a recurrent neural network such as a long term short term memory (LSTM) neural network.
この説明からわかるように、画像１０２、エゴモーション１０４、および最終的な特徴付け２０２に対してプランナー１１０によって実行される各動作は、微分可能演算、すなわちニューラルネットワーク演算または微分可能解析関数のいずれかである。 As can be seen from this description, each operation performed by the planner 110 on the image 102, the egomotion 104, and the final characterization 202 is a differentiable operation, ie, either a neural network operation or a differentiable analytic function. It is.
プランナー１１０が最終的な特徴付け１１２を生成すると、プランナー１１０は、その時間ステップにおけるアクションを選択する際に使用するために、最終的な特徴付け１１２をマッパ１２０に提供する。 Once the planner 110 generates the final characterization 112, the planner 110 provides the final characterization 112 to the mapper 120 for use in selecting an action at that time step.
図３は、微分可能階層プランナー１２０の例示的なアーキテクチャを示す。 FIG. 3 shows an exemplary architecture of the differentiable hierarchy planner 120.
一般に、階層プランナー１２０は、複数の空間スケールで計画する。特に、階層プランナー１２０は、複数の空間スケールの各々についてそれぞれの値サブシステム３１０Ａ〜Ｎを含む。この例では値サブシステム３１０Ｍおよび３１０Ｎのみが示されているが、プランナー１２０は、一般に、３つ以上の値サブシステムを含む。 In general, the hierarchical planner 120 plans on multiple spatial scales. In particular, the hierarchy planner 120 includes a respective value subsystem 310A-N for each of a plurality of spatial scales. Although only value subsystems 310M and 310N are shown in this example, planner 120 generally includes three or more value subsystems.
サブシステム３１０Ａ〜Ｎの第１の値サブシステム３１０Ａは、ｋ倍の空間的にダウンサンプリングされたマップに対応し、最後の値サブシステム３１０Ｎは、元の解像度、すなわちマッパ１１０から受信されたマップの解像度に対応する。 The first value subsystem 310A of subsystems 310A-N corresponds to a k-times spatially downsampled map, and the last value subsystem 310N corresponds to the original resolution, ie, the map received from mapper 110. Corresponding to the resolution.
各値サブシステムは、対応する空間スケールである１つまたは複数の値マップを生成するために、対応する空間スケールでｌ回の値反復を実行する。値マップは、空間スケールに対応する空間的にスケーリングされた環境マップ内の複数の領域の領域ごとに、目標を達成するために領域内にいるエージェントの値の評価、すなわち、エージェントが対応する地域にいることが目標を達成するためにどの程度価値があるかの評価を含む。 Each value subsystem performs one value iteration on the corresponding spatial scale to generate one or more value maps that are the corresponding spatial scale. A value map is an evaluation of the value of an agent in an area to achieve a goal for each of a plurality of areas in the spatially scaled environment map corresponding to the spatial scale, i.e., Includes an assessment of how valuable it is to achieve its goals.
次いで、プランナー１２０は、１つまたは複数の値マップをセンタークロップおよびアップサンプリングし、すなわち、センタークロップおよびアップサンプリングされた出力を、次の値サブシステムへの入力として提供することによって、より細かいスケールで値反復を行うために、センタークロップおよびアップサンプリングされた出力を使用する。プランナー１２０は、元の問題の解決に達するために、このプロセスを複数回、すなわち値サブシステムごとに１回反復する。 The planner 120 then fine-scales the one or more value maps by center-cropping and up-sampling, ie, providing the center-crop and up-sampled output as input to the next value subsystem. Use the center crop and the upsampled output to perform the value iteration on. Planner 120 repeats this process multiple times, once for each value subsystem, to arrive at a solution to the original problem.
最後の値サブシステム３１０Ｎがマッパ１１０から受信された環境マップと同じスケールの１つまたは複数の値マップを生成すると、プランナー１２０は、これらの値マップを使用してエージェント１３２によって実行されるべきアクションを選択する。図３の例では、プランナー１２０は、修正された線形ユニットアクティブ化関数を有する完全に接続された層のセットを使用してこれらの値マップを処理して、エージェントによって実行されるべき可能なアクションにわたるスコア分布を生成し、次いで、スコア分布からアクション１３２を選択する。他の例では、プランナー１２０は、最新の値マップから、エージェントの現在位置から到達可能であり、かつ値の最も高い評価を有する更新された値マップの特定の領域を決定し、次いで、時間ステップについての提案されたアクション１３２として、エージェントを特定の領域に連れて行くアクションを選択することができる。 When the last value subsystem 310N generates one or more value maps of the same scale as the environment map received from mapper 110, planner 120 uses these value maps to perform actions to be performed by agent 132. Select In the example of FIG. 3, the planner 120 processes these value maps using a set of fully connected layers with a modified linear unit activation function to determine the possible actions to be performed by the agent. , And then select action 132 from the score distribution. In another example, planner 120 determines from the latest value map a particular region of the updated value map that is reachable from the agent's current location and has the highest rating of the value, and then determines the time step As the suggested action 132 for, an action that takes the agent to a particular area can be selected.
より具体的には、プランナー１２０は、時間ステップについての最終的な特徴付け１１２を受信し、最終的な特徴付けから空間的にスケーリングされた環境マップのシーケンスを生成し、シーケンス内の各空間的にスケーリングされた環境マップは、シーケンス内の任意の後続の空間的にスケーリングされた環境マップに対してダウンサンプリングされ、値サブシステム３１０Ａ〜Ｎのうちの異なる１つに対応する。 More specifically, planner 120 receives final characterization 112 for the time step, generates a sequence of spatially scaled environment maps from the final characterization, Is downsampled to any subsequent spatially scaled environment map in the sequence, corresponding to a different one of the value subsystems 310A-N.
所与の空間スケールについて、そのスケールの値サブシステムは、（ｉ）対応するスケールの空間的にスケーリングされた環境マップ、および（ｉｉ）空間的にスケーリングされた環境マップに関連付けられた１つまたは複数の入力を処理して、空間的にスケーリングされた環境マップについての１つまたは複数の値マップを生成する。 For a given spatial scale, the value subsystem of that scale includes (i) a spatially scaled environment map of the corresponding scale, and (ii) one or more associated with the spatially scaled environment map. The plurality of inputs are processed to generate one or more value maps for the spatially scaled environment map.
空間的にスケーリングされた環境マップに関連付けられた入力は、一般に、シーケンス内の前の空間的なスケールについての１つまたは複数の値マップおよびエージェントの目標を識別するデータを含む。すなわち、シーケンス内の最後の値サブシステム以外の各値サブシステムは、値サブシステムによって生成された１つまたは複数の値マップを、シーケンス内の後続の空間的にスケーリングされた環境マップに関連付けられた入力として、すなわち、シーケンス内の次の値サブシステムへの入力として提供する。シーケンス内の第１の値サブシステム、すなわち、最もダウンサンプリングされた値サブシステムの場合、値マップは、１つまたは複数の所定の初期値マップとすることができる。 The inputs associated with the spatially scaled environment map generally include one or more value maps for the previous spatial scale in the sequence and data identifying the agent's goal. That is, each value subsystem other than the last value subsystem in the sequence associates one or more value maps generated by the value subsystem with subsequent spatially scaled environment maps in the sequence. As the input to the next value subsystem in the sequence. For the first value subsystem in the sequence, the most downsampled value subsystem, the value map may be one or more predetermined initial value maps.
目標が場所の場合、所与の値サブシステムに入力される目標を識別するデータは、サブシステムと同じスケールのマップとすることができる。したがって、細かいスケールの場合、エージェントが目標から遠く離れているとき、対応するサブシステムへの入力として提供されるデータに目標が見えないことがある。 If the goal is a location, the data identifying the goal input to a given value subsystem may be a map on the same scale as the subsystem. Thus, on a fine scale, when an agent is far away from the target, the target may not be visible in the data provided as input to the corresponding subsystem.
各値サブシステムは、フューザー３２０および値反復モジュール３３０を含む。 Each value subsystem includes a fuser 320 and a value iteration module 330.
フューザー３２０は、（ｉ）対応するスケールの空間的にスケーリングされた環境マップと、（ｉｉ）空間的にスケーリングされた環境マップに関連付けられた１つまたは複数の入力とを結合して、空間的にスケーリングされた環境マップと同じスケールの１つまたは複数の値マップを含む値反復入力を生成する。特に、フューザー３２０は、前の値サブシステムからの１つまたは複数の値マップをセンタークロップおよびアップサンプリングし、目標データ、空間的にスケーリングされた環境マップ、およびクロップおよびアップサンプリングされた値マップを深さ連結して、値反復入力を生成する。 The fuser 320 combines (i) a spatially scaled environment map of a corresponding scale and (ii) one or more inputs associated with the spatially scaled environment map to form a spatially scaled environment map. Generate a value iteration input that includes one or more value maps of the same scale as the scaled environment map. In particular, fuser 320 center-crops and up-samples one or more value maps from the previous value subsystem to generate target data, a spatially scaled environment map, and crop and up-sampled value maps. Depth concatenation to produce a value iteration input.
次いで、値反復モジュール３３０は、値反復ニューラルネットワークを使用して、値反復入力、すなわち１つまたは複数の初期値マップに対してｌ回の値反復を実行して、空間的にスケーリングされた環境マップについての１つまたは複数の値マップを生成する。値反復は、ダイクストラのアルゴリズムの一般化と考えることができ、ここで、各状態の値は、隣接する値とそれらの隣接する状態への遷移の報酬を超える最大値をとることによって、各反復で繰り返し再計算される。値反復を実行する値反復ニューラルネットワークは、交互の畳み込みとチャネルごとの最大プーリング操作とを含むニューラルネットワークであり、プランナー１２０がその入力について微分されることを可能にする。値反復および値反復ニューラルネットワークは、Ａ． Ｔａｍａｒ， Ｓ． Ｌｅｖｉｎｅ， ａｎｄ Ｐ． Ａｂｂｅｅｌ． Ｖａｌｕｅ ｉｔｅｒａｔｉｏｎ ｎｅｔｗｏｒｋｓ． Ｉｎ ＮＩＰＳ， ２０１６にさらに詳細に記載されている。 The value iteration module 330 then performs a value iteration input, i.e., 1 value iteration over one or more initial value maps, using a value iteration neural network to generate a spatially scaled environment. Generate one or more value maps for the map. Value iteration can be thought of as a generalization of Dijkstra's algorithm, where the value of each state is taken by taking the maximum value that exceeds the neighboring values and the reward for transitioning to those neighboring states. Is recalculated repeatedly. A value-repetition neural network that performs value repetition is a neural network that includes alternating convolutions and a maximum pooling operation per channel, allowing the planner 120 to be differentiated on its inputs. Value iteration and value iteration neural networks are described in Tamar, S.M. Levine, and P.M. Abbeel. Value iteration networks. In NIPS, 2016, which is described in further detail.
図４は、所与の時間ステップにおいて画像を処理するための例示的なプロセス４００のフロー図である。便宜上、プロセス４００は、１つまたは複数の場所に位置する１つまたは複数のコンピュータのシステムによって実行されるものとして説明する。たとえば、適切にプログラムされた、たとえば図１のナビゲーションシステム１００などのナビゲーションシステムは、プロセス４００を実行することができる。 FIG. 4 is a flow diagram of an exemplary process 400 for processing an image at a given time step. For convenience, the process 400 is described as being performed by one or more computer systems located at one or more locations. For example, a suitably programmed navigation system, such as, for example, the navigation system 100 of FIG. 1, can perform the process 400.
システムは、環境の画像、すなわち、エージェントが環境内を移動している間にエージェントのセンサーによってキャプチャされた画像を取得する（ステップ４０２）。システムはまた、先行する時間ステップからのエージェントのエゴモーションを識別するデータを受信する。 The system acquires an image of the environment, i.e., an image captured by a sensor of the agent while the agent is moving through the environment (step 402). The system also receives data identifying the agent's egomotion from the preceding time step.
システムは、画像およびエゴモーションから環境の特徴付けを生成する（ステップ４０４）。特徴付けは、確信度データおよび信頼度データを含む。すなわち、特徴付けは、マップ内の各場所にスコアを有する環境のトップダウンマップ、およびスコアについての信頼の尺度を含む。 The system generates an environment characterization from the image and the egomotion (step 404). The characterization includes confidence data and confidence data. That is, the characterization includes a top-down map of the environment with a score at each location in the map, and a measure of confidence in the score.
システムは、エージェントによって実行されるべき提案されたアクションを選択する（ステップ４０６）。特に、システムは、エージェントの目標を識別するデータおよび環境の特徴付けを使用して複数の空間スケールで値反復を実行して、１つまたは複数の最終的な値マップを生成する。次いでシステムは、１つまたは複数の最終的な値マップを使用して、エージェントによって実行されるべき提案されたアクションを選択する。 The system selects a suggested action to be performed by the agent (step 406). In particular, the system performs value iteration on multiple spatial scales using data and environmental characterizations that identify the goals of the agent to generate one or more final value maps. The system then uses the one or more final value maps to select a suggested action to be performed by the agent.
プロセス４００がプランナーおよびマッパのトレーニング中に実行されている場合、システムは、最適なアクション、すなわちエージェントによって実行されるべきであるアクションを取得する（ステップ４０８）。 If the process 400 is being performed during training of the planner and mapper, the system obtains the optimal action, that is, the action that should be performed by the agent (step 408).
システムは、選択されたアクションと最適なアクションとの間の誤差の尺度の勾配を決定する（ステップ４１０）。 The system determines the slope of the measure of error between the selected action and the optimal action (step 410).
システムは、マッパおよびプランナーのパラメータに対する更新を決定するために、プランナーおよびマッパを介して勾配を逆伝搬する（ステップ４１２）。したがって、システムは、マッパおよびプランナーをエンドツーエンドでトレーニングする、すなわち、トレーニング中に提案されたアクションの誤差は、プランナーのパラメータだけでなくマッパのパラメータも更新する。特に、いくつかの実装形態では、提案されたアクションの誤差は、マッパのパラメータを調整するために使用される唯一のフィードバックであり、したがって、マッパは、任意の追加の監視を必要とせず、正しいアクションを選択する際にプランナーにとって有用なマップを生成することを学習する。特に、マッパによって生成された確信度データ内の特定の特徴が何であるべきかを指定する外部ラベルはなく、マッパは、プランナーにとって有用な特徴を学習することができる。 The system backpropagates the gradient through the planner and mapper to determine updates to the mapper and planner parameters (step 412). Thus, the system trains the mapper and planner end-to-end, i.e., the error in the proposed action during training updates the mapper parameters as well as the planner parameters. In particular, in some implementations, the error in the proposed action is the only feedback used to adjust the parameters of the mapper, so the mapper does not need any additional monitoring and is correct Learn to generate maps that are useful to planners when choosing an action. In particular, there is no external label that specifies what a particular feature should be in the confidence data generated by the mapper, and the mapper can learn features that are useful to the planner.
プロセス４００がトレーニング後に実行されている場合、システムは、エージェントに、提案されたアクションを実行させることができる。プロセス４００がトレーニング中に実行されている場合、システムは、環境内を移動するエージェントに、提案されたアクションを確率ｐで実行させ、最適なアクションを確率１−ｐで実行させることができる。これによって、エージェントは、環境を探索し、潜在的に、目標を達成する際の性能の向上につながるアクションを発見することができる。いくつかの場合には、システムは、エージェントの動作中にｐを増加させて、最適なアクションを選択する確率を高める。 If the process 400 is being performed after training, the system may cause the agent to perform the suggested action. If the process 400 is being performed during training, the system can cause agents traveling in the environment to perform the proposed action with probability p and the optimal action with probability 1-p. This allows the agent to explore the environment and potentially find actions that lead to improved performance in achieving the goal. In some cases, the system increases p during the operation of the agent to increase the probability of selecting the optimal action.
本明細書は、システムおよびコンピュータプログラム構成要素に関して「構成された」という用語を使用する。１つまたは複数のコンピュータのシステムが特定の動作またはアクションを実行するように構成されるとは、システムが、動作中、システムに動作またはアクションを実行させるソフトウェア、ファームウェア、ハードウェア、またはそれらの組合せをインストールしていることを意味する。１つまたは複数のコンピュータプログラムが特定の動作またはアクションを実行するように構成されるとは、１つまたは複数のプログラムが、データ処理装置によって実行されると、装置に動作またはアクションを実行させる命令を含むことを意味する。 This specification uses the term "configured" with respect to system and computer program components. One or more computer systems are configured to perform a particular operation or action, when the system is in operation, software, firmware, hardware, or a combination thereof that causes the system to perform the operation or action. Means that you have installed. One or more computer programs are configured to perform a particular operation or action, wherein one or more programs, when executed by a data processing device, cause the device to perform the operation or action. Is included.
本明細書に記載された主題および機能的動作の実施形態は、デジタル電子回路、有形に具現化されたコンピュータソフトウェアまたはファームウェア、本明細書に開示される構造およびそれらの構造的均等物を含むコンピュータハードウェア、またはそれらの１つもしくは複数の組合せにおいて実装することができる。本明細書に記載される主題の実施形態は、１つまたは複数のコンピュータプログラム、すなわち、データ処理装置によって実行される、またはデータ処理装置の動作を制御するための有形の非一時的記憶媒体上に符号化されたコンピュータプログラム命令の１つまたは複数のモジュールとして実装することができる。コンピュータ記憶媒体は、機械可読記憶デバイス、機械可読記憶基板、ランダムまたはシリアルアクセスメモリデバイス、またはそれらの１つもしくは複数の組合せとすることができる。代替的にまたは追加として、プログラム命令は、人工的に生成された伝搬信号、たとえば、データ処理装置によって実行するための適切な受信機装置への送信のために情報を符号化するために生成された機械生成電気、光学、または電磁信号上で符号化することができる。 Embodiments of the subject matter and functional operations described herein may be implemented in digital electronic circuits, tangibly embodied computer software or firmware, computers including the structures disclosed herein and their structural equivalents. It can be implemented in hardware, or one or more combinations thereof. Embodiments of the subject matter described herein provide for one or more computer programs, i.e., on a tangible non-transitory storage medium, to be executed by or control the operation of a data processing device. Can be implemented as one or more modules of computer program instructions encoded in The computer storage medium may be a machine readable storage device, a machine readable storage substrate, a random or serial access memory device, or one or more combinations thereof. Alternatively or additionally, the program instructions may be generated to encode an artificially generated propagated signal, for example, information for transmission to a suitable receiver device for execution by a data processing device. It can be encoded on machine-generated electrical, optical, or electromagnetic signals.
「データ処理装置」という用語は、データ処理ハードウェアを指し、たとえば、プログラム可能プロセッサ、コンピュータ、または複数のプロセッサもしくはコンピュータを含む、データを処理するためのあらゆる種類の装置、デバイスおよび機械を包含する。装置は、ＦＰＧＡ（フィールドプログラマブルゲートアレイ）またはＡＳＩＣ（特定用途向け集積回路）などの専用論理回路でもよく、またはそれをさらに含むこともできる。装置は、随意に、ハードウェアに加えて、コンピュータプログラムの実行環境を生成するコード、たとえば、プロセッサファームウェア、プロトコルスタック、データベース管理システム、オペレーティングシステム、またはそれらの１つまたは複数の組合せを構成するコードを含むことができる。 The term "data processing apparatus" refers to data processing hardware and encompasses any type of apparatus, device, and machine for processing data, including, for example, a programmable processor, a computer, or multiple processors or computers. . The device may be, or may further include, dedicated logic circuitry such as an FPGA (Field Programmable Gate Array) or ASIC (Application Specific Integrated Circuit). The apparatus optionally includes, in addition to hardware, code for creating an execution environment for the computer program, eg, code for configuring processor firmware, a protocol stack, a database management system, an operating system, or one or more combinations thereof. Can be included.
プログラム、ソフトウェア、ソフトウェアアプリケーション、アプリ、モジュール、ソフトウェアモジュール、スクリプト、またはコードとも呼ばれるまたは記述されることもあるコンピュータプログラムは、コンパイルもしくはインタープリタ型言語、宣言型言語もしくは手続き型言語を含む、任意の形式のプログラミング言語で記述することができ、それは、スタンドアロンプログラムとして、またはモジュール、コンポーネント、サブルーチン、もしくはコンピューティング環境での使用に適した他のユニットとしてなど、あらゆる形式で展開できる。プログラムは、必ずしも必要はないが、ファイルシステム内のファイルに対応し得る。プログラムは、問題のプログラム専用の単一のファイル、またはたとえば、１つまたは複数のモジュール、サブプログラム、もしくはコードの一部を記憶するファイルなど、複数のコーディネートされたファイルに、たとえば、マークアップ言語文書に記憶された１つまたは複数のスクリプなど、他のプログラムまたはデータを保持するファイルの一部に記憶することができる。コンピュータプログラムは、１つのコンピュータ上で、または１つのサイトに位置するか、もしくは複数のサイトに分散され、データ通信ネットワークによって相互接続された複数のコンピュータ上で実行されるように配備することができる。 Computer program, also called or described as a program, software, software application, app, module, software module, script, or code, in any form, including a compiled or interpreted language, a declarative language, or a procedural language And can be deployed in any form, such as as a stand-alone program, or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. The program may be in a single file dedicated to the program in question, or in a plurality of coordinated files, such as files storing one or more modules, subprograms, or pieces of code, for example, in a markup language. It can be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a document. The computer program may be deployed to be executed on one computer or on multiple computers located at one site or distributed over multiple sites and interconnected by a data communication network. .
本明細書では、「データベース」という用語は、任意のデータの集まりを指すために広く使用されており、データは、任意の特定の方法で構造化する必要はなく、またはまったく構造化する必要はなく、１つまたは複数の場所にある記憶デバイスに記憶することができる。したがって、たとえば、インデックスデータベースは、複数のデータの集まりを含むことができ、それらの各々は、異なって編成されアクセスされてもよい。 As used herein, the term "database" is widely used to refer to any collection of data, and the data need not be structured in any particular way, or need not be structured at all. Rather, it can be stored on a storage device at one or more locations. Thus, for example, an index database may include multiple collections of data, each of which may be organized and accessed differently.
同様に、本明細書では、「エンジン」という用語は、１つまたは複数の特定の機能を実行するようにプログラムされているソフトウェアベースのシステム、サブシステム、またはプロセスを指すために広く使用されている。一般に、エンジンは、１つまたは複数の場所にある１つまたは複数のコンピュータにインストールされた１つまたは複数のソフトウェアモジュールまたは構成要素として実装される。いくつかの場合には、１つまたは複数のコンピュータが特定のエンジンに専用であり、他の場合には、複数のエンジンを、同じ１つまたは複数のコンピュータにインストールし、そこにおいて実行することができる。 Similarly, the term "engine" is used broadly herein to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. I have. Generally, an engine is implemented as one or more software modules or components installed on one or more computers at one or more locations. In some cases, one or more computers may be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same one or more computers. it can.
本明細書で記述されたプロセスおよび論理フローは、入力データ上で動作し、出力を生成することによって機能を実行するために、１つまたは複数のコンピュータプログラムを実行する１つまたは複数のプログラム可能コンピュータによって実行することができる。プロセスおよび論理フローは、たとえばＦＰＧＡまたはＡＳＩＣなどの専用論理回路によって、あるいは専用論理回路と１つまたは複数のプログラムされたコンピュータとの組合せによっても実行することができる。 The processes and logic flows described herein perform one or more programmable programs that execute one or more computer programs to perform functions by operating on input data and generating outputs. Can be performed by computer. The processes and logic flows may be performed by dedicated logic, such as, for example, an FPGA or ASIC, or by a combination of dedicated logic and one or more programmed computers.
コンピュータプログラムの実行に適したコンピュータは、汎用マイクロプロセッサもしくは専用マイクロプロセッサ、もしくはその両方、または他の種類の中央処理装置に基づくことができる。一般に、中央処理装置は、読取り専用メモリまたはランダムアクセスメモリまたはその両方から命令およびデータを受信する。コンピュータの必須要素は、命令を実行または実行するための中央処理装置、ならびに命令およびデータを記憶するための１つまたは複数のメモリデバイスである。中央処理装置およびメモリは、専用論理回路によって補うまたは組み込むことができる。一般に、コンピュータは、たとえば磁気、光磁気ディスク、または光ディスクなど、データを記憶するための１つまたは複数の大容量記憶デバイスをも含み、あるいは、１つまたは複数の大容量記憶デバイスからデータを受信する、それにデータを転送する、またはその両方のために動作可能に結合される。しかしながら、コンピュータはそのようなデバイスを有する必要はない。さらに、コンピュータは、別のデバイス、たとえば、ほんのいくつかの例を挙げれば、携帯電話、携帯情報端末（ＰＤＡ）、モバイルオーディオもしくはビデオプレーヤ、ゲームコンソール、全地球測位システム（ＧＰＳ）受信機、またはユニバーサルシリアルバス（ＵＳＢ）フラッシュドライブなどのポータブルストレージデバイス中に埋め込むことができる。 Computers suitable for the execution of a computer program can be based on general purpose or special purpose microprocessors, or both, or other types of central processing units. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or executing instructions, and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, dedicated logic. Generally, a computer also includes one or more mass storage devices for storing data, such as a magnetic, magneto-optical, or optical disk, or receives data from one or more mass storage devices. Operatively coupled to, transfer data to, or both. However, a computer need not have such a device. Further, the computer may be another device, such as a mobile phone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a global positioning system (GPS) receiver, or just to name a few examples. It can be embedded in a portable storage device such as a Universal Serial Bus (USB) flash drive.
コンピュータプログラム命令およびデータを記憶するのに適したコンピュータ可読媒体は、一例として、たとえば、ＥＰＲＯＭ、ＥＥＰＲＯＭ、およびフラッシュメモリデバイスなどの半導体メモリデバイス、たとえば内部ハードディスクまたはリムーバブルディスクなどの磁気ディスク、光磁気ディスク、およびＣＤ−ＲＯＭおよびＤＶＤ−ＲＯＭディスクを含むすべての形態の不揮発性メモリ、媒体、およびメモリデバイスを含む。 Computer readable media suitable for storing computer program instructions and data include, by way of example, semiconductor memory devices, such as, for example, EPROMs, EEPROMs, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; , And all forms of non-volatile memory, including CD-ROM and DVD-ROM discs, media, and memory devices.
ユーザとの対話を提供するために、本明細書に記載される主題の実施形態は、ユーザに情報を表示するための、ＣＲＴ（陰極線管）またはＬＣＤ（液晶ディスプレイ）モニタなどのディスプレイデバイス、ならびにキーボードおよび、ユーザがコンピュータに入力を提供することができる、たとえば、マウスまたはトラックボールなどのポインティングデバイスを有するコンピュータ上に実装することができる。他の種類のデバイスを使用して、ユーザとの対話を提供することもでき、たとえば、ユーザに提供されるフィードバックは、たとえば、視覚フィードバック、聴覚フィードバック、または触覚フィードバックなど、任意の形態の感覚フィードバックとすることができ、ユーザからの入力は、音響、音声、または触覚入力を含む任意の形態で受信することができる。さらに、コンピュータは、たとえば、ウェブブラウザから受信された要求に応答して、ユーザのデバイス上のウェブブラウザにウェブページを送信することによってなど、ユーザによって使用されるデバイスとの間でドキュメントを送受信することによって、ユーザと対話することができる。また、コンピュータは、テキストメッセージまたは他の形態のメッセージをパーソナルデバイス、たとえば、メッセージングアプリケーションを実行しているスマートフォンに送信し、代わりに、ユーザから応答メッセージを受信することによって、ユーザと対話することができる。 To provide for interaction with a user, embodiments of the subject matter described herein provide a display device, such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user, and It can be implemented on a computer that has a keyboard and a pointing device, such as a mouse or trackball, where the user can provide input to the computer. Other types of devices may also be used to provide interaction with the user, for example, the feedback provided to the user may be any form of sensory feedback, such as, for example, visual, audible, or tactile feedback And input from the user can be received in any form, including acoustic, audio, or tactile input. Further, the computer sends and receives documents to and from the device used by the user, such as by sending a web page to a web browser on the user's device in response to a request received from the web browser. By doing so, it is possible to interact with the user. Also, the computer may interact with the user by sending a text message or other form of message to a personal device, for example, a smartphone running a messaging application, and instead receiving a response message from the user. it can.
機械学習モデルを実装するためのデータ処理装置はまた、たとえば、機械学習のトレーニングまたは製作、すなわち推論、作業負荷の共通部分および計算集約的部分を処理するための専用ハードウェアアクセラレータユニットを含むことができる。 The data processing device for implementing the machine learning model may also include, for example, a dedicated hardware accelerator unit for processing or learning machine learning, i.e., inference, common parts of the workload and computationally intensive parts. it can.
機械学習モデルは、機械学習フレームワーク、たとえば、ＴｅｎｓｏｒＦｌｏｗフレームワーク、Ｍｉｃｒｏｓｏｆｔ Ｃｏｇｎｉｔｉｖｅ Ｔｏｏｌｋｉｔフレームワーク、Ａｐａｃｈｅ Ｓｉｎｇａフレームワーク、またはＡｐａｃｈｅ ＭＸＮｅｔフレームワークを使用して実装および展開することができる。 The machine learning model can be implemented and deployed using a machine learning framework, for example, a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
本明細書に記載される主題の実施形態は、たとえばデータサーバとしてのバックエンド構成要素を含む、またはアプリケーションサーバなどのミドルウェア構成要素を含む、またはたとえば、ユーザが本明細書に記載された主題の実装と対話することができる、グラフィカルユーザインタフェース、ウェブブラウザ、またはアプリを有するクライアントコンピュータなどのフロントエンド構成要素を含む、または１つもしくは複数のそのようなバックエンド、ミドルウェア、またはフロントエンド構成要素の任意の組合せを含むコンピューティングシステムにおいて実装することができる。システムの構成要素は、たとえば、通信ネットワークなど、任意の形式または媒体のデジタルデータ通信によって相互接続することができる。通信ネットワークの例には、ローカルエリアネットワーク（ＬＡＮ）およびワイドエリアネットワーク（ＷＡＮ）、たとえばインターネットがある。 Embodiments of the subject matter described herein may include, for example, a back-end component as a data server, or include a middleware component, such as an application server, or may, for example, provide a user with the subject matter described herein. Include front-end components, such as a graphical user interface, a web browser, or a client computer with an app, or interact with one or more such back-end, middleware, or front-end components It can be implemented in a computing system that includes any combination. The components of the system can be interconnected by any form or medium of digital data communication, for example, a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), such as the Internet.
コンピューティングシステムは、クライアントおよびサーバを含むことができる。クライアントとサーバとは、一般に、互いに遠隔であり、典型的には、通信ネットワークを介して対話する。クライアントとサーバとの関係は、それぞれのコンピュータ上で実行され、互いにクライアント−サーバ関係を有するコンピュータプログラムのおかげで生じる。いくつかの実施形態では、サーバは、たとえば、クライアントとして動作するデバイスと対話しているユーザにデータを表示し、ユーザからユーザ入力を受信するために、データ、たとえば、ＨＴＭＬページをユーザデバイスに送信する。たとえば、ユーザ対話の結果など、ユーザデバイスにおいて生成されたデータは、デバイスからサーバにおいて受信することができる。 Computing systems can include clients and servers. The client and server are generally remote from each other and typically interact through a communication network. The relationship between the client and server occurs on each computer, thanks to computer programs that have a client-server relationship with each other. In some embodiments, the server sends the data, eg, an HTML page, to the user device to display data to a user interacting with the device acting as a client, for example, and to receive user input from the user. I do. For example, data generated at a user device, such as the result of a user interaction, can be received at the server from the device.
本明細書は、多くの具体的な実施の詳細を含むが、これらは、いかなる発明の範囲または特許請求される可能性のある範囲に対する限定ではなく、むしろ特定の発明の特定の実施形態に固有であり得る特徴の説明として解釈されるものとする。別個の実施形態の文脈において本明細書で説明されるいくつかの特徴は、単一の実施形態において組み合わせて実装することもできる。逆に、単一の実施形態の文脈で記載されている様々な特徴は、複数の実施形態で別々にまたは任意の適切な部分組合せで実装することもできる。さらに、特徴は、いくつかの組合せで作用するものとして上述されており、当初はそのように請求されているものであるが、いくつかの場合、請求された組合せからの１つまたは複数の特徴を、組合せから削除することができ、請求された組合せは、部分組合せ、または部分組合せの変形を対象とし得る。 This specification includes many specific implementation details, which are not limitations on the scope of any invention or the scope of the invention, but rather are specific to particular embodiments of the particular invention. Shall be construed as a description of the features that may be. Certain features described herein in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented separately in multiple embodiments or in any appropriate subcombination. Furthermore, features may be described above as operating in some combinations and initially so claimed, but in some cases, one or more features from the claimed combination. Can be deleted from the combination, and the claimed combination can be directed to a sub-combination or a variant of a sub-combination.
同様に、動作が図面に示され、特許請求の範囲に特定の順序で記載されているが、これは、そのような動作が、示された特定の順序で、または順番に実行されること、あるいは望ましい結果を達成するために、図示されたすべての動作が実行されることを必要とするものとして理解されないものとする。いくつかの状況では、マルチタスキングおよび並列処理が有利であり得る。さらに、上述した実施形態における様々なシステムモジュールおよび構成要素の分離は、すべての実施形態においてそのような分離を必要とするものと理解されないものとし、記述されたプログラム構成要素およびシステムを、一般に、単一のソフトウェア製品に一緒に組み入れることができ、または複数のソフトウェア製品にパッケージ化することができることを理解されたい。 Similarly, although operations are shown in the drawings and are described in the claims in a particular order, this means that such operations are performed in the particular order shown, or in sequence. Alternatively, it is not to be understood that all illustrated acts need to be performed to achieve the desired result. In some situations, multitasking and parallel processing may be advantageous. Furthermore, the separation of various system modules and components in the embodiments described above is not to be understood as requiring such separation in all embodiments, and the described program components and systems are generally It should be understood that they can be incorporated together into a single software product or packaged into multiple software products.
主題の特定の実施形態が記載されている。他の実施形態は、以下の特許請求の範囲内にある。たとえば、特許請求の範囲に列挙されたアクションは、異なる順序で実行され、依然として望ましい結果を達成することができる。一例として、添付の図面に示されるプロセスは、望ましい結果を達成するために、示された特定の順序または逐次的な順序を必ずしも必要としない。いくつかの場合には、マルチタスキングおよび並列処理が有利であり得る。 Particular embodiments of the subject matter have been described. Other embodiments are within the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. By way of example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
１０４ エゴモーション
１１０ 微分可能マッパ
１１２ 特徴付け
１２０ 微分可能階層プランナー
１２２ 目標
１３２ アクション
２０２ 最終的な特徴付け
２１２ 第２の初期特徴付け
２１２ 微分可能ワーピング関数
２２０ ニューラルネットワーク
２２２ 第１の初期特徴付け
２３０ 結合関数
３１０ 値サブシステム
３２０ フューザー
３３０ 値反復モジュール
104 egomotion 110 differentiable mapper 112 characterization 120 differentiable hierarchy planner 122 goal 132 action 202 final characterization 212 second initial characterization 212 differentiable warping function 220 neural network 222 first initial characterization 230 coupling function 310 value subsystem 320 fuser 330 value iteration module
Claims (24)
複数の時間ステップの各時間ステップにおいて、環境の特徴付けを前記時間ステップにおける前記環境の画像から生成するように構成されたマッピングサブシステムを実装させ、前記特徴付けが、特定の特徴を有する前記環境内の場所を識別する環境マップを含み、前記特徴付けを生成することが、時間ステップごとに、
前記時間ステップにおいて前記環境の前記画像を取得することであり、前記画像が、前記環境内を移動するエージェントによってキャプチャされた画像である、取得することと、
前記時間ステップについての前記環境の第１の初期特徴付けを生成するために、ニューラルネットワークを使用して前記画像を処理することと、
前記時間ステップに先立つ先行する時間ステップについての前記環境の最終的な特徴付けを取得することと、
前記先行する時間ステップと前記時間ステップとの間の前記エージェントの動きの尺度を取得することと、
前記時間ステップについての前記環境の第２の初期特徴付けを生成するために、前記先行する時間ステップについての前記環境の前記特徴付けを処理することであり、前記第２の初期特徴付けを生成するために、前記先行する時間ステップについての前記環境の前記最終的な特徴付けおよび動きの前記尺度に微分可能ワーピング関数を適用することを含む、処理することと、
前記第１の初期特徴付けと前記第２の初期特徴付けとを結合して、前記時間ステップについての前記環境の最終的な特徴付けを生成することと
を含む、システム。 A system comprising one or more computers and one or more storage devices for storing instructions, wherein the instructions, when executed, cause the one or more computers to:
At each time step of the plurality of time steps, implementing a mapping subsystem configured to generate a characterization of the environment from an image of the environment at the time step, wherein the characterization has a particular characteristic. Generating an environment map that identifies locations within
Obtaining the image of the environment in the time step, wherein the image is an image captured by an agent traveling in the environment, obtaining;
Processing the image using a neural network to generate a first initial characterization of the environment for the time step;
Obtaining a final characterization of the environment for a preceding time step prior to the time step;
Obtaining a measure of the movement of the agent between the preceding time step and the time step;
Processing the characterization of the environment for the preceding time step to generate a second initial characterization of the environment for the time step, generating the second initial characterization. Processing, including applying a differentiable warping function to the final characterization of the environment and the measure of motion for the preceding time step;
Combining the first initial characterization and the second initial characterization to generate a final characterization of the environment for the time step.
前記第１の初期特徴付け、前記第２の初期特徴付け、前記先行する時間ステップについての前記最終的な特徴付け、および前記時間ステップについての前記最終的な特徴付けが、トップダウンの視点からのものである、
請求項１または２に記載のシステム。 The image is from a first person self-centered point of view of the agent;
The first initial characterization, the second initial characterization, the final characterization for the preceding time step, and the final characterization for the time step are from a top-down perspective. Is a thing,
The system according to claim 1.
前記第１の初期特徴付けおよび前記第２の初期特徴付けに更新関数を適用して、前記最終的な特徴付けを生成すること
を含む、請求項１から３のいずれか一項に記載のシステム。 Combining the first initial characterization and the second initial characterization to generate the final characterization for the time step;
4. The system of claim 1, comprising applying an update function to the first initial characterization and the second initial characterization to generate the final characterization. 5. .
前記更新関数が、以下の式の演算を実行することを含み、
ただし、ｆｔは、前記時間ステップについての前記最終的な特徴付けに対する前記スコアのセットであり、ｃｔは、前記時間ステップについての前記最終的な特徴付けに対する前記スコアのセットにおける前記信頼の尺度のセットであり、ｆｔ’は、前記第１の初期特徴付けに対する前記スコアのセットであり、ｃｔ’は、前記第１の初期特徴付けに対する前記スコアのセットにおける前記信頼の尺度のセットであり、ｆｔ−１は、前記第２の初期特徴付けに対する前記スコアのセットであり、ｃｔ−１は、前記第２の初期特徴付けに対する前記スコアのセットにおける前記信頼の尺度のセットである、請求項４に記載のシステム。 Each characterization includes: (i) a set of scores that indicate whether the plurality of locations in the environment have the particular feature; and (ii) a set of measures of confidence in the set of scores.
The updating function includes performing an operation of the following expression;
Where ft is the set of scores for the final characterization for the time step, and ct is the measure of the confidence in the set of scores for the final characterization for the time step. Ft ′ is the set of scores for the first initial characterization and ct ′ is the set of measures of confidence in the set of scores for the first initial characterization. And ft-1 is the set of scores for the second initial characterization and ct-1 is the set of measures of confidence in the set of scores for the second initial characterization. The system of claim 4.
前記場所が特定の特徴を有するか否かを表すスコアを含み、前記特徴付けが、前記場所ごとに、
前記場所についての前記スコアにおける信頼の尺度
をさらに含む、請求項１から６のいずれか一項に記載のシステム。 The environment map for a time step comprises: for each of the plurality of locations in the environment,
Including a score indicating whether the location has a particular feature, wherein the characterization is, for each location,
The system of claim 1, further comprising: a measure of confidence in the score for the location.
前記マッピングサブシステムから前記時間ステップについての前記最終的な特徴付けを取得し、
前記時間ステップにおいて前記環境と対話するエージェントによって実行されるべき提案されたアクションを選択するために、前記最終的な特徴付けを処理する
ように構成された計画サブシステム
をさらに含む、請求項１から７のいずれか一項に記載のシステム。 For each of the plurality of time steps,
Obtaining the final characterization for the time step from the mapping subsystem;
The planning subsystem further configured to process the final characterization to select a suggested action to be performed by an agent interacting with the environment at the time step. A system according to any one of the preceding claims.
前記時間ステップについての前記最終的な特徴付けから空間的にスケーリングされた環境マップのシーケンスを生成することであり、前記シーケンス内の各空間的にスケーリングされた環境マップが、前記シーケンス内の任意の後続の空間的にスケーリングされた環境マップに対してダウンサンプリングされる、生成することと、
最もダウンサンプリングされた空間的にスケーリングされた環境マップから始まる前記シーケンス内の空間的にスケーリングされた環境マップごとに、
前記空間的にスケーリングされた環境マップについての１つまたは複数の値マップを生成するために、前記空間的にスケーリングされた環境マップおよび前記空間的にスケーリングされた環境マップに関連付けられた１つまたは複数の入力を処理することであり、値マップが、前記空間的にスケーリングされた環境マップ内の複数の領域の領域ごとに、前記目標を達成するために前記領域内にいる前記エージェントの値の評価を含む、処理することと、
最後の空間的にスケーリングされた環境マップ以外の前記シーケンス内の空間的にスケーリングされた環境マップごとに、前記シーケンス内の後続の空間的にスケーリングされた環境マップに関連付けられた入力として、前記空間的にスケーリングされた環境マップについての前記１つまたは複数の値マップを提供することと
を含む、請求項８に記載のシステム。 The agent is performing an action to achieve a goal, and processing the final characterization to select the proposed action for the time step;
Generating a sequence of spatially scaled environment maps from the final characterization for the time steps, wherein each spatially scaled environment map in the sequence comprises any sequence in the sequence. Generating, down-sampled against a subsequent spatially scaled environment map;
For each spatially scaled environment map in the sequence starting from the most downsampled spatially scaled environment map,
One or more associated with the spatially scaled environment map and the spatially scaled environment map to generate one or more value maps for the spatially scaled environment map. Processing a plurality of inputs, wherein the value map is, for each region of the plurality of regions in the spatially scaled environment map, a value of the agent value within the region to achieve the goal. Processing, including evaluation;
For each spatially scaled environment map in the sequence other than the last spatially scaled environment map, the spatially scaled environment map as an input associated with a subsequent spatially scaled environment map in the sequence. Providing the one or more value maps for a globally scaled environment map.
前記空間的にスケーリングされた環境マップについての１つまたは複数の初期値マップを生成するために、前記空間的にスケーリングされた環境マップに関連付けられた入力を処理することと、
前記空間的にスケーリングされた環境マップについての前記１つまたは複数の値マップを生成するために、前記空間的にスケーリングされた環境マップについての前記１つまたは複数の初期値マップに対して１つまたは複数の値反復を実行することと
を含む、請求項９から１３のいずれか一項に記載のシステム。 Processing inputs associated with the spatially scaled environment map to generate the one or more value maps for the spatially scaled environment map;
Processing inputs associated with the spatially scaled environment map to generate one or more initial value maps for the spatially scaled environment map;
One for the one or more initial value maps for the spatially scaled environment map to generate the one or more value maps for the spatially scaled environment map Or performing a plurality of value iterations. 14. The system according to any one of claims 9 to 13, comprising:
前記シーケンス内の前記最後の空間的にスケーリングされた環境マップについての前記値マップに基づいて前記提案されたアクションを選択すること
を含む、請求項９から１４のいずれか一項に記載のシステム。 Processing the final characterization to select the proposed action,
15. The system of any one of claims 9 to 14, comprising selecting the proposed action based on the value map for the last spatially scaled environment map in the sequence.
前記シーケンス内の前記最後の空間的にスケーリングされた環境マップについての前記値マップから、値の最高の評価を有する更新された値マップの特定の領域を決定することと、
前記時間ステップについての前記提案されたアクションとして、前記特定の領域に前記エージェントを連れて行くアクションを選択することと
を含む、請求項１５に記載のシステム。 Selecting the proposed action based on the value map for the last spatially scaled environment map in the sequence;
Determining, from the value map for the last spatially scaled environment map in the sequence, a particular region of the updated value map having the highest rating of the value;
16. The system of claim 15, wherein the suggested action for the time step comprises selecting an action that will take the agent to the particular area.
前記提案されたアクションを選択するために１つまたは複数の修正された線形ユニットを含むニューラルネットワークを介して前記値マップを処理すること
を含む、請求項１５に記載のシステム。 Selecting the proposed action based on the value map for the last spatially scaled environment map in the sequence;
The system of claim 15, further comprising: processing the value map via a neural network including one or more modified linear units to select the proposed action.
画像を取得するステップと、
前記画像の時間ステップについてのオブジェクトの環境の最終的な特徴付けを決定するために、前記マッピングサブシステムを使用して前記画像を処理するステップと、
前記画像の前記時間ステップについての提案されたアクションを選択するために、前記計画サブシステムを使用して前記最終的な特徴付けを処理するステップと、
前記画像の前記時間ステップについての最適なアクションを取得するステップと、
前記選択されたアクションと前記最適なアクションとの間の誤差の尺度の勾配を決定するステップと、
前記マッピングサブシステムおよび前記計画サブシステムのパラメータに対する更新を決定するために、前記マッピングサブシステムおよび前記計画サブシステムを介して前記勾配を逆伝搬するステップと
を含む、方法。 A method for training a system comprising a mapping subsystem and a planning subsystem according to any one of claims 8 to 17, comprising:
Obtaining an image;
Processing the image using the mapping subsystem to determine a final characterization of an object's environment for a time step of the image;
Processing the final characterization using the planning subsystem to select a suggested action for the time step in the image;
Obtaining an optimal action for the time step of the image;
Determining a gradient of a measure of error between the selected action and the optimal action;
Back propagating the gradient through the mapping subsystem and the planning subsystem to determine updates to parameters of the mapping subsystem and the planning subsystem.
をさらに含む、請求項１８に記載の方法。 19. The method of claim 18, further comprising: causing an agent traveling in the environment to perform the proposed action with a probability p and perform the optimal action with a probability 1-p.
をさらに含む、請求項１９に記載の方法。 20. The method of claim 19, further comprising: increasing p during operation of the agent to increase the probability of selecting the optimal action.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762456945P | 2017-02-09 | 2017-02-09 | |
US62/456,945 | 2017-02-09 | ||
PCT/US2018/017666 WO2018148574A1 (en) | 2017-02-09 | 2018-02-09 | Agent navigation using visual inputs |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2020507857A true JP2020507857A (en) | 2020-03-12 |
JP7021236B2 JP7021236B2 (en) | 2022-02-16 |
Family
ID=63107124
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019543104A Active JP7021236B2 (en) | 2017-02-09 | 2018-02-09 | Agent navigation using visual input |
Country Status (6)
Country | Link |
---|---|
US (1) | US11010948B2 (en) |
EP (1) | EP3563344A4 (en) |
JP (1) | JP7021236B2 (en) |
KR (1) | KR102241404B1 (en) |
CN (1) | CN110268338B (en) |
WO (1) | WO2018148574A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10706840B2 (en) * | 2017-08-18 | 2020-07-07 | Google Llc | Encoder-decoder models for sequence to sequence mapping |
WO2020065024A1 (en) * | 2018-09-27 | 2020-04-02 | Deepmind Technologies Limited | Stacked convolutional long short-term memory for model-free reinforcement learning |
US11017499B2 (en) * | 2018-12-21 | 2021-05-25 | Here Global B.V. | Method, apparatus, and computer program product for generating an overhead view of an environment from a perspective image |
CN111044045B (en) * | 2019-12-09 | 2022-05-27 | 中国科学院深圳先进技术研究院 | Navigation method and device based on neural network and terminal equipment |
KR102430442B1 (en) | 2020-08-19 | 2022-08-09 | 경기대학교 산학협력단 | Agent learing reward system with region based alignment |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2004110802A (en) * | 2002-08-26 | 2004-04-08 | Sony Corp | Device, method for identifying environment, program, recording medium and robot device |
JP2011189481A (en) * | 2010-03-16 | 2011-09-29 | Sony Corp | Control apparatus, control method and program |
JP2015031922A (en) * | 2013-08-06 | 2015-02-16 | Kddi株式会社 | Information terminal equipment and program |
WO2017019555A1 (en) * | 2015-07-24 | 2017-02-02 | Google Inc. | Continuous control with deep reinforcement learning |
Family Cites Families (28)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR100204101B1 (en) * | 1990-03-02 | 1999-06-15 | 가나이 쓰도무 | Image processing apparatus |
US6954544B2 (en) * | 2002-05-23 | 2005-10-11 | Xerox Corporation | Visual motion analysis method for detecting arbitrary numbers of moving objects in image sequences |
CN1166922C (en) * | 2002-07-18 | 2004-09-15 | 上海交通大学 | Multiple-sensor and multiple-object information fusing method |
US7889905B2 (en) * | 2005-05-23 | 2011-02-15 | The Penn State Research Foundation | Fast 3D-2D image registration method with application to continuously guided endoscopy |
US7835820B2 (en) * | 2005-10-11 | 2010-11-16 | Vanderbilt University | System and method for image mapping and visual attention |
BRPI0712837B8 (en) * | 2006-06-11 | 2021-06-22 | Volvo Tech Corporation | method for determining and analyzing a location of visual interest |
US8175617B2 (en) * | 2009-10-28 | 2012-05-08 | Digimarc Corporation | Sensor-based mobile search, related methods and systems |
KR101832693B1 (en) * | 2010-03-19 | 2018-02-28 | 디지맥 코포레이션 | Intuitive computing methods and systems |
CN102243537B (en) * | 2010-05-14 | 2014-01-15 | 深圳市汇春科技有限公司 | Method and device for detecting displacement of motion image as well as optical mouse |
EP2609567A4 (en) * | 2010-08-25 | 2014-05-14 | Univ Sydney | Sensor data processing |
JP2012064131A (en) * | 2010-09-17 | 2012-03-29 | Tokyo Institute Of Technology | Map generating device, map generation method, movement method of mobile, and robot device |
US8553964B2 (en) * | 2010-10-20 | 2013-10-08 | Siemens Aktiengesellschaft | Unifying reconstruction and motion estimation in first pass cardiac perfusion imaging |
US8885941B2 (en) * | 2011-09-16 | 2014-11-11 | Adobe Systems Incorporated | System and method for estimating spatially varying defocus blur in a digital image |
CN102592287B (en) * | 2011-12-31 | 2014-06-04 | 浙江大学 | Convex optimization method for three-dimensional (3D)-video-based time-space domain motion segmentation and estimation model |
US9232140B2 (en) * | 2012-11-12 | 2016-01-05 | Behavioral Recognition Systems, Inc. | Image stabilization techniques for video surveillance systems |
US8700320B1 (en) * | 2012-11-13 | 2014-04-15 | Mordechai Teicher | Emphasizing featured locations during a journey |
US9761014B2 (en) * | 2012-11-15 | 2017-09-12 | Siemens Healthcare Gmbh | System and method for registering pre-operative and intra-operative images using biomechanical model simulations |
US10451428B2 (en) * | 2013-03-15 | 2019-10-22 | Volkswagen Aktiengesellschaft | Automatic driving route planning application |
US9195654B2 (en) * | 2013-03-15 | 2015-11-24 | Google Inc. | Automatic invocation of a dialog user interface for translation applications |
EP2869239A3 (en) * | 2013-11-04 | 2015-08-19 | Facebook, Inc. | Systems and methods for facial representation |
WO2016108847A1 (en) * | 2014-12-30 | 2016-07-07 | Nokia Technologies Oy | Methods and apparatus for processing motion information images |
US10062010B2 (en) * | 2015-06-26 | 2018-08-28 | Intel Corporation | System for building a map and subsequent localization |
US10093021B2 (en) * | 2015-12-02 | 2018-10-09 | Qualcomm Incorporated | Simultaneous mapping and planning by a robot |
US10788836B2 (en) * | 2016-02-29 | 2020-09-29 | AI Incorporated | Obstacle recognition method for autonomous robots |
CN105957060B (en) * | 2016-04-22 | 2019-01-11 | 天津师范大学 | A kind of TVS event cluster-dividing method based on optical flow analysis |
US9661473B1 (en) * | 2016-06-17 | 2017-05-23 | Qualcomm Incorporated | Methods and apparatus for determining locations of devices in confined spaces |
US10061987B2 (en) * | 2016-11-11 | 2018-08-28 | Google Llc | Differential scoring: a high-precision scoring method for video matching |
US10607134B1 (en) * | 2016-12-19 | 2020-03-31 | Jasmin Cosic | Artificially intelligent systems, devices, and methods for learning and/or using an avatar's circumstances for autonomous avatar operation |
-
2018
- 2018-02-09 JP JP2019543104A patent/JP7021236B2/en active Active
- 2018-02-09 KR KR1020197023399A patent/KR102241404B1/en active IP Right Grant
- 2018-02-09 CN CN201880010935.4A patent/CN110268338B/en active Active
- 2018-02-09 WO PCT/US2018/017666 patent/WO2018148574A1/en unknown
- 2018-02-09 EP EP18751319.7A patent/EP3563344A4/en active Pending
- 2018-02-09 US US16/485,140 patent/US11010948B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2004110802A (en) * | 2002-08-26 | 2004-04-08 | Sony Corp | Device, method for identifying environment, program, recording medium and robot device |
JP2011189481A (en) * | 2010-03-16 | 2011-09-29 | Sony Corp | Control apparatus, control method and program |
JP2015031922A (en) * | 2013-08-06 | 2015-02-16 | Kddi株式会社 | Information terminal equipment and program |
WO2017019555A1 (en) * | 2015-07-24 | 2017-02-02 | Google Inc. | Continuous control with deep reinforcement learning |
JP2018525759A (en) * | 2015-07-24 | 2018-09-06 | ディープマインド テクノロジーズ リミテッド | Continuous control by deep reinforcement learning |
Non-Patent Citations (6)
Title |
---|
NOBUTAKA KIMURA, ET AL.: ""Real-Time Updating of 2D Map for Autonomous Robot Locomotion Based on Distinction Between Static an", ADVENCED ROBOTICS, vol. Vol.26, No.11-12, JPN6021051925, 2012, pages 1343 - 1368, ISSN: 0004672967 * |
PATRAUCEAN, VIORICA, ET AL.: ""SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY"", ARXIV:1511.06309V5, vol. version v5, JPN6020029654, 1 September 2016 (2016-09-01), pages 1 - 13, ISSN: 0004324932 * |
TAMAR, AVIV, ET AL.: ""Value Iteration Networks"", PROCEEDINGS OF NIPS 2016, JPN6020029656, 2016, pages 1 - 9, ISSN: 0004324933 * |
江尻 理帆（外３名）: "「未知自然環境における単眼視によるナビゲーション方針」", 第２４回日本ロボット学会学術講演会予稿集, [CD-ROM], JPN6021051926, 14 September 2006 (2006-09-14), JP, pages 3, ISSN: 0004672966 * |
芦ヶ原 隆之: "「ロボットのためのビジョンシステムとその応用」", 映像情報メディア学会誌, vol. 60, no. 12, JPN6021051927, 1 December 2006 (2006-12-01), JP, pages 1914 - 1919, ISSN: 0004672965 * |
芦ヶ原隆之（外１名）: "「二足歩行ロボット用小型ステレオビジョンとそのアプリケーションの開発」", 情報処理学会研究報告, vol. 2003, no. 2, JPN6020029652, 17 January 2003 (2003-01-17), JP, pages 43 - 50, ISSN: 0004324931 * |
Also Published As
Publication number | Publication date |
---|---|
KR20190104587A (en) | 2019-09-10 |
JP7021236B2 (en) | 2022-02-16 |
KR102241404B1 (en) | 2021-04-16 |
CN110268338B (en) | 2022-07-19 |
EP3563344A1 (en) | 2019-11-06 |
US20190371025A1 (en) | 2019-12-05 |
WO2018148574A1 (en) | 2018-08-16 |
CN110268338A (en) | 2019-09-20 |
US11010948B2 (en) | 2021-05-18 |
EP3563344A4 (en) | 2020-07-22 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11790549B2 (en) | Unsupervised learning of image depth and ego-motion prediction neural networks | |
US11074481B2 (en) | Environment navigation using reinforcement learning | |
EP3535705B1 (en) | Reinforcement learning with auxiliary tasks | |
CN108027897B (en) | Continuous control with deep reinforcement learning | |
US20230252288A1 (en) | Reinforcement learning using distributed prioritized replay | |
CN110326004B (en) | Training a strategic neural network using path consistency learning | |
US10572798B2 (en) | Dueling deep neural networks | |
CN110546653B (en) | Action selection for reinforcement learning using manager and worker neural networks | |
JP2020507857A (en) | Agent navigation using visual input | |
US10860927B2 (en) | Stacked convolutional long short-term memory for model-free reinforcement learning | |
KR20220147154A (en) | Scene understanding and generation using neural networks | |
US20210073997A1 (en) | Future semantic segmentation prediction using 3d structure | |
JP7181415B2 (en) | Control agents for exploring the environment using the likelihood of observations | |
US20220215580A1 (en) | Unsupervised learning of object keypoint locations in images through temporal transport or spatio-temporal transport |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20191003 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20200817 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20201117 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20210412 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20210712 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20220104 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20220203 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7021236Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |