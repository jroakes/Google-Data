WO2023200718A1 - Contextual assistant using mouse pointing or touch cues - Google Patents
Contextual assistant using mouse pointing or touch cues Download PDFInfo
- Publication number
- WO2023200718A1 WO2023200718A1 PCT/US2023/018044 US2023018044W WO2023200718A1 WO 2023200718 A1 WO2023200718 A1 WO 2023200718A1 US 2023018044 W US2023018044 W US 2023018044W WO 2023200718 A1 WO2023200718 A1 WO 2023200718A1
- Authority
- WO
- WIPO (PCT)
- Prior art keywords
- query
- gui
- location
- displayed
- screen
- Prior art date
Links
- 230000004044 response Effects 0.000 claims abstract description 40
- 238000012545 processing Methods 0.000 claims abstract description 37
- 238000000034 method Methods 0.000 claims abstract description 33
- 238000013518 transcription Methods 0.000 claims abstract description 28
- 230000035897 transcription Effects 0.000 claims abstract description 28
- 230000015654 memory Effects 0.000 claims description 45
- 230000009471 action Effects 0.000 claims description 34
- 238000004891 communication Methods 0.000 claims description 12
- 238000001514 detection method Methods 0.000 claims description 6
- 238000013519 translation Methods 0.000 claims description 6
- 230000003213 activating effect Effects 0.000 claims description 4
- QQWUGDVOUVUTOY-UHFFFAOYSA-N 5-chloro-N2-[2-methoxy-4-[4-(4-methyl-1-piperazinyl)-1-piperidinyl]phenyl]-N4-(2-propan-2-ylsulfonylphenyl)pyrimidine-2,4-diamine Chemical compound COC1=CC(N2CCC(CC2)N2CCN(C)CC2)=CC=C1NC(N=1)=NC=C(Cl)C=1NC1=CC=CC=C1S(=O)(=O)C(C)C QQWUGDVOUVUTOY-UHFFFAOYSA-N 0.000 claims 18
- 238000004590 computer program Methods 0.000 description 8
- 230000003287 optical effect Effects 0.000 description 6
- 230000008569 process Effects 0.000 description 3
- 230000008901 benefit Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000004984 smart glass Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000005236 sound signal Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/033—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor
- G06F3/038—Control and interface arrangements therefor, e.g. drivers or device-embedded control circuitry
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/68—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/683—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/685—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using automatically derived transcript of audio data, e.g. lyrics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/017—Gesture based interaction, e.g. based on a set of recognized hand gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2203/00—Indexing scheme relating to G06F3/00 - G06F3/048
- G06F2203/038—Indexing scheme relating to G06F3/038
- G06F2203/0381—Multimodal input, i.e. interface arrangements enabling the user to issue commands by simultaneous use of input devices of different nature, e.g. voice plus gesture on digitizer
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
Definitions
- This disclosure relates to a contextual assistant using mouse pointing or touch cues.
- a speech-enabled environment permits a user to speak a query aloud and a digital assistant will perform an action to obtain an answer to the query.
- Digital assistants are particularly effective in providing accurate answers to general topic queries, where the query itself generates the necessary information for the digital assistant to obtain an answer to the query.
- the digital assistant requires additional context before it can obtain an answer to the query.
- identifying the attention of the user when the user spoke the query aloud provides the additional context needed to obtain an answer to the query. Consequently, the digital assistant that receives the query must have some way of identifying additional context of the user that spoke the query.
- One aspect of the disclosure provides a computer-implemented method that when executed by data processing hardware causes the data processing hardware to perform operations that include receiving audio data corresponding to a query spoken by a user and captured by an assistant-enabled device associated with the user.
- the operations also include receiving, in a graphical user interface (GUI) displayed on a screen in communication with the data processing hardware, a user input indication indicating a spatial input applied at a first location on the screen, and processing, using a speech recognition model, the audio data to determine a transcription of the query.
- the operations also include performing query interpretation on the transcription of the query to determine that the query is referring to an object displayed on the screen without uniquely identifying the object and requesting information about the object displayed on the screen.
- GUI graphical user interface
- the operations also include disambiguating, using the user input indication indicating the spatial input applied at the first location on the screen, the query to uniquely identify the object that the query is referring to, and in response to uniquely identifying the object, obtaining the information about the object requested by the query.
- the operations also include providing a response to the query that includes the obtained information about the object.
- Implementations of the disclosure may include one or more of the following optional features.
- the operations also include detecting a trigger event, and in response to detecting the trigger event, activating: the GUI displayed on the screen to enable detection of spatial inputs; and the speech recognition model to enable the performance of speech recognition on incoming audio data captured by the assistant-enabled device.
- detecting the trigger event includes detecting, by a hotword detector, a presence of a hotword in the received audio data.
- detecting the trigger event may include one of: receiving, in the GUI displayed on the screen, a user input indication indicating selection of a graphical element; receiving a user input indication indicating selection of a physical button disposed on the assistant-enabled device; detecting a predefined gesture performed by the user; or detecting a predefined movement/pose of the assistant-enabled device.
- receiving the user input indication indicating the spatial input applied at the first location comprises one of: detecting that a position of a cursor is displayed in the GUI at the first location when the user spoke the query; detecting a touch input received in the GUI at the first location when the user spoke the query; or detecting a lassoing action performed in the GUI at the first location when the user spoke the query.
- disambiguating the query to uniquely identify the object includes: receiving image data including a plurality of candidate objects displayed in the GUI and corresponding locations of the plurality of candidate objects displayed in the GUI; and identifying the candidate object from the plurality of candidate objects having the corresponding location that is closest to the first location as the object the query is referring to.
- receiving the user input indication indicating the spatial input applied at the first location includes receiving the user input indication indicating the spatial input applied at the first location, and disambiguating the query to uniquely identify the object includes uniquely identifying the sequence of characters underlined by the underlining action as the object the query is referring to.
- receiving the user input indication indicating the spatial input applied at the first location includes detecting a highlighting action performed in the GUI that highlights a sequence of characters displayed in the GUI at the first location, and disambiguating the query to uniquely identify the object includes uniquely identifying the sequence of characters highlighted by the highlighting action as the object the query is referring to.
- obtaining the information about the object requested by the query includes: querying a search engine using the uniquely identified object and one or more terms in the transcription of the query to obtain a list of results responsive to the query; and displaying, in the GUI displayed on the screen, the list of results responsive to the query.
- displaying the list of results responsive to the query may further include generating a graphical element representing a highest ranked result in the list of results responsive to the query and displaying, in the GUI displayed on the screen, the list of results responsive to the query at the first location on the screen.
- the operations may further include determining that the uniquely identified object includes text in a first language such that obtaining the information about the object requested by the query includes obtaining a translation of the text in a second language different than the first language.
- the memory hardware stores instructions that when executed on the data processing hardware causes the data processing hardware to perform operations that include receiving audio data corresponding to a query spoken by a user and captured by an assistant-enabled device associated with the user.
- the operations also include receiving, in a graphical user interface (GUI) displayed on a screen in communication with the data processing hardware, a user input indication indicating a spatial input applied at a first location on the screen, and processing, using a speech recognition model, the audio data to determine a transcription of the query.
- GUI graphical user interface
- the operations also include performing query interpretation on the transcription of the query to determine that the query is referring to an object displayed on the screen without uniquely identifying the object and requesting information about the object displayed on the screen.
- the operations also include disambiguating, using the user input indication indicating the spatial input applied at the first location on the screen, the query to uniquely identify the object that the query is referring to, and in response to uniquely identifying the object, obtaining the information about the object requested by the query.
- the operations also include providing a response to the query that includes the obtained information about the object.
- the operations also include detecting a trigger event, and in response to detecting the trigger event, activating: the GUI displayed on the screen to enable detection of spatial inputs; and the speech recognition model to enable the performance of speech recognition on incoming audio data captured by the assistant- enabled device.
- detecting the trigger event includes detecting, by a hotword detector, a presence of a hotword in the received audio data.
- detecting the trigger event may include one of: receiving, in the GUI displayed on the screen, a user input indication indicating selection of a graphical element; receiving a user input indication indicating selection of a physical button disposed on the assistant- enabled device; detecting a predefined gesture performed by the user; or detecting a predefined movement/pose of the assistant-enabled device
- receiving the user input indication indicating the spatial input applied at the first location comprises one of: detecting that a position of a cursor is displayed in the GUI at the first location when the user spoke the query; detecting a touch input received in the GUI at the first location when the user spoke the query; or detecting a lassoing action performed in the GUI at the first location when the user spoke the query.
- disambiguating the query to uniquely identify the object includes: receiving image data including a plurality of candidate objects displayed in the GUI and corresponding locations of the plurality of candidate objects displayed in the GUI; and identifying the candidate object from the plurality of candidate objects having the corresponding location that is closest to the first location as the object the query is referring to.
- receiving the user input indication indicating the spatial input applied at the first location includes receiving the user input indication indicating the spatial input applied at the first location, and disambiguating the query to uniquely identify the object includes uniquely identifying the sequence of characters underlined by the underlining action as the object the query is referring to.
- receiving the user input indication indicating the spatial input applied at the first location includes detecting a highlighting action performed in the GUI that highlights a sequence of characters displayed in the GUI at the first location, and disambiguating the query to uniquely identify the object includes uniquely identifying the sequence of characters highlighted by the highlighting action as the object the query is referring to.
- obtaining the information about the object requested by the query includes: querying a search engine using the uniquely identified object and one or more terms in the transcription of the query to obtain a list of results responsive to the query; and displaying, in the GUI displayed on the screen, the list of results responsive to the query.
- displaying the list of results responsive to the query may further include generating a graphical element representing a highest ranked result in the list of results responsive to the query and displaying, in the GUI displayed on the screen, the list of results responsive to the query at the first location on the screen.
- the operations may further include determining that the uniquely identified object includes text in a first language such that obtaining the information about the object requested by the query includes obtaining a translation of the text in a second language different than the first language.
- FIG. l is a schematic view of an example system including a contextual assistant using mouse pointing or touch cues.
- FIG. 2 is a schematic view of example components of the contextual assistant.
- FIGS. 3 A-3C are example graphical user interfaces (GUIs) rendered on a screen of a user device including the contextual assistant.
- GUIs graphical user interfaces
- FIG. 4 is a flowchart of an example arrangement of operations for a method of disambiguating a query using mouse pointing or touch cues.
- FIG. 5 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
- a user’s manner of interacting with an assistant-enabled device is designed to be primarily, if not exclusively, by means of voice input. While assistant-enabled devices are effective at obtaining answers to general topic queries (e g., what’s the capital of Michigan?), context-driven queries require the assistant-enable device to obtain additional information to obtain an accurate answer. For instance, the assistant-enabled device may struggle to obtain a confident/accurate answer to the query “show me more of these,” without more context.
- the assistant-enabled device benefits from including image data derived from a screen of the assistant-enabled device. For instance, a user might query the assistant- enabled device in a natural manner by speaking “Show me more windows like that.” Here, the spoken query identifies that the user is looking for windows similar to an object but is ambiguous because the object is unknown from the linguistic content of the query.
- image data from the screen of the assistant-enabled device may allow the assistant- enabled device to narrow the potential windows to search for from an entire screen showing a city down to a distinct sub-region including a specific building in the city where a user input applied at a particular location on the screen has been detected in conjunction with the spoken query.
- the assistant-enable device is able to generate a response to a query about the building in the city despite the user needing to explicitly identify the building in the spoken query.
- FIG. 1 is an example of a system 100 including a user device 10 and/or a remote system 60 in communication with the user device 10 via a network 40.
- the user device 10 and/or the remote system 60 executes a point assistant 200 that a user 102 may interact with through speech and spatial inputs such that the point assistant 200 is capable of generating responses to queries referring to objects displayed on a screen of the user device 10, despite the query failing to uniquely identify an object for which the query seeks information.
- the user device 10 corresponds to a smart phone, however the user device 10 can include other computing devices having, or in communication with, display screens, such as, without limitation, a tablet, smart display, desktop/laptop, smart watch, smart appliance, smart glasses/headset, or vehicle infotainment device.
- the user device 10 includes data processing hardware 12 and memory hardware 14 storing instructions that when executed on the data processing hardware 12 cause the data processing hardware 12 to perform operations.
- the remote system 60 e.g., server, cloud computing environment
- the point assistant 200 executing on the user device 10 and/or the remote system 60 includes a speech recognizer 210 and a response generator 250, and has access to one or more information sources 240 stored on the memory hardware 14, 64. In some examples, execution of the point assistant 200 is shared across the user device 10 and the remote system 60.
- the user device 10 includes an array of one or more microphones 16 configured to capture acoustic sounds such as speech directed toward the user device 10.
- the user device 10 also executes, for display on a screen 18 in communication with the data processing hardware 12, a graphical user interface (GUI) 300 configured to capture user input indications via any one of touch, gesture, gaze, and/or an input device (e.g., mouse, trackpad, or stylist) for controlling functionality of the user device 10.
- the GUI 300 may be an interface associated with an application 50 executing on the user device 10 that presents a plurality of objects in the GUI 300.
- the user device 10 may further include, or be in communication with, an audio output device (e.g., a speaker) 19 that may output audio such as music and/or synthesized speech from the point assistant 200.
- the user device 10 may also include a physical button 17 disposed on the user device 10 and configured to receive a tactile selection by a user 102 for invoking the point assistant 200.
- the user device 10 may include an audio subsystem 106 for extracting audio data 202 (FIG. 2) from a query 104.
- the audio subsystem 106 may receive streaming audio captured by the one or more microphones 16 of the user device 10 that corresponds to an utterance 106 of a query 104 spoken by the user 102 and extract the audio data (e.g., acoustic frames) 202.
- the audio data 202 may include acoustic features such as Mel -frequency cepstrum coefficients (MFCCs) or filter bank energies computed over windows of an audio signal.
- MFCCs Mel -frequency cepstrum coefficients
- filter bank energies computed over windows of an audio signal.
- the query 104 spoken by the user 102 includes “Hey Google, what is this?”
- the user device 10 may execute (i.e., on the data processing hardware 12) a hotword detector 20 configured to detect a presence of a hotword 105 in streaming audio without performing semantic analysis or speech recognition processing on the streaming audio.
- the hotword detector 20 may execute on the audio subsystem 106.
- the hotword detector 202 may receive the audio data 202 to determine whether the utterance 106 includes a particular hotword 105 (e.g., Hey Google) spoken by the user 102. That is, the hotword detector 20 may be trained to detect the presence of the hotword 105 (e.g., Hey Google) or one or more other variants of the hotword (e.g., Ok Google) in the audio data 202.
- a hotword detector 20 configured to detect a presence of a hotword 105 in streaming audio without performing semantic analysis or speech recognition processing on the streaming audio.
- the hotword detector 20 may execute on the audio subsystem 106.
- the hotword detector 202 may receive the audio data 202 to determine whether the utterance 106
- Detecting the presence of the hotword 105 in the audio data 202 may correspond to a trigger event that invokes the point assistant 200 to activate the GUI 300 displayed on the screen 18 to enable the detection of spatial inputs 112, and activate the speech recognizer 210 to perform speech recognition on the audio data 202 corresponding to the utterance 106 of the hotword 105 and/or one or more other terms characterizing the query 104 that follows the hotword.
- the hotword 105 is spoken in the utterance 106 subsequent to the query 105 such the portion of the audio data 202 characterizing the query 104 is buffered and retrieved by the speech recognizer 210 retrieves a portion of the audio data 202 upon detection of the hotword 105 in the audio data 202.
- the trigger event includes receiving, in the GUI 300, a user input indication indicating selection of a graphical element 21 (e.g., a graphical microphone). In other implementations, the trigger event includes receiving a user input indication indicating selection of the physical button 17 disposed on the user device 10. In other implementations, the trigger event includes detecting (e.g., via image and/or radar sensors) a predefined gesture performed by the user 102, or detecting a predefined movement/pose of the user device 10 (e.g., using one or more sensors such as an accelerometer and/or gyroscope).
- the user device 10 may further include an image subsystem 108 configured to extract a location 114 (e.g., an X-Y coordinate location) on the screen 18 of a spatial input 112 applied in the GUI 300.
- a location 114 e.g., an X-Y coordinate location
- the user 102 may provide a user input indication 110 indicating the spatial input 112 in the GUI 300 at the location 114 on the screen.
- the image subsystem 108 may additionally extract image data (e.g., pixels) 204 corresponding to one or more objects 116 currently displayed on the screen 18.
- the GUI 300 receives the user input indication 110 indicating the spatial input 112 applied at a first location 114 on the screen 18, wherein the image data 202 includes an object (i.e., a golden retriever) 116 displayed on the screen 18 proximate to the first location 114.
- an object i.e., a golden retriever
- the speech recognizer 210 executes an automatic speech recognition (ASR) model (e.g., a speech recognition model) 212 that receives, as input, the audio data 202 and generates/predicts, as output, a corresponding transcription 214 of the query 104.
- ASR automatic speech recognition
- the query 104 includes the phrase, “what is this?”, that requests information 246 about an object 116 displayed in the GUI 300 on the screen without uniquely identifying the object 116.
- the point assistant 200 uses the spatial input 112 applied at the first location 114 on the screen 118 to disambiguate the query 104 for uniquely identify the object 116 that the query 104 is referring to.
- the point assistant 200 may obtain the information 246 about the object and generate a response 252 to the query 104 that includes obtained information 246 about the object 116.
- the response generator 250 may generate the response 252 to the query 104 as a textual representation.
- the point assistant 200 instructs the user device 10 to display the response 252 in the GUI 300 for the user 102 to read.
- the point assistant 200 generates a textual representation of the response 252 “That is a golden retriever” for display in the GUI 300.
- the point assistant 200 may require the additional context extracted by the image subsystem 108 (i.e., that the user 102 applied a spatial input 112 at the first location 114 corresponding to the object 116) in order to uniquely identify the object 116 the query 104 is referring to in order to obtain the information 246 for inclusion in the response 252.
- the response generator 250 employs a text-to-speech (TTS) system 260 to convert the textual representation of the response 252 into synthesized speech.
- TTS text-to-speech
- the point assistant 200 generates the synthesized speech for audible output from the speaker 19 of the user device 10 in addition to, or in lieu of, displaying the textual representation of the response 252 in the GUI 300.
- the point assistant 200 further includes a natural language understanding (NLU) module 220 configured to perform query interpretation on the corresponding transcription 214 to ultimately determine a meaning behind the transcription 214.
- the NLU module 220 may also receive context information 201 to assist with interpreting the transcription 214.
- the context information 201 may indicate an application 50 (FIG. 1) currently executing on the user device 10, previous queries 104 from the user 102, a particular hotword 105 was detected, or any other information that the NLU module 220 can leverage for interpreting the query 104.
- the context information 201 may indicate that the user is interacting with a webbased application 50 executing on the user device 102 and the NLU module 230 performs query interpretation to determine that the query 104 specifies an action 232 to obtain a description/information about some object 116 displayed in the GUI 300 that the user 102 is likely viewing.
- the NLU module 230 determines that the query 104 is ambiguous since the object 116 is not explicitly identified in the transcription 214 but for the term “this”.
- query interpretation performed by the NLU module 230 determines that the query 104 refers an object 116 displayed on the screen 18 without uniquely identifying the object 116 and specifies an action 232 to request information 246 about the object 116.
- the NLU module 220 needs to disambiguate the query 104 to uniquely identify the object 116 the query 104 is referring to. For example, in a scenario where a query 104 includes a corresponding transcription 214 “show me similar bicycles” while multiple bicycles are currently displayed on the screen 18,” the NLU module 220 may perform query interpretation on the corresponding transcription 214 to identify that the user 102 is referring to an object (i.e., a bicycle) 116 displayed in the GUI 300 without uniquely identifying the object 116, and requesting information 246 about the object 116 (i.e., other objects similar to the bicycle 116).
- an object i.e., a bicycle
- the NLU module 220 determines that query 104 specifies an action 232 to retrieve images of bicycles similar to one of the bicycles displayed on the screen, but cannot fulfil the query 104 because the bicycle that the query is referring to cannot be ascertained from the transcription 214.
- the NLU module 220 may use a user input indication indicating a spatial input 112 applied at the first location 114 on the screen as additional context for disambiguating the query 104 to uniquely identify the object 116 the query is referring to.
- the NLU module 220 may additionally use image data 204 for disambiguating the query 104.
- the image data 204 may include a plurality of candidate objects displayed in the GUI and corresponding locations of the plurality of candidate objects displayed in the GUI.
- the image data 204 may be extracted by the image subsystem 108 from graphical content rendered for display in the GUI 300.
- the image data 204 may include labels that identify the candidate objects.
- the image subsystem 108 performs one or more object recognition techniques on the graphical content in order to identify the candidate objects.
- the NLU module 220 may be able uniquely identify the object as an object rendered for display in the GUI 300 that is closest to the first location 114 of the spatial input 118.
- the content of the transcription 214 can further narrow down the possibility of objects the query refers to by at least describing a type of object or indicating one or more features/characteristics of the object the query refers to.
- receiving the user input indication 110 indicating the spatial input 112 at the first location 114 includes detecting that a position of a cursor 310 is displayed in a GUI 300a at the first location 114 when the user 102 spoke the query 104.
- the NLU module 220 further receives image data 204 including a plurality of candidate objects 320, 320a-c displayed in the GUI 300.
- Each candidate object 320 of the plurality of candidate objects 320 includes a corresponding location 322, 322a-c in the GUI 300a displayed on the screen.
- These locations may, for example, be quantified or otherwise characterized using one or more coordinate systems such as Cartesian coordinates using a pixel coordinate system where the origin is defined by the bottom left of the GUI 300a, or a polar coordinate system.
- each of the candidate objects 320 may be spatially defined by a bounding box 330a, 330a-c or a box with the smallest measure within which all of the candidate object 320 lies.
- the NLU module 220 may identify a candidate object 320c from the plurality of candidate objects 320 as having the corresponding location 322c that is closest to the first location 114 as the object 116 the query 104 is referring to.
- the NLU module 220 may employ a best intersection technique to compute the overlap between the two or more bounding boxes 330 in order to identify the object 116 the query 104 is referring to.
- the position of the cursor 310 indicates the spatial input 112 is applied at the location 114 where an object 116 that includes the sun is displayed.
- the user input indication 110 indicating the spatial input 112 at the first location 114 includes detecting a touch input received in GUI 300 at the first location 114 when the user 102 spoke the query 104.
- the user input indication 110 indicating the spatial input 112 at the first location 114 includes detecting a lassoing action performed in the GUI 300 at the first location 114 when the user 102 spoke the query 104.
- receiving the user input indication 110 indicating the spatial input 112 at the first location 114 includes detecting a lassoing action performed in a GUI 300b at a first location 114.
- the NLU module 220 uses the first location 114 in the image data 204, to crop a subset of the image data 204 contained within a region identified by the lassoing action and located at the first location 114 to uniquely identify the object 116 the query 104 is referring to.
- the object within the region of the lassoing action includes a building.
- receiving the user input indication 110 indicating the spatial input 112 at the first location includes detecting an underlining action performed in a GUI 300c at a first location 114.
- the query 104 may be directed to a sequence of characters (e.g., “Bienvenue au cours de frangais!” jdisplayed in the GUI 300c at the first location 114.
- the query 104 may include the phrase “What does this say?” Like in FIG.
- the NLU module 220 may identify a candidate object 320 (e.g., the underlined sequence of characters) as having a corresponding location 322 that is closest to the first location 114 as the object 116 the query 104 is referring to.
- the user input indication 110 indicating the spatial input 112 at the first location 114 includes detecting a highlighting action performed in the GUI 300c that highlights the sequence of characters (e.g., “Bienvenue au cours de frangais!”) at the first location 114.
- the disambiguation model 230 disambiguates the query 104 to uniquely identify the object 116 the query is referring to as the sequence of characters highlighted by the highlighting action.
- the NLU module 220 inserts the object 116 into a missing object slot of the action 232 and performs the action 232 of obtaining the information 246 about the uniquely identified object 116 requested by the query 104.
- the point assistant 200 performs the identified action 232 to obtain the information 246 about the object 116 requested by the query 104 by querying an information source 240.
- the information source 240 may include a search engine 242, where the point assistant 200 queries the search engine 242 using the uniquely identified object 116 and one or more terms in the transcription 214 of the query 104 to obtain the information 246 about the object 116 requested by the query 104.
- the point assistant 200 queries the search engine to obtain information 238 that includes a description of a golden retriever uniquely identified as the object 116 requested by the query, in addition to the one or more words in the transcription 214 “what is this?”
- the information source may include an object recognition engine 244 that applies image processing techniques to detect and recognize patterns (i.e., a golden retriever) in the image data 204 in order obtain the information 238 that classifies the object 116 as a golden retriever and provides information about golden retrievers.
- the information could include a link to a content source (e.g., webpage). That is, the information source 240 may use the image data 204 along with the transcription 214 of the query 104 to obtain the information 246 requested by the query 104.
- the response generator 250 receives the information 246 requested by the query 104 and generates the response 252 “That is a golden retriever.” As discussed above, the response generator 250 may generate the response 252 to the query 104 as a textual representation 19 displayed in the GUI 300 on the screen of the user device 10.
- the point assistant 200 queries the search engine 242 to obtain a list of results responsive to the query 104.
- the query 104 may be a similarity query 104, where the user 102 seeks a list of results with a visual similarity to the object 116 in the GUI 300 on the screen of the user device 10.
- the response generator 250 may generate the response 252 to the query 104 as a textual representation 19 including the list of results displayed in the GUI 300 on the screen of the user device 10.
- the point assistant 200 may further generate a graphical element representing a highest ranked result in the list of results responsive to the query 104, where the highest ranked result is displayed more prominently (e.g., larger font, highlighted color, at the first location 114) than the remaining results in the list of ranked results.
- the point assistant 200 determines that the uniquely identified object 116 includes text in a first language (e.g., French).
- a first language e.g., French
- the user 102 that spoke the query 104 may speak only speak a second language (e.g., English) different than the first language.
- the uniquely identified object 116 includes text in a first language “Bienvenue au cours de franqais!”
- the information source 240 may obtain a translation of the uniquely identified object 116 in the second language “Welcome to French class!”
- the information source 240 may include a text-to-text machine translation model.
- FIG. 4 is a flowchart of an exemplary arrangement of operations for a method 400 for a contextual assistant to use mouse pointing or touch cues.
- the method 400 includes, at operation 402, receiving audio data 202 corresponding to a query 104 spoken by a user 102 and captured by an assistant-enabled device (e.g., a user device) 10 associated with the user 102.
- the method 400 further includes, at operation 404, receiving, in a graphical user interface 300 displayed on a screen in communication with data processing hardware 12, a user input indication 110 indicating a spatial input 112 applied at a first location 114 on the screen.
- the method 400 includes processing, using a speech recognition model 212, the audio data 202 to determine a transcription 214 of the query 104.
- the method 400 also includes performing query interpretation on the transcription 214 of the query 104 to determine that the query 104 is referring to an object 116 displayed on the screen without uniquely identifying the object 116, and requesting information 256 about the object 116 displayed on the screen.
- the method 400 further includes, at operation 410, disambiguating, using the user input indication 110 indicating the spatial input 112 applied at the first location 114 on the screen, the query 104 to uniquely identify the object 116 that the query 104 is referring to.
- the method 400 in response to uniquely identifying the object 116, the method 400 includes obtaining the information 246 about the object 116 requested by the query 104.
- the method 400 further includes, at operation 414, providing a response 252 to the query 104 that includes the obtained information 246 about the object 116.
- FIG. 5 is schematic view of an example computing device 500 that may be used to implement the systems and methods described in this document.
- the computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 500 includes a processor 510, memory 520, a storage device 530, a high-speed interface/controller 540 connecting to the memory 520 and high-speed expansion ports 550, and a low speed interface/controller 560 connecting to a low speed bus 570 and a storage device 530.
- Each of the components 510, 520, 530, 540, 550, and 560, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 510 e.g., data processing hardware 12 or data processing hardware 62 of FIG.
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 500 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 520 e g., memory hardware 14 or memory hardware 64 of FIG.
- the memory 520 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 520 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 500.
- Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) I electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 530 is capable of providing mass storage for the computing device 500.
- the storage device 530 is a computer- readable medium.
- the storage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 520, the storage device 530, or memory on processor 510.
- the high speed controller 540 manages bandwidth-intensive operations for the computing device 500, while the low speed controller 560 manages lower bandwidthintensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 540 is coupled to the memory 520, the display 580 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 550, which may accept various expansion cards (not shown).
- the low-speed controller 560 is coupled to the storage device 530 and a low-speed expansion port 590.
- the low-speed expansion port 590 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 500a or multiple times in a group of such servers 500a, as a laptop computer 500b, or as part of a rack server system 500c.
- Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- a software application may refer to computer software that causes a computing device to perform a task.
- a software application may be referred to as an “application,” an “app,” or a “program.”
- Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- the non-transitory memory may be physical devices used to store programs (e g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device.
- the non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of nonvolatile memory include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- RAM random access memory
- DRAM dynamic random access memory
- SRAM static random access memory
- PCM phase change memory
- the processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input
Abstract
A method (400) includes receiving audio data (202) corresponding to a query (104) spoken by a user, receiving, in a graphical user interface (300) displayed on a screen, a user input indication indicating a spatial input (212) applied at a first location (114) on the screen, and processing the audio data to determine a transcription (214) of the query. The method also includes performing query interpretation on the transcription to determine that the query is referring to an object (116) displayed on the screen without uniquely identifying the object, and requesting information about the object. The method further includes disambiguating, using the user input indication indicating the spatial input applied at the first location on the screen, the query to uniquely identify the object that the query is referring to, obtaining the information about the object requested by the query, and providing a response (252) to the query.
Description
Contextual Assistant Using Mouse Pointing or Touch Cues
TECHNICAL FIELD
[0001] This disclosure relates to a contextual assistant using mouse pointing or touch cues.
BACKGROUND
[0002] A speech-enabled environment permits a user to speak a query aloud and a digital assistant will perform an action to obtain an answer to the query. Digital assistants are particularly effective in providing accurate answers to general topic queries, where the query itself generates the necessary information for the digital assistant to obtain an answer to the query. However, where a query is ambiguous, the digital assistant requires additional context before it can obtain an answer to the query. In some instances, identifying the attention of the user when the user spoke the query aloud provides the additional context needed to obtain an answer to the query. Consequently, the digital assistant that receives the query must have some way of identifying additional context of the user that spoke the query.
SUMMARY
[0003] One aspect of the disclosure provides a computer-implemented method that when executed by data processing hardware causes the data processing hardware to perform operations that include receiving audio data corresponding to a query spoken by a user and captured by an assistant-enabled device associated with the user. The operations also include receiving, in a graphical user interface (GUI) displayed on a screen in communication with the data processing hardware, a user input indication indicating a spatial input applied at a first location on the screen, and processing, using a speech recognition model, the audio data to determine a transcription of the query. The operations also include performing query interpretation on the transcription of the query to determine that the query is referring to an object displayed on the screen without uniquely identifying the object and requesting information about the object displayed on the screen. The operations also include disambiguating, using the user input indication
indicating the spatial input applied at the first location on the screen, the query to uniquely identify the object that the query is referring to, and in response to uniquely identifying the object, obtaining the information about the object requested by the query. The operations also include providing a response to the query that includes the obtained information about the object.
[0004] Implementations of the disclosure may include one or more of the following optional features. In some implementations, the operations also include detecting a trigger event, and in response to detecting the trigger event, activating: the GUI displayed on the screen to enable detection of spatial inputs; and the speech recognition model to enable the performance of speech recognition on incoming audio data captured by the assistant-enabled device. In these implementations, detecting the trigger event includes detecting, by a hotword detector, a presence of a hotword in the received audio data. Alternatively, detecting the trigger event may include one of: receiving, in the GUI displayed on the screen, a user input indication indicating selection of a graphical element; receiving a user input indication indicating selection of a physical button disposed on the assistant-enabled device; detecting a predefined gesture performed by the user; or detecting a predefined movement/pose of the assistant-enabled device.
[0005] In some examples, receiving the user input indication indicating the spatial input applied at the first location comprises one of: detecting that a position of a cursor is displayed in the GUI at the first location when the user spoke the query; detecting a touch input received in the GUI at the first location when the user spoke the query; or detecting a lassoing action performed in the GUI at the first location when the user spoke the query. In these examples, disambiguating the query to uniquely identify the object includes: receiving image data including a plurality of candidate objects displayed in the GUI and corresponding locations of the plurality of candidate objects displayed in the GUI; and identifying the candidate object from the plurality of candidate objects having the corresponding location that is closest to the first location as the object the query is referring to.
[0006] In additional examples, receiving the user input indication indicating the spatial input applied at the first location includes receiving the user input indication
indicating the spatial input applied at the first location, and disambiguating the query to uniquely identify the object includes uniquely identifying the sequence of characters underlined by the underlining action as the object the query is referring to. In other examples, receiving the user input indication indicating the spatial input applied at the first location includes detecting a highlighting action performed in the GUI that highlights a sequence of characters displayed in the GUI at the first location, and disambiguating the query to uniquely identify the object includes uniquely identifying the sequence of characters highlighted by the highlighting action as the object the query is referring to.
[0007] In some implementations, obtaining the information about the object requested by the query includes: querying a search engine using the uniquely identified object and one or more terms in the transcription of the query to obtain a list of results responsive to the query; and displaying, in the GUI displayed on the screen, the list of results responsive to the query. Here, displaying the list of results responsive to the query may further include generating a graphical element representing a highest ranked result in the list of results responsive to the query and displaying, in the GUI displayed on the screen, the list of results responsive to the query at the first location on the screen. Optionally, the operations may further include determining that the uniquely identified object includes text in a first language such that obtaining the information about the object requested by the query includes obtaining a translation of the text in a second language different than the first language.
[0008] Another aspect of the disclosure provides a system including data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that when executed on the data processing hardware causes the data processing hardware to perform operations that include receiving audio data corresponding to a query spoken by a user and captured by an assistant-enabled device associated with the user. The operations also include receiving, in a graphical user interface (GUI) displayed on a screen in communication with the data processing hardware, a user input indication indicating a spatial input applied at a first location on the screen, and processing, using a speech recognition model, the audio data
to determine a transcription of the query. The operations also include performing query interpretation on the transcription of the query to determine that the query is referring to an object displayed on the screen without uniquely identifying the object and requesting information about the object displayed on the screen. The operations also include disambiguating, using the user input indication indicating the spatial input applied at the first location on the screen, the query to uniquely identify the object that the query is referring to, and in response to uniquely identifying the object, obtaining the information about the object requested by the query. The operations also include providing a response to the query that includes the obtained information about the object.
[0009] This aspect may include one or more of the following optional features. In some implementations, the operations also include detecting a trigger event, and in response to detecting the trigger event, activating: the GUI displayed on the screen to enable detection of spatial inputs; and the speech recognition model to enable the performance of speech recognition on incoming audio data captured by the assistant- enabled device. In these implementations, detecting the trigger event includes detecting, by a hotword detector, a presence of a hotword in the received audio data. Alternatively, detecting the trigger event may include one of: receiving, in the GUI displayed on the screen, a user input indication indicating selection of a graphical element; receiving a user input indication indicating selection of a physical button disposed on the assistant- enabled device; detecting a predefined gesture performed by the user; or detecting a predefined movement/pose of the assistant-enabled device
[0010] In some examples, receiving the user input indication indicating the spatial input applied at the first location comprises one of: detecting that a position of a cursor is displayed in the GUI at the first location when the user spoke the query; detecting a touch input received in the GUI at the first location when the user spoke the query; or detecting a lassoing action performed in the GUI at the first location when the user spoke the query. In these examples, disambiguating the query to uniquely identify the object includes: receiving image data including a plurality of candidate objects displayed in the GUI and corresponding locations of the plurality of candidate objects displayed in the GUI; and identifying the candidate object from the plurality of candidate objects having
the corresponding location that is closest to the first location as the object the query is referring to.
[00111 In additional examples, receiving the user input indication indicating the spatial input applied at the first location includes receiving the user input indication indicating the spatial input applied at the first location, and disambiguating the query to uniquely identify the object includes uniquely identifying the sequence of characters underlined by the underlining action as the object the query is referring to. In other examples, receiving the user input indication indicating the spatial input applied at the first location includes detecting a highlighting action performed in the GUI that highlights a sequence of characters displayed in the GUI at the first location, and disambiguating the query to uniquely identify the object includes uniquely identifying the sequence of characters highlighted by the highlighting action as the object the query is referring to.
[0012] In some implementations, obtaining the information about the object requested by the query includes: querying a search engine using the uniquely identified object and one or more terms in the transcription of the query to obtain a list of results responsive to the query; and displaying, in the GUI displayed on the screen, the list of results responsive to the query. Here, displaying the list of results responsive to the query may further include generating a graphical element representing a highest ranked result in the list of results responsive to the query and displaying, in the GUI displayed on the screen, the list of results responsive to the query at the first location on the screen. Optionally, the operations may further include determining that the uniquely identified object includes text in a first language such that obtaining the information about the object requested by the query includes obtaining a translation of the text in a second language different than the first language.
[0013] The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
DESCRIPTION OF DRAWINGS
[0014] FIG. l is a schematic view of an example system including a contextual assistant using mouse pointing or touch cues.
[0015] FIG. 2 is a schematic view of example components of the contextual assistant.
[0016] FIGS. 3 A-3C are example graphical user interfaces (GUIs) rendered on a screen of a user device including the contextual assistant.
[0017] FIG. 4 is a flowchart of an example arrangement of operations for a method of disambiguating a query using mouse pointing or touch cues.
[0018] FIG. 5 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
[0019] Like reference symbols in the various drawings indicate like elements.
DETAILED DESCRIPTION
[0020] A user’s manner of interacting with an assistant-enabled device is designed to be primarily, if not exclusively, by means of voice input. While assistant-enabled devices are effective at obtaining answers to general topic queries (e g., what’s the capital of Michigan?), context-driven queries require the assistant-enable device to obtain additional information to obtain an accurate answer. For instance, the assistant-enabled device may struggle to obtain a confident/accurate answer to the query “show me more of these,” without more context.
[0021] In scenarios where the spoken query requires additional context to answer the query, the assistant-enabled device benefits from including image data derived from a screen of the assistant-enabled device. For instance, a user might query the assistant- enabled device in a natural manner by speaking “Show me more windows like that.” Here, the spoken query identifies that the user is looking for windows similar to an object but is ambiguous because the object is unknown from the linguistic content of the query. Using image data from the screen of the assistant-enabled device may allow the assistant- enabled device to narrow the potential windows to search for from an entire screen showing a city down to a distinct sub-region including a specific building in the city where a user input applied at a particular location on the screen has been detected in
conjunction with the spoken query. By including input data and image data in conjunction with the query, the assistant-enable device is able to generate a response to a query about the building in the city despite the user needing to explicitly identify the building in the spoken query.
[0022] FIG. 1 is an example of a system 100 including a user device 10 and/or a remote system 60 in communication with the user device 10 via a network 40. The user device 10 and/or the remote system 60 executes a point assistant 200 that a user 102 may interact with through speech and spatial inputs such that the point assistant 200 is capable of generating responses to queries referring to objects displayed on a screen of the user device 10, despite the query failing to uniquely identify an object for which the query seeks information. In the example shown, the user device 10 corresponds to a smart phone, however the user device 10 can include other computing devices having, or in communication with, display screens, such as, without limitation, a tablet, smart display, desktop/laptop, smart watch, smart appliance, smart glasses/headset, or vehicle infotainment device. The user device 10 includes data processing hardware 12 and memory hardware 14 storing instructions that when executed on the data processing hardware 12 cause the data processing hardware 12 to perform operations. The remote system 60 (e.g., server, cloud computing environment) also includes data processing hardware 62 and memory hardware 64 storing instructions that when executed on the data processing hardware 62 cause the data processing hardware 62 to perform operations. As described in greater detail below, the point assistant 200 executing on the user device 10 and/or the remote system 60 includes a speech recognizer 210 and a response generator 250, and has access to one or more information sources 240 stored on the memory hardware 14, 64. In some examples, execution of the point assistant 200 is shared across the user device 10 and the remote system 60.
[0023] The user device 10 includes an array of one or more microphones 16 configured to capture acoustic sounds such as speech directed toward the user device 10. The user device 10 also executes, for display on a screen 18 in communication with the data processing hardware 12, a graphical user interface (GUI) 300 configured to capture user input indications via any one of touch, gesture, gaze, and/or an input device (e.g.,
mouse, trackpad, or stylist) for controlling functionality of the user device 10. The GUI 300 may be an interface associated with an application 50 executing on the user device 10 that presents a plurality of objects in the GUI 300. The user device 10 may further include, or be in communication with, an audio output device (e.g., a speaker) 19 that may output audio such as music and/or synthesized speech from the point assistant 200. The user device 10 may also include a physical button 17 disposed on the user device 10 and configured to receive a tactile selection by a user 102 for invoking the point assistant 200.
[0024] The user device 10 may include an audio subsystem 106 for extracting audio data 202 (FIG. 2) from a query 104. For instance, referring to FIG. 1, the audio subsystem 106 may receive streaming audio captured by the one or more microphones 16 of the user device 10 that corresponds to an utterance 106 of a query 104 spoken by the user 102 and extract the audio data (e.g., acoustic frames) 202. The audio data 202 may include acoustic features such as Mel -frequency cepstrum coefficients (MFCCs) or filter bank energies computed over windows of an audio signal. In the example shown, the query 104 spoken by the user 102 includes “Hey Google, what is this?”
[0025] The user device 10 may execute (i.e., on the data processing hardware 12) a hotword detector 20 configured to detect a presence of a hotword 105 in streaming audio without performing semantic analysis or speech recognition processing on the streaming audio. The hotword detector 20 may execute on the audio subsystem 106. The hotword detector 202 may receive the audio data 202 to determine whether the utterance 106 includes a particular hotword 105 (e.g., Hey Google) spoken by the user 102. That is, the hotword detector 20 may be trained to detect the presence of the hotword 105 (e.g., Hey Google) or one or more other variants of the hotword (e.g., Ok Google) in the audio data 202. Detecting the presence of the hotword 105 in the audio data 202 may correspond to a trigger event that invokes the point assistant 200 to activate the GUI 300 displayed on the screen 18 to enable the detection of spatial inputs 112, and activate the speech recognizer 210 to perform speech recognition on the audio data 202 corresponding to the utterance 106 of the hotword 105 and/or one or more other terms characterizing the query 104 that follows the hotword. In some examples, the hotword 105 is spoken in the
utterance 106 subsequent to the query 105 such the portion of the audio data 202 characterizing the query 104 is buffered and retrieved by the speech recognizer 210 retrieves a portion of the audio data 202 upon detection of the hotword 105 in the audio data 202. In some implementations, the trigger event includes receiving, in the GUI 300, a user input indication indicating selection of a graphical element 21 (e.g., a graphical microphone). In other implementations, the trigger event includes receiving a user input indication indicating selection of the physical button 17 disposed on the user device 10. In other implementations, the trigger event includes detecting (e.g., via image and/or radar sensors) a predefined gesture performed by the user 102, or detecting a predefined movement/pose of the user device 10 (e.g., using one or more sensors such as an accelerometer and/or gyroscope).
[0026] The user device 10 may further include an image subsystem 108 configured to extract a location 114 (e.g., an X-Y coordinate location) on the screen 18 of a spatial input 112 applied in the GUI 300. For example, the user 102 may provide a user input indication 110 indicating the spatial input 112 in the GUI 300 at the location 114 on the screen. The image subsystem 108 may additionally extract image data (e.g., pixels) 204 corresponding to one or more objects 116 currently displayed on the screen 18. In the example shown, the GUI 300 receives the user input indication 110 indicating the spatial input 112 applied at a first location 114 on the screen 18, wherein the image data 202 includes an object (i.e., a golden retriever) 116 displayed on the screen 18 proximate to the first location 114.
[0027] With continued reference to the system 100 of FIG. 1 and the point assistant 200 of FIG. 2, the speech recognizer 210 executes an automatic speech recognition (ASR) model (e.g., a speech recognition model) 212 that receives, as input, the audio data 202 and generates/predicts, as output, a corresponding transcription 214 of the query 104. In the example shown, the query 104 includes the phrase, “what is this?”, that requests information 246 about an object 116 displayed in the GUI 300 on the screen without uniquely identifying the object 116. Described in greater detail below, the point assistant 200 uses the spatial input 112 applied at the first location 114 on the screen 118 to disambiguate the query 104 for uniquely identify the object 116 that the query 104 is
referring to. Once the object 116 is uniquely identified, the point assistant 200 may obtain the information 246 about the object and generate a response 252 to the query 104 that includes obtained information 246 about the object 116. The response generator 250 may generate the response 252 to the query 104 as a textual representation. Here, the point assistant 200 instructs the user device 10 to display the response 252 in the GUI 300 for the user 102 to read. In the example shown, the point assistant 200 generates a textual representation of the response 252 “That is a golden retriever” for display in the GUI 300. As will be discussed in further detail below, the point assistant 200 may require the additional context extracted by the image subsystem 108 (i.e., that the user 102 applied a spatial input 112 at the first location 114 corresponding to the object 116) in order to uniquely identify the object 116 the query 104 is referring to in order to obtain the information 246 for inclusion in the response 252. In some examples, the response generator 250 employs a text-to-speech (TTS) system 260 to convert the textual representation of the response 252 into synthesized speech. In these examples, the point assistant 200 generates the synthesized speech for audible output from the speaker 19 of the user device 10 in addition to, or in lieu of, displaying the textual representation of the response 252 in the GUI 300.
[0028] Referring to FIG. 2, the point assistant 200 further includes a natural language understanding (NLU) module 220 configured to perform query interpretation on the corresponding transcription 214 to ultimately determine a meaning behind the transcription 214. The NLU module 220 may also receive context information 201 to assist with interpreting the transcription 214. The context information 201 may indicate an application 50 (FIG. 1) currently executing on the user device 10, previous queries 104 from the user 102, a particular hotword 105 was detected, or any other information that the NLU module 220 can leverage for interpreting the query 104. Continuing with the example, the context information 201 may indicate that the user is interacting with a webbased application 50 executing on the user device 102 and the NLU module 230 performs query interpretation to determine that the query 104 specifies an action 232 to obtain a description/information about some object 116 displayed in the GUI 300 that the user 102 is likely viewing. However, the NLU module 230 determines that the query 104 is
ambiguous since the object 116 is not explicitly identified in the transcription 214 but for the term “this”. In other words, query interpretation performed by the NLU module 230 determines that the query 104 refers an object 116 displayed on the screen 18 without uniquely identifying the object 116 and specifies an action 232 to request information 246 about the object 116.
[0029] In order to fulfill the query 104, the NLU module 220 needs to disambiguate the query 104 to uniquely identify the object 116 the query 104 is referring to. For example, in a scenario where a query 104 includes a corresponding transcription 214 “show me similar bicycles” while multiple bicycles are currently displayed on the screen 18,” the NLU module 220 may perform query interpretation on the corresponding transcription 214 to identify that the user 102 is referring to an object (i.e., a bicycle) 116 displayed in the GUI 300 without uniquely identifying the object 116, and requesting information 246 about the object 116 (i.e., other objects similar to the bicycle 116). In this example, the NLU module 220 determines that query 104 specifies an action 232 to retrieve images of bicycles similar to one of the bicycles displayed on the screen, but cannot fulfil the query 104 because the bicycle that the query is referring to cannot be ascertained from the transcription 214.
[0030] The NLU module 220 may use a user input indication indicating a spatial input 112 applied at the first location 114 on the screen as additional context for disambiguating the query 104 to uniquely identify the object 116 the query is referring to. The NLU module 220 may additionally use image data 204 for disambiguating the query 104. Here, the image data 204 may include a plurality of candidate objects displayed in the GUI and corresponding locations of the plurality of candidate objects displayed in the GUI. The image data 204 may be extracted by the image subsystem 108 from graphical content rendered for display in the GUI 300. The image data 204 may include labels that identify the candidate objects. In some examples, the image subsystem 108 performs one or more object recognition techniques on the graphical content in order to identify the candidate objects. By using the image data 204 and the received user input indication the spatial input 118 applied at the first location 114, the NLU module 220 may be able uniquely identify the object as an object rendered for display in the GUI 300 that is
closest to the first location 114 of the spatial input 118. In some examples, the content of the transcription 214 can further narrow down the possibility of objects the query refers to by at least describing a type of object or indicating one or more features/characteristics of the object the query refers to. Once the object 116 is uniquely identified, the point assistant 200 adds the object 116 to perform the action 232 of obtaining the information 246 about the object 116 requested by the query 104. Once the point assistant 200 obtains the information 246 about the object 116 requested by the query 104, the response generator 250 provides a response 252 to the query 104 that includes the obtained information 246 about the object 116.
[0031] Referring to FIG. 3A, in some implementations, receiving the user input indication 110 indicating the spatial input 112 at the first location 114 includes detecting that a position of a cursor 310 is displayed in a GUI 300a at the first location 114 when the user 102 spoke the query 104. In these implementations, the NLU module 220 further receives image data 204 including a plurality of candidate objects 320, 320a-c displayed in the GUI 300. Each candidate object 320 of the plurality of candidate objects 320 includes a corresponding location 322, 322a-c in the GUI 300a displayed on the screen. These locations may, for example, be quantified or otherwise characterized using one or more coordinate systems such as Cartesian coordinates using a pixel coordinate system where the origin is defined by the bottom left of the GUI 300a, or a polar coordinate system.
[0032] In addition, each of the candidate objects 320 may be spatially defined by a bounding box 330a, 330a-c or a box with the smallest measure within which all of the candidate object 320 lies. The NLU module 220 may identify a candidate object 320c from the plurality of candidate objects 320 as having the corresponding location 322c that is closest to the first location 114 as the object 116 the query 104 is referring to. In some examples, where the bounding box 330 of two or more candidate objects 320 overlap, the NLU module 220 may employ a best intersection technique to compute the overlap between the two or more bounding boxes 330 in order to identify the object 116 the query 104 is referring to. In the example shown, the position of the cursor 310 indicates the
spatial input 112 is applied at the location 114 where an object 116 that includes the sun is displayed.
[00331 In other implementations (not shown), the user input indication 110 indicating the spatial input 112 at the first location 114 includes detecting a touch input received in GUI 300 at the first location 114 when the user 102 spoke the query 104. Alternatively, the user input indication 110 indicating the spatial input 112 at the first location 114 includes detecting a lassoing action performed in the GUI 300 at the first location 114 when the user 102 spoke the query 104.
[0034] Referring to FIG. 3B, in some implementations, receiving the user input indication 110 indicating the spatial input 112 at the first location 114 includes detecting a lassoing action performed in a GUI 300b at a first location 114. In response to detecting the lassoing action, the NLU module 220 uses the first location 114 in the image data 204, to crop a subset of the image data 204 contained within a region identified by the lassoing action and located at the first location 114 to uniquely identify the object 116 the query 104 is referring to. In the example shown, the object within the region of the lassoing action includes a building.
[0035] Referring to FIG. 3C, in some implementations, receiving the user input indication 110 indicating the spatial input 112 at the first location includes detecting an underlining action performed in a GUI 300c at a first location 114. In these implementations, the query 104 may be directed to a sequence of characters (e.g., “Bienvenue au cours de frangais!” jdisplayed in the GUI 300c at the first location 114. For instance, the query 104 may include the phrase “What does this say?” Like in FIG. 3 A, the NLU module 220 may identify a candidate object 320 (e.g., the underlined sequence of characters) as having a corresponding location 322 that is closest to the first location 114 as the object 116 the query 104 is referring to. In other implementations (not shown), the user input indication 110 indicating the spatial input 112 at the first location 114 includes detecting a highlighting action performed in the GUI 300c that highlights the sequence of characters (e.g., “Bienvenue au cours de frangais!”) at the first location 114. In these implementations, the disambiguation model 230 disambiguates the
query 104 to uniquely identify the object 116 the query is referring to as the sequence of characters highlighted by the highlighting action.
[00361 Referring back to FIG. 2, once the NLU 220 disambiguates the query 104 to uniquely identify the object 116, the NLU module 220 inserts the object 116 into a missing object slot of the action 232 and performs the action 232 of obtaining the information 246 about the uniquely identified object 116 requested by the query 104. In some implementations, the point assistant 200 performs the identified action 232 to obtain the information 246 about the object 116 requested by the query 104 by querying an information source 240. In these implementations, the information source 240 may include a search engine 242, where the point assistant 200 queries the search engine 242 using the uniquely identified object 116 and one or more terms in the transcription 214 of the query 104 to obtain the information 246 about the object 116 requested by the query 104. For example, the point assistant 200 queries the search engine to obtain information 238 that includes a description of a golden retriever uniquely identified as the object 116 requested by the query, in addition to the one or more words in the transcription 214 “what is this?” The information source may include an object recognition engine 244 that applies image processing techniques to detect and recognize patterns (i.e., a golden retriever) in the image data 204 in order obtain the information 238 that classifies the object 116 as a golden retriever and provides information about golden retrievers. The information could include a link to a content source (e.g., webpage). That is, the information source 240 may use the image data 204 along with the transcription 214 of the query 104 to obtain the information 246 requested by the query 104. The response generator 250 receives the information 246 requested by the query 104 and generates the response 252 “That is a golden retriever.” As discussed above, the response generator 250 may generate the response 252 to the query 104 as a textual representation 19 displayed in the GUI 300 on the screen of the user device 10.
[0037] In other examples, the point assistant 200 queries the search engine 242 to obtain a list of results responsive to the query 104. In these examples, the query 104 may be a similarity query 104, where the user 102 seeks a list of results with a visual similarity to the object 116 in the GUI 300 on the screen of the user device 10. Once the
information source 240 returns the information 246 including the list of results, the response generator 250 may generate the response 252 to the query 104 as a textual representation 19 including the list of results displayed in the GUI 300 on the screen of the user device 10. When the point assistant 200 displays the response 252, it may further generate a graphical element representing a highest ranked result in the list of results responsive to the query 104, where the highest ranked result is displayed more prominently (e.g., larger font, highlighted color, at the first location 114) than the remaining results in the list of ranked results.
[0038] In some implementations, the point assistant 200 determines that the uniquely identified object 116 includes text in a first language (e.g., French). Here, the user 102 that spoke the query 104 may speak only speak a second language (e.g., English) different than the first language. For example, as shown in FIG. 3C, the uniquely identified object 116 includes text in a first language “Bienvenue au cours de franqais!” When the point assistant queries the information source 240 for information 246 about the object, the information source 240 may obtain a translation of the uniquely identified object 116 in the second language “Welcome to French class!” For instance, the information source 240 may include a text-to-text machine translation model.
[0039] FIG. 4 is a flowchart of an exemplary arrangement of operations for a method 400 for a contextual assistant to use mouse pointing or touch cues. The method 400 includes, at operation 402, receiving audio data 202 corresponding to a query 104 spoken by a user 102 and captured by an assistant-enabled device (e.g., a user device) 10 associated with the user 102. The method 400 further includes, at operation 404, receiving, in a graphical user interface 300 displayed on a screen in communication with data processing hardware 12, a user input indication 110 indicating a spatial input 112 applied at a first location 114 on the screen. At operation 406, the method 400 includes processing, using a speech recognition model 212, the audio data 202 to determine a transcription 214 of the query 104.
[0040] At operation 408, the method 400 also includes performing query interpretation on the transcription 214 of the query 104 to determine that the query 104 is referring to an object 116 displayed on the screen without uniquely identifying the object
116, and requesting information 256 about the object 116 displayed on the screen. The method 400 further includes, at operation 410, disambiguating, using the user input indication 110 indicating the spatial input 112 applied at the first location 114 on the screen, the query 104 to uniquely identify the object 116 that the query 104 is referring to. At operation 412, in response to uniquely identifying the object 116, the method 400 includes obtaining the information 246 about the object 116 requested by the query 104. The method 400 further includes, at operation 414, providing a response 252 to the query 104 that includes the obtained information 246 about the object 116.
[0041] FIG. 5 is schematic view of an example computing device 500 that may be used to implement the systems and methods described in this document. The computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
[0042] The computing device 500 includes a processor 510, memory 520, a storage device 530, a high-speed interface/controller 540 connecting to the memory 520 and high-speed expansion ports 550, and a low speed interface/controller 560 connecting to a low speed bus 570 and a storage device 530. Each of the components 510, 520, 530, 540, 550, and 560, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 510 (e.g., data processing hardware 12 or data processing hardware 62 of FIG. 1) can process instructions for execution within the computing device 500, including instructions stored in the memory 520 or on the storage device 530 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 580 coupled to high speed interface 540. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices 500 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
[0043] The memory 520 (e g., memory hardware 14 or memory hardware 64 of FIG.
1) stores information non-transitorily within the computing device 500. The memory 520 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). The non-transitory memory 520 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 500. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) I electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
[0044] The storage device 530 is capable of providing mass storage for the computing device 500. In some implementations, the storage device 530 is a computer- readable medium. In various different implementations, the storage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory 520, the storage device 530, or memory on processor 510.
[0045] The high speed controller 540 manages bandwidth-intensive operations for the computing device 500, while the low speed controller 560 manages lower bandwidthintensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 540 is coupled to the memory 520, the display 580 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 550, which may accept various expansion cards (not shown). In some
implementations, the low-speed controller 560 is coupled to the storage device 530 and a low-speed expansion port 590. The low-speed expansion port 590, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
[0046] The computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 500a or multiple times in a group of such servers 500a, as a laptop computer 500b, or as part of a rack server system 500c.
[0047] Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
[0048] A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform a task. In some examples, a software application may be referred to as an “application,” an “app,” or a “program.” Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
[0049] These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable
medium” and “computer-readable medium” refer to any computer program product, non- transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
[0050] The non-transitory memory may be physical devices used to store programs (e g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device. The non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of nonvolatile memory include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
[0051] The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data
from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
[0052] To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
[0053] A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims
1. A computer-implemented method when executed by data processing hardware (510) causes the data processing hardware (510) to perform operations comprising: receiving audio data (202) corresponding to a query (104) spoken by a user and captured by an assistant-enabled device associated with the user; receiving, in a graphical user interface (GUI (300)) displayed on a screen in communication with the data processing hardware (510), a user input indication (110) indicating a spatial inputs (112) applied at a first location (114) on the screen; processing, using a speech recognition model (212), the audio data (202) to determine a transcription (214) of the query (104); performing query interpretation on the transcription (214) of the query (104) to determine that the query (104) is: referring to an object (116) displayed on the screen without uniquely identifying the object (116); and requesting information about the object (116) displayed on the screen; disambiguating, using the user input indication (110) indicating the spatial inputs (112) applied at the first location (114) on the screen, the query (104) to uniquely identify the object (116) that the query (104) is referring to; in response to uniquely identifying the object (116), obtaining the information about the object (116) requested by the query (104); and providing a response (252) to the query (104) that includes the obtained information about the object (116).
2. The method (400) of claim 1, wherein the operations further comprise detecting a trigger event, and in response to detecting the trigger event, activating: the GUI (300) displayed on the screen to enable detection of spatial inputs (112); and the speech recognition model (212) to enable the performance of speech recognition on incoming audio data (202) captured by the assistant-enabled device.
3. The method (400) of claim 2, wherein detecting the trigger event comprises detecting, by a hotword detector, a presence of a hotword (105) in the received audio data (202).
4. The method (400) of claim 2, wherein detecting the trigger event comprises one of: receiving, in the GUI (300) displayed on the screen, a user input indication (110) indicating selection of a graphical element (21); receiving a user input indication (110) indicating selection of a physical button (17) disposed on the assistant-enabled device; detecting a predefined gesture performed by the user; or detecting a predefined movement/pose of the assistant-enabled device.
5. The method (400) of any of claims 1-4, wherein: receiving the user input indication (110) indicating the spatial inputs (112) applied at the first location (114) comprises one of: detecting that a position of a cursor (310) is displayed in the GUI (300) at the first location (114) when the user spoke the query (104); detecting a touch input received in the GUI (300) at the first location (114) when the user spoke the query (104); or detecting a lassoing action (232) performed in the GUI (300) at the first location (114) when the user spoke the query (104); and disambiguating the query (104) to uniquely identify the object (116) comprises: receiving image data (204) comprising a plurality of candidate objects (116) displayed in the GUI (300) and corresponding locations of the plurality of candidate objects (116) displayed in the GUI (300); and identifying the candidate object (116) from the plurality of candidate objects (116) having the corresponding location (114) that is closest to the first location (114) as the object (116) the query (104) is referring to.
6. The method (400) of any of claims 1-5, wherein: receiving the user input indication (110) indicating the spatial inputs (112) applied at the first location (114) comprises detecting an underlining action (232) performed in the GUI (300) that underlines a sequence of characters displayed in the GUI (300) at the first location (114); and disambiguating the query (104) to uniquely identify the object (116) comprises uniquely identifying the sequence of characters underlined by the underlining action (232) as the object (116) the query (104) is referring to.
7. The method (400) of any of claims 1-6, wherein: receiving the user input indication (110) indicating the spatial inputs (112) applied at the first location (114) comprises detecting a highlighting action (232) performed in the GUI (300) that highlights a sequence of characters displayed in the GUI (300) at the first location (114); and disambiguating the query (104) to uniquely identify the object (116) comprises uniquely identifying the sequence of characters highlighted by the highlighting action (232) as the object (116) the query (104) is referring to.
8. The method (400) of any of claims 1-7, wherein obtaining the information about the object (116) requested by the query (104) comprises: querying a search engine (242) using the uniquely identified object (116) and one or more terms in the transcription (214) of the query (104) to obtain a list of results responsive to the query (104); and providing the response (252) to the query (104) that includes the obtained information comprises displaying, in the GUI (300) displayed on the screen, the list of results responsive to the query (104).
9. The method (400) of claim 8, wherein displaying the list of results responsive to the query (104) further comprises:
generating a graphical element (21) representing a highest ranked result in the list of results responsive to the query (104); and displaying, in the GUI (300) displayed on the screen, the list of results responsive to the query (104) at the first location (114) on the screen.
10. The method (400) of any of claims 1-9, wherein the operations further comprise determining that the uniquely identified object (116) comprises text in a first language, wherein obtaining the information about the object (116) requested by the query (104) comprises obtaining a translation of the text in a second language different than the first language.
11. A system (100) comprising: data processing hardware (510); and memory hardware (520) in communication with the data processing hardware (510), the memory hardware (520) storing instructions that when executed on the data processing hardware (510) cause the data processing hardware (510) to perform operations comprising: receiving audio data (202) corresponding to a query (104) spoken by a user and captured by an assistant-enabled device associated with the user; receiving, in a graphical user interface (GUI (300)) displayed on a screen in communication with the data processing hardware (510), a user input indication (110) indicating a spatial inputs (112) applied at a first location (114) on the screen; processing, using a speech recognition model (212), the audio data (202) to determine a transcription (214) of the query (104); performing query interpretation on the transcription (214) of the query (104) to determine that the query (104) is: referring to an object (116) displayed on the screen without uniquely identifying the object (116); and requesting information about the object (116) displayed on the screen;
disambiguating, using the user input indication (110) indicating the spatial inputs (112) applied at the first location (114) on the screen, the query (104) to uniquely identify the object (116) that the query (104) is referring to; in response to uniquely identifying the object (116), obtaining the information about the object (116) requested by the query (104); and providing a response (252) to the query (104) that includes the obtained information about the object (116).
12. The system (100) of claim 11, wherein the operations further comprise detecting a trigger event, and in response to detecting the trigger event, activating: the GUI (300) displayed on the screen to enable detection of spatial inputs (112); and the speech recognition model (212) to enable the performance of speech recognition on incoming audio data (202) captured by the assistant-enabled device.
13. The system (100) of claim 12, wherein detecting the trigger event comprises detecting, by a hotword detector, a presence of a hotword (105) in the received audio data (202).
14. The system (100) of claim 12, wherein detecting the trigger event comprises one of: receiving, in the GUI (300) displayed on the screen, a user input indication (110) indicating selection of a graphical element (21); receiving a user input indication (110) indicating selection of a physical button (17) disposed on the assistant-enabled device; detecting a predefined gesture performed by the user; or detecting a predefined movement/pose of the assistant-enabled device.
15. The system (100) of any of claims 11-14, wherein:
receiving the user input indication (110) indicating the spatial inputs (112) applied at the first location (114) comprises one of: detecting that a position of a cursor (310) is displayed in the GUI (300) at the first location (114) when the user spoke the query (104); detecting a touch input received in the GUI (300) at the first location (114) when the user spoke the query (104); or detecting a lassoing action (232) performed in the GUI (300) at the first location (114) when the user spoke the query (104), and disambiguating the query (104) to uniquely identify the object (116) comprises: receiving image data (204) comprising a plurality of candidate objects (116) displayed in the GUI (300) and corresponding locations of the plurality of candidate objects (116) displayed in the GUI (300); and identifying the candidate object (116) from the plurality of candidate objects (116) having the corresponding location (114) that is closest to the first location (114) as the object (116) the query (104) is referring to.
16. The system (100) of any of claims 11-15, wherein: receiving the user input indication (110) indicating the spatial inputs (112) applied at the first location (114) comprises detecting an underlining action (232) performed in the GUI (300) that underlines a sequence of characters displayed in the GUI (300) at the first location (114); and disambiguating the query (104) to uniquely identify the object (116) comprises uniquely identifying the sequence of characters underlined by the underlining action (232) as the object (116) the query (104) is referring to.
17. The system (100) of any of claims 11-16, wherein: receiving the user input indication (110) indicating the spatial inputs (112) applied at the first location (114) comprises detecting a highlighting action (232) performed in the GUI (300) that highlights a sequence of characters displayed in the GUI (300) at the first location (114); and
disambiguating the query (104) to uniquely identify the object (116) comprises uniquely identifying the sequence of characters highlighted by the highlighting action (232) as the object (116) the query (104) is referring to.
18. The system (100) of any of claims 11-17, wherein obtaining the information about the object (116) requested by the query (104) comprises: querying a search engine (242) using the uniquely identified object (116) and one or more terms in the transcription (214) of the query (104) to obtain a list of results responsive to the query (104); and providing the response (252) to the query (104) that includes the obtained information comprises displaying, in the GUI (300) displayed on the screen, the list of results responsive to the query (104).
19. The system (100) of claim 18, wherein displaying the list of results responsive to the query (104) further comprises: generating a graphical element (21) representing a highest ranked result in the list of results responsive to the query (104); and displaying, in the GUI (300) displayed on the screen, the list of results responsive to the query (104) at the first location (114) on the screen.
20. The system (100) of any of claims 11-19, wherein the operations further comprise determining that the uniquely identified object (116) comprises text in a first language, wherein obtaining the information about the object (116) requested by the query (104) comprises obtaining a translation of the text in a second language different than the first language.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/717,292 US11709653B1 (en) | 2022-04-11 | 2022-04-11 | Contextual assistant using mouse pointing or touch cues |
US17/717,292 | 2022-04-11 |
Publications (1)
Publication Number | Publication Date |
---|---|
WO2023200718A1 true WO2023200718A1 (en) | 2023-10-19 |
Family
ID=86286010
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
PCT/US2023/018044 WO2023200718A1 (en) | 2022-04-11 | 2023-04-10 | Contextual assistant using mouse pointing or touch cues |
Country Status (2)
Country | Link |
---|---|
US (2) | US11709653B1 (en) |
WO (1) | WO2023200718A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11308962B2 (en) * | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20140136310A (en) * | 2013-05-20 | 2014-11-28 | 엘지전자 주식회사 | Image display device and control method thereof |
US20140379341A1 (en) * | 2013-06-20 | 2014-12-25 | Samsung Electronics Co., Ltd. | Mobile terminal and method for detecting a gesture to control functions |
US20150042570A1 (en) * | 2012-10-30 | 2015-02-12 | Motorola Mobility Llc | Method and apparatus for keyword graphic selection |
US20180336009A1 (en) * | 2017-05-22 | 2018-11-22 | Samsung Electronics Co., Ltd. | System and method for context-based interaction for electronic devices |
Family Cites Families (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7756887B1 (en) | 2004-12-30 | 2010-07-13 | Google Inc. | System and method for modulating search relevancy using pointer activity monitoring |
US7657126B2 (en) | 2005-05-09 | 2010-02-02 | Like.Com | System and method for search portions of objects in images and features thereof |
US9098533B2 (en) | 2011-10-03 | 2015-08-04 | Microsoft Technology Licensing, Llc | Voice directed context sensitive visual search |
US9841879B1 (en) * | 2013-12-20 | 2017-12-12 | Amazon Technologies, Inc. | Adjusting graphical characteristics for indicating time progression |
US10200824B2 (en) * | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US10586535B2 (en) * | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11221744B2 (en) * | 2017-05-16 | 2022-01-11 | Apple Inc. | User interfaces for peer-to-peer transfers |
US10607082B2 (en) | 2017-09-09 | 2020-03-31 | Google Llc | Systems, methods, and apparatus for image-responsive automated assistants |
US11030205B2 (en) * | 2017-12-13 | 2021-06-08 | Microsoft Technology Licensing, Llc | Contextual data transformation of image content |
WO2020076288A1 (en) * | 2018-10-08 | 2020-04-16 | Google Llc | Operating modes that designate an interface modality for interacting with an automated assistant |
-
2022
- 2022-04-11 US US17/717,292 patent/US11709653B1/en active Active
-
2023
- 2023-04-10 WO PCT/US2023/018044 patent/WO2023200718A1/en unknown
- 2023-06-08 US US18/331,643 patent/US20230325148A1/en active Pending
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150042570A1 (en) * | 2012-10-30 | 2015-02-12 | Motorola Mobility Llc | Method and apparatus for keyword graphic selection |
KR20140136310A (en) * | 2013-05-20 | 2014-11-28 | 엘지전자 주식회사 | Image display device and control method thereof |
US20140379341A1 (en) * | 2013-06-20 | 2014-12-25 | Samsung Electronics Co., Ltd. | Mobile terminal and method for detecting a gesture to control functions |
US20180336009A1 (en) * | 2017-05-22 | 2018-11-22 | Samsung Electronics Co., Ltd. | System and method for context-based interaction for electronic devices |
Also Published As
Publication number | Publication date |
---|---|
US20230325148A1 (en) | 2023-10-12 |
US11709653B1 (en) | 2023-07-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11929072B2 (en) | Using textual input and user state information to generate reply content to present in response to the textual input | |
US11817085B2 (en) | Automatically determining language for speech recognition of spoken utterance received via an automated assistant interface | |
US11227585B2 (en) | Intent re-ranker | |
JP7191987B2 (en) | Speaker diarization using speaker embeddings and trained generative models | |
US9805718B2 (en) | Clarifying natural language input using targeted questions | |
EP4254402A2 (en) | Automatically determining language for speech recognition of spoken utterance received via an automated assistant interface | |
US9583105B2 (en) | Modification of visual content to facilitate improved speech recognition | |
US10217458B2 (en) | Technologies for improved keyword spotting | |
US20230325148A1 (en) | Contextual Assistant Using Mouse Pointing or Touch Cues | |
JP2020003926A (en) | Interaction system control method, interaction system and program | |
US20240055002A1 (en) | Detecting near matches to a hotword or phrase | |
CN112465144A (en) | Multi-modal demonstration intention generation method and device based on limited knowledge | |
JP2023511091A (en) | Biasing Alphanumeric Strings for Automatic Speech Recognition | |
US9984688B2 (en) | Dynamically adjusting a voice recognition system | |
US20230103677A1 (en) | Automated assistant control of non-assistant applications via synonymous term indentification and/or speech processing biasing | |
US20210311701A1 (en) | Technique for generating a command for a voice-controlled electronic device | |
US20230298580A1 (en) | Emotionally Intelligent Responses to Information Seeking Questions | |
US20230306965A1 (en) | Speech Recognition Using Word or Phoneme Time Markers Based on User Input | |
US11830497B2 (en) | Multi-domain intent handling with cross-domain contextual signals | |
US20240013782A1 (en) | History-Based ASR Mistake Corrections | |
WO2022271555A1 (en) | Early invocation for contextual data processing |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
121 | Ep: the epo has been informed by wipo that ep was designated in this application |
Ref document number: 23721167Country of ref document: EPKind code of ref document: A1 |