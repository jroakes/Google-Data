JP7204989B2 - Expressivity Control in End-to-End Speech Synthesis Systems - Google Patents
Expressivity Control in End-to-End Speech Synthesis Systems Download PDFInfo
- Publication number
- JP7204989B2 JP7204989B2 JP2022506820A JP2022506820A JP7204989B2 JP 7204989 B2 JP7204989 B2 JP 7204989B2 JP 2022506820 A JP2022506820 A JP 2022506820A JP 2022506820 A JP2022506820 A JP 2022506820A JP 7204989 B2 JP7204989 B2 JP 7204989B2
- Authority
- JP
- Japan
- Prior art keywords
- text
- current input
- style
- input text
- embeddings
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000015572 biosynthetic process Effects 0.000 title description 9
- 238000003786 synthesis reaction Methods 0.000 title description 9
- 238000013528 artificial neural network Methods 0.000 claims description 160
- 238000000034 method Methods 0.000 claims description 104
- 230000005236 sound signal Effects 0.000 claims description 68
- 238000012549 training Methods 0.000 claims description 61
- 230000000306 recurrent effect Effects 0.000 claims description 57
- 238000012545 processing Methods 0.000 claims description 48
- 230000008569 process Effects 0.000 claims description 41
- 230000004044 response Effects 0.000 claims description 33
- 239000013598 vector Substances 0.000 claims description 22
- 230000004913 activation Effects 0.000 claims description 21
- 238000001994 activation Methods 0.000 claims description 21
- 230000002457 bidirectional effect Effects 0.000 claims description 15
- 230000002194 synthesizing effect Effects 0.000 claims description 14
- 238000004891 communication Methods 0.000 claims description 6
- 230000015654 memory Effects 0.000 description 44
- 230000000875 corresponding effect Effects 0.000 description 25
- 239000004115 Sodium Silicate Substances 0.000 description 20
- 238000010586 diagram Methods 0.000 description 18
- 238000006243 chemical reaction Methods 0.000 description 11
- 239000001177 diphosphate Substances 0.000 description 11
- 238000004590 computer program Methods 0.000 description 9
- 238000012805 post-processing Methods 0.000 description 8
- 238000012546 transfer Methods 0.000 description 8
- 230000006870 function Effects 0.000 description 7
- 238000010606 normalization Methods 0.000 description 7
- 238000011176 pooling Methods 0.000 description 5
- 230000001364 causal effect Effects 0.000 description 4
- 230000002996 emotional effect Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000009877 rendering Methods 0.000 description 4
- 230000002123 temporal effect Effects 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 230000008451 emotion Effects 0.000 description 3
- 230000007613 environmental effect Effects 0.000 description 3
- 230000007774 longterm Effects 0.000 description 3
- 230000001537 neural effect Effects 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 230000037007 arousal Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000007704 transition Effects 0.000 description 2
- 239000008272 agar Substances 0.000 description 1
- 230000002776 aggregation Effects 0.000 description 1
- 238000004220 aggregation Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000007596 consolidation process Methods 0.000 description 1
- 230000001276 controlling effect Effects 0.000 description 1
- 230000002596 correlated effect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000014509 gene expression Effects 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000033764 rhythmic process Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 238000011524 similarity measure Methods 0.000 description 1
- CDBYLPFSWZWCQE-UHFFFAOYSA-L sodium carbonate Substances [Na+].[Na+].[O-]C([O-])=O CDBYLPFSWZWCQE-UHFFFAOYSA-L 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000001228 spectrum Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/033—Voice editing, e.g. manipulating the voice of the synthesiser
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/08—Text analysis or generation of parameters for speech synthesis out of text, e.g. grapheme to phoneme translation, prosody generation or stress or intonation determination
- G10L13/10—Prosody rules derived from text; Stress or intonation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
- G06N3/0455—Auto-encoder networks; Encoder-decoder networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/04—Details of speech synthesis systems, e.g. synthesiser structure or memory management
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/27—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique
- G10L25/30—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique using neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/04—Details of speech synthesis systems, e.g. synthesiser structure or memory management
- G10L13/047—Architecture of speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/08—Text analysis or generation of parameters for speech synthesis out of text, e.g. grapheme to phoneme translation, prosody generation or stress or intonation determination
Description
本開示は、表現力豊かなエンドツーエンド音声合成システムにおけるコンテキスト的特徴を使用することに関する。 The present disclosure relates to using contextual features in expressive end-to-end speech synthesis systems.
ニューラルネットワークは、受信した入力に対する出力を予測するために非線形ユニットの1つまたは複数の層を用いる機械学習モデルである。たとえば、ニューラルネットワークは、入力テキストを出力音声に変換し得る。いくつかのニューラルネットワークは、出力層に加えて1つまたは複数の隠れ層を含む。各隠れ層の出力は、ネットワーク内の次の層、すなわち、次の隠れ層または出力層への入力として使用される。ネットワークの各層は、パラメータのそれぞれのセットの現在の値に従って、受信した入力から出力を生成する。 A neural network is a machine learning model that uses one or more layers of nonlinear units to predict the output given the input received. For example, a neural network may transform input text into output speech. Some neural networks contain one or more hidden layers in addition to the output layer. The output of each hidden layer is used as input to the next layer in the network, ie the next hidden or output layer. Each layer of the network produces an output from the input it receives according to the current values of its respective set of parameters.
いくつかのニューラルネットワークは、リカレントニューラルネットワークである。リカレントニューラルネットワークは、入力シーケンスを受信し、入力シーケンスから出力シーケンスを生成するニューラルネットワークである。特に、リカレントニューラルネットワークは、現在の時間ステップにおける出力を計算する際に、前の時間ステップからのネットワークの内部状態の一部またはすべてを使用することができる。リカレントニューラルネットワークの例は、1つまたは複数の長短期(LSTM)メモリブロックを含むLSTMニューラルネットワークである。各LSTMメモリブロックは、たとえば、現在のアクティベーションを生成する際に使用するために、またはLSTMニューラルネットワークの他の構成要素に提供されるように、セルがセルに関する以前の状態を記憶することを可能にする、入力ゲートと、忘却ゲートと、出力ゲートとを各々が含む1つまたは複数のセルを含むことができる。 Some neural networks are recurrent neural networks. A recurrent neural network is a neural network that receives an input sequence and produces an output sequence from the input sequence. In particular, recurrent neural networks can use some or all of the network's internal state from previous time steps in computing the output at the current time step. An example of a recurrent neural network is an LSTM neural network that includes one or more long short term (LSTM) memory blocks. Each LSTM memory block allows cells to store previous states for the cell, for example, for use in generating current activations or to be provided to other components of the LSTM neural network. It may contain one or more cells each containing an input gate, a forget gate, and an output gate that enable.
本開示の一態様は、現在の入力テキストの表現力豊かなスピーチの出力オーディオ信号を生成するためのシステムを提供する。システムは、コンテキストエンコーダと、コンテキストエンコーダと通信するテキスト予測ネットワークと、テキスト予測ネットワークと通信するテキスト読み上げ(TTS)モデルとを含む。コンテキストエンコーダは、表現力豊かなスピーチに合成されるべき現在の入力テキストに関連する1つまたは複数のコンテキスト特徴を受信し、現在の入力テキストに関連するコンテキスト埋め込みを生成するために1つまたは複数のコンテキスト特徴を処理するように構成される。各コンテキスト特徴は、現在の入力テキストのテキストソースから導出される。テキスト予測ネットワークは、テキストソースから現在の入力テキストを受信し、コンテキストエンコーダから現在の入力テキストに関連するコンテキスト埋め込みを受信し、現在の入力テキストのためのスタイル埋め込みを出力として予測するために、現在の入力テキストと現在の入力テキストに関連するコンテキスト埋め込みとを処理するように構成される。テキストソースは、表現力豊かなスピーチに合成されるべきテキストのシーケンスを含み、スタイル埋め込みは、現在の入力テキストを表現力豊かなスピーチに合成するための特定の韻律および/またはスタイルを指定する。TTSモデルは、テキストソースから現在の入力テキストを受信し、テキスト予測ネットワークによって予測されたスタイル埋め込みを受信し、現在の入力テキストの表現力豊かなスピーチの出力オーディオ信号を生成するために、現在の入力テキストとスタイル埋め込みとを処理するように構成される。出力オーディオ信号は、スタイル埋め込みによって指定された特定の韻律および/またはスタイルを有する。 One aspect of the present disclosure provides a system for generating an output audio signal of expressive speech of current input text. The system includes a context encoder, a text prediction network in communication with the context encoder, and a text-to-speech (TTS) model in communication with the text prediction network. A context encoder receives one or more context features associated with the current input text to be synthesized into expressive speech, and generates one or more context embeddings associated with the current input text. is configured to process the context features of Each context feature is derived from the text source of the current input text. A text prediction network receives the current input text from the text source, receives the context embeddings associated with the current input text from the context encoder, and predicts the style embeddings for the current input text as output. input text and context embeddings associated with the current input text. A text source contains a sequence of text to be synthesized into expressive speech, and a style embedding specifies a particular prosody and/or style for synthesizing the current input text into expressive speech. The TTS model receives the current input text from the text source, receives the style embeddings predicted by the text prediction network, and generates an output audio signal of expressive speech for the current input text, using the current input text. It is configured to process input text and style embeddings. The output audio signal has a particular prosody and/or style specified by style embeddings.
本開示の実装形態は、以下のオプションの特徴のうちの1つまたは複数を含み得る。いくつかの実装形態において、現在の入力テキストに関連する1つまたは複数のコンテキスト特徴は、現在の入力テキスト、現在の入力テキストに先行するテキストソースからの前のテキスト、前のテキストから合成された前のスピーチ、現在の入力テキストに続くテキストソースからの次のテキスト、または前のテキストと前のテキストに関連する前のコンテキスト埋め込みとに基づいてテキスト予測ネットワークによって予測された前のスタイル埋め込みのうちの少なくとも1つを含む。いくつかの例において、テキストソースは、テキスト文書を含み、現在の入力テキストに関連する1つまたは複数のコンテキスト特徴は、テキスト文書のタイトル、テキスト文書内の章のタイトル、テキスト文書内の節のタイトル、テキスト文書内の見出し、テキスト文書内の1つもしくは複数の箇条書き、テキスト文書から抽出された概念グラフからのエンティティ、テキスト文書から抽出された1つもしくは複数の構造化された回答表現のうちの少なくとも1つを含む。 Implementations of the disclosure may include one or more of the following optional features. In some implementations, the one or more contextual features associated with the current input text are the current input text, the previous text from the text source that precedes the current input text, and the previous text synthesized from the previous text. of the previous speech, the next text from the text source following the current input text, or the previous style embedding predicted by the text prediction network based on the previous text and previous contextual embeddings related to the previous text including at least one of In some examples, the text source includes a text document, and the one or more contextual features associated with the current input text are the title of the text document, the title of the chapter within the text document, the title of the section within the text document. A title, a heading within a text document, one or more bullet points within a text document, an entity from a concept graph extracted from the text document, one or more of the structured answer representations extracted from the text document. contains at least one of
他の例において、テキストソースは、対話トランスクリプトを含み、現在の入力テキストは、対話トランスクリプト内の現在のターンに対応する。これらの例において、現在の入力テキストに関連する1つまたは複数のコンテキスト特徴は、対話トランスクリプト内の前のターンに対応する対話トランスクリプト内の前のテキスト、または対話トランスクリプト内の次のターンに対応する対話トランスクリプト内の次のテキストのうちの1つなくとも1つを含む。 In another example, the text source includes a dialogue transcript and the current input text corresponds to the current turn within the dialogue transcript. In these examples, the one or more contextual features associated with the current input text are the previous text in the dialogue transcript corresponding to the previous turn in the dialogue transcript, or the next turn in the dialogue transcript. Contains at least one of the following text in the dialogue transcript corresponding to .
テキストソースは、クエリ応答システムも含み得、現在の入力テキストは、クエリ応答システムにおいて受信された現在のクエリへの応答に対応する。ここで、現在の入力テキストに関連する1つまたは複数のコンテキスト特徴は、現在のクエリに関連するテキスト、もしくはクエリ応答システムにおいて受信されたクエリのシーケンスに関連するテキスト、または現在のクエリに関連するオーディオ特徴、もしくはクエリ応答システムにおいて受信されたクエリのシーケンスに関連するオーディオ特徴のうちの少なくとも1つを含み得る。クエリのシーケンスは、現在のクエリと、現在のクエリに先行する1つまたは複数のクエリとを含み得る。 A text source may also include a query response system, where the current input text corresponds to a response to a current query received at the query response system. Here, the one or more contextual features related to the current input text are text related to the current query, or text related to the sequence of queries received in the query response system, or text related to the current query. At least one of audio features or audio features associated with the sequence of queries received in the query response system may be included. A sequence of queries may include a current query and one or more queries that precede the current query.
いくつかの実装形態において、TTSモデルは、エンコーダニューラルネットワークと、連結器と、注意ベースのデコーダリカレントニューラルネットワークとを含む。エンコーダニューラルネットワークは、テキストソースから現在の入力テキストを受信し、現在の入力テキストのそれぞれの符号化シーケンスを生成するために、現在の入力テキストを処理するように構成される。連結器は、エンコーダニューラルネットワークから現在の入力テキストのそれぞれの符号化シーケンスを受信し、テキスト予測ネットワークによって予測されたスタイル埋め込みを受信し、現在の入力テキストのそれぞれの符号化シーケンスとスタイル埋め込みとの間の連結を生成するように構成される。注意ベースのデコーダリカレントニューラルネットワークは、デコーダ入力のシーケンスを受信し、シーケンス内のデコーダ入力ごとに、出力オーディオ信号のrフレームを生成するために、対応するデコーダ入力と、現在の入力テキストのそれぞれの符号化シーケンスとスタイル埋め込みとの間の連結とを処理するように構成され、ここで、rは、1よりも大きい整数を含む。 In some implementations, the TTS model includes an encoder neural network, a concatenator, and an attention-based decoder recurrent neural network. The encoder neural network is configured to receive current input text from a text source and process the current input text to generate respective encoded sequences of the current input text. A concatenator receives the respective encoded sequences of the current input text from the encoder neural network, receives the style embeddings predicted by the text prediction network, and compares the respective encoded sequences of the current input text with the style embeddings. configured to generate connections between An attention-based decoder recurrent neural network receives a sequence of decoder inputs and, for each decoder input in the sequence, generates r frames of the output audio signal with the corresponding decoder input and each of the current input text. Concatenation between encoding sequences and style embeddings, where r comprises an integer greater than one.
TTSモデルがエンコーダニューラルネットワークを含む場合の実装形態において、エンコーダニューラルネットワークは、エンコーダプレネットニューラルネットワークと、エンコーダCBHGニューラルネットワークとを含み得る。エンコーダプレネットニューラルネットワークは、現在の入力テキストの文字のシーケンス内の各文字のそれぞれの埋め込みを受信し、文字ごとに、文字のそれぞれの変換された埋め込みを生成するためにそれぞれの埋め込みを処理するように構成される。エンコーダCBHGニューラルネットワークは、エンコーダプレネットニューラルネットワークによって生成された変換された埋め込みを受信し、現在の入力テキストのそれぞれの符号化シーケンスを生成するために、変換された埋め込みを処理するように構成される。いくつかの構成において、エンコーダCBHGニューラルネットワークは、1次元畳み込みフィルタのバンクを含み、その後にハイウェイネットワークが続き、その後に双方向リカレントニューラルネットワークが続く。 In implementations where the TTS model includes an encoder neural network, the encoder neural network may include an encoder prenet neural network and an encoder CBHG neural network. An encoder prenet neural network receives a respective embedding of each character in a sequence of characters of the current input text and processes each embedding to produce a respective transformed embedding of the character, character by character. configured as The encoder CBHG neural network is configured to receive the transformed embeddings produced by the encoder prenet neural network and process the transformed embeddings to produce a respective encoded sequence of the current input text. be. In some configurations, the encoder CBHG neural network includes a bank of one-dimensional convolutional filters, followed by a highway network, followed by a bidirectional recurrent neural network.
いくつかの構成において、テキスト予測ネットワークは、時間集約ゲート付き回帰型ユニット(GRU)リカレントニューラルネットワーク(RNN)と、1つまたは複数の完全接続層とを含む。GRU RNNは、現在の入力テキストに関連するコンテキスト埋め込みと、現在の入力テキストの符号化シーケンスとを受信し、コンテキスト埋め込みと符号化シーケンスとを処理することによって、固定長の特徴ベクトルを生成するように構成される。1つまたは複数の完全接続層は、固定長の特徴ベクトルを処理することによって、スタイル埋め込みを予測するように構成される。これらの構成において、1つまたは複数の完全接続層は、ReLUアクティベーションを使用する1つまたは複数の隠れ完全接続層と、予測されたスタイル埋め込みを発するためにtanhアクティベーションを使用する出力層とを含み得る。 In some configurations, the text prediction network includes a time aggregate gated recurrent unit (GRU) recurrent neural network (RNN) and one or more fully connected layers. A GRU RNN receives context embeddings associated with the current input text and an encoded sequence of the current input text, and generates a fixed-length feature vector by processing the context embeddings and the encoded sequence. configured to One or more fully connected layers are configured to predict style embeddings by processing fixed-length feature vectors. In these configurations, the one or more fully connected layers are divided into one or more hidden fully connected layers using ReLU activations and an output layer using tanh activations to emit predicted style embeddings. can include
コンテキストモデル、テキスト予測モデル、およびTTSモデルは、共同でトレーニングされ得る。代替的には、2ステップトレーニング手順は、トレーニング手順の第1のステップ中にTTSモデルをトレーニングし、トレーニング手順の第2のステップ中にコンテキストモデルとテキスト予測モデルとを共同で別々にトレーニングし得る。 A context model, a text prediction model, and a TTS model can be jointly trained. Alternatively, a two-step training procedure may train the TTS model during the first step of the training procedure and jointly and separately train the context model and the text prediction model during the second step of the training procedure. .
本開示の別の態様は、現在の入力テキストの表現力豊かなスピーチの出力オーディオ信号を生成するための方法を提供する。方法は、データ処理ハードウェアにおいて、テキストソースから現在の入力テキストを受信するステップを含む。現在の入力テキストは、テキスト読み上げ(TTS)モデルによって表現力豊かなスピーチに合成されることになっている。方法は、データ処理ハードウェアによって、コンテキストモデルを使用して、テキストソースから導出された1つまたは複数のコンテキスト特徴を処理することによって、現在の入力に関連するコンテキスト埋め込みを生成するステップも含む。方法は、データ処理ハードウェアによって、テキスト予測ネットワークを使用して、現在の入力テキストと現在の入力テキストに関連するコンテキスト埋め込みとを処理することによって、現在の入力テキストのためのスタイル埋め込みを予測するステップも含む。スタイル埋め込みは、現在の入力テキストを表現力豊かなスピーチに合成するための特定の韻律および/またはスタイルを指定する。方法は、データ処理ハードウェアによって、TTSモデルを使用して、スタイル埋め込みと現在の入力テキストとを処理することよって、現在の入力テキストの表現力豊かなスピーチの出力オーディオ信号を生成するステップも含む。出力オーディオ信号は、スタイル埋め込みによって指定された特定の韻律および/またはスタイルを有する。 Another aspect of the present disclosure provides a method for generating an output audio signal of expressive speech of current input text. The method includes receiving, at data processing hardware, current input text from a text source. Current input text is to be synthesized into expressive speech by a text-to-speech (TTS) model. The method also includes generating contextual embeddings associated with the current input by processing, by data processing hardware, one or more contextual features derived from the text source using the context model. The method predicts style embeddings for the current input text by processing the current input text and contextual embeddings associated with the current input text using a text prediction network with data processing hardware. Also includes steps. Style embeddings specify a particular prosody and/or style for synthesizing the current input text into expressive speech. The method also includes generating an output audio signal of expressive speech of the current input text by processing the style embeddings and the current input text using the TTS model by data processing hardware. . The output audio signal has a particular prosody and/or style specified by style embeddings.
この態様は、以下のオプションの特徴のうちの1つまたは複数を含み得る。いくつかの実装形態において、現在の入力テキストに関連する1つまたは複数のコンテキスト特徴は、現在の入力テキスト、現在の入力テキストに先行するテキストソースからの前のテキスト、前のテキストから合成された前のスピーチ、現在の入力テキストに続くテキストソースからの次のテキスト、または前のテキストと前のテキストに関連する前のコンテキスト埋め込みとに基づいてテキスト予測ネットワークによって予測された前のスタイル埋め込みのうちの少なくとも1つを含む。いくつかの例において、テキストソースは、テキスト文書を含み、現在の入力テキストに関連する1つまたは複数のコンテキスト特徴は、テキスト文書のタイトル、テキスト文書内の章のタイトル、テキスト文書内の節のタイトル、テキスト文書内の見出し、テキスト文書内の1つもしくは複数の箇条書き、テキスト文書から抽出された概念グラフからのエンティティ、テキスト文書から抽出された1つもしくは複数の構造化された回答表現のうちの少なくとも1つを含む。 This aspect may include one or more of the following optional features. In some implementations, the one or more contextual features associated with the current input text are the current input text, the previous text from the text source that precedes the current input text, and the previous text synthesized from the previous text. of the previous speech, the next text from the text source following the current input text, or the previous style embedding predicted by the text prediction network based on the previous text and previous contextual embeddings related to the previous text including at least one of In some examples, the text source includes a text document, and the one or more contextual features associated with the current input text are the title of the text document, the title of the chapter within the text document, the title of the section within the text document. A title, a heading within a text document, one or more bullet points within a text document, an entity from a concept graph extracted from the text document, one or more of the structured answer representations extracted from the text document. contains at least one of
他の例において、テキストソースは、対話トランスクリプトを含み、現在の入力テキストは、対話トランスクリプト内の現在のターンに対応する。これらの例において、現在の入力テキストに関連する1つまたは複数のコンテキスト特徴は、対話トランスクリプト内の前のターンに対応する対話トランスクリプト内の前のテキスト、または対話トランスクリプト内の次のターンに対応する対話トランスクリプト内の次のテキストのうちの少なくとも1つを含む。 In another example, the text source includes a dialogue transcript and the current input text corresponds to the current turn within the dialogue transcript. In these examples, the one or more contextual features associated with the current input text are the previous text in the dialogue transcript corresponding to the previous turn in the dialogue transcript, or the next turn in the dialogue transcript. Contain at least one of the following text in the dialogue transcript corresponding to:
テキストソースは、クエリ応答システムも含み得、現在の入力テキストは、クエリ応答システムにおいて受信された現在のクエリへの応答に対応する。ここで、現在の入力テキストに関連する1つまたは複数のコンテキスト特徴は、現在のクエリに関連するテキスト、もしくはクエリ応答システムにおいて受信されたクエリのシーケンスに関連するテキスト、または現在のクエリに関連するオーディオ特徴、もしくはクエリ応答システムにおいて受信されたクエリのシーケンスに関連するオーディオ特徴のうちの少なくとも1つを含み得る。クエリのシーケンスは、現在のクエリと、現在のクエリに先行する1つまたは複数のクエリとを含み得る。 A text source may also include a query response system, where the current input text corresponds to a response to a current query received at the query response system. Here, the one or more contextual features related to the current input text are text related to the current query, or text related to the sequence of queries received in the query response system, or text related to the current query. At least one of audio features or audio features associated with the sequence of queries received in the query response system may be included. A sequence of queries may include a current query and one or more queries that precede the current query.
いくつかの実装形態において、出力オーディオ信号を生成するステップは、テキスト読み上げモデルのエンコーダニューラルネットワークにおいて、テキストソースから現在の入力テキストを受信するステップと、エンコーダニューラルネットワークを使用して、現在の入力テキストのそれぞれの符号化シーケンスを生成するステップと、テキスト読み上げモデルの連結器を使用して、現在の入力テキストのそれぞれの符号化シーケンスとスタイル埋め込みとの間の連結を生成するステップと、テキスト読み上げモデルの注意ベースのデコーダリカレントニューラルネットワークにおいて、デコーダ入力のシーケンスを受信するステップと、デコーダ入力のシーケンス内のデコーダ入力ごとに、注意ベースのデコーダリカレントニューラルネットワークを使用して、出力オーディオ信号のrフレームを生成するために、対応するデコーダ入力と、現在の入力テキストのそれぞれの符号化シーケンスとスタイル埋め込みとの間の連結とを処理するステップとを含み、ここで、rは、1よりも大きい整数を含む。これらの実装形態において、現在の入力テキストのそれぞれの符号化シーケンスを生成するステップは、エンコーダニューラルネットワークのエンコーダプレネットニューラルネットワークにおいて、現在の入力テキストの文字のシーケンス内の各文字のそれぞれの埋め込みを受信するステップと、文字のシーケンス内の文字ごとに、エンコーダプレネットニューラルネットワークを使用して、文字のそれぞれの変換された埋め込みを生成するために、それぞれの埋め込みを処理するステップと、エンコーダニューラルネットワークのエンコーダCBHGニューラルネットワークを使用して、変換された埋め込みを処理することによって、現在の入力テキストのそれぞれの符号化シーケンスを生成するステップとを含む。いくつかの構成において、エンコーダCBHGニューラルネットワークは、1次元畳み込みフィルタのバンクを含み、その後にハイウェイネットワークが続き、その後に双方向リカレントニューラルネットワークが続く。 In some implementations, generating the output audio signal includes, in an encoder neural network of the text-to-speech model, receiving current input text from a text source; and using the concatenator of the text-to-speech model to generate a concatenation between each encoded sequence of the current input text and the style embeddings, and the text-to-speech model receiving a sequence of decoder inputs in the attention-based decoder recurrent neural network of and for each decoder input in the sequence of decoder inputs, generating r frames of the output audio signal using the attention-based decoder recurrent neural network processing the corresponding decoder input and the concatenation between each encoded sequence of the current input text and the style embeddings, where r is an integer greater than 1, to generate include. In these implementations, generating a respective encoded sequence of the current input text includes, in an encoder prenet neural network of the encoder neural network, a respective embedding of each character in the sequence of characters of the current input text. receiving and, for each character in the sequence of characters, processing each embedding to produce a transformed embedding for each of the characters using an encoder prenet neural network; and generating respective encoded sequences of the current input text by processing the transformed embeddings using the encoder CBHG neural network of . In some configurations, the encoder CBHG neural network includes a bank of one-dimensional convolutional filters, followed by a highway network, followed by a bidirectional recurrent neural network.
いくつかの例において、現在の入力テキストのためのスタイル埋め込みを予測するステップは、テキスト予測モデルの時間集約ゲート付き回帰型ユニット(GRU)リカレントニューラルネットワーク(RNN)を使用して、現在の入力テキストに関連するコンテキスト埋め込みと現在の入力テキストの符号化シーケンスとを処理することによって、固定長の特徴ベクトルを生成するステップと、GRU-RNNに続くテキスト予測モデルの1つまたは複数の完全接続層を使用して、固定長の特徴ベクトルを処理することによって、スタイル埋め込みを予測するステップとを含む。1つまたは複数の完全接続層は、ReLUアクティベーションを使用する1つまたは複数の隠れ完全接続層と、予測されたスタイル埋め込みを発するためにtanhアクティベーションを使用する出力層とを含み得る。 In some examples, the step of predicting style embeddings for the current input text uses a time-aggregate gated recurrent unit (GRU) recurrent neural network (RNN) of the text prediction model to predict the style embeddings for the current input text. generating fixed-length feature vectors by processing the context embeddings associated with and the encoded sequence of the current input text, and one or more fully-connected layers of the text prediction model following the GRU-RNN. and predicting the style embeddings by processing the fixed-length feature vectors using. The one or more fully connected layers may include one or more hidden fully connected layers using ReLU activations and an output layer using tanh activations to emit predicted style embeddings.
コンテキストモデル、テキスト予測モデル、およびTTSモデルは、共同でトレーニングされ得る。代替的には、2ステップトレーニング手順は、トレーニング手順の第1のステップ中にTTSモデルをトレーニングし、トレーニング手順の第2のステップ中にコンテキストモデルとテキスト予測モデルとを共同で別々にトレーニングし得る。 A context model, a text prediction model, and a TTS model can be jointly trained. Alternatively, a two-step training procedure may train the TTS model during the first step of the training procedure and jointly and separately train the context model and the text prediction model during the second step of the training procedure. .
本開示の1つまたは複数の実装形態の詳細は、添付図面および以下の説明において記載されている。他の態様、特徴、および利点は、説明および図面から、ならびに特許請求の範囲から明らかになるであろう。 The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
様々な図面における同様の参照記号は、同様の要素を示す。 Like reference symbols in the various drawings indicate like elements.
現実的な人間のスピーチの合成は、同じテキスト入力が無限の数の合理的な発話認識を有するという点で劣決定問題である。エンドツーエンドのニューラルネットワークベースの手法は、短いアシスタントのような発話について人間のパフォーマンスに一致するように進歩しているが、ニューラルネットワークモデルは、ときには、各々が洗練された言語表現または音声表現において動作する複数の処理ステップを含むより従来のモデルよりも、解釈可能または制御可能ではない。 Synthesizing realistic human speech is an underdetermined problem in that the same text input has an infinite number of reasonable speech recognitions. Although end-to-end neural network-based approaches have progressed to match human performance for short assistant-like utterances, neural network models sometimes fall short in their respective sophisticated verbal or speech representations. It is less interpretable or controllable than more traditional models that involve multiple processing steps to operate.
テキスト読み上げ(TTS)システムの主要な課題は、所与の入力テキストに対して自然に聞こえる発話スタイルを生成するためのモデルを開発することである。特に、自然に聞こえるスピーチを生成するための課題に寄与する要因のうちのいくつかは、高いオーディオ忠実度と、正しい発音と、許容可能な韻律およびスタイルとを含み、それによって、「韻律」とは、一般に、ピッチ、強勢、中断、およびリズムなどの低レベルの特性を指す。韻律は、「スタイル」に影響を与え、スタイルは、感情価および覚醒などの、スピーチのより高いレベルの特性を指す。このように、韻律およびスタイルは、合成されるべきテキストにおいて指定されていない情報を包含し、合成されたスピーチが無限の方法で話されることを可能にするので、モデル化するのが困難である。簡単に言えば、テキストは、スタイルおよび韻律に関する情報が利用できないという点で指定不足であり、テキストからスピーチへのマッピングに、1対多の問題を残す。 A major challenge for text-to-speech (TTS) systems is to develop models for generating natural-sounding speaking styles for given input text. In particular, some of the factors that contribute to the challenge of producing natural-sounding speech include high audio fidelity, correct pronunciation, and acceptable prosody and style, whereby "prosody" and generally refers to low-level properties such as pitch, stress, pauses, and rhythm. Prosody influences "style," which refers to higher-level characteristics of speech such as emotional valence and arousal. Thus, prosody and style are difficult to model because they contain information not specified in the text to be synthesized, allowing synthesized speech to be spoken in an infinite number of ways. be. Simply put, text is underspecified in that no information about style and prosody is available, leaving text-to-speech mapping a one-to-many problem.
シンセサイザへの入力として、高レベルのスタイルラベル(たとえば、感情を伝える)または低レベルの注釈(たとえば、音節ストレスマーカ、速度制御、ピッチトラックなど)を提供することは、韻律およびスタイルのモデル化を改善し得るが、これらの手法にはいくつかの欠点が存在する。すなわち、明示的なラベルは、正確に定義することが困難であり、取得にコストがかかり、本質的にノイズが多く、聞き手による知覚品質との相関を保証しない。さらに、韻律とスタイルとをモデル化するための明示的なラベル入力は、しばしば、手動で調整されたヒューリスティックまたは個別にトレーニングされたモデルから導出される。それに加えて、これらの入力が導出されたコンテキストは、通常、失われる。 Providing high-level style labels (e.g., conveying emotion) or low-level annotations (e.g., syllable stress markers, rate controls, pitch tracks, etc.) as input to the synthesizer facilitates modeling of prosody and style. Although capable of improvement, these approaches have several drawbacks. That is, explicit labels are difficult to define precisely, are expensive to acquire, are inherently noisy, and do not guarantee correlation with perceived quality by listeners. Moreover, explicit label inputs for modeling prosody and style are often derived from manually tuned heuristics or individually trained models. Additionally, the context from which these inputs were derived is typically lost.
一般に、TTSシステムは、一度に単一の文または段落を合成することによってスピーチを生成する。結果として、テキストが抜き取られるコンテキストにアクセスできない場合、結果として生じる合成スピーチの自然な表現度は、制限される。オーディオブックなどの長い形式の表現力豊かなテキストのデータセットからスピーチを合成する場合、幅広い発話スタイルを伝えることは、特に困難である。たとえば、幅広い様々な音声特性を単一の平均化された音律スタイルのモデルに単純に折りたたむことは、テキストが伝えることを意図した適切な感情価および覚醒を正確に反映しない可能性がある特定の発話スタイルを有する合成スピーチを結果として生じる。一例において、単一の平均化された韻律スタイルのモデルをオーディオブックのためのスピーチを合成するために適用することは、オーディオブックにおける幸せな章から次の悲しい章への感情的な遷移など、様々な感情を伝えるのに必要な発話スタイルのすべてを適切に表現しないことになる。同様に、オーディオブックは、著しくスタイルが変動するキャラクターの声を含む場合がある。これらの例において、平均化された韻律スタイルのモデルを使用することは、感情的な遷移または異なるキャラクターの声の間のスタイルの変動を伝えない単調に聞こえるスピーチを生成することになる。合成されるべきスピーチのための目標の韻律スタイルを伝える参照オーディオを提供すること、または推論時に目標の韻律スタイルを選択するために手動で重みを選択することは、様々な発話スタイルの要因を効果的に解きほぐし得るが、これらの手法は、教師あり学習モデルにおいてトレーニングされ、このような長い形式の表現力豊かな入力テキストのデータセット(たとえば、オーディオブック)からスピーチを合成するには理想的ではない。 Generally, TTS systems generate speech by synthesizing a single sentence or paragraph at a time. As a result, the natural expressiveness of the resulting synthesized speech is limited if the context from which the text is extracted is not accessible. Conveying a wide range of speaking styles is particularly difficult when synthesizing speech from datasets of long-form, expressive text such as audiobooks. For example, simply collapsing a wide variety of phonetic characteristics into a single, averaged model of temperamental style may not accurately reflect the appropriate emotional valence and arousal that the text is intended to convey, especially in particular Synthetic speech with speaking styles results. In one example, applying a single averaged prosody-style model to synthesize speech for an audiobook shows the emotional transition, such as from a happy chapter to the next sad chapter in an audiobook. You will not adequately express all of the speaking styles necessary to convey different emotions. Similarly, audiobooks may contain character voices that vary significantly in style. In these examples, using an averaged prosodic style model will produce monotonous-sounding speech that does not convey emotional transitions or stylistic variations between the voices of different characters. Providing reference audio that conveys the target prosodic style for the speech to be synthesized, or manually selecting weights to select the target prosodic style during inference, effects various speaking style factors. Although generally untangling, these methods are not ideal for training in supervised learning models to synthesize speech from such long-form, expressive input text datasets (e.g., audiobooks). Absent.
本明細書における実装形態は、入力テキストシーケンスから合成スピーチを生成するためのエンドツーエンドテキスト読み上げ(TTS)モデルにおいて使用するための「仮想」発話スタイルラベルとして韻律スタイル埋め込みを適用するように構成された例示的なアーキテクチャに向けられている。明らかになるように、これらの例示的なアーキテクチャは、入力テキストシーケンスのみから導出されたコンテキストから文体レンダリングを学習および予測するために教師なしモデルを使用してトレーニングされ得、トレーニング中の明示的なラベルも、推論時の他の補助入力も必要としない。このように、これらの実装形態は、テキストのみから、発話スタイルと背景雑音とを含む、話者に依存しない変動要因を捕捉することができる。 Implementations herein are configured to apply prosodic style embeddings as "virtual" speaking style labels for use in end-to-end text-to-speech (TTS) models for generating synthetic speech from input text sequences. is directed to an exemplary architecture. As will become apparent, these exemplary architectures can be trained using unsupervised models to learn and predict stylistic renderings from contexts derived from input text sequences only, with explicit It does not require labels or other auxiliary inputs during inference. Thus, these implementations can capture speaker-independent variables, including speaking style and background noise, from text alone.
本明細書における実装形態は、さらに、現在の入力テキストシーケンスに関する文体レンダリングを予測するための条件付き入力として追加のコンテキスト特徴を受信するように構成されたコンテキスト予測システムに向けられている。ここで、入力テキストシーケンスおよび各コンテキスト特徴は、入力テキストシーケンスから合成されたスピーチの適切な文体レンダリングを予測するためのコンテキストとして機能し得る。コンテキスト特徴は、単語埋め込み、文埋め込み、および/またはスピーチタグ(たとえば、名詞、動詞、形容詞など)を含み得る。本明細書で使用される場合、利用可能なコンテキスト特徴は、限定はしないが、前の/過去のテキスト、次の/将来のテキスト、および前の/過去のオーディオを含むことができる。別の言い方をすれば、コンテキスト特徴は、合成されるべき現在の入力テキストのテキストソースから導出され得る。コンテキスト特徴の追加のソースは、タイトル、章のタイトル、節のタイトル、見出し、箇条書きなどの、合成されるべきテキストを含む文書構造から取得され得る。いくつかの例において、概念グラフ(たとえば、ウィキペディア)および/または構造化された回答表現からのエンティティに関連する概念は、ソースのコンテキスト特徴である。さらに、デジタルアシスタント環境において、クエリ(またはクエリのシーケンス)から導出されたオーディオ/テキスト特徴が、応答を合成するときにコンテキスト特徴として使用され得、一方、対話内の前のおよび/または次の「ターン」のテキストが、対応する対話を合成するためのコンテキスト特徴として導出され得る。追加的または代替的に、仮想環境内に存在するキャラクターおよびオブジェクト(たとえば、絵文字)も、現在の入力テキストシーケンスに関する文体レンダリングを予測するためのコンテキスト特徴のソースであり得る。 Implementations herein are further directed to a context prediction system configured to receive additional context features as conditional inputs for predicting stylistic rendering for a current input text sequence. Here, the input text sequence and each context feature can serve as a context for predicting proper stylistic rendering of speech synthesized from the input text sequence. Contextual features may include word embeddings, sentence embeddings, and/or speech tags (eg, nouns, verbs, adjectives, etc.). As used herein, available contextual features may include, but are not limited to, previous/past text, next/future text, and previous/past audio. Stated another way, contextual features may be derived from the text source of the current input text to be synthesized. Additional sources of contextual features can be obtained from the document structure containing the text to be synthesized, such as titles, chapter titles, section titles, headings, bullet points, and the like. In some examples, concepts associated with entities from concept graphs (eg, Wikipedia) and/or structured answer expressions are contextual features of the source. Furthermore, in a digital assistant environment, audio/text features derived from a query (or sequence of queries) can be used as contextual features when synthesizing responses, while The text of the "turn" can be derived as a contextual feature for synthesizing the corresponding dialogue. Additionally or alternatively, characters and objects (eg, glyphs) existing within the virtual environment may also be sources of contextual features for predicting stylistic rendering for the current input text sequence.
図1を参照すると、いくつかの実装形態において、例示的なテキスト読み上げ(TTS)変換システム100は、入力テキスト104を入力として受信し、スピーチ120を出力として生成するために入力テキスト104を処理するように構成されたサブシステム102を含む。入力テキスト104は、特定の自然言語の文字のシーケンスを含む。文字のシーケンスは、アルファベット、数字、句読点、および/または他の特殊文字を含み得る。入力テキスト104は、様々な長さの文字のシーケンスであり得る。テキスト読み上げ変換システム100は、1つまたは複数の場所における1つまたは複数のコンピュータ上のコンピュータプログラムとして実装されたシステムの例であり、以下で説明されるシステム、構成要素、および技法が実装され得る。たとえば、システム100は、図9のコンピュータシステム900上で実行され得る。
Referring to FIG. 1, in some implementations, an exemplary text-to-speech (TTS)
入力テキスト104を処理するために、サブシステム102は、シーケンス間リカレントニューラルネットワーク106(以下、「seq2seqネットワーク106」)と、後処理ニューラルネットワーク108と、波形シンセサイザ110とを含むエンドツーエンドテキスト読み上げモデル150と対話するように構成される。 To process input text 104, subsystem 102 uses an end-to-end text-to-speech model that includes an inter-sequence recurrent neural network 106 (hereinafter "seq2seq network 106"), a post-processing neural network 108, and a waveform synthesizer 110. Configured to interact with 150.
サブシステム102が、特定の自然言語における文字のシーケンスを含む入力テキスト104を受信した後、サブシステム102は、seq2seqネットワーク106への入力として文字のシーケンスを提供する。seq2seqネットワーク106は、サブシステム102から文字のシーケンスを受信し、特定の自然言語における文字のシーケンスの口頭発話のスペクトログラムを生成するために文字のシーケンスを処理するように構成される。 After subsystem 102 receives input text 104 containing a sequence of characters in a particular natural language, subsystem 102 provides the sequence of characters as input to seq2seq network 106 . The seq2seq network 106 is configured to receive a sequence of characters from the subsystem 102 and process the sequence of characters to generate a spoken speech spectrogram for the sequence of characters in a particular natural language.
特に、seq2seqネットワーク106は、(i)エンコーダプレネットニューラルネットワーク114とエンコーダCBHGニューラルネットワーク116とを含むエンコーダニューラルネットワーク112と、(ii)注意ベースのデコーダリカレントニューラルネットワーク118とを使用して文字のシーケンスを処理する。CBHGは、畳込み、フィルタバンク、およびハイウェイ層、ゲート付き回帰型ユニットの頭文字である。文字のシーケンス内の各文字は、ワンホットベクトルとして表現され、連続ベクトルに埋め込まれ得る。すなわち、サブシステム102は、シーケンス内の各文字をワンホットベクトルとして表現し、次いで、シーケンスをseq2seqネットワーク106への入力として提供する前に、文字の埋め込み、すなわち、ベクトルまたは数値の他の順序付きコレクションを生成し得る。 In particular, the seq2seq network 106 uses (i) an encoder neural network 112 that includes an encoder prenet neural network 114 and an encoder CBHG neural network 116, and (ii) an attention-based decoder recurrent neural network 118 to generate sequences of characters. process. CBHG is an acronym for Convolution, Filterbank and Highway Layer, Gated Recursive Unit. Each character in a sequence of characters can be represented as a one-hot vector and embedded in successive vectors. That is, subsystem 102 represents each character in the sequence as a one-hot vector, then embeds the characters, i.e., vectors or other ordered sequences of numbers, before providing the sequence as input to seq2seq network 106 . You can create collections.
エンコーダプレネットニューラルネットワーク114は、シーケンス内の各文字のそれぞれの埋め込みを受信し、文字の変換された埋め込みを生成するために各文字のそれぞれの埋め込みを処理するように構成される。たとえば、エンコーダプレネットニューラルネットワーク114は、変換された埋め込みを生成するために、非線形変換のセットを各埋め込みに適用することができる。場合によっては、エンコーダプレネットニューラルネットワーク114は、収束速度を向上させ、トレーニング中のシステムの汎化能力を改善するために、ドロップアウトを有するボトルネックニューラルネットワーク層を含む。 Encoder prenet neural network 114 is configured to receive a respective embedding of each character in the sequence and process the respective embedding of each character to produce a transformed embedding of the character. For example, encoder prenet neural network 114 can apply a set of non-linear transforms to each embedding to generate a transformed embedding. In some cases, the encoder prenet neural network 114 includes a bottleneck neural network layer with dropouts to improve the speed of convergence and improve the generalization ability of the system under training.
エンコーダCBHGニューラルネットワーク116は、エンコーダプレネットニューラルネットワーク114から変換された埋め込みを受信し、文字のシーケンスの符号化表現を生成するために、変換された埋め込みを処理するように構成される。エンコーダCBHGニューラルネットワーク112は、図2を参照して以下により詳細に説明されるCBHGニューラルネットワーク200(図2)を含む。本明細書で説明されているようなエンコーダCBHGニューラルネットワーク112の使用は、オーバフィッティングを低減し得る。それに加えて、エンコーダCBHGニューラルネットワーク112は、たとえば、多層RNNエンコーダと比較したときに、より少ない誤った発音を結果として生じ得る。 Encoder CBHG neural network 116 is configured to receive the transformed embeddings from encoder prenet neural network 114 and process the transformed embeddings to generate an encoded representation of the sequence of characters. Encoder CBHG neural network 112 includes CBHG neural network 200 (FIG. 2), which is described in more detail below with reference to FIG. Use of encoder CBHG neural network 112 as described herein may reduce overfitting. Additionally, the encoder CBHG neural network 112 may result in fewer mispronunciations, eg, when compared to a multi-layer RNN encoder.
注意ベースのデコーダリカレントニューラルネットワーク118(本明細書では「デコーダニューラルネットワーク118」と呼ぶ)は、デコーダ入力のシーケンスを受信するように構成される。シーケンス内のデコーダ入力ごとに、デコーダニューラルネットワーク118は、文字のシーケンスのスペクトログラムの複数のフレームを生成するために、デコーダ入力と、エンコーダCBHGニューラルネットワーク116によって生成された符号化表現とを処理するように構成される。すなわち、各デコーダステップにおいて1つのフレームを生成する(予測する)代わりに、デコーダニューラルネットワーク118は、スペクトログラムのrフレームを生成し、rは、1よりも大きい整数である。多くの場合、rフレームのセット間に重複はない。 Attention-based decoder recurrent neural network 118 (referred to herein as "decoder neural network 118") is configured to receive a sequence of decoder inputs. For each decoder input in the sequence, decoder neural network 118 processes the decoder input and the encoded representation generated by encoder CBHG neural network 116 to generate multiple frames of spectrograms for the sequence of characters. configured to That is, instead of generating (predicting) one frame at each decoder step, decoder neural network 118 generates r frames of the spectrogram, where r is an integer greater than one. In many cases there is no overlap between sets of r frames.
特に、デコーダステップtにおいて、デコーダステップt-1において生成されたrフレームの少なくとも最後のフレームは、デコーダステップt+1におけるデコーダニューラルネットワーク118への入力として供給される。いくつかの実装形態において、デコーダステップt-1において生成されたrフレームのすべてが、デコーダステップt+1におけるデコーダニューラルネットワーク118への入力として供給される。第1のデコーダステップのためのデコーダ入力は、すべてゼロのフレーム(すなわち、<GO>フレーム)であり得る。符号化表現に対する注意は、たとえば、従来の注意メカニズムを使用して、すべてのデコーダステップに適用される。デコーダニューラルネットワーク118は、所与のデコーダステップにおいてrフレームを同時に予測するために、線形アクティベーションを有する完全接続ニューラルネットワーク層を使用し得る。たとえば、各フレームが80-D(80次元)ベクトルである5フレームを予測するために、デコーダニューラルネットワーク118は、400次元ベクトルを予測し、5フレームを取得するために400次元ベクトルを再形成するために、線形アクティベーションを有する完全接続ニューラルネットワークを使用する。 Specifically, at decoder step t, at least the last of the r frames generated at decoder step t-1 is provided as input to decoder neural network 118 at decoder step t+1. In some implementations, all of the r frames generated at decoder step t-1 are provided as inputs to decoder neural network 118 at decoder step t+1. The decoder input for the first decoder step may be a frame of all zeros (ie, a <GO> frame). Attention to the coded representation is applied to all decoder steps, for example using conventional attention mechanisms. Decoder neural network 118 may use fully connected neural network layers with linear activations to predict r frames simultaneously in a given decoder step. For example, to predict 5 frames where each frame is an 80-D (80-dimensional) vector, the decoder neural network 118 predicts 400-dimensional vectors and reforms the 400-dimensional vectors to obtain 5 frames. For this purpose, we use a fully connected neural network with linear activation.
各時間ステップにおいてrフレームを生成することによって、デコーダニューラルネットワーク118は、デコーダステップの総数をrによって除算し、したがって、モデルサイズ、トレーニング時間、および推論時間を低減する。それに加えて、すなわち、この技法は、注意メカニズムによって学習されるようにフレームと符号化表現との間のはるかにより高速の(かつより安定した)アライメントを結果として生じるので、収束速度を大幅に向上させる。これは、隣接するスピーチフレームが相関しており、各文字が、通常、複数のフレームに対応するからである。時間ステップにおいて複数のフレームを発することは、デコーダニューラルネットワーク118が、トレーニング中に符号化表現を効率的に処理する方法を迅速に学習するため、すなわち、トレーニングされるためにこの品質を活用することを可能にする。 By generating r frames at each time step, decoder neural network 118 divides the total number of decoder steps by r, thus reducing model size, training time, and inference time. In addition, i.e., this technique results in much faster (and more stable) alignment between the frame and the coded representation as learned by the attention mechanism, thus greatly improving the convergence speed. Let This is because adjacent speech frames are correlated and each character typically corresponds to multiple frames. Emitting multiple frames at a time step allows the decoder neural network 118 to quickly learn how to efficiently process the encoded representations during training, i.e., take advantage of this quality to be trained. enable
デコーダニューラルネットワーク118は、1つまたは複数のゲート付き回帰型ユニットニューラルネットワーク層を含み得る。収束を高速化するために、デコーダニューラルネットワーク118は、1つまたは複数の垂直残差接続(vertical residual connection)を含み得る。いくつかの実装形態において、スペクトログラムは、メルスケールスペクトログラムなどの圧縮されたスペクトログラムである。たとえば、生のスペクトログラムの代わりに圧縮されたスペクトログラムを使用することは、冗長性を低減し、それによって、トレーニングおよび推論中に必要な計算を低減し得る。 Decoder neural network 118 may include one or more gated recurrent unit neural network layers. To speed up convergence, decoder neural network 118 may include one or more vertical residual connections. In some implementations, the spectrogram is a compressed spectrogram, such as a melscale spectrogram. For example, using compressed spectrograms instead of raw spectrograms may reduce redundancy, thereby reducing the computation required during training and inference.
後処理ニューラルネットワーク108は、圧縮されたスペクトログラムを受信し、波形シンセサイザ入力を生成するために、圧縮されたスペクトログラムを処理するように構成される。圧縮されたスペクトログラムを処理するために、後処理ニューラルネットワーク108は、CBHGニューラルネットワーク200(図2)を含む。特に、CBHGニューラルネットワーク200は、1次元畳み込みサブネットワークを含み、その後にハイウェイネットワークが続き、その後に双方向リカレントニューラルネットワークが続く。CBHGニューラルネットワーク200は、1つまたは複数の残差接続を含み得る。1次元畳み込みサブネットワークは、1次元畳み込みフィルタのバンクと、それに続くストライド1を有する時間層に沿った最大プーリングとを含み得る。場合によっては、双方向リカレントニューラルネットワークは、ゲート付き回帰型ユニット(GRU)リカレントニューラルネットワーク(RNN)である。CBHGニューラルネットワーク200について、図2を参照して以下でより詳細に説明される。 A post-processing neural network 108 is configured to receive the compressed spectrogram and process the compressed spectrogram to generate a waveform synthesizer input. To process the compressed spectrogram, post-processing neural network 108 includes CBHG neural network 200 (FIG. 2). In particular, CBHG neural network 200 includes a one-dimensional convolutional sub-network, followed by a highway network, followed by a bidirectional recurrent neural network. CBHG neural network 200 may include one or more residual connections. A one-dimensional convolutional sub-network may include a bank of one-dimensional convolution filters followed by max pooling along the temporal layer with stride one. In some cases, the bidirectional recurrent neural network is a gated recurrent unit (GRU) recurrent neural network (RNN). CBHG neural network 200 is described in more detail below with reference to FIG.
いくつかの実装形態において、後処理ニューラルネットワーク108およびシーケンス間リカレントニューラルネットワーク106は、共同でトレーニングされる。すなわち、トレーニング中、システム１００の(または外部システム)は、同じニューラルネットワークトレーニング技法、たとえば、勾配降下トレーニング技法を使用して、同じトレーニングデータセットにおいて後処理ニューラルネットワーク108とseq2seqネットワーク106とをトレーニングする。より具体的には、システム100(または外部システム)は、後処理ニューラルネットワーク108およびseq2seqネットワーク106のすべてのネットワークパラメータの現在の値を共同で調整するために、損失関数の勾配の推定値をバックプロパゲーションすることができる。別々にトレーニングされるかまたは事前にトレーニングされる必要がある構成要素を有し、したがって、各構成要素のエラーが複合的になる可能性がある従来のシステムとは異なり、共同でトレーニングされる後処理ニューラルネットワーク108およびseq2seqネットワーク106を有するシステムは、より堅牢である(たとえば、それらはより小さいエラーを有し、最初からトレーニングされ得る)。これらの利点は、現実世界において見られる非常に大量の豊かで表現力豊かでありながら、しばしばノイズの多いデータにおけるエンドツーエンドテキスト読み上げモデル150のトレーニングを可能にする。 In some implementations, the post-processing neural network 108 and the inter-sequence recurrent neural network 106 are jointly trained. That is, during training, system 100 (or an external system) uses the same neural network training technique, e.g., gradient descent training technique, to train post-processing neural network 108 and seq2seq network 106 on the same training data set. . More specifically, the system 100 (or an external system) backs up the estimate of the slope of the loss function to jointly adjust the current values of all network parameters of the post-processing neural network 108 and the seq2seq network 106. can be propagated. After being jointly trained, unlike conventional systems that have components that need to be trained separately or pre-trained, and thus the error of each component can be compounded. Systems with processing neural networks 108 and seq2seq networks 106 are more robust (eg, they have smaller errors and can be trained from scratch). These advantages enable training of end-to-end text-to-speech models 150 on very large amounts of rich, expressive yet often noisy data found in the real world.
波形シンセサイザ110は、波形シンセサイザ入力を受信し、特定の自然言語における文字の入力シーケンスの口頭発話の波形を生成するために波形シンセサイザ入力を処理するように構成される。いくつかの実装形態において、波形シンセサイザは、Griffin-Limシンセサイザである。いくつかの他の実装形態において、波形シンセサイザは、ボコーダである。いくつかの他の実装形態において、波形シンセサイザは、トレーニング可能なスペクトログラムから波形へのインバータである。波形シンセサイザ110が波形を生成した後、サブシステム102は、波形を使用してスピーチ120を生成し、生成されたスピーチ120を、たとえば、ユーザデバイス上で再生するために提供するか、または他のシステムがスピーチを生成および再生することを可能にするために、生成された波形を別のシステムに提供することができる。いくつかの例において、WaveNetニューラルボコーダが、波形シンセサイザ110を置き換える。WaveNetニューラルボコーダは、波形シンセサイザ110によって生成された合成スピーチと比較して、合成音声の異なるオーディオ忠実度を提供し得る。 Waveform synthesizer 110 is configured to receive a waveform synthesizer input and process the waveform synthesizer input to generate a spoken speech waveform of an input sequence of characters in a particular natural language. In some implementations, the waveform synthesizer is a Griffin-Lim synthesizer. In some other implementations, the waveform synthesizer is a vocoder. In some other implementations, the waveform synthesizer is a trainable spectrogram-to-waveform inverter. After waveform synthesizer 110 generates the waveform, subsystem 102 generates speech 120 using the waveform and provides generated speech 120 for playback, for example, on a user device or other device. The generated waveforms can be provided to another system to enable the system to generate and reproduce speech. A WaveNet neural vocoder replaces the waveform synthesizer 110 in some examples. A WaveNet neural vocoder may provide different audio fidelity of synthesized speech compared to synthesized speech produced by waveform synthesizer 110 .
図2は、例示的なCBHGニューラルネットワーク200を示す。CBHGニューラルネットワーク200は、エンコーダCBHGニューラルネットワーク116内に含まれるCBHGニューラルネットワーク、または図1の後処理ニューラルネットワーク108内に含まれるCBHGニューラルネットワークであり得る。CBHGニューラルネットワーク200は、1次元畳み込みサブネットワーク208を含み、その後にハイウェイネットワーク212が続き、その後に双方向リカレントニューラルネットワーク214が続く。CBHGニューラルネットワーク200は、1つまたは複数の残差接続、たとえば、残差接続210を含み得る。 FIG. 2 shows an exemplary CBHG neural network 200. As shown in FIG. CBHG neural network 200 may be the CBHG neural network contained within encoder CBHG neural network 116 or the CBHG neural network contained within post-processing neural network 108 of FIG. CBHG neural network 200 includes a one-dimensional convolutional sub-network 208 followed by highway network 212 followed by bidirectional recurrent neural network 214 . CBHG neural network 200 may include one or more residual connections, eg, residual connections 210 .
1次元畳み込みサブネットワーク208は、1次元畳み込みフィルタのバンク204と、それに続く1のストライドを有する時間層に沿った最大プーリング206とを含み得る。1次元畳み込みフィルタのバンク204は、1次元畳み込みフィルタのKセットを含み得、k番目のセットは、各々がkの畳み込み幅を有するCkフィルタを含む。1次元畳み込みサブネットワーク208は、入力シーケンス202、たとえば、エンコーダプレネットニューラルネットワーク114(図1)によって生成された文字のシーケンスの変換された埋め込みを受信するように構成される。サブネットワーク208は、入力シーケンス202の畳み込み出力を生成するために、1次元畳み込みフィルタのバンク204を使用して入力シーケンス202を処理する。次いで、サブネットワーク208は、畳込み出力を一緒にスタックし、最大プールされた出力を生成するために、ストライド1を有する時間層に沿った最大プーリング206を使用して、スタックされた畳込み出力を処理する。次いで、サブネットワーク208は、サブネットワーク208のサブネットワーク出力を生成するために、1つまたは複数の固定幅の1次元畳み込みフィルタを使用して、最大プールされた出力を処理する。
A one-dimensional convolutional sub-network 208 may include a bank of one-dimensional convolution filters 204 followed by max pooling 206 along the temporal layer with a stride of one. The bank of one-dimensional convolution filters 204 may contain K sets of one-dimensional convolution filters, the kth set containing C k filters each having a convolution width of k. One-dimensional convolutional sub-network 208 is configured to receive input sequence 202, eg, a transformed embedding of the sequence of characters generated by encoder planet neural network 114 (FIG. 1). A sub-network 208 processes the input sequence 202 using a bank of one-
1次元畳み込みサブネットワーク208がサブネットワーク出力を生成した後、残差接続210は、畳み込み出力を生成するために、サブネットワーク出力を元の入力シーケンス202と結合するように構成される。次いで、ハイウェイネットワーク212および双方向リカレントニューラルネットワーク214は、文字のシーケンスの符号化表現を生成するために畳み込み出力を処理するように構成される。特に、ハイウェイネットワーク212は、文字のシーケンスの高レベルの特徴表現を生成するために畳み込み出力を処理するように構成される。いくつかの実装形態において、ハイウェイネットワークは、1つまたは複数の完全接続ニューラルネットワーク層を含む。 After the one-dimensional convolution sub-network 208 has produced the sub-network output, residual connection 210 is configured to combine the sub-network output with the original input sequence 202 to produce the convolution output. Highway network 212 and bidirectional recurrent neural network 214 are then configured to process the convolutional output to produce an encoded representation of the sequence of characters. In particular, highway network 212 is configured to process the convolution output to generate a high-level feature representation of the sequence of characters. In some implementations, the highway network includes one or more fully connected neural network layers.
双方向リカレントニューラルネットワーク214は、文字のシーケンスの逐次的特徴表現を生成するために高レベルの特徴表現を処理するように構成される。逐次的特徴表現は、特定の文字の周りの文字のシーケンスの局所的構造を表す。逐次的特徴表現は、特徴ベクトルのシーケンスを含み得る。いくつかの実装形態において、双方向リカレントニューラルネットワークは、ゲート付き回帰型ユニットニューラルネットワークである。 Bidirectional recurrent neural network 214 is configured to process high-level representations to generate sequential representations of sequences of characters. A sequential feature representation represents the local structure of a sequence of characters around a particular character. A sequential feature representation may include a sequence of feature vectors. In some implementations, the bidirectional recurrent neural network is a gated recurrent unit neural network.
トレーニング中、1次元畳み込みサブネットワーク208の畳み込みフィルタの1つまたは複数は、S. IoffeおよびC. Szegedy、「Batch normalization: Accelerating deep network training by reducing internal covariate shift」、arXiv preprint arXiv:1502.03167、2015年に詳細に記載されているバッチ正規化方法を使用してトレーニングされ得る。いくつかの実装形態において、CBHGニューラルネットワーク200内の1つまたは複数の畳み込みフィルタは、非因果的畳み込みフィルタ、すなわち、所与の時間ステップTにおいて、両方の方向において周囲の入力(たとえば、T-1、T-2、およびT+1、T+2、...など)と畳み込むことができる畳み込みフィルタである。対照的に、因果的畳み込みフィルタは、前の入力(...T-2、T-2など)とのみ畳み込むことができる。いくつかの他の実装形態において、いくつかの他の実装形態において、CBHGニューラルネットワーク200内のすべての畳み込みフィルタは、非因果的畳み込みフィルタである。非因果的畳み込みフィルタ、バッチ正規化、残差接続、およびストライド1を有する時間層に沿った最大プーリングの使用は、入力シーケンスにおけるCBHGニューラルネットワーク200の汎化能力を改善し、したがって、テキスト読み上げ変換システムが高品質のスピーチを生成することを可能にする。
During training, one or more of the convolutional filters of the one-dimensional convolutional sub-network 208 are modified according to S. Ioffe and C. Szegedy, "Batch normalization: Accelerating deep network training by reducing internal covariate shift," arXiv preprint arXiv:1502.03167, 2015. can be trained using the batch normalization method described in detail in . In some implementations, one or more convolutional filters in CBHG neural network 200 are non-causal convolutional filters, i.e., at a given time step T, surrounding inputs in both directions (e.g., T- 1, T-2, and T+1, T+2, etc.). In contrast, causal convolution filters can only convolve with previous inputs (...T-2, T-2, etc.). In some other implementations, all convolution filters in CBHG neural network 200 are non-causal convolution filters. The use of non-causal convolutional filters, batch normalization, residual connection, and max pooling along temporal layers with
図3は、文字のシーケンスからスピーチを生成する方法300のための動作の例示的な配置である。便宜上、方法300は、1つまたは複数の場所において配置された1つまたは複数のコンピュータのシステムによって実行されるものとして説明される。たとえば、適切にプログラムされたテキスト読み上げ変換システム(たとえば、図1のテキスト読み上げ変換システム100)またはテキスト読み上げ変換システムのサブシステム(たとえば、図1のサブシステム102)が、方法300を実行することができる。
FIG. 3 is an exemplary arrangement of operations for a
動作302において、方法300は、システムが特定の自然言語における文字のシーケンスを受信することを含み、動作304において、方法300は、システムが、特定の自然言語における文字のシーケンスの口頭発話のスペクトログラムを出力として取得するために、文字のシーケンスをシーケンス間(seq2seq)リカレントニューラルネットワーク106に入力として提供することを含む。いくつかの実装形態において、スペクトログラムは、圧縮されたスペクトログラム、たとえば、メルスケールスペクトログラムである。特に、seq2seqリカレントニューラルネットワーク106は、エンコーダプレネットニューラルネットワーク114とエンコーダCBHGニューラルネットワーク116とを含むエンコーダニューラルネットワーク112を使用して、シーケンス内の文字の各々のそれぞれの符号化表現を生成するために、文字のシーケンスを処理する。
At
より具体的には、文字のシーケンス内の各文字は、ワンホットベクトルとして表され、連続ベクトルに埋め込まれ得る。エンコーダプレネットニューラルネットワーク114は、シーケンス内の各文字のそれぞれの埋め込みを受信し、文字の変換された埋め込みを生成するために、シーケンス内の各文字のそれぞれの埋め込みを処理する。たとえば、エンコーダプレネットニューラルネットワーク114は、変換された埋め込みを生成するために、非線形変換のセットを各埋め込みに適用することができる。次いで、エンコーダCBHGニューラルネットワーク116は、エンコーダプレネットニューラルネットワーク114から変換された埋め込みを受信し、文字のシーケンスの符号化表現を生成するために、変換された埋め込みを処理する。 More specifically, each character in a sequence of characters can be represented as a one-hot vector and embedded in a continuous vector. Encoder prenet neural network 114 receives the respective embeddings of each character in the sequence and processes the respective embeddings of each character in the sequence to produce transformed embeddings of the characters. For example, encoder prenet neural network 114 can apply a set of non-linear transforms to each embedding to generate a transformed embedding. Encoder CBHG neural network 116 then receives the transformed embeddings from encoder prenet neural network 114 and processes the transformed embeddings to produce an encoded representation of the sequence of characters.
文字のシーケンスの口頭発話のスペクトログラムを生成するために、seq2seqリカレントニューラルネットワーク106は、注意ベースのデコーダリカレントニューラルネットワーク118を使用して、符号化表現を処理する。特に、注意ベースのデコーダリカレントニューラルネットワーク118は、デコーダ入力のシーケンスを受信する。シーケンス内の第1のデコーダ入力は、所定の初期フレームである。シーケンス内のデコーダ入力ごとに、注意ベースのデコーダリカレントニューラルネットワーク118は、スペクトログラムのrフレームを生成するために、デコーダ入力と符号化表現とを処理し、rは、1よりも大きい整数である。生成されたrフレームのうちの1つまたは複数は、シーケンス内の次のデコーダ入力として使用され得る。言い換えれば、シーケンス内の各々の他のデコーダは、シーケンス内のデコーダ入力に先行するデコーダ入力を処理することによって生成されたrフレームのうちの1つまたは複数である。 The seq2seq recurrent neural network 106 uses an attention-based decoder recurrent neural network 118 to process the encoded representations to generate a spectrogram of the spoken speech of the sequence of characters. In particular, attention-based decoder recurrent neural network 118 receives a sequence of decoder inputs. The first decoder input in the sequence is a given initial frame. For each decoder input in the sequence, attention-based decoder recurrent neural network 118 processes the decoder input and the encoded representation to generate r frames of the spectrogram, where r is an integer greater than one. One or more of the generated r frames may be used as the next decoder input in the sequence. In other words, each other decoder in the sequence is one or more of the r frames produced by processing the decoder inputs that precede the decoder input in the sequence.
したがって、注意ベースのデコーダリカレントニューラルネットワークの出力は、スペクトログラムを形成するフレームの複数のセットを含み、各セットは、rフレームを含む。多くの場合、rフレームのセット間に重複はない。一度にrフレームを生成することによって、注意ベースのデコーダリカレントニューラルネットワークによって実行されるデコーダステップの総数は、r分の1に低減され、したがって、トレーニングおよび推論時間を短縮する。この技法は、注意ベースのデコーダリカレントニューラルネットワークおよび概してシステムの収束速度および学習速度を向上させるのにも役立つ。 Therefore, the output of the attention-based decoder recurrent neural network includes multiple sets of frames forming a spectrogram, each set including r frames. In many cases there is no overlap between sets of r frames. By generating r frames at a time, the total number of decoder steps performed by the attention-based decoder recurrent neural network is reduced by a factor of r, thus reducing training and inference time. This technique also helps improve the convergence and learning speed of attention-based decoder recurrent neural networks and systems in general.
動作306において、方法300は、特定の自然言語における文字のシーケンスの口頭発話のスペクトログラムを使用してスピーチを生成することを含む。いくつかの実装形態において、スペクトログラムが圧縮されたスペクトログラムである場合、システムは、圧縮されたスペクトログラムから波形を生成し、波形を使用してスピーチを生成することができる。
At
動作308において、方法300は、再生のために生成された音声を提供することを含む。たとえば、方法300は、再生のためにネットワークを介してシステムからユーザデバイス(たとえば、オーディオスピーカ)に生成されたスピーチを送信することによって、再生のために生成されたスピーチを提供し得る。
At
図4は、「Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron」、arXiv preprint arXiv:1803.09047、2018年3月24日によって開示されている決定論的参照エンコーダ400を示し、その内容は、参照によりその全体が組み込まれる。いくつかの実装形態において、参照エンコーダ400は、参照オーディオ信号402を受信し、参照オーディオ信号402から固定長の韻律埋め込みPE450(「韻律埋め込み」とも呼ばれる)を生成/予測するように構成される。韻律埋め込みPE450は、音韻情報、ならびに強勢、イントネーション、およびタイミングなどの特有な話者の特徴とは無関係に、参照オーディオ信号402の特性を捕捉し得る。韻律埋め込みPE450は、参照話者とは完全に異なる話者に対して合成スピーチが生成されるが、参照話者の韻律を示す、韻律転送を実行するための入力として使用され得る。
Figure 4 shows a
示されている例において、参照オーディオ信号402は、長さLRと寸法DRとを有するスペクトログラムスライスとして表され得る。参照オーディオ信号402に関連するスペクトログラムスライスは、メルワープされたスペクトルを示し得る。示されている例において、参照エンコーダ400は、各層が2×2ストライドを有する3×3フィルタと、SAMEパディングと、ReLUアクティベーションとを有する6層畳み込み層ネットワーク404を含む。バッチ正規化が、すべての層に適用され、各層におけるフィルタの数は、ダウンサンプリング、32、32、64、128、128の半分の割合で2倍になる。単一の128幅のゲート付き回帰型ユニット(GRU-RNN)層を有するリカレントニューラルネットワーク410は、最後の畳み込み層から出力406を受信し、予測された韻律埋め込みPE450を出力するアクティベーション関数430がその後に続く完全接続層420に適用される128次元出力412を出力する。リカレントニューラルネットワーク410は、他のタイプの双方向リカレントニューラルネットワークを含み得る。
In the example shown, the
参照エンコーダ400におけるアクティベーション関数430(たとえば、ソフトマックスまたはtanh)の選択は、韻律埋め込みPE450内に含まれる情報を制約し、韻律埋め込みPE450の大きさを制御することによって学習を促進するのを助け得る。さらに、参照エンコーダ400に入力される参照オーディオ信号402の長さLRおよび寸法DRの選択は、エンコーダ400によって学習される韻律の様々な側面に影響を与える。たとえば、エンコーダは、エネルギー情報を含まないので、ピッチトラック表現は、いくつかの言語におけるプロミネンスのモデリングを許可しない場合があり、一方、メル周波数ケプストラム係数(MFCC(Mel Frequency Cepstral Coefficient))表現は、少なくともある程度、トレーニングされた係数の数に応じて、エンコーダ400がイントネーションをモデル化することを妨げる場合がある。
Selection of activation function 430 (e.g., softmax or tanh) in
参照エンコーダ400から出力された韻律埋め込みPE450は、合成スピーチを生成するための多数の異なるTTSアーキテクチャにおいて使用され得るが、推論時に韻律埋め込みPE450を生成するためのシード信号(たとえば、参照オーディオ信号402)が必要とされる。たとえば、シード信号は、「このように言って(Say it like this)」参照オーディオ信号402であり得る。代替的には、意図された韻律/スタイルを有する合成スピーチを伝達するために、いくつかのTTSアーキテクチャは、参照エンコーダ400を使用して、シード信号から韻律埋め込みPE450を出力する代わりに、推論時に手動スタイル埋め込み選択を使用するように適合され得る。図5Aおよび図5Bを参照すると、いくつかの実装形態において、テキスト予測システム500、500a～bは、シード信号(たとえば、参照オーディオ信号402)または推論時の手動スタイル埋め込み選択なしで、入力テキスト502からスタイル埋め込みSE550を予測し、予測されたスタイル埋め込みSE550を、入力テキスト502をスタイル埋め込みSE550によって指定されたスタイル/韻律を有する合成スピーチ680(図6Aおよび図6B)に変換するためのエンドツーエンドTTSモデル650に提供するように構成される。すなわち、テキスト予測システム500は、推論時に補助入力に依存することなく、TTSモデル650によって合成された表現力豊かなスピーチ680のための発話スタイルを予測するために、入力テキスト502をコンテキストのソースとして使用する。
The
トレーニング中、図5Aおよび図5Bのテキスト予測システム500は、参照エンコーダ400と、スタイルトークン層510と、テキスト予測モデル520、520a～bと、エンドツーエンドTTSモデル650とを含む。テキスト予測モデル520は、テキスト予測ネットワーク520と呼ばれる場合もある。参照エンコーダ400は、図4を参照して上記で説明されている参照エンコーダ400を含み得る。示されている例において、参照エンコーダ400は、参照オーディオ信号402から韻律埋め込みPE450を出力し、参照オーディオ信号402に関連する韻律および/またはスタイル情報を伝達するスタイル埋め込みSE550を生成するために、韻律埋め込みPE450をスタイルトークン層510に提供するように構成される。デコーダ658から出力される結果として生じる出力オーディオ信号670(図6Aおよび図6B)が参照オーディオ信号402と一致するように、参照オーディオ信号402のトランスクリプトは、TTSモデル650のテキストエンコーダ652に入力される入力テキスト502の文字のシーケンス(「入力テキストシーケンス」とも呼ばれる)と一致する。それに加えて、テキスト予測モデル520は、スタイルトークン層510によって生成されたスタイル埋め込みSE550に関連する組合せ重み(CW)516P(図5A)を予測するため、またはスタイルトークン層510によって生成されたスタイル埋め込みSE550と一致するスタイル埋め込みSE550P(図5B)を直接予測するために、参照オーディオ信号402のトランスクリプトに対応する入力テキスト502の各トレーニングサンプルを受信するために、テキストエンコーダ652も使用する。したがって、トレーニング段階は、テキスト予測モデル520の共同トレーニングを可能にし、入力テキスト502のトレーニングサンプルごとにスタイル埋め込みSE550Pを予測するために、参照オーディオ信号402のトレーニングセット(たとえば、グランドトゥルース)と入力テキスト502の対応するトランスクリプトとを使用し、ターゲットスタイル埋め込みSE550Tによって指定されたスタイル/韻律を有し、参照オーディオ信号402のトレーニングサンプルと一致する出力オーディオ信号670を(デコーダ658を介して)決定するためにTTSモデル650を使用する。
During training, the
いくつかの実装形態において、スタイルトークン層510は、「Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthsis」、arXiv preprint arXiv:1803.09017、2018年3月23日によって開示されているスタイルトークン層を含み、その内容は、参照によりその全体が組み込まれる。スタイルトークン層510は、トレーニング中、参照エンコーダ400から出力された韻律埋め込みPE450を表すトレーニング可能なスタイルトークン514、514a～nの凸結合を教師なし方式で学習するように構成されたスタイル注目モジュール512を含む。ここで、スタイルトークン層510は、韻律埋め込みとランダムに初期化されたスタイルトークン514、514a～nのバンク内の各スタイルトークン514との間の類似性尺度を学習するように構成された注意モジュール512へのクエリベクトルとして、韻律埋め込みPE450を使用する。スタイルトークン514(「スタイル埋め込み」とも呼ばれる)は、すべてのトレーニングシーケンスにわたって共有される対応する埋め込みを含み得る。したがって、注意モジュール512は、符号化された韻律埋め込みPE450に対する各スタイルトークン514の寄与を表す組合せ重み516、516a～nのセットを出力する。注意モジュール512は、ソフトマックスアクティベーションを介してスタイルトークン514を正規化することによって、組合せ重み516を決定し得る。スタイルトークン層510から出力される結果として生じるスタイル埋め込みSE550は、スタイルトークン514の加重和に対応する。各スタイルトークン514は、テキストエンコーダ502の状態の次元と一致する次元を含み得る。例は、5つのスタイルトークン514を含むスタイルトークン層510を示しているが、スタイルトークン層510は、任意の数のスタイルトークン514を含み得る。いくつかの例において、トレーニングデータにおける豊富な種類の韻律次元を提供するために、10個のスタイルトークン514が選択される。
In some implementations, the style token layer 510 is disclosed by "Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthsis," arXiv preprint arXiv:1803.09017, March 23, 2018. style token layer, whose contents are incorporated by reference in their entirety. Style token layer 510, during training, is configured to unsupervisedly learn convex combinations of trainable style tokens 514, 514a-n that represent
いくつかの構成において、スタイルトークン層510は、TTSモデル650およびテキスト予測モデル520と共同でトレーニングされる。他の構成において、スタイルトークン層510およびTTSモデル650は、別々にトレーニングされ、一方、スタイルトークン層510およびテキスト予測モデル520は、共同でトレーニングされる。
In some configurations, style token layer 510 is jointly trained with
図5Aおよび図5Bを引き続き参照すると、テキスト予測ネットワーク520は、各々、入力として、TTSモデル650のテキストエンコーダ652から出力された符号化シーケンス653を受信する。ここで、符号化シーケンス653は、入力テキストシーケンス502の符号化に対応する。いくつかの例において、テキストエンコーダ652は、入力テキストシーケンス502内の局所的およびコンテキスト情報を明示的にモデル化するために、入力テキストシーケンス502を可変長符号化シーケンス653に符号化するためのCBHGニューラルネットワーク200(図2)を含む。書記素から発音を学習するモデルの能力ではなく、韻律が扱われているので、入力テキストシーケンス502は、テキスト正規化フロントエンドおよび語彙集によって生成された音素入力を含み得る。テキスト予測ネットワーク520は、可変長入力(たとえば、符号化シーケンス553)を固定長(たとえば、64次元)出力524に時間集約することによって、128ユニットGRU-RNN410(図4)が参照エンコーダ400のためのサマライザとして機能する方法と同様にテキストエンコーダ502のためのサマライザとして機能する64ユニット時間集約GRU-RNN522などの双方向RNN522を含む。ここで、固定長出力524は、固定長テキスト特徴ベクトル、すなわち、固定長テキスト特徴524に対応する。
With continued reference to FIGS. 5A and 5B,
テキスト予測ネットワーク520a、520bは、入力テキスト502に基づく推論中にスタイル埋め込み550を予測するための2つのテキスト予測経路を提供する。これらのネットワーク520a、520bの各々は、勾配流を停止するように構成された演算子を使用することによって共同でトレーニングされ得る。図5Aを参照すると、テキスト予測モデル520aは、入力テキストシーケンス502から予測された組合せ重み516、516Pを使用することによって、トレーニング中に学習されたスタイルトークン514を予測するための第1のテキスト予測経路を提供する。テキスト予測モデル520aは、テキスト予測組合せ重み(TPCW)モデル520aと呼ばれることもある。モデル520aが教師なしでトレーニングされるトレーニング段階中、モデル520aは、スタイルトークン層510によって決定された組合せ重み516を予測ターゲットとして設定し、次いで、時間集約GRU-RNN522から出力された固定長テキスト特徴524を完全接続層526に供給する。したがって、組合せ重み516、516Tは、ターゲット組合せ重み(CW)516Tと呼ばれることがある。バックプロパゲーションは、スタイル注意モジュール512とスタイルトークン514とを更新することができるので、組合せ重み516Tは、トレーニング段階中に移動するターゲットを形成し得る。いくつかの例において、完全接続層526は、モデル520aが予測された組合せ重み516Pとスタイルトークン層510から出力されたターゲット組合せ重み516Tとの間のクロスエントロピー損失を決定することを可能にするために、予測された組合せ重み516Pに対応するロジットを出力するように構成される。補間により、スタイル埋め込みSE550は、これらの予測された組合せ重み516Pから予測され得る。その後、モデル520aは、スタイルトークン層510を通る任意のテキスト予測エラーのバックプロパゲーションを防止するために、勾配流を停止するように構成され得る。さらに、クロスエントロピー損失は、トレーニング中のTTSモデル650の最終的な損失に追加され得る。
図5Aを引き続き参照すると、推論段階の間、スタイルトークン514は、固定され、テキスト予測モデル520a(TPCWモデル520a)は、入力テキストシーケンス502のみに基づいて組合せ重み516Pを予測するように構成される。ここで、入力テキストシーケンス502は、TTSモデル650が表現力豊かなスピーチに合成することになる現在の入力テキストに対応する。したがって、テキストエンコーダ652は、入力テキストシーケンス502を符号化シーケンス653に符号化し、符号化シーケンス653を、TTSモデル650の連結器654と組合せ重み516Pを予測するためのテキスト予測モデル520aの両方に提供する。ここで、モデル520aは、予測されたスタイル埋め込みSE550Pを決定するために予測された組合せ重み516Pを使用し、予測されたスタイル埋め込みSE550Pを、TTSモデル650の連結器654に提供し得る。いくつかの例において、連結器654は、テキストエンコーダ652から出力された符号化シーケンス653と、予測されたスタイル埋め込みSE550Pとを連結し、連結を、予測されたスタイル埋め込みSEによって指定されたスタイル/韻律を有する合成スピーチ680に変換するためにTTSモデル650のデコーダ658に提供する。
With continued reference to FIG. 5A, during the inference stage, style tokens 514 are fixed and
図5Bを参照すると、テキスト予測モデル520bは、トレーニング中に学習されたスタイルトークン514および組合せ重み516を無視し、入力テキストシーケンス502からスタイル埋め込みSE550を直接予測する。テキスト予測モデル520bは、テキスト予測スタイル埋め込み(TPSE)モデル520bと呼ばれることもある。モデル520bが教師なし方式で(また、図5Aのモデル520aと共同で)トレーニングされるトレーニング段階中、モデル520bは、スタイル埋め込みSE550、550Tを予測ターゲットとして設定し、予測されたスタイル埋め込みSE550、550Pを出力するために、時間集約GRU-RNN522から出力された固定長テキスト特徴524を1つまたは複数の完全接続層527に供給する。いくつかの例において、完全接続層527は、ReLUアクティベーションを使用する1つまたは複数の隠れ完全接続層と、テキスト予測スタイル埋め込みSE550Pを発するためにtanhを使用する出力層とを含む。いくつかの例において、出力層によって適用されるtanhアクティベーションは、テキストエンコーダ652の最終的な双方向GRU-RNN(たとえば、図2のCBHGニューラルネットワーク200の双方向RNN214)のtanhアクティベーションと一致するように選択される。同様に、このtanhアクティベーションは、スタイルトークン層510の注意スタイルモジュール512によって使用されるスタイルトークンtanhアクティベーションと一致し得る。
Referring to FIG. 5B,
いくつかの実装形態において、テキスト予測モデル520は、予測されたスタイル埋め込みSE550Pと、スタイルトークン層510によって出力されたターゲットスタイル埋め込みSE550Tとの間のL1損失を決定する。その後、モデル520bは、スタイルトークン層510を通る任意のテキスト予測エラーのバックプロパゲーションを防止するために、勾配流を停止するように構成され得る。さらに、クロスエントロピー損失は、トレーニング中のTTSモデル650の最終的な損失に追加され得る。
In some implementations,
図5Bを引き続き参照すると、推論段階の間、テキスト予測モデル520b(TPSEモデル520b)は、スタイルトークン層510を無視し、入力テキストシーケンス502のみに基づいてスタイル予測SE550Pを直接予測する。図5AのTPCWモデル520aと同様に、入力テキストシーケンス502は、TTSモデル650が表現力豊かなスピーチに合成することになる現在の入力テキストに対応する。したがって、テキストエンコーダ652は、入力テキストシーケンス502を符号化シーケンス653に符号化し、符号化シーケンス653を、TTSモデル650の連結器654とスタイル埋め込みSE550Pを予測するためのテキスト予測モデル520bの両方に提供する。スタイル埋め込みSE550Pを予測した後、システム520bは、予測されたスタイル埋め込みSE550Pを、TTSモデル650の連結器654に提供する。いくつかの例において、連結器654は、テキストエンコーダ652から出力された符号化シーケンス653と、予測されたスタイル埋め込みSE550Pとを連結し、連結を、予測されたスタイル埋め込みSEによって指定されたスタイル/韻律を有する合成スピーチ680に変換するためにTTSモデル650のデコーダ658に提供する。
With continued reference to FIG. 5B, during the inference stage,
図6Aおよび図6Bは、シード信号(たとえば、参照オーディオ信号402)または推論時の手動スタイル埋め込み選択なしで、入力テキスト502および入力テキスト502に関連する1つまたは複数のコンテキスト特徴602からスタイル埋め込みSE550を予測するように構成されたコンテキスト予測システム600のトレーニング段階(図6A)と推論段階(図6B)とを含む。図5Aおよび図5Bのテキスト予測システム500と同様に、予測スタイル埋め込みSE550は、入力テキスト502をスタイル埋め込みSE550によって指定されたスタイル/韻律を有する出力オーディオ信号670に変換するために、テキスト予測ネットワーク520からエンドツーエンドTTSモデル650に供給される。システム600は、メモリハードウェア920(図9)上に記憶された命令を使用して、データ処理ハードウェア910(図9)上で実行され得る。示されている例において、システム600は、コンテキストモデル610と、参照エンコーダ400と、コンテキストモデル610と通信するテキスト予測ネットワーク520と、テキスト予測モデル520と通信するTTSモデル650とを含む。
6A and 6B illustrate style embeddings S from
一般に、コンテキストモデル610は、現在の入力テキスト502に関連するコンテキスト埋め込み612を生成するために、1つまたは複数のコンテキスト特徴602を受信および処理するように構成される。現在の入力テキスト502は、表現力豊かなスピーチ680に合成されるべき文字のシーケンスを指す。現在の入力テキスト502は、いくつかの例では単一の文であり得、他の例では、現在の入力テキスト502は、段落を含む。現在の入力テキスト502内の文字のシーケンス、および現在の入力テキスト502の結果として生じる合成された表現力豊かなスピーチ680は、特定の言語に関連付けられている。さらに、各コンテキスト特徴602は、現在の入力テキスト502のテキストソース800(図8)から導出され得、それによって、テキストソース800は、表現力豊かなスピーチ680に合成されるべきテキストのシーケンスを含む。
In general, context model 610 is configured to receive and process one or more context features 602 to generate
テキスト予測モデル520は、図5Aおよび図5Bを参照して上記で説明されているテキスト予測モデル520を含み得る。本明細書で使用される場合、「テキスト予測モデル」および「テキスト予測ネットワーク」という用語は、交換可能に使用される。しかしながら、図5Aおよび図5Bとは対照的に、システム600は、現在の入力テキスト502に加えて、現在の入力テキスト502に関連する1つまたは複数のコンテキスト特徴602に基づいてコンテキストモデル610によって生成されたコンテキスト埋め込み612を入力として受信するように、テキスト予測モデル520を修正し得る。その後、コンテキスト予測システム600のテキスト予測モデル520は、出力として、現在の入力テキスト502のためのスタイル埋め込みSE550、550Pを予測するために、現在の入力テキスト502と、現在の入力テキスト502に関連するコンテキスト埋め込み612とを処理するように構成される。図5Aを参照して上記で説明されているように、テキスト予測モデル520は、予測されたスタイル埋め込みSE550Pがスタイルトークン514の加重和に基づいて補間され得るように、スタイルトークン514のセットの寄与を表す組合せ重み516Pを予測するように構成され得る。一方、図5Bを参照して上記で説明されているように、テキスト予測モデル520は、現在の入力テキスト502およびコンテキスト埋め込み612からスタイル埋め込みSE550Pを直接予測するように構成され得る。スタイル埋め込みSE550Pが、補間を介してテキスト予測モデル520によって予測されるか、または直接予測されるかにかかわらず、スタイル埋め込みSE550Pは、シード信号(たとえば、参照オーディオ信号402)または推論時の手動スタイル埋め込み選択を使用せずに予測される。
いくつかの例において、TTSモデル650は、現在の入力テキスト502を(たとえば、テキストソース800から)受信し、テキスト予測モデル520によって予測されたスタイル埋め込みSE550Pを受信し、現在の入力テキスト502の表現力豊かなスピーチの出力オーディオ信号670を生成するために、入力テキスト502とスタイル埋め込みSE550Pとを処理するように構成される。ここで、出力オーディオ信号670は、スタイル埋め込みSE550によって指定された特定の韻律およびスタイルを有する。
In some examples,
TTSモデル650は、エンコーダ652と、連結器654と、注意モジュール656と、デコーダ658と、シンセサイザ675とを含む。いくつかの実装形態において、TTSモデル650は、図1のTTSモデル150を含む。たとえば、エンコーダ652、注意モジュール656、およびデコーダ658は、集合的にseq2seqリカレントニューラルネットワーク106に対応し、シンセサイザ675は、波形シンセサイザ110またはWaveNetニューラルボコーダを含み得る。しかしながら、シンセサイザ675の選択は、合成スピーチ680の結果として生じる韻律および/またはスタイルに影響を与えず、実際には、合成スピーチ680のオーディオ忠実度にのみ影響を与える。注意モジュール656は、長い発話への汎化を改善するために、ガウス混合モデル(GMM(Gaussian Mixture Model))を含み得る。したがって、TTSモデル650のエンコーダ652は、入力テキスト502を連結器654に供給される符号化シーケンス653に符号化するために、CBHGニューラルネットワーク200(図2)を使用し得る。テキスト予測モデル520から出力された予測されたスタイル埋め込みSE550Pも連結器654に供給され、連結器654は、現在の入力テキスト502のそれぞれの符号化シーケンス653とスタイル埋め込みSE550Pとの間の連結655を生成するように構成される。いくつかの例において、連結器654は、ブロードキャスト連結器を含む。いくつかの実装形態において、注意モジュール656は、出力オーディオ信号670、ytを生成するために、デコーダ658の出力ステップごとに、連結655を固定長コンテキストベクトル657に変換するように構成される。
書記素から発音を学習するモデルの能力ではなく、韻律が扱われているので、入力テキスト502は、テキスト正規化フロントエンドおよび語彙集によって生成された音素入力を含み得る。しかしながら、入力テキスト502は、追加的または代替的に、書記素入力を含み得る。注意モジュール656およびデコーダ658は、注意ベースのデコーダリカレントニューラルネットワーク118(図1)を集合的に含み、2に等しい低減係数を使用し、それによって、タイムステップあたり2つのスペクトログラムフレーム(たとえば、出力オーディオ信号670)を生成し得る。いくつかの例において、0.1に等しい確率でゾーンアウトを使用する256セル長短期記憶(LSTM)の2つの層が、デコーダ658のGRUセルを置き換え得る。他の実装形態において、TTSモデル650は、2018年8月8日に出願された米国出願第16/058,640号において開示されている音声合成システムを含み、その内容は、参照によりその全体が組み込まれる。
Since prosody is being addressed rather than the model's ability to learn pronunciation from graphemes,
トレーニング段階の間、図6Aは、参照オーディオ信号402から韻律埋め込みPE450を出力し、参照オーディオ信号402に関連する韻律および/またはスタイル情報を伝達するスタイル埋め込みSE550、550Tを生成するためにスタイルトークン層510に韻律埋め込みPE450を提供するように構成された参照エンコーダ400を含むコンテキスト予測システム600を示す。参照エンコーダ400およびスタイルトークン層510については、図5Aおよび図5Bを参照して上記で説明されている。デコーダ658から出力された結果として生じる出力オーディオ信号670、ytが参照オーディオ信号402と一致するように、参照オーディオ信号402のトランスクリプトは、テキストエンコーダ652に入力される入力テキスト502の文字のシーケンス(「入力テキストシーケンス」とも呼ばれる)と一致する。一例において、参照オーディオ信号402は、テキスト文書(たとえば、テキストソース)を読む話者を含み得、入力テキスト502の対応するトランスクリプトは、話者が読んでいるテキスト文書内のテキスト/文に対応する。
During the training phase, FIG. 6A outputs
コンテキスト特徴602は、現在の入力テキスト502のテキストソース800から導出され、コンテキストモデル610は、コンテキスト特徴602を処理することによって、現在の入力テキスト502に関連するコンテキスト埋め込み612を生成し、コンテキスト埋め込み612をテキスト予測モデル520に供給するように構成される。たとえば、上記の例では、コンテキスト特徴602は、テキスト文書から導出され、限定はしないが、合成されるべき現在の入力テキスト502(Tt)と、現在の入力テキストに先行するテキストソースからの前のテキスト(Tt-1)と、前のテキストから合成された前のスピーチ680(たとえば、前の出力オーディオ信号670(yt-1))と、現在の入力テキストに続くテキストソースからの次のテキスト(Tt+1)と、前のテキストと前のテキストに関連する前のコンテキスト埋め込みとに基づいてテキスト予測ネットワーク520によって予測された前のスタイル埋め込みとを含み得る。それに加えて、テキスト文書から導出された1つまたは複数のコンテキスト特徴602は、テキスト文書のタイトル、テキスト文書内の章のタイトル、テキスト文書内の節のタイトル、テキスト文書内の見出し、テキスト文書内の1つもしくは複数の箇条書き、テキスト文書から抽出された概念グラフからのエンティティ、またはテキスト文書から抽出された1つもしくは複数の構造化された回答表現のうちの少なくとも1つを含み得る。いくつかの例において、テキスト(たとえば、現在の入力テキスト、前のテキスト、次のテキストなど)に関連するコンテキスト特徴602は、限定はしないが、母音レベルの埋め込み、単語レベルの埋め込み、文レベルの埋め込み、段落レベルの埋め込み、および/または単語ごとのスピーチタグ(たとえば、名詞、動詞、形容詞など)を含み得る、テキストから抽出された特徴を含む。
The context features 602 are derived from the text source 800 of the
それに加えて、テキスト予測モデル520は、スタイルトークン層510によって生成されたスタイル埋め込みSE550に関連する組合せ重み(CW)516P(図5A)を予測するため、またはスタイルトークン層510によって生成されたスタイル埋め込みSE550と一致するスタイル埋め込みSE550P(図5B)を直接予測するために、参照オーディオ信号402のトランスクリプトに対応する入力テキスト502の各トレーニングサンプルと、入力テキスト502のトレーニングサンプルごとに生成された対応するコンテキスト埋め込み612とを受信する。したがって、トレーニング段階は、コンテキストモデル610およびテキスト予測モデル520の共同トレーニングを可能にし、入力テキスト502のトレーニングサンプルごとにスタイル埋め込みSE550Pを予測するために、参照オーディオ信号402のトレーニングセット(たとえば、グランドトゥルース)と、入力テキスト502の対応するトランスクリプトと、入力テキスト502のトランスクリプトから導出されたコンテキスト特徴602とを使用し、ターゲットスタイル埋め込みSE550Tによって指定されたスタイル/韻律を有し、参照オーディオ信号402のトレーニングサンプルと一致する出力オーディオ信号670を(デコーダ658を介して)決定するためにTTSモデル650を使用する。しかしながら、いくつかの構成において、トレーニング段階は、代わりに、参照エンコーダ400、スタイルトークン層510、およびTTSモデル650がトレーニング手順の第1のステップの間に事前にトレーニングされてフリーズされ、コンテキストモデル610およびテキスト予測モデル520がトレーニング手順の第2のステップの間に別々にトレーニングされる、2ステップトレーニング手順を含む。
In addition,
図6Bは、現在の入力テキスト502(Tt)および現在の入力テキスト502に関連する1つまたは複数のコンテキスト特徴602からスタイル埋め込みSE550Pを予測するための推論段階の間に、参照エンコーダ400とスタイルトークン層510とを省略したコンテキスト予測システム600を示す。テキスト予測モデル520は、第1のテキスト予測経路(図5A)または第2のテキスト予測経路(図5B)のいずれかを介して、スタイル埋め込みSE550Pを予測し得る。ここで、現在の入力テキスト502は、TTSモデル650が表現力豊かなスピーチに合成することになるテキストソース800(図8)からの現在の入力テキストに対応する。図8は、表現力豊かなスピーチに合成されるべきテキストのシーケンスを含む例示的なテキストソース800を示す。テキストソース800は、例のみのために提供され、表現力豊かなスピーチに合成され得るテキストを含む他のテキストソース800(図示せず)を含み得る。テキストソース800は、テキスト文書、対話トランスクリプト、クエリ応答システム、または仮想環境を含み得る。テキスト文書は、小説/教科書などの長い形式のテキスト文書から、ウェブページまたは会話形文書などの短い形式の文書まで、多種多様な文書を包含し得る。
6B illustrates
テキスト文書について、コンテキスト特徴602は、前のテキスト(たとえば、現在のテキスト502の前のN文)、前のテキストに対応する前のオーディオ670、次のテキスト(たとえば、現在のテキスト502の後のN文)などのモノローグコンテキストを含み得る。たとえば、悲しい出来事について記述する前のテキストは、悲しい感情を示す韻律/スタイルを伝える現在のテキストの表現力豊かなスピーチを合成するためのスタイル埋め込みを予測するのに役立つことができる。コンテキスト特徴602は、タイトル、章のタイトル、節のタイトル、見出し、箇条書きなどの文書構造からも導出され得る。テキスト文書は、コンテキスト特徴602として抽出され得る概念グラフ(たとえば、ウィキペディアエントリ)からのエンティティなどの概念も含み得る。
For text documents, the context features 602 are the previous text (eg, N sentences before the current text 502), the
クエリ応答システム(たとえば、質問および応答)について、コンテキスト特徴602は、音声クエリからのオーディオ/テキスト特徴、または現在のテキスト502が表現力豊かなスピーチに合成されるべき応答のトランスクリプトに対応するテキストクエリからのテキスト特徴を含み得る。コンテキスト特徴602は、現在の応答につながるクエリのシーケンスからのオーディオ/テキスト特徴を含み得る。追加的または代替的に、コンテキスト特徴602は、デジタルアシスタントによって使用される応答の構造化された回答表現から抽出され得る。対話トランスクリプト(発話交替)について、コンテキスト特徴602は、対話内の前の「ターン」の前のテキスト特徴および/または対話内の次の「ターン」の次のテキスト特徴を含み得る。仮想環境に対応するテキストソース800は、仮想環境内に存在する任意のキャラクターおよび/またはオブジェクトに対応するコンテキスト特徴802を提供し得る。
For query response systems (e.g., questions and responses), context features 602 are audio/text features from the spoken query or text corresponding to the transcript of the response to which the
図6Bの推論段階に戻って参照すると、現在の入力テキスト502は、本(たとえば、テキスト文書)などのテキストソース内に含まれるテキスト(たとえば、1つまたは複数の文)であり得、1つまたは複数のコンテキスト特徴602は、テキストソース800から導出される。たとえば、テキスト文書は、電子書籍(eブック)であり得、コンピューティングデバイス900は、eブックを表現力豊かなスピーチ680に合成する電子リーダソフトウェアを実行し得る。したがって、電子リーダソフトウェアを実行するコンピューティングデバイス900は、(たとえば、韻律/スタイルを制御/選択するいかなる補助入力も使用せずに)入力テキスト502およびコンテキスト特徴602のみに基づいて自然に聞こえる韻律/スタイルを有する表現力豊かなスピーチ680を合成するために、コンテキスト予測システム600を実行し得る。別の例において、テキストソース800が対話トランスクリプトを含む場合、合成されるべき現在の入力テキスト502は、対話トランスクリプト内の現在のターンに対応する。この例において、コンテキスト特徴602は、対話トランスクリプト内の前のターンに対応する対話トランスクリプト内の前のテキスト、および/または対話トランスクリプト内の次のターンに対応する対話トランスクリプト内の次のテキストを含み得る。さらに別の例において、テキストソース800が、ユーザがテキストまたは音声クエリをコンピューティングデバイス900(図9)に入力することを可能にするクエリ応答システム(たとえば、デジタルアシスタントなど)を含み、検索エンジン(リモートまたはユーザデバイス上)が、コンピューティングデバイスからの可聴出力のために表現力豊かなスピーチ680に合成されるべき応答をフェッチする場合、現在の入力テキストは、現在のクエリに対する応答に対応し、コンテキスト特徴は、現在のクエリに関連する少なくとも1つのテキスト、および現在のクエリに関連するテキストもしくはクエリ応答システムにおいて受信されたクエリのシーケンスに関連するテキスト、または現在のクエリに関連するオーディオ特徴もしくはクエリ応答システムにおいて受信されたクエリのシーケンスに関連するオーディオ特徴のうちの少なくとも1つを含む。これらのコンテキスト特徴602は、現在の入力テキスト502から合成された表現力豊かなスピーチ680の自然なスタイル/韻律を最もよく伝えるスタイル埋め込みSE550をより正確に予測するための追加のコンテキストを提供するために、テキストソース800から容易に導出され得る。
Referring back to the inference stage of FIG. 6B,
図7A～図7Dは、複数の時間ステップにわたって表現力豊かなスピーチを合成するための図6Aおよび図6Bのコンテキスト予測システム600を実装する例示的なコンテキストTTSネットワーク700a～dを示す。TTSネットワーク700a～dは、スタイル埋め込みSE550を予測するために、コンテキスト特徴602と入力テキスト502の両方を利用するが、TTSネットワーク700a～dは、図5Aおよび図5Bのテキスト予測システム500を参照して上記で説明されているように、入力テキスト502のみを使用してスタイル埋め込みSE550を予測するように変更され得る。簡略化のため、コンテキストTTSネットワーク700a～dは、TTSモデル650と、図6Aおよび図6Bを参照して説明されているコンテキストモデル610とテキスト予測モデル520とを集合的に含むコンテキストモジュール710を含む。現在の入力テキストのみが使用される(たとえば、テキスト予測システム500を実装する)構成において、コンテキストモジュール710は、現在の入力テキストがコンテキストモジュール710への唯一のコンテキストモジュール入力であるテキスト予測モデル520を単に含み得る。本明細書で使用される場合、「T」は、テキスト入力502を示し、「t」は、時間ステップを示すインデックスを示し、「x」は、コンテキストモジュール入力を示し、「y」は、TTSモデル650から出力される出力オーディオ信号670を示し、「SE」は、スタイル埋め込み550を示す。
Figures 7A-7D show example
図7Aは、オーディオ再構成エラーを最小化するために単一のモデルをエンドツーエンドでトレーニングし、各時間ステップにおいて、すべての以前のコンテキストモジュール入力(xt-1、xt、xt+1)にわたる注意を使用してコンテキストモジュール710のそれぞれのコンテキスト状態(st-2、st-1、st、st+1)を計算することができる完全コンテキストTTSネットワーク700aの概略図を示す。各時間ステップ(t-1、t、t+1)の間、コンテキストモジュール710は、前の時間ステップにおいてコンテキストモジュール710から出力されたコンテキスト状態(st-2、st-1、st、st+1)と、現在のテキスト入力Tt、前のテキスト入力Tt-1、および前の出力オーディオ信号yt-1の任意の組合せを含むコンテキストモジュール入力(xt-1、xt、xt+1)とを受信する。ここで、前の出力オーディオ信号は、前の時間ステップt-1の前の入力テキストTt-1に対してTTSモデル650から出力された出力オーディオ信号に対応する。各時間ステップ(たとえば、現在の時間ステップ「t」)の間、コンテキストモジュール710は、コンテキスト状態(st-1)と現在のコンテキストモジュール入力(xt)とを処理することによって、対応するコンテキスト出力(ct)を計算する。いくつかの例において、コンテキストモジュール入力xtは、他の前述の入力の任意の組合せありまたはなしで、後続の時間ステップt+1の間にTTSモデル650によって合成されるべき次のテキストTt+1も含み得る。このオプションは、eブック内のテキストのスピーチを合成するためのコンピューティングデバイス上で実行される電子リーダなどの長い形式のアプリケーションに特に有益であり得る。いくつかの実装形態において、TTSモデル650が会話スピーチ合成用である場合、ネットワーク700aは、完全報酬関数を用いて実際の環境内の再構成損失(RL)を使用してトレーニングされる。これらの実装形態において、コンテキストモジュール入力xtは、会話スピーチ合成に関連する1つまたは複数の環境入力Etをさらに含み得る。
Figure 7A trains a single model end-to-end to minimize the audio reconstruction error, and at each time step, all previous context module inputs (x t-1 , x t , x t+ 1 ) a schematic diagram of a full-
図7Bは、図7Aのネットワーク700aにおけるように、以前のすべてのコンテキストモジュール入力にわたってコンテキスト状態を計算しない1ステップコンテキストTTSネットワーク700bの概略図である。代わりに、各時間ステップ(たとえば、現在の時間ステップ「t」)の間、コンテキストモジュール710は、現在のテキスト入力Tt、前のテキスト入力Tt-1、前の出力オーディオ信号yt-1の任意の組合せを含むコンテキストモジュール入力(xt-1, xt, xt+1)のみを受信し、現在のコンテキストモジュール入力(xt)を処理することによって、対応するコンテキスト出力(ct)を計算する。コンテキストモジュール入力xtは、会話スピーチ合成に関連する1つまたは複数の環境入力Etをさらに含み得る。図7Aの完全コンテキストTTSネットワーク700aと同様に、1ステップコンテキストTTSネットワーク700bは、単一のモデルをエンドツーエンドでトレーニングするが、以前のすべてのコンテキストモジュール入力にわたる注意を使用するコンテキスト状態が計算されないので、長期コンテキストを追跡することができない。いくつかの例において、ネットワーク700bは、トレーニング効率を高めるために、切り捨てられたマルコフ(1ステップ)状態においてトレーニングする。
FIG. 7B is a schematic diagram of a one-step
図7Cは、単一のモデルをエンドツーエンドでトレーニングするのではなく、コンテキストモジュール710およびTTSモデル650が別々にトレーニングされる、分離完全コンテキストTTSネットワーク700cの概略図を示す。すなわち、ネットワーク700cは、2ステップトレーニング手順を使用してトレーニングされる。たとえば、TTSモデル650は、時間ステップ(t)ごとに参照オーディオ信号yref(t)に基づいてターゲットスタイル埋め込みSE(t)を生成するように構成されたスタイルエンコーダ750と関連して、トレーニング手順の第1のステップ中に予めトレーニングされる。いくつかの例において、スタイルエンコーダ750は、図5Aおよび図5Bの韻律エンコーダ400とスタイルトークン層410とを集合的に含む。次いで、TTSモデル650は、出力オーディオ信号ytを生成するために、入力テキストTtとターゲットスタイル埋め込みSEとを受信および処理する。ここで、現在の時間ステップtについて、出力オーディオ信号ytは、参照オーディオ信号yref(t)と一致し、入力テキストTtは、参照オーディオ信号yref(t)のトランスクリプトに対応する。
FIG. 7C shows a schematic diagram of a decoupled full-
2ステップトレーニング手順の第2のステップの間、分離コンテキストモジュール710は、対応するスタイル埋め込みSE(t)を予測するために、タイムステップ(t)ごとに、事前トレーニングされたスタイルエンコーダ750によって生成されたターゲットスタイル埋め込みSE(t)を使用する。図7Aの完全コンテキストTTSネットワーク700aと同様に、分離完全コンテキストTTSネットワーク700bも、各時間ステップにおいて、以前のすべてのコンテキストモジュール入力(xt-1、xt、xt+1)にわたる注意を使用してコンテキストモジュール710のそれぞれのコンテキスト状態(st-2、st-1、st、st+1)を計算することができる。しかしながら、コンテキストモジュール710は、分離されているので、各時間ステップにおけるコンテキストモジュール入力(xt-1、xt、xt+1)は、前の時間ステップt-1の前の入力テキストTt-1に対してTTSモデル650から出力された前の出力オーディオ信号を含まない。代わりに、各時間ステップにおけるコンテキストモジュール入力は、現在の入力テキストTt、前のスタイル埋め込みSE(t-1)、および後続の時間ステップt+1中にTTSモデル650によって合成されるべき次のテキストTt+1の任意の組合せを含む。ここで、前のスタイル埋め込みSE(t-1)は、前の時間ステップt-1の前のコンテキストモジュール入力xt-1に対してコンテキストモジュール710から出力されたスタイル埋め込みに対応する。
During the second step of the two-step training procedure, the
図7Dは、図7Cのネットワーク700cにおけるように、以前のすべてのコンテキストモジュール入力にわたってコンテキスト状態を計算しない分離1ステップコンテキストTTSネットワーク700dの概略図である。代わりに、各時間ステップ(たとえば、現在の時間ステップ「t」)の間、コンテキストモジュール710は、現在の入力テキストTt、前のスタイル埋め込みSE(t-1)、および次のテキストTt+1の任意の組合せを含むコンテキストモジュール入力(xt-1、xt、xt+1)のみを受信し、次いで、現在のコンテキストモジュール入力(xt)を処理することによって、対応する現在のスタイル埋め込みSE(t)を計算/予測する。コンテキストモジュール入力xtは、会話スピーチ合成に関連する1つまたは複数の環境入力Etをさらに含み得る。図7Dの分離完全コンテキストTTSネットワーク700cと同様に、分離1ステップコンテキストTTSネットワーク700dは、スタイルエンコーダ750およびTTSモデル650がコンテキストモジュール710から分離され、別々に事前にトレーニングされる2ステップトレーニング手順を使用してトレーニングされるが、以前のすべてのコンテキストモジュール入力にわたる注意を使用したコンテキスト状態が計算されないので、長期コンテキストを追跡することができない。
FIG. 7D is a schematic diagram of a decoupled one-step
コンテキストモジュール710をTTSモデル650から分離することによって、ネットワーク700c、700dは、各々、長期コンテキストを追跡する能力がネットワーク700cにおいてのみ利用可能である良好なトレーニング効率を提供する。それに加えて、TTSモデル650を分離することは、(図5A～図6Bにおいて説明されているような)コンテキストモードと、スタイル埋め込み空間が制御インターフェースとして機能する韻律/スタイル転送(たとえば、「このように言って」)の両方に対してTTSモデル650を使用することを可能にする。すなわち、スタイル埋め込みが入力テキストのみ(図5Aおよび図5Bのテキスト予測システム500)または入力テキストとコンテキスト特徴との組合せ(図6Aおよび図6Bのコンテキスト予測システム600)から(参照オーディオ信号または手動スタイル埋め込み選択を使用することなく)生成されるコンテキストモードと、参照オーディオ信号(たとえば、このように言って)もしくは手動スタイル埋め込み選択が韻律スタイルをある話者から別の話者に転送するための推論において提供される韻律転送の両方において使用するために、単一のTTSモデル650がトレーニングされ得る。
By separating the
ソフトウェアアプリケーション(すなわち、ソフトウェアリソース)は、コンピュータデバイスにタスクを実行させるコンピュータソフトウェアを指す場合がある。いくつかの例では、ソフトウェアアプリケーションは、「アプリケーション」、「アプリ」、または「プログラム」と呼ばれる場合がある。例示的なアプリケーションは、限定はしないが、システム診断アプリケーション、システム管理アプリケーション、システムメンテナンスアプリケーション、ワード処理アプリケーション、スプレッドシートアプリケーション、メッセージングアプリケーション、メディアストリーミングアプリケーション、ソーシャルネットワーキングアプリケーション、およびゲームアプリケーションを含む。 A software application (ie, software resource) may refer to computer software that causes a computing device to perform a task. In some examples, software applications may be referred to as "applications," "apps," or "programs." Exemplary applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
非一時的メモリは、コンピューティングデバイスによって使用するために、プログラム(たとえば、命令のシーケンス)またはデータ(たとえば、プログラム状態情報)を一時的または永続的に記憶するために使用される物理デバイスであり得る。非一時的メモリは、揮発性および/または不揮発性のアドレス可能な半導体メモリであり得る。不揮発性メモリの例は、限定はしないが、フラッシュメモリおよび読み取り専用メモリ(ROM)/プログラマブル読み取り専用メモリ(PROM)/消去可能なプログラマブル読み取り専用メモリ(EPROM)/電気的消去可能なプログラマブル読み取り専用メモリ(EEPROM)(たとえば、典型的には、ブートプログラムなどのファームウェアのために使用される)を含む。揮発性メモリの例は、限定はしないが、ランダムアクセスメモリ(RAM)、ダイナミックランダムアクセスメモリ(DRAM)、スタティックランダムアクセスメモリ(SRAM)、相変化メモリ(PCM)、ならびにディスクまたはテープを含む。 Non-transitory memory is a physical device used to temporarily or permanently store programs (e.g. sequences of instructions) or data (e.g. program state information) for use by a computing device. obtain. Non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory. (EEPROM) (for example, typically used for firmware such as boot programs). Examples of volatile memory include, without limitation, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM), and disk or tape.
図9は、この文書で説明されているシステムおよび方法を実装するために使用され得る例示的なコンピューティングデバイス900の概略図である。コンピューティングデバイス900は、ラップトップ、デスクトップ、ワークステーション、携帯情報端末、サーバ、ブレードサーバ、メインフレーム、および他の適切なコンピュータなどの様々な形態のデジタルコンピュータを表すことを意図している。ここに示されている構成要素、それらの接続および関係、ならびにそれらの機能は、例示のみであることを意図しており、この文書において説明および/または特許請求されている本発明の実装形態を限定することを意図していない。 FIG. 9 is a schematic diagram of an exemplary computing device 900 that can be used to implement the systems and methods described in this document. Computing device 900 is intended to represent various forms of digital computers such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other suitable computers. The components, their connections and relationships, and their functions shown herein are intended to be exemplary only, and to implement the invention described and/or claimed in this document. Not intended to be limiting.
コンピューティングデバイス900は、データ処理ハードウェア(たとえば、プロセッサ)910と、メモリ920と、記憶デバイス930と、メモリ920および高速拡張ポート950に接続する高速インターフェース/コントローラ940と、低速バス970および記憶デバイス930に接続する低速インターフェース/コントローラ960とを含む。コンピューティングデバイス900は、(データ処理ハードウェア910上での実行を介して)テキスト読み上げ変換システム100と、TTSモデル150、650と、参照エンコーダ400と、決定論的参照エンコーダ400と、コンテキストモデル610と、テキスト予測モデル520とを提供し得る。構成要素910、920、930、940、950、および960の各々は、様々なバスを使用して相互接続されており、共通のマザーボード上に、または必要に応じて他の方法で取り付けられ得る。プロセッサ910は、高速インターフェース940に結合されたディスプレイ980などの外部入力/出力デバイス上にグラフィカルユーザインターフェース(GUI)のためのグラフィカル情報を表示するために、メモリ920内または記憶デバイス930上に記憶された命令を含む、コンピューティングデバイス900内で実行するための命令を処理することができる。他の実装形態において、複数のメモリおよびメモリのタイプとともに、必要に応じて、複数のプロセッサおよび/または複数のバスが使用され得る。また、(たとえば、サーババンク、ブレードサーバのグループ、またはマルチプロセッサシステムとして)複数のコンピューティングデバイス900が接続され、各デバイスが必要な動作の一部を提供し得る。
Computing device 900 includes data processing hardware (e.g., processor) 910,
メモリ920は、コンピューティングデバイス900内に情報を非一時的に記憶する。メモリ920は、コンピュータ可読媒体、揮発性メモリユニット、または不揮発性メモリユニットであり得る。非一時的メモリ920は、コンピューティングデバイス900によって使用するために、プログラム(たとえば、命令のシーケンス)またはデータ(たとえば、プログラム状態情報)を一時的または永続的に記憶するために使用される物理デバイスであり得る。不揮発性メモリの例は、限定はしないが、フラッシュメモリおよび読み取り専用メモリ(ROM)/プログラマブル読み取り専用メモリ(PROM)/消去可能なプログラマブル読み取り専用メモリ(EPROM)/電気的消去可能なプログラマブル読み取り専用メモリ(EEPROM)(たとえば、典型的には、ブートプログラムなどのファームウェアのために使用される)を含む。揮発性メモリの例は、限定はしないが、ランダムアクセスメモリ(RAM)、ダイナミックランダムアクセスメモリ(DRAM)、スタティックランダムアクセスメモリ(SRAM)、相変化メモリ(PCM)、ならびにディスクまたはテープを含む。
記憶デバイス930は、コンピューティングデバイス900のための大容量ストレージを提供することができる。いくつかの実装形態において、記憶デバイス930は、コンピュータ可読媒体である。様々な異なる実装形態において、記憶デバイス930は、フロッピーディスクデバイス、ハードディスクデバイス、光ディスクデバイス、もしくはテープデバイス、フラッシュメモリもしくは他の同様のソリッドステートメモリデバイス、またはストレージエリアネットワークもしくは他の構成におけるデバイスを含むデバイスのアレイであり得る。追加の実装形態において、コンピュータプログラム製品は、情報キャリア内に実態的に具体化される。コンピュータプログラム製品は、実行されると、上記で説明されている方法などの1つまたは複数の方法を実行する命令を含む。情報キャリアは、メモリ920、記憶デバイス930、またはプロセッサ910上のメモリなどの、コンピュータ可読媒体または機械可読媒体である。
高速コントローラ940は、コンピューティングデバイス900のための帯域幅を消費する動作を管理し、低速コントローラ960は、より帯域幅を消費しない動作を管理する。そのような役割の割り当ては、単なる例示である。いくつかの実装形態において、高速コントローラ940は、メモリ920、(たとえば、グラフィックスプロセッサまたはアクセラレータを介して)ディスプレイ980、および様々な拡張カード(図示せず)を受け入れ得る高速拡張ポート950に結合される。いくつかの実装形態において、低速コントローラ960は、記憶デバイス930および低速拡張ポート990に結合される。様々な通信ポート(たとえば、USB、Bluetooth、イーサネット、ワイヤレスイーサネット)を含み得る低速拡張ポート990は、キーボード、ポインティングデバイス、スキャナ、または、たとえば、ネットワークアダプタを介してスイッチもしくはルータなどのネットワーキングデバイスなどの、1つまたは複数の入力/出力デバイスに結合され得る。
The
コンピューティングデバイス900は、図に示されるように、いくつかの異なる形態において実装され得る。たとえば、コンピューティングデバイス900は、標準的なサーバ900aとして、もしくはサーバ900aのグループ内で複数回、ラップトップコンピュータ900bとして、またはラックサーバシステム900cの一部として実装され得る。
Computing device 900 may be implemented in a number of different forms, as shown. For example, the computing device 900 may be implemented as a
図10は、入力テキスト502から表現力豊かな合成スピーチ680のための出力信号670を生成する方法1000のための動作の例示的な配置のフローチャートを示す。方法は、図5A～図6Bを参照して説明され得る。データ処理ハードウェア910(図9)は、方法1000のための動作の例示的な配置を実行するために、メモリハードウェア920上に記憶された命令を実行し得る。動作1002において、方法1000は、データ処理ハードウェア910において、テキストソース800から現在の入力テキスト502を受信することを含む。ここで、現在の入力テキスト502は、テキスト読み上げ(TTS)モデル650によって表現力豊かなスピーチ680に合成されるべきである。
FIG. 10 shows a flowchart of an exemplary arrangement of operations for a
動作1004において、方法1000は、データ処理ハードウェア910によって、コンテキストモデル610を使用して、テキストソース800から導出された1つまたは複数のコンテキスト特徴602を処理することによって、現在の入力テキスト502に関連するコンテキスト埋め込み612を生成することを含む。動作1006において、方法1000は、データ処理ハードウェア910によって、テキスト予測ネットワーク(「テキスト予測モデル」とも呼ばれる)520を使用して、現在の入力テキスト502と、現在の入力テキスト502に関連するコンテキスト埋め込み612とを処理することによって、現在の入力テキスト502に対してスタイル埋め込み550を予測することを含む。特に、テキスト予測ネットワーク520によって予測されたスタイル埋め込み550は、現在の入力テキスト502を表現力豊かなスピーチ680に合成するための特定の韻律および/またはスタイルを指定する。スタイル埋め込み550は、図5Aのテキスト予測ネットワーク520aまたは図5Bのテキスト予測ネットワーク520bのいずれか1つによって予測され得る。
At
動作1008において、方法1000は、データ処理ハードウェア910によって、TTSモデル650を使用して、スタイル埋め込み550と現在の入力テキスト502とを処理することによって、現在の入力テキスト502の表現力豊かなスピーチ680の出力オーディオ信号670を生成することを含む。ここで、出力オーディオ信号670は、スタイル埋め込み550によって指定された特定の韻律および/またはスタイルを有する。上記で説明されているように、TTSモデル650(またはモデル650の下流の他のシステム)は、結果として生じる表現力豊かなスピーチ680を合成するためにシンセサイザ675を使用する。したがって、表現力豊かなスピーチ680は、合成スピーチを指す。
At
本明細書で説明されているシステムおよび技法の様々な実装形態は、デジタル電子回路および/もしくは光回路、集積回路、特別に設計されたASIC(特定用途向け集積回路)、コンピュータハードウェア、ファームウェア、ソフトウェア、ならびに/またはそれらの組合せにおいて実現することができる。これらの様々な実装形態は、記憶装置、少なくとも1つの入力デバイス、および少なくとも1つの出力デバイスからデータおよび命令を受信し、記憶装置、少なくとも1つの入力デバイス、および少なくとも1つの出力デバイスにデータおよび命令を送信するように結合された、専用または汎用であり得る少なくとも1つのプログラマブルプロセッサを含むプログラマブルシステム上で実行可能および/または解釈可能な1つまたは複数のコンピュータプログラムにおける実装形態を含むことができる。 Various implementations of the systems and techniques described herein may include digital electronic and/or optical circuits, integrated circuits, specially designed ASICs (Application Specific Integrated Circuits), computer hardware, firmware, can be implemented in software, and/or combinations thereof. These various implementations receive data and instructions from a storage device, at least one input device, and at least one output device, and send data and instructions to a storage device, at least one input device, and at least one output device. can include implementation in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor, which may be special purpose or general purpose, coupled to transmit the
これらのコンピュータプログラム(プログラム、ソフトウェア、ソフトウェアアプリケーション、またはコードとしても知られる)は、プログラマブルプロセッサのための機械命令を含み、高水準手続き型および/もしくはオブジェクト指向プログラミング言語、ならびに/またはアセンブリ/機械語において実装することができる。本明細書で使用される場合、「機械可読媒体」および「コンピュータ可読媒体」という用語は、機械命令を機械可読信号として受信する機械可読媒体を含む、プログラマブルプロセッサに機械命令および/またはデータを提供するために使用される任意のコンピュータプログラム製品、非一時的コンピュータ可読媒体、装置、および/またはデバイス(たとえば、磁気ディスク、光ディスク、メモリ、プログラマブルロジックデバイス(PLD(Programmable Logic Device)))を指す。「機械可読信号」という用語は、機械命令および/またはデータをプログラマブルプロセッサに提供するために使用される任意の信号を指す。 These computer programs (also known as programs, software, software applications, or code) contain machine instructions for programmable processors and are written in high-level procedural and/or object-oriented programming languages and/or assembly/machine language. can be implemented in As used herein, the terms "machine-readable medium" and "computer-readable medium" provide machine instructions and/or data to a programmable processor, including machine-readable media that receive machine instructions as machine-readable signals. refers to any computer program product, non-transitory computer-readable medium, apparatus, and/or device (e.g., magnetic disk, optical disk, memory, Programmable Logic Device (PLD)) used to The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
本明細書で説明されているプロセスおよび論理フローは、入力データに対して動作し、出力を生成することによって機能を実行するために1つまたは複数のコンピュータプログラムを実行する、データ処理ハードウェアとも呼ばれる1つまたは複数のプログラマブルプロセッサによって実行され得る。プロセスおよび論理フローは、専用論理回路、たとえば、FPGA(フィールドプログラマブルゲートアレイ)またはASIC(特定用途向け集積回路)によっても実行され得る。コンピュータプログラムの実行に適したプロセッサは、例として、汎用および専用マイクロプロセッサの両方、ならびに任意の種類のデジタルコンピュータの任意の1つまたは複数のプロセッサを含む。一般に、プロセッサは、読み取り専用メモリ、またはランダムアクセスメモリ、またはその両方から命令とデータとを受信する。コンピュータの本質的な要素は、命令を実行するためのプロセッサと、命令とデータとを記憶するための1つまたは複数のメモリである。一般に、コンピュータは、データを記憶するための1つまたは複数の大容量記憶デバイス、たとえば、磁気ディスク、光磁気ディスク、または光ディスクも含むか、またはそれらからデータを受信、もしくはそれらにデータを送信、もしくはその両方を行うように動作可能に結合される。しかしながら、コンピュータは、そのようなデバイスを有する必要はない。コンピュータプログラム命令とデータとを記憶するのに適したコンピュータ可読媒体は、例として、半導体メモリデバイス、たとえば、EPROM、EEPROM、およびフラッシュメモリデバイス、磁気ディスク、たとえば、内部ハードディスクまたはリムーバブルディスク、光磁気ディスク、ならびにCD ROMおよびDVD-ROMディスクを含む、すべての形態の不揮発性メモリ、媒体、およびメモリデバイスを含む。プロセッサおよびメモリは、専用論理回路によって補完され得、または専用論理回路の中に組み込まれ得る。 The processes and logic flows described herein are also data processing hardware that executes one or more computer programs to perform functions by operating on input data and generating output. may be executed by one or more programmable processors called The processes and logic flows may also be performed by dedicated logic circuits, such as FPGAs (Field Programmable Gate Arrays) or ASICs (Application Specific Integrated Circuits). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor receives instructions and data from read-only memory, random-access memory, or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally, a computer also includes, receives data from, or sends data to, one or more mass storage devices, such as magnetic, magneto-optical, or optical disks, for storing data; or operably coupled to do both. However, a computer need not have such devices. Computer-readable media suitable for storing computer program instructions and data include, by way of example, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices, magnetic disks such as internal hard disks or removable disks, magneto-optical disks, , and all forms of non-volatile memory, media, and memory devices, including CD ROM and DVD-ROM discs. The processor and memory may be supplemented by, or embodied within, dedicated logic circuitry.
ユーザとの対話を提供するために、本開示の1つまたは複数の態様は、ユーザに情報を表示するための表示デバイス、たとえば、CRT(陰極線管)、LCD(液晶ディスプレイ)モニタ、またはタッチスクリーンと、オプションで、ユーザがコンピュータに入力を提供することができるキーボードおよびポインティングデバイス、たとえば、マウスまたはトラックボールとを有するコンピュータ上に実装され得る。他の種類のデバイスも同様にユーザとの対話を提供するために使用され得、ユーザに提供されるフィードバックは、任意の形態の感覚的フィードバック、たとえば、視覚的フィードバック、聴覚的フィードバック、または触覚的フィードバックであり得、ユーザからの入力は、音響的入力、音声入力、または触覚的入力を含む任意の形態で受信され得る。それに加えて、コンピュータは、ユーザによって使用されるデバイスに文書を送信し、デバイスから文書を受信することによって、たとえば、ユーザのクライアントデバイス上のウェブブラウザから受信された要求に応答して、ウェブブラウザにウェブページを送信することによって、ユーザと対話することができる。 To provide interaction with a user, one or more aspects of the present disclosure include a display device, such as a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen, for displaying information to a user. and optionally on a computer having a keyboard and pointing device, such as a mouse or trackball, that allows a user to provide input to the computer. Other types of devices may be used to provide interaction with the user as well, and the feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback. Feedback may be input from the user, which may be received in any form including acoustic, speech, or tactile input. In addition, the computer can send documents to and receive documents from the device used by the user, e.g., in response to requests received from the web browser on the user's client device. You can interact with users by sending web pages to
いくつかの実装形態について説明されてきた。それにもかかわらず、本開示の要旨および範囲から逸脱することなく、様々な変更が行われ得ることが理解されよう。したがって、他の実装形態は、以下の特許請求の範囲内にある。 A number of implementations have been described. Nevertheless, it will be understood that various changes can be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
100 テキスト読み上げ(TTS)変換システム
102 サブシステム
104 入力テキスト
106 シーケンス間リカレントニューラルネットワーク、seq2seqネットワーク
108 後処理ニューラルネットワーク
110 波形シンセサイザ
112 エンコーダニューラルネットワーク
114 エンコーダプレネットニューラルネットワーク
116 エンコーダCBHGニューラルネットワーク
118 注意ベースのデコーダリカレントニューラルネットワーク、デコーダニューラルネットワーク
120 スピーチ
150 エンドツーエンドテキスト読み上げモデル、TTSモデル
200 CBHGニューラルネットワーク
202 入力シーケンス
204 1次元畳み込みフィルタのバンク
206 ストライド1の時間層に沿った最大プーリング
208 1次元畳み込みサブネットワーク、サブネットワーク
210 残差接続
212 ハイウェイネットワーク
214 双方向リカレントニューラルネットワーク、双方向RNN
400 決定論的参照エンコーダ、参照エンコーダ
402 参照オーディオ信号
404 6層畳み込み層ネットワーク
405 韻律埋め込みPE
406 出力
410 リカレントニューラルネットワーク、128ユニットGRU-RNN
412 128次元出力
420 完全接続層
430 アクティベーション関数
450 固定長の韻律埋め込みPE、韻律埋め込みPE
500 テキスト予測システム
500a テキスト予測システム
500b テキスト予測システム
502 入力テキスト、テキストエンコーダ、入力テキストシーケンス
510 スタイルトークン層
512 注意モジュール、スタイル注意モジュール、注意スタイルモジュール
514 スタイルトークン
514a～n スタイルトークン
516 組合せ重み
516a～n 組合せ重み
516P 組合せ重み(CW)、予測された組合せ重み
516T 組合せ重み、ターゲット組合せ重み(CW)
520 テキスト予測モデル、テキスト予測ネットワーク、ネットワーク
520a テキスト予測モデル、テキスト予測ネットワーク、ネットワーク、テキスト予測組合せ重み(TPCW)モデル、モデル、TPCWモデル
520b テキスト予測モデル、テキスト予測ネットワーク、テキスト予測スタイル埋め込み(TPSE)モデル、TPSEモデル
522 64ユニット時間集約GRU-RNN522、双方向RNN522、時間集約GRU-RNN
524 固定長出力、固定長テキスト特徴
527 完全接続層
550 スタイル埋め込みSE、スタイル埋め込み
550P スタイル埋め込みSE、テキスト予測スタイル埋め込みSE
550T ターゲットスタイル埋め込みSE
553 符号化シーケンス
600 コンテキスト予測システム、システム
602 コンテキスト特徴
610 コンテキストモデル
612 コンテキスト埋め込み
650 エンドツーエンドTTSモデル、TTSモデル
652 テキストエンコーダ、エンコーダ
653 符号化シーケンス
654 連結器
655 連結
656 注意モジュール
657 固定長コンテキストベクトル
658 デコーダ
670 出力オーディオ信号
675 シンセサイザ
680 表現力豊かなスピーチ、合成スピーチ
700a～d コンテキストTTSネットワーク、TTSネットワーク
700a 完全コンテキストTTSネットワーク、ネットワーク
700b 1ステップコンテキストTTSネットワーク、ネットワーク
700c 分離完全コンテキストTTSネットワーク
700d 分離1ステップコンテキストTTSネットワーク
710 コンテキストモジュール、分離コンテキストモジュール
750 スタイルエンコーダ
800 テキストソース
802 コンテキスト特徴
900 コンピューティングデバイス
900a 標準的なサーバ
900b ラップトップコンピュータ
900c ラックサーバシステム
910 データ処理ハードウェア、構成要素
920 メモリハードウェア、メモリ、構成要素
930 記憶デバイス、構成要素
940 高速インターフェース/コントローラ、構成要素
950 高速拡張ポート、構成要素
960 低速インターフェース/コントローラ、構成要素
970 低速バス
980 ディスプレイ
100 text-to-speech (TTS) conversion system
102 subsystems
104 input text
106 inter-sequence recurrent neural network, seq2seq network
108 Postprocessing Neural Networks
110 Waveform Synthesizer
112 Encoder Neural Network
114 Encoder Prenet Neural Network
116 Encoder CBHG Neural Network
118 Attention-Based Decoder Recurrent Neural Networks, Decoder Neural Networks
120 speech
150 end-to-end text-to-speech model, TTS model
200 CBHG neural network
202 Input sequence
Bank of 204 one-dimensional convolution filters
206 max pooling along time strata with
208 1D Convolutional Subnetwork, Subnetwork
210 residual connection
212 Highway Network
214 Bidirectional Recurrent Neural Networks, Bidirectional RNN
400 Deterministic Reference Encoder, Reference Encoder
402 reference audio signal
404 6-layer convolutional layer network
405 Prosody Embedding P E
406 output
410 recurrent neural networks, 128 unit GRU-RNN
412 128-dimensional output
420 Fully Connected Layer
430 Activation Function
450 fixed-length prosodic embeddings P E , prosodic embeddings P E
500 text prediction system
500a text prediction system
500b text prediction system
502 input text, text encoder, input text sequence
510 style token layer
512 attention module, style attention module, attention style module
514 Style Token
514a-n Style Tokens
516 Combined Weights
516a-n combination weights
516P Combined Weight (CW), Predicted Combined Weight
516T combination weight, target combination weight (CW)
520 text prediction models, text prediction networks, networks
520a text prediction model, text prediction network, network, text prediction combined weight (TPCW) model, model, TPCW model
520b text prediction model, text prediction network, text prediction style embedding (TPSE) model, TPSE model
522 64 units time-aggregated GRU-RNN522, two-way RNN522, time-aggregated GRU-RNN
524 fixed length output, fixed length text feature
527 Fully Connected Layer
550 style embedded S E , style embedded
550P style embedding S E , text prediction style embedding S E
550T Target Style Embedded S E
553 encoded sequence
600 context prediction system, system
602 Context Features
610 context model
612 context embedding
650 end-to-end TTS model, TTS model
652 text encoder, encoder
653 encoded sequence
654 coupler
655 Consolidation
656 Attention Module
657 fixed length context vector
658 decoder
670 output audio signal
675 Synthesizer
680 Expressive Speech, Synthetic Speech
700a-d Context TTS network, TTS network
700a full context TTS network, network
700b 1 step context TTS network, network
700c Isolated Full Context TTS Network
700d Separation 1-Step Context TTS Network
710 context module, isolated context module
750 style encoder
800 text sources
802 Context Features
900 computing devices
900a standard server
900b laptop computer
900c rack server system
910 data processing hardware, components
920 memory hardware, memory, components
930 storage device, component
940 high speed interface/controller, building blocks
950 high speed expansion port, building blocks
960 Low Speed Interface/Controller, Components
970 Slow Bus
980 display
Claims (24)
前記現在の入力テキスト(502)に関連するコンテキスト埋め込み(612)を生成するために前記1つまたは複数のコンテキスト特徴(602)を処理する動作と
を行うように構成されたコンテキストエンコーダと、
前記コンテキストエンコーダと通信し、
前記テキストソース(800)から前記現在の入力テキスト(502)を受信する動作であって、前記テキストソース(800)が表現力豊かなスピーチ(680)に合成されるべきテキストのシーケンスを含む、動作と、
前記コンテキストエンコーダから前記現在の入力テキスト(502)に関連する前記コンテキスト埋め込み(612)を受信する動作と、
前記現在の入力テキスト(502)に対してスタイル埋め込み(550)を出力として予測するために、前記現在の入力テキスト(502)と前記現在の入力テキストに関連する前記コンテキスト埋め込み(612)とを処理する動作であって、前記スタイル埋め込み(550)が、前記現在の入力テキスト(502)を表現力豊かなスピーチ(680)に合成するための特定の韻律および/またはスタイルを指定する、動作と
を行うように構成されたテキスト予測ネットワーク(520)と、
前記テキスト予測ネットワーク(520)と通信し、
前記テキストソース(800)から前記現在の入力テキスト(502)を受信する動作と、
前記テキスト予測ネットワークによって予測された前記スタイル埋め込み(550)を受信する動作と、
前記現在の入力テキスト(502)の表現力豊かなスピーチ(680)の出力オーディオ信号(670)を生成するために前記現在の入力テキスト(502)と前記スタイル埋め込み(550)とを処理する動作であって、前記出力オーディオ信号(670)が前記スタイル埋め込み(550)によって指定された特定の韻律および/またはスタイルを有する、動作と
を行うように構成されたテキスト読み上げモデル(650)と
を備えるシステム(900)。 An act of receiving one or more contextual features (602) associated with current input text (502) to be synthesized into expressive speech (680), wherein each contextual feature (602) is a an action derived from the text source (800) of the input text (502) of
a context encoder configured to: process the one or more context features (602) to generate context embeddings (612) associated with the current input text (502);
Communicate with the context encoder ;
An act of receiving said current input text (502) from said text source (800), wherein said text source (800) comprises a sequence of text to be synthesized into expressive speech (680). When,
an act of receiving from the context encoder the context embeddings (612) associated with the current input text (502);
processing the current input text (502) and the context embeddings (612) associated with the current input text to predict as output style embeddings (550) for the current input text (502); wherein said style embedding (550) specifies a particular prosody and/or style for synthesizing said current input text (502) into expressive speech (680); a text prediction network (520) configured to:
in communication with said text prediction network (520);
an act of receiving said current input text (502) from said text source (800);
an act of receiving said style embeddings (550) predicted by said text prediction network;
in the act of processing said current input text (502) and said style embeddings (550) to generate an output audio signal (670) of expressive speech (680) of said current input text (502). a text-to-speech model (650) configured to perform an action, wherein said output audio signal (670) has a particular prosody and/or style specified by said style embeddings (550); (900).
前記現在の入力テキスト(502)、
前記現在の入力テキスト(502)に先行する前記テキストソース(800)からの前のテキスト、
前記前のテキストから合成された前のスピーチ、
前記現在の入力テキスト(502)に続く前記テキストソース(800)からの次のテキスト、または
前記前のテキストと前記前のテキストに関連する前のコンテキスト埋め込み(612)とに基づいて前記テキスト予測ネットワーク(520)によって予測された前のスタイル埋め込み(550)
のうちの少なくとも1つを含む、請求項1に記載のシステム(900)。 said one or more contextual features (602) associated with said current input text (502) comprising:
said current input text (502);
previous text from said text source (800) preceding said current input text (502);
previous speech synthesized from said previous text;
next text from said text source (800) following said current input text (502); or said text prediction network based on said previous text and previous context embeddings (612) associated with said previous text. Previous style embedding (550) predicted by (520)
2. The system (900) of claim 1, comprising at least one of:
前記現在の入力テキスト(502)に関連する前記1つまたは複数のコンテキスト特徴(602)が、
前記テキスト文書のタイトル、
前記テキスト文書内の章のタイトル、
前記テキスト文書内の節のタイトル、
前記テキスト文書内の見出し、
前記テキスト文書内の1つもしくは複数の箇条書き、
前記テキスト文書から抽出された概念グラフからのエンティティ、または
前記テキスト文書から抽出された1つもしくは複数の構造化された回答表現
のうちの少なくとも1つを含む、
請求項1または2に記載のシステム(900)。 the text source (800) comprises a text document;
said one or more contextual features (602) associated with said current input text (502) comprising:
a title of said text document;
titles of chapters in said text document;
a title of a section within said text document;
a heading within said text document;
one or more bullet points in said text document;
entities from a conceptual graph extracted from said text document; or one or more structured answer representations extracted from said text document;
3. The system (900) of claim 1 or 2.
前記現在の入力テキスト(502)が、前記対話トランスクリプト内の現在のターンに対応し、
前記現在の入力テキスト(502)に関連する前記1つまたは複数のコンテキスト特徴(602)が、
前記対話トランスクリプト内の前のターンに対応する前記対話トランスクリプト内の前のテキスト、または
前記対話トランスクリプト内の次のターンに対応する前記対話トランスクリプト内の次のテキスト
のうちの少なくとも1つを含む、
請求項1から3のいずれか一項に記載のシステム(900)。 the text source (800) comprises a dialogue transcript;
said current input text (502) corresponds to a current turn within said dialog transcript;
said one or more contextual features (602) associated with said current input text (502) comprising:
at least one of: previous text in said dialogue transcript corresponding to a previous turn in said dialogue transcript; or next text in said dialogue transcript corresponding to a next turn in said dialogue transcript. including,
4. The system (900) of any one of claims 1-3.
前記現在の入力テキスト(502)が、前記クエリ応答システムにおいて受信された現在のクエリに対する応答に対応し、
前記現在の入力テキスト(502)に関連する前記1つまたは複数のコンテキスト特徴(602)が、
前記現在のクエリに関連するテキスト、もしくは前記クエリ応答システムにおいて受信されたクエリのシーケンスに関連するテキストであって、前記クエリのシーケンスが、前記現在のクエリと、前記現在のクエリに先行する1つもしくは複数のクエリを含む、テキスト、または
前記現在のクエリに関連するオーディオ特徴、もしくは前記クエリ応答システムにおいて受信された前記クエリのシーケンスに関連するオーディオ特徴
のうちの少なくとも1つを含む、
請求項1から4のいずれか一項に記載のシステム(900)。 the text source (800) comprises a query response system;
said current input text (502) corresponds to a response to a current query received at said query response system;
said one or more contextual features (602) associated with said current input text (502) comprising:
Text associated with the current query or text associated with a sequence of queries received at the query response system, wherein the sequence of queries comprises the current query and one preceding the current query. or audio features associated with said current query, or audio features associated with said sequence of queries received at said query response system;
The system (900) of any one of claims 1-4.
前記テキストソース(800)から前記現在の入力テキスト(502)を受信し、
前記現在の入力テキスト(502)のそれぞれの符号化シーケンス(653)を生成するために、前記現在の入力テキスト(502)を処理する
ように構成されたエンコーダニューラルネットワーク(112)と、
前記エンコーダニューラルネットワーク(112)から前記現在の入力テキスト(502)の前記それぞれの符号化シーケンス(653)を受信し、
前記テキスト予測ネットワークによって予測された前記スタイル埋め込み(550)を受信し、
前記現在の入力テキスト(502)の前記それぞれの符号化シーケンス(653)と前記スタイル埋め込み(550)との間の連結(655)を生成する
ように構成された連結器(654)と、
デコーダ入力のシーケンスを受信し、
前記シーケンス内のデコーダ入力ごとに、前記出力オーディオ信号(670)のrフレームを生成するために、対応するデコーダ入力と、前記現在の入力テキスト(502)の前記それぞれの符号化シーケンス(653)と前記スタイル埋め込み(550)との間の前記連結(655)とを処理する
ように構成された注意ベースのデコーダリカレントニューラルネットワーク(118)であって、rが1よりも大きい整数を含む、注意ベースのデコーダリカレントニューラルネットワーク(118)と
を備える、請求項1から5のいずれか一項に記載のシステム(900)。 The text-to-speech model (650) is
receiving said current input text (502) from said text source (800);
an encoder neural network (112) configured to process said current input text (502) to generate an encoded sequence (653) for each of said current input text (502);
receiving said respective encoded sequences (653) of said current input text (502) from said encoder neural network (112);
receiving said style embeddings (550) predicted by said text prediction network;
a concatenator (654) configured to generate a concatenation (655) between said respective encoded sequence (653) of said current input text (502) and said style embeddings (550);
receive a sequence of decoder inputs;
for each decoder input in said sequence, a corresponding decoder input and said respective encoded sequence (653) of said current input text (502) to produce r frames of said output audio signal (670); an attention-based decoder recurrent neural network (118) configured to process said connection (655) between said style embeddings (550), wherein r comprises an integer greater than 1; 6. The system (900) of any one of claims 1 to 5, comprising a decoder recurrent neural network (118) of .
前記現在の入力テキスト(502)の文字のシーケンス内の各文字のそれぞれの埋め込みを受信し、
文字ごとに、前記文字のそれぞれの変換された埋め込みを生成するために、前記それぞれの埋め込みを処理する
ように構成されたエンコーダプレネットニューラルネットワーク(114)と、
前記エンコーダプレネットニューラルネットワークによって生成された前記変換された埋め込みを受信し、
前記現在の入力テキスト(502)の前記それぞれの符号化シーケンス(653)を生成するために、前記変換された埋め込みを処理する
ように構成されたエンコーダCBHGニューラルネットワーク(116)と
を備える、請求項6に記載のシステム(900)。 The encoder neural network (112) is
receiving a respective embedding of each character in a sequence of characters of said current input text (502);
an encoder prenet neural network (114) configured, for each character, to process said respective embeddings to produce respective transformed embeddings of said characters;
receiving the transformed embeddings produced by the encoder prenet neural network;
an encoder CBHG neural network (116) configured to process said transformed embeddings to produce said respective encoded sequences (653) of said current input text (502). 6. The system (900) of 6.
前記現在の入力テキスト(502)に関連する前記コンテキスト埋め込み(612)と、前記現在の入力テキスト(502)の符号化シーケンス(653)とを受信し、
前記コンテキスト埋め込み(612)と前記符号化シーケンス(653)とを処理することによって、固定長の特徴ベクトルを生成する
ように構成された時間集約ゲート付き回帰型ユニット(GRU)リカレントニューラルネットワーク(RNN)と、
前記固定長の特徴ベクトルを処理することによって、前記スタイル埋め込み(550)を予測するように構成された1つまたは複数の完全接続層と
を備える、
請求項1から8のいずれか一項に記載のシステム(900)。 The text prediction network (520) comprises:
receiving the context embeddings (612) associated with the current input text (502) and an encoding sequence (653) of the current input text (502);
A time aggregate gated recurrent unit (GRU) recurrent neural network (RNN) configured to generate fixed length feature vectors by processing said context embedding (612) and said encoding sequence (653). When,
and one or more fully connected layers configured to predict the style embedding (550) by processing the fixed-length feature vector.
The system (900) of any one of claims 1-8.
データ処理ハードウェア(910)において、テキストソース(800)から現在の入力テキスト(502)を受信するステップであって、前記現在の入力テキスト(502)が、テキスト読み上げ(TTS)モデル(650)によって表現力豊かなスピーチ(680)に合成されることになっている、ステップと、
前記データ処理ハードウェア(910)によって、コンテキストモデル(610)を使用して、前記テキストソース(800)から導出された1つまたは複数のコンテキスト特徴(602)を処理することによって、現在の入力テキスト(502)に関連するコンテキスト埋め込み(612)を生成するステップと、
前記データ処理ハードウェア(910)によって、テキスト予測ネットワーク(520)を使用して、前記現在の入力テキスト(502)と前記現在の入力テキスト(502)に関連する前記コンテキスト埋め込み(612)とを処理することによって、前記現在の入力テキスト(502)のためのスタイル埋め込み(550)を予測するステップであって、前記スタイル埋め込み(550)が、前記現在の入力テキスト(502)を表現力豊かなスピーチ(680)に合成するための特定の韻律および/またはスタイルを指定する、ステップと、
前記データ処理ハードウェア(910)によって、前記TTSモデル(650)を使用して、前記スタイル埋め込み(550)と前記現在の入力テキスト(502)とを処理することよって、前記現在の入力テキスト(502)の表現力豊かなスピーチ(680)の前記出力オーディオ信号(670)を生成するステップであって、前記出力オーディオ信号(670)が、前記スタイル埋め込み(550)によって指定された前記特定の韻律および/またはスタイルを有する、ステップと
を含む、方法(1000)。 A method (1000) for generating an output audio signal (670) of expressive synthesized speech (680), said method (1000) comprising:
In data processing hardware (910), receiving current input text (502) from a text source (800), said current input text (502) being read by a text-to-speech (TTS) model (650). steps to be synthesized into expressive speech (680),
current input text by processing, by said data processing hardware (910), one or more context features (602) derived from said text source (800) using a context model (610); generating context embeddings (612) associated with (502);
processing, by said data processing hardware (910), said current input text (502) and said context embeddings (612) associated with said current input text (502) using a text prediction network (520); predicting style embeddings (550) for said current input text (502) by: (680) specifying a particular prosody and/or style for synthesizing;
said current input text (502) by processing said style embeddings (550) and said current input text (502) using said TTS model (650) by said data processing hardware (910); ), wherein the output audio signal (670) conforms to the particular prosody specified by the style embeddings (550) and A method (1000) including steps and/or styles.
前記現在の入力テキスト(502)、
前記現在の入力テキスト(502)に先行する前記テキストソース(800)からの前のテキスト、
前記前のテキストから合成された前のスピーチ、
前記現在の入力テキスト(502)に続く前記テキストソース(800)からの次のテキスト、または
前記前のテキストと前記前のテキストに関連する前のコンテキスト埋め込み(612)とに基づいて前記テキスト予測ネットワーク(520)によって予測された前のスタイル埋め込み(550)
のうちの少なくとも1つを含む、
請求項13に記載の方法(1000)。 said one or more contextual features (602) associated with said current input text (502) comprising:
said current input text (502);
previous text from said text source (800) preceding said current input text (502);
previous speech synthesized from said previous text;
next text from said text source (800) following said current input text (502); or said text prediction network based on said previous text and previous context embeddings (612) associated with said previous text. Previous style embedding (550) predicted by (520)
including at least one of
14. The method (1000) of claim 13.
前記現在の入力テキスト(502)に関連する前記1つまたは複数のコンテキスト特徴(602)が、
前記テキスト文書のタイトル、
前記テキスト文書内の章のタイトル、
前記テキスト文書内の節のタイトル、
前記テキスト文書内の見出し、
前記テキスト文書内の1つもしくは複数の箇条書き、
前記テキスト文書から抽出された概念グラフからのエンティティ、または
前記テキスト文書から抽出された1つもしくは複数の構造化された回答表現
のうちの少なくとも1つを含む、
請求項13または14に記載の方法(1000)。 the text source (800) comprises a text document;
said one or more contextual features (602) associated with said current input text (502) comprising:
a title of said text document;
titles of chapters in said text document;
a title of a section within said text document;
a heading within said text document;
one or more bullet points in said text document;
entities from a conceptual graph extracted from said text document; or one or more structured answer representations extracted from said text document;
15. A method (1000) according to claim 13 or 14.
前記現在の入力テキスト(502)が、前記対話トランスクリプト内の現在のターンに対応し、
前記現在の入力テキスト(502)に関連する前記1つまたは複数のコンテキスト特徴(602)が、
前記対話トランスクリプト内の前のターンに対応する前記対話トランスクリプト内の前のテキスト、または
前記対話トランスクリプト内の次のターンに対応する前記対話トランスクリプト内の次のテキスト
のうちの少なくとも1つを含む、
請求項13から15のいずれか一項に記載の方法(1000)。 the text source (800) comprises a dialogue transcript;
said current input text (502) corresponds to a current turn within said dialog transcript;
said one or more contextual features (602) associated with said current input text (502) comprising:
at least one of: previous text in said dialogue transcript corresponding to a previous turn in said dialogue transcript; or next text in said dialogue transcript corresponding to a next turn in said dialogue transcript. including,
The method (1000) of any one of claims 13-15.
前記現在の入力テキスト(502)が、前記クエリ応答システムにおいて受信された現在のクエリに対する応答に対応し、
前記現在の入力テキスト(502)に関連する前記1つまたは複数のコンテキスト特徴(602)が、
前記現在のクエリに関連するテキスト、もしくは前記クエリ応答システムにおいて受信されたクエリのシーケンスに関連するテキストであって、前記クエリのシーケンスが、前記現在のクエリと、前記現在のクエリに先行する1つもしくは複数のクエリを含む、テキスト、または
前記現在のクエリに関連するオーディオ特徴、もしくは前記クエリ応答システムにおいて受信された前記クエリのシーケンスに関連するオーディオ特徴
のうちの少なくとも1つを含む、
請求項13から16のいずれか一項に記載の方法(1000)。 the text source (800) comprises a query response system;
said current input text (502) corresponds to a response to a current query received at said query response system;
said one or more contextual features (602) associated with said current input text (502) comprising:
Text associated with the current query or text associated with a sequence of queries received at the query response system, wherein the sequence of queries comprises the current query and one preceding the current query. or audio features associated with said current query, or audio features associated with said sequence of queries received at said query response system;
17. The method (1000) of any one of claims 13-16.
前記テキスト読み上げモデル(650)のエンコーダニューラルネットワーク(112)において、前記テキストソース(800)から前記現在の入力テキスト(502)を受信するステップと、
前記エンコーダニューラルネットワーク(112)を使用して、前記現在の入力テキスト(502)のそれぞれの符号化シーケンス(653)を生成するステップと、
前記テキスト読み上げモデル(650)の連結器(654)を使用して、前記現在の入力テキスト(502)の前記それぞれの符号化シーケンス(653)と前記スタイル埋め込み(550)との間の連結(655)を生成するステップと、
前記テキスト読み上げモデル(650)の注意ベースのデコーダリカレントニューラルネットワーク(118)において、デコーダ入力のシーケンスを受信するステップと、
前記デコーダ入力のシーケンス内のデコーダ入力ごとに、前記注意ベースのデコーダリカレントニューラルネットワーク(118)を使用して、前記出力オーディオ信号(670)のrフレームを生成するために、対応するデコーダ入力と、前記現在の入力テキスト(502)の前記それぞれの符号化シーケンス(653)と前記スタイル埋め込み(550)との間の前記連結(655)とを処理するステップであって、rが、1よりも大きい整数を含む、ステップと
を含む、
請求項13から17のいずれか一項に記載の方法(1000)。 generating the output audio signal (670) comprises:
receiving, in the encoder neural network (112) of the text-to-speech model (650), the current input text (502) from the text source (800);
generating an encoded sequence (653) for each of said current input text (502) using said encoder neural network (112);
concatenating (655) between said respective encoded sequences (653) of said current input text (502) and said style embeddings (550) using a concatenator (654) of said text-to-speech model (650); ), and
receiving a sequence of decoder inputs in an attention-based decoder recurrent neural network (118) of the text-to-speech model (650);
for each decoder input in the sequence of decoder inputs to generate r frames of the output audio signal (670) using the attention-based decoder recurrent neural network (118); and processing said respective encoded sequences (653) of said current input text (502) and said concatenations (655) between said style embeddings (550), wherein r is greater than one; including an integer, including a step,
The method (1000) of any one of claims 13-17.
前記エンコーダニューラルネットワーク(112)のエンコーダプレネットニューラルネットワーク(114)において、前記現在の入力テキスト(502)の文字のシーケンス内の各文字のそれぞれの埋め込みを受信するステップと、
前記文字のシーケンス内の文字ごとに、前記エンコーダプレネットニューラルネットワーク(114)を使用して、前記文字のそれぞれの変換された埋め込みを生成するために、前記それぞれの埋め込みを処理するステップと、
前記エンコーダニューラルネットワーク(112)のエンコーダCBHGニューラルネットワーク(116)を使用して、前記変換された埋め込みを処理することによって、前記現在の入力テキスト(502)のそれぞれの符号化シーケンス(653)を生成するステップと
を含む、
請求項18に記載の方法(1000)。 generating said respective encoded sequences (653) of said current input text (502),
receiving, in an encoder planet neural network (114) of the encoder neural network (112), a respective embedding of each character in a sequence of characters of the current input text (502);
for each character in said sequence of characters, processing said respective embedding to produce a respective transformed embedding of said character using said encoder-planet neural network (114);
An encoded sequence (653) of each of said current input text (502) is generated by processing said transformed embeddings using an encoder CBHG neural network (116) of said encoder neural network (112). and
19. The method (1000) of claim 18.
前記テキスト予測ネットワーク(520)の時間集約ゲート付き回帰型ユニット(GRU)リカレントニューラルネットワーク(RNN)を使用して、前記現在の入力テキスト(502)に関連する前記コンテキスト埋め込み(612)と前記現在の入力テキスト(502)の符号化シーケンス(653)とを処理することによって、固定長の特徴ベクトルを生成するステップと、
前記GRU-RNNに続く前記テキスト予測ネットワーク(520)の1つまたは複数の完全接続層を使用して、前記固定長の特徴ベクトルを処理することによって、前記スタイル埋め込み(550)を予測するステップと
を含む、
請求項13から20のいずれか一項に記載の方法(1000)。 predicting said style embeddings (550) for said current input text (502);
Using a time aggregate gated recurrent unit (GRU) recurrent neural network (RNN) of the text prediction network (520), the context embedding (612) associated with the current input text (502) and the current generating a fixed-length feature vector by processing the encoded sequence (653) of the input text (502);
predicting said style embeddings (550) by processing said fixed length feature vectors using one or more fully connected layers of said text prediction network (520) following said GRU-RNN; including,
The method (1000) of any one of claims 13-20.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2022210470A JP2023036888A (en) | 2019-08-03 | 2022-12-27 | Controlling expressivity in end-to-end speech synthesis system |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962882511P | 2019-08-03 | 2019-08-03 | |
US62/882,511 | 2019-08-03 | ||
PCT/US2020/042416 WO2021025844A1 (en) | 2019-08-03 | 2020-07-16 | Controlling expressivity in end-to-end speech synthesis systems |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022210470A Division JP2023036888A (en) | 2019-08-03 | 2022-12-27 | Controlling expressivity in end-to-end speech synthesis system |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2022536558A JP2022536558A (en) | 2022-08-17 |
JP7204989B2 true JP7204989B2 (en) | 2023-01-16 |
Family
ID=72050918
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022506820A Active JP7204989B2 (en) | 2019-08-03 | 2020-07-16 | Expressivity Control in End-to-End Speech Synthesis Systems |
JP2022210470A Pending JP2023036888A (en) | 2019-08-03 | 2022-12-27 | Controlling expressivity in end-to-end speech synthesis system |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022210470A Pending JP2023036888A (en) | 2019-08-03 | 2022-12-27 | Controlling expressivity in end-to-end speech synthesis system |
Country Status (6)
Country | Link |
---|---|
US (2) | US11676573B2 (en) |
EP (2) | EP4345815A2 (en) |
JP (2) | JP7204989B2 (en) |
KR (2) | KR20240001262A (en) |
CN (1) | CN114175143A (en) |
WO (1) | WO2021025844A1 (en) |
Families Citing this family (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11954577B2 (en) * | 2019-09-13 | 2024-04-09 | Intuit Inc. | Deep neural network based user segmentation |
US11282495B2 (en) * | 2019-12-12 | 2022-03-22 | Amazon Technologies, Inc. | Speech processing using embedding data |
US11562744B1 (en) * | 2020-02-13 | 2023-01-24 | Meta Platforms Technologies, Llc | Stylizing text-to-speech (TTS) voice response for assistant systems |
US11322133B2 (en) * | 2020-07-21 | 2022-05-03 | Adobe Inc. | Expressive text-to-speech utilizing contextual word-level style tokens |
KR102392904B1 (en) * | 2020-09-25 | 2022-05-02 | 주식회사 딥브레인에이아이 | Method and apparatus for synthesizing voice of based text |
CN112017644B (en) * | 2020-10-21 | 2021-02-12 | 南京硅基智能科技有限公司 | Sound transformation system, method and application |
CN113096641B (en) * | 2021-03-29 | 2023-06-13 | 北京大米科技有限公司 | Information processing method and device |
CN113327575B (en) * | 2021-05-31 | 2024-03-01 | 广州虎牙科技有限公司 | Speech synthesis method, device, computer equipment and storage medium |
CN113096638B (en) * | 2021-06-09 | 2021-09-07 | 北京世纪好未来教育科技有限公司 | Speech synthesis model training method, speech synthesis method and device |
GB2607903A (en) * | 2021-06-14 | 2022-12-21 | Deep Zen Ltd | Text-to-speech system |
CN113838448B (en) * | 2021-06-16 | 2024-03-15 | 腾讯科技（深圳）有限公司 | Speech synthesis method, device, equipment and computer readable storage medium |
CN113628610B (en) * | 2021-08-12 | 2024-02-13 | 科大讯飞股份有限公司 | Voice synthesis method and device and electronic equipment |
WO2023112095A1 (en) * | 2021-12-13 | 2023-06-22 | 日本電信電話株式会社 | Speech synthesis device, speech synthesis method, and program |
CN115578995B (en) * | 2022-12-07 | 2023-03-24 | 北京邮电大学 | Speech synthesis method, system and storage medium for speech dialogue scene |
CN117153144B (en) * | 2023-10-31 | 2024-02-06 | 杭州宇谷科技股份有限公司 | Battery information voice broadcasting method and device based on terminal calculation |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150364128A1 (en) | 2014-06-13 | 2015-12-17 | Microsoft Corporation | Hyper-structure recurrent neural networks for text-to-speech |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9195656B2 (en) * | 2013-12-30 | 2015-11-24 | Google Inc. | Multilingual prosody generation |
US9697820B2 (en) * | 2015-09-24 | 2017-07-04 | Apple Inc. | Unit-selection text-to-speech synthesis using concatenation-sensitive neural networks |
US11069335B2 (en) * | 2016-10-04 | 2021-07-20 | Cerence Operating Company | Speech synthesis using one or more recurrent neural networks |
US10475438B1 (en) | 2017-03-02 | 2019-11-12 | Amazon Technologies, Inc. | Contextual text-to-speech processing |
US10796686B2 (en) * | 2017-10-19 | 2020-10-06 | Baidu Usa Llc | Systems and methods for neural text-to-speech using convolutional sequence learning |
EP3739572A4 (en) * | 2018-01-11 | 2021-09-08 | Neosapience, Inc. | Text-to-speech synthesis method and apparatus using machine learning, and computer-readable storage medium |
US10799795B1 (en) * | 2019-03-26 | 2020-10-13 | Electronic Arts Inc. | Real-time audio generation for electronic games based on personalized music preferences |
-
2020
- 2020-07-16 EP EP24156282.6A patent/EP4345815A2/en active Pending
- 2020-07-16 JP JP2022506820A patent/JP7204989B2/en active Active
- 2020-07-16 WO PCT/US2020/042416 patent/WO2021025844A1/en active Application Filing
- 2020-07-16 US US16/931,336 patent/US11676573B2/en active Active
- 2020-07-16 KR KR1020237043408A patent/KR20240001262A/en active Application Filing
- 2020-07-16 EP EP20754849.6A patent/EP4007997B1/en active Active
- 2020-07-16 KR KR1020227004782A patent/KR102616214B1/en active IP Right Grant
- 2020-07-16 CN CN202080055081.9A patent/CN114175143A/en active Pending
-
2022
- 2022-12-27 JP JP2022210470A patent/JP2023036888A/en active Pending
-
2023
- 2023-05-09 US US18/314,556 patent/US20230274728A1/en active Pending
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150364128A1 (en) | 2014-06-13 | 2015-12-17 | Microsoft Corporation | Hyper-structure recurrent neural networks for text-to-speech |
Non-Patent Citations (1)
Title |
---|
STANTON Daisy, et al.，"PREDICTING EXPRESSIVE SPEAKING STYLE FROM TEXT IN END-TO-END SPEECH SYNTHESIS"，arXiv.org[online]，2018年08月04日，pp.1-8，[arXiv:1808.014140]、[2022年9月9日検索]、インターネット<https://arxiv.org/abs/1808.014140><https://arxiv.org/pdf/1808.014140.pdf> |
Also Published As
Publication number | Publication date |
---|---|
JP2022536558A (en) | 2022-08-17 |
WO2021025844A1 (en) | 2021-02-11 |
KR20220035180A (en) | 2022-03-21 |
US20210035551A1 (en) | 2021-02-04 |
EP4345815A2 (en) | 2024-04-03 |
US11676573B2 (en) | 2023-06-13 |
EP4007997A1 (en) | 2022-06-08 |
JP2023036888A (en) | 2023-03-14 |
EP4007997B1 (en) | 2024-03-27 |
US20230274728A1 (en) | 2023-08-31 |
KR20240001262A (en) | 2024-01-03 |
CN114175143A (en) | 2022-03-11 |
KR102616214B1 (en) | 2023-12-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7204989B2 (en) | Expressivity Control in End-to-End Speech Synthesis Systems | |
KR102579843B1 (en) | Variational embedding capacity in expressive end-to-end speech synthesis. | |
CN110782870B (en) | Speech synthesis method, device, electronic equipment and storage medium | |
US11929059B2 (en) | Method, device, and computer readable storage medium for text-to-speech synthesis using machine learning on basis of sequential prosody feature | |
US10872598B2 (en) | Systems and methods for real-time neural text-to-speech | |
KR102057927B1 (en) | Apparatus for synthesizing speech and method thereof | |
KR20230003056A (en) | Speech recognition using non-speech text and speech synthesis | |
KR20230034423A (en) | 2-level speech rhyme transmission | |
US11763797B2 (en) | Text-to-speech (TTS) processing | |
CN111771213A (en) | Speech style migration | |
KR20230156121A (en) | Unsupervised parallel tacotron non-autoregressive and controllable text-to-speech | |
US11475874B2 (en) | Generating diverse and natural text-to-speech samples | |
KR20230084229A (en) | Parallel tacotron: non-autoregressive and controllable TTS | |
Shiga et al. | Text-to-speech synthesis | |
KR20200111609A (en) | Apparatus for synthesizing speech and method thereof | |
Liu et al. | Integrating Discrete Word-Level Style Variations into Non-Autoregressive Acoustic Models for Speech Synthesis. | |
Evrard | Transformers in automatic speech recognition | |
Barakat et al. | Deep learning-based expressive speech synthesis: a systematic review of approaches, challenges, and resources | |
Eirini | End-to-End Neural based Greek Text-to-Speech Synthesis | |
Oralbekova et al. | Current advances and algorithmic solutions in speech generation | |
CN117316140A (en) | Speech synthesis method, apparatus, device, storage medium, and program product | |
Dou | Waveform level synthesis | |
Cornille | Controllable Expressive Speech Synthesis |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20220301 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20220301 |
|
A871 | Explanation of circumstances concerning accelerated examination |
Free format text: JAPANESE INTERMEDIATE CODE: A871Effective date: 20220301 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20220920 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20221128 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20221228 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7204989Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |