CN116823593A - Stylized input image - Google Patents
Stylized input image Download PDFInfo
- Publication number
- CN116823593A CN116823593A CN202310738876.5A CN202310738876A CN116823593A CN 116823593 A CN116823593 A CN 116823593A CN 202310738876 A CN202310738876 A CN 202310738876A CN 116823593 A CN116823593 A CN 116823593A
- Authority
- CN
- China
- Prior art keywords
- image
- style
- input
- neural network
- layer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013528 artificial neural network Methods 0.000 claims abstract description 125
- 230000005012 migration Effects 0.000 claims abstract description 74
- 238000013508 migration Methods 0.000 claims abstract description 74
- 238000000034 method Methods 0.000 claims abstract description 50
- 238000012545 processing Methods 0.000 claims abstract description 32
- 230000008569 process Effects 0.000 claims abstract description 22
- 238000010606 normalization Methods 0.000 claims description 85
- 238000012549 training Methods 0.000 claims description 22
- 230000004044 response Effects 0.000 claims description 4
- 230000001131 transforming effect Effects 0.000 claims description 3
- 238000004590 computer program Methods 0.000 description 12
- 238000013527 convolutional neural network Methods 0.000 description 8
- 238000010801 machine learning Methods 0.000 description 8
- 230000006870 function Effects 0.000 description 6
- 230000009471 action Effects 0.000 description 5
- 230000008901 benefit Effects 0.000 description 5
- 238000004891 communication Methods 0.000 description 5
- 238000013459 approach Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000009467 reduction Effects 0.000 description 3
- 238000012546 transfer Methods 0.000 description 3
- 238000010586 diagram Methods 0.000 description 2
- 230000005251 gamma ray Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000009466 transformation Effects 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- PXFBZOLANLWPMH-UHFFFAOYSA-N 16-Epiaffinine Natural products C1C(C2=CC=CC=C2N2)=C2C(=O)CC2C(=CC)CN(C)C1C2CO PXFBZOLANLWPMH-UHFFFAOYSA-N 0.000 description 1
- 244000025254 Cannabis sativa Species 0.000 description 1
- 241000009334 Singa Species 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000011478 gradient descent method Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000010422 painting Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
- G06T11/001—Texturing; Colouring; Generation of texture or colour
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/40—Software arrangements specially adapted for pattern recognition, e.g. user interfaces or toolboxes therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/096—Transfer learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20084—Artificial neural networks [ANN]
Abstract
The present disclosure relates to stylized input images. A method for applying a style to an input image to generate a stylized image. The method includes maintaining data specifying respective parameter values for each image style of a set of image styles, receiving an input including an input image and data identifying the input style to be applied to the input image to generate a stylized image in the input style, determining parameter values for the input style from the maintained data, and generating the stylized image by processing the input image using a style migration neural network configured to process the input image to generate the stylized image.
Description
Description of the division
The application belongs to a divisional application of Chinese patent application 201780065307.1 with the application date of 2017, 10, 20.
Cross Reference to Related Applications
The present application claims priority from U.S. provisional application Ser. No. 62/411,414 filed on day 2016, 10 and 21. The disclosure of this prior application is considered to be part of the disclosure of the present application and is incorporated by reference in the disclosure of the present application.
Technical Field
The present description relates to applying styles to input images using neural networks.
Background
The machine learning model receives input and generates output, such as predicted output, based on the received input. Some machine learning models are parametric models and generate an output based on received inputs and values of model parameters.
Some machine learning models are depth models that employ a multi-layer model to generate an output of a received input. For example, a deep neural network is a deep machine learning model that includes an output layer and one or more hidden layers, each of which applies a nonlinear transformation to a received input to generate an output.
Disclosure of Invention
The present specification describes systems and methods for applying a style to an input image to generate a stylized image.
One method for applying a style to an input image to generate a stylized image includes: the method includes maintaining data specifying respective parameter values for each image style of a set of image styles, receiving an input including an input image and data identifying the input style to be applied to the input image to generate a stylized image in the input style, determining parameter values for the input style from the maintained data, and generating the stylized image by processing the input image using a style transfer neural network configured to process the input image to generate the stylized image.
The style migration neural network includes a condition instance normalization layer between a first neural network layer and a second neural network layer. The condition instance normalization layer is configured to, during processing of the input image by the style migration neural network: the method includes receiving a first layer output generated by a first neural network layer, transforming the first layer output according to a current value of a condition instance normalization layer parameter to generate a condition instance normalization layer output, and providing the condition instance normalization layer output as an input to a second neural network layer. Generating the stylized image includes: to process an input image through a style migration neural network, a current value of a conditional instance normalization layer parameter is set to a parameter value of an input style.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The system described in this specification implements a single scalable depth style migration neural network that can capture multiple different styles of images. Such neural networks are generalized across a number of different styles. The system described in this specification allows a user or other system to arbitrarily combine the styles in which a neural network has been trained for application. The style migration neural network can be effectively trained to apply a plurality of different styles to the input image because only a small portion of the parameters of the neural network depend on the style in which the neural network is applied to the input image. Thus, the time required to train the neural network to apply a plurality of different styles to an input image is reduced, and the amount of computer processing resources required to apply a plurality of different styles to the same image is also reduced. This reduction in processing resources is particularly advantageous when the style migration neural network is implemented in, for example, a mobile device in which power consumption and processing resource management are particularly important. In addition, the output image may be provided to the user more efficiently (i.e., in a shorter time), which provides other related advantages, such as reduced bright screen time, with further benefits in terms of power consumption of the device. In certain embodiments where the style migration neural network is distributed across a large system of such devices, the reduction in processing and power consumption requirements, as well as the reduction in network requirements between devices in the system, may generally result in large scale efficiencies on the system.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example image style migration system.
FIG. 2 is an example training diagram for training a style migration neural network.
FIG. 3 is a flow chart of an example process for generating a stylized image from an input image and an input style.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
The present specification generally describes an image style migration system that may perform style migration on an input image. Style migration may be defined as generating a stylized image (i.e., a hybrid (backup) image) from a content image and a style image that has content similar to that of the content image but a style similar to that of the style image. Typically, the content image is an image captured by a camera, and the style image is a drawing that has been drawn and painted according to the painting style (i.e., artistic style), or a digital image that has been edited using one or more image editing techniques. The style of the style image may include one or more of the following: (i) Repeated spatial patterns (motifs) within the stylized image, such as visual textures (e.g., grass) and drawing strokes, (ii) color palettes of the stylized image, and placement of stylized elements including spatial patterns and colors based on semantics contained in the stylized image.
FIG. 1 illustrates an example image style migration system 100. Image style migration system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations, in which the systems, components, and techniques described below may be implemented. In general, the image style migration system 100 is a system that applies a style to an input image to generate a stylized image.
The image style migration system 100 is configured to receive an input 102, the input 102 comprising an input image and data identifying the input style, and process the input image to generate a stylized image 114 in the input style from the input image, i.e., a stylized image having content similar to the input image but having the input style.
In some implementations, the system 100 may present a user interface to a user of the system that allows the user to select an input style from a set of image styles maintained by the system 100 (i.e., by selecting from style images that are each in a different style), or to select a combination of multiple image styles from the set of image styles, or to specify weights that should be applied to each of the combination of multiple image styles.
To generate the stylized image 114, the image style migration system 100 includes a subsystem 110 and a style migration neural network 112.
Subsystem 110 maintains data specifying respective parameter values for each image style in a set of image styles 116. Each image style in the set of image styles 116 is a style in which a particular image or a set of multiple particular images are depicted, e.g., a style in which a drawing is drawn, a style or particular manner in which a digital image is edited (e.g., using a grid (raster) graphics editing technique or other image editing technique), or a particular configuration of camera settings for capturing a particular image or multiple particular images. The respective parameter values for each image style have been determined by training the style migration neural network 112. An example process for training the style migration neural network 112 is described in detail below with reference to fig. 2.
The subsystem 110 is configured to receive an input 102, the input 102 comprising an input image and data identifying an input style to be applied to the input image to generate a stylized image 114 in the input style. Subsystem 110 is configured to determine parameter values for the input style from the maintained data based on the data identifying the input style. In particular, subsystem 110 determines that the input style is a particular image style from the set of image styles and designates a parameter value of the particular image style as a parameter value of the input style.
For example, subsystem 110 may maintain a table or other suitable data structure that generally maps each image style with corresponding parameter values for that image style. Each image style has a corresponding index, such as an integer or a string (e.g., name of the style), which may be used to find or retrieve the corresponding value of the parameter of the image style from a table or other suitable data structure. If the input style is a particular image style from the set of image styles maintained by subsystem 110, the data identifying the input style may include a corresponding index of image styles, such as a name of the image style.
In some implementations, when the input style is a combination of two or more image styles from the set of image styles, the subsystem 110 combines the respective parameter values of the two or more image styles to determine the parameter value of the input style.
In some implementations, when the input 102 specifies respective weights to be applied to each of the two or more image styles, the subsystem 110 combines the respective parameter values of the two or more image styles by weighting the respective parameter values of each of the two or more image styles with the weights to be applied to the image styles to determine the parameter values of the input styles. In some cases, subsystem 110 receives user input defining respective weights.
The subsystem 112 is then configured to generate a stylized image 114 in the input style by processing the input image using the style migration neural network 112. The style migration neural network 112 is a neural network configured to receive an input image and output a stylized version of the input image.
While many styles may share some degree of computation, conventional approaches typically ignore this sharing when building an N-style migration system by training N networks from scratch. For example, many impression-group drawings share similar drawing strokes, but use different color palettes. In this case, it is wasteful to treat a set of N impression pie drawings as a completely independent style. To account for this, the style migration neural network 112 includes one or more condition instance normalization layers and trains over a plurality of different styles, thereby enabling the system 100 to generalize over a plurality of different styles. The conditional instance normalization layer models styles by determining scaling and shifting parameters after normalizing each particular style. In other words, all convolution weights (i.e., parameters) of the style migration neural network 112 may be shared among many styles, and it is sufficient to adjust the parameters of the affine transformation after normalizing each style.
Examples of style migration neural network architectures that can be modified to include a condition instance normalization layer are described in just Johnson, alexandre Alahi, and Li Fei-Fei, "live-style migration and perceived loss of super resolution (Perceptual losses for real-time style transfer and super-resolution)" (arXiv preprint arXiv:1603.08155,2016). For example, the architecture may be modified by replacing some or all of the batch (batch) normalization layers with conditional instance normalization layers.
Each of the one or more condition instance normalization layers in the style migration neural network 112 is located after a respective one of the neural network 112 (e.g., a convolutional neural network layer). For example, as shown in fig. 1, the neural network 112 includes a condition instance normalization layer 106 that is located after the first neural network layer 104 and before the second neural network layer 108. The first neural network layer 104 and the second neural network layer 108 are convolutional neural network layers.
To style the input image in the input style, the subsystem 110 uses the determined parameter values of the input style to set current values of the condition instance normalization layer parameters for each condition instance normalization layer in the neural network. Subsystem 110 sets current values of parameters of one or more condition instance normalization layers (e.g., layer 106) without modifying parameter values of other neural network layers (e.g., layers 104 and 108) of neural network 112. In this way, the subsystem 110 keeps the parameter values of other layers in the neural network the same for all input images, but modifies the parameter values of all condition instance normalization layers according to the input style in which the input image is to be stylized.
If there is a single condition instance normalization layer in the neural network, the subsystem 110 sets the current value of the condition instance normalization layer parameter to the parameter value of the input style. If there are multiple condition instance normalization layers in the neural network, the parameter values of the input style include a corresponding subset of parameter values for each condition instance normalization layer, and the system sets the current value of the condition instance normalization layer parameter for each layer to the corresponding subset of parameter values of the input style.
The subsystem 110 is then configured to generate a stylized image 114 by processing the input image via each of the neural network layers of the neural network 112 when setting the current value of the condition instance normalization layer parameter to the determined parameter value of the input style. During processing of the input image by the neural network 112, each condition instance normalization layer is configured to receive a layer output generated by a neural network layer preceding the condition instance normalization layer and transform the layer output according to a current value of a parameter of the condition instance normalization layer to generate a condition instance normalization layer output and provide the condition instance normalization layer output as an input to another neural network layer in the neural network. For example, the condition instance normalization layer 106 is configured to receive the layer output 118 generated by the first neural network layer 104 and transform the layer output 118 according to the current values of parameters of the condition instance normalization layer 106 to generate a condition instance normalization layer output 120, and provide the condition instance normalization layer output 120 as an input to the second neural network layer 108.
To transform the layer output, the condition instance normalization layer normalizes the layer output to generate a normalized layer output and transforms the normalized layer output according to the current value of the condition instance normalization layer parameter to generate a condition instance normalized layer output.
More specifically, the conditional instance normalization layer normalizes the layer outputs by determining, for each depth dimension of the layer outputs, a normalization statistic for the components of the first layer output in the spatial dimension of the first layer output and normalizing the components of the first layer output using the normalization statistic.
The conditional instance normalization layer then transforms the normalized layer output by scaling the normalized layer output according to the current value of the scaled conditional instance normalization layer parameter to generate a scaled normalized layer output and by shifting the scaled normalized layer output according to the current value of the shifted conditional instance normalization layer parameter to generate a conditional instance normalization layer output.
For example, the conditional instance normalization layer output z, which is specific to the input style s, may have the form:
where x is the layer output generated by the neural network layer preceding the conditional instance normalization layer, μ is the average value of x, and σ is the standard deviation of x obtained on the spatial axis of the layer output x. Gamma ray s And beta s Is the identified parameter values of the input styles s, which are set as the current values of the parameters of the conditional instance normalization layer. Gamma ray s And beta s Also referred to as scaling and shifting parameters, respectively.
The condition instance normalization layer output is then provided as an input to the next neural network layer for processing. This process continues until the neural network has processed the input image through all layers of the neural network to generate a stylized image.
By incorporating a condition instance normalization layer, the style migration neural network 112 provides a number of technical advantages over existing approaches. For example, condition instance normalization allows the system 100 to style a single input image into multiple styles, such as N styles, where a single feed-forward propagation (forward feed pass) of the network 112 has a batch size of N instead of requiring N feed-forward propagates to perform N-style migration as with other single-style networks. In addition, because the conditional instance normalization layer only acts on the scaling and shifting parameters γ and β, training the style migration neural network 112 over N styles requires fewer parameters than existing methods of training N separate networks. In practice, because the size of γ and β grows linearly with respect to the number of feature maps in the style migration neural network 112, the conditional instance normalization approach requires O (nxl) parameters, where L is the total number of feature maps in the style migration neural network 112.
In some implementations, the input 102 can identify a video that includes a plurality of video frames, and the input image is a video frame from the video. In these embodiments, the system 100 may be configured to generate a respective stylized image for each of the plurality of video frames in the video by applying the input style to each of the plurality of video frames using the style migration neural network 112 in the same manner as described above, but the style migration neural network 112 has been additionally trained to ensure that the stylized images of the plurality of video frames in the video have similar stylized.
After generating the stylized image of the input image or the plurality of stylized images of the plurality of video frames in the video, the system 100 may provide the stylized image 114 or the plurality of stylized images for presentation on the user device. In some cases, the user device is a mobile device, and in these cases, the style migration neural network 112 is implemented on the mobile device. The neural network 112 is more suitable for implementation on a mobile device because it has fewer parameters and therefore requires less computing resources than a conventional style migration network.
FIG. 2 is an example diagram for training a style migration neural network (e.g., style migration neural network 112 of FIG. 1). For convenience, training of the style migration neural network will be described as being performed by a system of one or more computers located at one or more locations or subsystems of the system. For example, an appropriately programmed image style migration system (e.g., image style migration system 100 of FIG. 1) or a subsystem of an image style migration system (e.g., subsystem 110 of FIG. 1) may perform training.
The system provides an input image 202 and one or more stylistic images 204 to the stylistic migrating neural network 112. For each of the stylistic images 204, the stylistic migration neural network 112 may identify a corresponding index s for the style in which the stylistic image is depicted.
For each style image with index s, the style migration neural network 112 maintains a set of scaling and shifting condition instance normalization layer parameters (γ s ,γ s ) As training parameters. For each image style, the style migration neural network 112 generates a stylized image 206 from the input image 202 and an index s corresponding to the image style. The system then provides the input image 202, the stylized image 206, and the style image as inputs to the trained classifier 208. The trained classifier 208 includes a plurality of convolutional neural network layers, such as convolutional neural network layers 210-220. The trained classifier 208 may have been trained to perform any of a variety of conventional image processing tasks, such as image classification, object detection, etc., using conventional training techniques.
The trained classifier 208 is configured to process the input through each convolutional neural network layer to generate an intermediate representation of the input. The system uses the resulting intermediate representation to calculate content loss L c And style loss L s . In particular, content loss L c Representing the similarity between the content of the input image 202 and the content of the stylized image 206. Two images are similar in content if the high-level features of the two images extracted by the trained classifier 208 are close in euclidean distance. Content loss L is calculated using the first several intermediate representations generated by trained classifier 208 (e.g., intermediate representations generated by convolutional neural network layers 210-214) c . Style loss L S Representing the similarity between the style of the input image 202 and the style of the style image. If the low-level features of the two images extracted by the trained classifier 208 share the same statistics, or more specifically if the differences between the Gram matrices of features have a small Frobenius norm, the two images are similar in style. Wind is calculated using a back intermediate representation generated by the trained classifier 208 (e.g., intermediate representations generated by convolutional neural network layers 216-220)Lattice loss L s 。
The system then uses the content loss L c And style loss L S To form a training objective function as follows:
L(s,c)＝λ s L s (T(c,s))+λ c L c (T(c,s)),
wherein lambda is s And lambda (lambda) c Is style loss L s And content loss L c And T (c, s) is a stylized image 206 generated by the style migration neural network 112 using the input image c (202) and the image style s.
The system trains the style migration neural network 112 using standard training methods (e.g., gradient descent methods) to minimize training objective functions. For each image style s, the system sets a corresponding set of training parameters (γ s ,β s ) Stored, for example, in a table or other suitable data structure maintained, for example, by subsystem 110 of fig. 1, for future use during execution, for example, to determine corresponding parameter values for a given input style.
Because parameters in the style-migrating neural network 112 are shared between styles, the system can learn a new set of parameters (γ) by keeping the trained parameters fixed s′ ,β s′ ) To incorporate the new style s' into the training network 112. Thus, the system can handle multiple styles simultaneously without increasing the number of other training parameters, which results in faster training time and reduced memory space and system complexity.
FIG. 3 is a flow chart of an example process 300 for generating a stylized image from an input image and an input style. For convenience, process 300 will be described as being performed by a system of one or more computers located at one or more locations or by a subsystem of the system. For example, a suitably programmed image style migration system (e.g., image style migration system 100 of FIG. 1) or a subsystem of an image style migration system (e.g., subsystem 110 of FIG. 1) may perform process 300.
The system maintains data specifying respective parameter values for each image style in a set of image styles (step 302).
Each image style of the set of image styles is a style in which a particular image or a set of multiple particular images are depicted, e.g., a style in which a drawing is drawn, a style or particular manner in which a digital image is edited (e.g., using a grid pattern editing technique or other image editing technique), or a particular configuration of camera settings for capturing a particular image or multiple particular images. The respective parameter values for each image style have been determined by training a style migration neural network.
The system receives an input comprising an input image and data identifying an input style to be applied to the input image to generate a stylized image in the input style (step 304). In some implementations, the system can receive user input through a user interface that identifies a particular input style. In some implementations, the input style is a combination of two or more image styles from the set of image styles. In these embodiments, the input may specify a respective weight to be applied to each of the two or more image styles. In some cases, the system may receive user input defining respective weights.
The system determines parameter values for the input styles from the maintained data (step 306).
In particular, the system determines that the input style is a particular image style from the set of image styles and designates a parameter value of the particular image style as a parameter value of the input style.
In some implementations, when the input style is a combination of two or more image styles from the set of image styles, the system combines respective parameter values of the two or more image styles to determine the parameter value of the input style. When the input specifies a respective weight to be applied to each of the two or more image styles, the system combines the respective parameter values of the two or more image styles by weighting the respective parameter values of each of the two or more image styles with the weight to be applied to the image styles to determine the parameter value of the input style.
The system generates a stylized image by processing the input image using a style migration neural network configured to process the input image to generate a stylized image (step 308).
The style migration neural network includes one or more condition instance normalization layers. Each of the one or more condition instance normalization layers in the style migration neural network is located after a respective one of the neural networks (e.g., a convolutional neural network layer).
To stylize an input image in an input style, the system uses the determined parameter values of the input style to set current values of the condition instance normalization layer parameters for each condition instance normalization layer in the neural network. The system sets current values of parameters of one or more condition instance normalization layers without modifying parameter values of other neural network layers of the neural network.
If there is a single condition instance normalization layer in the neural network, the system sets the current value of the condition instance normalization layer parameter to the parameter value of the input style. If there are multiple condition instance normalization layers in the neural network, the parameter values of the input style include a respective subgroup corresponding to each condition instance normalization layer, and the system sets the current value of the condition instance normalization layer parameter for each layer to the corresponding subgroup of parameter values of the input style.
The style migration neural network is then configured to generate a stylized image by processing the input image via each neural network layer of the neural network when setting the current value of the condition instance normalization layer parameter to the determined parameter value of the input style.
During processing of the input image by the neural network, each condition instance normalization layer is configured to receive a layer output generated by a neural network layer preceding the condition instance normalization layer, and transform the layer output according to a current value of a condition instance normalization layer parameter to generate a condition instance normalization layer output, and provide the condition instance normalization layer output as an input to another neural network layer in the neural network.
To transform the layer output, the conditional instance normalization layer normalizes the layer output to generate a normalized layer output and transforms the normalized layer output according to the current value of the conditional instance normalization layer parameter to generate a conditional instance normalization layer output.
More specifically, the conditional instance normalization layer normalizes the layer outputs by determining, for each depth dimension of the layer outputs, a normalization statistic for the components of the first layer output in the spatial dimension of the first layer output and normalizing the components of the first layer output using the normalization statistic.
The conditional instance normalization layer then transforms the normalized layer output by scaling the normalized layer output according to the current value of the scaled conditional instance normalization layer parameter to generate a scaled normalized layer output and by shifting the scaled normalized layer output according to the current value of the shifted conditional instance normalization layer parameter to generate a conditional instance normalization layer output. The condition instance normalization layer output is then provided as an input to the next neural network layer for processing. This process continues until the neural network has processed the input image through all layers of the neural network to generate a stylized image.
In some implementations, the input may identify a video that includes a plurality of video frames, and the input image is a video frame from the video. In these embodiments, the system may be configured to generate a respective stylized image for each of the plurality of video frames in the video by applying the input style to each of the plurality of video frames using the style migration neural network in the same manner as described above, but the style migration neural network has been additionally trained to ensure that the stylized images of the plurality of video frames in the video have similar stylized. For example, the system may modify the objective function described above with reference to fig. 2, i.e., by adding constraints to the objective function to ensure that the stylized images of multiple video frames in the video have similar styles, and may train the style migration neural network to minimize the modified objective function.
After generating a stylized image of an input image or a plurality of stylized images of a plurality of video frames in a video, the system may optionally provide the stylized image or the plurality of stylized images for presentation on a user device (step 310). In some cases, the user device is a mobile device, and in these cases, the style migration neural network is implemented on the mobile device.
The term "configured" is used herein with respect to systems and computer program components. A system of one or more computers being configured to perform a particular operation or action means that the system has installed thereon software, firmware, hardware, or a combination thereof, which in operation causes the system to perform the operation or action. The one or more computer programs being configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware (including the structures disclosed in this specification and their structural equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and includes all kinds of apparatus, devices and machines for processing data, including for example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program can also be called or described as a program, software application, app, module, software module, script, or code, which can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located on one site or distributed across multiple sites and interconnected by a data communication network
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or combinations of, special purpose logic circuitry (e.g., an FPGA or ASIC) and one or more programmed computers.
A computer adapted to execute a computer program may be based on a general purpose or a special purpose microprocessor or both, or any other kind of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. In addition, a computer may be embedded in another device (e.g., a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device such as a Universal Serial Bus (USB) flash drive, to name a few).
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; and CD ROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on a user device in response to a request received from the web browser. Further, the computer may interact with the user by sending text messages or other forms of messages to a personal device (e.g., a smart phone running a messaging application) and receiving response messages from the user as replies.
The data processing means for implementing the machine learning model may also comprise, for example, dedicated hardware accelerator units for handling public and computationally intensive parts of machine learning training or production, i.e. inference, workload.
The machine learning model may be implemented and deployed using a machine learning framework, such as a TensorFlow framework, microsoft cognitive toolkit framework, apache Singa framework, or Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having an app through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data (e.g., HTML pages) to the user device, e.g., for the purpose of displaying data to and receiving user input from a user interacting with the device as a client. Data generated at the user device, e.g., results of a user interaction, may be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings and described in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (20)
1. A computer-implemented method, comprising:
maintaining, by the computing system, data specifying respective parameter values for each image style in the set of image styles;
in response to receiving, by the computing system, an input comprising an input image and data identifying the input style to be applied to the input image to generate a stylized image in an input style:
determining, by the computing system, a parameter value for the input style from the maintained data; and
generating, by the computing system, the stylized image by processing the input image using a style migration neural network configured to process the input image to generate the stylized image,
wherein the style migration neural network comprises a first neural network layer, a second neural network layer, and a conditional instance normalization layer between the first neural network layer and the second neural network layer, and
Wherein generating the stylized image comprises: in order to process the input image through the style migration neural network, the current value of the condition instance normalization layer parameter is set to the parameter value of the input style without modifying parameter values of the first and second neural network layers.
2. The method of claim 1, further comprising:
the stylized image is provided by the computing device for presentation to a user.
3. The method of claim 1, wherein the computing device is a mobile device, and wherein the style migration neural network is implemented on the mobile device.
4. The method of claim 1, wherein normalizing the first layer output to generate a normalized layer output comprises: for each depth dimension of the first layer output:
determining a normalized statistic of the components of the first layer output in the spatial dimension of the first layer output; and
the components of the first layer output are normalized using the normalization statistic.
5. The method of claim 4, wherein transforming the normalized layer output comprises:
Scaling the normalized layer output according to a current value of a scaling condition instance normalized layer parameter to generate a scaled normalized layer output; and
the scaled normalized layer output is shifted according to a current value of a shift condition instance normalized layer parameter to generate the condition instance normalized layer output.
6. The method of claim 1, wherein determining the parameter value of the input style from the maintained data comprises:
determining that the input style data identifies a single image style from the set of image styles; and
designating the parameter value of the single image style as the parameter value of the input style.
7. The method of claim 6, wherein receiving the input comprises:
user input identifying the single image style is received.
8. The method of claim 1, wherein the input style data identifies a combination of two or more image styles from the set of image styles, and wherein determining parameter values for the input styles from the maintained data comprises:
combining respective parameter values of the two or more image styles to determine a parameter value of the input style, wherein combining the respective parameter values includes weighting the respective parameter value of each of the two or more image styles with weights to be applied to the image styles.
9. The method of claim 8, wherein receiving the input comprises:
user input defining the respective weights is received.
10. The method of claim 1, wherein the respective parameter values for each image style have been determined by training the style migration neural network.
11. The method of claim 1, wherein the input identifies a video, and wherein the input image is a video frame from the video.
12. The method of claim 1, wherein a respective stylized image is generated for each of a plurality of video frames in the video by applying the input style to each video frame.
13. The method of claim 12, wherein the style migration neural network has been trained to ensure that stylized images of the plurality of video frames in the video have similar stylization.
14. One or more non-transitory computer-readable storage media storing instructions that, when executed by one or more computers of a computing system, cause the one or more computers to perform operations comprising:
maintaining data specifying respective parameter values for each image style in a set of image styles;
In response to receiving an input comprising an input image and data identifying an input style to be applied to the input image to generate a stylized image in the input style:
determining a parameter value for the input style from the maintained data; and
generating the stylized image by processing the input image using a style migration neural network configured to process the input image to generate the stylized image,
wherein the style migration neural network comprises a first neural network layer, a second neural network layer, and a conditional instance normalization layer between the first neural network layer and the second neural network layer, and
wherein generating the stylized image comprises: in order to process the input image through the style migration neural network, the current value of the condition instance normalization layer parameter is set to the parameter value of the input style without modifying parameter values of the first and second neural network layers.
15. A system implemented by one or more computers, the system comprising:
a style migration neural network configured to process an input image to generate a stylized image from the input image,
Wherein the style migration neural network comprises a first neural network layer, a second neural network layer, and a conditional instance normalization layer between the first neural network layer and the second neural network layer, and
a subsystem configured to perform operations comprising:
maintaining data specifying respective parameter values for each image style in a set of image styles;
receiving an input comprising an input image and data identifying an input style to be applied to the input image to generate a stylized image in an input style;
determining a parameter value for the input style from the maintained data;
setting the current value of the condition instance normalization layer parameter to the parameter value of the input style without modifying parameter values of the first and second neural network layers; and
the stylized image is generated by processing the input image using a style migration neural network configured to process the input image to generate the stylized image.
16. The system of claim 15, wherein the operations further comprise:
The stylized image is provided for presentation on a mobile device, and wherein the style migration neural network is implemented on the mobile device.
17. The system of claim 15, wherein normalizing a first layer output to generate the normalized layer output comprises: for each depth dimension of the first layer output:
determining a normalized statistic of the components of the first layer output in the spatial dimension of the first layer output; and
the components of the first layer output are normalized using the normalization statistic.
18. The system of claim 17, wherein transforming the normalized layer output comprises:
scaling the normalized layer output according to a current value of a scaling condition instance normalized layer parameter to generate a scaled normalized layer output; and
the scaled normalized layer output is shifted according to a current value of a shift condition instance normalized layer parameter to generate the condition instance normalized layer output.
19. The system of claim 15, wherein determining the parameter value of the input style from the maintained data comprises:
determining that the input style data identifies a single image style of a set of image styles; and
Designating the parameter value of the single image style as the parameter value of the input style.
20. The system of claim 15, wherein the input style data identifies a combination of two or more image styles from the set of image styles, and wherein determining parameter values for the input styles from the maintained data comprises:
combining respective parameter values of the two or more image styles to determine a parameter value of the input style, wherein combining the respective parameter values includes weighting the respective parameter value of each of the two or more image styles with weights to be applied to the image styles.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662411414P | 2016-10-21 | 2016-10-21 | |
US62/411,414 | 2016-10-21 | ||
PCT/US2017/057657 WO2018075927A1 (en) | 2016-10-21 | 2017-10-20 | Stylizing input images |
CN201780065307.1A CN109863537B (en) | 2016-10-21 | 2017-10-20 | Stylized input image |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780065307.1A Division CN109863537B (en) | 2016-10-21 | 2017-10-20 | Stylized input image |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116823593A true CN116823593A (en) | 2023-09-29 |
Family
ID=60269933
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202310738876.5A Pending CN116823593A (en) | 2016-10-21 | 2017-10-20 | Stylized input image |
CN201780065307.1A Active CN109863537B (en) | 2016-10-21 | 2017-10-20 | Stylized input image |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780065307.1A Active CN109863537B (en) | 2016-10-21 | 2017-10-20 | Stylized input image |
Country Status (4)
Country | Link |
---|---|
US (3) | US10535164B2 (en) |
EP (1) | EP3526770B1 (en) |
CN (2) | CN116823593A (en) |
WO (1) | WO2018075927A1 (en) |
Families Citing this family (46)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2018075927A1 (en) | 2016-10-21 | 2018-04-26 | Google Llc | Stylizing input images |
US10909657B1 (en) | 2017-09-11 | 2021-02-02 | Apple Inc. | Flexible resolution support for image and video style transfer |
CN109034382A (en) * | 2017-10-30 | 2018-12-18 | 上海寒武纪信息科技有限公司 | The recognition methods of scene or object and Related product |
US11244484B2 (en) * | 2018-04-23 | 2022-02-08 | Accenture Global Solutions Limited | AI-driven design platform |
CN108596830B (en) * | 2018-04-28 | 2022-04-22 | 国信优易数据股份有限公司 | Image style migration model training method and image style migration method |
JP7008845B2 (en) | 2018-05-02 | 2022-01-25 | コーニンクレッカ フィリップス エヌ ヴェ | Generation of simulation images for newborns |
CN108875787B (en) | 2018-05-23 | 2020-07-14 | 北京市商汤科技开发有限公司 | Image recognition method and device, computer equipment and storage medium |
CN108765278B (en) * | 2018-06-05 | 2023-04-07 | Oppo广东移动通信有限公司 | Image processing method, mobile terminal and computer readable storage medium |
KR102096388B1 (en) * | 2018-06-05 | 2020-04-06 | 네이버 주식회사 | Optimization for dnn conposition with real-time inference in mobile environment |
CN110827191A (en) * | 2018-08-08 | 2020-02-21 | Oppo广东移动通信有限公司 | Image processing method and device, storage medium and electronic equipment |
US20190362461A1 (en) * | 2018-08-10 | 2019-11-28 | Intel Corporation | Multi-object, three-dimensional modeling and model selection |
CN112585646A (en) * | 2018-09-03 | 2021-03-30 | 三星电子株式会社 | Method and system for performing editing operations on media |
US10789769B2 (en) | 2018-09-05 | 2020-09-29 | Cyberlink Corp. | Systems and methods for image style transfer utilizing image mask pre-processing |
CN112823379A (en) * | 2018-10-10 | 2021-05-18 | Oppo广东移动通信有限公司 | Method and device for training machine learning model and device for video style transfer |
CN109300170B (en) * | 2018-10-18 | 2022-10-28 | 云南大学 | Method for transmitting shadow of portrait photo |
CN109359687B (en) * | 2018-10-19 | 2020-11-24 | 百度在线网络技术（北京）有限公司 | Video style conversion processing method and device |
CN109766895A (en) * | 2019-01-03 | 2019-05-17 | 京东方科技集团股份有限公司 | The training method and image Style Transfer method of convolutional neural networks for image Style Transfer |
US11393144B2 (en) * | 2019-04-11 | 2022-07-19 | City University Of Hong Kong | System and method for rendering an image |
US20200364303A1 (en) * | 2019-05-15 | 2020-11-19 | Nvidia Corporation | Grammar transfer using one or more neural networks |
WO2020235862A1 (en) * | 2019-05-17 | 2020-11-26 | Samsung Electronics Co., Ltd. | Image manipulation |
US11367163B2 (en) * | 2019-05-31 | 2022-06-21 | Apple Inc. | Enhanced image processing techniques for deep neural networks |
EP3973497A4 (en) * | 2019-06-11 | 2022-08-03 | Guangdong Oppo Mobile Telecommunications Corp., Ltd. | Method, system, and computer-readable medium for stylizing video frames |
KR20210011162A (en) * | 2019-07-22 | 2021-02-01 | 삼성전자주식회사 | Display apparatus and control method thereof |
KR102248150B1 (en) * | 2019-09-27 | 2021-05-04 | 영남대학교 산학협력단 | Total style transfer with a single feed-forward network |
KR102127913B1 (en) * | 2019-10-29 | 2020-06-29 | 주식회사 루닛 | Method for Training Neural Network and Device Thereof |
US11704802B2 (en) | 2019-11-07 | 2023-07-18 | Accenture Global Solutions Limited | Multi-dimensional model merge for style transfer |
US11145042B2 (en) * | 2019-11-12 | 2021-10-12 | Palo Alto Research Center Incorporated | Using convolutional neural network style transfer to automate graphic design creation |
CN110909790A (en) * | 2019-11-20 | 2020-03-24 | Oppo广东移动通信有限公司 | Image style migration method, device, terminal and storage medium |
US11455552B2 (en) | 2019-11-22 | 2022-09-27 | Accenture Global Solutions Limited | Intelligent design platform using industrialized experience in product designs |
CN110956654B (en) * | 2019-12-02 | 2023-09-19 | Oppo广东移动通信有限公司 | Image processing method, device, equipment and storage medium |
US11080834B2 (en) * | 2019-12-26 | 2021-08-03 | Ping An Technology (Shenzhen) Co., Ltd. | Image processing method and electronic device |
US11790950B2 (en) * | 2020-01-14 | 2023-10-17 | Robert Salem Abraham | Film-making using style transfer |
KR20220128406A (en) * | 2020-03-01 | 2022-09-20 | 레이아 인코포레이티드 | Multiview style transition system and method |
US11586783B2 (en) | 2020-05-21 | 2023-02-21 | Accenture Global Solutions Limited | Intelligent design platform using digital assistants for design process support |
CN112101546B (en) * | 2020-09-01 | 2024-01-05 | 浙江大学 | Style migration method, apparatus, computer device and storage medium for generating diversified results |
CN112752147A (en) * | 2020-09-04 | 2021-05-04 | 腾讯科技（深圳）有限公司 | Video processing method, device and storage medium |
CN112381707B (en) * | 2020-11-02 | 2023-06-20 | 腾讯科技（深圳）有限公司 | Image generation method, device, equipment and storage medium |
CN112884636B (en) * | 2021-01-28 | 2023-09-26 | 南京大学 | Style migration method for automatically generating stylized video |
US11941771B2 (en) | 2021-02-03 | 2024-03-26 | Accenture Global Solutions Limited | Multi-dimensional model texture transfer |
CN113012082A (en) * | 2021-02-09 | 2021-06-22 | 北京字跳网络技术有限公司 | Image display method, apparatus, device and medium |
CN113012036B (en) * | 2021-03-05 | 2022-07-15 | 清华大学 | Human motion style migration method and system based on generative flow model |
EP4057228A1 (en) * | 2021-03-10 | 2022-09-14 | Mindtech Global Limited | Image domain matching technique |
CN113112580B (en) * | 2021-04-20 | 2022-03-25 | 北京字跳网络技术有限公司 | Method, device, equipment and medium for generating virtual image |
US11435885B1 (en) | 2021-06-10 | 2022-09-06 | Nvidia Corporation | User interfaces and methods for generating a new artifact based on existing artifacts |
CN113554549B (en) * | 2021-07-27 | 2024-03-29 | 深圳思谋信息科技有限公司 | Text image generation method, device, computer equipment and storage medium |
US11935154B2 (en) * | 2022-03-02 | 2024-03-19 | Microsoft Technology Licensing, Llc | Image transformation infrastructure |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5832119C1 (en) * | 1993-11-18 | 2002-03-05 | Digimarc Corp | Methods for controlling systems using control signals embedded in empirical data |
US8175617B2 (en) * | 2009-10-28 | 2012-05-08 | Digimarc Corporation | Sensor-based mobile search, related methods and systems |
KR101832693B1 (en) * | 2010-03-19 | 2018-02-28 | 디지맥 코포레이션 | Intuitive computing methods and systems |
US8625890B1 (en) * | 2011-10-17 | 2014-01-07 | Google Inc. | Stylizing geographic features in photographic images based on image content |
US20150324686A1 (en) * | 2014-05-12 | 2015-11-12 | Qualcomm Incorporated | Distributed model learning |
US9576351B1 (en) * | 2015-11-19 | 2017-02-21 | Adobe Systems Incorporated | Style transfer for headshot portraits |
EP3507773A1 (en) * | 2016-09-02 | 2019-07-10 | Artomatix Ltd. | Systems and methods for providing convolutional neural network based image synthesis using stable and controllable parametric models, a multiscale synthesis framework and novel network architectures |
WO2018075927A1 (en) | 2016-10-21 | 2018-04-26 | Google Llc | Stylizing input images |
-
2017
- 2017-10-20 WO PCT/US2017/057657 patent/WO2018075927A1/en active Search and Examination
- 2017-10-20 EP EP17795119.1A patent/EP3526770B1/en active Active
- 2017-10-20 CN CN202310738876.5A patent/CN116823593A/en active Pending
- 2017-10-20 CN CN201780065307.1A patent/CN109863537B/en active Active
-
2019
- 2019-04-10 US US16/380,010 patent/US10535164B2/en active Active
- 2019-11-12 US US16/681,391 patent/US11776167B2/en active Active
-
2023
- 2023-09-06 US US18/242,723 patent/US20230410389A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20200082578A1 (en) | 2020-03-12 |
EP3526770A1 (en) | 2019-08-21 |
WO2018075927A1 (en) | 2018-04-26 |
US20190236814A1 (en) | 2019-08-01 |
US11776167B2 (en) | 2023-10-03 |
US10535164B2 (en) | 2020-01-14 |
CN109863537B (en) | 2023-07-04 |
CN109863537A (en) | 2019-06-07 |
US20230410389A1 (en) | 2023-12-21 |
EP3526770B1 (en) | 2020-04-15 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109863537B (en) | Stylized input image | |
US11734572B2 (en) | Spatial transformer modules | |
US20230177343A1 (en) | Scene understanding and generation using neural networks | |
US10817805B2 (en) | Learning data augmentation policies | |
CN111279362B (en) | Capsule neural network | |
CN110574049B (en) | Multi-task multi-modal machine learning system | |
CN110268422B (en) | Device layout optimization with reinforcement learning | |
US20200279134A1 (en) | Using simulation and domain adaptation for robotic control | |
CN111386536A (en) | Semantically consistent image style conversion | |
CN110476173B (en) | Hierarchical device placement with reinforcement learning | |
US11257217B2 (en) | Image segmentation using neural networks | |
DE102022107186A1 (en) | GENERATOR UTILIZATION FOR DEEPFAKE DETECTION | |
EP4095758A1 (en) | Training large-scale vision transformer neural networks | |
DE102022106057A1 (en) | AUTHENTICATOR-INTEGRATED GENERATIVE ADVERSARIAL NETWORK (GAN) FOR SECURE DEEPFAKE GENERATION | |
DE102022128165A1 (en) | DATA PATH CIRCUIT DESIGN USING REINFORCEMENT LEARNING | |
US20220004849A1 (en) | Image processing neural networks with dynamic filter activation | |
US20240062046A1 (en) | Computer vision models using global and local information | |
DE102020122028A1 (en) | SELF-MONITORED HIERARCHICAL MOTION LEARNING FOR VIDEO ACTION DETECTION | |
US20240070816A1 (en) | Diffusion model image generation | |
WO2024058797A1 (en) | Visual prompt tuning for generative transfer learning |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |