US8788507B1 - Watermarking of structured results and watermark generation - Google Patents
Watermarking of structured results and watermark generation Download PDFInfo
- Publication number
- US8788507B1 US8788507B1 US13/296,451 US201113296451A US8788507B1 US 8788507 B1 US8788507 B1 US 8788507B1 US 201113296451 A US201113296451 A US 201113296451A US 8788507 B1 US8788507 B1 US 8788507B1
- Authority
- US
- United States
- Prior art keywords
- result
- structured
- alternative
- results
- hash
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/957—Browsing optimisation, e.g. caching or content distillation
- G06F16/9577—Optimising the visualization of content, e.g. distillation of HTML documents
Definitions
- Machine learning-based applications provide structured results to given inputs such as phrases to be machine translated, search queries, text and rich media for machine summarization, data to be processed, etc.
- the machine learning algorithms used in these applications often rely upon human-generated data, some of which may be drawn from public sources, such as the web.
- machine translation applications can rely upon human-generated text on the web as a source of parser training data.
- the implementation can receive a plurality of alternative structured results of a query and compute a function (such as a hash) of at least part of each structured result to produce a bit sequence.
- the implementation can compute a ranking score for each structured result based upon the detectability of a watermark (e.g., based upon one or more properties of the bit sequence) and the quality of the result.
- the implementation can select a structured result from among the alternatives based upon a ranking score that suitably balances watermark detectability and quality of the result.
- the selected result can be published and later tested to determine if it is a watermarked result.
- watermarking machine-generated output machine-generated content can be distinguished from human-generated content.
- FIG. 1 shows a flowchart for watermarking a structured result according to an embodiment of the disclosed subject matter.
- FIG. 2 shows a computer according to an embodiment of the disclosed subject matter.
- FIG. 3 shows a network configuration according to an embodiment of the disclosed subject matter.
- FIG. 4 shows a flowchart for detecting a watermark in a structured result according to an embodiment of the disclosed subject matter.
- FIG. 5 shows a system for watermarking a structured result according to an embodiment of the disclosed subject matter.
- FIG. 6 shows a system for detecting a watermark in a structured result according to an embodiment of the disclosed subject matter.
- Embodiments of the disclosed subject matter can be used to generate structured results for publication that contain a watermark that can be probabilistically identified.
- the watermark can be robust to local editing operations and can be manipulated to accommodate tradeoffs between the ability to identify the watermark and the quality of the watermarked structured results.
- a structured result can be an output that is generated in response to an input (such as a query, a phrase to be translated, content to be processed, etc.), where the output includes elements whose order and/or identity are important to the quality of the result.
- structured output include machine translations, automatic text and rich media summarizations, machine-generated search results, human-generated output such as candidate translations of a source text, etc.
- At least part of a structured result can be hashed to produce a bit string.
- a hash function can include any one-way procedure or mathematical function that converts a large, potentially variable-sized amount of data into a generally smaller datum, which is often of fixed size.
- the value returned by a hash function can be called a hash value or simply a hash. Examples of hashing techniques include MD-4, MD-5, SHA-1, etc.
- the watermark, hash function and bit sequence may be invisible to a viewer of the structured result. For example, a user who receives search results based on a user input search query may not detect or easily decipher a watermark, hash function or bit sequence associated with the watermark.
- a bit sequence is any sequence of zeros and ones.
- a hash value can be a bit sequence that may represent a numerical value in a binary form.
- the length of the bit sequence may be based on (without limitation) a numerical value, a predetermined length, the number of entries in an outputted structured result, etc., or a combination thereof.
- the implementation can receive several alternative structured results 310 in response to a given input.
- a search engine can return alternative sets of search results in response to a given query, or a machine-translation application can return a set of alternative candidate translations based upon a given input sentence.
- the alternative structured results may be ordered based on the estimated or actual quality.
- the alternative search results may be ordered by best match, view count, etc. or a combination thereof.
- a set of candidate translations may be ordered based on (without limitation) a ranking of the translation quality made by the machine-translation application, a k-best list of parsed translations, etc., or a combination thereof.
- a hash can be computed 320 for at least part of each of the alternative structured results to produce a bit sequence for each structured result. For example, if a search query results in one hundred ordered elements (such as links to resources on the Web), then a hash may be computed for the twentieth to the fortieth element, the third to the ninth and the fifty first to the sixty third, etc., or for the whole list of elements in its entirety.
- a structured result can be divided into a plurality of sub-results or the purpose of producing a bit sequence.
- the sub-results can be selected based on, but not limited to, a random selection, a predetermined algorithm, a percentage of the number of results, etc. or a combination thereof.
- a sub-result hash value may be calculated for each of the plurality of sub-results. As shown in Table 2, a search query resulting in a hundred results is divided into four equal parts, each sub-result containing twenty-five search results. A sub-result hash value is calculated for each of the four sub-result groups.
- Sub-Result Hash Value 1 1-25 1100 2 26-50 1001 3 51-75 1000 4 76-100 1111
- the hash values shown above have been shortened for the purpose of illustration. The actual hash values are likely to be longer.
- a bit sequence for the structured result can be generated by concatenating the bit sequences of the sub-results.
- a bit sequence for the structured result of Table 2 may be calculated by concatenating the sub-result hash values for all four sub-results. Accordingly, a hash value for the structured result can be 1100100110001111.
- Dividing a structured result into sub-results, generating sub-result hash values and concatenating the hash values allows for robust hash value. If a portion of the structured result is modified (e.g., by an editing operation), the impact to the structured result's bit sequence is lower than if the implementation produces a bit sequence based on a single hash of the whole result. For example, according to the search results of Table 2, if a change is made to ordered search results 9-12 then the sub-result hash value for the second sub-result group may change to a different value than 1100. If it changes to 0101, only the first four bits of the structured result hash value change such that the modified structured result hash value is 0101100110001111. In contrast, if the structured result is not divided into sub-groups, the entire hash value can change drastically based upon even a small change in one part of the structured result.
- a hash function whose output bits are independent (do not rely on adjacent bits) can be used according to embodiments of the disclosed subject matter.
- a watermark can be generated by selecting an alternative structured result having a hash that exhibits one or more properties that are unlikely under a given probability distribution.
- bits sequence can be generated by functions other than a hash function and may be selected in view of probability distributions other than the binomial distribution.
- a ranking score 330 for at least some of the structured results can be computed based upon a combination of the detectability of a watermark within a structured result and the quality of the structured result.
- a structured result of a plurality of structured results may be selected based on the ranking score 340 .
- the detectability of a watermark can be measured by a gain in the watermarking signal and is higher for structured results that are more detectable.
- a gain in the watermarking signal may be based on (without limitation) the number of occurrences of an element in a hash value, the ratio of a first element to a second element in a hash value, the number of consecutive similar elements in a hash value, etc. or a combination thereof.
- the implementation may create a watermark by selecting an alternative structured result or altering a structured result.
- the loss in quality should not exceed a maximum loss in quality threshold.
- a search engine can provide several possible alternative results for a given query and rank the results from highest to lowest quality.
- the watermark can be created by selecting the highest quality result that meets a minimum detectability measure.
- a search engine can provide several possible alternative results for a given query and rank the results from highest to lowest detectability.
- the watermark can be created by selecting the most detectable result that meets a minimum quality measure.
- each result can contain several elements.
- the alternative results can vary in terms of their content and/or the ordering of elements. Quality can be related to the completeness, correctness and/or accuracy of the result in relation to the query.
- Such a selected structured result is detectable and can be said to have a high gain in the watermarking signal.
- the selected structured result may be more detectable as a watermark than the highest quality result, while the quality of the selected result may be sufficiently good to publish.
- the loss in quality may be a number, a percentage, a ranking, a rating, etc. and may be based on, but is not limited to, a task specific quality metric, relevance of the order of the elements in the structured result, accuracy, popularity, view count, relevance, the deviation from an optimal structured result, etc. or a combination thereof.
- the implementation may produce a watermark by selecting an alternative structured result or altering a structured result given that the loss in quality does not exceed a maximum loss in quality threshold. For example, as shown in Table 3, a search query results in search result Set 1 (optimal search result set) and alternatively search result Set 2, Set 3 and Set 4.
- the maximum loss in quality threshold is 50%.
- the loss in quality corresponding to search result Set 1, Set 2, and Set 3 are 0%, 4%, and 29% respectively. Accordingly, Sets 1, 2, and 3 are below the maximum loss in quality threshold of 50% and thus are approved for being used as watermarked sets.
- Set 4 has a loss in quality of 54%, which exceeds the maximum loss in quality threshold of 50% and thus is not approved for being used as a watermarked set.
- a structured result that is selected to be a watermark can be made publicly available.
- the structured result that is selected as the watermarked result can be made available via (without limitation) an Internet based protocol, a World Wide Web browser, a network interface, a personal computer, a mobile computer, a mobile tablet, a software application, etc. or a combination thereof.
- r ′ ar ⁇ g ⁇ ⁇ max r ⁇ D k ⁇ ( q ) ⁇ w ⁇ ( r , D k , ⁇ ( q ) , h ) Equation ⁇ ⁇ 1 where w ranks r ⁇ D k (q) based on the presentation of a watermarking signal by r, where the watermarking signal is computed using a function h, which can be a hashing operation.
- w ⁇ ( r , D K ⁇ ( q ) , h ) # ⁇ ( 1 , h ⁇ ( r ) ) ⁇ h ⁇ ( r ) ⁇ Equation ⁇ ⁇ 2
- h(r) can be a function (such as a hash function) that returns a bit sequence for result r and #(x,y) counts the number of occurrences of x in sequence y.
- the strength of a watermarking signal within a structured result can be estimated.
- a bit sequence hash value for the structured result can be calculated 420 by applying a known hash function.
- the probability that this null hypothesis is true for a given bit sequence can be expressed as a p-value.
- the likelihood that a bit sequence having n bits will have more ones than a given number x is:
- the p-value can be compared against a desired significance level ⁇ , and the null hypothesis can be rejected for sequences for which the p-value is less than ⁇ . Comparing this p-value against the desired significance level ⁇ , the null hypothesis can be rejected to results that have P n (X ⁇ x) ⁇ .
- the implementation can define a watermarking approach that exhibits a fixed false positive rate and a task-independent hashing and selection criteria.
- Implementations in accordance with the described subject matter can be robust to edit operations. Without a robustness feature, even slight modifications to the elements within an item r would yield completely different bit sequences (e.g., made using a hash function) that may no longer preserve the biases introduced by the watermark selection function.
- an implementation can map individual results into a set of sub-results, each one representing some local structure of r. Function h can then be applied to each sub-result, which can be concatenated to represent r. The mapping can be defined as a component of the h operation. While a particular edit operation may affect a small number of sub-results, the majority of the bits in the concatenated bit sequence for r may remain untouched, thereby limiting the damage to the biases selected during watermarking.
- , p 0.5).
- the null hypothesis is based on the assumption that collections of results draw uniformly from the space of possible results. This assumption may not always hold and depends on the types of the results and collection. For example, considering a text document as a collection of sentences we can expect that some sentences may repeat more frequently than others. This scenario can be even more likely when applying a mapping into sub-results. n-gram sequences follow long-tailed or Zipfian distributions, with a small number of n-grams contributing heavily toward the total number of n-grams in a document.
- a random hash function can guarantee that inputs are distributed uniformly at random over the output range. However, the same input will be assigned the same output deterministically. Therefore if the distribution of inputs is heavily skewed to certain elements of the input space, the output distribution will not be uniformly distributed.
- the bit sequences resulting from the high frequency sub-results have the potential to generate inherently biased distributions when accumulated at the collection level.
- a mapping can be chosen that tends towards generating uniformly from the space of sub-results. The quality of a sub-result mapping for a specific task can be measured by computing the false positive rate on non-watermarked collections. For a given significance level ⁇ , a mapping can result in false positive rates close to ⁇ as well.
- the results in this group can be expected to share several sub-results.
- the group can be the highest quality search results in response to a query. It can be expected that these results share many of the same elements.
- D k (q) represents the top results of a dynamic programming algorithm within an exponential search space.
- alternative translations of the same input sentence are likely to share most of their n-grams, making it difficult to generate bit sequences that are significantly different with respect to the first best result. If the first best result has a negative signal (e.g., more 0s than 1s), it could be difficult to find a result that does not dilute the collection bias.
- This selection criteria promotes skewed bit sequences on a per result level, regardless of direction, i.e., more 0s or more 1s.
- a non-watermarked collection can be one whose individual results each generate bit sequences with approximately equal numbers of 1s and 0s.
- the corresponding statistical test uses the following p-value defined over the bits generated for C N . Rather than considering the probability of the accumulated bit sequence, we retain the boundaries between the bits generated for each result r i .
- the watermarking described in Equation 3 chooses alternative results on a per result basis, with the goal of influencing collection level bit sequences.
- the selection criteria as described will choose the most biased candidates available in D k (q).
- the parameter k determines the extent to which lesser quality alternatives can be chosen. If all of the alternatives in each D k (q) are of relatively similar quality, the minimal degradation will ordinarily occur due to watermarking.
- the loss (r, D k (q)) reflects the quality degradation that results from selecting alternative r as opposed to the best-ranked candidate in D k (q). For example,
- w ⁇ ( r , D K ⁇ ( q ) , h ) # ⁇ ( 1 , h ⁇ ( r ) ) ⁇ h ⁇ ( r ) ⁇ Equation ⁇ ⁇ 2
- rank(r) returns the rank of r within D k (q)
- cost(r) represents a weighted sum of features (not normalized over the search space) in a log-linear model
- r 1 is the highest ranked alternative in D k (q).
- loss rank provides a generally applicable criteria to select alternatives, penalizing selection from deep within D k (q). This estimate of the quality degradation does not reflect the generating model's opinion on relative quality.
- loss cost considers the relative increase in the generating model's cost assigned to the alternative translation.
- the gain(r, D k (q), ⁇ w ) function represents the gain in the watermarking signal by selecting candidate r.
- FIG. 2 is an example computer 20 suitable for implementing embodiments of the presently disclosed subject matter.
- the computer 20 includes a bus 21 which interconnects major components of the computer 20 , such as a central processor 24 , a memory 27 (typically RAM, but which may also include ROM, flash RAM, or the like), an input/output controller 28 , a user display 22 , such as a display screen via a display adapter, a user input interface 26 , which may include one or more controllers and associated user input devices such as a keyboard, mouse, and the like, and may be closely coupled to the I/O controller 28 , fixed storage 23 , such as a hard drive, flash storage, Fibre Channel network, SAN device, SCSI device, and the like, and a removable media component 25 operative to control and receive an optical disk, flash drive, and the like.
- a bus 21 which interconnects major components of the computer 20 , such as a central processor 24 , a memory 27 (typically RAM, but which may also include ROM, flash
- the bus 21 allows data communication between the central processor 24 and the memory 27 , which may include read-only memory (ROM) or flash memory (neither shown), and random access memory (RAM) (not shown), as previously noted.
- the RAM is generally the main memory into which the operating system and application programs are loaded.
- the ROM or flash memory can contain, among other code, the Basic Input-Output system (BIOS) that controls basic hardware operation such as the interaction with peripheral components.
- BIOS Basic Input-Output system
- Applications resident with the computer 20 are generally stored on and accessed via a computer readable medium, such as a hard disk drive (e.g., fixed storage 23 ), an optical drive, floppy disk, or other storage medium 25 .
- a network interface 29 may provide a direct connection to a remote server via a telephone link, to the Internet via an Internet Service Provider (ISP), or a direct connection to a remote server via a direct network link to the Internet via a POP (point of presence) or other technique.
- the network interface 29 may provide such connection using wireless techniques, including digital cellular telephone connection, Cellular Digital Packet Data (CDPD) connection, digital satellite data connection or the like.
- CDPD Cellular Digital Packet Data
- the network interface 29 may allow the computer to communicate with other computers via one or more local, wide-area, or other networks, as shown in FIG. 3 .
- FIG. 2 Many other devices or components (not shown) may be connected in a similar manner (e.g., document scanners, digital cameras and so on). Conversely, all of the components shown in FIG. 2 need not be present to practice the present disclosure. The components can be interconnected in different ways from that shown. The operation of a computer such as that shown in FIG. 2 is readily known in the art and is not discussed in detail in this application. Code to implement the present disclosure can be stored in computer-readable storage media such as one or more of the memory 27 , fixed storage 23 , removable media 25 , or on a remote storage location.
- FIG. 3 shows an example network arrangement according to an embodiment of the disclosed subject matter.
- One or more clients 10 , 11 such as local computers, smart phones, tablet computing devices, and the like may connect to other devices via one or more networks 7 .
- the network may be a local network, wide-area network, the Internet, or any other suitable communication network or networks, and may be implemented on any suitable platform including wired and/or wireless networks.
- the clients may communicate with one or more servers 13 and/or databases 15 .
- the devices may be directly accessible by the clients 10 , 11 , or one or more other devices may provide intermediary access such as where a server 13 provides access to resources stored in a database 15 .
- the clients 10 , 11 also may access remote platforms 17 or services provided by remote platforms 17 such as cloud computing arrangements and services.
- the remote platform 17 may include one or more servers 13 and/or databases 15 .
- various embodiments of the presently disclosed subject matter may include or be embodied in the form of computer-implemented processes and apparatuses for practicing those processes.
- Embodiments also may be embodied in the form of a computer program product having computer program code containing instructions embodied in non-transitory and/or tangible media, such as floppy diskettes, CD-ROMs, hard drives, USB (universal serial bus) drives, or any other machine readable storage medium, wherein, when the computer program code is loaded into and executed by a computer, the computer becomes an apparatus for practicing embodiments of the disclosed subject matter.
- Embodiments also may be embodied in the form of computer program code, for example, whether stored in a storage medium, loaded into and/or executed by a computer, or transmitted over some transmission medium, such as over electrical wiring or cabling, through fiber optics, or via electromagnetic radiation, wherein when the computer program code is loaded into and executed by a computer, the computer becomes an apparatus for practicing embodiments of the disclosed subject matter.
- the computer program code segments configure the microprocessor to create specific logic circuits.
- a set of computer-readable instructions stored on a computer-readable storage medium may be implemented by a general-purpose processor, which may transform the general-purpose processor or a device containing the general-purpose processor into a special-purpose device configured to implement or carry out the instructions.
- Embodiments may be implemented using hardware that may include a processor, such as a general purpose microprocessor and/or an Application Specific Integrated Circuit (ASIC) that embodies all or part of the techniques according to embodiments of the disclosed subject matter in hardware and/or firmware.
- the processor may be coupled to memory, such as RAM, ROM, flash memory, a hard disk or any other device capable of storing electronic information.
- the memory may store instructions adapted to be executed by the processor to perform the techniques according to embodiments of the disclosed subject matter.
- Embodiments of the disclosed subject matter can be implemented such that the watermarking selection process is performed by one entity and the watermark verification process is performed by a second, distinct entity.
- user 501 sends a query to search engine 502 , which generates sets of alternative results.
- a watermark server 504 receives the sets of alternative results 505 .
- the watermark server 504 computes a hash 506 and computes a ranking of the results 507 .
- the watermark server 504 selects a watermarked result from the set of alternative results 508 and returns it to the search engine, which in turn can return the watermarked result to the querying user 501 .
- This example is meant to be illustrative and not limiting.
- the watermarking server 504 can be used to select a watermarked result from any structured results, including translations, automatic text and rich media summarization, etc. This example computes a hash of the results 506 , but any suitable function that yields a bit sequence can be used.
- user 601 sends a search result to a watermark verification service 602 .
Abstract
Description
TABLE 2 | ||
Sub-Result Group | Ordered Search Results | Sub-Result Hash Value |
1 | 1-25 | 1100 |
2 | 26-50 | 1001 |
3 | 51-75 | 1000 |
4 | 76-100 | 1111 |
The hash values shown above have been shortened for the purpose of illustration. The actual hash values are likely to be longer. A bit sequence for the structured result can be generated by concatenating the bit sequences of the sub-results. For example, a bit sequence for the structured result of Table 2 may be calculated by concatenating the sub-result hash values for all four sub-results. Accordingly, a hash value for the structured result can be 1100100110001111. Dividing a structured result into sub-results, generating sub-result hash values and concatenating the hash values allows for robust hash value. If a portion of the structured result is modified (e.g., by an editing operation), the impact to the structured result's bit sequence is lower than if the implementation produces a bit sequence based on a single hash of the whole result. For example, according to the search results of Table 2, if a change is made to ordered search results 9-12 then the sub-result hash value for the second sub-result group may change to a different value than 1100. If it changes to 0101, only the first four bits of the structured result hash value change such that the modified structured result hash value is 0101100110001111. In contrast, if the structured result is not divided into sub-groups, the entire hash value can change drastically based upon even a small change in one part of the structured result.
TABLE 3 | |||
Loss in | Quality Approval | ||
Quality | (Maximum Loss in | ||
Search Result Set | (%) | Quality Threshold = 50%) | |
Set 1 (Optimal Set) | 0 | Yes | |
Set 2 (Alternative Set) | 4 | Yes | |
Set 3 (Alternative Set) | 29 | Yes | |
Set 4 (Alternative Set) | 54 | No | |
where w ranks rεDk(q) based on the presentation of a watermarking signal by r, where the watermarking signal is computed using a function h, which can be a hashing operation.
where h(r) can be a function (such as a hash function) that returns a bit sequence for result r and #(x,y) counts the number of occurrences of x in sequence y. Parameter w can be used as a biasing criteria to measure how skewed a given bit sequence is away from the p=0.5 binomial distribution.
where p=0.5. The p-value can be compared against a desired significance level α, and the null hypothesis can be rejected for sequences for which the p-value is less than α. Comparing this p-value against the desired significance level α, the null hypothesis can be rejected to results that have Pn(X≧x)<α. For example, if one wishes to have a 94% confidence that a given sequence has been produced by a watermarking algorithm rather than having been generated randomly, one would require the sequence to have a p-value less than 0.06. This further implies a possible false positive rate of about six out of every 100 sequences. That is, the test will falsely indicate that a sequence has been produced using a watermarking algorithm about six out of every hundred sequences tested, when in fact those six sequences were not watermarked. The parameter α can be controlled on an application-specific basis. By biasing the selection of candidate results to produce more 1s than 0s (or vice versa), the implementation can define a watermarking approach that exhibits a fixed false positive rate and a task-independent hashing and selection criteria.
w(r,D k(q),h)=P n(X≧#(1,h(r)) Equation 5
where Pn provides probabilities from binomial (n=|h(r)|, p=0.5).
y i=max(#1,h(r i)),#(0,h(r i)))
where the δ<(x,y) returns x if x<y. This sums only those events that have lower probabilities than c1 . . . cN. Computing this probability is exponential to N. We can optimize its computation using principles from a K-best extraction algorithm. By computing the highest probability elements of the sum first, the computation can be terminated if the running sum exceeds α. Additional approximations, such as selecting samples from CN, can also be applied to determine whether P(CN>c1 . . . cN)>α. This alternative biasing method and classification function can increase the likelihood of finding an unlikely bit sequence at a higher rank from Dk(q).
w(r,D k(q),ƒw)=λ*gain(r,D k(q),ƒw)−(1−λ)*loss(r,D k(q)) Equation 9
The loss (r, Dk(q)) reflects the quality degradation that results from selecting alternative r as opposed to the best-ranked candidate in Dk(q). For example,
where rank(r) returns the rank of r within Dk(q), cost(r) represents a weighted sum of features (not normalized over the search space) in a log-linear model and r1 is the highest ranked alternative in Dk(q). lossrank provides a generally applicable criteria to select alternatives, penalizing selection from deep within Dk(q). This estimate of the quality degradation does not reflect the generating model's opinion on relative quality. losscost considers the relative increase in the generating model's cost assigned to the alternative translation.
Claims (16)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/296,451 US8788507B1 (en) | 2011-03-30 | 2011-11-15 | Watermarking of structured results and watermark generation |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201161469418P | 2011-03-30 | 2011-03-30 | |
US13/296,451 US8788507B1 (en) | 2011-03-30 | 2011-11-15 | Watermarking of structured results and watermark generation |
Publications (1)
Publication Number | Publication Date |
---|---|
US8788507B1 true US8788507B1 (en) | 2014-07-22 |
Family
ID=51178018
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/296,460 Active 2032-07-20 US8812517B1 (en) | 2011-03-30 | 2011-11-15 | Watermarking of structured results and watermark detection |
US13/296,451 Active 2032-08-06 US8788507B1 (en) | 2011-03-30 | 2011-11-15 | Watermarking of structured results and watermark generation |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/296,460 Active 2032-07-20 US8812517B1 (en) | 2011-03-30 | 2011-11-15 | Watermarking of structured results and watermark detection |
Country Status (1)
Country | Link |
---|---|
US (2) | US8812517B1 (en) |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6397234B1 (en) * | 1999-08-20 | 2002-05-28 | The United States Of America As Represented By The Secretary Of The Navy | System and apparatus for the detection of randomness in time series distributions made up of sparse data sets |
US20100169293A1 (en) * | 2008-12-30 | 2010-07-01 | International Business Machines Corporation | Search engine service utilizing hash algorithms |
-
2011
- 2011-11-15 US US13/296,460 patent/US8812517B1/en active Active
- 2011-11-15 US US13/296,451 patent/US8788507B1/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6397234B1 (en) * | 1999-08-20 | 2002-05-28 | The United States Of America As Represented By The Secretary Of The Navy | System and apparatus for the detection of randomness in time series distributions made up of sparse data sets |
US20100169293A1 (en) * | 2008-12-30 | 2010-07-01 | International Business Machines Corporation | Search engine service utilizing hash algorithms |
Non-Patent Citations (7)
Title |
---|
"Electronic Marking and Identification Techniques to Discourage Document Copying" J. Brassil, S,Low, N. Maxemchuk, L. O'Gorman 1994-AT&T Bell Laboratories. * |
"Electronic Marking and Identification Techniques to Discourage Document Copying" J. Brassil, S,Low, N. Maxemchuk, L. O'Gorman 1994—AT&T Bell Laboratories. * |
"Watermarking relational data: framework, algorithms and analysis" Rakesh Agrawal, Peter J. Haas, Jerry Kiernan Jul. 10, 2003-The VLDB Journal. * |
"Watermarking relational data: framework, algorithms and analysis" Rakesh Agrawal, Peter J. Haas, Jerry Kiernan Jul. 10, 2003—The VLDB Journal. * |
Agrawal, R. et al., "Watermarking relational data: framework, algorithms and analysis," The VLDB Journal (2003), vol. 12, pp. 157-169. |
Brassil, J. et al., "Electronic Marking and Identification Technique to Discourage Document Copying," AT&T Bell Laboratories, IEEE 1994, pp. 1278-1287. |
U.S. Office Action dated Feb. 27, 2014 for co-pending related U.S. Appl. No. 13/296,460 (10 pages). |
Also Published As
Publication number | Publication date |
---|---|
US8812517B1 (en) | 2014-08-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
WO2020182019A1 (en) | Image search method, apparatus, device, and computer-readable storage medium | |
US11301637B2 (en) | Methods, devices, and systems for constructing intelligent knowledge base | |
US20210118431A1 (en) | Intelligent short text information retrieve based on deep learning | |
US8401314B2 (en) | Systems and methods for character correction in communication devices | |
US10289957B2 (en) | Method and system for entity linking | |
JP5984917B2 (en) | Method and apparatus for providing suggested words | |
US20140052688A1 (en) | System and Method for Matching Data Using Probabilistic Modeling Techniques | |
US8037069B2 (en) | Membership checking of digital text | |
US20190384809A1 (en) | Methods and systems for providing universal portability in machine learning | |
US9317606B1 (en) | Spell correcting long queries | |
CN107885717B (en) | Keyword extraction method and device | |
CN109670153B (en) | Method and device for determining similar posts, storage medium and terminal | |
CN111930949B (en) | Search string processing method and device, computer readable medium and electronic equipment | |
CN112527967A (en) | Text matching method, device, terminal and storage medium | |
JP5179564B2 (en) | Query segment position determination device | |
US10754880B2 (en) | Methods and systems for generating a replacement query for a user-entered query | |
US8788507B1 (en) | Watermarking of structured results and watermark generation | |
US20180225291A1 (en) | Identifying Documents | |
US20200192922A1 (en) | System and method for adaptively adjusting related search words | |
CN112364135B (en) | Object pushing method, device, equipment and storage medium based on multi-source data | |
KR102351264B1 (en) | Method for providing personalized information of new books and system for the same | |
WO2022053018A1 (en) | Text clustering system, method and apparatus, and device and medium | |
CN109190115B (en) | Text matching method, device, server and storage medium | |
TW202109349A (en) | Communications server apparatus, communications device(s) and methods of operation thereof | |
CN114385777A (en) | Text data processing method and device, computer equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:VENUGOPAL, ASHISH;GANITKEVIC, JURIJ;OCH, FRANZ JOSEF;AND OTHERS;SIGNING DATES FROM 20111108 TO 20111117;REEL/FRAME:027325/0386 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044277/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |