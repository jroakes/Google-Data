CN116324829A - Scroller interface for transcription navigation - Google Patents
Scroller interface for transcription navigation Download PDFInfo
- Publication number
- CN116324829A CN116324829A CN202180066430.1A CN202180066430A CN116324829A CN 116324829 A CN116324829 A CN 116324829A CN 202180066430 A CN202180066430 A CN 202180066430A CN 116324829 A CN116324829 A CN 116324829A
- Authority
- CN
- China
- Prior art keywords
- text
- tag
- model
- transcription
- sections
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000035897 transcription Effects 0.000 title claims abstract description 88
- 238000013518 transcription Methods 0.000 title claims abstract description 87
- 238000000034 method Methods 0.000 claims abstract description 66
- 230000006870 function Effects 0.000 claims description 22
- 238000010801 machine learning Methods 0.000 claims description 19
- 230000004931 aggregating effect Effects 0.000 claims description 9
- 230000004044 response Effects 0.000 claims description 9
- 238000001914 filtration Methods 0.000 claims description 6
- 230000001419 dependent effect Effects 0.000 claims description 2
- 230000008569 process Effects 0.000 description 16
- 238000004891 communication Methods 0.000 description 10
- 230000011278 mitosis Effects 0.000 description 9
- 230000002776 aggregation Effects 0.000 description 6
- 238000004220 aggregation Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 230000033001 locomotion Effects 0.000 description 6
- 238000012545 processing Methods 0.000 description 5
- 230000010365 information processing Effects 0.000 description 4
- 230000003993 interaction Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 230000005540 biological transmission Effects 0.000 description 3
- 238000012552 review Methods 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 238000013528 artificial neural network Methods 0.000 description 2
- 238000013478 data encryption standard Methods 0.000 description 2
- 238000013136 deep learning model Methods 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 238000003058 natural language processing Methods 0.000 description 2
- 230000011218 segmentation Effects 0.000 description 2
- 244000141359 Malus pumila Species 0.000 description 1
- 241000699666 Mus <mouse, genus> Species 0.000 description 1
- 241000699670 Mus sp. Species 0.000 description 1
- 235000021016 apples Nutrition 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000032823 cell division Effects 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 239000000470 constituent Substances 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 238000006073 displacement reaction Methods 0.000 description 1
- 230000009977 dual effect Effects 0.000 description 1
- 238000005538 encapsulation Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 230000005057 finger movement Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000007689 inspection Methods 0.000 description 1
- 238000002955 isolation Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 230000021121 meiosis Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 239000000779 smoke Substances 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/0485—Scrolling or panning
- G06F3/04855—Interaction with scrollbars
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
Abstract
A method comprising: at a computing device, a transcription of an audio recording is received, wherein the transcription is partitioned into a plurality of text sections. The method additionally comprises: one or more tags are associated with each text section of the plurality of text sections, wherein each associated tag is extracted from the transcribed text. The method further comprises the steps of: a subset of the plurality of text sections is selected based at least on one or more tags associated with each of the plurality of text sections. The method further comprises the steps of: a representation of the transcription and a scroller interface are provided for display on the computing device, wherein the scroller interface allows navigation to each text segment in the subset based on user navigation of a tag associated with the text segment.
Description
Cross Reference to Related Applications
The present application claims priority from U.S. patent application Ser. No.63/084,800, filed on 9/29/2020, the contents of which are incorporated herein by reference.
Many modern computing devices, including mobile phones, personal computers, and tablet computers, provide a Graphical User Interface (GUI) to allow a user to interact with the computing device. For example, an application can use a GUI to communicate with a user using images, text, and graphical elements (such as windows, dialog boxes, pop-up windows, images, buttons, scroll bars, and icons). The GUI can also receive input from user interface devices such as touch screens, computer mice, keyboards, and other user interface devices to allow a user to control the GUI and thereby the application.
One such application may cause a computing device to record audio content, such as spoken words. The application may then display a transcription of the recorded audio content to allow the user to view a textual representation of the spoken word. The application may include a GUI that allows the user to navigate the transcription.
Disclosure of Invention
The present disclosure includes systems and methods that provide a scroller interface to facilitate user navigation of a transcription of recorded audio content.
In a first aspect, a method is provided. The method comprises the following steps: at a computing device, a transcription of an audio recording is received, wherein the transcription is partitioned into a plurality of text sections. The method additionally comprises: one or more tags are associated with each text section of the plurality of text sections, wherein each associated tag is extracted from the transcribed text. The method further comprises the steps of: a subset of the plurality of text sections is selected based at least on one or more tags associated with each of the plurality of text sections. The method additionally comprises: a representation of the transcription and a scroller interface are provided for display on the computing device, wherein the scroller interface allows navigation to each text segment in the subset based on user navigation of a tag associated with the text segment.
In a second aspect, a computing device is provided. The computing device includes one or more processors and a non-transitory computer readable medium including program instructions executable by the one or more processors to perform functions. The functions include receiving a transcription of an audio recording, wherein the transcription is divided into a plurality of text sections. The functions additionally include: one or more tags are associated with each text section of the plurality of text sections, wherein each associated tag is extracted from the transcribed text. The functions further include: a subset of the plurality of text sections is selected based at least on one or more tags associated with each of the plurality of text sections. The functions additionally include: a representation of the transcription and a scroller interface are provided for display on the computing device, wherein the scroller interface allows navigation to each text segment in the subset based on user navigation of a tag associated with the text segment.
In a third aspect, a non-transitory computer readable medium is provided that includes program instructions executable by one or more processors to cause the one or more processors to perform functions. The functions include receiving a transcription of an audio recording, wherein the transcription is divided into a plurality of text sections. The functions additionally include: one or more tags are associated with each text section of the plurality of text sections, wherein each associated tag is extracted from the transcribed text. The functions further include: a subset of the plurality of text sections is selected based at least on one or more tags associated with each of the plurality of text sections. The functions additionally include: a representation of the transcription and a scroller interface are provided for display on the computing device, wherein the scroller interface allows navigation to each text segment in the subset based on user navigation of a tag associated with the text segment.
In a fourth aspect, a system is provided that includes means for receiving a transcription of an audio recording, wherein the transcription is divided into a plurality of text sections. The system additionally includes means for associating one or more tags with each text segment of the plurality of text segments, wherein each associated tag is extracted from the transcribed text. The system also includes means for selecting a subset of the plurality of text sections based at least on one or more tags associated with each of the plurality of text sections. The system additionally includes means for providing a representation of the transcription and a scroller interface for display on the computing device, wherein the scroller interface allows navigation to each text segment in the subset based on user navigation of a tag associated with the text segment.
Other aspects, embodiments, and implementations will become apparent to those of ordinary skill in the art upon review of the following detailed description, with appropriate reference to the accompanying drawings.
Drawings
1A, 1B, 1C, 1D, and 1E illustrate a series of views of a GUI of a computing device according to an example embodiment.
Fig. 2 is a block diagram of a system architecture according to an example embodiment.
Fig. 3 is a block diagram of a method according to an example embodiment.
FIG. 4 is a functional block diagram of a computing device according to an example embodiment.
Detailed Description
Example methods, apparatus, and systems are described herein. It should be understood that the words "example" and "exemplary" are used herein to mean "serving as an example, instance, or illustration. Any embodiment or feature described herein as "example" or "exemplary" is not necessarily to be construed as preferred or advantageous over other embodiments or features. Other embodiments can be used, and other changes can be made, without departing from the scope of the subject matter presented herein.
Accordingly, the example embodiments described herein are not meant to be limiting. The aspects of the present disclosure generally described herein and illustrated in the figures can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are contemplated herein.
Further, the features illustrated in each of the figures may be used in combination with each other unless the context suggests otherwise. Thus, the drawings should generally be regarded as constituent aspects of one or more general embodiments, but it is understood that not all illustrated features are necessary for every embodiment.
I. Summary of the invention
The software application may be programmed to record audio content, such as conversations between people. The application may provide an interface to record the transcription of words and display the transcription to a user, such as a user of a mobile device. In some cases, this type of recorder application may record long dialogs, resulting in very long transcriptions (e.g., dialogs spanning the hour duration of multiple pages of text). In some such cases, it may be difficult to find important sections of transcription when a user attempts to view multiple pages of text. In some examples described herein, a particular informative section in text may be marked or highlighted to facilitate user navigation. In addition, a particular informative keyword from each such section may be extracted based on the section content and associated with that section to further facilitate user navigation and reduce the amount of time required to navigate to the desired section of the overall transcription.
The transcription may initially be divided into segments based on a transcription model that takes the entire transcription as input and outputs segment divisions between different text segments. One or more section selector models may then be applied to the sections to identify the most salient sections for understanding the entire text. For each such salient section, the tag may be attributed to the section of text that best describes the section. Each tag may be a single word or a plurality of consecutive words extracted from a text segment that best explains what the segment is about and how unique the segment is about the entire text content. The tags may allow an easy user to navigate (e.g., scroll) to navigate to important sections of text that discuss topics described by the associated tags.
In some examples, highlighting of important sections and identification of keywords that serve as labels in these sections may both be performed in the context of the overall transcription. For example, a transcribed text section may describe both the National Basketball Association (NBA) and Lebro james. Thus, both "NBA" and "le bron" can potentially act as tags for this segment. However, the transcribed overall text may include many references to NBA, but other sections may not contain references to lebur james. Thus, "le Brown" may be identified as a section tag because the text section is exclusively focused on le Brown james. In this way, the tag can be extracted with a contextual understanding of the overall text to highlight the uniqueness of the associated text segment in terms of the content that the segment adds to the complete transcription.
One or more separate models may be used to attribute tags to associated sections. In some examples, results from two different models may be combined to identify which tags belong to which section. The first such model may be a deep learning model that is trained based on user data to understand text and find important tags in the text. For example, the first model may be a deep learning Natural Language Processing (NLP) model, such as a bi-directional encoder representation (BERT) based on a converter. The second model may be a heuristic or algorithmic model, rather than a machine learning model. For example, the second model may be the term frequency reversed document frequency (TF-IDF) model that considers extracted potential tags and counts the number of times each potential tag appears in each section. Separate weights may be applied to the results of the machine learning model (e.g., weighting the scores of the tags in the context of the overall text) and the results of the algorithm model (e.g., weighting the scores of the tags based on occurrence). The dual model approach may be advantageous because each model may have separate tag identification capabilities. The algorithmic tag-based selector may be well equipped to find common terms that often occur, but may not identify specific terms well. The machine learning model may be better equipped to find specific terms, but sometimes may also identify apparently wrong specific terms. Combining these two approaches (model for generic term and model for specific term) can provide two best aspects. After aggregating the results of the two models, only labels from the two models with high scores may be selected so that each model provides for inspection of the other models.
In some examples, only nouns may be extracted as labels. Separate noun extractor models may be used to separate out other parts of speech. Based on feedback from the user, nouns have been identified as more useful labels than other parts of speech, such as adjectives and verbs, for some example applications. In a further example, to find the occurrence count of tags, a language stem may be applied to each tag such that different forms of tags may be counted when they occur. For example, the same occurrence count may be applied to singular nouns, plural nouns, and all lattice nouns (e.g., apples's).
The result of the application of the one or more tag-home models may be a list of one or more tags that are assigned to each section. In some examples, tag attribution may be performed in parallel with text section selection (in identifying tags for text sections, it may not be known which text sections are to be selected). In a further example, the tag list for each section may be ranked in order or otherwise scored to allow the top tag of each selected section to be displayed later. Duplicate tags may also be filtered out to avoid associating the same tag to different sections. Thus, each text section may be provided with a different associated tag.
The identification of important text sections to be highlighted may also be performed using one or more models. In some examples, results from the first model and the second model may be combined. The first model may be a machine learning model (e.g., a deep learning model trained to understand grammar and context). The second model may be an algorithmic model (e.g., a label-based model configured to obtain all labels present in each section and aggregate the labels). The results (e.g., scores) from each model may be normalized and a predefined weighting scheme may be applied to aggregate the results. The output of this tag selection process may be a segment relevance score for each text segment transcribed (except for a tag relevance score that is generated separately for all tags that appear in each segment). Similar to the tag selection process, a combination of different methods (e.g., machine learning model and algorithm model) may provide better results than either model in isolation.
Finally, a subset of the transcribed text sections may be selected for use as part of an intelligent scroll user interface. To optimize the user experience, text sections identified as important may be prioritized, which also have associated tags that have been identified as important. Since the tag will be displayed as part of the resulting user interface, text sections identified as important that have no well-associated tag options may not be selected. Thus, a text section may be identified by balancing the relevance score of the text section with the relevance score of the associated tag. In some examples, a predetermined number of text sections may be selected for the user interface at all times. For example, eight text sections may be selected for display as part of a scroller interface. In a further example, evenly distributed filtering may be applied to penalize scores of segments that are too close to each other in the overall transcription. By not highlighting adjacent sections, more navigation values may be obtained.
Example GUI
The GUI may be provided as part of a scroller interface to facilitate user navigation of text. In some examples, the user interface includes a graphical panel that appears once the user begins scrolling through text, indicating that the user is skimming or searching for text. The user interface may reveal a scroll bar on the right and present a marked area and labels as the user scrolls through these areas. As a more specific example, the user interface may include two portions: (1) Gray timeline, which represents a global overview of the entire transcription; and (2) a blue bubble that acts as a magnifying glass for the text currently visible on the screen. In some examples, the number of sections to be shown on the screen in the gray timeline may be limited regardless of the text size. For example, the number may be eight to ten sections at maximum. The appearance of the GUI may differ from the examples specifically illustrated and described herein.
1A, 1B, 1C, 1D, and 1E illustrate a series of views of a GUI of a computing device according to an example embodiment. More specifically, computing device 100 is illustrated as a mobile phone or tablet device having a display showing a transcribed portion of an audio recording. The GUI additionally includes user interface elements to facilitate user navigation to different portions of the transcription.
Fig. 1A illustrates an initial view of a transcribed portion on a display of a computing device 100. The transcription is divided into discrete text sections. In this case, each discrete text segment is represented as a paragraph. In alternative examples, different divisions of discrete text segments may be used instead (e.g., sentences or pages). Referring to fig. 1A, three text sections are shown on the display of computing device 100: text section 102, text section 106, and text section 110. Additionally, time stamps 104 and 108 are displayed to indicate the start time of transcription of the corresponding text segment.
FIG. 1A additionally illustrates a user interface mode in which a user has selected "transcribe" to switch to the text representation of the audio recording being displayed. In the illustrated example, an "audio" mode may also be used to allow a user to navigate to different portions of an audio recording for playback. The view illustrated in fig. 1A may represent user navigation of the transcription in real-time (e.g., during or shortly after audio recording). The view illustrated in fig. 1A may also represent user navigation through the transcript at a later point in time after recording. In this example, the audio recordings are associated with a professor of a teaching biological class. Thus, the user interface and associated software applications may be used by students to navigate to different portions of the transcription of the classroom record after a class. The illustrated example is a typical case where a user interface that allows efficient navigation may be particularly beneficial. For example, a student may record an entire classroom, but only need to review a particular section at a later time. Thus, the software application may provide an advantageous alternative to playback of the entire audio recording of the classroom.
To view the transcription, the user may touch the touch screen interface of the computing device 100 at a touch area 120 on the display. The user may then move his or her finger upward as illustrated by user input 122. This upward movement may initiate a scroll down of text displayed on the screen. Conversely, a downward motion may initiate an upward scroll of text displayed on the screen. In alternative examples, different types of user inputs and/or different types of input devices may be used to allow scrolling text displayed on the screen of computing device 100.
FIG. 1B illustrates a subsequent view of the display of the computing device 100 after the user input illustrated in FIG. 1A. In fig. 1B, touch area 120 is illustrated as further upward on the screen of computing device 100 to illustrate the upward movement of the user's finger. The transcript is scrolled to show a text representation of a later portion of the audio recording. Thus, text sections 106 and 110 are now shown on screen along with time stamps 108 and 112. Thus, FIG. 1B illustrates the result of manual navigation through the transcription. The example transcripts illustrated in FIGS. 1A-1E are only a few minutes long. However, in practice, the transcription may be generated for longer audio recordings (e.g., one hour or more). In this case, manual navigation by conventional scrolling may be very inefficient for the user.
FIG. 1B additionally illustrates interface elements 130 displayed in response to user finger movement indicated by touch area 120. In this case, upward movement of the user's finger indicates a desire to browse through the transcribed text (e.g., find a particular portion of a classroom lecture). Thus, the interface element 130 is displayed to provide the user with a selectable option to initiate a separate intelligent scroller interface. In this example, up and down arrows of interface element 130 are provided to help illustrate the significance of interface element 130. In other examples, different types of interface elements may be displayed. In other examples, the interface elements may not be displayed, instead a complete intelligent scroller interface may be displayed in response to an initial scroll input by the user. In other examples, alternative interface means may be provided to the user to cause the computing device 100 to also display or alternatively display the intelligent scroller interface.
FIG. 1C additionally illustrates an interface panel 132 displayed in response to user input selecting interface element 130. Touch area 120 indicates a user touch input to select interface element 130. A panel 132 is then displayed that includes a timeline 134, the timeline 134 illustrating the entire length of the transcription. In addition, panel 132 also includes labels 136 associated with different selected text sections from the transcription. Both segment relevance and tag relevance may be considered in the context of the overall transcription to determine which segments to select for inclusion within the intelligent scroller interface and which tag to display for each selected segment. In the description of the example architecture shown in fig. 2, more details are provided regarding the section and tag selection process. The panel 132 and the included user interface components provide the user with an alternative means of quickly navigating to different portions of the overall transcription.
In some examples, a predetermined number of text sections may always be selected for inclusion in the intelligent scroller interface. In the example illustrated in fig. 1C, six tags are extracted from the corresponding text sections of the transcription. In this case, the tag 136 is: "interval", "cell division", "organism", "meiosis", "examination" and "mitosis". Each of the tags 136 provides a contextual indicator of relevance of the transcribed corresponding selected section to facilitate user navigation via interaction with the intelligent scroller interface. In some examples, such as those illustrated herein, a noun may be selected for all tags 136. In alternative examples, different types of tags may also or alternatively be extracted.
FIG. 1D illustrates navigating to different portions of a transcription in response to user input navigating within an intelligent scroller interface. More specifically, the user may select to navigate to a particular one of the tags 136 in order to navigate to a different section of the transcription. In different examples, the manner in which navigation to a particular tag within the intelligent scroller interface is performed may be different. In some examples, the interface may allow a user to drag interface element 130 to a different portion of timeline 134 and/or directly touch timeline 134 to move interface element 130 to a different portion of timeline 134. In the illustrated example, the location of the touch area 120 illustrates user navigation of the tag "mitosis" near the end of transcription. The highlighted bubble 140 illustrates that the user has navigated to "mitosis", and the highlighted bubble 140 also displays a corresponding timestamp ("02:16") at which the text section in the audio recording corresponding to the label "mitosis" begins. In a further example, the interface may also or alternatively allow the user to directly select a particular tab to navigate to a corresponding section in the transcription.
As a result of the user interaction with panel 132, different portions of the transcription are displayed on computing device 100. Specifically, text sections 114 and 118 and timestamp 116 are displayed. In this example, the text section 118 corresponds to the label "mitosis" and begins at 02:16 of the audio recording. Thus, the user input provided to panel 132 indicates an interest in reviewing portions of the classroom that are particularly relevant to mitosis. The transcribed replacement sections correspond to other labels 136 within the panel 132 that may be used for user navigation.
In other examples, the GUI representing the intelligent scroller interface may be different. For example, in some examples, only the tags may be displayed for selection, without displaying the timeline. The tags may also or alternatively be displayed in a different format, such as a drop down list. In other examples, as the user navigates up and down through the timeline, only the proximity labels of the current navigation location in the transcript may be displayed. In any event, the interface may allow for quick navigation to the transcription segment identified as highly relevant based on the tag identified as highly relevant in the context of the overall transcription.
FIG. 1E illustrates a resulting display of selected portions of a transcription based on user interaction with an intelligent scroller interface. Specifically, based on the user navigating to the label "mitosis," text section 118 is now fully visible on the display screen of computing device 100. In some examples such as those illustrated herein, the interface may enable a user to select to view a text section corresponding to a particular tag by removing touch input after navigating to the particular tag. The panel 132 may be removed in response to user input to allow for full viewing of the selected portion of the transcription. In a further example, after the panel 132 is removed, one or more occurrences of the tag within the text section may be temporarily highlighted to indicate the effect of the user input. For example, referring to FIG. 1E, highlighted box 150 highlights the occurrence of "mitosis" within text section 118 to help the user understand that he or she has navigated to a portion of the transcription identified as being particularly relevant to the description of mitosis in the context of the overall transcription.
To repeat the process, the user may provide additional touch input to the display of the computing device 100 to reenter the intelligent scroller interface. From there, the user may choose to navigate to a different tab to view a different associated text section. In this way, the user's experience in browsing transcribed text may be improved compared to interfaces that only allow manual scrolling up and down.
Example architecture
Fig. 2 is a block diagram of a system architecture according to an example embodiment. More specifically, fig. 2 illustrates an example architecture 200 (arrangement of software modules) that can operate on provided inputs to generate outputs that enable operation of the intelligent scroller interface described herein. The arrangement of fig. 2 is provided for illustration. Alternative examples may include fewer modules, additional modules, and/or modules arranged or combined in a different manner than explicitly illustrated in fig. 2.
The text input 202 may be provided in a transcribed form of an audio recording for modular processing illustrated in fig. 2. In some examples, the same computing device may generate text input 202 from an audio recording and process the text input 202 using architecture 200. In other embodiments, the first computing device may generate the text input 202 from an audio recording, and the second computing device may process the text input 202 using the architecture 200. For example, the first computing device may be a mobile user device and the second computing device may be a remote server.
Fig. 2 will be described in the context of processing text input 202 in the form of a transcription of an audio recording. It is noted, however, that the methodology illustrated and described with respect to fig. 2 and other figures included herein is equally applicable to other types of text input. For example, the methodology may also be applied to generate intelligent scroller interfaces for navigating books (e.g., for electronic readers), web pages (e.g., for web browsers), or news stories (e.g., for news viewers), among other possible applications.
The tag extractor 204 is a module that is applied to the text input 202 to generate candidate or potential tags from the transcription. These candidate tags may be later evaluated during the tag selection process and the text section selection process to determine which tags to include in the intelligent scroller interface. The tag extractor 204 may be a machine-learning model trained based on user data to find important terms across the entire text transcribed.
Text section 206 is also generated based on text input 202. In some examples, text section 206 may be provided with text input 202 as part of the input to a computing device using architecture 200. In other examples, the computing device may directly process the text input 202 to divide the text input 202 into discrete sections that make up the text section 206. Each of the text sections 206 may be identified as an individual paragraph or other text block within the overall transcription. In some examples, a machine learning model, an algorithmic model, or a combination of both may be used to find appropriate division points between successive portions of transcription to generate text section 206.
Both the text section 206 and the tags generated by the tag extractor 204 may be input into the noun extractor 208, which noun extractor 208 is a module configured to filter tags to include only nouns. In some examples, the noun extractor 208 may be a machine learning module, such as a neural network trained for part-of-speech tagging. In a further example, the noun extractor 208 may apply language stem heuristics such that nouns of different forms are counted as the same potential label (including singular, plural, all bins, etc.). The output from noun extractor 208 may be input into tag occurrence counter 210, which tag occurrence counter 210 is a module configured to count occurrences of all forms of potential tags. The resulting outputs of the noun extractor 208 and the tag occurrence counter 210 include the potential noun tag and associated occurrence count, which may be used as inputs to drive both the tag selection and the segment selection process.
Deep dialogue language understanding (CLU) tag selector 212 is a machine-learned language model that is trained based on user data to understand grammar and context in order to score and/or rank potential tags for each segment. In some examples, deep CLU tag selector 212 is a BERT-based Recurrent Neural Network (RNN) model. Depth CLU tag selector 212 takes text section 206 and potential tags from tag extractor 204 as inputs to score and/or rank the potential tags for each section. The output from deep CLU label selector 212 is a score and/or rank, which is then input into normalizer 222. Normalizer 222 is a module that adjusts the score and/or ranking of tags to enable aggregation of the score and/or ranking of tags output by a plurality of different models (e.g., machine learning models and algorithmic models).
The tag-based tag selector 214 is an algorithm or heuristic model that scores and/or ranks tags from among the potential tags generated by the tag extractor 204. In some examples, the tag-based tag selector 214 may be a TF-IDF model with curve fitting. The TF-IDF model is based on numerical statistics that are intended to reflect the importance of words to documents in a collection or corpus. The TF-IDF value increases in proportion to the number of occurrences of a term in a document and is offset by the number of documents in the corpus containing the term, which helps to accommodate the fact that some terms generally occur more frequently. In some examples, the tag-based tag selector 214 operates by scoring and/or ranking potential tags in each section based on the occurrence count information from the tag occurrence counter 210. The output of the tag-based tag selector 214 is input into a normalizer 224. Similar to normalizer 222, normalizer 224 is a module that adjusts the score and/or ranking of tags so that the scores and/or rankings of tags output by multiple different models can be aggregated.
The tag aggregator 226 is a module that takes as input the output scores and/or rankings of tags from both the normalizer 222 and the normalizer 224. In some examples, aggregating the results of multiple models may yield better results. The tag aggregator 226 applies a weighted average of the outputs from the multiple different models. In some examples, the weighted average is based on a predetermined weight. In other examples, some or all of the weights may be adjusted periodically. The output of the tag aggregator 226 is input into a repetition filter 228. The repetition filter 228 is a module that removes duplicate tags such that a different tag is selected for each section.
The resulting output of the label aggregation process (the output after the repetition filter 228 is applied to the output of the label aggregator 226) is a different highest ranking label for each text section (at which time it is not known which particular text sections will be selected for the intelligent scroller interface). In some examples, the output of the tag aggregation process is instead a ranking of the plurality of tags in each text section (e.g., ranking from most relevant to least relevant).
Turning now to the section selection process, the tag-based section scorer 232 is an algorithm or heuristic module that scores and/or ranks text sections by evaluating and summarizing all potential tags contained therein. The tag-based section scorer 232 takes as input the text section 206, the potential tags from the tag extractor 204, and the presence information from the tag presence counter 210. The score and/or ranking of text sections output by the tag-based section scorer 232 may be input into the normalizer 242. Normalizer 242 is a module that adjusts the score and/or ranking of text segments to enable aggregation of scores and/or rankings of text segments output by multiple different models (e.g., machine learning models and algorithmic models).
Depth CLU segment ranker 234 is a machine-learned language model that is trained based on user data to understand grammars and contexts for scoring and/or ranking text segments. In some examples, the depth CLU section scorer 234 is a BERT-based RNN model. Depth CLU section scorer 234 takes text section 206 and potential tags from tag extractor 204 as inputs to score and/or rank the text section. The output from the depth CLU section scorer 234 is the scoring and/or ranking of the text sections, which are then input into a normalizer 244. Similar to normalizer 242, normalizer 244 is a module that adjusts the score and/or ranking so that the scores and/or rankings of segments output by multiple different models (e.g., machine learning models and algorithm models) can be aggregated.
The section aggregator 246 is a module that takes as input the output scores and/or rankings of the text sections from the normalizers 242 and 244. In some examples, aggregating the results of multiple models may yield better results. The segment aggregator 246 applies a weighted average of the outputs from the multiple different models. In some examples, the weighted average is based on a predetermined weight. In other examples, some or all of the weights may be adjusted periodically. The output of the section aggregator 246 is a scoring and/or ranking of the text sections.
In some examples, when selecting a subset of text sections to be shown as part of the intelligent scroller interface, a section relevance score for each text section and a label relevance score for each label in each section may be considered. The evenly distributed filtering 250 is a module that may be applied first to penalize fractions of segments that are close together (e.g., adjacent) in the overall transcription. The section selector 252 is a module that then performs a section selection process based on the tag scoring and/or ranking information from the tag aggregator 226 and the text section scoring and/or ranking information from the section aggregator 246.
The output of the section selector 252 is a limit result 260, which is a subset of best-performing text sections based on all scoring information, with best-performing tags for each section for display within the intelligent scroller interface. In some examples, the segment selector 252 always selects the limit result 260 to have a predetermined number of best performing segments (e.g., the first eight segments). In other examples, the number of text sections selected by section selector 252 for the intelligent scroller interface may be adjusted based on one or more factors, such as user preferences and/or transcription length.
For any of the machine learning based modules illustrated in fig. 2, user data may be collected first and used for training purposes. More specifically, the user may be required to review the transcription of the audio recording to highlight important sections in order to train a model for section selection. The user may also be required to select words that best describe the importance of each section in the context of the overall transcription in order to train a model for tag selection. Other types of user feedback data may also or alternatively be used to train one or more of the machine learning models described.
Exemplary method
Fig. 3 illustrates a method 300 according to an example embodiment. The blocks of method 300 represent functions, operations, or steps that may be performed by one or more computing devices. The blocks of method 300 may be performed by a mobile computing device, such as computing device 100 illustrated and described with reference to fig. 1A-1E, or by a different computing device. Further, the method 300 may be performed by a computing device configured with any or all of the components of the architecture illustrated in fig. 2. In further examples, some or all of the blocks of method 300 may be performed by a remote computing device or split across multiple computing devices.
With respect to any ladder diagrams, scenarios, and flow charts illustrated in fig. 3 and other figures, each block and/or communication may represent the processing of information and/or the transmission of information in accordance with example embodiments. Alternate embodiments are included within the scope of these example embodiments. In these alternative embodiments, functions described as, for example, blocks, transmissions, communications, requests, responses, and/or messages may be performed out of the order shown or discussed, including substantially concurrently or in reverse order, depending on the functionality involved. Further, more or fewer blocks and/or functions may be used with any of the trapezoids, scenarios, and flowcharts discussed herein, and these trapezoids, scenarios, and flowcharts may be combined with one another, in part or in whole.
Blocks representing information processing may correspond to circuitry capable of being configured to perform specific logical functions of the methods or techniques described herein. Alternatively or additionally, blocks representing information processing may correspond to computer programs, modules, segments, or portions of program code (including related data). Program code may include one or more instructions executable by a processor for performing specific logical functions or acts in a method or technique. The program code and/or related data may be stored on any type of computer-readable medium, such as a storage device including a diskette or hard drive, or other storage medium.
Referring to fig. 3, block 302 includes receiving, at a computing device, a transcription of an audio recording. The transcription may be divided into a plurality of text sections. In some examples, the transcription and segmentation may be received from a transcription software program or module running on the computing device. The software program or module may be configured to process the recorded audio data to generate a transcription. In other examples, the transcription and the segmentation may be received from separate computing devices.
Block 304 includes associating one or more tags with each text section of the plurality of text sections. Each associated tag may be extracted from the transcribed text.
In some examples, associating the tag with the text section may involve aggregating results from applying the first model and the second model. The first model may be an algorithmic model. The second model may be a machine learning model. In a further example, the aggregation may involve assigning the score to the tag by applying a first predetermined weight to the result from applying the first model and a second predetermined weight to the result from applying the second model. In a further example, a machine learning model may be trained to identify context-dependent tags by taking transcriptions as input.
In additional examples, associating the tags may involve determining a count of occurrences of potential tags in each text section. In a further example, the language stem may be applied prior to determining the occurrence count of potential tags in each text section.
In a further example, a noun extractor may be applied to transcribed text such that each tag is guaranteed to be a noun. In additional examples, duplicate filtering may be applied to potential tags such that each tag is guaranteed to be different.
Block 306 includes selecting a subset of the plurality of text sections based at least on one or more tags associated with each of the plurality of text sections. A subset of the text sections may be selected as information sections in the context of the overall transcription. The associated tags may be considered in the text section selection process in view of the displayed end user interface results including the information tags associated with each selected text section.
In some examples, selecting the subset of text sections involves aggregating results from applying the first model and the second model. The first model may be an algorithmic model. The second model may be a machine learning model. In a further example, the aggregation may involve assigning the score to the text segment by applying a first predetermined weight to the results from applying the first model and a second predetermined weight to the results from applying the second model. In additional examples, the algorithm model may be configured to assign a score to a text segment based on a label extracted from the transcription.
Additional examples may include assigning a selection relevance score to each of the plurality of text sections and assigning a tag relevance score to each tag associated with each of the plurality of text sections. The selection subset may then be based on the section relevance score assigned to each text section and the tag relevance score assigned to each tag associated with each text section.
In a further example, selecting a subset of text sections may involve applying evenly distributed filtering across a plurality of text sections. In additional examples, a subset of the text sections may be selected to include a predetermined number of text sections (e.g., eight text sections).
Block 308 includes providing a representation of the transcription and a scroller interface for display on a computing device. The scroller interface may allow navigation to each text segment in the subset based on the user navigating to a tag associated with the text segment.
In some examples, a ranking of tags associated with each text segment may be determined. The ranking may be based on an estimated relevance of each of the tags of the text section in the context of the overall transcription. Each tab displayed in the scroller interface may then be the highest ranked tab of the associated text segment.
In a further example, the scroller interface may include a scrollbar with annotations displayed for each tag associated with a text segment of the subset. In such an example, the annotations displayed for each tag may be positioned relative to the scroll bar based on the time stamps of the text sections associated with the tags. Further examples may involve highlighting text corresponding to a tab in the transcription representation in response to a user navigating to the tab in the scroller interface. The scroller interface may be varied in a variety of other ways.
V. example apparatus
Fig. 4 is a functional block diagram of an example computing device 400 according to an example embodiment. In some examples, the computing device 400 shown in fig. 4 can be configured to perform at least one function described with respect to the computing device 100 illustrated in fig. 1A-1E, at least one function described with respect to the architecture 200 illustrated in fig. 2, and/or at least one function described with respect to the method 300 illustrated in fig. 3.
Computing device 400 may include a user interface module 401, a network communication interface module 402, one or more processors 403, a data store 404, and one or more sensors 420, all of which may be linked together via a system bus, network, or other connection mechanism 405.
The user interface module 401 can be operable to send data to and/or receive data from an external user input/output device. For example, the user interface module 401 can be configured to send data to and/or receive data from a user input device (such as a touch screen, computer mouse, keyboard, keypad, touchpad, trackball, joystick, camera, voice recognition module, and/or other similar devices). The user interface module 401 can also be configured to provide output to a user display device, such as one or more Cathode Ray Tubes (CRTs), liquid crystal displays, light Emitting Diodes (LEDs), displays using Digital Light Processing (DLP) technology, printers, light bulbs, and/or other similar devices now known or later developed. The user interface module 401 can also be configured to generate audible output, such as speakers, speaker jacks, audio output ports, audio output devices, headphones, and/or other similar devices. The user interface module 401 can also be configured with one or more haptic devices capable of generating haptic output, such as vibrations and/or other output detectable through touch and/or physical contact with the computing device 400. In some embodiments, the user interface module 401 can be used to provide a Graphical User Interface (GUI) for utilizing the computing device 400.
The network communication interface module 402 can include one or more wireless interfaces 407 and/or one or more wired interfaces 408, which can be configured to communicate via a network. The wireless interface 407 can include one or more wireless transmitters, receivers, and/or transceivers, such as bluetooth TM A transceiver(s),
In some embodiments, the network communication interface module 402 can be configured to provide reliable, secure, and/or authenticated communication. For each communication described herein, information for ensuring reliable communication (i.e., guaranteed messaging) can be provided, possibly as part of a message header and/or footer (e.g., packet/message ordering information, encapsulation header and/or footer, size/time information, and transmission verification information such as a CRC and/or parity value). One or more cryptographic protocols and/or algorithms can be used to secure (e.g., encoded or encrypted) and/or decrypt/decode communications, such as, but not limited to, a Data Encryption Standard (DES), an Advanced Encryption Standard (AES), a Rivest-Shamir-Adelman (RSA) algorithm, a Diffie-Hellman algorithm, a secure socket protocol such as secure socket layer SSL or Transport Layer Security (TLS), and/or a Digital Signature Algorithm (DSA). Other cryptographic protocols and/or algorithms can also be used, or in addition to those listed herein, for protecting (and then decrypting/decoding) the communication.
The one or more processors 403 can include one or more general purpose processors, and/or one or more special purpose processors (e.g., digital signal processors, graphics processing units, application specific integrated circuits, etc.). The one or more processors 403 can be configured to execute the computer-readable program instructions 406 and/or other instructions described herein contained in the data store 404.
The data store 404 can include one or more computer-readable storage media that can be read and/or accessed by at least one of the one or more processors 403. The one or more computer-readable storage media can include volatile and/or nonvolatile storage components, such as optical, magnetic, organic, or other memory or disk storage, that can be fully or partially integrated with at least one of the one or more processors 403. In some embodiments, data store 404 can be implemented using a single physical device (e.g., an optical, magnetic, organic, or other memory or disk storage unit), while in other embodiments data store 404 can be implemented using two or more physical devices.
The data store 404 may include computer readable program instructions 406 and possibly additional data. In some embodiments, the data store 404 can additionally include storage required to perform at least a portion of the methods, scenarios, and techniques described herein, and/or at least a portion of the functionality of the devices and networks described herein.
In some embodiments, computing device 400 can include one or more sensors 420. The sensor 420 can be configured to measure user interaction with the computing device 400. The sensor 420 can also be configured to measure conditions in the environment of the computing device 400 and provide data about the environment. For example, the sensor 420 can include one or more of the following: (i) An identification sensor for identifying other objects and/or devices, such as, but not limited to, radio Frequency Identification (RFID) readers, proximity sensors, one-dimensional bar code readers, two-dimensional bar code (e.g., quick Response (QR) code) readers, and laser trackers, wherein the identification sensor can be configured to read an identifier, such as an RFID tag, bar code, QR code, and/or other device and/or object configured to be read, and at least provide identification information; (ii) Sensors for measuring the location and/or movement of computing device 400, such as, but not limited to, tilt sensors, gyroscopes, accelerometers, doppler sensors, global Positioning System (GPS) devices, sonar sensors, radar devices, laser displacement sensors, and compasses; (iii) An environmental sensor for obtaining data indicative of an environment of computing device 400, such as, but not limited to, an infrared sensor, an optical sensor, a light sensor, a camera, a biological sensor, a capacitive sensor, a touch sensor, a temperature sensor, a wireless sensor, a radio sensor, a motion sensor, a microphone, a sound sensor, an ultrasonic sensor, and/or a smoke sensor; and (iv) force sensors for measuring one or more forces (e.g., inertial and/or gravitational forces) acting about computing device 400, such as, but not limited to, measuring one or more of the following: one or more dimensions of force, torque, ground force, friction, and/or Zero Moment Point (ZMP) sensors identifying ZMP and/or ZMP locations. Many other examples of sensor 420 are possible.
With respect to any example described herein that may involve recording audio data or other information from a user, a control may be provided to the user that allows the user to determine whether and when the system, program, or feature described herein may enable the collection and/or storage of recorded audio data or other information (e.g., information about the user's social network, social actions or activities, profession, user preferences, or the user's current location). In addition, the particular data may be processed in one or more ways before it is stored or used such that the personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information of the user cannot be determined, or the geographic location of the user where location information is obtained may be generalized (such as to a city, zip code, or state county level) such that a particular location of the user cannot be determined. Thus, the user can control what information is collected about the user, how that information is used, and what information is provided to the user.
The particular arrangements shown in the drawings should not be construed as limiting. It should be understood that other embodiments may include more or less of each of the elements shown in a given figure. Further, some of the illustrated elements may be combined or omitted. Still further, the illustrative embodiments may include elements not shown in the figures.
The steps or blocks representing information processing can correspond to circuitry capable of being configured to perform specific logical functions of the methods or techniques described herein. Alternatively or additionally, steps or blocks representing information processing can correspond to modules, segments, or portions of program code (including related data). The program code can include one or more instructions executable by a processor for performing specific logical functions or acts in a method or technique. The program code and/or related data can be stored on any type of computer-readable medium, such as a storage device including a diskette, hard drive, or other storage medium.
Computer-readable media can also include non-transitory computer-readable media such as computer-readable media that store data for a short period of time, such as register memory, processor cache, and Random Access Memory (RAM). The computer-readable medium can also include a non-transitory computer-readable medium that stores the program code and/or data for a longer period of time. Thus, for example, the computer readable medium may include secondary or persistent long term storage, such as Read Only Memory (ROM), optical or magnetic disk, compact disk read only memory (CD-ROM). The computer readable medium can also be any other volatile or non-volatile storage system. For example, a computer-readable medium can be considered a computer-readable storage medium or a tangible storage device.
While various examples and embodiments have been disclosed, other examples and embodiments will be apparent to those skilled in the art. The various disclosed examples and embodiments are for illustrative purposes and are not intended to be limiting, with the true scope indicated by the following claims.
Claims (20)
1. A method, comprising:
at a computing device, receiving a transcription of an audio recording, wherein the transcription is divided into a plurality of text sections;
associating one or more tags with each text section of the plurality of text sections, wherein each associated tag is extracted from the transcribed text;
selecting a subset of the plurality of text sections based at least on the one or more tags associated with each text section of the plurality of text sections; and
providing a representation of the transcription and a scroller interface for display on the computing device, wherein the scroller interface allows navigation to each text segment in the subset based on user navigation to the tag associated with the text segment.
2. The method of claim 1, further comprising: for each text section of the plurality of text sections, determining a ranking of the one or more tags associated with the text section, wherein each tag displayed in the scroller interface is a highest ranked tag of an associated text section.
3. The method of claim 1, wherein associating the one or more tags with each text segment of the plurality of text segments comprises: aggregating results from applying a first model and a second model, wherein the first model is an algorithmic model, and wherein the second model is a machine learning model.
4. The method of claim 3, wherein aggregating the results from applying the first model and the second model comprises: a score is assigned to a label by applying a first predetermined weight to the result from applying the first model and by applying a second predetermined weight to the result from applying the second model.
5. A method according to claim 3, wherein the machine learning model is configured to identify context-dependent tags by taking the transcription as input.
6. The method of claim 1, wherein associating the one or more tags with each text segment of the plurality of text segments comprises: a count of occurrences of potential tags in each of the plurality of text sections is determined.
7. The method of claim 6, further comprising: a language stem is applied prior to determining the occurrence count of potential tags in each of the plurality of text sections.
8. The method of claim 1, further comprising: a noun extractor is applied to the transcribed text such that each label displayed in the scroller interface is a noun.
9. The method of claim 1, further comprising: repeated filtering is applied to the potential labels such that each label displayed in the scroller interface is different.
10. The method of claim 1, wherein selecting the subset of the plurality of text sections comprises: aggregating results from applying a first model and a second model, wherein the first model is an algorithmic model, and wherein the second model is a machine learning model.
11. The method of claim 10, wherein aggregating the results from applying the first model and the second model comprises: a score is assigned to a segment by applying a first predetermined weight to the result from applying the first model and by applying a second predetermined weight to the result from applying the second model.
12. The method of claim 10, wherein the algorithmic model is configured to assign scores to segments based on labels extracted from the transcription.
13. The method of claim 1, further comprising:
assigning a section relevance score to each text section of the plurality of text sections;
assigning a tag relevance score to each tag associated with each text segment of the plurality of text segments; and
wherein selecting the subset is based on the segment relevance scores assigned to each of the plurality of text segments and the tag relevance scores assigned to each tag associated with each of the plurality of text segments.
14. The method of claim 1, wherein selecting the subset comprises: an evenly distributed filtering is applied across the plurality of text sections.
15. The method of claim 1, wherein the subset includes a predetermined number of text sections.
16. The method of claim 1, wherein the scroller interface comprises a scrollbar having an annotation displayed for each tag associated with a text segment of the subset.
17. The method of claim 16, wherein the annotation displayed for each tag is positioned relative to the scrollbar based on a timestamp of the text segment associated with the tag.
18. The method of claim 1, further comprising: in response to user navigation to a particular tag, text corresponding to the particular tag is highlighted in the representation of the transcription.
19. A computing device, comprising:
one or more processors; and
a non-transitory computer readable medium comprising program instructions executable by the one or more processors to perform functions comprising:
receiving a transcription of an audio recording, wherein the transcription is divided into a plurality of text sections;
associating one or more tags with each text section of the plurality of text sections, wherein each associated tag is extracted from the transcribed text;
selecting a subset of the plurality of text sections based at least on the one or more tags associated with each text section of the plurality of text sections; and
providing a representation of the transcription and a scroller interface for display on the computing device, wherein the scroller interface allows navigation to each text segment in the subset based on user navigation to the tag associated with the text segment.
20. A non-transitory computer readable medium comprising program instructions executable by one or more processors to perform functions comprising:
receiving a transcription of an audio recording, wherein the transcription is divided into a plurality of text sections;
associating one or more tags with each text section of the plurality of text sections, wherein each associated tag is extracted from the transcribed text;
selecting a subset of the plurality of text sections based at least on the one or more tags associated with each text section of the plurality of text sections; and
providing for display a representation of the transcription and a scroller interface, wherein the scroller interface allows navigation to each text segment in the subset based on user navigation to the tag associated with the text segment.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063084800P | 2020-09-29 | 2020-09-29 | |
US63/084,800 | 2020-09-29 | ||
PCT/US2021/071613 WO2022072992A1 (en) | 2020-09-29 | 2021-09-28 | Scroller interface for transcription navigation |
Publications (2)
Publication Number | Publication Date |
---|---|
CN116324829A true CN116324829A (en) | 2023-06-23 |
CN116324829A8 CN116324829A8 (en) | 2023-08-04 |
Family
ID=78414085
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180066430.1A Pending CN116324829A (en) | 2020-09-29 | 2021-09-28 | Scroller interface for transcription navigation |
Country Status (5)
Country | Link |
---|---|
US (1) | US11899921B2 (en) |
EP (1) | EP4222589A1 (en) |
CN (1) | CN116324829A (en) |
DE (1) | DE112021005137T5 (en) |
WO (1) | WO2022072992A1 (en) |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030093790A1 (en) * | 2000-03-28 | 2003-05-15 | Logan James D. | Audio and video program recording, editing and playback systems using metadata |
US20120087637A1 (en) * | 2002-01-29 | 2012-04-12 | Logan James D | Methods and apparatus for recording and replaying video broadcasts |
US9069754B2 (en) * | 2010-09-29 | 2015-06-30 | Rhonda Enterprises, Llc | Method, system, and computer readable medium for detecting related subgroups of text in an electronic document |
US10719222B2 (en) | 2017-10-23 | 2020-07-21 | Google Llc | Method and system for generating transcripts of patient-healthcare provider conversations |
US11568231B2 (en) | 2017-12-08 | 2023-01-31 | Raytheon Bbn Technologies Corp. | Waypoint detection for a contact center analysis system |
-
2021
- 2021-09-28 EP EP21799158.7A patent/EP4222589A1/en active Pending
- 2021-09-28 CN CN202180066430.1A patent/CN116324829A/en active Pending
- 2021-09-28 US US18/043,701 patent/US11899921B2/en active Active
- 2021-09-28 DE DE112021005137.7T patent/DE112021005137T5/en active Pending
- 2021-09-28 WO PCT/US2021/071613 patent/WO2022072992A1/en unknown
Also Published As
Publication number | Publication date |
---|---|
WO2022072992A1 (en) | 2022-04-07 |
CN116324829A8 (en) | 2023-08-04 |
DE112021005137T5 (en) | 2023-08-10 |
US20230266874A1 (en) | 2023-08-24 |
EP4222589A1 (en) | 2023-08-09 |
US11899921B2 (en) | 2024-02-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11163935B2 (en) | Intelligent navigation via a transient user interface control | |
US20210042662A1 (en) | Interactive Information Capture and Retrieval with User-Defined and/or Machine Intelligence Augmented Prompts and Prompt Processing | |
RU2710966C2 (en) | Methods for understanding incomplete natural language query | |
Borg et al. | Accessibility to electronic communication for people with cognitive disabilities: a systematic search and review of empirical evidence | |
US10229167B2 (en) | Ranking data items based on received input and user context information | |
CN107408116B (en) | Using dynamic knowledge graphs to facilitate discovery of information items | |
US20140325407A1 (en) | Collection, tracking and presentation of reading content | |
US9483462B2 (en) | Generating training data for disambiguation | |
US20090249198A1 (en) | Techniques for input recogniton and completion | |
WO2017136440A1 (en) | Proofing task pane | |
EP3720060B1 (en) | Apparatus and method for providing conversation topic | |
CN112262421B (en) | Programmable interface for automatic learning and reviewing | |
US11881209B2 (en) | Electronic device and control method | |
WO2019045848A1 (en) | Contextual skills discovery | |
US10936815B2 (en) | Removable spell checker device | |
Crestani et al. | Mobile information retrieval | |
US11836172B2 (en) | Facilitating generation of data visualizations via natural language processing | |
US20230186029A1 (en) | Systems and Methods for Generating Names Using Machine-Learned Models | |
US20170148123A1 (en) | Method and system for generating a physician recommendation | |
US20140136963A1 (en) | Intelligent information summarization and display | |
US11411902B2 (en) | Information processing apparatus and non-transitory computer readable medium storing information processing program | |
US11899921B2 (en) | Scroller interface for transcription navigation | |
US10891320B1 (en) | Digital content excerpt identification | |
EP4328764A1 (en) | Artificial intelligence-based system and method for improving speed and quality of work on literature reviews | |
US10380226B1 (en) | Digital content excerpt identification |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
CI02 | Correction of invention patent application |
Correction item: PriorityCorrect: 63/084,800 2020.09.29 USNumber: 25-02Page: The title pageVolume: 39Correction item: PriorityCorrect: 63/084,800 2020.09.29 USNumber: 25-02Volume: 39 |
|
CI02 | Correction of invention patent application | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |