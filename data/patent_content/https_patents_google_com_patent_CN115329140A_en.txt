CN115329140A - Dynamic small batch size - Google Patents
Dynamic small batch size Download PDFInfo
- Publication number
- CN115329140A CN115329140A CN202210852423.0A CN202210852423A CN115329140A CN 115329140 A CN115329140 A CN 115329140A CN 202210852423 A CN202210852423 A CN 202210852423A CN 115329140 A CN115329140 A CN 115329140A
- Authority
- CN
- China
- Prior art keywords
- host computer
- mini
- batches
- batch
- training
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012549 training Methods 0.000 claims abstract description 182
- 238000000034 method Methods 0.000 claims abstract description 71
- 238000013528 artificial neural network Methods 0.000 claims abstract description 64
- 230000008569 process Effects 0.000 claims abstract description 53
- 238000012545 processing Methods 0.000 claims abstract description 37
- 230000015654 memory Effects 0.000 claims description 47
- 238000004590 computer program Methods 0.000 abstract description 11
- 238000005192 partition Methods 0.000 description 9
- 230000009471 action Effects 0.000 description 7
- 238000010586 diagram Methods 0.000 description 6
- 210000002569 neuron Anatomy 0.000 description 6
- 238000004422 calculation algorithm Methods 0.000 description 5
- 230000005540 biological transmission Effects 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000012360 testing method Methods 0.000 description 2
- 238000013459 approach Methods 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 238000012804 iterative process Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/901—Indexing; Data structures therefor; Storage structures
- G06F16/9017—Indexing; Data structures therefor; Storage structures using directory or table look-up
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/213—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
- G06F18/2148—Generating training patterns; Bootstrap methods, e.g. bagging or boosting characterised by the process organisation or structure, e.g. boosting cascade
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
Abstract
The invention relates to dynamic small batch sizes. Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for using dynamic small batch sizes during neural network training. One of the methods comprises: receiving, by each of a plurality of host computers, a respective batch of training examples, each training example having zero or more features; calculating a minimum number of mini-batches into which the host computer can divide training examples of a respective batch, such that the host computer can process each mini-batch using an embedded layer of a neural network without exceeding available computing resources; determining a maximum minimum number of small batches (N) into which any host computer can divide training examples of its respective batch; generating N small batches from training examples of the respective batch received by the host computer; and processing the N small batches using the embedding layer by each host computer.
Description
Description of the cases
The application belongs to divisional application of Chinese invention patent application No.201980058339.8 with application date of 2019, 12 and 16.
Background
This specification relates to artificial neural networks.
Neural networks are machine learning models that employ multiple layers of operation to predict one or more outputs from one or more inputs. Neural networks typically include one or more hidden layers located between an input layer and an output layer. The output of each hidden layer serves as the input to the next layer (e.g., the next hidden layer or output layer).
Each layer of the neural network specifies one or more transformation operations to be performed on the input of that layer. Some neural network layers include units called neurons. Each neuron receives one or more inputs and generates outputs that are received by other neural network layers. Typically, each neuron receives input from one or more other neurons, and each neuron provides output to one or more other neurons.
Each layer generates one or more outputs using current values of a set of parameters for that layer. Training the neural network involves continuously performing a forward pass on the input, calculating gradient values, and updating the current values of the parameter sets for each layer. Once the neural network is trained, a final set of parameters can be used for prediction.
Some neural networks have a hidden layer as an embedded layer. Typically, the embedding layer transforms sparse neural network inputs (i.e., neural network inputs that have only a small number of features with non-zero values relative to the total number of features) into a low-dimensional vector called embedding.
Disclosure of Invention
This specification describes a distributed computing system that can process an embedded layer of a neural network during training of the neural network. The system includes host computers each storing trainable lookup table embedded partitions. To process a particular training example, for each feature of the training example, the host computer obtains an embedding from a respective one of the partitions of the lookup table stored on the respective host computer. Obtaining an embedding involves sending a lookup request to a host computer that stores the partition of the lookup table to which the embedding belongs. After obtaining the embeddings for each feature, the host computer processes the embeddings to generate an embedding layer output for the training example.
The host computer processes multiple training examples at once. In particular, each host computer receives a respective batch of training examples for processing during an iteration of training. Typically, each batch of training examples has the same number of training examples, and each training example has one or more features. Each host computer calculates the minimum number of embedded mini-batches into which the host computer can divide its batch of training examples, so that the host computer can process each embedded mini-batch using the embedded layer of the neural network without exceeding the available computing resources on the host computer. In this specification, embedding a minilot will be referred to simply as "minilot". The system determines the maximum minimum number of small batches (N) into which any host computer can divide its training examples of batches. Each host computer divides its training examples of the batch into N mini-batches and processes the N mini-batches using the embedded layer of the neural network.
Distributed computing systems operate under several constraints. First, each host computer must typically process the same number of small batches as each other host computer. This is because (i) the host computers receive synchronous parallel instructions, and (ii) the host computers must exchange embedding with each other during each iteration of training. Second, the host computers each have a limited amount of embedded memory that stores data obtained from other host computers. More specifically, the host computer is a special purpose computer for processing neural networks, with only a small amount of scratch pad memory to store the embedding, whereas a conventional CPU has a large amount of main memory or cache memory. Scratch pad memory is a high-speed internal memory used to temporarily store computations, data, and other ongoing work. Thus, the number of inlays available to a host computer at one time is limited, which in turn limits the maximum minibatch size. Third, the number of features in each training example and the amount of scratch pad memory required to store the embedding of each training example vary.
In some implementations, the distributed computing system is configured to perform operations comprising: receiving, by each of a plurality of host computers, a respective batch of training examples, each training example having zero or more features; calculating, by each host computer, a minimum number of mini-batches into which the host computer can divide training examples of a respective batch, such that the host computer can process each mini-batch using an embedded layer of a neural network without exceeding available computing resources; determining a maximum minimum number of mini-batches (N) into which any host computer can divide training examples of its respective batch based on the calculated minimum number of mini-batches; generating, by each host computer, N small batches from the training examples of the respective batch received by the host computer; and processing the N small batches using the embedding layer by each host computer.
In view of these constraints, the subject matter described in this specification can be implemented in particular embodiments in order to realize one or more of the following advantages. Dividing a batch of training examples into a large number of small batches increases the number of times each host computer must obtain the embedding from the other host computers, resulting in an avoidable read/write latency. On the other hand, due to available resources on each host computer, such as scratch pad memory, it may not be possible to partition a batch of training examples into a small number of batches with a large number of training examples. The system described in this specification dynamically adjusts the number of mini-batches before each iteration of training. In particular, the system calculates the maximum minimum number of small batches (N) into which any host computer can divide its respective batch of training examples without exceeding the available computing resources on any host computer. Each host computer then divides its training examples of the batch into N small batches. This process minimizes the number of mini-batches into which each set of unique training examples is divided in each iteration of training. This, in turn, reduces the latency and overhead associated with each iteration of training, which reduces the overall training time and amount of computing resources, e.g., processing power, consumed by the training process.
The details of one or more embodiments of the subject matter in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a diagram of an exemplary distributed computing system.
FIG. 2 is a flow diagram of an example process for dividing a batch of neural network training examples into small batches.
FIG. 3 is a flow diagram of an exemplary process for processing small batches.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
FIG. 1 is a diagram of an exemplary distributed computing system 100. The distributed computing system 100 is configured to handle the embedded layer of the neural network.
The term "configured to" is used herein in connection with systems, apparatuses, and computer program components. For a system of one or more computers configured to perform a particular operation or action means: the system has installed thereon software, firmware, hardware, or a combination thereof that, in operation, causes the system to perform the operations or actions.
The embedding layer is a neural network layer that transforms sparse neural network inputs into embedded neural network inputs. Embedding is a dense multidimensional vector representation of the input. Intuitively, the values of each dimension of a particular embedding encode different characteristics of the input represented by the embedding.
Typically, the embedding layer is trained by: completing the forward transmission of the whole neural network to which the embedded layer belongs; determining whether there is a difference between an actual output of the neural network and an expected output of the given input; and updating the weights of the neural network, including the weights of the embedding layer, for example, by using conventional back propagation techniques or similar techniques known in the art. In other words, the embedding layer may be trained during the normal training process of the neural network.
The embedding layer can be reduced to a look-up table that stores an embedding for each potential feature of the neural network input. For example, if a particular neural network is configured to generate movie recommendations for a user of a streaming service by modeling the user's viewing history, a look-up table of the neural network may store a separate embedding for each movie available on the streaming service. As another example, if a particular neural network is configured to generate search engine results based on queries written in english, the look-up table of the neural network may store a separate embedding for each word in english.
Each feature is represented by a different index, e.g. by a different integer. The features may be arranged in a look-up table by indexing. Processing input using an embedding layer involves: looking up an index for each feature in the input in a look-up table; retrieving the embedding at each index; and combining the embeddings, for example by adding the embeddings together. In a lookup table implementation of the embedding layer, training the embedding layer involves updating values in the lookup table during the normal training process.
The distributed computing system 100 includes host computers 110a-110n. Host computers 110a-110n each store a partition of the lookup table of the embedding layer in embedded memory 112a-112 n. The look-up table is distributed between the host computers because the number of input features and the dimensions of each feature make the entire look-up table unsuitable for a scratch pad memory on a single host computer. In addition, obtaining an embedding from multiple host computers in parallel is faster than obtaining all the embeddings from a single host computer. In some implementations, the embedding memories 112a-112n store partitions of multiple lookup tables, where each lookup table may store embedding for a different type of feature. For example, referring again to a neural network configured to generate movie recommendations for users of a streaming service by modeling the viewing history of the users, the neural network may additionally receive input specifying genre preferences of the users. A separate look-up table may store genre embedding.
During iterations of training, each host computer in the distributed computing system 100 receives a batch of training examples. Each training example includes one or more features. To process a particular training example, the host computer obtains the embedding of each feature in the training example from one partition of the lookup table (i.e., from one or more of the embedding memories 112a-112 n). The host computer exchanges the embedding using the input/output units 114a-114 n. An input/output unit is a hardware component that includes temporary register space and a communication channel (e.g., a bus that connects host computers to each other). The host computer stores the obtained embeddings and metadata about those embeddings, e.g., deduplication information and the number of embeddings in the training examples, in the scratch pad memories 118a-118 n. The scratch pad memories 118a-118n are high speed internal memories, in close physical proximity to the processing units 116a-116n, and are used for temporary storage of computations and data. The temporary memory stores 118a-118n may be any suitable memory structure, such as SRAM.
Using separate embedding, the host computer uses the processing units 116a-116n to compute the embedding layer outputs for their respective training examples. The processing units 116a-116n may include dedicated hardware components to compute the embedded layer output. The host computer may include additional processing components not depicted in FIG. 1, such as a general purpose CPU. These additional components may perform some or all of the techniques described in this specification to determine the mini-batch size.
Each host computer in the distributed computing system may provide an embedded layer output to the system or subsystem implementing the remaining layers of the neural network. Such systems or subsystems may be implemented on a host computer or on separate physical hardware. Training continues as described above. Updating the embedding layer involves: updating, by each host computer, values in the partitions of the lookup table stored on the host computer based on the gradients received from other systems or subsystems implementing the remaining layers of the neural network.
Because each host computer has a limited amount of scratch pad memory to store the obtained embeddings, each host computer may only obtain embeddings for a limited number of training examples at a time. However, the amount of scratch pad memory used by the training examples varies depending on the number of features the training examples have. For example, referring again to a neural network configured to generate movie recommendations for users of a streaming service by modeling the viewing histories of the users, some users have a greater viewing history than others. For example, a training example of a first user may include hundreds of features (i.e., movies), while a training example of a second user may include only tens of features. Because the training examples of the first user have more features, the host computer will have to obtain and store more inlays for the first training examples, and those inlays will take up more space in the scratch pad memory. As another example, referring again to a neural network configured to generate search engine results based on queries written in english, some queries have more words than others. The host computer will have to obtain and store more embeddings for queries with more words, and those embeddings will take up more space in the scratch pad memory than embeddings for queries with fewer words. In addition, the neural network may take into account the query history of the user, but the number of queries in the query history may vary by user. Query histories with larger query numbers have more embeddings and therefore take up more space in the scratch pad. Rather, some queries include repeated words. The host computer need only obtain a single copy of the embedding for any repeated words, so queries with repeated words will have less embedding and therefore take up less space in the scratch pad memory. Similarly, two training examples in a single small batch may have common features. The host computer need only obtain a single copy of the embedding for any features that are repeated between training examples.
In summary, the number of training examples that a host computer can process at one time varies widely based on the nature of the training examples. In particular, the more unique features that occur in a given set of training examples, the more embedding the host computer must obtain from the look-up table. More embedding takes up more space than less embedding in the scratch pad memory. The distributed computing system described in this specification can optimize the number of training examples processed by the host computer at one time, i.e., the size of each mini-batch, and thus the mini-batch number of training examples for each batch. Specifically, each host computer receives a batch of training examples, divides the batch of training examples into an optimal number of mini-batches N, and processes one mini-batch at a time until it generates an embedding for each training example. This method will be described in more detail with reference to fig. 2.
FIG. 2 is a flow diagram of an exemplary process 200 for dividing a batch of neural network training examples into an appropriate number of small batches. The exemplary process 200 may be performed by the distributed computing system 100.
Each host computer in the system receives a respective batch of training examples for processing during an iteration of a training process for training a neural network (210). Each batch of training examples typically has the same number of training examples. Each training example has zero or more features. The number of features in each training example varies.
Each host computer calculates a minimum number of mini-batches into which it can divide the received training examples of the respective batch, such that it can process each mini-batch using the embedded layer of the neural network without exceeding the computing resources of the host computer (220). Typically, the host computer is limited by the amount of scratch memory (e.g., SRAM) on the host computer because to process the training examples, the host computer obtains the embedding from a partition of a lookup table stored on one of the host computers of the distributed computing system for each feature in each training example. The host computer stores the obtained inlays in a scratch pad memory on the host computer in preparation for processing those inlays.
Calculating the minimum number of small batches into which the host computer may divide its training examples of batches involves repeating the following for values of M from the upper limit to the lower limit: (i) Splitting training examples of a corresponding batch received by a host computer into M small batches; and (ii) for each mini-batch, determining whether the embedding corresponding to each input feature in each training example in the mini-batch fits into a scratch pad memory on the host computer.
The following pseudo-code illustrates this process:
in the pseudo code, i is the training example index and len is the total number of training examples in the batch of training examples. The first conditional statement tests whether a small batch including training examples 0 to i-1 fits in scratch pad on the host computer. If so, the host computer increments i. The host computer repeats this process until a small batch including training examples 0 through i-1 does not fit into the scratch pad memory. If the last mini-batch on the host computer that fits into the scratch pad has no training examples, the host computer will return an error. Otherwise, the host computer outputs the next largest mini-batch that fits into the scratch pad memory, i.e., the mini-batch that includes training examples 0 through i-2. Let i equal 7. This means that small batches including training examples 0 to 5 are suitable for scratch pad memory, but small batches including training examples 0 to 6 are not. The host computer then sets the variable to start at 6 and repeats this process for a small batch including training examples 6 through i-1. The result of this iterative process is a number of small batches, each fitting into a scratch pad memory on the host computer.
In some implementations, the above algorithm is modified to test out-of-order training examples. For example, while a small batch including training examples 0 through 6 may not fit into scratch pad memory on a host computer, a small batch including training examples 0 through 5 and 7 may fit. As mentioned above, this is because the size of each training example varies. In some cases, modifying the algorithm in this manner may reduce the number of resulting mini-batches. The above algorithm is merely illustrative and other suitable algorithms may be used.
Determining whether a particular set of training examples (i.e., a particular mini-batch) fits in scratch memory on a host computer typically involves: (ii) calculating the amount of scratch pad memory used by each training example, for example, by multiplying the number of features in each training example by the size of the features, (ii) summing the results of step (i), and (iii) determining whether the sum exceeds the total available scratch pad memory on the host computer. In calculating the amount of scratch pad memory used by a particular training example, the host computer considers any embeddings that have been considered, such as duplicate embeddings from previous training examples. In this example, the amount of scratch pad memory required is reduced by the amount corresponding to the repeated embedding from the previous training example.
The host computer also takes into account the amount of scratch pad memory it uses for processing (e.g., for performing the optimization algorithm described above).
Using the results calculated in the previous step, the system determines the maximum minimum number of small batches (N) into which any of the host computers can divide the training examples of its respective batch (230). In some implementations, one of the host computers is a designated master host, and the designated master host makes the determination. In such implementations, each other host computer sends its calculated minimum number of mini-batches to the designated master host, which determines the maximum of the minimum numbers. The designated master host may rotate among the host computers for different iterations of the training process. This is advantageous because the system can simultaneously count the number of small batches of training examples for multiple batches. After calculating N, the designated master host broadcasts the value N to all other host computers. In some other implementations, each host sends its minimum number of computations to an individual component of the distributed computing system, which determines the maximum of the minimum values. Other techniques for determining the maximum minimum number of small batches are also possible.
Each host computer divides the training examples of the respective batch received by the host computer into N small batches (240). In some implementations, when the host computer calculates the minimum number of mini-batches into which the host computer can divide the training examples of the respective batch, it also divides the training examples of the batch into that number of mini-batches. In such implementations, dividing the training examples of the respective batch into N mini-batches involves re-dividing or sub-dividing the mini-batch already present on the host computer into N mini-batches. In some cases, N may be the same as the minimum number of mini-batches of host computers. In other cases, N may be greater than the minimum number of mini-batches of host computers. In such cases, the host computer may generate a small batch without training examples. In other implementations, the host computer may split one mini-batch into multiple mini-batches, which may involve splitting the mini-batch into two mini-batches with the same number of training examples. Other splitting approaches are possible.
Finally, each host computer processes N mini-batches using the embedded layer of the neural network (250). Each host computer processes only one small batch at a time. This process will be described in more detail with reference to fig. 3.
FIG. 3 is a flow diagram of an exemplary process 300 for processing small batches. The process 300 may be performed by the distributed computing system 100.
Host computers of the distributed computing system each receive instructions to execute a batch of training examples (310). Typically, the number of training examples in each batch is the same, but the training examples in each batch are different.
The host computers perform the process described with reference to FIG. 2, i.e., each host computer divides the training examples of its respective batch into N mini-batches (320). The host computer processes these small batches in parallel. However, due to the computational constraints previously described, a particular host computer only processes one of its N mini-batches at a time.
Processing of the mini-batch by the host computer involves, for each input feature in each training example in the mini-batch, obtaining an embedding corresponding to the input feature from a trainable lookup table (330). The trainable lookup table is distributed among the host computers, so obtaining the embedding includes sending lookup requests to other host computers. The lookup request includes the desired embedded index. The host computer stores the obtained embedding in a scratch pad memory on the host computer.
The host computer processes the embeddings to generate an embedding layer output for each training example (340). Generating the embedding layer output for the particular training example involves adding or concatenating the embedding of each feature of the particular training example. Certain techniques are described in more detail in U.S. patent No. 9,898,441, which is incorporated herein by reference.
The host computer provides the embedded layer output for each training example to a system configured to implement the remaining layers of the neural network (350). Such a system may be implemented on a host computer or on separate physical hardware.
In some cases where the embedding is updated as part of the training process for training the neural network, the host computer receives back-propagation data from other systems. The host computer updates the embedding stored in the distributed lookup table using the back-propagation data (360). In particular, using the back-propagation data of the batch of training examples, each host computer can calculate a gradient of the loss with respect to the current value of each embedded dimension that the host computer stores. To reduce the loss contribution for each dimension, the host computer may adjust the value of the dimension by the ratio of the gradients. This is called a random gradient descent. Other suitable methods known in the art may be used to update these embeddings. This completes one iteration of training. The example process 300 can also be used to process a batch of input during reasoning by simply omitting step 360.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, tangibly embodied in computer software or firmware, in computer hardware (including the structures disclosed in this specification and their structural equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs (i.e., one or more modules of computer program instructions) encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and includes all kinds of apparatus, devices and machines for processing data, including for example a programmable processor, a computer or multiple processors or computers. The apparatus can also be or include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, app, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may (but need not) correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a data communication network.
For a system of one or more computers configured to perform a particular operation or action means: the system has installed thereon software, firmware, hardware, or a combination thereof that, in operation, causes the system to perform an operation or action. For one or more computer programs configured to perform certain operations or actions means: the one or more programs include instructions that, when executed by the data processing apparatus, cause the apparatus to perform operations or actions.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in combination with, special purpose logic circuitry, e.g., an FPGA or an ASIC.
A computer adapted to execute a computer program may be based on a general-purpose or special-purpose microprocessor or both, or any other kind of central processing unit. Typically, the central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or carrying out instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such devices. Further, the computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a Universal Serial Bus (USB) flash drive, etc.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example: semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse, a trackball, or a presence-sensitive display, or other surface) by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with the user; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on the user device in response to a request received from the web browser. Further, the computer may interact with the user by sending a text message or other form of message to a personal device (e.g., a smartphone), running a messaging application, and receiving a response message back from the user.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (20)
1. A system comprising a plurality of host computers configured to process an embedded layer of a neural network during training of the neural network, wherein the system is configured to perform operations comprising:
receiving, by each host computer, a respective batch of training examples for processing during an iteration of a training process to train the neural network, each training example having zero or more features, and wherein at least two different training examples in the respective batch received by the host computer have a different number of features;
calculating a minimum number of mini-batches into which the respective batches of training examples can be divided by the host computer based on the number of features in each of the training examples in the respective batches received by the host computer such that the host computer can process each mini-batch using the embedding layer of the neural network without exceeding available computing resources on the host computer, wherein at least two of the host computers calculate a different minimum number of mini-batches as a result of the at least two different training examples having different numbers of features;
determining a maximum minimum number of mini-batches N among the calculated minimum number of mini-batches per host computer from the minimum number of mini-batches;
generating N mini-batches from the training examples of the respective batch received by the host computer; and
processing, by each host computer, the N mini-batches of the training examples using the embedded layer of the neural network.
2. The system of claim 1, wherein processing a respective small batch of training examples using the embedding layer of the neural network comprises:
for each input feature in each training example in the small batch, obtaining a vector corresponding to the input feature from a trainable look-up table; and
for each training example, the obtained vector is processed to generate a vector representation.
3. The system of claim 2, wherein the trainable lookup table is distributed in the host computer.
4. The system of claim 3, wherein calculating the minimum number of mini-batches M into which the host computer can divide the respective batch of training examples comprises repeating, for values of M starting from an upper limit:
splitting the training examples of the respective batch received by the host computer into M small batches;
for each of the M mini-batches, determining whether the vector corresponding to each input feature in each training example in the mini-batch fits in memory on the host computer; and
until any one of the M mini-batches does not fit into memory on the host computer, M is decremented.
5. The system of claim 1, wherein the operations further comprise:
dividing the training examples of the respective batch received by the host computer into the calculated minimum number of mini-batches of the host computer; and
if N is greater than the calculated minimum number of mini-batches for the host computer, one mini-batch is re-divided into a plurality of mini-batches.
6. The system of claim 5, wherein splitting a mini-batch into multiple mini-batches comprises generating a mini-batch without training examples.
7. The system of claim 1, wherein one of the host computers is a designated master host, wherein the designated master host is configured to perform operations comprising:
receiving, from each host computer, the minimum number of mini-batches into which the host computer can divide training examples of the respective batch;
determining the maximum minimum number of mini-batches N into which any host computer can divide its respective batch of training examples; and
data specifying N is sent to each host computer.
8. The system of claim 7, wherein the designated master host rotates among the host computers for different iterations of the training process.
9. A method performed by a plurality of host computers configured to process an embedded layer of a neural network during training of the neural network, the method comprising:
receiving, by each host computer, a respective batch of training examples for processing during an iteration of a training process to train the neural network, each training example having zero or more features, and wherein at least two different training examples in the respective batch received by the host computer have a different number of features;
calculating a minimum number of mini-batches into which the respective batches of training examples can be divided by the host computer based on the number of features in each of the training examples in the respective batches received by the host computer such that the host computer can process each mini-batch using the embedding layer of the neural network without exceeding available computing resources on the host computer, wherein at least two of the host computers calculate a different minimum number of mini-batches as a result of the at least two different training examples having different numbers of features;
determining a maximum minimum number of mini-batches N among the calculated minimum number of mini-batches per host computer from the minimum number of mini-batches;
generating N mini-batches from the training examples of the respective batch received by the host computer; and
processing, by each host computer, the N mini-batches of the training examples using the embedded layer of the neural network.
10. The method of claim 9, wherein processing a respective small batch of training examples using the embedding layer of the neural network comprises:
for each input feature in each training example in the small batch, obtaining a vector corresponding to the input feature from a trainable look-up table; and
for each training example, the obtained vector is processed to generate a vector representation.
11. The method of claim 10, wherein the trainable lookup table is distributed in the host computer.
12. The method of claim 11, wherein calculating the minimum number of mini-batches M into which the host computer can divide the respective batch of training examples comprises repeating, for values of M starting from an upper limit:
splitting the training examples of the respective batch received by the host computer into M small batches;
for each of the M mini-batches, determining whether the vector corresponding to each input feature in each training example in the mini-batch fits in memory on the host computer; and
until any one of the M mini-batches does not fit into memory on the host computer, M is decremented.
13. The method of claim 9, further comprising:
dividing the training examples of the respective batch received by the host computer into the calculated minimum number of mini-batches of the host computer; and
if N is greater than the calculated minimum number of mini-batches of the host computer, one mini-batch is re-divided into a plurality of mini-batches.
14. The method of claim 13, wherein splitting a mini-batch into multiple mini-batches comprises generating a mini-batch without training examples.
15. The method of claim 9, wherein one of the host computers is a designated master host, wherein the designated master host is configured to perform operations comprising:
receiving, from each host computer, the minimum number of mini-batches into which the host computer can divide training examples of the respective batch;
determining the maximum minimum number of mini-batches N into which any host computer can divide training examples of its respective batch; and
data specifying N is sent to each host computer.
16. The method of claim 15, wherein the designated master host rotates among the host computers for different iterations of the training process.
17. One or more non-transitory computer-readable storage media storing instructions that, when executed by a processor of a system comprising a plurality of host computers configured to process an embedded layer of a neural network during training of the neural network, cause the system to perform operations comprising:
receiving, by each host computer, a respective batch of training examples for processing during an iteration of a training process to train the neural network, each training example having zero or more features, and wherein at least two different training examples in the respective batch received by the host computer have a different number of features;
based on the number of features in each of the training examples in the respective batch received by the host computer, calculating a minimum number of mini-batches into which the host computer can divide the training examples of the respective batch such that the host computer can process each mini-batch using the embedding layer of the neural network without exceeding available computing resources on the host computer, wherein at least two of the host computers calculate a different minimum number of mini-batches as a result of the at least two different training examples having different numbers of features;
determining a maximum minimum number of mini-batches N among the calculated minimum number of mini-batches per host computer from the minimum number of mini-batches;
generating N mini-batches from the training examples of the respective batch received by the host computer; and
processing, by each host computer, the N mini-batches of the training examples using the embedded layer of the neural network.
18. The computer-readable storage medium of claim 17, wherein processing a respective small batch of training examples using the embedded layer of the neural network comprises:
for each input feature in each training example in the small batch, obtaining a vector corresponding to the input feature from a trainable look-up table; and
for each training example, the obtained vector is processed to generate a vector representation.
19. The computer-readable storage medium of claim 18, wherein the trainable lookup table is distributed in the host computer.
20. The computer-readable storage medium of claim 19, wherein calculating the minimum number of mini-batches M into which the host computer can divide the training examples of the respective batch comprises repeating, for values of M starting from an upper limit:
splitting the training examples of the respective batch received by the host computer into M small batches;
for each of the M mini-batches, determining whether the vector corresponding to each input feature in each training example in the mini-batch fits in memory on the host computer; and
until any one of the M mini-batches does not fit into memory on the host computer, M is decremented.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/246,371 | 2019-01-11 | ||
US16/246,371 US10789510B2 (en) | 2019-01-11 | 2019-01-11 | Dynamic minibatch sizes |
PCT/US2019/066626 WO2020146098A1 (en) | 2019-01-11 | 2019-12-16 | Dynamic minibatch sizes |
CN201980058339.8A CN112655005B (en) | 2019-01-11 | 2019-12-16 | Dynamic small batch size |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980058339.8A Division CN112655005B (en) | 2019-01-11 | 2019-12-16 | Dynamic small batch size |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115329140A true CN115329140A (en) | 2022-11-11 |
Family
ID=69174605
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202210852423.0A Pending CN115329140A (en) | 2019-01-11 | 2019-12-16 | Dynamic small batch size |
CN201980058339.8A Active CN112655005B (en) | 2019-01-11 | 2019-12-16 | Dynamic small batch size |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980058339.8A Active CN112655005B (en) | 2019-01-11 | 2019-12-16 | Dynamic small batch size |
Country Status (5)
Country | Link |
---|---|
US (2) | US10789510B2 (en) |
EP (1) | EP3827376A1 (en) |
CN (2) | CN115329140A (en) |
TW (2) | TWI758223B (en) |
WO (1) | WO2020146098A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11354573B2 (en) * | 2019-03-25 | 2022-06-07 | International Business Machines Corporation | Dynamically resizing minibatch in neural network execution |
WO2023195011A1 (en) * | 2022-04-04 | 2023-10-12 | R-Stealth Ltd. | System and method for model training in decentralized computing environments |
Family Cites Families (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20120120159A (en) * | 2009-12-04 | 2012-11-01 | 다카토시 야나세 | Table search device, table search method, and table search system |
US8904149B2 (en) | 2010-06-24 | 2014-12-02 | Microsoft Corporation | Parallelization of online learning algorithms |
WO2015116909A1 (en) | 2014-01-31 | 2015-08-06 | Google Inc. | Generating vector representations of documents |
US20150324686A1 (en) | 2014-05-12 | 2015-11-12 | Qualcomm Incorporated | Distributed model learning |
ES2738319T3 (en) | 2014-09-12 | 2020-01-21 | Microsoft Technology Licensing Llc | Computer system to train neural networks |
WO2016123409A1 (en) * | 2015-01-28 | 2016-08-04 | Google Inc. | Batch normalization layers |
US10083395B2 (en) * | 2015-05-21 | 2018-09-25 | Google Llc | Batch processing in a neural network processor |
US10552454B2 (en) * | 2015-11-13 | 2020-02-04 | Sap Se | Efficient partitioning of related database tables |
US9898441B2 (en) | 2016-02-05 | 2018-02-20 | Google Llc | Matrix processing apparatus |
KR102155261B1 (en) * | 2016-04-13 | 2020-09-11 | 구글 엘엘씨 | Wide and deep machine learning models |
CN106127702B (en) * | 2016-06-17 | 2018-08-14 | 兰州理工大学 | A kind of image defogging method based on deep learning |
US11394426B2 (en) * | 2016-06-22 | 2022-07-19 | Korrus, Inc. | Intelligent modules for intelligent networks |
JP2018018451A (en) * | 2016-07-29 | 2018-02-01 | 富士通株式会社 | Machine learning method, machine learning program and information processing device |
CA3047353C (en) * | 2017-01-06 | 2023-05-23 | The Toronto-Dominion Bank | Learning document embeddings with convolutional neural network architectures |
US10915817B2 (en) * | 2017-01-23 | 2021-02-09 | Fotonation Limited | Method of training a neural network |
US11144828B2 (en) * | 2017-06-09 | 2021-10-12 | Htc Corporation | Training task optimization system, training task optimization method and non-transitory computer readable medium for operating the same |
TWI636404B (en) * | 2017-07-31 | 2018-09-21 | 財團法人工業技術研究院 | Deep neural network and method for using the same and computer readable media |
-
2019
- 2019-01-11 US US16/246,371 patent/US10789510B2/en active Active
- 2019-12-16 CN CN202210852423.0A patent/CN115329140A/en active Pending
- 2019-12-16 EP EP19839215.1A patent/EP3827376A1/en active Pending
- 2019-12-16 WO PCT/US2019/066626 patent/WO2020146098A1/en unknown
- 2019-12-16 CN CN201980058339.8A patent/CN112655005B/en active Active
- 2019-12-31 TW TW110131206A patent/TWI758223B/en active
- 2019-12-31 TW TW108148515A patent/TWI740338B/en active
-
2020
- 2020-09-28 US US17/034,338 patent/US20210019570A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
TWI740338B (en) | 2021-09-21 |
WO2020146098A1 (en) | 2020-07-16 |
EP3827376A1 (en) | 2021-06-02 |
US20210019570A1 (en) | 2021-01-21 |
US10789510B2 (en) | 2020-09-29 |
TW202145078A (en) | 2021-12-01 |
CN112655005A (en) | 2021-04-13 |
US20200226424A1 (en) | 2020-07-16 |
TW202026954A (en) | 2020-07-16 |
TWI758223B (en) | 2022-03-11 |
CN112655005B (en) | 2022-07-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11687832B1 (en) | Training a model using parameter server shards | |
US20230252327A1 (en) | Neural architecture search for convolutional neural networks | |
US8209271B1 (en) | Predictive model training on large datasets | |
US20200265315A1 (en) | Neural architecture search | |
EP3602419B1 (en) | Neural network optimizer search | |
CN111652378B (en) | Learning to select vocabulary for category features | |
US20240127058A1 (en) | Training neural networks using priority queues | |
EP3542319A1 (en) | Training neural networks using a clustering loss | |
US11948086B2 (en) | Accelerated embedding layer computations | |
WO2021195095A1 (en) | Neural architecture search with weight sharing | |
CN112655005B (en) | Dynamic small batch size | |
WO2022216879A2 (en) | Full-stack hardware accelerator search | |
CN117121016A (en) | Granular neural network architecture search on low-level primitives | |
JP2024504179A (en) | Method and system for lightweighting artificial intelligence inference models | |
EP4182850A1 (en) | Hardware-optimized neural architecture search | |
WO2021062219A1 (en) | Clustering data using neural networks based on normalized cuts | |
WO2022216878A1 (en) | Optimizing off-chip memory accesses on a neural network hardware accelerator |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
REG | Reference to a national code |
Ref country code: HKRef legal event code: DERef document number: 40082723Country of ref document: HK |