CN115210692A - Interface and mode selection for digital motion execution - Google Patents
Interface and mode selection for digital motion execution Download PDFInfo
- Publication number
- CN115210692A CN115210692A CN202080097834.2A CN202080097834A CN115210692A CN 115210692 A CN115210692 A CN 115210692A CN 202080097834 A CN202080097834 A CN 202080097834A CN 115210692 A CN115210692 A CN 115210692A
- Authority
- CN
- China
- Prior art keywords
- digital
- computing device
- data processing
- client computing
- processing system
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/455—Emulation; Interpretation; Software simulation, e.g. virtualisation or emulation of application or operating system execution engines
- G06F9/45504—Abstract machines for programme code execution, e.g. Java virtual machine [JVM], interpreters, emulators
- G06F9/45508—Runtime interpretation or emulation, e g. emulator loops, bytecode interpretation
- G06F9/45512—Command shells
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/34—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment
- G06F11/3438—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment monitoring of user actions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y02—TECHNOLOGIES OR APPLICATIONS FOR MITIGATION OR ADAPTATION AGAINST CLIMATE CHANGE
- Y02D—CLIMATE CHANGE MITIGATION TECHNOLOGIES IN INFORMATION AND COMMUNICATION TECHNOLOGIES [ICT], I.E. INFORMATION AND COMMUNICATION TECHNOLOGIES AIMING AT THE REDUCTION OF THEIR OWN ENERGY USE
- Y02D10/00—Energy efficient computing, e.g. low power processors, power management or thermal management
Abstract
An interface and mode selection for digital action execution is provided. For example, the system loads a script library embedded in the electronic resource. The system determines a level of historical engagement between the client computing device and the one or more digital assistants. The system selects a type of digital interface based on a first attribute of the client computing device and a historical level of engagement. The system generates a digital interface with a call action based on the type of digital interface. The system determines an execution mode in response to executing the instruction that invokes the action. The system selects the digital assistant and the second client device to perform the invoking action. The system transmits the call action to the second client device for execution.
Description
Background
The computing device is capable of executing applications. An application can provide a user interface that can receive input information, perform functions, and output information.
Disclosure of Invention
The technical solution generally relates to selecting the type of interface and mode for performing a digital action. The technical solution enables an interface or mode to be adjusted based on the type of device and the historical level of engagement between the device and the digital assistant. For example, executing different types of interfaces or modes can utilize different amounts of computational resources, energy or power resources, network bandwidth resources, display resources, sensor resources, or time. Thus, selecting an interface that consumes too much resources may be inefficient relative to resources that use fewer resources. However, certain resources may not be compatible with certain types of devices or may perform inefficiently in certain computing environments, resulting in potentially more efficient interfaces that ultimately consume too much resources to complete the performance of a digital action. Thus, the systems and methods of the present technical solution are able to select the type of interface or mode to perform the digital action to reduce excessive resource consumption, thereby increasing the efficiency of the computing environment to perform the digital action.
At least one aspect of the technical solution relates to a system for adjusting the execution of a digital action. The system can include a data processing system including one or more processors. The data processing system is capable of loading a script library embedded in the electronic resource. The data processing system is capable of loading a script library via the client computing device. The client device can be one of a plurality of client computing devices linked to the electronic account. The script library can include one or more invocation actions for the electronic resources that are configured for execution by one or more digital assistants provided by each of the client computing devices. The data processing system can query the digital assistant components to determine a level of historical engagement between the client computing device and the one or more digital assistants. The data processing system can select a type of digital interface in which to present a calling action of the plurality of calling actions based on the first attribute of the client computing device and the historical engagement level. The data processing system can generate a digital interface with a call action based on a type of digital interface selected according to a first attribute and a historical level of engagement of the client computing device. The data processing system is capable of detecting, via the digital interface, an instruction to perform a call action. The data processing system can determine, in response to the instruction to perform the invoking action, an execution mode for invoking the action based on the second attribute of the client computing device and the historical engagement levels. The data processing system can select a digital assistant from the one or more digital assistants and a second client device of the plurality of client computing devices to perform the invoking action based on the execution mode. The data processing system can transmit the call action to the second client device to cause the second client device to call the digital assistant to perform the call action.
At least one aspect of the technical solution relates to a method of adjusting the execution of a digital action. The method can be performed by a data processing system comprising one or more processors. The method can include the data processing system loading, via a client computing device of a plurality of client computing devices linked to the electronic account, a script library embedded in the electronic resource. The script library can include invocation actions for the electronic resources that are configured for execution by one or more digital assistants provided by each of the client computing devices. The method can include the data processing system querying a digital assistant component to determine a level of historical engagement between a plurality of client computing devices and one or more digital assistants. The method can include the data processing system selecting a type of digital interface in which to present a calling action of the plurality of calling actions based on a first attribute of the client computing device and a historical level of engagement. The method can include the data processing system generating a digital interface with a call action based on a type of digital interface selected according to a first attribute and a historical engagement level of the client computing device. The method can include the data processing system detecting, via the digital interface, an instruction to perform a call action. The method can include the data processing system determining, in response to executing the instruction to invoke the action, an execution mode for invoking the action based on a second attribute of the client computing device and the historical engagement level. The method can include the data processing system selecting a digital assistant from the one or more digital assistants and a second client device of the plurality of client computing devices to perform the invoking action based on the execution mode. The method can include the data processing system transmitting a call action to the second client device to cause the second client device to call the digital assistant to perform the call action.
These and other aspects and embodiments are discussed in detail below. The foregoing information and the following detailed description include illustrative examples of various aspects and embodiments, and provide an overview or framework for understanding the nature and character of the claimed aspects and embodiments. The accompanying drawings are included to provide an illustration and a further understanding of the various aspects and embodiments, and are incorporated in and constitute a part of this specification.
Drawings
The figures are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing. On the attached sheet
In the figure:
fig. 1 is a diagram of a system to adjust digital actions, according to an embodiment.
Fig. 2 is an illustration of a process of adjusting the execution of a digital action according to an embodiment.
Fig. 3 is an illustration of a process of adjusting the execution of a digital action according to an embodiment.
Fig. 4 is an illustration of a graphical user interface for performing a digital action, according to an embodiment.
Fig. 5 is an illustration of a graphical user interface for performing a digital action, according to an embodiment.
Fig. 6 is an illustration of a method of adjusting performance of a digital action according to an embodiment.
FIG. 7 is a block diagram illustrating the general architecture of a computer system that can be employed to implement the elements of the system depicted in FIG. 1, the processes and methods depicted in FIGS. 2, 3, and 6, and the graphical user interface elements depicted in FIGS. 4 and 5.
Detailed Description
The following is a more detailed description of various concepts and embodiments thereof related to methods, apparatus and systems for interface and mode selection for digital action execution. For example, the method, apparatus and system can adjust the performance of digital actions. The various concepts introduced above and discussed in greater detail below may be implemented in any of numerous ways.
The technical solution generally relates to selecting an interface or mode for performing a digital action. The system and method of the technical solution is capable of adjusting the execution of the digital action by selecting the type of interface or mode of execution of the digital action.
The actions can be linked for execution by the voice-based digital assistant. An action (or function) can refer to or include any type of digital action that can be performed by one or more applications, computing devices, servers, or electronic resources. For third party ("3P") applications or electronic resources that can interface with a digital assistant, it can be challenging or impossible to efficiently provide notifications of functions that the digital assistant can perform on the application. As the number of types of actions that digital assistants are capable of performing continues to increase, it has been found that these functions can become increasingly challenging and inefficient, let alone performing functions properly in an optimal time or manner in a given context.
For example, the notification can include displaying or playing audio that indicates the type of function or action that the digital assistant is capable of performing. However, displaying or playing audio that lists all available digital assistant functionality with respect to the 3P application can be time consuming, inefficient, introduce delay, and provide sub-windows or wasted user interfaces, as many of these functions may not be relevant in the current computing environment or context. Systems that lack knowledge of the relevant functions that a digital assistant is capable of performing on a 3P application or electronic resource can result in the digital assistant or application being underutilized or inefficiently utilized by performing unnecessary or multiple actions that may otherwise be performed by more efficient digital assistant functions. Thus, the systems and methods of the present technology solution are able to select an interface or execution mode that can improve the efficiency with which an action can be performed (e.g., by selecting a digital assistant function or action).
The present technical solution can facilitate identifying and selecting available and related functions that a digital assistant having a speech-based interface can perform on a 3P application, and facilitating performance of the related functions. For example, the system can capture the available actions or functions that the digital assistant can perform on a particular application. The system can identify which actions are relevant to the computing environment context. The system can provide an indication of those related actions (e.g., invoke the action) via the user interface. The system can immediately perform those related actions from within the application, linking the invocation action directly to the related action on the digital assistant (e.g., action link).
For example, the user can navigate to a website, such as an electronic financial instrument transaction website. The system can capture or detect contextual computing information (e.g., actions associated with a website, such as logging in, authenticating an account to determine a current portfolio value, a transaction status, or performing a transaction or fund transfer). The system can identify available actions associated with the current computing environment that the voice-based digital assistant is configured to perform. The system can provide an indication (e.g., invoke an action) directly via a user interface within the website that provides notification of available digital assistant actions related to the current computing environment. The system can receive or detect an instruction, a command, or an interaction with one of the digital assistant actions presented via the notification. The system can provide a digital assistant interface that can facilitate execution or invocation of the selected action. Thus, the technical solution identifies and selects relevant actions on third party applications or electronic resources that the digital assistant can perform more efficiently, relative to actions performed directly by the 3P application or electronic resource without the digital assistant. The system can discover that the digital assistant is configured to perform one or more actions associated with an application or electronic resource (e.g., a website) when the application is invoked, identify which actions are relevant to the current computing environment, display the invoked actions that provide notification of the relevant actions, and then perform the relevant actions.
The system can provide dynamic interface adjustment for invoking action discovery. For 3P applications or electronic resources that provide functionality through a voice interface, it can be challenging or impossible to effectively provide notification of the application or resource-provided functionality in a manner that does not disrupt current non-voice computing interactions with the application or resource. The system can provide a platform-specific action linking mechanism for a voice-based digital assistant. For 3P applications or electronic resources that provide functionality through a voice interface, it can be challenging or impossible to allow efficient invocation of 3P-provided voice interface functionality from non-voice contexts that inform a user of the functionality.
For example, upon discovering an available action that is relevant to the current computing environment context, the system can determine what type of invocation action (notification of the available action) is provided to the user. The system can include a dynamic notifier component that can dynamically determine a level of engagement with an application or electronic resource (e.g., based on parameters, characteristics, or features associated with a current computing environment) in order to determine which invocation action to display, and a format or type of invocation action. For example, if a client device is actively interacting with a website to perform certain digital actions, the dynamic notifier of the technical solution can provide a pop-up window with the discovered invocation actions, as the device (or its user) may be more likely to engage with a digital assistant to perform the corresponding actions. By providing a more prominent notification (e.g., relative to an embedded notification or a banner notification or a small icon located at a corner of a website), a dynamic notifier can increase the likelihood of engaging with a digital assistant to perform an action, thereby preventing direct execution of a function via a non-speech-based interface with an application that may consume additional computing resources or waste time.
The system can determine that a user of the client device is passively viewing the application or electronic resource. Passively viewing an electronic resource can refer to a non-interactive state or viewing the electronic resource without interacting with the electronic resource. Not interacting with the electronic resource can mean that the client device has not received input for the electronic resource for a period of time. Examples of applications or electronic resources that can be viewed passively can include music, video, streaming music, streaming video, or other types of audio or video content that can be viewed or consumed without interaction for a period of time. Upon detecting that an electronic resource is being passively viewed, the dynamic notifier can select a different format or type of notification that can be less noticeable than the type of notification that can be selected for the application or electronic resource that is being actively consumed. Examples of less obvious notifications can include an audio or voice overlay, or a small icon positioned in a corner of a display screen or application graphical user interface.
When the system receives an interaction with a call action on a third party application or electronic resource, the system can execute a platform specific action link for the voice-based digital assistant. Platform-specific action linking can refer to the system determining the appropriate platform-specific technology for guiding or navigating the user to the selected action within the digital assistant. The system can include a digital assistant selector component (e.g., a dynamic device selector) that can dynamically select a device to be used to perform a digital action. The system is capable of identifying a plurality of devices linked to each other for the purpose of performing a digital action. A device that displays notifications and receives instructions to perform a call action (e.g., a digital action) can be linked to an account that can be associated or linked with multiple devices. One or more devices linked to the account can be configured to or can perform or execute digital actions. The device linked to the account can be configured with a digital assistant. One or more devices can be available to perform actions in response to instructions or during some other desired time interval. The system can use a selection policy to identify or select a device for performing the digital action. The selection policy can select a device based on a current computing environment or context.
An example of platform-specific action linking can include using a deep link to initiate or trigger a local digital assistant to execute on a client device to perform an action. In another example, the system can generate a generic application link or a custom uniform resource locator to launch a non-native digital assistant application on the client device to act. In yet another example, the system can select a different device with a digital assistant to act on. A platform specific action link can refer to or include, for example, performing an action based on the type of operating system, the type of device, the characteristics of the device, the configuration of the device, or the capabilities of the device.
Thus, in some cases, the system can dynamically determine the execution mode based on the configuration, the type of device, the application installed on the device, the network connection, or other characteristics associated with the computing environment. For example, if a digital assistant application is installed, the system can navigate to the digital assistant application and enter commands to perform digital actions in response to invoking the actions. On the other hand, if the digital assistant application is not installed on the device, the system can provide a notification requesting installation of the digital assistant application before performing the digital action. The notification can include a link, path, or pointer to a cloud-based repository of application executables that can be downloaded and installed by the device.
The system can launch the digital assistant on a different device to perform the digital action. For example, the electronic resource can include a web page rendered in an application such as a web browser. The invoking action can be presented via a web browser. The invoking action can be overlaid on the electronic resource as a banner, pop-up, break-in, or in some other manner. The system can detect interaction with the invoked action or other indication to perform a digital action corresponding to the invoked action. The system can determine current computing environment information, such as the type of web browser, the device, the network connection, account information associated with the device executing the application, or additional devices linked to the account. The system can display a web page having a list of devices linked to the same account as the device providing the call action via a dynamic device selector component or function. The system can pre-process or filter the list of devices so that the only devices presented are those devices that are compatible or capable of performing the digital action corresponding to the invoked action. In some cases, the system can rank the list of devices to weight the devices based on relevance, capability, compatibility, performance, or other factors. For example, the system can rank or filter the devices based on relevance to the digital actions (e.g., only show the devices a screen of actions that can provide graphical output). The dynamic device selector of the present technical solution is capable of ranking the list of devices such that the devices most relevant to the current computational context are ranked higher than the devices less relevant to the current computational context (e.g., the most recently used devices are ranked higher than the least recently used devices, and the frequently used devices are ranked higher than the less frequently used devices).
FIG. 1 illustrates an example system 100 that selects an interface and mode for execution of a digital action. The system 100 is capable of adjusting the performance of digital actions. The system 100 can include a content selection architecture. The system 100 can include a data processing system 102. Data processing system 102 can include computing device 122 or execute on computing device 122. The data processing system 102 is capable of communicating with one or more of the 3P digital content provider device 160, the remote data processing system 136, or the 3P electronic resource servers 142, 148 via the network 105. The network 105 can include a computer network, such as the internet, a local area network, a wide area network, a metropolitan area network, or other area network, an intranet, a satellite network, and other communication networks, such as a voice or data mobile telephone network. The network 105 can be used to access information resources, such as web pages, web sites, domain names, or uniform resource locators, which can be presented, output, rendered, or displayed on at least one computing device 122, such as a laptop computer, desktop computer, tablet computer, personal digital assistant, smart phone, portable computer, or speaker. For example, via network 105, a user of computing device 122 can access information or data provided by 3P digital content provider device 134. Computing device 122 may or may not include a display; for example, a computing device may include a limited type of user interface, such as a microphone and speaker. In some cases, the primary user interface of the computing device 122 may be a microphone and a speaker. The computing device 122 can interact with or be included in a speech-based computing environment.
The network 105 can be used by the data processing system 102 to access information resources, such as applications, web pages, web sites, domain names, or uniform resource locators, that can be rendered, output, rendered, or displayed by the client computing devices 122. For example, a user of client computing device 122 can access information or data provided by 3P digital content provider device 134 via network 105. The network 105 can include or constitute a sub-network of information resources available on the internet that are associated with a content delivery or search engine results system, or that are eligible to include third party digital components as part of a digital component delivery campaign.
The network 105 may be any type or form of network and may include any of the following: point-to-point networks, broadcast networks, wide area networks, local area networks, telecommunications networks, data communication networks, computer networks, ATM (asynchronous transfer mode) networks, SONET (synchronous optical network) networks, SDH (synchronous digital hierarchy) networks, wireless networks and wired networks. The network 105 may include a wireless link, such as an infrared channel or satellite band. The topology of the network 105 may include a bus, star, or ring network topology. The network may include a mobile telephone network using any one or more protocols for communicating between mobile devices, including advanced mobile phone protocol ("AMPS"), time division multiple access ("TDMA"), code division multiple access ("CDMA"), global system for mobile communications ("GSM"), general packet radio service ("GPRS"), or universal mobile telecommunications system ("UMTS"). Different types of data may be transmitted via different protocols, or the same type of data may be transmitted via different protocols.
The system 100 can include, interface with, communicate with, or otherwise access a 3P electronic resource server 142. The data processing system 102 is capable of communicating with the 3P electronic resource server 142 via the network 105. The 3P electronic resource server 142 can be remote and distinct from the data processing system 102, the remote data processing system 136, the 3P digital content provider device 134, and the computing device 122. The 3P electronic resource server 142 can be associated with a developer of the electronic resource 112. The 3P electronic resource server 142 can facilitate execution of the electronic resource 112. For example, the 3P electronic resource server 142 can perform back-end processing on the electronic resource 112 executed by the application 110. An application 110 executing on the computing device 122 can execute the front-end components of the application 110, and a 3P electronic resource server 142 can execute the back-end components of the application 110. An application 110 executing on computing device 122 can transmit a remote procedure call or other request or data to 3P electronic resource server 142. The 3P electronic resource server 142 can transmit data, information or requests to the application 110 to perform functions or perform actions. The 3P electronic resource server 142 can modify or change the state of the application 110 or electronic resource 112 executed or provided via the application 110 executing on the computing device 122.
The system 100 can include at least one remote data processing system 136. The remote data processing system 136 can include at least one logical device, such as a computing device having a processor to communicate with, for example, the data processing system 102 via the network 105, a 3P digital content provider device 134 (e.g., a content provider). The remote data processing system 136 can include at least one computing resource, server, processor, or memory. For example, the remote data processing system 136 can include a plurality of computing resources or servers located in at least one data center. The remote data processing system 136 can include a number of servers grouped logically together and can facilitate distributed computing techniques. A logical group of servers may be referred to as a data center, a server farm, or a machine farm. Servers can also be geographically dispersed. A data center or machine farm can be managed as a single entity, or a machine farm can include multiple machine farms. The servers within each computer farm can be heterogeneous — one or more servers or machines can operate according to one or more operating system platforms.
The servers in the machine farm can be stored in a high density rack system with associated storage systems and can be located in an enterprise data center. For example, consolidating servers in this manner may improve system manageability, data security, physical security of the system, and system performance by placing servers and high performance storage systems on a localized high performance network. The centralization of all or some of the remote data processing system 136 components, including servers and storage systems, and coupling them together with advanced system management tools allows for more efficient utilization of server resources, which saves power and processing requirements and reduces bandwidth usage.
The remote data processing system 136 can include a digital assistant server 138 and a content selector component 140. The digital assistant server 138 can be designed, constructed, and operated to perform one or more functions of communicating with the digital assistant component 106 of the computing device 122. The content selector component 140 can be designed, constructed and operated to select digital component items (e.g., content items) provided by the 3P digital content provider device 134. The content selector component 140 can select content items in response to requests for content from the computing device 122. The content selector component 140 can transmit the selected content item to the computing device 122 for presentation (e.g., audio output, visual output, or audiovisual output).
The system 100 can include, access, or otherwise interact with at least one 3P digital content provider device 134. The 3P digital content provider device 134 can include at least one logic device, such as a computing device having a processor, to communicate with, for example, the computing device 122, the data processing system 102, or a remote data processing system 136 via the network 105. The 3P digital content provider device 134 can include at least one computing resource, server, processor, or memory. For example, the 3P digital content provider device 134 can include a plurality of computing resources or servers located in at least one data center. The 3P digital content provider device 134 can include or refer to a service provider device or a goods provider device.
The 3P digital content provider device 134 can provide audio-based digital components for rendering by the computing device 122 as audio output digital components. The numeric component can include a response to a search query or request. The digital component can include information from a database, search engine, or network resource. For example, the digital components can include news information, weather information, sports information, encyclopedia entries, dictionary entries, or information from digital textbooks. The digital components can include offers for goods or services, such as stating "do you want me to give you a taxi? (Wauld you like me to order you a taxi). The 3P digital content provider device 134 can include a memory to store a series of audio digital components that can be provided in response to a voice-based query. The 3P digital content provider device 134 can also provide the audio-based digital components (or other digital components) to a remote data processing system 136 where they can be stored for selection by the content selector component 140. The remote data processing system 136 can select the audio digital component and provide (or instruct the content provider computing device 140 to provide) the digital component to the client computing device 122. The audio-based digital component can be specialized audio, or can be combined with text, image, or video data. The digital components or content items can include one or more formats of image, text, video, multimedia, or other types of content.
The remote data processing system 136 may include a content delivery system having at least one computing resource or server. The remote data processing system 136 can include at least one content selector component 108 coupled to or otherwise in communication with the at least one content selector component 140. The remote data processing system 136 can include, interface with, or otherwise communicate with at least one digital assistant server 138.
The content selector component 140 and the digital assistant server 138 can each include at least one processing unit or other logic device, such as a programmable logic array engine, or modules configured to communicate with each other or other resources or databases. The content selector component 140 and the digital assistant server 138 can be separate components, a single component, or part of the remote data processing system 136. The system 100 and its components, such as the remote data processing system 136, can include hardware elements, such as one or more processors, logic devices, or circuits.
The remote data processing system 136 is able to obtain anonymous computer network activity information associated with the plurality of computing devices 122. The user of computing device 122 can positively authorize the remote data processing system 136 to obtain network activity information corresponding to the user's computing device 122. For example, the remote data processing system 136 can prompt a user of the computing device 122 for consent to obtain one or more network activity information. The identity of the user of the computing device 122 can remain anonymous, and the computing device 122 can be associated with a unique identifier (e.g., a unique identifier for the user or the computing device or the user of the computing device provided by the data processing system). The remote data processing system 136 can associate each observation with a corresponding unique identifier.
The 3P digital content provider device 134 is capable of establishing an electronic content campaign. The electronic content campaign can be stored as content data in a repository of the content selector component 140. An electronic content campaign can refer to one or more groups of content that correspond to a common topic. The content campaign can include a hierarchical data structure including content groups, digital component data objects, and content selection criteria. To create a content campaign, the 3P digital content provider device 134 can specify values for the campaign level parameters of the content campaign. The activity level parameters can include, for example, the name of the activity, a preferred content network for delivering the digital component object, the value of the resource to be used for the content activity, the start and end dates of the content activity, the duration of the content activity, the schedule of delivery of the digital component object, the language, the geographic location, the type of computing device on which the digital component object is to be provided. In some cases, an impression can refer to when a digital component object is extracted from its source (e.g., remote data processing system 136 or digital content provider device 134) and is countable. In some cases, robot activity may be filtered and excluded as an impression due to the possibility of click fraud. Thus, in some cases, an impression can refer to a measure of the response from the Web server to a page request from the browser that filters out robot activity and error codes and is recorded at a point as close as possible to the opportunity to render the digital component object for display on the display device 122. In some cases, an impression can refer to a visible or audible impression; for example, the digital component object is at least partially visible (e.g., 20%, 30%, 40%, 50%, 60%, 70%, or more) on a display device of the client computing device 122 or audible via a speaker of the computing device 122. Clicking or selecting can refer to a user's interaction with a digital component object, such as a voice response to an audible impression, a mouse click, a touch interaction, a gesture, a shake, an audio interaction, or a keyboard click. Conversion can refer to a user taking a desired action with respect to a digital component object; for example, purchasing a product or service, completing a survey, visiting a physical store corresponding to a digital component, or completing an electronic transaction.
The 3P digital content provider device 134 can further establish one or more content groups for the content campaign. The content set includes one or more digital component objects and corresponding content selection criteria, such as keywords, words, terms, phrases, geographic locations, type of computing device, time of day, interests, topics, or vertical. Content groups under the same content campaign are able to share the same campaign level parameters, but the specifications may be customized for particular content group level parameters, such as keywords, negative keywords (e.g., to prevent digital components from being released if negative keywords are present on the primary content), keyword bids, or parameters associated with bids or content campaigns.
To create a new content group, the 3P digital content provider device 134 can provide the value of the content group level parameter for the content group. Content group level parameters include, for example, content group name or content group subject matter and bids for different content delivery opportunities (e.g., automatic delivery or managed delivery) or results (e.g., clicks, impressions, or conversions). The content group name or content group subject matter can be one or more terms that the 3P digital content provider device 134 can use to capture for display the topic or subject matter of the content group whose digital component object is to be selected. For example, an auto dealer can create a different content set for each brand of vehicle that it sells, and can further create a different content set for each model of vehicle that it sells. Examples of content group themes that can be used by car dealers include, for example, "brand a sports car (Make a sports car)", "brand B sports car (Make B sports car)", "brand C sedan (Make C stop)", "brand C truck (Make C truck)", "brand C hybrid vehicle (Make C hybrid)" or "brand D hybrid vehicle (Make D hybrid)". An example content activity theme can be "hybrid vehicle" and include, for example, a content group for both a "brand C hybrid" and a "brand D hybrid".
The 3P digital content provider device 134 can provide one or more keywords and digital component objects to each content group. The keywords can include terms related to the product or service associated with or identified by the digital component object. The keywords can include one or more terms or phrases. For example, an automobile dealer can include "sports car," "V-6 engine," "four wheel drive," "fuel efficiency" as a keyword for a content group or content activity. In some cases, negative keywords can be specified by the content provider to avoid, prevent, block, or disable content delivery to certain terms or keywords. The content provider can specify a match type, such as an exact match, a phrase match, or a broad match, for selecting the digital component object.
The 3P digital content provider device 134 can provide one or more keywords for use by the remote data processing system 136 to select the digital component object provided by the 3P digital content provider device 134. The 3P digital content provider device 134 can identify one or more keywords to bid on and further provide bid amounts for the various keywords. The 3P digital content provider device 134 can provide other content selection criteria for use by the remote data processing system 136 to select digital component objects. Multiple 3P digital content provider devices 134 can bid on the same or different keywords, and the remote data processing system 136 can run a content selection process or an advertising auction in response to receiving an indication of a keyword of an electronic message.
The 3P digital content provider device 134 can provide one or more digital component objects for selection by the remote data processing system 136. The remote data processing system 136 (e.g., via the content selector component 140) can select a digital component object when content delivery opportunities become available that match resource allocation, content schedule, highest bids, keywords, and other selection criteria specified for the content group. Different types of digital component objects can be included in the content set, such as a voice digital component, an audio digital component, a text digital component, an image digital component, a video digital component, a multimedia digital component, or a digital component link. Upon selection of the digital component, the remote data processing system 136 can transmit the digital component object for presentation via the computing device 122, rendering on the computing device 122, or on a display device of the computing device 122. Rendering can include displaying the digital component on a display device or playing the digital component via a speaker of the computing device 122. The remote data processing system 136 can provide instructions to the computing device 122 to render the digital component objects. The remote data processing system 136 can instruct the digital assistant component 106 of the computing device 122 or the audio driver 132 of the computing device 122 to generate an audio signal or sound wave. The remote data processing system 136 can instruct the application 110 executed by the computing device 122 to render the selected digital component object. For example, the application 110 can include slots (e.g., content slots) in which digital component objects can be rendered (e.g., audio slots or visual slots).
The data processing system 102 can include, execute, or otherwise communicate with the content selector component 140 to receive a query, keyword, or trigger keyword identified by the natural language processor and select a digital component based on the trigger keyword. The content selector component 140 can select the digital component via a real-time content selection process. The content selection process can include, for example, performing a search via a search engine, or accessing a database stored on a remote server or device, such as the 3P digital content provider device 134. The content selection process can refer to or include: a sponsored digital component object provided by the third-party content provider 134 is selected. The real-time content selection process can include a service in which digital components provided by multiple content providers are parsed, processed, weighted, or matched in order to select one or more digital components to provide to the computing device 122. The content selector component 140 can perform the content selection process in real time. Performing the content selection process in real-time can refer to performing the content selection process in response to a request for content received via the client computing device 122. The real-time content selection process can be performed within a time interval (e.g., 1 second, 2 seconds, 5 seconds, 10 seconds, 20 seconds, 30 seconds, 1 minute, 2 minutes, 3 minutes, 5 minutes, 10 minutes, or 20 minutes) in which the request is received. The real-time content selection process can be performed during a communication session with the client computing device 122 or within a time interval after the communication session is terminated. The data processing system 102 can select an audio beep to be provided after completing a digital component selection process or other digital task.
For example, the digital data processing system 102 can include a content selector component 140 designed, constructed, configured, or operable to select a digital component object. To select digital components for display in a speech-based environment, the data processing system 102 (e.g., via the NLP component of the digital assistant component 106) can parse the input audio signal to identify queries, keywords (e.g., trigger keywords), and use the keywords to select matching digital components. The data processing system 102 can select a matching numeric component based on a broad match, an exact match, or a phrase match. For example, the content selector component 140 can analyze, parse, or otherwise process the subject matter of the candidate digital component to determine whether the subject matter of the candidate digital component corresponds to the subject matter of a keyword or phrase of the input audio signal detected by a microphone of the client computing device 122. The content selector component 140 can use image processing techniques, character recognition techniques, natural language processing techniques, or database lookups to identify, analyze, or recognize speech, audio, terms, characters, words, symbols, or images of candidate numeric components. The candidate digital components may include metadata indicating the subject matter of the candidate digital components, in which case the content selector component 140 may process the metadata to determine whether the subject matter of the candidate digital components corresponds to the input audio signal.
The 3P digital content provider 134 may provide other indicators when setting up a content campaign that includes digital components. The content provider may provide information at the content campaign or content group level that the content selector component 140 may identify by performing a lookup using information about candidate digital components. For example, the candidate digital components may include unique identifiers that may map to content groups, content campaigns, or content providers. The content selector component 140 can determine information about the 3P digital content provider device 134 based on information in a content campaign data structure stored in the data store 108.
The remote data processing system 136 can receive requests for content for presentation on the computing device 122 via a computer network. The data processing system 102 can identify the request by processing an input audio signal detected by a microphone of the client computing device 122. The request can include selection criteria for the request, such as device type, location, and keywords associated with the request.
In response to the request, the remote data processing system 136 can select a digital component object from a data store or database that can include content provided by one or more 3P digital content provider devices 134, and provide the digital component for presentation via the computing device 122 via the network 105. Computing device 122 is capable of interacting with digital component objects. The computing device 122 can receive an audio response to the digital component. Computing device 122 can receive an indication of a selection of a hyperlink or other button associated with the digital component object that causes or allows computing device 122 to identify a service provider, request a service from the service provider, instruct the service provider to perform the service, transmit information to the service provider, or otherwise query the service provider device.
Computing device 122 (or a client computing device or client device) can include or execute data processing system 102. The data processing system 102 can include at least one interface 104 with which to interface or otherwise communicate. The data processing system 102 can include, interface with, or otherwise communicate with at least one local digital assistant component 106 (or digital assistant component 106). The local digital assistant component 106 can include natural language processing capabilities or functionality. The local digital assistant component 106 can include a natural language processor ("NLP") component. The data processing system 102 can include, interface with, or otherwise communicate with at least one data repository 108. The data processing system 102 is capable of including or executing at least one application 110. The data processing system 102 is capable of executing, at least in part, at least one application 110 (e.g., an application may include a client component and a server component). The application 110 is capable of executing or providing an electronic resource 112. The electronic resource 112 can include a script library 114. The script library 114 can include, access, provide, or otherwise utilize a dynamic notifier component 116 or a digital assistant selector component 120.
The data store 108 can include one or more local or distributed databases, and can include a database management system. The data store 108 can include a computer data storage device or memory and can store applications 110, application data, profiles, indexes, device configuration information, account information, preferences, data associated with electronic resources, and other data. The application 110 can include executable files, application package files, configuration files, or other data that facilitates execution of the application.
The interface 104, digital assistant component 106, application 110, or other components of the data processing system 102 can each include or utilize at least one processing unit or other logic device, such as a programmable logic array engine, or modules configured to communicate with each other or other resources or databases. The interface 104, digital assistant component 106, application 110, or other component of the data processing system 102 can be a separate component, a single component, or a portion of the data processing system 102. System 100 and its components, such as data processing system 102, can include hardware elements, such as one or more processors, logic devices, or circuits. The components, systems or modules of data processing system 102 are capable of being executed, at least in part, by remote data processing system 136.
The computing device 122 can include, interface with, or otherwise communicate with at least one sensor 130, transducer 126, audio driver 132, pre-processor 124, or display device 128. The sensors 130 can include, for example, ambient light sensors, proximity sensors, temperature sensors, accelerometers, gyroscopes, motion detectors, GPS sensors, location sensors, microphones, or touch sensors. The transducer 126 can include a speaker or a microphone. The audio driver 132 can provide a software interface to the hardware transducer 126. The audio driver can execute audio files or other instructions provided by the data processing system 102 to control the transducer 126 to generate corresponding sound or sound waves. The display device 128 can include one or more components or functions of the display 735 depicted in fig. 7. The preprocessor 124 can be configured to detect a trigger keyword, a predetermined hotword, an initialization keyword, or an activation keyword. In some cases, the trigger key can include a request to perform an action. In some cases, the trigger keyword can include a predetermined action keyword to enable or activate the computing device 122, and the request keyword can follow the trigger keyword or hotword. The preprocessor 124 can be configured to detect keywords and perform actions based on the keywords. The preprocessor 124 can detect a wake word or other keyword or hotword and, in response to the detection, invoke the digital assistant component 106 of the data processing system 102 executed by the computing device 122. In some cases, the preprocessor 124 can filter out one or more terms or modify terms before transmitting the terms to the remote data processing system 136 for further processing. The pre-processor 124 can convert the analog audio signal detected by the microphone into a digital audio signal and transmit or provide one or more data packets carrying the digital audio signal to the data processing system 102 or a remote data processing system 136 via the network 105. In some cases, in response to detecting an instruction to perform such a transmission, the pre-processor 124 can provide data packets carrying some or all of the input audio signal to the digital assistant component 106 or the remote data processing system 136. The instructions can include, for example, a trigger key or other key or permission to transmit a data packet including the input audio signal to the data processing system 102 or the remote data processing system 136.
The client computing device 122 can be associated with an end user that inputs a voice query as audio input (via the sensor 130) into the client computing device 140, and receives audio output in the form of computer-generated speech that can be provided from the data processing system 102 (or 3P digital content provider device 134) to the client computing device 122, output from the transducer 126 (e.g., speaker). The computer generated speech can include recordings from real persons or computer generated language.
The data processing system 102 can include at least one interface 104, the interface 104 designed, configured, constructed, or operable to receive and transmit information using, for example, data packets. The interface 104 is capable of receiving and transmitting information using one or more protocols, such as a network protocol. The interface 104 can include a hardware interface, a software interface, a wired interface, or a wireless interface. The interface 104 may facilitate converting or formatting data from one format to another. For example, the interface 104 can include an application programming interface that includes definitions for communicating between various components, such as software components. The interface 104 can facilitate communication between one or more components of the system 100, such as between the digital assistant component 106, the application 110, and the data store 108.
The data processing system 102 can include a local digital assistant component 106. The local digital assistant component 106 can include an application, script, or program installed at the client computing device 122. The local digital assistant component 106 can receive input signals, data packets, or other information. The local digital assistant component 106 can receive data packets or other input via the interface 104. Local digital assistant 106 can include an application for receiving input audio signals from interface 104 of data processing system 102 and driving components of the client computing device to render output audio signals. The data processing system 102 is capable of receiving data packets or other signals that include or identify audio input signals. For example, the local digital assistant component 106 can be configured with NLP techniques, functions, or components that can receive or obtain audio signals and parse the audio signals. The digital assistant component 106 can provide for interaction between a human and a computer. The digital assistant component 106 can be configured with techniques for understanding natural language and allowing the data processing system 102 to derive meaning from human or natural language input. The digital assistant component 106 can include or be configured with machine learning based techniques, such as statistical machine learning. The digital assistant component 106 can parse the input audio signal utilizing a decision tree, a statistical model, or a probabilistic model. The digital assistant component 106 can perform functions such as, for example, named entity recognition (e.g., given a stream of text, determining which terms in the text map to appropriate names such as people or places, and what type of each such name is, such as people, locations, or organizations), natural language generation (e.g., converting information from a computer database or semantic intent into intelligible human language), natural language understanding (e.g., converting text into a more formal representation, such as a first-order logical structure that a computer module can manipulate), machine translation (e.g., automatically translating text from one human language to another), morphological segmentation (e.g., separating words into individual morphemes and identifying categories of morphemes, which can be challenging based on the morphological or structural complexity of the words of the language under consideration), question answering (e.g., determining answers to human language questions, which can be specific or open), semantic processing (e.g., processing that can occur after identifying words and encoding their meanings to correlate with other words having similar meanings).
The digital assistant component 106 can convert the audio input signal into recognized text (e.g., using NLP techniques, functions, or components) by comparing the input signal to a representative set of stored audio waveforms and selecting the closest match. The collection of audio waveforms can be stored in the data store 108 or other database accessible to the data processing system 102. Representative waveforms can be generated across a large set of users, and can then be enhanced with speech samples from the users. After the audio signal is converted to recognized text, the digital assistant component 106 can match the text to words that are associated with actions that the data processing system 102 can provide, for example, via using models stored in the data store 108 that have been trained across users or by manual designation.
The audio input signal can be detected by a sensor 130 or transducer 126 (e.g., a microphone) of the client computing device 122. Via the transducer 126, the audio driver 132, or other components, the client computing device 122 can provide audio input signals to the data processing system 102, where the audio input signals can be received (e.g., through the interface 104) and provided to the local digital assistant 106 or stored in the data store 108.
The digital assistant component 106 can obtain an input audio signal. From the input audio signal, the digital assistant component 106 can identify at least one request or at least one trigger keyword corresponding to the request. The request can indicate an intent or subject matter of the input audio signal. The trigger key can indicate the type of action that may be taken. For example, the digital assistant component 106 can parse the input audio signal to identify at least one request to invoke an application. The digital assistant component 106 can parse the input audio signal to identify at least one request, such as a request to eat dinner and watch a movie away from home in the evening. The keywords can include at least one word, phrase, root or partial word, or derivative indicating an action to be taken. For example, the keyword "go (go)" or "to go (to go to)" from the input audio signal can indicate a need for traffic. In this example, the input audio signal (or the identified request) does not directly express an intent for traffic, however, the keyword indicates that traffic is an ancillary action to the at least one other action indicated by the request.
The digital assistant component 106 can parse the input audio signal to identify, determine, retrieve, or otherwise obtain the request and keywords. For example, the digital assistant component 106 can apply semantic processing techniques to the input audio signal to identify keywords or requests. The digital assistant component 106 can apply semantic processing techniques to the input audio signal to identify one or more keywords. The keywords can include one or more terms or phrases. The digital assistant component 106 can apply semantic processing techniques to identify an intent to perform a digital action.
For example, the computing device 122 can receive an input audio signal detected by a sensor 130 (e.g., a microphone) of the client computing device 122. The input audio signal can be "digital assistant I need to have a person to wash and dry clean". The pre-processor 124 of the client computing device 122 can detect a wake word, a hotword, or a trigger keyword, such as "digital assistant," in the input audio signal. The preprocessor 124 can detect a wake-up word, a hotword, or a trigger keyword by comparing an audio signature or waveform in the input audio signal with a model audio signature or waveform corresponding to the trigger keyword. The preprocessor 124 can determine that the input audio signal includes a wake word, a hotword, or a trigger keyword that indicates that the input audio signal is to be processed by the digital assistant component 106. In response to detecting a hotword, a wake-up word, or a trigger keyword, the preprocessor 124 can determine, authorize, route, forward, or otherwise provide the detected input audio signal to the data processing system 102 for processing by the digital assistant component 106.
The digital assistant component 106 can receive the input audio signal and apply semantic processing techniques or other natural language processing techniques to the input audio signal including sentences to identify the trigger phrases "do my laundry" and "do my dry cleaning". In some cases, the digital assistant component 106 can provide data packets corresponding to the input audio signal to the remote data processing system 136 to cause the digital assistant server 138 to process the input audio signal. The digital assistant component 106 can process the input audio signal in conjunction with or via the digital assistant server 138. The digital assistant component 106 can further identify a plurality of keywords such as laundry and dry cleaning.
The digital assistant component 106 can identify search queries, keywords, intents, or phrases that correspond to performing searches or other requests for information. The digital assistant component 106 can determine that the input audio signal corresponds to a request for information about a topic, event, current event, news event, dictionary definition, historical event, person, place, or thing. For example, the digital assistant component 106 can determine that the input audio signal corresponds to a query, request, intent, or action to make a travel arrangement, reserve a trip, obtain information, perform a Web search, check stock prices, launch an application, view news, order food, or purchase other products, goods, or services.
The digital assistant component 106 can use one or more techniques to parse or process the input audio signal. The techniques can include rule-based techniques or statistical techniques. The techniques can utilize machine learning or deep learning. Example techniques can include named entity recognition, sentiment analysis, text summarization, aspect mining, or topic mining. Techniques can include or be based on text embedding (e.g., real-valued vector representations of character strings), machine translation (e.g., language analysis and language generation), or dialogs and conversations (e.g., models used by artificial intelligence). Techniques can include determining or utilizing grammatical techniques (e.g., word arrangements in grammar-based sentences), such as tokenization, morphological segmentation, word segmentation, part-of-speech tagging, parsing, sentence breaking, or word stems. Techniques can include determining or utilizing semantic techniques such as named entity recognition (e.g., determining portions of text that can be recognized and classified as a current group, such as the name, person, or location of the application 152), word sense disambiguation, or natural language generation.
In some cases, the digital assistant component 106 can launch the application 110. In some cases, the application 110 may have been launched before the digital assistant component 106 receives the input audio signal. For example, based on processing or parsing of the input audio signal, the digital assistant component 106 can identify the application 110 to invoke, launch, open, or otherwise activate. The digital assistant component 106 can identify the application 110 based on parsing the input audio signal to identify terms, keywords, trigger keywords, or phrases. The digital assistant component 106 can perform a lookup in the data store 108 using the identified term, keyword, trigger keyword, or phrase to identify the application 110. In some cases, the keywords can include an identifier of the Application 110, such as "Application _ Name _ a" or "Application _ Name _ B". In some cases, the keywords can indicate a type or category of application 110, such as a car pool application, a restaurant reservation application, a movie ticket application, a news application, a weather application, a navigation application, a streaming media music application, a streaming media video application, a restaurant review application, or other type or category of application 110. For the case where the application 110 may have been launched and executed prior to receiving the input audio signal, the digital assistant component 106 can process the input audio signal to determine an action to be performed in the application 110 or a response to a call action presented via the electronic resource rendered by the application 110.
The client computing device 122 is capable of executing the application 110. The data processing system 102 is capable of executing applications 110. The data processing system 102 can include or execute an operating system via which the client computing device 122 can execute the applications 110. The applications 110 can include any type of application that the client computing device 122 is configured to execute, run, launch, or otherwise provide. The applications 110 can include multimedia applications, music players, video players, web browsers, word processors, mobile applications, desktop applications, tablet applications, electronic games, e-commerce applications, or other types of applications 110. The application 110 can execute, render, load, parse, process, present, or otherwise output data corresponding to the electronic resource 112. The electronic resources 112 can include, for example, websites, web pages, multimedia web content, video content, audio content, travel content, entertainment content, content related to shopping or services, or other content.
The application 110 can receive data associated with the electronic resource 112 from a third party ("3P") electronic resource server 142. The 3P electronic resource server 142 is capable of providing the electronic resource 112 for execution by the application 110. The 3P electronic resource server 142 can include a file server, web server, game server, multimedia server, cloud computing environment, or other back-end computing system configured to provide data to cause the application 110 to render or provide the electronic resource 112 via the computing device 122. Computing device 122 is able to access 3P electronic resource server 142 via network 105.
An administrator of the 3P electronic resource server 142 can develop, build, maintain, or otherwise provide the electronic resource 112. The 3P electronic resource server 142 can transmit the electronic resource 112 to the computing device 122 in response to a request for the electronic resource 112. The electronic resource 112 can be associated with an identifier, such as a uniform resource locator ("URL"), a uniform resource identifier, a web address, or a filename or file path. The 3P electronic resource server 142 can receive a request for an electronic resource 112 from an application 110. The electronic resources 112 can include electronic documents, web pages, multimedia content, streaming media content, audio, video, text, images, video games, or other digital or electronic content.
An administrator of the 3P electronic resource server 142 can provide the script library 114. The 3P electronic resource server 142 can embed the script library 114 in the electronic resource 112. The script library 114 can include a JavaScript library. The script library 114 can be configured to communicate with a remote data processing system 136 to provide digital assistant-based functionality for the electronic resource 112. The script library 114 can provide user interface elements (e.g., buttons, forms, auto-complete suggestions) on the electronic resource 112.
The data processing system 102 can load graphical user interface elements corresponding to the script library 114 at locations on the electronic resource 112 established by the provider of the electronic resource. The script library 114 can be launched in an iframe ("iframe") on the electronic resource 112. By launching or executing in an iframe on the electronic resource 112, the data processing system 102 is able to host the script library 114 or corresponding user interface element in a secure computing environment. The computing environment can be secure with respect to the electronic resource or 3P electronic resource server 142. For example, the iframe in which the script library 114 executes can control or prevent access to the content or data of the iframe. The iframe can prevent the electronic resource 112 or the 3P electronic resource server 142 from accessing content or data associated with the script library 114. The script library 114 executing in the iframe is able to communicate data with the digital assistant server 138 without the electronic resource 112 (or the 3P electronic resource server 142) gaining access to the data. The script library 114 can obtain data or information associated with the data processing system 102, an account logged into the data processing system 102, the computing device 122, or other information.
An administrator of the electronic resource 112 can place hypertext markup language ("HTML") tags on the electronic resource 112. The electronic resource 112 can include HTML tags. The HTML markup on the electronic resource 112 can include a description or configuration setting for the manner in which the script library displays user interface elements on the electronic resource 112. The following illustrates an example of HTML markup of the electronic resource 112:
the customized web element < digital-assisted-action-link-group > can act as a container that holds the call actions provided by the 3P electronic resource server 142 for display on the electronic resource 112. Each < digital-assisted-action-link > within a container can represent a single invocation action to be displayed on the electronic resource 112.3P can specify additional configurations for invoking actions on the page, such as icons, item identifiers, or attributes. For example, an attribute can be a hypertext reference ("href") and have the type "string". The href attribute can include a link to the action link uniform resource locator "URL" in the local digital assistant experience. The URL can be verified prior to rendering the selected action user interface. Data processing system 102 can block or not render invalid action links. To verify the action link, the data processing system 102 or the remote data processing system 136 can: 1) Determining whether the loaded script library 114 suffix matches the action link's URL suffix; 2) An act of determining that the loaded script library 114 has an intent for action linking; and 3) determine whether the loaded script library 114 has links enabled for the intent of action linking.
Another example attribute can include "title" and have the type "string". The title attribute can display or provide a title. This title can be the same as the intent _ title of the action link or a different title set by the administrator of the electronic resource 112.
An application 110, such as a web browser, can load an electronic resource 112. Loading the electronic resource 112 can include downloading data or content of the electronic resource 112 from a 3P electronic resource server 142 or other file server or web server or host server. Loading the electronic resource 112 can include the application 110 rendering or executing the electronic resource 112. Loading the electronic resource 112 can include the application 110 loading a script library 114 embedded with the electronic resource 112. The script library can include a plurality of invocation actions for the electronic resource 112 that are configured to be executed by one or more digital assistants. The invoking action can be configured to be performed by the local digital assistant component 106 or the digital assistant server 138. The invoking action can be performed by both the local digital assistant component 106 and the digital assistant server 138. For example, the local digital assistant component 106 and the digital assistant server 138 can communicate with each other to perform a call action, or to perform different portions of a call action.
Upon loading the electronic resource 112, the application 110 can cause the script library 114 to be loaded into the secure iframe of the electronic resource 112. The script repository 114 can initiate, invoke, initiate, execute, call or otherwise provide one or more components, such as a dynamic notifier component 116 and a digital assistant selector component 120. The components of the script library 114 can communicate with one or more components or elements of the data processing system 102 or the remote data processing system 136 in a secure manner, such as in the case where the 3P electronic resource server 142 does not obtain the transmitted data. The script library can be established by a third party service provider (e.g., 3P electronic resource server 142) with a call action that includes a predetermined set of call actions selected by the third party service provider. An administrator of the 3P electronic resource server 142 can establish a call action for the electronic resource 112 and embed the call action in the script library 114. Each call action can be associated with attributes or metadata that indicates what task, when, or under what conditions the call action is to call or render the call action. The script library 114 can be designed, configured, and operated to perform one or more functions, processes, or methods depicted in fig. 2 and 3, such as processes 200 and 300.
Dynamic notifier component 116 can be designed, constructed and operated to query digital assistant component 106 to determine a level of historical engagement between computing device 122 and one or more digital assistants. Dynamic notifier component 116 is capable of determining a historical level of engagement between the digital assistant and an account associated with computing device 122. An account can be associated with multiple computing devices. In some cases, the historical engagement level can be based on engagement with digital assistants on all computing devices 122 associated with the same account. In some cases, the historical engagement level can be based on engagement with a digital assistant associated with computing device 122 loading script library 114.
The dynamic notifier component 116 can determine what calling actions are presented and then select the type of digital interface with which the calling actions are presented. To select the invocation action, dynamic notifier component 116 can determine a current context or state of electronic resource 112. For example, a state or context can refer to or include a search state, a purchase state, a checkout flow, or an information collection flow. The context can indicate that the user is searching for flights or cannot identify flights with a desired price, time, or duration, for example. The electronic resource 112 can present surveys or prompts to obtain contextual information. Dynamic notifier component 116 can identify an invoked action associated with a current context or state of electronic resource 112.
To select the type of interface to generate, dynamic notifier component 116 can determine a historical engagement level. The historical engagement level can refer to a previous engagement level. The historical engagement level can refer to the level of engagement prior to the current time. The historical level of engagement can refer to the level of engagement over the last 24 hours, 48 hours, 72 hours, 1 week, 2 weeks, 3 weeks, 4 weeks, 30 days, 60 days, 90 days, 6 months, or more.
The data processing system 102 (e.g., via dynamic notifier component 116) can receive information about historical engagement levels from a digital assistant server 138 remote from the data processing system 102 via network 105. The dynamic notifier component 116 can determine the level of engagement based on the amount of usage of the digital assistant. The level of engagement can be based on the number of uses of the digital assistant, the number of interactions with the digital assistant, or the number of times the digital assistant has been initiated or invoked. The level of engagement can be based on the type of interaction with the digital assistant. The types of interactions can include, for example, a category or type of task performed by the digital assistant, a type of query, a type of request, or a type of invocation action. Example types of tasks or invoked actions can include launching an application, performing an online search, making a purchase, ordering a service, a car pool request, checking weather, or other types of tasks. Categories of tasks or invoked actions can include, for example, requests for information, entertainment tasks, purchases, commerce, car pool requests, or multimedia requests. The dynamic notifier component 116 can determine a historical engagement level based on a number of previous interactions between the plurality of client computing devices during a time interval (e.g., the past 24 hours, 48 hours, 7 days, or 30 days).
The dynamic notifier component 116 is able to determine the level of engagement for a particular category or task. The level of engagement can be determined for a particular electronic resource 112 or application 110. The level of engagement can be determined for a particular time of day, day of week, or location of computing device 122. The level of engagement can be determined for a particular account logged into computing device 122. The level of engagement can be determined for a particular user of computing device 122.
The dynamic notifier component 116 can determine the level of engagement as a score, value, percentage or other label. For example, dynamic notifier component 116 can determine the engagement level as one of low, medium or high. The dynamic notifier component 116 can determine a numerical value or score to represent a score level. For example, dynamic notifier component 116 can represent the engagement levels using a scale from 1 to 10, where 1 is the lowest engagement level and 10 is the highest engagement level. In another example, dynamic notifier component 116 can represent the engagement levels as levels, such as A, B, C, D or F, where a is the highest engagement level and F is the lowest engagement level. The dynamic notifier component 116 can represent the level of engagement using binary values, where 1 indicates that there has been a historical engagement between the account and the digital assistant during a previous time interval (e.g., the last 1 week) and 0 indicates that there is no engagement or an account was recently created between the account and the digital assistant.
The level of engagement can be based on a number of factors. The multiple factors can be combined to determine an overall level of engagement. Factors can include, for example, the type of engagement, the category of engagement, or other aspects of historical engagement. For example, dynamic notifier component 116 can assign values for multiple engagement factors and then combine these values (e.g., sum, product, average, or weighted average) to determine an overall score. For example: number of engagements of the past week + number of engagements with the same category of the same electronic resource 112 + number of engagements on the same computing device 122. The dynamic notifier component 116 can determine the total score based on these factors. The dynamic notifier component 116 can compare the score to a threshold to determine whether the level of engagement is low or high; or to map the score to a more granular level such as a low, medium-high or high engagement level. The threshold can be predetermined or configured by an administrator of the remote data processing system 136. The threshold can be dynamically determined based on the aggregated level of engagement of other accounts.
For example, the electronic resource 112 can be a website that involves the purchase of airline tickets. The dynamic notifier component 116 is able to determine that the digital assistant is being used to facilitate the purchase of tickets on the electronic resource 112 in the past 30 days. Thus, dynamic notifier component 116 is able to determine that the level of engagement of the account with the electronic resource 112 is high.
To determine the historical engagement level, dynamic notifier component 116 can communicate with digital assistant server 138 via network 105. Dynamic notifier component 116 can transmit a request or query to digital assistant server 138 to obtain information from digital assistant server 138 regarding the historical level of engagement between the account logged into computing device 122 and the digital assistant. In some cases, dynamic notifier component 116 can obtain information from local digital assistant component 106. For example, dynamic notifier component 116 can query or otherwise communicate with local digital assistant component 106 to determine the level of engagement. Local digital assistant component 106 can store information related to historical or previous engagements, interactions, uses, invocations, or uses of digital assistant component 106 or digital assistant 106 on other computing devices associated with the same account.
Dynamic notifier component 116 is capable of accessing information related to the historical engagement level in a secure manner. For example, the script library 114 can be launched in an iframe, which can be a secure computing environment or a sandbox computing environment that prevents the electronic resource 112 from accessing data obtained or used by the dynamic notifier component 116. Thus, dynamic notifier component 116 is able to determine a historical level of engagement of the account without obtaining information by electronic resource 112.
The dynamic notifier component 116 is capable of determining attributes of the computing device 122 (e.g., client computing device). Attributes of computing device 122 can refer to or include the type of computing device, the location of the computing device, the configuration of the computing device, applications installed on the computing device, the operating system of the computing device, battery charge, performance capabilities, or available interfaces. For example, the attributes related to the device type can include a mobile or desktop device, a smartphone, a tablet, a desktop, a smart watch, a wearable device, a smart speaker, a smart television, or an appliance. The attributes of the computing device 122 can indicate whether the computing device 122 has a local digital assistant, microphone, or speaker. Local digital assistant can refer to a local digital assistant component 106 that is integrated with or is part of an operating system installed or executing on computing device 122. A computing device 122 that does not have a local digital assistant can have a digital assistant component installed on the computing device 122 as a third party application. In some cases, the computing device 122 may not have any digital assistant installed, in which case the attribute value can indicate that no digital assistant is installed or available. The attributes can include whether an account associated with the electronic resource has been logged in.
The dynamic notifier component 116 can determine the device attributes. The dynamic notifier component 116 is capable of obtaining device attributes from the remote data processing system 136. For example, the remote data processing system 136 can transmit the device attributes to the data processing system 102. In some cases, data processing system 102 can obtain device attributes directly from computing device 122. Attributes can include, for example: an account identifier; a device operating system type; a device operating system version; or device location.
The dynamic notifier component 116 is capable of selecting a type of digital interface in which to render a calling action of the plurality of calling actions included in the script repository 114. Dynamic notifier component 116 is able to select the type of digital interface based on the attributes and historical engagement levels of computing device 122. The invoking action can correspond to a task associated with the electronic resource 112 that the digital assistant can perform. Example tasks can include, for example, ordering a ride from a passenger sharing application, booking a flight on a travel electronic resource, purchasing goods or services from a commercial electronic resource, continuing or repeating a task on an electronic resource, or conducting a financial transaction.
The type of digital interface in which the invocation action is presented can include an icon, banner, pop-up window, overlay, animated icon, scrolling text, flashing icon, inline text or image, or audio output located in a corner of an electronic resource or display device of computing device 122. The type of digital interface can be a distinct digital interface (e.g., overlaid over the entire electronic resource 112 or a substantial area of the electronic resource 112), or a subtle or non-distinct digital interface (e.g., an icon in the upper right corner of the electronic resource 112 that does not obscure the rest of the electronic resource).
The dynamic notifier component 116 is able to select the type of digital interface based on the attributes and historical engagement levels of the client computing device 122. Dynamic notifier component 116 can employ various techniques to select a digital interface based on one or more attributes and historical engagement levels. For example, the selection technique used by the dynamic notifier component 116 can use or include one or more of account preferences, predetermined configurations, mappings, historical performance of digital interfaces, or machine learning.
The dynamic notifier component 116 can use the account preferences to select a digital interface. The account preferences can indicate preferences for the digital interface depending on the type of computing device. For example, a user of computing device 122 can establish account preferences. The account preferences can include a ranking of preferences for the type of digital interface for the type of computing device 122. For example, the ranking of the digital interface of the smartphone can be: 1) A screen corner icon; 2) Banner; and 3) overlaid on the electronic resource, where 1 represents the highest ranking (or most preferred type of digital interface) and 3 represents the lowest ranking (or least preferred type of digital interface). The dynamic notifier component 116 can utilize a historical level of engagement associated with the smartphone to increase the ranking to select the type of digital interface. For example, if the historical engagement level has been high, the dynamic notifier component 116 can select the highest ranking type of digital interface. If the historical engagement level has been low, dynamic notifier component 116 can determine to select a different type of digital interface in order to increase the likelihood of engagement with the digital assistant. In another example, if the level of historical engagement with the digital assistant is high, the dynamic notifier component 116 can override the ranking to select a more obvious type of digital interface in order to increase the likelihood of engagement with the digital assistant. Increasing the likelihood of engagement with a digital assistant can reduce computing resource utilization because the digital assistant can increase the efficiency of execution of computing tasks.
The dynamic notifier component 116 can utilize a predetermined mapping to select the type of digital interface. An example mapping is illustrated in table 1.
Table 1: illustrative mapping of attribute and historical engagement levels to digital interface types
Interface
The dynamic notifier component 116 can utilize mappings provided by an administrator of the digital assistant server 138 or the 3P electronic resource server 142. The mapping can be referred to as a selection technique, policy, or logic. The dynamic notifier component 116 is capable of determining attributes and historical engagement levels to identify the type of digital interface to select. The dynamic notifier component 116 can perform a lookup in the mapping using the attributes and levels to identify the corresponding type of digital interface.
The dynamic notifier component 116 can use historical performance of the digital interfaces to determine the digital interface to select. The dynamic notifier component 116 is able to obtain attributes and historical engagement levels associated with the performance of the digital interface. The performance of a digital interface can mean that the interface causes an interaction or performs a call action. Actively performing can refer to performing a call action to engage with the digital assistant, while passively performing can refer to shutting down, terminating, or otherwise not performing a call action or not calling or using the digital assistant to perform a task. The dynamic notifier component 116 can utilize a model trained by machine learning techniques, wherein features of the model can include one or more attributes, historical engagement levels, types of digital interfaces, and performance of the digital interfaces (e.g., binary values indicating performance of a calling action or performance of no calling action).
The dynamic notifier component 116 can employ machine learning techniques to select the type of digital interface. One or more components or systems can generate the model using machine learning techniques. For example, the remote data processing system 136 (e.g., via the digital assistant server 138) can generate a model, the local digital assistant component 106 can generate a model, or the dynamic notifier component 116 can generate a model. The data is used to train a model used by machine learning using the data for the particular account. The dynamic notifier component 116 can use data associated with a particular account to generate or use a model for that account. In some cases, dynamic notifier component 116 can aggregate de-identified or anonymized data associated with multiple accounts to train the model.
The dynamic notifier component 116 can apply a weight to the score or value associated with the attribute or historical engagement level to determine the score. The dynamic notifier component 116 can use the score to select the type of digital interface. For example, the presence or absence of an attribute can be expressed as a numerical value. The historical engagement level can be expressed as a numerical value. The dynamic notifier component 116 can weight the score (e.g., apply a multiplier to the score). The dynamic notifier component 116 can combine the scores (or weighted scores) to determine a total score for selecting a digital interface. The dynamic notifier component 116 can map the score to the type of digital interface. A mapping based on example scores is illustrated in table 2.
Score threshold | Type of digital interface |
Less than 3 | Icons in corners of electronic resources |
3 to 5 | Banner at bottom of electronic resource |
Greater than 5 and less than 7 | Interface with apparent coverage on electronic resources |
Greater than 7 | Voice-based interface |
Accordingly, dynamic notifier component 116 enables customized, dynamic selection of the type of digital interface based on real-time information associated with attributes of computing device 122 and historical engagement levels of the digital assistant in order to increase the computational efficiency with which computing device 122, 3P electronic resource server 142, or remote data processing system 136 can perform tasks or functions associated with electronic resource 112. For example, the data processing system 102 (e.g., via the dynamic notifier component 116) can select the type of digital interface as the voice-based interface based on a property of the client computing device 122 indicating that the client computing device 122 includes a microphone and a speaker and a historical level of engagement indicating that a number of interactions between the client computing device 122 and one or more digital assistants associated with the same account is greater than a threshold (e.g., 7).
The dynamic notifier component 116 is capable of generating a digital interface with a call action. Dynamic notifier component 116 may generate a digital interface based on the type of digital interface selected based on the first attribute of client computing device 122 and the historical level of engagement. The dynamic notifier component 116 can generate a digital interface in response to the selection. The dynamic notifier component 116 can use, call, populate, or otherwise utilize templates, images, text, scripts, functions, user interface elements, audio, multimedia, or other components to generate a digital interface. For example, the dynamic notifier component 116 can access a template for the digital interface and populate fields in the template based on a call action established by the 3P electronic resource server 142 for the electronic resource 112. In another example, the script library 114 can include pre-established or configured digital interfaces for each type of digital interface. The dynamic notifier component 116 is capable of rendering a pre-established or pre-configured digital interface corresponding to the selected digital interface type for invoking the action. In another example, the dynamic notifier component 116 can select the type of digital interface as the graphical user interface based on a first attribute of the client computing device 122 indicating that the client computing device 122 lacks a microphone (or access to a microphone or speaker of the client computing device 122 by one or more digital assistants has been disabled). If access to the microphone is disabled, the dynamic notifier component 116 can select the type of digital interface that is capable of receiving non-voice based input, such as touch input, or keyboard or mouse input.
The data processing system 102 can detect an instruction to perform a call action. The instructions can be detected via an input device of the client computing device 122. Computing device 122 can receive input via a touch interface, mouse input, keyboard input, voice input, gesture-based input, or other type of input. The input can be an interaction with a digital interface, a selection of a button, link, or other interactive widget presented via the interface. Thus, data processing system 102 is able to detect, via the digital interface, an instruction to perform a call action.
The data processing system 102 or the script library 114 can include or execute a digital assistant selector component 120 designed, constructed and operative to determine an execution mode for a call action, select a digital assistant to execute the call action, and communicate the call action. The digital assistant selector component 120 can determine an execution mode for the invoking action based on one or more attributes of the client computing device 122 and the historical engagement levels in response to executing the instruction to invoke the action. Execution mode can refer to or include executing a local digital assistant on computing device 122 or executing a non-local digital assistant on computing device 122. A non-native digital assistant can refer to or include a digital assistant that does not have a pre-installed operating system. A non-native digital assistant can refer to a digital assistant that is installed on computing device 122 as a third party application.
The execution modes can include, for example: navigate to a non-native digital assistant application installed on computing device 122; redirecting to an electronic resource with a prompt or indication to download and install the digital assistant; navigate to a local digital assistant installed on the computing device 122; navigate to the local digital assistant and immediately perform a call action; navigating to the digital assistant electronic resource with a prompt indicating how to perform the invocation action; providing a prompt with a list of other computing devices 122, the other computing devices 122 having a digital assistant linked to the same account as the computing device 122 executing the data processing system 102, and providing a prompt to select one of the other computing devices 122 to initiate the digital assistant to perform the invoked action.
The data processing system 102 can select an execution mode to invoke the action based on the one or more attributes of the client computing device indicating a type of operating system of the client computing device. The attributes used by the data processing system 102 to select the execution mode can be referred to as the second attribute or the second one or more attributes, while the attributes used to select the digital interface can be referred to as the first attribute or the first one or more attributes. The first attribute can be the same as the second attribute. The first one or more attributes can overlap with the second one or more attributes. The first attribute can be different from the second attribute. The first one or more attributes can be mutually exclusive from the second one or more attributes.
To select the execution mode, the digital assistant selector component 120 can employ one or more functions or techniques similar to the dynamic notifier component 116. For example, the digital assistant selector component 120 can utilize mapping techniques, indexing, machine learning techniques, logic-based techniques, rules, or policies.
The digital assistant selector component 120 can select an execution mode based on the second attribute and the historical engagement level. The digital assistant selector component 120 can perform a lookup in a map or index having a second attribute and a historical engagement level to identify a corresponding execution mode. The digital assistant selector component 120 can determine an execution mode using a machine learning model or a model trained using machine learning techniques. The model can include features such as attributes, historical engagement levels, different execution modes, and historical execution of the digital assistant or execution of actions with corresponding execution modes.
The digital assistant selector component 120 can select an execution mode based on preferences associated with the account, or a configuration provided by the 3P electronic resource server 142 or the digital assistant server 138.
The digital assistant selector component 120 can select a digital assistant from the one or more digital assistants and a second client device of the plurality of client computing devices to perform the invoking action based on the execution mode. For example, the execution mode can include launching, invoking, executing, or otherwise utilizing the digital assistant on a second computing device 122 (e.g., a computing device 122 different from the computing device 122 on which the electronic resource 112 and the script library 114 were initially loaded). The digital assistant selector component 120 can provide a list of available computing devices 122 and receive a selection of a computing device. The digital assistant selector component 120 can rank a list of computing devices 122 that are linked to the same account and that are configured or operated to initiate the digital assistant to perform a call action. The digital assistant selector component 120 can automatically select the highest ranked computing device 122 or receive an indication of the selection of the computing device.
For example, the list of computing devices 122 having digital assistants configured to perform the call action can be ranked based on one or more of the following: how often computing device 122 is used to launch a digital assistant to perform a call action; the time that computing device 122 was most recently used; how close (location/proximity) a computing device 122 that initially loads the script library 114 is to another computing device 122; or how relevant the computing device 122 is to invoke the action. For example, a digital assistant-enabled smart speaker computing device 122 is highly correlated with a call action for playing music, while a smart watch computing device 122 is less correlated with such a call action. In another example, a digital assistant-enabled smart television is highly relevant to invoking actions for playing videos, while the smart watch computing device 122 is less relevant to such invoking actions.
The digital assistant selector component 120 can select a second computing device 122, different from the first computing device (e.g., the computing device that initially loaded the script library 114), to perform the invocation action. The digital assistant selector component 120 can transmit the invocation action to the second client computing device 122 to cause the second client computing device 122 to invoke the digital assistant to perform the invocation action. The digital assistant selector component 120 can transmit information that facilitates performing the invoked action, such as information associated with the electronic resource 112 or an account associated with the electronic resource 112. This information may include references, pointers, metadata, location information, or other data that facilitates the execution of the invoked action by the digital assistant of second computing device 122.
In some cases, the system 100 can select a digital component for provisioning. The system can select a content item or digital component from the 3P digital content provider device 134. The system 100 can provide the selected digital component for presentation on the electronic resource 112 with a digital interface via the computing device 122, or on a second computing device 122 that performs the invoked action. Content items or digital components can be selected based on electronic resources 112, attributes, historical engagement levels, calling actions, or other information.
For example, the remote data processing system 136 can include a content selector component 140. The content selector component 140 of the remote data processing system 136 can select a content item (e.g., a digital component object). The content selector component 140 can select content items based on or in response to one or more of the following: a request for content, a selection of a digital interface based on the dynamic notifier component 116, a selection of an execution or mode of the computing device 122 based on the digital assistant selector component 120, a transmission based on a call action, or a call action according to or in response to a request. The content selector component 140 can select content items that are related to or otherwise match information associated with the invoked action. For example, if the application is an e-commerce application that sells shoes, the content selector component 140 can select content items for one type of athletic shoe. In another example, if the invoking action involves booking a flight, the remote data processing system 136 can select a content item related to the travel (e.g., a coupon for the flight).
In some cases, after the dynamic notifier component 116 of the data processing system 102 selects the type of digital interface, the content selector component 140 of the remote data processing system 136 can determine to select a content item (e.g., a digital component object) provided by the 3P digital content provider device 134. After the dynamic notifier component 116 selects the type of digital interface, the dynamic notifier component 116 can provide an indication of the selected type of digital interface to the remote data processing system 136. The dynamic notifier component 116 can provide an indication to the digital assistant server 138. The digital assistant server 138 can generate a request for a content item and forward the request to the content selector component 140. The digital assistant server 138 can generate a request for content in response to receiving an indication of a selection of a type of digital interface. In some cases, the digital assistant server 138 can determine to generate a request for content based on the type of digital interface generated by the dynamic notifier component 116. For example, if the dynamic notifier component 116 selects a prominent or distinct interface that occupies more screen space, the digital assistant server 138 can determine that there may be sufficient space to include the content item. Thus, the digital assistant server 138 can generate a request for content in response to determining that sufficient screen space exists for supplementing the content item. However, if the type of digital interface selected is less obvious, such as a small icon in a corner of the screen, the digital assistant server 138 can determine that no request for content is generated, thereby reducing wasted computing resource utilization.
In generating the request for content, the digital assistant server 138 can forward the request for content to the content selector component 140. The content selector component 140 can perform a real-time content selection process to select content items. The content selector component 140 can select content items using one or more parameters or selection criteria. For example, the request received by content selector component 140 can include information about electronic resource 112 (e.g., keywords, topics, concepts, or other contextual information), a call action to be provided in a digital interface, a type of digital interface selected, or information associated with an electronic account. The content selector component 140 can use this information to select related content items.
The remote data processing system 136 can provide the selected content item to the data processing system 102 for presentation. The remote data processing system 136 can provide the content items to the script repository 114 to cause the script repository 114 to display or render the content items via the digital interface selected by the dynamic notifier component 116. The dynamic notifier component 116 is capable of displaying the content item and the invocation action presented via the digital interface. Thus, the real-time content selection process can occur after the type of digital interface is selected and before the digital interface is presented for display.
In some cases, the data processing system 102 can present the content item via the digital assistant component 106. The remote data processing system 136 can provide the selected content item to the application 110 for presentation. The remote data processing system 136 can provide the selected content items to the digital assistant component 106 to be provided to the application 110 for presentation. The content items can be presented via a user interface of the application 110, such as in a visual content slot or audio output. The dynamic notifier component 116 is capable of providing or presenting the content item and the selected digital interface. The digital assistant selector component 120 can provide the content item to the second computing device 122 selected for performing the invocation action to cause the second computing device 122 to present the content item.
The digital assistant component 106 can present the content item separately or independently from the application 110. For example, the digital assistant component 106 can invoke a separate user interface, such as a pop-up window or banner content, for display via the display device 128 of the computing device 122. In some cases, the digital assistant component 106 can provide the content item as an audio output. The audio output can be presented before, during, or after the audio output of the application 110. In the event that the application 110 does not provide audio output, the digital assistant component 106 can provide the content item via the audio output independent of the interface used by the application. Thus, the data processing system 102 can present the content items via a user interface (e.g., an audio interface) or an application 110 (e.g., a graphical user interface) of the digital assistant component 106.
In some cases, the remote data processing system 136 can transmit the selected content item to a second computing device 122 that is different from the computing device 122 that initially loaded the electronic resource 112. For example, the remote data processing system 136 can receive an indication from the digital assistant selector component 120 identifying the second computing device 122. The digital assistant selector component 120 can select the second computing device 122 to perform a call action provided via the digital interface. Thus, rather than presenting the selected content item via the computing device 122, the system 100 can present the content item via the second computing device 122 performing the invoked action.
To do so, in some cases, the remote data processing system 136 can transmit the content item to the data processing system 102, and the data processing system 102 can determine to delay presenting the content item until the digital assistant selector component 120 selects the digital assistant of the second computing device 122 to perform the invoking action. Upon selecting the digital assistant of the second computing device 122, the data processing system 102 can forward the selected content item to the corresponding second computing device 122 along with the invocation action for execution.
In some cases, the data processing system 102 can present the content item along with the digital interface and then transmit the content item to the selected digital assistant of the second computing device 122 to cause the second computing device 122 to redisplay the selected content item or related content items.
Fig. 2 is an illustration of a process of adjusting the execution of a digital action according to an embodiment. The process 200 can be performed by one or more systems or components depicted in fig. 1 or fig. 7, including, for example, a data processing system, a remote data processing system, a local digital assistant, a script library, a dynamic notifier component, or a digital selector component. The method 200 can be initiated by checking whether an account has been activated for the session. If the account is not activated, the data processing system may be able to determine that it may not be possible to dynamically select a digital interface because it may not be possible to determine the historical engagement level. Thus, if the account is not active or logged in for the session, the data processing system can select the default digital interface. The default digital interface, when selected, can generate a prompt requesting account login credentials.
If an account is logged in, the script library can check whether the account has been engaged with the digital assistant by: (1) Communicate with the digital assistant server to confirm whether there are any assistant-enabled devices associated with the account, or (2) whether the account has completed a past digital assistant upload experience. If the account is not an assistant user, the script library can select the same default, non-customized digital interface button as if the account is not logged in, because there is no signal on the historical level of engagement that can be used to dynamically select a more prominent digital interface. However, if the account is logged in and the account has historically been engaged with a digital assistant, the library can check whether there has been any past interaction between the user and the third party electronic resource provider via the digital assistant associated with the account. The script repository is capable of determining a level of historical engagement with a digital assistant performing an action on an electronic resource by querying the digital assistant server.
If the account has not used the digital assistant to interact with the electronic resource in the past (e.g., if there has not been a recorded interaction between the electronic resource and a digital assistant associated with the account in the past), the scripting library can select the same non-personalized default digital interface button because there is no signal (affirmative or negative) as to whether they can benefit from a more explicit invocation action for the digital assistant voice functionality.
If the account is logged in, the account has engaged with the digital assistant, and the account has engaged with the digital assistant to interact with or perform an action on the electronic resource, the script library can determine to show a more obvious call action for the digital assistant voice function on the electronic resource.
For example, at act 202, the data processing system executing the script library can determine whether there are accounts of available assistant providers. An account is not available (e.g., not logged in), the script library can proceed to act 218 to display a default digital interface that is not customized for the in-line button entry point, and then to act 220 to provide a web browsing-based action link, such as a link with information about the digital assistant, and to act 210 to provide preemption of the underlying form entry point. However, if at act 202 the script library determines that an account associated with the digital assistant is logged in for the session (e.g., via an operating system, application, or electronic resource of the computing device), the script library (or a system executing the script library) can proceed to act 204 to determine whether the account has been engaged with the digital assistant. At act 204, the script library can determine a historical level of engagement for the account. The script library can determine whether the account has used a digital assistant in the past.
If the scripting library determines that the account has not used a digital assistant in the past at act 204, the scripting library can proceed to act 218 to select a default digital interface for the embedded button entry point. If the scripting library determines at act 204 that the account has been engaged with the digital assistant, the scripting library can proceed to act 206 to determine whether the account (or the computing device 122 on which the account is logged) has performed a calling action using the digital assistant for the electronic resource. If at act 206 the script library determines that the user has previously performed an action on the electronic resource using the digital assistant, the script library can proceed to act 208 to select a digital interface corresponding to the level of engagement based on the attributes of the device and then proceed to act 210. If, at act 206, the script library determines that the account has not used the digital assistant to perform an action for the electronic resource, the script library can proceed to act 210 to provide for preempting the underlying form entry point.
At act 210, the script library can provide preemption underlying table entry points, which can include determining an execution mode of the digital assistant. The script library can determine to provide a more prominent digital interface. The script library can proceed to act 212 to determine if an account link to the electronic resource is required. The account link can link the digital assistant to the electronic resource to facilitate the digital assistant in performing an action associated with the electronic resource. The script library can determine whether an account link is required based on the type of action to be performed (e.g., purchase). If at act 212 the script repository determines that account linking is required, the script repository can proceed to act 214 to execute an account linking flow, which can include a prompt for account credentials or a request to link an account of the electronic resource with the digital assistant, and then proceed to act 216. If the script library determines in act 212 that an account link is not needed, the script library can proceed to act 216 to provide an action link user interface with a device selector. The script library can rank available devices configured with the digital assistant to perform the action and receive a selection of devices to perform the action. In some cases, the script library can automatically select the highest ranked device to perform the action.
Fig. 3 is an illustration of a process of adjusting the execution of a digital action according to an embodiment. The process 300 can be performed by one or more systems or components depicted in fig. 1 or fig. 7, including, for example, a data processing system, a remote data processing system, a local digital assistant, a script library, a dynamic notifier component, or a digital selector component. Process 300 can be used for devices that already have linked accounts. For example, at act 302, the script library can determine whether an account of the digital assistant provider is available. The script library can determine whether the digital assistant account is activated or logged in for the session, such as in an application, operating system, or device. If, at act 302, the scripting library determines that an account of the digital assistant is not logged in or available, the scripting library can proceed to act 306 to select and provide an in-line button entry point. The script library can provide a default digital interface for call actions that are not customized. The script library can proceed to act 308 to provide a web browsing-based action link, which can refer to or include a link to an electronic resource.
However, if at act 302 the script library determines that the digital assistant account is logged in, the script library can proceed to act 304 to determine a level of historical engagement with the digital assistant. If the account has not historically engaged with the digital assistant, or has a low level of engagement with the digital assistant that is below a threshold (e.g., more than 3 months since the account engaged with the digital assistant), the script library can proceed to act 306 to provide a default, non-customized digital interface.
If at act 304 the script repository determines that the account has engaged with the digital assistant or has engaged more than a threshold, the script repository can proceed to act 310 to determine whether the account of the electronic resource has been linked with the account of the digital assistant. The script library is capable of determining that the digital assistant account is an account linked to a 3P electronic resource or a 3P electronic resource server. If the digital assistant account is not linked to an account of the electronic resource, the script library can proceed to act 318 to perform an account linking flow.
However, if at act 310 the scripting library determines that the digital assistant account is linked to an electronic resource account (e.g., an account of a travel website, music service, or online marketplace), the scripting library can proceed to act 312 to determine whether the device or account has used the digital assistant to perform the invoking action for the electronic resource. If the scripting library determines at 312 that the device has not used the digital assistant to perform an action for the electronic resource, the scripting library can proceed to act 316 to return an action-linked user interface ("UI") having a device selector. However, if the scripting library determines that the digital assistant has been used to perform an action for an electronic resource, the scripting library can proceed to action 314 to select an embedded button entry point, which can be a customized or personalized digital interface that is prominently displayed with the call action for the digital assistant. The script library can select a more prominent digital interface based on the account that is linked and previously used to perform the action of the electronic resource. The script library can then proceed to act 318 to provide an action link UI with a device selector. The script library can rank the available devices for selection, or automatically select the highest ranked device.
Fig. 4 is an illustration of a graphical user interface for performing a digital action, according to an embodiment. The graphical user interface 400 can be provided by one or more of the systems or components depicted in fig. 1 or fig. 7, including, for example, a data processing system, a remote data processing system, a local digital assistant, a script library, a dynamic notifier component, a digital selector component, or a display device. The graphical user interface 400 can include an electronic resource 112, primary content 402, and an icon or embedded entry point button 404. Primary content 402 can include or refer to text, images, multimedia content, or other content provided by a provider of electronic resource 112. Icon 404 can be an example of a digital interface type and can include an embedded button entry point with a prompt, such as "try with digital assistant. The script library can select button 404 based on detecting that the digital assistant account is not logged in (e.g., via process 200 or 300). The script library can display an icon 404 below the main content 402 so as not to obscure or otherwise block the main content 402.
Fig. 5 is an illustration of a graphical user interface for performing a digital action, according to an embodiment. The graphical user interface 500 can be provided by one or more of the systems or components depicted in fig. 1 or fig. 7, including, for example, a data processing system, a remote data processing system, a local digital assistant, a script library, a dynamic notifier component, a digital selector component, or a display device. The graphical user interface 500 can include a display of the electronic resource 112, the primary content 402, a pop-up window 502 with buttons that link to an account 504 or do not link to an account 506. The graphical user interface 500 can be provided via the process 300 or 400.
The pop-up window can be a digital interface and correspond to the type of digital interface selected by the script library based on the attributes and the historical engagement level. The script library can select, generate, and provide a pop-up window 502 in response to determining that the digital assistant account is logged in, that the account has been previously engaged with the digital assistant, but that the account of the electronic resource provider is not linked with the digital assistant provider. For example, the script library can determine to select a digital interface type with a more prominent call action based on historical engagement levels and device attributes. For example, if the first attribute indicates that the client computing device is a smartphone enabled with a digital assistant, and the historical engagement level indicates that the computing device interacted with one or more digital assistants during a time interval prior to a scripting library loaded by the data processing system for an electronic resource, the scripting library is able to select a digital interface having a pop-up icon 502 overlaid on the electronic resource 112. The pop-up window 502 is more prominent than the icon 404 depicted in fig. 4 because the pop-up window 502 blocks a portion of the primary content 402 (e.g., half of the primary content 402) of the electronic resource 112. The script library can determine to provide a more prominent type of digital interface. As depicted in processes 200 and 300, link account button 504 can execute an account link flow.
Fig. 6 is an illustration of a method of adjusting performance of a digital action according to an embodiment. Method 600 can be performed by one or more systems or components depicted in fig. 1 or fig. 7, including, for example, a data processing system, a remote data processing system, a local digital assistant, a script library, a dynamic notifier component, or a digital selector component.
At 602, method 600 can include a computing device or data processing system loading a script library of an electronic resource or embedding in an electronic resource. The script library can be embedded in a web page provided by a web server. A data processing system or device is capable of executing a script library within an iframe of an electronic resource. The script library can include one or more calling actions, components, or modules. The data processing system can load a script library embedded in the electronic resource via a client computing device linked to an electronic account of the digital assistant. The script library can include a plurality of invocation actions for the electronic resource that are configured for execution by one or more digital assistants provided by the client computing device.
At 604, the script library can determine whether the digital assistant account is logged in for the session (e.g., currently logged in on the device). The script library can query an application or operating system of the device to determine whether the digital assistant account is active, or otherwise logged in. If the scripting library determines that the digital assistant account is not active or not logged in, the scripting library can proceed to act 606 to provide a default digital interface for invoking actions that are not customized or configured for digital assistant execution.
If, however, at act 604 the script library determines that the digital assistant account is active, the script library can proceed to act 608. At 608, the script library can determine a historical engagement level. The script library can query the digital assistant server to determine a level of historical engagement between the account and the digital assistant. The script library can query the digital assistant server to determine a level of historical engagement between a client computing device linked to the electronic account and one or more digital assistants.
At 610, the script library can select and generate a digital interface based on a historical level of engagement between the account and the digital assistant. The script library can select a digital interface based on a first attribute of the client computing device and a historical level of engagement. The script library can provide the call action via a digital interface.
At 612, the script library can detect an instruction to perform the call action and determine an execution mode for the call action. The script library can determine an execution mode for the invoking action based on a second attribute of the client computing device and the historical engagement levels in response to the instruction to execute the invoking action. The execution mode can be, for example, to launch a local digital assistant or a 3P-provided digital assistant.
At act 614, the script library can determine if any compatible devices are available. The script library is capable of identifying available digital assistant-enabled devices. If the script library is unable to identify any digital assistant-enabled devices currently available, the script library can proceed to act 606 to provide a default digital interface. For example, the execution mode can be to use a local digital assistant, but the client device may not have an installed local digital assistant. In another example, the execution mode can be using a third party provided digital assistant, but the current client device may not have an installed third party digital assistant, and the second linking device may be offline or not located near the first computing device.
However, if the script library identifies available digital assistant-enabled devices that are capable of performing the invoking action via the selected execution mode, the script library can proceed to act 616. At act 616, the script library can select the digital assistant to perform the call action. The script library can select the second computing device based on the one or more attributes and the historical engagement level. For example, the script library can select a digital assistant configured on the second computing device that was most recently used to perform a similar call action and that is proximate to the first computing device (e.g., on the same WIFI network). At act 618, the script library can transmit the invoking action to the selected second computing device to cause the digital assistant of the second computing device to perform the invoking action.
Fig. 7 is a block diagram of an exemplary computer system 700. Computer system or computing device 700 can include or be used to implement system 100 or components thereof, such as data processing system 102. Computer system 700 includes a bus 705 or other communication component for communicating information, and a processor 710 or processing circuit coupled to bus 705 for processing information. The computing system 700 can also include one or more processors 710 or processing circuits coupled to the bus for processing information. Computing system 700 also includes main memory 715, such as a Random Access Memory (RAM) or other dynamic storage device, coupled to bus 705 for storing information and instructions to be executed by processor 710. The main memory 715 can be or include a data store. Main memory 715 also can be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 710. Computing system 300 may further include a Read Only Memory (ROM) 720 or other static storage device coupled to bus 705 for storing static information and instructions for processor 310. A storage device 725, such as a solid state device, magnetic disk or optical disk, can be coupled to bus 705 for persistently storing information and instructions. The storage device 725 can include or be part of a data store.
The processes, systems, and methods described herein can be implemented by the computing system 700 in response to the processor 710 executing an arrangement of instructions contained in main memory 715. Such instructions can be read into main memory 715 from another computer-readable medium, such as storage device 725. Execution of the arrangement of instructions contained in main memory 715 causes computing system 700 to perform the illustrative processes described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 715. Hard-wired circuitry can be used in place of or in combination with software instructions and the systems and methods described herein. The systems and methods described herein are not limited to any specific combination of hardware circuitry and software.
Although an exemplary computing system is depicted in FIG. 7, the subject matter including the operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their equivalents, or in combinations of one or more of them.
Where the systems described herein collect or utilize personal information about a user or an application installed on a user device, the user is provided with an opportunity to control whether programs or features can collect user information (e.g., information about the user's social network, social behavior or activity, profession, the user's preferences, or the user's current location). Additionally or alternatively, certain data can be processed in one or more ways to remove personal information before it is stored or used.
The subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs, e.g., one or more circuits of computer program instructions encoded on one or more computer storage media, for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium can be or be included in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Although the computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage media can also be or be included in one or more separate components or media (e.g., multiple CDs, disks, or other storage devices). The operations described in this specification can be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The terms "data processing system," "computing device," "component," or "data processing apparatus" encompass various devices, apparatuses, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or a plurality of systems on a chip, or a combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment are capable of implementing a variety of different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures. The digital assistant component 106 and other data processing system 102 or remote data processing system 136 components can include or share one or more data processing apparatus, systems, computing devices, or processors. The digital assistant server 138 and the content selector component 140 can include or share, for example, one or more data processing apparatus, systems, computing devices, or processors.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. The computer program can correspond to a file in a file system. A computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs (e.g., components of data processing system 102) to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
The subject matter described herein can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or a combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
A computing system, such as system 100 or system 700, can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network, such as network 105. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, the server transmits data (e.g., data packets representing digital components) to the client device (e.g., for the purpose of displaying data to or receiving user input from a user interacting with the client device). Data (e.g., the results of the user interaction) generated at the client device can be received at the server from the client device (e.g., received by the digital assistant server 138 from the digital assistant component 106 or the 3P digital content provider device 134 of the computing device 122).
Although operations are depicted in the drawings in a particular order, these operations need not be performed in the particular order shown or in sequential order, and all illustrated operations need not be performed. The actions described herein can be performed in a different order.
The separation of various system components need not require separation in all embodiments, and the described program components can be included in a single hardware or software product. For example, the dynamic notifier component 116 and the digital assistant selector component 120 can be a single component, application or program, or a logical device having one or more processing circuits, or executed by one or more processors of the data processing system 102.
Having now described some illustrative embodiments, it will be apparent that the foregoing is illustrative and not limiting, having been presented by way of example. In particular, although many of the examples presented herein involve specific combinations of method acts or system elements, these acts and those elements may be combined in other ways to accomplish the same objectives. Acts, elements and features discussed in connection with one embodiment are not intended to be excluded from a similar role in other embodiments.
The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of "including," "comprising," "having," "containing," "involving," "characterized by," and variations thereof herein, is meant to encompass the items listed thereafter and equivalents thereof as well as additional items and alternative embodiments consisting of the items listed thereafter. In one embodiment, the systems and methods described herein include one, each combination of more than one, or all of the described elements, acts, or components.
Any reference to an embodiment, element, or act of the systems and methods herein in the singular may also encompass embodiments comprising a plurality of these elements, and any reference to any embodiment, element, or act herein in the plural may also encompass embodiments comprising only one element. References in the singular or plural form are not intended to limit the presently disclosed systems or methods, their components, acts, or elements to a single or multiple configurations. References to any action or element based on any information, action, or element may include embodiments in which the action or element is based, at least in part, on any information, action, or element.
Any embodiment disclosed herein may be combined with any other embodiment or examples, and references to "an embodiment," "some embodiments," "one embodiment," etc. are not necessarily mutually exclusive and are intended to indicate that a particular feature, structure, or characteristic described in connection with the embodiment may be included in at least one embodiment or example. The terms used herein do not necessarily all refer to the same embodiment. Any embodiment may be included or exclusively combined with any other embodiment in any manner consistent with aspects and embodiments disclosed herein.
References to "or" may be construed as inclusive such that any term described using "or" may indicate a single, more than one, or all of the described terms. A reference to at least one of the combination list of terms can be interpreted as being inclusive or indicating a single, more than one, or all of the terms so described. For example, a reference to "at least one of a 'and' B" can include only 'a', only 'B', and both 'a' and 'B'. Such references, used in conjunction with "including" or other open-ended terms, can include additional items.
Where technical features in the figures, detailed description or any claims are followed by reference signs, the reference signs have been included to increase the intelligibility of the figures, detailed description and claims. Accordingly, neither the reference numerals nor their absence have any limiting effect on the scope of any claim element.
The systems and methods described herein may be embodied in other specific forms without departing from the characteristics thereof. For example, a device, product, or service described as a 3P or third party, such as 3P digital content provider device 160, can be partially or completely or include a first party device, product, or service, and can be commonly owned by an entity associated with data processing system 102, digital assistant server 138, or other component. The foregoing embodiments are illustrative and not limiting of the described systems and methods. The scope of the systems and methods described herein is, therefore, indicated by the appended claims rather than by the foregoing description, and all changes that come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein.
Claims (20)
1. A system for adjusting the execution of a digital action, comprising:
a data processing system comprising one or more processors to:
loading, via a client computing device of a plurality of client computing devices linked to an electronic account, a script library embedded in an electronic resource, the script library comprising a plurality of invocation actions for the electronic resource, the plurality of invocation actions configured for execution by one or more digital assistants provided by each of the plurality of client computing devices;
query a digital assistant component to determine a level of historical engagement between the plurality of client computing devices and the one or more digital assistants;
selecting a type of digital interface in which to present a calling action of the plurality of calling actions based on a first attribute of the client computing device and the historical engagement level;
generating a digital interface with the invoking action based on the type of the digital interface selected according to the first attribute of the client computing device and the historical engagement level;
detecting, via the digital interface, an instruction to perform the call action;
in response to an instruction to perform the invoking action, determining an execution mode for the invoking action based on a second attribute of the client computing device and the historical engagement levels;
selecting a digital assistant from the one or more digital assistants and a second client device of the plurality of client computing devices to perform the invoking action based on the execution mode; and
transmitting the invoking action to the second client device to cause the second client device to invoke the digital assistant to perform the invoking action.
2. The system of claim 1, wherein the script library is established by a third party service provider with the plurality of invocation actions comprising a predetermined set of invocation actions selected by the third party service provider.
3. The system of claim 1, comprising:
the data processing system is for loading a graphical user interface element corresponding to the script library at a location on the electronic resource established by a provider of the electronic resource.
4. The system of claim 1, comprising:
the data processing system is for receiving information about the historical engagement level from the digital assistant component remote from the data processing system via a network.
5. The system of claim 1, comprising:
the data processing system determines the historical engagement level based on a number of previous interactions between the plurality of client computing devices during a time interval.
6. The system of claim 1, comprising:
the data processing system is configured to determine a historical engagement level for a type of invoking action.
7. The system of claim 1, comprising:
the data processing system is to select a type of digital interface that includes a voice-based interface based on the first attribute of the client computing device indicating that the client computing device includes a microphone and a speaker and the historical level of engagement indicating that a number of interactions between the plurality of client computing devices and the one or more digital assistants is greater than a threshold.
8. The system of claim 1, comprising:
the data processing system is to select a type of digital interface that includes a graphical user interface based on the first attribute of the client computing device indicating that the client computing device lacks a microphone or that access to the microphone or speaker of the client computing device by the one or more digital assistants has been disabled.
9. The system of claim 1, comprising:
the data processing system is to select the execution mode for the invoking action based on the second attribute of the client computing device indicating a type of operating system of the client computing device.
10. The system of claim 1, wherein the first attribute indicates that the client computing device is a smartphone enabled with the digital assistant, and the historical engagement level indicates that the plurality of client computing devices interacted with the one or more digital assistants during a time interval prior to a script library loaded by the data processing system for the electronic resource, including:
the data processing system selects the digital interface that includes a pop-up icon overlaid on the electronic resource.
11. A method of adjusting performance of a digital action, comprising:
loading, by a data processing system comprising one or more processors via a client computing device of a plurality of client computing devices linked to an electronic account, a script library embedded in an electronic resource, the script library comprising a plurality of invocation actions for the electronic resource, the plurality of invocation actions configured for execution by one or more digital assistants provided by each of the plurality of client computing devices;
querying, by the data processing system, a digital assistant component to determine a level of historical engagement between the plurality of client computing devices and the one or more digital assistants;
selecting, by the data processing system, a type of digital interface in which to present a calling action of the plurality of calling actions based on a first attribute of the client computing device and the historical engagement level;
generating, by the data processing system, a digital interface with the invoking action based on the type of the digital interface selected according to the first attribute of the client computing device and the historical engagement level;
detecting, by the data processing system via the digital interface, an instruction to perform the call action;
determining, by the data processing system and in response to the instruction to perform the invoking action, an execution mode for the invoking action based on a second attribute of the client computing device and the historical engagement levels;
selecting, by the data processing system, a digital assistant from the one or more digital assistants and a second client device of the plurality of client computing devices to perform the invoking act based on the execution mode; and
transmitting, by the data processing system, the invoking action to the second client device to cause the second client device to invoke the digital assistant to perform the invoking action.
12. The method of claim 11, wherein the script library is established by a third party service provider with the plurality of invocation actions comprising a predetermined set of invocation actions selected by the third party service provider.
13. The method of claim 11, comprising:
loading a graphical user interface element corresponding to the script library at a location on the electronic resource established by a provider of the electronic resource.
14. The method of claim 11, comprising:
receiving information regarding the historical engagement level from the digital assistant component remote from the data processing system via a network.
15. The method of claim 11, comprising:
determining the historical engagement level based on a number of previous interactions between the plurality of client computing devices during a time interval.
16. The method of claim 11, comprising:
determining the historical engagement level for a type of invoking action.
17. The method of claim 11, comprising:
selecting a type of digital interface that includes a voice-based interface based on the first attribute of the client computing device indicating that the client computing device includes a microphone and a speaker and the historical engagement level indicating that a number of interactions between the plurality of client computing devices and the one or more digital assistants is greater than a threshold.
18. The method of claim 11, comprising:
selecting a type of the digital interface comprising a graphical user interface based on the first attribute of the client computing device indicating that the client computing device lacks a microphone or that access to the microphone or speaker of the client computing device by the one or more digital assistants has been disabled.
19. The method of claim 11, comprising:
selecting the execution mode for the invoking action based on the second attribute of the client computing device indicating a type of operating system of the client computing device.
20. The method of claim 11, wherein the first attribute indicates that the client computing device is a smartphone enabled with the digital assistant, and the historical engagement level indicates that the plurality of client computing devices interacted with the one or more digital assistants during a time interval prior to a script library loaded by the data processing system for the electronic resource, comprising:
selecting the digital interface that includes a pop-up icon overlaid on the electronic resource.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/020341 WO2021173151A1 (en) | 2020-02-28 | 2020-02-28 | Interface and mode selection for digital action execution |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115210692A true CN115210692A (en) | 2022-10-18 |
Family
ID=70058472
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080097834.2A Pending CN115210692A (en) | 2020-02-28 | 2020-02-28 | Interface and mode selection for digital motion execution |
Country Status (6)
Country | Link |
---|---|
US (5) | US11922193B2 (en) |
EP (1) | EP4111304A1 (en) |
JP (1) | JP7440654B2 (en) |
KR (1) | KR20220141891A (en) |
CN (1) | CN115210692A (en) |
WO (1) | WO2021173151A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11657295B2 (en) * | 2020-03-31 | 2023-05-23 | Bank Of America Corporation | Cognitive automation platform for dynamic unauthorized event detection and processing |
KR20220046964A (en) * | 2020-10-08 | 2022-04-15 | 삼성전자주식회사 | Electronic apparatus for responding to question using multi chat-bot and control method thereof |
US11475122B1 (en) * | 2021-04-16 | 2022-10-18 | Shape Security, Inc. | Mitigating malicious client-side scripts |
US20230074240A1 (en) * | 2021-09-08 | 2023-03-09 | BlueStack Systems, Inc. | Methods, Systems and Computer Program Products for Fine Grained Cloud Gaming Session Control |
US11842026B2 (en) * | 2022-03-31 | 2023-12-12 | Microsoft Technology Licensing, Llc | Intelligent placement of a browser-added user interface element on a webpage |
Family Cites Families (32)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3720024B2 (en) * | 2003-01-10 | 2005-11-24 | 株式会社東芝 | Electronic device system and operation control method |
US9318108B2 (en) * | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US20110106835A1 (en) * | 2009-10-29 | 2011-05-05 | International Business Machines Corporation | User-Defined Profile Tags, Rules, and Recommendations for Portal |
US8626511B2 (en) * | 2010-01-22 | 2014-01-07 | Google Inc. | Multi-dimensional disambiguation of voice commands |
US20110238731A1 (en) | 2010-03-23 | 2011-09-29 | Sony Corporation | Method to provide an unlimited number of customized user interfaces |
JP5727964B2 (en) | 2012-04-19 | 2015-06-03 | 日本電信電話株式会社 | Link area highlighting apparatus and operation method thereof |
US9085303B2 (en) * | 2012-11-15 | 2015-07-21 | Sri International | Vehicle personal assistant |
US10735552B2 (en) * | 2013-01-31 | 2020-08-04 | Google Llc | Secondary transmissions of packetized data |
US9341479B2 (en) * | 2013-03-05 | 2016-05-17 | Google Inc. | Configurable point of interest alerts |
US10445115B2 (en) * | 2013-04-18 | 2019-10-15 | Verint Americas Inc. | Virtual assistant focused user interfaces |
US10482490B2 (en) * | 2014-04-09 | 2019-11-19 | Sailthru, Inc. | Behavioral tracking system and method in support of high-engagement communications |
US9467743B2 (en) * | 2014-09-15 | 2016-10-11 | Verizon Patent And Licensing Inc. | Personalized content aggregation platform |
US9922357B2 (en) * | 2014-09-18 | 2018-03-20 | Adobe Systems Incorporated | Interactive notifications for mobile commerce applications |
US9762945B2 (en) * | 2015-05-19 | 2017-09-12 | Rovi Guides, Inc. | Methods and systems for recommending a display device for media consumption |
US20170187815A1 (en) * | 2015-12-24 | 2017-06-29 | Intel Corporation | Indoor device control assistance |
US10069934B2 (en) * | 2016-12-16 | 2018-09-04 | Vignet Incorporated | Data-driven adaptive communications in user-facing applications |
US11089132B2 (en) * | 2016-03-29 | 2021-08-10 | Microsoft Technology Licensing, Llc | Extensibility for context-aware digital personal assistant |
US10115400B2 (en) * | 2016-08-05 | 2018-10-30 | Sonos, Inc. | Multiple voice services |
US11150922B2 (en) | 2017-04-25 | 2021-10-19 | Google Llc | Initializing a conversation with an automated agent via selectable graphical element |
US10237209B2 (en) | 2017-05-08 | 2019-03-19 | Google Llc | Initializing a conversation with an automated agent via selectable graphical element |
US10841122B1 (en) * | 2017-08-02 | 2020-11-17 | Vivint, Inc. | Automatic custom rule generation for home automation system |
US20190095069A1 (en) * | 2017-09-25 | 2019-03-28 | Motorola Solutions, Inc | Adaptable interface for retrieving available electronic digital assistant services |
US10546023B2 (en) * | 2017-10-03 | 2020-01-28 | Google Llc | Providing command bundle suggestions for an automated assistant |
US10368333B2 (en) * | 2017-11-20 | 2019-07-30 | Google Llc | Dynamically adapting provision of notification output to reduce user distraction and/or mitigate usage of computational resources |
EP3745236A4 (en) | 2018-01-24 | 2021-03-24 | Sony Corporation | Information processing device and information processing method |
DK201870335A1 (en) * | 2018-05-07 | 2019-12-04 | Apple Inc. | Devices, methods, and graphical user interfaces for proactive management of notifications |
US11087748B2 (en) | 2018-05-11 | 2021-08-10 | Google Llc | Adaptive interface in a voice-activated network |
US11169668B2 (en) * | 2018-05-16 | 2021-11-09 | Google Llc | Selecting an input mode for a virtual assistant |
US10860096B2 (en) * | 2018-09-28 | 2020-12-08 | Apple Inc. | Device control using gaze information |
US10497361B1 (en) * | 2018-12-26 | 2019-12-03 | Capital One Services, Llc | Systems and methods for providing a virtual assistant |
US11507855B2 (en) * | 2019-03-22 | 2022-11-22 | Motorola Mobility Llc | Generating action suggestions based on a change in user mood |
US11290574B2 (en) * | 2019-05-20 | 2022-03-29 | Citrix Systems, Inc. | Systems and methods for aggregating skills provided by a plurality of digital assistants |
-
2020
- 2020-02-28 US US16/643,648 patent/US11922193B2/en active Active
- 2020-02-28 CN CN202080097834.2A patent/CN115210692A/en active Pending
- 2020-02-28 JP JP2022551353A patent/JP7440654B2/en active Active
- 2020-02-28 KR KR1020227033444A patent/KR20220141891A/en unknown
- 2020-02-28 WO PCT/US2020/020341 patent/WO2021173151A1/en unknown
- 2020-02-28 EP EP20716009.4A patent/EP4111304A1/en active Pending
- 2020-03-12 US US16/816,794 patent/US11086644B1/en active Active
-
2021
- 2021-08-03 US US17/393,276 patent/US11620143B2/en active Active
-
2023
- 2023-04-03 US US18/194,854 patent/US20230325217A1/en active Pending
-
2024
- 2024-01-19 US US18/417,681 patent/US20240152369A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR20220141891A (en) | 2022-10-20 |
US20230325217A1 (en) | 2023-10-12 |
US11922193B2 (en) | 2024-03-05 |
US20210271497A1 (en) | 2021-09-02 |
US20240152369A1 (en) | 2024-05-09 |
US20210365281A1 (en) | 2021-11-25 |
JP2023515158A (en) | 2023-04-12 |
JP7440654B2 (en) | 2024-02-28 |
US11620143B2 (en) | 2023-04-04 |
US11086644B1 (en) | 2021-08-10 |
WO2021173151A1 (en) | 2021-09-02 |
US20230168909A1 (en) | 2023-06-01 |
EP4111304A1 (en) | 2023-01-04 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11620143B2 (en) | Interface and mode selection for digital action execution | |
US11924644B2 (en) | Secure communication in mobile digital pages | |
US11599336B2 (en) | Generating and updating voice-based software applications using application templates | |
US11893993B2 (en) | Interfacing with applications via dynamically updating natural language processing | |
US11514896B2 (en) | Interfacing with applications via dynamically updating natural language processing | |
CN112262391A (en) | Secure digital assistant integration in web pages | |
US20220308987A1 (en) | Debugging applications for delivery via an application delivery server | |
US20240144928A1 (en) | Systems and methods to verify trigger keywords in acoustic-based digital assistant applications | |
US11385990B2 (en) | Debugging applications for delivery via an application delivery server |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |