JP2021509749A - Selecting content to render on the display of the assistant device - Google Patents
Selecting content to render on the display of the assistant device Download PDFInfo
- Publication number
- JP2021509749A JP2021509749A JP2020537174A JP2020537174A JP2021509749A JP 2021509749 A JP2021509749 A JP 2021509749A JP 2020537174 A JP2020537174 A JP 2020537174A JP 2020537174 A JP2020537174 A JP 2020537174A JP 2021509749 A JP2021509749 A JP 2021509749A
- Authority
- JP
- Japan
- Prior art keywords
- data items
- user
- display
- assistant
- multimodal
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 claims abstract description 61
- 238000001514 detection method Methods 0.000 claims abstract description 20
- 230000008685 targeting Effects 0.000 claims abstract description 14
- 230000004044 response Effects 0.000 claims description 31
- 230000000694 effects Effects 0.000 claims description 30
- 230000015654 memory Effects 0.000 claims description 14
- 230000002452 interceptive effect Effects 0.000 claims description 10
- 238000009877 rendering Methods 0.000 claims description 9
- 230000009471 action Effects 0.000 claims description 4
- 230000007704 transition Effects 0.000 description 17
- 241001077868 Joanna Species 0.000 description 12
- 238000007792 addition Methods 0.000 description 11
- 230000003993 interaction Effects 0.000 description 8
- 238000004891 communication Methods 0.000 description 6
- 239000000463 material Substances 0.000 description 5
- 230000007246 mechanism Effects 0.000 description 5
- 230000000007 visual effect Effects 0.000 description 5
- 241000699666 Mus <mouse, genus> Species 0.000 description 4
- 235000013550 pizza Nutrition 0.000 description 4
- 230000008569 process Effects 0.000 description 4
- 238000012545 processing Methods 0.000 description 4
- 230000007958 sleep Effects 0.000 description 4
- 230000006870 function Effects 0.000 description 3
- 230000004913 activation Effects 0.000 description 2
- 238000013475 authorization Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 235000013399 edible fruits Nutrition 0.000 description 2
- 238000002474 experimental method Methods 0.000 description 2
- 235000013305 food Nutrition 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 238000010587 phase diagram Methods 0.000 description 2
- 238000010079 rubber tapping Methods 0.000 description 2
- 206010021703 Indifference Diseases 0.000 description 1
- 241000699670 Mus sp. Species 0.000 description 1
- 240000005561 Musa balbisiana Species 0.000 description 1
- 235000018290 Musa x paradisiaca Nutrition 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 230000001815 facial effect Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2457—Query processing with adaptation to user needs
- G06F16/24578—Query processing with adaptation to user needs using ranking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/26—Power supply means, e.g. regulation thereof
- G06F1/32—Means for saving power
- G06F1/3203—Power management, i.e. event-based initiation of a power-saving mode
- G06F1/3206—Monitoring of events, devices or parameters that trigger a change in power modality
- G06F1/3231—Monitoring the presence, absence or movement of users
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2457—Query processing with adaptation to user needs
- G06F16/24575—Query processing with adaptation to user needs using context
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/30—Authentication, i.e. establishing the identity or authorisation of security principals
- G06F21/44—Program or device authentication
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
Abstract
スタンドアロンマルチモダルアシスタントデバイスを使用して表示するためのコンテンツを選択することに関する技術について説明する。様々な実装形態では、未確認のユーザがディスプレイを備えたスタンドアロンマルチモダルアシスタントデバイスと併存しているとして検出され得る。その検出に基づいて、様々な実装形態では、1人または複数の登録ユーザをターゲットにした複数のデータ項目が取得され得る。1人または複数の登録ユーザは、スタンドアロンマルチモダルアシスタントデバイス上で少なくとも部分的に動作する自動アシスタントにとってアクセス可能なアカウントを有し得る。複数のデータ項目は、登録ユーザのアカウントに基づいて取得され得る。複数のデータ項目は、登録ユーザ関係の優先順位および非登録ユーザ関係の優先順位など、様々な信号に基づいてランク付けされ得る。ランク付けされた複数のデータ項目を示すグラフィカル情報がディスプレイ上にレンダリングされ得る。Describes techniques for selecting content for display using a stand-alone multimodal assistant device. In various implementations, an unidentified user can be detected as co-existing with a stand-alone multimodal assistant device with a display. Based on that detection, in various implementations, multiple data items targeting one or more registered users may be retrieved. One or more registered users may have an account accessible to an automated assistant running at least partially on a standalone multimodal assistant device. Multiple data items can be retrieved based on the registered user's account. Multiple data items can be ranked based on various signals, such as the priority of registered user relationships and the priority of unregistered user relationships. Graphical information showing multiple ranked data items can be rendered on the display.
Description
本発明は、アシスタントデバイスのディスプレイにレンダリングするコンテンツの選択に関する。 The present invention relates to the selection of content to render on the display of an assistant device.
人々は、本明細書で「自動アシスタント」と呼ばれる(「チャットボット」、「対話型パーソナルアシスタント」、「インテリジェントパーソナルアシスタント」、「パーソナルボイスアシスタント」、「会話型エージェント(conversational agents)」などとも呼ばれる)対話型ソフトウェアアプリケーションを用いて人間対コンピュータ対話に関与することができる。たとえば、(自動アシスタントと対話するときには「ユーザ」と呼ばれることがあり、または会議の文脈では「参加者」と呼ばれることがある)人間は、テキストに変換され、次いで処理される音声発話であり得る自由形式の自然言語入力を使用して、かつ/またはタイプ入力された自由形式の自然言語入力によって、命令、クエリ、および/または要求(本明細書で「クエリ」と総称される)を提供することができる。 People are also referred to herein as "automatic assistants" ("chatbots", "interactive personal assistants", "intelligent personal assistants", "personal voice assistants", "conversational agents", etc. ) Can engage in human-to-computer dialogue using interactive software applications. For example, a human (sometimes called a "user" when interacting with an automated assistant, or a "participant" in the context of a conference) can be a voice utterance that is converted to text and then processed. Provides instructions, queries, and / or requests (collectively, "queries" herein) using free-form natural language input and / or by type-typed free-form natural language input. be able to.
自動アシスタントに音声で関与する能力をユーザに提供するスタンドアロン音声応答スピーカは、家庭内で、またビジネスにおいてますます普及してきている。これらのデバイスは、一般に、存在する場合でも、恐らくは消音ボタン、音量を調整するためのタッチセンシティブインターフェースなど以外には、ごく少数のハードウェア入力機構を含む。これらのスピーカの目標は、音楽を再生する、リマインダをセットアップする、(たとえば、特定の情報を取得するために)探索を実行する、天気予報を要求する、タイマーをセットする、スマートホームデバイス(たとえば、照明、サーモスタット、ロックなど)を制御する、アラームをセットする、リスト(たとえば、買い物リスト)を作成する、商品および/またはサービスを注文するなど、様々なタスクを実行するために、キーボードまたはマウスなどのユーザインターフェース要素と物理的に対話することをユーザに要求せずに、ユーザが自動アシスタントと容易に音声で関与することを可能にすることである。多くの点で、スタンドアロン音声応答スピーカは、ヒューマンパーソナルアシスタントと極めて同様に機能することが多い。 Stand-alone voice-answering speakers, which provide users with the ability to engage with the automatic assistant by voice, are becoming more and more popular in the home and in the business. These devices generally include a very small number of hardware input mechanisms, if any, other than perhaps a mute button, a touch-sensitive interface for adjusting volume, and so on. The goals of these speakers are to play music, set up reminders, perform searches (for example, to get specific information), request weather forecasts, set timers, smart home devices (eg, for example). Keyboard or mouse to perform various tasks such as controlling lighting, thermostats, locks, setting alarms, creating lists (eg shopping lists), ordering goods and / or services, etc. It is to allow the user to easily engage with the automatic assistant by voice without requiring the user to physically interact with the user interface elements such as. In many respects, stand-alone voice-answering speakers often function much like a human personal assistant.
従来のスタンドアロン音声応答スピーカには、一般に、本格的なディスプレイが欠如している。従来のスタンドアロン音声応答スピーカは、多くても、単純なメッセージを伝えるために基本的な色および/または動画を利用することができる、発光ダイオードなどの比較的単純な視覚的出力機構を含む傾向がある。次世代のスタンドアロン音声応答スピーカは、ディスプレイまたはさらにタッチスクリーンディスプレイなど、よりロバストな視覚的出力機構を含み得る。これらのデバイスは、スタンドアロン音声応答スピーカと対照的に、本明細書で「スタンドアロンマルチモダルアシスタントデバイス(standalone multi-modal assistant devices)」と呼ばれる。従来のスタンドアロン対話型スピーカの場合と同様に、スタンドアロンマルチモダルアシスタントデバイスは、音声で対話するように設計可能であり、一般に、キーボード、マウス、または他の複雑な物理的入力構成要素を含まないことになる。しかしながら、いくつかは、タッチスクリーンを含むことがある。 Traditional stand-alone voice-answering speakers generally lack a full-fledged display. Traditional stand-alone voice response speakers tend to include relatively simple visual output mechanisms such as light emitting diodes that can utilize basic colors and / or video to convey simple messages at most. is there. Next-generation stand-alone voice-responsive speakers may include more robust visual output mechanisms, such as displays or even touch screen displays. These devices are referred to herein as "standalone multi-modal assistant devices," as opposed to stand-alone voice-answering speakers. As with traditional stand-alone interactive speakers, stand-alone multimodal assistant devices can be designed for voice interaction and generally do not include a keyboard, mouse, or other complex physical input component. become. However, some may include a touch screen.
一般的なアシスタンスを提供するヒューマンパーソナルアシスタントと同種のスタンドアロン音声応答スピーカがサービスする一般的な機能に沿って、スタンドアロンマルチモダルアシスタントデバイスのディスプレイをコンテンツで埋めるときには注意が払われるべきである。たとえば、いくつかのスタンドアロンマルチモダルデバイスは、付近の未確認の人物の検出に応答して、ディスプレイをアクティブ化し得る。(たとえば、音声認識を使用して)併存している人物が識別される前には、1人または複数の特定の登録ユーザをターゲットにするか、またはさもなければ彼らによって制御されるコンテンツ(以下で、「ターゲットコンテンツ」と呼ばれる)を含み得る、どのコンテンツがディスプレイ上に提示されるべきかが明瞭でないことがある。そしてターゲットコンテンツが表示された後、その人物が次いで識別され得る。これは、他の登録ユーザをターゲットにしたコンテンツが引き続き表示されるべきであるかどうか、またはディスプレイを識別された人物をターゲットにしたコンテンツに限定すべきかどうかという問題を提起する。場合によっては、その人物は、識別されるのに先立って、たとえば、そのコンテンツをスワイプする(すなわち、退ける)ことによって、そのコンテンツと対話することがある。これは、退けられたコンテンツが、それらの人々が未確認であるか、または異なるユーザとして識別されるのかにかかわらず、他の人々に引き続き提示されるべきであるかどうかという問題を提起する。 Care should be taken when filling the display of a stand-alone multimodal assistant device with content, in line with the common features provided by stand-alone voice-answering speakers similar to human personal assistants that provide general assistance. For example, some stand-alone multimodal devices may activate the display in response to the detection of an unidentified person in the vicinity. Content that targets or is otherwise controlled by one or more specific registered users (eg, using voice recognition) before coexisting individuals are identified (below). It may not be clear which content should be presented on the display, which may include (called "target content"). Then, after the target content is displayed, the person can then be identified. This raises the question of whether content targeted to other registered users should continue to be displayed, or whether the display should be limited to content targeted to identified individuals. In some cases, the person may interact with the content, for example, by swiping (ie, dismissing) the content prior to being identified. This raises the question of whether rejected content should continue to be presented to others, regardless of whether they are unidentified or identified as different users.
スタンドアロンマルチモダルアシスタントデバイスを使用して表示するためのコンテンツを選択するための技法について本明細書で説明する。様々な実装形態では、スタンドアロンマルチモダルアシスタントデバイスは、そのディスプレイをアクティブ化すること、たとえば、付近の人物の検出に応答して、ディスプレイを非アクティブ状態(たとえば、スリープ)からアクティブ状態に遷移させることができる。スタンドアロンマルチモダルアシスタントデバイスは、カメラ、マイクロフォン、パッシブ赤外線(「PIR:passive infrared」)センサー、人々が携行するデバイス(たとえば、モバイルフォン、スマートウォッチ)によって放出された信号など、様々な技法を使用して付近の人々を検出することができる。 Techniques for selecting content for display using a stand-alone multimodal assistant device are described herein. In various implementations, a stand-alone multimodal assistant device activates its display, for example, transitioning the display from an inactive state (eg, sleep) to an active state in response to the detection of a nearby person. Can be done. Stand-alone multimodal assistant devices use a variety of techniques, including cameras, microphones, passive infrared ("PIR") sensors, and signals emitted by people's devices (eg mobile phones, smartwatches). Can detect people in the vicinity.
様々なコンテンツは、アクティブ化時にディスプレイ上にレンダリングされ得る。いくつかの実装形態では、天気予報、一般的なニュース項目、交通情報(特定のユーザのスケジュールされた旅行経路に対して選択されていないとき)など、特定のユーザをターゲットにしない一般的なコンテンツがレンダリングされ得る。追加または代替として、いくつかの実装形態では、1人または複数のユーザは、スタンドアロンマルチモダルアシスタントデバイス上で少なくとも部分的に動作する自動アシスタントに登録しているアカウント(たとえば、オンラインプロファイル)を有することがある。この登録により、自動アシスタントは、登録ユーザの制御下のコンテンツなど、個々の登録ユーザをターゲットにしたコンテンツに対するアクセスを有し得る。1つまたは複数の個別のデータ項目を含み得る、このターゲットコンテンツは、たとえば、自動アシスタントが自然言語出力を使用してこのコンテンツを登録ユーザに音声認識可能に提示し得る方法と同様に、ディスプレイ上に提示され得る。このように、デバイスとの追加のユーザ対話を必要とせずにコンテンツを提示することが可能であり、それにより、そのような対話を解釈しそれに応答する際の計算負荷を低減するだけでなく、コンテンツがアクセスされ得る速度および効率性を改善する。 Various contents can be rendered on the display upon activation. In some implementations, general content that does not target a specific user, such as weather forecasts, general news items, traffic information (when not selected for a specific user's scheduled travel route), etc. Can be rendered. As an addition or alternative, in some implementations, one or more users have an account (eg, an online profile) registered with the Automatic Assistant that runs at least partially on a standalone Multimodal Assistant device. There is. This registration allows the automated assistant to have access to content targeted to individual registered users, such as content under the control of the registered user. This target content, which may contain one or more individual data items, is on the display, similar to how an automated assistant can present this content to registered users in a voice-recognizable manner, for example using natural language output. Can be presented to. In this way, it is possible to present content without the need for additional user interaction with the device, which not only reduces the computational load of interpreting and responding to such interaction. Improve the speed and efficiency with which content can be accessed.
データ項目は、汎用であれ、または特定の登録ユーザをターゲットにしたものであれ、様々な方法で表示され得る。いくつかの実装形態では、データ項目は、スマートフォン、タブレット、スマートウォッチなどの上にレンダリングされるものと同様に、「カード」または「タイル」などのグラフィカル要素によって表され得る。いくつかのカード/タイルは、たとえば、カード/タイルに関連するデータ項目を修正するために使用可能なソフトウェアインターフェースを開始するために、追加情報を取得するために、カード/タイルを非常に重要なものとして標示するためになど、対話型であり得る。カード/タイルは、追加または代替として、たとえば、(タッチスクリーンであり得る)ディスプレイ上でスワイプされることによって、退けられてもよい。いくつかの実装形態では、あるカード/タイルを別のカード/タイル上にスワイプすることによって、複数のカード/タイルを組み合わせて、たとえば、グラフィカル「ビン」にすることが可能である。データ項目は、同様に、ポップアップウィンドウ、「ティッカースタイル(ticker-style)」リーダなどを(たとえば、ディスプレイの上部または下部に)使用するなど、他の方法で視覚的に提示されてもよい。 Data items, whether general purpose or targeted at a particular registered user, can be displayed in a variety of ways. In some implementations, data items can be represented by graphical elements such as "cards" or "tiles", as well as those rendered on top of smartphones, tablets, smartwatches, and the like. Some cards / tiles are very important for getting additional information, for example, to start a software interface that can be used to modify data items related to the card / tile. It can be interactive, such as to mark it as a thing. Cards / tiles may be dismissed as an addition or alternative, for example by swiping on a display (which can be a touch screen). In some implementations, it is possible to combine multiple cards / tiles into, for example, a graphical "bin" by swiping one card / tile onto another. Data items may also be presented visually in other ways, such as using pop-up windows, "ticker-style" readers, etc. (eg, at the top or bottom of the display).
スタンドアロンマルチモダルアシスタントデバイスのディスプレイが最初にアクティブ化されるとき、多くの場合、登録されたどのユーザがそのデバイスと併存しているのか、または併存している人物が登録ユーザであるかどうかさえ知られていないことがある。したがって、いくつかの実装形態では、たとえば、登録ユーザのプライバシーを保護するために、(前に説明した)汎用データ項目のみがグラフィカル要素として提示され得る。しかしながら、家庭またはビジネスのシナリオでは、個々のユーザプライバシーはそれほど重要でない場合があり、場合によっては、(たとえば、家族の一員が家族の他の構成員が会議中であることを知るように)登録ユーザが他の登録ユーザをターゲットにしたコンテンツを見ることが望ましい場合すらある。したがって、いくつかの実装形態では、複数の登録ユーザをターゲットにした複数のデータ項目は、たとえば、その登録ユーザのアカウントに関連するオンラインリソースから取得され、ディスプレイ上にグラフィカル要素としてレンダリングされることが可能である。いくつかの実装形態では、これらのターゲットデータ項目を表すグラフィカル要素は、汎用データ項目を表すグラフィカル要素と組み合わせてレンダリングされてよい。他の実装形態では、ターゲットデータ項目を表すグラフィカル要素は、単独でレンダリングされてよい。 When the display of a standalone multimodal assistant device is first activated, it often knows which registered users co-exist with the device, or even if the co-existing person is a registered user. It may not have been done. Thus, in some implementations, for example, only generic data items (described earlier) may be presented as graphical elements to protect the privacy of registered users. However, in home or business scenarios, individual user privacy may not be as important, and in some cases (for example, one member of the family knows that another member of the family is in a meeting). It may even be desirable for a user to view content that targets other registered users. Therefore, in some implementations, multiple data items targeting multiple registered users may be taken, for example, from online resources associated with that registered user's account and rendered as graphical elements on the display. It is possible. In some implementations, the graphical elements representing these target data items may be rendered in combination with the graphical elements representing generic data items. In other implementations, the graphical element representing the target data item may be rendered independently.
いくつかの実装形態では、ターゲットおよび/または汎用データ項目は、グラフィカル要素として表示されるのに先立ってランク付けされ得る。データ項目は、データ項目に割り当てられた優先順位など、様々な基準に基づいてランク付けされ得る。様々な実装形態では、データ項目の優先順位は、たとえば、登録ユーザによって手動で割り当てられてよく、または、たとえば、データ項目に関連するデータおよび/または時間に基づいて自動的に割り当てられてもよい。たとえば、明日発生する第1の次回のイベントに関連するデータ項目に、一週間以内に発生する第2の次回のイベントに関連する別のデータ項目よりも高い優先順位を割り当てることができる。 In some implementations, target and / or generic data items can be ranked prior to being displayed as graphical elements. Data items can be ranked based on various criteria, such as the priority assigned to the data item. In various implementations, data item priorities may be, for example, manually assigned by a registered user, or, for example, automatically based on the data and / or time associated with the data item. .. For example, a data item related to the first next event that occurs tomorrow can be assigned a higher priority than another data item related to the second next event that occurs within a week.
いくつかの実装形態では、優先順位は、登録ユーザ関係であってよく、または非登録ユーザ関係であってもよい。スタンドアロンマルチモダルアシスタントデバイスと対話している人物の識別情報が知られているとき、登録ユーザ関係の優先順位は、より大きな影響を、データ項目、特にターゲットデータ項目のランク付けに及ぼし得る。スタンドアロンマルチモダルアシスタントデバイスと併存している人物が未確認であるとき、データ項目に関連する次回の日付のみに基づくデータ項目に割り当てられた優先順位など、非登録ユーザ関係の優先順位はより大きな影響をランク付けに及ぼし得る。 In some implementations, the priority may be a registered user relationship or may be a non-registered user relationship. When the identity of the person interacting with the standalone multimodal assistant device is known, the priority of registered user relationships can have a greater impact on the ranking of data items, especially target data items. When the person coexisting with the standalone multimodal assistant device is unidentified, the priority of the unregistered user relationship has a greater impact, such as the priority assigned to the data item based only on the next date associated with the data item. Can affect ranking.
たとえば、スタンドアロンマルチモダルアシスタントデバイスの近傍で登録ユーザが検出されたと仮定する。さらに、スタンドアロンマルチモダルアシスタントデバイスが即時にまたは比較的迅速に登録ユーザを識別することが可能であると仮定する。これは、たとえば、(たとえば、登録ユーザが室内に足を踏み入れたと同時に)自動アシスタントに対して起動フレーズを発話したことからその登録ユーザの存在が検出されたために、または登録ユーザのモバイルフォンが、彼らの存在の検出と彼らの識別の両方を行うために使用された信号を放出するために、発生し得る。 For example, suppose a registered user is found near a standalone multimodal assistant device. Further assume that a stand-alone multimodal assistant device can identify a registered user immediately or relatively quickly. This is because, for example, the presence of the registered user was detected by speaking an activation phrase to the automatic assistant (for example, as soon as the registered user stepped into the room), or the registered user's mobile phone. It can occur to emit a signal that was used to both detect their presence and identify them.
このシナリオでは、スタンドアロンマルチモダルアシスタントデバイスは、その上で少なくとも部分的に動作する自動アシスタントにより、そのディスプレイがアクティブ化されたとき、登録ユーザの識別情報を知る。それは、スタンドアロンマルチモダルアシスタントデバイスが、その特定の登録ユーザが存在するという事実に基づいて、ターゲットデータ項目をランク付けすることが即時に可能であることを意味する。したがって、その登録ユーザを具体的にターゲットにしたデータ項目は、たとえば、汎用データ項目および/または異なる登録ユーザをターゲットにしたデータ項目よりも高くランク付けされ得る。 In this scenario, the stand-alone multimodal assistant device knows the identity of the registered user when its display is activated by an automated assistant running on it, at least partially. That means that a stand-alone multimodal assistant device can instantly rank target data items based on the fact that that particular registered user exists. Thus, a data item specifically targeted to that registered user can be ranked higher than, for example, a generic data item and / or a data item targeted to a different registered user.
次に、登録ユーザが、汎用データ項目または異なる登録ユーザをターゲットにしたデータ項目を表すグラフィカル要素をスワイプすると仮定する。同じ登録ユーザが去り、次いで第2のセッションのために後で戻る(また、スタンドアロンマルチモダルアシスタントデバイスによって識別される)場合、データ項目を表すグラフィカル要素は、ディスプレイ上に再度レンダリングされ得る。しかしながら、その登録ユーザは前に無関心を示したため、この第2のセッションの間、スワイプされたデータ項目は、他のグラフィカル要素とともにディスプレイ上にグラフィカル要素としてレンダリングされないことになる可能性が高くなる。 Next, suppose a registered user swipes on a graphical element that represents a generic data item or a data item that targets a different registered user. If the same registered user leaves and then returns later for a second session (also identified by a standalone multimodal assistant device), the graphical elements representing the data items can be re-rendered on the display. However, because the registered user has previously shown indifference, it is likely that the swiped data item will not be rendered as a graphical element on the display along with other graphical elements during this second session.
スワイプされたデータ項目が異なる第2の登録ユーザをターゲットにした(たとえば、第1の登録ユーザは夫であり、第2の登録ユーザは妻である)と仮定する。さらに、第1の登録ユーザが第2の登録ユーザをターゲットにしたデータ項目をスワイプした後の何らかの時点で、第2の登録ユーザがスタンドアロンマルチモダルアシスタントデバイスと併存しているとして識別されたと仮定する。いくつかの実装形態では、第1の登録ユーザが前にそのグラフィカル要素を退けたにもかかわらず、スワイプされたデータ項目を表すグラフィカル要素は第2の登録ユーザに提示され得るが、これは、そのデータ項目が第2のユーザをターゲットにしていたからである。概して、この機能性は、他の人がそのコンテンツをスワイプしたために登録ユーザが彼らを具体的にターゲットにしたコンテンツを見逃すことになる機会を除去または低減する。たとえば、未確認の人物が、特定の登録ユーザをターゲットにしたデータ項目を表すグラフィカル要素をスワイプする同様のシナリオでは、その特定の登録ユーザが後でスタンドアロンマルチモダルアシスタントデバイスと併存していると識別された場合、スワイプされたコンテンツは再度提示され得る。 Suppose you targeted a second registered user with a different swiped data item (for example, the first registered user is the husband and the second registered user is the wife). Further assume that at some point after the first registered user swipes a data item targeting the second registered user, the second registered user is identified as co-existing with a standalone multimodal assistant device. .. In some implementations, a graphical element representing a swiped data item can be presented to a second registered user, even though the first registered user has previously rejected the graphical element. This is because the data item was targeted at a second user. In general, this functionality eliminates or reduces the chance that registered users will miss content that specifically targets them because someone else swipes the content. For example, in a similar scenario where an unidentified person swipes a graphical element that represents a data item that targets a particular registered user, that particular registered user is later identified as co-existing with a standalone multimodal assistant device. If so, the swiped content may be presented again.
これらの例示的な使用事例は、限定的であることを意味しない。いくつかの他の事例について本明細書で説明する。 These exemplary use cases do not mean to be limited. Some other cases are described herein.
いくつかの実装形態では、1つまたは複数のプロセッサによって実行される方法が提供され、この方法は、未確認のユーザがスタンドアロンマルチモダルアシスタントデバイスと併存していることを検出するステップであって、スタンドアロンマルチモダルアシスタントデバイスがディスプレイを備える、検出するステップと、その検出に基づいて、1人または複数の登録ユーザをターゲットにした複数のデータ項目を取得するステップであって、1人または複数の登録ユーザが、スタンドアロンマルチモダルアシスタントデバイス上で少なくとも部分的に動作する自動アシスタントにとってアクセス可能なアカウントを有し、複数のデータ項目が、登録ユーザのアカウントに基づいて取得される、取得するステップと、複数のデータ項目に関連する優先順位に基づいて、複数のデータ項目をランク付けするステップであって、優先順位が1人または複数の登録ユーザとは無関係である、ランク付けするステップと、ランク付けされた複数のデータ項目を示すグラフィカル情報をディスプレイ上にレンダリングするステップとを含む。 Some implementations provide a method performed by one or more processors, which is a step in detecting that an unidentified user coexists with a standalone multimodal assistant device, which is standalone. A multimodal assistant device with a display, a step of detection and a step of retrieving multiple data items targeting one or more registered users based on the detection, one or more registered users. Has an account accessible to an automated assistant that runs at least partially on a stand-alone multimodal assistant device, and multiple data items are retrieved based on the registered user's account, the steps to retrieve, and multiple. A ranking step that ranks multiple data items based on the priority associated with the data item, with a priority independent of one or more registered users. Includes steps to render graphical information on the display that indicates multiple data items.
本明細書で開示する技法のこれらのおよび他の実装形態は、以下の特徴のうちの1つまたは複数を随意に含み得る。 These and other implementations of the techniques disclosed herein may optionally include one or more of the following features:
様々な実装形態では、この方法は、検出に基づいて、ディスプレイを非アクティブ状態からアクティブ状態に遷移させるステップをさらに含み得る。様々な実装形態では、検出するステップは、未確認のユーザから自然言語入力を受信するステップを含む。様々な実装形態では、検出するステップは、存在センサーから信号を受信するステップを含み得る。様々な実装形態では、存在センサーは、カメラ、マイクロフォン、およびパッシブ赤外線センサーのうちの1つまたは複数を含み得る。 In various implementations, this method may further include a step of transitioning the display from an inactive state to an active state based on detection. In various implementations, the detecting step includes receiving natural language input from an unidentified user. In various implementations, the detecting step may include the step of receiving a signal from the presence sensor. In various implementations, the presence sensor may include one or more of a camera, a microphone, and a passive infrared sensor.
様々な実装形態では、この方法は、未確認のユーザの識別情報を判定するステップをさらに含み、識別情報に基づいて、取得されたデータ項目をランク付けするステップをさらに含むことができる。様々な実装形態では、ユーザの判定された識別情報は、取得されたデータ項目がランク付けされる方法に対して、1人または複数の登録ユーザとは無関係である優先順位よりも大きな影響を及ぼし得る。 In various implementations, the method may further include determining the identity of the unidentified user and further include ranking the acquired data items based on the identity. In various implementations, the user's determined identity has a greater impact on how the retrieved data items are ranked than priorities that are independent of one or more registered users. obtain.
様々な実装形態では、ランク付けされた複数のデータ項目を示す情報は、複数のグラフィカル要素を含んでよく、各グラフィカル要素は、複数のデータ項目のそれぞれのデータ項目を表す。様々な実装形態では、ディスプレイは、タッチスクリーンを含んでよく、複数のグラフィカル要素のうちの1つまたは複数は対話型である。 In various implementations, the information indicating a plurality of ranked data items may include a plurality of graphical elements, each graphical element representing a respective data item of the plurality of data items. In various implementations, the display may include a touch screen, one or more of the graphical elements being interactive.
別の態様では、方法は、ある人物がスタンドアロンマルチモダルアシスタントデバイスと併存していることを検出するステップであって、スタンドアロンマルチモダルアシスタントデバイスが、ディスプレイを備える、検出するステップと、スタンドアロンマルチモダルアシスタントデバイス上で少なくとも部分的に動作する自動アシスタントにとってアクセス可能なアカウントを有する、複数の登録ユーザの所与の登録ユーザとしてその人物を識別するステップと、その検出および識別に基づいて、複数の登録ユーザをターゲットにした複数のデータ項目を取得するステップであって、複数のデータ項目が、登録ユーザのアカウントに基づいて取得される、取得するステップと、所与の登録ユーザの識別情報に基づいて、複数のデータ項目をランク付けするステップと、ランク付けされた複数のデータ項目を示す情報をディスプレイ上にレンダリングするステップとを含み得る。 In another aspect, the method is a step of detecting that a person coexists with a stand-alone multimodal assistant device, wherein the stand-alone multimodal assistant device has a display, a step of detecting, and a stand-alone multimodal assistant. Multiple Registered Users Based on the steps to identify a person as a given Registered User of Multiple Registered Users, and their detection and identification, with an account accessible to an automated assistant running at least partially on the device. A step of retrieving a plurality of data items targeting the above, in which a plurality of data items are retrieved based on the registered user's account, based on the retrieving step and the identification information of a given registered user. It may include a step of ranking a plurality of data items and a step of rendering information indicating the plurality of ranked data items on a display.
様々な実装形態では、ランク付けされた複数のデータ項目を示す情報は、複数のグラフィカル要素を含んでよく、各グラフィカル要素は、複数のデータ項目のそれぞれのデータ項目を表す。様々な実装形態では、複数のグラフィカル要素は、複数のグラフィカルカードまたはグラフィカルタイルを含み得る。 In various implementations, the information indicating a plurality of ranked data items may include a plurality of graphical elements, each graphical element representing a respective data item of the plurality of data items. In various implementations, multiple graphical elements can include multiple graphical cards or graphical tiles.
様々な実装形態では、この方法は、所与の登録ユーザから入力を受信するステップであって、入力が、複数のグラフィカル要素の所与のグラフィカル要素に対して作用し、所与のグラフィカル要素が、複数の登録ユーザのうちの異なる登録ユーザをターゲットにした、ランク付けされた複数のデータ項目のうちのデータ項目に関連する、受信するステップと、その入力に基づいて、所与のグラフィカル要素をディスプレイから除去するステップと、別の人物がスタンドアロンマルチモダルアシスタントデバイスと併存していることをその後に検出するステップと、その別の人物を、異なる登録ユーザとして識別するステップと、所与のグラフィカル要素をディスプレイ上にレンダリングするステップとをさらに含み得る。 In various implementations, this method is the step of receiving input from a given registered user, where the input acts on a given graphical element of multiple graphical elements, and the given graphical element , A given graphical element based on the steps received and their inputs, related to the data item of the ranked data items, targeting different registered users of the multiple registered users. A step to remove from the display, a step to subsequently detect that another person coexists with the standalone multimodal assistant device, a step to identify that other person as a different registered user, and a given graphical element. Can further include the step of rendering on the display.
様々な実装形態では、ランク付けされた複数のデータ項目を示す情報は、自動アシスタントを用いて所与の登録ユーザによって開始された未完了のタスクを示すグラフィカル要素を含み得る。様々な実装形態では、グラフィカル要素は、自動アシスタントとの音声関与を介してタスクの完了を開始するために選択可能であり得る。様々な実装形態では、グラフィカル要素は、ディスプレイ上にレンダリングされたグラフィカルユーザインターフェースとの対話によってタスクの完了を開始するために選択可能であり得る。 In various implementations, information indicating a plurality of ranked data items may include graphical elements indicating incomplete tasks initiated by a given registered user using an automated assistant. In various implementations, graphical elements may be selectable to initiate task completion via voice involvement with an automated assistant. In various implementations, the graphical elements may be selectable to initiate the completion of the task by interacting with the graphical user interface rendered on the display.
加えて、いくつかの実装形態は、1つまたは複数のコンピューティングデバイスの1つまたは複数のプロセッサを含み、1つまたは複数のプロセッサは、関連するメモリ内に記憶された命令を実行するように動作可能であり、これらの命令は、前述の方法のうちのいずれかを実行させるように構成される。いくつかの実装形態は、前述の方法のうちのいずれかを実行するために1つまたは複数のプロセッサによって実行可能なコンピュータ命令を記憶した、1つまたは複数の非一時的コンピュータ可読記憶媒体をやはり含む。 In addition, some implementations include one or more processors of one or more computing devices so that one or more processors execute instructions stored in the associated memory. It is operational and these instructions are configured to perform any of the methods described above. Some embodiments also include one or more non-temporary computer-readable storage media that store computer instructions that can be executed by one or more processors to perform any of the methods described above. Including.
前述の概念、および本明細書でより詳細に説明する追加の概念のすべての組合せが本明細書で開示する主題の部分として企図されることを諒解されたい。たとえば、本開示の最後に出現する特許請求される主題のすべての組合せは、本明細書で開示する主題の部分として企図される。 It should be appreciated that all combinations of the above concepts, and any additional concepts described in more detail herein, are intended as part of the subject matter disclosed herein. For example, all combinations of claims appearing at the end of this disclosure are intended as part of the subject matter disclosed herein.
次に、図1を参照すると、本明細書で開示する技法が実装され得る例示的な環境を示す。この例示的な環境は、1つまたは複数のクライアントコンピューティングデバイス1061〜Nを含む。各クライアントデバイス106は、自動アシスタントクライアント118のそれぞれのインスタンスを実行し得る。自然言語プロセッサ122など、1つまたは複数のクラウドベースの自動アシスタント構成要素119は、概して110において示される、1つまたは複数のローカルおよび/またはワイドエリアネットワーク(たとえば、インターネット)を介してクライアントデバイス1061〜Nに通信可能に結合された、1つまたは複数のコンピューティングシステム(「クラウド」コンピューティングシステムと総称される)上で実装され得る。
Next, with reference to FIG. 1, an exemplary environment in which the techniques disclosed herein can be implemented is shown. This exemplary environment includes one or more client computing devices 106 1-N . Each client device 106 may run its own instance of the automatic
自動アシスタントクライアント118のインスタンスは、1つまたは複数のクラウドベースの自動アシスタント構成要素119とのその対話によって、ユーザの観点から、ユーザがそれを用いて人間対コンピュータ対話に関与し得る自動アシスタント120の論理インスタンスであるように見えるものを形成し得る。そのような自動アシスタント120の2つのインスタンスを図1に示す。破線で囲まれた第1の自動アシスタント120Aは、第1のクライアントデバイス1061を動作させている第1のユーザ(図示せず)にサービスし、自動アシスタントクライアント1181および1つまたは複数のクラウドベースの自動アシスタント構成要素119を含む。破線-破線-点で囲まれた第2の自動アシスタント120Bは、別のクライアントデバイス106Nを動作させている第2のユーザ(図示せず)にサービスし、自動アシスタントクライアント118Nおよび1つまたは複数のクラウドベースの自動アシスタント構成要素119を含む。したがって、いくつかの実装形態では、クライアントデバイス106上で実行している自動アシスタントクライアント118に関与する各ユーザは、事実上、自動アシスタント120の自らの独自の論理インスタンスに関与し得ることを理解されたい。簡単かつ簡潔にするために、本明細書で、特定のユーザに「サービス」するとして使用される「自動アシスタント」という用語は、ユーザが動作させるクライアントデバイス106上で実行している自動アシスタントクライアント118と(複数の自動アシスタントクライアント118の間で共有され得る)1つまたは複数のクラウドベースの自動アシスタント構成要素119との組合せを指す。いくつかの実装形態では、自動アシスタント120は、そのユーザが自動アシスタント120のその特定のインスタンスによって実際に「サービスされ」ているかどうかにかかわらず、任意のユーザからの要求に応答し得ることをやはり理解されたい。
An instance of the
クライアントデバイス1061〜Nは、たとえば、デスクトップコンピューティングデバイス、ラップトップコンピューティングデバイス、タブレットコンピューティングデバイス、モバイルフォンコンピューティングデバイス、ユーザの車両のコンピューティングデバイス(たとえば、車内通信システム、車内娯楽システム、車内ナビゲーションシステム)、スタンドアロン音声応答スピーカ、スタンドアロンマルチモダルアシスタントデバイス、スマートテレビジョンなどのスマートアプリケーション、および/またはコンピューティングデバイス(たとえば、コンピューティングデバイスを有するユーザのウォッチ、コンピューティングデバイスを有するユーザの眼鏡、仮想または拡張現実のコンピューティングデバイス)を含むユーザのウェアラブル装置のうちの1つまたは複数を含み得る。追加のおよび/または代替のクライアントコンピューティングデバイスが提供されてもよい。 Client devices 106 1-N are, for example, desktop computing devices, laptop computing devices, tablet computing devices, mobile phone computing devices, user's vehicle computing devices (eg, in-vehicle communication systems, in-vehicle entertainment systems, etc.). Smart applications such as in-car navigation systems), stand-alone voice response speakers, stand-alone multimodal assistant devices, smart television, and / or computing devices (eg, watches for users with computing devices, glasses for users with computing devices). Can include one or more of the user's wearable devices, including virtual or augmented reality computing devices). Additional and / or alternative client computing devices may be provided.
本開示のために、図1では、第1のクライアントデバイス1061は、それを介して自動アシスタント120Aが自然言語出力を提供し得るスピーカ1091を含むスタンドアロン音声応答スピーカである。上述のように、第1のクライアントデバイス1061は、音量および/または消音構成要素など、基本的なハードウェア入力構成要素のみを含んでよく、キーボードまたはマウスなど、より複雑なハードウェア入力構成要素は含まない。第2のクライアントデバイス106Nは、スピーカ109Nおよびディスプレイ111Nを備えたスタンドアロンマルチモダルアシスタントデバイスの形態をとり、(いくつかの実装形態ではタッチスクリーンであるディスプレイ111の他には)複雑な物理的入力構成要素がやはり欠如していることがある。本明細書で説明する技法について、106Nなどのスタンドアロンマルチモダルアシスタントデバイスを使用して実行される文脈で説明することになるが、これは、限定的であることを意味しない。本明細書で説明する技法は、主に音声交換によって対話することを意味する車両コンピューティングデバイスなど、他のフォームファクタ(ただし依然として標準的なキーボードおよびマウスが欠如した)を有するクライアントデバイス上で実装され得る。
For the purposes of the present disclosure, in FIG. 1, the first client device 106 1 is a stand-alone voice-answering speaker, including a speaker 109 1 through which the
本明細書でより詳細に説明するように、自動アシスタント120は、1つまたは複数のクライアントデバイス1061〜Nのユーザインターフェース入力デバイスおよびユーザインターフェース出力デバイスを介して1人または複数のユーザと人間対コンピュータダイアログセッションに関与する。クライアントデバイス106Nなどのスタンドアロンマルチモダルアシスタントデバイスの場合、これらの入力デバイスは、マイクロフォン(図示せず)およびディスプレイ111(ディスプレイ111がタッチスクリーンである実装形態において)、ならびに付近の人物の存在を検出するために使用され得る他のパッシブセンサー(たとえば、PIR、カメラ)に限定され得る。いくつかの実装形態では、自動アシスタント120は、クライアントデバイス1061〜Nのうちの1つの、1つまたは複数のユーザインターフェース入力デバイスを介してユーザによって提供されたユーザインターフェース入力に応答して、ユーザとの人間対コンピュータ対話セッションに関与し得る。それらの実装形態のいくつかでは、ユーザインターフェース入力は、明示的に自動アシスタント120を対象とする。たとえば、特定のユーザインターフェース入力は、ハードウェアボタンおよび/または仮想ボタン(たとえば、タップ、ロングタップ)、口頭コマンド(たとえば、「おい、自動アシスタント」)、および/または他の特定のユーザインターフェース入力を用いたユーザ対話であり得る。 As described in more detail herein, the automatic assistant 120 is human-to-human with one or more users via the user interface input and user interface output devices of one or more client devices 106 1-N. Participate in computer dialog sessions. For stand-alone multimodal assistant devices such as the client device 106 N , these input devices detect the presence of a microphone (not shown) and display 111 (in an implementation where display 111 is a touch screen), as well as nearby people. Can be limited to other passive sensors that can be used to (eg, PIR, camera). In some embodiments, the automatic assistant 120 responds to a user interface input provided by the user through one or more user interface input devices of one of the client devices 106 1-N. Can be involved in human-computer dialogue sessions with. In some of those implementations, user interface input explicitly targets the automatic assistant 120. For example, certain user interface inputs include hardware buttons and / or virtual buttons (for example, taps, long taps), verbal commands (for example, "Hey, Auto Assistant"), and / or other specific user interface inputs. It can be the user interaction used.
いくつかの実装形態では、自動アシスタント120は、ユーザがコマンド、検索などを発話することができるように、対話型音声応答(「IVR:interactive voice response」)に関与し得、自動アシスタントは、自然言語処理および/または1つまたは複数の文法を利用して、発話をテキストに変換し、それに応じて、そのテキストに応答することができる。いくつかの実装形態では、自動アシスタント120は、追加または代替として、発話をテキストに変換せずに発話に応答し得る。たとえば、自動アシスタント120は、音声入力を埋め込みに変換し、(音声入力内に存在する1つまたは複数のエンティティを示す)エンティティ表現、および/または他の「非テキスト」表現に変換し、そのような非テキスト表現に対して動作し得る。したがって、音声入力から変換されたテキストに基づいて動作するとして本明細書で説明する実装形態は、追加および/または代替として、直接的に音声入力に対して、かつ/または音声入力の他の非テキスト表現に対して動作し得る。 In some embodiments, the automatic assistant 120 may be involved in an interactive voice response (IVR) so that the user can speak commands, searches, etc., and the automatic assistant is natural. Language processing and / or one or more grammars can be used to translate utterances into text and respond to that text accordingly. In some implementations, the automatic assistant 120 may, as an addition or alternative, respond to the utterance without converting the utterance into text. For example, Auto Assistant 120 translates a voice input into an embedded, entity representation (indicating one or more entities present in the voice input), and / or other "non-text" representations, and so on. Can work for non-textual representations. Therefore, the implementations described herein as operating on the text converted from the voice input are, as additions and / or alternatives, directly to the voice input and / or other non-voice inputs. Can work for textual representations.
クライアントコンピューティングデバイス1061〜N、およびクラウドベースの自動アシスタント構成要素119を動作させているコンピューティングデバイスの各々は、データおよびソフトウェアアプリケーションを記憶するための1つまたは複数のメモリ、データにアクセスし、アプリケーションを実行するための1つまたは複数のプロセッサ、およびネットワークを介した通信を円滑にする他の構成要素を含み得る。クライアントコンピューティングデバイス1061〜Nのうちの1つまたは複数によって、かつ/または自動アシスタント120によって実行される動作は、複数のコンピュータシステムにわたって分散され得る。自動アシスタント120は、たとえば、ネットワークを介して互いに結合された1つまたは複数の場所内で1つまたは複数のコンピュータ上で実行しているコンピュータプログラムとして実装され得る。
Each of the client computing devices 106 1 to N , and the computing devices running the cloud-based auto-
上述のように、様々な実装形態では、クライアントコンピューティングデバイス1061-Nの各々は、自動アシスタントクライアント118を動作させることができる。様々な実装形態では、各自動アシスタントクライアント118は、対応する音声捕捉/テキスト対音声(「TTS:text-to-speech」)/STTモジュール114を含み得る。他の実装形態では、音声捕捉/TTS/STTモジュール114の1つまたは複数の態様は、自動アシスタントクライアント118とは別個に実装され得る。
As mentioned above, in various implementations, each of the client computing devices 106 1-N can operate the automatic
各音声捕捉/TTS/STTモジュール114は、1つまたは複数の機能を実行するように、たとえば、(場合によっては、存在センサー105を備え得る)マイクロフォンを介してユーザの音声を捕捉し、その捕捉されたオーディオをテキスト(および/または、他の表現または埋め込み)に変換し、かつ/またはテキストを音声に変換するように構成され得る。たとえば、いくつかの実装形態では、クライアントデバイス106は、コンピューティングリソース(たとえば、プロセッササイクル、メモリ、バッテリーなど)の点で比較的制約され得るため、各クライアントデバイス106に対して局所的な音声捕捉/TTS/STTモジュール114は、有限数の異なる音声フレーズ、特に、自動アシスタント120を起動させるフレーズをテキストに(または、より低い次元の埋め込みなど、他の形態に)変換するように構成され得る。他の音声入力は、クラウドベースのTTSモジュール116および/またはクラウドベースのSTTモジュール117を含み得るクラウドベースの自動アシスタント構成要素119に送られてよい。
Each voice capture / TTS / STT module 114 captures the user's voice and captures the user's voice, for example, through a microphone (which may include presence sensor 105) to perform one or more functions. It can be configured to convert audio to text (and / or other representations or embeds) and / or text to voice. For example, in some implementations, the client device 106 can be relatively constrained in terms of computing resources (eg, processor cycle, memory, battery, etc.), so local voice capture for each client device 106. The / TTS / STT module 114 may be configured to convert a finite number of different phonetic phrases, especially those that activate the automatic assistant 120, into text (or in other forms, such as lower dimension embedding). Other voice inputs may be sent to cloud-based auto-
クラウドベースのSTTモジュール117は、クラウドの実質的に無制限のリソースを活用して、音声捕捉/TTS/STTモジュール114によって補足されたオーディオデータを(次いで、自然言語プロセッサ122に提供され得る)テキストに変換するように構成され得る。クラウドベースのTTSモジュール116は、クラウドの実質的に無制限のリソースを活用して、テキストデータ(たとえば、自動アシスタント120によって編成された自然言語応答)をコンピュータ生成された音声出力に変換するように構成され得る。いくつかの実装形態では、TTSモジュール116は、たとえば、1つまたは複数のスピーカを使用して、直接出力されるように、コンピュータ生成された音声出力をクライアントデバイス106に提供し得る。他の実装形態では、自動アシスタント120によって生成されたテキストデータ(たとえば、自然言語応答)は、音声捕捉/TTS/STTモジュール114に提供され得、音声捕捉/TTS/STTモジュール114は、次いで、テキストデータを、局所的に出力される、コンピュータ生成された音声に変換し得る。
The cloud-based
自動アシスタント120(および、具体的には、クラウドベースの自動アシスタント構成要素119)は、自然言語プロセッサ122、前述のTTSモジュール116、前述のSTTモジュール117、ダイアログ状態トラッカー124、ダイアログマネージャ126、および自然言語生成器128(いくつかの実装形態では、TTSモジュール116と組み合わせられてよい)、ならびに本開示に特に関連するコンテンツ表示エンジン130を含み得る。いくつかの実装形態では、自動アシスタント120のエンジンおよび/またはモジュールのうちの1つまたは複数は、省略されてよく、組み合わされてよく、かつ/または自動アシスタント120とは別個である構成要素内で実装されてよい。
The Auto Assistant 120 (and specifically the cloud-based Auto Assistant component 119) includes a
いくつかの実装形態では、自動アシスタント120は、自動アシスタント120との人間対コンピュータ対話セッションの間に、クライアントデバイス1061〜Nのうちの1つのユーザによって生成された様々な入力に応答して応答コンテンツを生成する。自動アシスタント120は、ダイアログセッションの一部としてユーザに提示するための応答コンテンツを(たとえば、ユーザのクライアントデバイスとは別個であるとき、1つまたは複数のネットワークを介して)提供し得る。たとえば、自動アシスタント120は、クライアントデバイス1061-Nのうちの1つを介して提供された自由形式の自然言語入力に応答して、応答コンテンツを生成し得る。本明細書で使用される自由形式の入力は、ユーザによって編成され、ユーザによる選択のために提示されるオプションのグループに制約されない入力である。 In some implementations, Auto Assistant 120 responds to various inputs generated by one user of client devices 106 1 through N during a human-to-computer dialogue session with Auto Assistant 120. Generate content. The Auto Assistant 120 may provide response content (eg, over one or more networks when separate from the user's client device) for presentation to the user as part of a dialog session. For example, Auto Assistant 120 may generate response content in response to free-form natural language input provided via one of client devices 106 1-N. The free-form inputs used herein are inputs that are organized by the user and are not constrained by the group of options presented for selection by the user.
本明細書で使用される「ダイアログセッション」は、ユーザと自動アシスタント120との間の1つまたは複数のメッセージの論理的に自己完結型の交換、および/または自動アシスタント120による1つまたは複数の応答活動の履行を含み得る。自動アシスタント120は、セッション間の時間の経過、セッション間のユーザコンテキスト(たとえば、場所、スケジュールされた会議の前/最中/後、など)の変更、そのユーザと自動アシスタントとの間のダイアログ以外のユーザとクライアントデバイスとの間に介在する1つまたは複数の対話の検出(たとえば、ユーザがアプリケーションをしばらく切り替える、ユーザがスタンドアロン音声応答スピーカまたはスタンドアロンマルチモダルアシスタントデバイスから立ち去り、次いで後でそこに戻る)、セッション間のクライアントデバイスのロック/スリープ、自動アシスタント120の1つまたは複数のインスタンスと対話するために使用されるクライアントデバイスの変更など、様々な信号に基づいてユーザとの複数のダイアログセッションを区別し得る。 As used herein, a "dialog session" is a logically self-contained exchange of one or more messages between a user and the Auto Assistant 120, and / or one or more by the Auto Assistant 120. It may include the performance of response activities. The Auto Assistant 120 does not include the passage of time between sessions, changes in user context between sessions (eg, location, before / during / after a scheduled meeting, etc.), and dialogs between that user and the Auto Assistant. Detection of one or more interactions between the user and the client device (for example, the user switches applications for a while, the user leaves the stand-alone voice-answering speaker or stand-alone multimodal assistant device, and then returns there later. ), Locking / sleeping client devices between sessions, changing client devices used to interact with one or more instances of Auto Assistant 120, and multiple dialog sessions with users based on various signals. Can be distinguished.
自動アシスタント120の自然言語プロセッサ122(代替として「自然言語理解エンジン」と呼ばれる)は、クライアントデバイス1061〜Nを介してユーザによって生成された自由形式の自然言語入力を処理し、いくつかの実装形態では、自動アシスタント120の1つまたは複数の他の構成要素による使用のために注釈付き出力を生成し得る。たとえば、自然言語プロセッサ122は、クライアントデバイス1061の1つまたは複数のユーザインターフェース入力デバイスを介してユーザによって生成された自由形式の自然言語入力を処理することができる。生成された注釈付き出力は、自然言語入力の1つまたは複数の注釈、および随意に自然言語入力の用語のうちの1つまたは複数(たとえば、すべて)を含み得る。
The Automatic Assistant 120's Natural Language Translator 122 (alternatively called the "Natural Language Understanding Engine") processes free-form natural language input generated by the user via client devices 106 1-N and implements several. In the form, it may generate annotated output for use by one or more other components of the Automatic Assistant 120. For example, the
いくつかの実装形態では、自然言語プロセッサ122は、自然言語入力内の様々なタイプの文法情報を識別して注釈を付けるように構成される。たとえば、自然言語プロセッサ122は、その文法上の役割で用語に注釈を付けるように構成された品詞タガー(tagger)(図示せず)を含み得る。たとえば、品詞タガーは、「名詞」、「動詞」、「形容詞」、「代名詞」などの品詞で各用語にタグ付けすることができる。また、たとえば、いくつかの実装形態では、自然言語プロセッサ122は、追加および/または代替として、自然言語入力内の用語間の構文関係を判定するように構成された係り受け解析装置(dependency parser)(図示せず)を含み得る。たとえば、係り受け解析装置は、どの用語が他の用語を修飾するか、文の主語および動詞、などを判定することができ(たとえば、解析木)、そのような係り受けに注釈を付けることができる。
In some implementations, the
いくつかの実装形態では、自然言語プロセッサ122は、追加および/または代替として、(たとえば、文学の登場人物、著名人、公人などを含む)人々、組織、場所(現実および架空)などの参照など、1つまたは複数のセグメント内のエンティティ参照に注釈を付けるように構成されたエンティティタガー(図示せず)を含み得る。いくつかの実装形態では、エンティティに関するデータは、知識グラフ(図示せず)など、1つまたは複数のデータベース内に記憶され得る。いくつかの実装形態では、知識グラフは、周知のエンティティ(および、場合によっては、エンティティ属性)を表すノード、ならびにノードを接続し、エンティティ間の関係を表すエッジを含み得る。たとえば、「バナナ」ノードは、「フルーツ」ノードに(たとえば、子として)接続され得、「フルーツ」ノードは、次に、「農産物」および/または「食料」ノードに(たとえば、子として)接続され得る。別の例として、「Hypothetical Cafe」と呼ばれるレストランは、その住所、提供される食品のタイプ、時間、連絡情報などの属性をやはり含むノードによって表され得る。「Hypothetical Cafe」ノードは、いくつかの実装形態では、エッジによって、「レストラン」ノード、「ビジネス」ノード、そのレストランが位置する街および/または州を表すノードなど、(たとえば、子対親の関係を表す)1つまたは複数の他のノードに接続され得る。
In some implementations, the
自然言語プロセッサ122のエンティティタガーは、(たとえば、人々など、エンティティクラスのすべての参照の識別を可能にするために)高いレベルの粒度で、かつ/または(たとえば、特定の人物など、特定のエンティティのすべての参照の識別を可能にするために)低いレベルの粒度でエンティティの参照に注釈を付けることができる。エンティティタガーは、特定のエンティティを転換する(resolve)ために自然言語入力のコンテンツに依存し得、かつ/または特定のエンティティを転換するために知識グラフまたは他のエンティティデータベースと随意に通信し得る。
The entity tagger of the
いくつかの実装形態では、自然言語プロセッサ122は、追加および/または代替として、1つまたは複数の文脈上の手掛かりに基づいて、同じエンティティの参照をグループ化するか、または「クラスタ化」するように構成された相互参照レゾルバ(coreference resolver)(図示せず)を含み得る。たとえば、相互参照レゾルバを利用して、自然言語入力「私たちが前回あそこで食事をとったときHypothetical Cafeが気に入りました」において「あそこ」という用語を「Hypothetical Cafe」に転換することができる。
In some implementations, the
いくつかの実装形態では、自然言語プロセッサ122の1つまたは複数の構成要素は、自然言語プロセッサ122の1つまたは複数の他の構成要素からの注釈に依存し得る。たとえば、いくつかの実装形態では、指定されたエンティティタガーは、特定のエンティティのすべての言及に注釈を付ける際に、相互参照レゾルバおよび/または係り受け解析器からの注釈に依存し得る。また、たとえば、いくつかの実装形態では、相互参照レゾルバは、同じエンティティの参照をクラスタ化する際に、係り受け解析器からの注釈に依存し得る。いくつかの実装形態では、特定の自然言語入力を処理する際に、自然言語プロセッサ122の1つまたは複数の構成要素は、関連する前の入力および/または特定の自然言語入力外の他の関係データを使用して、1つまたは複数の注釈を判定し得る。
In some implementations, one or more components of the
いくつかの実装形態では、ダイアログ状態トラッカー124は、たとえば、人間対コンピュータ対話セッションの過程にわたる、複数のダイアログセッションにわたる、かつ/または会議ダイアログセッションの間の、1つまたは複数のユーザの目標(または「意図」)の信用状態を含む「ダイアログ状態」を追跡するように構成され得る。ダイアログ状態を判定する際に、いくつかのダイアログ状態トラッカーは、ダイアログセッション内のユーザ発話およびシステム発話に基づいて、ダイアログ内でインスタンス化されたスロットに対して最も可能性が高い値を判定することを求めることができる。いくつかの技法は、スロットのセットおよびそれらのスロットに関連する値のセットを定義する固定されたオントロジーを利用する。いくつかの技法は、追加または代替として、個々のスロットおよび/または領域に合うように適合され得る。たとえば、いくつかの技法は、各領域内の各スロットタイプに対してモデルをトレーニングすることを要求し得る。
In some implementations, the
ダイアログマネージャ126は、たとえば、ダイアログ状態トラッカー124によって提供された現在のダイアログ状態を、複数の候補応答活動の1つまたは複数の「応答活動」にマッピングするように構成され得、次いで、それらの応答活動が自動アシスタント120によって実行される。応答活動は、現在のダイアログ状態に応じて、様々な形態で出現し得る。たとえば、最後の順番(たとえば、最終的なユーザ所望のタスクが実行されるとき)に先立って発生するダイアログセッションの順番に対応する初期および中間のダイアログ状態は、自動アシスタント120が追加の自然言語ダイアログを出力することを含めて、様々な応答活動にマッピングされ得る。この応答ダイアログは、たとえば、ユーザは、そのユーザが実行することを意図するとダイアログ状態トラッカー124が確信するある活動(たとえば、スロットを満たすこと)に関するパラメータを提供するという要求を含み得る。いくつかの実装形態では、応答活動は、「要求する」(たとえば、スロットを満たすためのパラメータを求める)、「提供する」(たとえば、活動または活動の過程をユーザに示唆する)、「選択する」、「知らせる」(たとえば、ユーザに要求された情報を提供する)、「一致なし」(たとえば、ユーザの最後の入力が理解されなかったことをユーザに通知する)などの活動を含み得る。
Dialog manager 126 may be configured, for example, to map the current dialog state provided by
様々な実装形態では、コンテンツ表示エンジン130は、スタンドアロンマルチモダルアシスタントデバイス106Nなど、ディスプレイ111を備えたクライアントデバイス106を使用してレンダリングするためのコンテンツを選択するように構成され得る。コンテンツ表示エンジン130は、汎用コンテンツ、および本明細書で「ターゲットコンテンツ」と呼ばれるもののうちの1つまたは両方の表示を選択し得る。汎用コンテンツは、特定の人物をターゲットにしないデータ項目を表すグラフィカル要素(たとえば、テキスト、タイル、動画、カードなど)を含み得る。汎用コンテンツは、天候関連情報、一般的なニュース記事、ジョーク、雑項目などのようなものを含み得る。
In various implementations, the
ターゲットコンテンツは、対照的に、1人または複数の特定の人々を対象とする。たとえば、図1では、ユーザアカウントエンジン132は、たとえば、1人または複数の所謂「登録ユーザ」によって制御される、ユーザプロファイルインデックス134またはその他の中に記憶されたコンテンツに対するアクセスを自動アシスタント120に提供するように構成され得る。登録ユーザは、たとえば、その登録ユーザの制御に関連する、かつ/またはさもなければ、その登録ユーザの制御下にある情報を含む、インデックス134内に記憶されたオンラインプロファイルを有し得る。これは、登録ユーザのオンラインカレンダー、登録ユーザの電子メール、登録ユーザのソーシャルメディアアカウントおよび/または活動、登録ユーザのテキストメッセージ、登録ユーザの選好、登録ユーザの関心、登録ユーザによって作成されたかつ/または登録ユーザによって少なくとも部分的に制御された文書、登録ユーザによって制御されたスマートアプライアンス(たとえば、照明、ロック、サーモスタット)、登録ユーザに関連するタスク/リマインダ、登録ユーザがアクセス可能なメディアライブラリ、登録ユーザと自動アシスタント120との間の過去のダイアログセッション(たとえば、記録、論じた話題、会話の文脈など)を示すデータ、(たとえば、登録ユーザが動作させる1つまたは複数のデバイスの位置座標センサーによって生成された)登録ユーザの現在のかつ/または過去の場所など、幅広い情報を含み得る。登録ユーザに関連するすべての情報をインデックス134内に記憶する必要はない。この情報は、追加または代替として、他の場所に記憶されてよい。自動アシスタント120がユーザ制御されたリソースにアクセスできるように、そのアカウントが自動アシスタント120に「登録される」という意味で、ユーザのアカウントは「登録され」得る。
Targeted content, in contrast, targets one or more specific people. For example, in FIG. 1, the
様々な実装形態では、コンテンツ表示エンジン130は、登録ユーザのアカウントに関連する(すなわち、彼らのオンラインプロファイルに関連する)情報にアクセスする(たとえば、そこから、プルする、プッシュされる)ように構成され得る。たとえば、コンテンツ表示エンジン130は、登録ユーザのオンラインカレンダーからの次回のイベント、登録ユーザのリマインダリストからのリマインダ、登録ユーザの買い物リスト、登録ユーザによる過去の媒体消費(たとえば、聴いた歌、観たビデオなど)、登録ユーザによって作成されたかつ/または登録ユーザに関するソーシャルメディア掲示などのデータ項目を取得し得る。これらのデータ項目は、たとえば、スタンドアロンマルチモダルアシスタントデバイス106Nのディスプレイ111上に表示され得るターゲットグラフィカル要素を生成するために、たとえば、コンテンツ表示エンジン130によって使用され得る。コンテンツ表示エンジン130はクラウドベースの自動アシスタント構成要素119の一部として示されるが、これは限定的であることを意味しない。様々な実装形態では、コンテンツ表示エンジン130は、1つまたは複数のクライアントデバイス106上など、異なるコンピューティングシステム上で全体的にまたは一部実装され得る。
In various implementations, the
様々な実装形態では、スタンドアロンマルチモダルアシスタントデバイス106Nのディスプレイ111は、付近の人物の検出に応答して、非アクティブ状態(たとえば、わずかな電力を使用するか、または電力を全く使用しない、スリープ)からアクティブ状態(たとえば、コンテンツのレンダリング)に遷移し得る。スタンドアロンマルチモダルアシスタントデバイス106Nは、PIRセンサー、マイクロフォン(雑音を検出するため)、カメラ、人物が携行するデバイスによって放出された信号など、付近の人物を検出するために様々なセンサーに依存し得る。 In various implementations, the display 111 of the standalone multimodal assistant device 106 N sleeps in an inactive state (eg, using little power or no power at all) in response to detection of nearby people. ) Can transition to the active state (for example, rendering content). The stand-alone multimodal assistant device 106 N can rely on various sensors to detect nearby people, such as PIR sensors, microphones (to detect noise), cameras, and signals emitted by people-carrying devices. ..
図2は、様々な実装形態による、たとえば、ディスプレイ111および/またはコンテンツ表示エンジン130によって実装され得る1つの例示的な状態図を示す。状態図は、4つの状態、すなわち、「表示オフ」、「周囲(AMBIENT)」、「ホーム」、および「活動要求」を含む。「表示オフ」は、ディスプレイ111が、たとえば、わずかな電力を使用するか、または電力を全く使用しないスリープ状態に留まるデフォルト状態であり得る。スタンドアロンマルチモダルアシスタントデバイス106Nが、付近に人々がいない状態で孤立した状態に留まる間、「表示オフ」は現在の状態に留まることができる。いくつかの実装形態では、現在の状態が「表示オフ」である間、ユーザ(存在するものとしてまだ検出されていない)は、たとえば、呼出しフレーズに続き、現在の状態を「活動要求」状態に直接遷移し得る特定の要求を話すことによって、依然として自動アシスタント120に活動を要求し得る。
FIG. 2 shows one exemplary phase diagram that can be implemented by various implementations, eg, display 111 and / or
いくつかの実装形態では、1人または複数の人物が付近に検出されたとき(すなわち、「占有」)、現在の状態は「周囲」状態に遷移し得る。「周囲」状態で、コンテンツ表示エンジン130は、たとえば、その美的な魅力に基づいて選択され得る周囲コンテンツを表示し得る。たとえば、コンテンツ表示エンジン130は、従来のコンピューティングデバイス上にスクリーンセーバとして表示されることが多い風景または他の類似のコンテンツの1つまたは複数のデジタル画像および/またはビデオを表示し得る。いくつかの実装形態では、占有が、たとえば、少なくとも所定の時間期間にわたって、スタンドアロンマルチモダルアシスタントデバイスともはや併存していないと判定された場合、現在の状態は、「周囲」から「表示オフ」に再び遷移し得る。図2に示すように、いくつかの実装形態では、現在の状態が「周囲」である場合、ユーザは、たとえば、呼出しフレーズに続き、現在の状態を「活動要求」状態に遷移し得る特定の要求を話すことによって、依然として、自動アシスタント120から活動を要求することができる。他の実装形態では、「周囲」状態が存在しないことがあり、人物の併存(占有)の検出に応答して、現在の状態は「表示オフ」から「ホーム」に直接遷移し得る。
In some implementations, the current state can transition to the "surrounding" state when one or more persons are detected in the vicinity (ie, "occupied"). In the "surroundings" state, the
「ホーム」状態で、コンテンツ表示エンジン130は、上記で説明した汎用および/またはターゲットデータ項目を表す様々なグラフィカル要素を表示し得る。いくつかの実装形態では、データ項目は、(たとえば、ディスプレイ111がタッチスクリーンであるか否かに応じて)対話型であってもまたはそうでなくてもよい、カードまたはタイルとして表示され得る。先に述べたように、データ項目は、場合によっては、データ項目に(自動的にまたは手動で)割り当てられた優先順位、(判定された場合)併存している人物の識別情報、時刻、時期など、様々な基準に基づいてランク付けされ得る。データ項目がカードとして、たとえば、スタック内に提示されるとき、たとえば、下位のカードが比較的低い優先順位を有する状態で、一番上のカードが最高の優先順位であることにより、ランク付けが反映され得る。たとえば、ディスプレイ111の一部分を占有するタイルとしてデータ項目が提示されるとき、たとえば、タイルの配置(たとえば、左上または右上が最高優先順位であり得る)および/またはタイルのサイズ(たとえば、タイルが大きければ大きいほど、優先順位が高まる)でランク付けが反映され得る。
In the "home" state, the
「ホーム」状態の間、ユーザが、たとえば、タイルまたはカードをタップすることによってデータ項目を表すグラフィカル要素のうちの1つまたは複数に関与した場合、現在の状態は、「活動要求」状態に遷移し得る。同様に、ユーザが自動アシスタント120に対して音声要求を発話した場合(たとえば、「OK、アシスタント、…って何ですか?」)、現在の状態は、「活動要求」状態に遷移し得る。いくつかの実装形態では、併存しているユーザが、少なくとも所定の時間間隔にわたって、自動アシスタント120と音声で関与せず、ディスプレイ111上にレンダリングされたデータ項目と対話してもいない場合(すなわち、「タイムアウト」)、現在の状態は、「ホーム」から「周囲」に再度遷移することが可能であり、または「周囲」状態が存在しない場合には、「表示オフ」に遷移することすら可能である。「ホーム」状態から「周囲」(または、「表示オフ」)状態への遷移をトリガし得る他のイベントは、ユーザからの特定の要求(たとえば、ディスプレイ上の終了ボタンをタップすること)、併存しているユーザが「周囲」に再度遷移する意図をシグナリングし得る、戻るジェスチャー(たとえば、カメラまたは他のセンサーの前で手を振る)からの特定の要求などを含むが、これらに限定されない。 During the "home" state, if the user is involved in one or more of the graphical elements that represent a data item, for example by tapping a tile or card, the current state transitions to the "activity request" state. Can be done. Similarly, if the user utters a voice request to the automatic assistant 120 (eg, "OK, assistant, ... what?"), The current state can transition to the "activity request" state. In some implementations, co-existing users are not voice-engaged with the automatic assistant 120 and interact with data items rendered on display 111 (ie, for at least a predetermined time interval). "Timeout"), the current state can transition from "home" to "surroundings" again, or even transition to "display off" if the "surroundings" state does not exist. is there. Other events that can trigger a transition from the "home" state to the "surrounding" (or "display off") state coexist with a particular request from the user (for example, tapping the end button on the display). It includes, but is not limited to, specific requests from back gestures (eg, waving in front of a camera or other sensor) that may signal the intent of the user to transition back to "surroundings".
「活動要求」状態において、いくつかの実施形態では、要求された活動またはタスクに関するコンテンツが、たとえば、ディスプレイ全体にわたって、または新しいデータ項目のカードまたはタイルとして、ディスプレイ111上にレンダリングされ得る。たとえば、併存しているユーザがキッチンタイマーを5分に設定させる音声要求を発話したと仮定する。いくつかの実装形態では、ディスプレイ全体、またはその一部分(たとえば、カードまたはタイル)は、キッチンタイマーにどの程度の時間が残っているかを表示し得る。併存しているユーザが著名人に関する情報に対して音声要求を発話したと仮定する。いくつかの実装形態では、応答コンテンツは、自動アシスタント120による自然言語出力として音声で提供され得、かつ/またはディスプレイ上にレンダリングされ得る。いくつかの実装形態では、自動アシスタント120が応答コンテンツを可聴的に提供すると同時に、ユーザの要求または応答コンテンツに関する他のコンテンツ(ただし、必ずしもユーザによって具体的に要求されるとは限らない)が表示され得る。たとえば、ユーザがその著名人の誕生日について尋ねた場合、その著名人の誕生日が可聴的に出力され得ると同時に、その著名人に関する他の情報(たとえばその著名人が主演する映画の上映時間へのディープリンク、その著名人の写真など)がディスプレイ111上にレンダリングされ得る。他の実装形態では、ディスプレイ111は、(実際には「活動要求」状態を有さない)「活動要求」状態にある間、「ホーム」状態から変更されない状態に留まることができ、ユーザは、自動アシスタント120から可聴応答のみを受信することができる。 In the "Activity Request" state, in some embodiments, content relating to the requested activity or task may be rendered on the display 111, for example, across the display or as a card or tile of a new data item. For example, suppose a co-existing user utters a voice request that causes the kitchen timer to be set to 5 minutes. In some implementations, the entire display, or a portion thereof (eg, a card or tile), may indicate how much time is left on the kitchen timer. Suppose a co-existing user utters a voice request for information about a celebrity. In some implementations, the response content may be provided as audio as natural language output by the Auto Assistant 120 and / or rendered on the display. In some implementations, the Auto Assistant 120 provides audible response content while displaying other content related to the user's request or response content (although not necessarily specifically requested by the user). Can be done. For example, if a user asks about a celebrity's birthday, the celebrity's birthday can be audibly output while other information about the celebrity (eg, the show time of a movie starring the celebrity). A deep link to, a photo of that celebrity, etc.) can be rendered on display 111. In other implementations, the display 111 can remain unchanged from the "home" state while in the "activity request" state (which does not actually have the "activity request" state). Only audible responses can be received from Auto Assistant 120.
現在の状態は、様々なイベントに応答して、「活動要求」状態から「ホーム」状態に(または、「周囲」状態もしくは「表示オフ」状態にすら)遷移し得る。たとえば、要求された活動は完了し得る(たとえば、キッチンタイマーは取り消され得るか、またはキッチンタイマーが経過し、併存しているユーザによって無音化され得る)。そのような遷移をトリガし得る他のイベントは、タイムアウト、戻るジェスチャー、「ホーム」状態に戻る特定の要求などを含む。 The current state can transition from an "activity request" state to a "home" state (or even a "surrounding" state or even a "display off" state) in response to various events. For example, the requested activity can be completed (for example, the kitchen timer can be canceled or the kitchen timer has expired and can be muted by co-existing users). Other events that can trigger such a transition include timeouts, return gestures, specific requests to return to the "home" state, and so on.
いくつかの実装形態では、自動アシスタント120によって実行される活動および/またはタスクは、完了しない場合があり、かつ/または保留状態に留まる場合がある。たとえば、ユーザは、歌またはビデオを途中で休止することがある。別の例として、ユーザは、いくつかのスロットが活動パラメータで満たされることを要求するタスクの要求を開始することが可能であったが、要求されたスロットのすべてを満たすことができない可能性がある。たとえば、ユーザはピザを注文し始めることが可能であったが、中止して、部屋を去り、何のトッピングを望むかを他の人に尋ねるか、または他の人々から支払情報を要求することがある。十分な時間が経過した場合、タイムアウトが発生し得、現在の状態は、「活動要求」状態または「ホーム」状態から「周囲」状態または「表示オフ」状態に遷移し得る。 In some implementations, activities and / or tasks performed by Auto Assistant 120 may not complete and / or may remain on hold. For example, a user may pause a song or video in the middle. As another example, the user could initiate a request for a task that requires some slots to be filled with activity parameters, but may not be able to fill all of the requested slots. is there. For example, a user could start ordering pizza, but stop, leave the room, ask others what toppings they want, or request payment information from others. There is. If sufficient time has passed, a timeout can occur and the current state can transition from the "activity request" or "home" state to the "surrounding" or "display off" state.
様々な実装形態では、未完了のタスクを表すターゲットデータ項目を生成することができ、そのタスクを完了するための対話型の対応するグラフィカル要素がレンダリングされ得る。たとえば、ユーザがピザ注文の完了に戻るとき、いくつかの実装形態では、未完了の注文を表す新しいタイルまたはカードがディスプレイ111上にレンダリングされ得る。場合によっては、たとえば、自動アシスタント120が満たされていないスロット値(たとえば、ピザのトッピング、支払情報など)をユーザから音声で要求することで、注文プロセスを継続するために、ユーザはこの新しいタイルまたはカードをタップし得る。いくつかの実装形態では、別のユーザが室内に入り、データ項目を表すグラフィカル要素がそのユーザに提示された場合に、未完了のタスクデータ項目を表すグラフィカル要素が提示されても提示されなくてもよいように、未完了のタスクを表すこの新しいデータ項目は、要求側のユーザをターゲットにし得る。 In various implementations, target data items can be generated to represent unfinished tasks, and corresponding interactive graphical elements can be rendered to complete the tasks. For example, when the user returns to the completion of a pizza order, in some implementations a new tile or card representing an incomplete order may be rendered on display 111. In some cases, the user can use this new tile to continue the ordering process, for example, by voice requesting an unfilled slot value from the user for the automatic assistant 120 (for example, pizza toppings, payment information, etc.). Or you can tap the card. In some implementations, when another user enters the room and a graphical element representing a data item is presented to that user, the graphical element representing an incomplete task data item is presented but not presented. As you can see, this new data item, which represents an incomplete task, can target the requesting user.
図3は、ユーザ302が付近に検出されたとき、ディスプレイ311を備えたスタンドアロンマルチモダルアシスタントデバイス306がどのように動作し得るかの一例を示す。この例では、ユーザ302は、たとえば、PIRセンサー、マイクロフォン、カメラなど、存在センサー(図示せず)を使用して、アシスタントデバイス306によって単に検出されている。この例では、スタンドアロンマルチモダルアシスタントデバイス306は、ユーザの占有の検出に応答して、前に説明した「表示オフ」状態から「周囲」状態に遷移した。したがって、ディスプレイ311上にレンダリングされるコンテンツは、時間(4:15)、日付(日曜、7月13日)、および外部温度(52度)など、汎用コンテンツを含む。また、ディスプレイ311上には、山の景色の画像もレンダリングされる。上述のように、従来のスクリーンセーバと同様に、スタンドアロンマルチモダルアシスタントデバイス306が「周囲」状態にある間に、様々な画像および/または動画がディスプレイ311上にレンダリングされ得る。
FIG. 3 shows an example of how the standalone multimodal
図4は、たとえば、ユーザ302の特定の識別情報が判定されるのに先立って「ホーム」状態にあるとき、図3のスタンドアロンマルチモダルアシスタントデバイス306がどのように動作し得るかの一例を示す。いくつかの実装形態では、「周囲」状態が存在しない場合があり、スタンドアロンマルチモダルアシスタントデバイス306は、ユーザ302の存在の検出に応答して、図4に示す「ホーム」状態に直接遷移し得る。この例では、カードおよび/またはタイルとして、3つのグラフィカル要素450が示されている。第1のグラフィカル要素450Aは、買い物リストを含む。第1のグラフィカル要素450Aの基礎をなすデータ項目は、ターゲットデータであり得、場合によっては、複数のユーザをターゲットにし得る。たとえば、家族は、家族のいずれかの構成員が編集する(たとえば、商品を追加する、商品を除去する)ことができる、共有された買い物リストを保持することがある。
FIG. 4 shows, for example, an example of how the standalone multimodal
第2のグラフィカル要素450Bは、Thadという名の登録ユーザをターゲットにした歯科医の予約を含む。第3のグラフィカル要素450Cは、Joannaという名のユーザをターゲットにした美容院の予約を含む。ユーザ302の識別情報はまだ判定されていないため、グラフィカル要素450A〜Cは、ユーザ302の識別情報に基づいてランク付けされていない。代わりに、それらをランク付けするために、他の信号が使用され得る。たとえば、Thadの歯科医の予約は、Joannaの美容院の予約よりも早く発生するため、Joannaの美容院の予約よりも高くランク付けされる(したがって、それより上位にレンダリングされる)。
The second
次に、スタンドアロンマルチモダルアシスタントデバイス306はユーザ302がThadであると判定することができると仮定する。たとえば、ユーザ302は、ユーザ302がThadであると判定するために(たとえば、インデックス134内に記憶された)登録ユーザの音声プロファイルに対して照合される(スタンドアロンマルチモダルアシスタントデバイスまたは他のものに向けて)発話を行うことができる。追加または代替として、Thad(302)は、スタンドアロンマルチモダルアシスタントデバイス306によって検出される信号(たとえば、Wi-Fi、Bluetooth、RFIDなど)を放出するスマートフォンまたはスマートウォッチなど、モバイルデバイスを動作させている可能性がある。どちらの事例であっても、図4のグラフィカル要素450は、同様の配置で依然としてレンダリングされ得るが、これは、Thadの予約が最短にスケジュールされているため、既にThadの予約に最高優先順位が与えられているためである。しかしながら、いくつかの実装形態では、Thadは併存しているユーザであることが知られているため、「Thad」と言う名前を含むグラフィカル要素450Bの代わりに、「あなた」という用語を代わりに含んでもよい(これは図6に示されている)。
Next, it is assumed that the standalone multimodal
図5は、異なるユーザ303が存在するとして検出され、Joannaとして識別されるとき、図3〜図4のスタンドアロンマルチモダルアシスタントデバイス306が「ホーム」状態でどのように動作し得るかの一例を示す。同じグラフィカル要素450A〜Cが示されている。しかしながら、それらはここでは異なってランク付けされている。具体的には、グラフィカル要素450Cによって表されるJoannaの美容院の予約は、グラフィカル要素450Bによって表されるThadの歯科医の予約より後に発生するが、Joannaは併存しているユーザとして検出されているため、グラフィカル要素450Cは、ここでは、より高くランク付けされている。言い換えれば、Joannaの検出された識別情報は、グラフィカル要素450Bおよび450Cの基礎となる予約に関連する相対的な時間的緊急性よりも大きな影響をランク付けに及ぼす。
FIG. 5 shows an example of how the standalone multimodal
次に、図4において、Thadがグラフィカル要素450C(Joannaの美容院の予約)をスワイプしたと仮定する。それにより、Thadの存在下では図4のグラフィカル要素450Cをもはやレンダリングさせないことになる。場合によっては、異なるグラフィカル要素(図示せず)がそれに置き替わった可能性がある。図5に戻ると、Joannaがスタンドアロンマルチモダルアシスタントデバイス306と併存していると検出されたとき、Thadが前にスワイプしたにもかかわらず、グラフィカル要素450Cがレンダリングされ得る。これは、グラフィカル要素450Cの基礎となるデータ項目が、Thadではなく、Joannaをターゲットにしているためである。したがって、Thadがグラフィカル要素450Cをスワイプすることは、Joannaがその要素をスワイプするまで(または、予約がもはや関連しない十分な時間が経過するまで)、Thadが併存しているとして検出されるときはいつでもレンダリングされることを防ぐことができるが、Joannaが併存しているとして検出されるときには、グラフィカル要素450Cは、ディスプレイ311上に引き続きレンダリングされることになる。
Next, in Figure 4, assume that Thad swipes the
図6では、同じスタンドアロンマルチモダルデバイス306は、併存しているユーザ302をThadとして認識し、現在「ホーム」状態にある。この例では、ピザ注文のタスクを開始するために、スタンドアロンマルチモダルアシスタントデバイス306、または自動アシスタントインターフェース(118)を提供する(たとえば、スタンドアロンマルチモダルアシスタントデバイス306を含むクライアントデバイスの協調エコシステムの)別のクライアントデバイスのいずれかを使用して、Thadが前に自動アシスタント120に関与したと仮定する。たとえば、Thadは何のトッピングを好むかを様々な家族の構成員に尋ねる必要があったため、Thadは注文を完了しなかったとさらに仮定する。図6では、この未完了のタスクは、満たされているスロット(たとえば、クラストスタイル=厚い、サイズ=ラージ)および満たされていないスロット値(トッピング=???)など、未完了のタスクに関する情報を含むグラフィカル要素450Dとして提示されている。様々な実装形態では、Thad(302)は、グラフィカル要素450D上をタップして、タスクの完了を開始することができる。いくつかの実装形態では、これは、自動アシスタント120が何らかの見逃したスロット値を音声認識可能に要求することができることを含み得る。追加または代替として、いくつかの実装形態では、Thadは、ディスプレイ311と対話して、タスクを完了することができる。やはり図6に示すのは、新しいグラフィカル要素450Eである。Thadをターゲットにし得るグラフィカル要素450Eは、ThadのソーシャルメディアアカウントにプッシュされたThadの友人によって掲載されたソーシャルメディア更新の形態でデータ項目を表している。
In FIG. 6, the same standalone
図7は、本明細書で開示する実装形態による例示的な方法700を示すフローチャートである。便宜上、フローチャートの動作について、それらの動作を実行するシステムを参照しながら説明する。このシステムは、自動アシスタント120を実装するコンピューティングシステムの、かつ/またはクライアントデバイス(たとえば、106、306)の1つまたは複数の構成要素など、様々なコンピュータシステムの様々な構成要素を含み得る。その上、方法700の動作は特定の順序で示されているが、これは限定的であることを意味しない。1つまたは複数の動作は、並べ替えられてよく、省かれてよく、または追加されてもよい。
FIG. 7 is a flowchart showing an
ブロック702において、システムは、占有の監視を実行し得る。たとえば、システムは、PIRセンサー、カメラ、マイクロフォンなどのセンサーからの信号を監視し得る。信号に対する潜在的に急激な変化に基づいて、かつ/または様々な従来の存在検出技法を使用して、ブロック704において、システムは、占有を検出する場合もまたは検出しない場合もある。たとえば、いくつかの実装形態では、存在センサーは、連続的であることに加えて、またはその代わりに、たとえば、エネルギー、コンピューティングリソースなどを節約するために、周期的に動作し得る。各センサー動作の間、占有が検出されない場合、方法700はブロック702に戻り得る。
At block 702, the system may perform occupancy monitoring. For example, the system may monitor signals from sensors such as PIR sensors, cameras, and microphones. At
しかしながら、少なくとも1人の人物の占有がブロック704において検出された場合、方法700は、ブロック706に進むことができる。ブロック706において、システムは、スタンドアロンマルチモダルデバイス(たとえば、106N、306)のディスプレイ(たとえば、111、311)を非アクティブ状態(たとえば、オフ、スリープなど)からアクティブ状態に遷移させることができる。「周囲」状態が採用されるいくつかの実装形態では、ディスプレイは、最初、汎用データ項目に関連するグラフィカル情報で、かつ/または一般的なスクリーンセーバスタイルの風景でポピュレートされ得る。他の実装形態では、ディスプレイは、最初、「ホーム」状態に進み得る。
However, if the occupancy of at least one person is detected in
ブロック706の後に示されるが、その前に、または同時に、発生し得るブロック708において、システムは、たとえば、ユーザアカウントエンジン132によって、1人または複数の登録ユーザをターゲットにした複数のデータ項目を取得し得る。本明細書で述べるように、様々な実装形態では、1人または複数の登録ユーザは、スタンドアロンマルチモダルアシスタントデバイス上で少なくとも部分的に動作する自動アシスタント(120)にとってアクセス可能なアカウントを有し得る。複数のデータ項目は、登録ユーザのアカウントに基づいて取得可能であり、リマインダ、買い物リスト、登録ユーザが特に関心を持つ新しい商品、ソーシャルメディア更新、予約、着信通信(たとえば、電子メール、テキスト)など、様々な異なるものを含み得る。
Shown after block 706, but before or at the same time, in
ブロック710において、システムは、検出された人物の識別情報が検出されるかどうかを判定し得る。たとえば、検出された人物が発話を行った場合、システムは、話者の音声と前に記憶された音声プロファイルの照合を試行することができる。いくつかの実装形態では、発話の次元削減された埋め込みを生成し、登録ユーザによって提供された発話から前に生成された1つまたは複数の他の次元削減された基準埋め込みと比較することができる。追加または代替として、検出された人物は、基準熱シグネチャと照合し得る、PIRセンサーによって生成された熱シグネチャなど、他の信号を使用して、(カメラが利用可能であるとき)顔認識によって、人物が携行するクライアントデバイスが放出する信号を検出することによって、特定の登録ユーザが一定の時間に付近にいることがスケジュールされていることを(たとえば、オンラインカレンダーを介して)判定することなどによって、識別され得る。 At block 710, the system may determine if the identified information of the detected person is detected. For example, if the detected person speaks, the system can attempt to match the speaker's voice with a previously stored voice profile. In some implementations, a dimension-reduced embedding of an utterance can be generated and compared to one or more other dimension-reduced reference embeddings previously generated from an utterance provided by a registered user. .. As an addition or alternative, the detected person can be matched against a reference thermal signature, using other signals, such as a thermal signature generated by a PIR sensor, by facial recognition (when a camera is available). By detecting signals emitted by a client device carried by a person, such as by determining that a particular registered user is scheduled to be nearby at a given time (eg, via an online calendar). , Can be identified.
ブロック710において、検出された人物が識別された場合、方法700はブロック712に進む。ブロック712において、システムは、たとえば、コンテンツ表示エンジン130によって、検出された識別情報に基づいて、複数のデータ項目をランク付けし得る。ブロック714において、システムは、1人または複数の登録ユーザとは無関係である複数のデータ項目に関連する優先順位に基づいて、複数のデータ項目をさらにランク付けし得る。いくつかの実装形態では、検出された人物の判定された識別情報は、1人または複数の登録ユーザとは無関係である優先順位よりも大きな影響をランク付けに及ぼし得る。ブロック710において、検出された人物の識別情報が検出されない場合、いくつかの実装形態では、ブロック712は省かれてよく、方法700は直接ブロック714に進むことができる。
If the detected person is identified in block 710,
ブロック716において、システムは、ランク付けされた複数のデータ項目を示すグラフィカル情報をディスプレイ上にレンダリングし得る。このグラフィカル情報は、カード、タイル、ティッカーテープ、ポップアップウィンドウ、通知などのグラフィカル要素を含み得る。本明細書で述べるように、ディスプレイがタッチスクリーンである実装形態では、これらのグラフィカル要素のうちの1つまたは複数は、ユーザがそれらのグラフィカル要素の上をタップして、追加の情報を取得するように、かつ/もしくは他の活動を実行するように、ならびに/またはそれらのグラフィカル要素をスワイプして退けるように、対話型であり得る。
At
図8は、本明細書で説明する技法の1つまたは複数の態様を実行するために随意に利用され得る例示的なコンピューティングデバイス810のブロック図である。コンピューティングデバイス810は、一般に、バスサブシステム812を介していくつかの周辺デバイスと通信する、少なくとも1つのプロセッサ814を含む。これらの周辺デバイスは、たとえば、メモリサブシステム825およびファイル記憶サブシステム826を含む記憶サブシステム824と、ユーザインターフェース出力デバイス820と、ユーザインターフェース入力デバイス822と、ネットワークインターフェースサブシステム816とを含み得る。入力デバイスおよび出力デバイスは、コンピューティングデバイス810とのユーザ対話を可能にする。ネットワークインターフェースサブシステム816は、外部ネットワークに対するインターフェースを提供し、他のコンピューティングデバイス内の対応するインターフェースデバイスに結合される。
FIG. 8 is a block diagram of an
ユーザインターフェース入力デバイス822は、キーボード、マウスなどのポインティングデバイス、トラックボール、タッチパッド、またはグラフィカルタブレット、スキャナ、ディスプレイ内に組み込まれたタッチスクリーン、音声認識システム、マイクロフォンなどの音声入力デバイス、および/または他のタイプの入力デバイスを含み得る。概して、「入力デバイス」という用語の使用は、すべての考えられるタイプのデバイス、および情報をコンピューティングデバイス810内にまたは通信ネットワーク上に入力するための方法を含むことを意図する。
The user
ユーザインターフェース出力デバイス820は、ディスプレイサブシステム、プリンタ、ファックス機械、またはオーディオ出力デバイスなどの非視覚的ディスプレイを含み得る。ディスプレイサブシステムは、陰極線管(CRT)、液晶ディスプレイ(LCD)などのフラットパネルデバイス、投影デバイス、または視覚画像を作成するためのいくつかの他の機構を含み得る。ディスプレイサブシステムは、オーディオ出力デバイスを介するなどして、非視覚的な表示を提供してもよい。概して、「出力デバイス」という用語の使用は、すべての考えられるタイプのデバイス、および情報をコンピューティングデバイス810からユーザに、もしくは別の機械に、またはコンピューティングデバイスに出力する方法を含むことを意図する。
The user
記憶サブシステム824は、本明細書で説明するモジュールのうちのいくつかまたはすべての機能性を提供するプログラミング構造およびデータ構造を記憶する。たとえば、記憶サブシステム824は、図7の方法の選択された態様を実行するため、ならびに図1に示した様々な構成要素を実装するための論理を含み得る。
The
これらのソフトウェアモジュールは、概して、プロセッサ814によって単独で、または他のプロセッサと組み合わせて実行される。記憶サブシステム824内で使用されるメモリ825は、プログラム実行の間に命令およびデータを記憶するためのメインランダムアクセスメモリ(RAN)830、および固定命令が記憶される読取り専用メモリ(ROM)832を含むいくつかのメモリを含み得る。ファイル記憶サブシステム826は、プログラムファイルおよびデータファイルに対して永続的記憶を提供することができ、ハードディスクドライブ、関連するリムーバブルメディアとともにフロッピーディスクドライブ、CD-ROMドライブ、光ドライブ、またはリムーバブルメディアカートリッジを含み得る。いくつかの実装形態の機能性を実装するモジュールは、ファイル記憶サブシステム826によって記憶サブシステム824内に、またはプロセッサ814によってアクセス可能な他の機械の中に記憶され得る。
These software modules are generally executed by
バスサブシステム812は、コンピューティングデバイス810の様々な構成要素およびサブシステムに互いと意図されるように通信させるための機構を提供する。バスサブシステム812は、単一のバスとして概略的に示されているが、バスサブシステムの代替実装形態は、複数のバスを使用し得る。
The
コンピューティングデバイス810は、ワークステーション、サーバ、コンピューティングクラスタ、ブレードサーバ、サーバファーム、または任意の他のデータ処理システムもしくはデータ処理コンピューティングデバイスを含めて、様々なタイプのものであってよい。コンピュータおよびネットワークの絶えず変化する性質により、図8に示すコンピューティングデバイス810の記述は、いくつかの実装形態を示すための特定の例として単に意図される。図8に示すコンピューティングデバイスよりも多数のまたは少数の構成要素を有するコンピューティングデバイス810の多くの他の構成が可能である。
The
本明細書で論じるいくつかの実装形態がユーザに関する個人情報(たとえば、他の電子通信から抽出されたユーザデータ、ユーザのソーシャルネットワークに関する情報、ユーザの場所、ユーザの時間、ユーザの生体情報、ならびにユーザの活動および人口統計情報、ユーザ間の関係など)を収集して使用することができる状況において、情報が収集されるかどうか、個人情報が記憶されるかどうか、個人情報が使用されるかどうか、およびユーザに関する情報がどのように収集され、記憶され、使用されるかを制御するための1つまたは複数の機会がユーザに提供される。すなわち、本明細書で論じるシステムおよび方法は、関連するユーザからそれを行うための明示的な認可を受領したときのみ、ユーザの個人情報を収集、記憶、および/または使用する。 Some implementations discussed herein include personal information about the user (eg, user data extracted from other electronic communications, information about the user's social network, user location, user time, user biometric information, as well. Whether information is collected, personal information is stored, and personal information is used in situations where it is possible to collect and use user activity and demographic information, relationships between users, etc. It provides the user with one or more opportunities to control how and how information about the user is collected, stored and used. That is, the systems and methods discussed herein collect, store, and / or use your personal information only when you receive explicit authorization to do so from the relevant user.
たとえば、プログラムまたは特徴が、その特定のユーザ、またはそのプログラムまたは特徴に関連する他のユーザに関するユーザ情報を収集するかどうかに対する制御がユーザに提供される。個人情報が収集されることになる各ユーザには、その情報が収集されるかどうかに関して、かつその情報のどの部分が収集されるべきかに関して、許可または認可を提供するために、そのユーザに関連する情報収集に対する制御を可能にするための1つまたは複数のオプションが提示される。たとえば、通信ネットワーク上で1つまたは複数のそのような制御オプションがユーザに提供され得る。加えて、一定のデータは、個人的に識別可能な情報が除去されるように、そのデータが記憶または使用される前に、1つまたは複数の方法で処理され得る。一例として、個人的に識別可能な情報が判定され得ないように、ユーザの識別情報は処理され得る。別の例として、ユーザの詳しい場所が判定され得ないように、ユーザの地理的場所はより大きな領域に一般化され得る。 For example, a user is provided with control over whether a program or feature collects user information about that particular user, or other users associated with that program or feature. To provide each user with whom personal information will be collected permission or authorization with respect to whether that information is collected and what portion of that information should be collected. One or more options are presented to allow control over relevant information gathering. For example, one or more such control options may be provided to the user on the communication network. In addition, certain data may be processed in one or more ways before it is stored or used so that personally identifiable information is removed. As an example, user identification information can be processed so that personally identifiable information cannot be determined. As another example, a user's geographic location can be generalized to a larger area so that the user's detailed location cannot be determined.
いくつかの実装形態が本明細書で説明され示されてきたが、本明細書で説明した、機能を実行するため、および/または結果および/または1つまたは複数の利点を取得するために様々な他の手段および/または構造を利用することが可能であり、そのような変種および/または修正の各々が本明細書で説明した実装形態の範囲内であると見なされる。より一般的に、本明細書で説明した、すべてのパラメータ、寸法、材料、および構成は、例示的であることを意味し、実際のパラメータ、寸法、材料、および/または構成は、この/これらの教示が使用される特定の1つまたは複数の適用例に左右されることになる。当業者は、通常の実験以上の実験を使用せずに、本明細書で説明した特定の実装形態に対する多くの均等物を認識するであろうし、またはそれらを確認することが可能であろう。したがって、前述の実装形態は、例示としてのみ提示され、添付の特許請求の範囲およびそれに対する均等物の範囲内で、具体的に説明され、特許請求される実装形態以外の実装形態を別様に実践することが可能であることを理解されよう。本開示の実装形態は、本明細書で説明したそれぞれ個々の特徴、システム、物品、材料、キット、および/または方法に関する。加えて、2つ以上のそのような特徴、システム、物品、材料、キット、および/または方法の任意の組合せは、そのような特徴、システム、物品、材料、キット、および/または方法が相互に矛盾がない場合、本開示の範囲内に含まれる。 Several implementations have been described and presented herein, but vary in order to perform the functions described herein and / or to obtain results and / or one or more benefits. Other means and / or structures are available, and each such variant and / or modification is considered to be within the implementation described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary, and actual parameters, dimensions, materials, and / or configurations are this / these. The teachings of will depend on one or more specific applications in which they are used. One of ordinary skill in the art will recognize or be able to identify many equivalents for the particular implementations described herein without using more experiments than normal experiments. Therefore, the above-mentioned implementations are presented only as an example, and are specifically described within the scope of the appended claims and their equivalents, and the implementations other than the claims are different. It will be understood that it is possible to practice. The embodiments of the present disclosure relate to individual features, systems, articles, materials, kits, and / or methods described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and / or methods may have such features, systems, articles, materials, kits, and / or methods mutually. If there is no contradiction, it is included within the scope of this disclosure.
105 存在センサー
1061〜N クライアントコンピューティングデバイス
1061 第1のクライアントデバイス、クライアントデバイス
106N 別のクライアントデバイス、第2のクライアントデバイス、クライアントデバイス、スタンドアロンマルチモダルアシスタントデバイス
1091 スピーカ
109N スピーカ
111 ディスプレイ
111N ディスプレイ
114 音声捕捉/テキスト対音声(「TTS」)/STTモジュール
116 TTSモジュール
117 STTモジュール
118 自動アシスタントクライアント
1181 自動アシスタントクライアント
118N 自動アシスタントクライアント
119 クラウドベースの自動アシスタント構成要素
120 自動アシスタント
120A 第1の自動アシスタント、自動アシスタント
120B 第2の自動アシスタント
122 自然言語プロセッサ
124 ダイアログ状態トラッカー
126 ダイアログマネージャ
128 自然言語生成器
130 コンテンツ表示エンジン
132 ユーザアカウントエンジン
134 ユーザプロファイルインデックス、インデックス
302 ユーザ
303 ユーザ
306 スタンドアロンマルチモダルアシスタントデバイス、アシスタントデバイス
311 ディスプレイ
450 グラフィカル要素
450A〜E グラフィカル要素
450A 第1のグラフィカル要素
450B 第2のグラフィカル要素
450C 第3のグラフィカル要素
700 方法
810 コンピューティングデバイス
812 バスサブシステム
814 プロセッサ
816 ネットワークインターフェースサブシステム
820 ユーザインターフェース出力デバイス
822 ユーザインターフェース入力デバイス
824 記憶サブシステム
825 メモリサブシステム、メモリ
826 ファイル記憶サブシステム
830 メインランダムアクセスメモリ(RAM)
832 読取り専用メモリ(ROM)
105 Presence sensor
106 1 to N Client Computing Devices
106 1 First client device, client device
106 N Another client device, second client device, client device, stand-alone multimodal assistant device
109 1 speaker
109 N speaker
111 display
111 N display
114 Speech Capture / Text vs. Speech (“TTS”) / STT Module
116 TTS module
117 STT module
118 Automatic Assistant Client
118 1 Automatic Assistant Client
118 N Automatic Assistant Client
119 Cloud-based automatic assistant component
120 Automatic Assistant
120A 1st automatic assistant, automatic assistant
120B Second automatic assistant
122 natural language processor
124 Dialog State Tracker
126 Dialog Manager
128 natural language generator
130 Content Display Engine
132 User account engine
134 User Profile Index, Index
302 users
303 users
306 Standalone multimodal assistant device, assistant device
311 display
450 graphical elements
450A ~ E Graphical elements
450A First graphical element
450B Second graphical element
450C Third graphical element
700 methods
810 computing device
812 bus subsystem
814 processor
816 Network Interface Subsystem
820 user interface output device
822 User Interface Input Device
824 Storage subsystem
825 Memory subsystem, memory
826 File storage subsystem
830 Main Random Access Memory (RAM)
832 Read-only memory (ROM)
Claims (23)
未確認のユーザがスタンドアロンマルチモダルアシスタントデバイスと併存していることを検出するステップであって、前記スタンドアロンマルチモダルアシスタントデバイスがディスプレイを備える、検出するステップと、
前記検出に基づいて、
1人または複数の登録ユーザをターゲットにした複数のデータ項目を取得するステップであって、前記1人または複数の登録ユーザが、前記スタンドアロンマルチモダルアシスタントデバイス上で少なくとも部分的に動作する自動アシスタントにとってアクセス可能なアカウントを有し、前記複数のデータ項目が、前記登録ユーザの前記アカウントに基づいて取得される、取得するステップと、
前記複数のデータ項目に関連する優先順位に基づいて、前記複数のデータ項目をランク付けするステップであって、前記優先順位が、前記1人または複数の登録ユーザとは無関係である、ランク付けするステップと、
前記ランク付けされた複数のデータ項目を示すグラフィカル情報を前記ディスプレイ上にレンダリングするステップと
を含む、方法。 A method implemented by one or more processors
A step of detecting that an unidentified user coexists with a stand-alone multimodal assistant device, the step of detecting that the stand-alone multimodal assistant device has a display, and the step of detecting.
Based on the above detection
For an automated assistant who retrieves multiple data items targeting one or more registered users, the one or more registered users at least partially operating on the standalone multimodal assistant device. The step of acquiring, which has an accessible account, and the plurality of data items are acquired based on the account of the registered user.
A step of ranking the plurality of data items based on the priorities associated with the plurality of data items, the priorities being independent of the one or more registered users. Steps and
A method comprising rendering on the display graphical information indicating the ranked plurality of data items.
前記識別情報に基づいて、前記取得されたデータ項目をさらにランク付けするステップと
をさらに含む、請求項1から5のいずれか一項に記載の方法。 The step of determining the identification information of the unconfirmed user and
The method according to any one of claims 1 to 5, further comprising a step of further ranking the acquired data items based on the identification information.
ある人物がスタンドアロンマルチモダルアシスタントデバイスと併存していることを検出するステップであって、前記スタンドアロンマルチモダルアシスタントデバイスがディスプレイを備える、検出するステップと、
前記スタンドアロンマルチモダルアシスタントデバイス上で少なくとも部分的に動作する自動アシスタントにとってアクセス可能なアカウントを有する、複数の登録ユーザの所与の登録ユーザであるとして前記人物を識別するステップと、
前記検出および前記識別に基づいて、
前記複数の登録ユーザをターゲットにした複数のデータ項目を取得するステップであって、前記複数のデータ項目が、前記登録ユーザの前記アカウントに基づいて取得される、取得するステップと、
前記所与の登録ユーザの識別情報に基づいて、前記複数のデータ項目をランク付けするステップと、
前記ランク付けされた複数のデータ項目を示す情報を前記ディスプレイ上にレンダリングするステップと
を含む、方法。 A method implemented by one or more processors
A step of detecting that a person coexists with a stand-alone multimodal assistant device, wherein the stand-alone multimodal assistant device has a display.
A step of identifying the person as a given registered user of a plurality of registered users having an account accessible to an automated assistant running at least partially on the stand-alone multimodal assistant device.
Based on the detection and the identification
A step of acquiring a plurality of data items targeting the plurality of registered users, wherein the plurality of data items are acquired based on the account of the registered user.
A step of ranking the plurality of data items based on the identification information of the given registered user, and
A method comprising rendering information indicating the ranked plurality of data items onto the display.
前記入力に基づいて、前記所与のグラフィカル要素を前記ディスプレイから除去するステップと、
別の人物が前記スタンドアロンマルチモダルアシスタントデバイスと併存していることをその後に検出するステップと、
前記別の人物を、前記異なる登録ユーザとして識別するステップと、
前記所与のグラフィカル要素を前記ディスプレイ上にレンダリングするステップと
をさらに含む、請求項11または12に記載の方法。 In the step of receiving input from the given registered user, the input acts on a given graphical element of the plurality of graphical elements, and the given graphical element is the plurality of registered users. Receiving steps associated with a data item among the ranked data items, targeting different registered users of the
A step of removing the given graphical element from the display based on the input.
Subsequent detection of another person co-existing with the standalone multimodal assistant device, and
The step of identifying the other person as the different registered user,
The method of claim 11 or 12, further comprising the step of rendering the given graphical element onto the display.
未確認のユーザがスタンドアロンマルチモダルアシスタントデバイスと併存していることを検出する動作であって、前記スタンドアロンマルチモダルアシスタントデバイスがディスプレイを備える、検出する動作と、
前記検出に基づいて、
前記ディスプレイを非アクティブ状態からアクティブ状態に遷移させる動作と、
1人または複数の登録ユーザをターゲットにした複数のデータ項目を取得する動作であって、前記1人または複数の登録ユーザが、前記スタンドアロンマルチモダルアシスタントデバイス上で少なくとも部分的に動作する自動アシスタントにとってアクセス可能なアカウントを有し、前記複数のデータ項目が、前記登録ユーザの前記アカウントに基づいて取得される、取得する動作と、
前記複数のデータ項目に関連する優先順位に基づいて、前記複数のデータ項目をランク付けする動作であって、前記優先順位が、前記1人または複数の登録ユーザとは無関係である、ランク付けする動作と、
前記ランク付けされた複数のデータ項目を示すグラフィカル情報を前記ディスプレイ上にレンダリングする動作と
を実行させる、システム。 A system comprising one or more processors and a memory operably coupled to the one or more processors, wherein the memory stores instructions and the instructions are the one or more. In response to the execution of the instruction by the processor, the one or more processors
The operation of detecting that an unidentified user coexists with the stand-alone multimodal assistant device, the operation of detecting that the stand-alone multimodal assistant device has a display, and the operation of detecting.
Based on the above detection
The operation of transitioning the display from the inactive state to the active state, and
For an automated assistant that retrieves multiple data items targeting one or more registered users, the one or more registered users at least partially operating on the stand-alone multimodal assistant device. The operation of acquiring the plurality of data items having an accessible account and acquiring the plurality of data items based on the account of the registered user.
An operation that ranks the plurality of data items based on the priority related to the plurality of data items, and the priority is irrelevant to the one or more registered users. Operation and
A system that performs an operation of rendering graphical information indicating a plurality of ranked data items on the display.
前記識別情報に基づいて、前記取得されたデータ項目をさらにランク付けする
ための命令をさらに含み、
前記ユーザの前記判定された識別情報が、前記取得されたデータ項目がどのようにランク付けされるかに対して、前記1人または複数の登録ユーザとは無関係である前記優先順位よりも大きな影響を及ぼす
請求項17に記載のシステム。 The identification information of the unconfirmed user is determined, and
It further includes an instruction to further rank the acquired data item based on the identification information.
The determined identification information of the user has a greater effect on how the acquired data item is ranked than the priority, which is irrelevant to the one or more registered users. 17. The system according to claim 17.
ある人物がスタンドアロンマルチモダルアシスタントデバイスと併存していることを検出する動作であって、前記スタンドアロンマルチモダルアシスタントデバイスがディスプレイを備える、検出する動作と、
前記スタンドアロンマルチモダルアシスタントデバイス上で少なくとも部分的に動作する自動アシスタントにとってアクセス可能なアカウントを有する、複数の登録ユーザの所与の登録ユーザであるとして前記人物を識別する動作と、
前記検出および前記識別に基づいて、
前記複数の登録ユーザをターゲットにした複数のデータ項目を取得する動作であって、前記複数のデータ項目が、前記登録ユーザの前記アカウントに基づいて取得される、取得する動作と、
前記所与の登録ユーザの識別情報に基づいて、前記複数のデータ項目をランク付けする動作と、
前記ランク付けされた複数のデータ項目を示す情報を前記ディスプレイ上にレンダリングする動作と
を実行させる、少なくとも1つの非一時的コンピュータ可読媒体。 At least one non-transitory computer-readable medium containing an instruction, wherein the instruction is sent to the one or more processors in response to execution of the instruction by one or more processors.
An action to detect that a person coexists with a stand-alone multimodal assistant device, the action to detect that the stand-alone multimodal assistant device has a display,
The act of identifying the person as a given registered user of a plurality of registered users having an account accessible to an automated assistant running at least partially on the stand-alone multimodal assistant device.
Based on the detection and the identification
An operation of acquiring a plurality of data items targeting the plurality of registered users, wherein the plurality of data items are acquired based on the account of the registered user, and an operation of acquiring the data items.
The operation of ranking the plurality of data items based on the identification information of the given registered user, and
At least one non-transitory computer-readable medium that performs an operation of rendering information indicating the ranked data items onto the display.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2022168504A JP7471371B2 (en) | 2018-01-05 | 2022-10-20 | Selecting content to render on the assistant device's display |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862614332P | 2018-01-05 | 2018-01-05 | |
US62/614,332 | 2018-01-05 | ||
PCT/US2019/012347 WO2019136248A1 (en) | 2018-01-05 | 2019-01-04 | Selecting content to render on display of assistant device |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022168504A Division JP7471371B2 (en) | 2018-01-05 | 2022-10-20 | Selecting content to render on the assistant device's display |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2021509749A true JP2021509749A (en) | 2021-04-01 |
JP7164615B2 JP7164615B2 (en) | 2022-11-01 |
Family
ID=65279634
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2020537174A Active JP7164615B2 (en) | 2018-01-05 | 2019-01-04 | Selecting content to render on the assistant device display |
JP2022168504A Active JP7471371B2 (en) | 2018-01-05 | 2022-10-20 | Selecting content to render on the assistant device's display |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022168504A Active JP7471371B2 (en) | 2018-01-05 | 2022-10-20 | Selecting content to render on the assistant device's display |
Country Status (6)
Country | Link |
---|---|
US (1) | US11455176B2 (en) |
EP (1) | EP3555761A1 (en) |
JP (2) | JP7164615B2 (en) |
KR (1) | KR102498263B1 (en) |
CN (1) | CN111684438A (en) |
WO (1) | WO2019136248A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112445341B (en) * | 2020-11-23 | 2022-11-08 | 青岛小鸟看看科技有限公司 | Keyboard perspective method and device of virtual reality equipment and virtual reality equipment |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2008167132A (en) * | 2006-12-28 | 2008-07-17 | Kyocera Corp | Personal digital assistant |
JP2011242943A (en) * | 2010-05-17 | 2011-12-01 | Canon Inc | Information processor, control method and program |
US20140142953A1 (en) * | 2012-11-20 | 2014-05-22 | Lg Electronics Inc. | Mobile terminal and controlling method thereof |
JP2015046818A (en) * | 2013-08-29 | 2015-03-12 | 三菱電機インフォメーションシステムズ株式会社 | Application system, portable terminal, server computer, and computer program |
WO2017057010A1 (en) * | 2015-10-02 | 2017-04-06 | シャープ株式会社 | Terminal device and control server |
JP2017123564A (en) * | 2016-01-07 | 2017-07-13 | ソニー株式会社 | Controller, display unit, method, and program |
Family Cites Families (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2002041276A (en) | 2000-07-24 | 2002-02-08 | Sony Corp | Interactive operation-supporting system, interactive operation-supporting method and recording medium |
US6996777B2 (en) * | 2001-11-29 | 2006-02-07 | Nokia Corporation | Method and apparatus for presenting auditory icons in a mobile terminal |
JP2005103679A (en) | 2003-09-29 | 2005-04-21 | Toshiba Corp | Robot device |
WO2008081946A1 (en) | 2006-12-28 | 2008-07-10 | Kyocera Corporation | Mobile information terminal |
JP2009054027A (en) | 2007-08-28 | 2009-03-12 | Ricoh Co Ltd | Information processor, display screen customizing method, and display screen customizing program |
US10241752B2 (en) * | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US9443272B2 (en) * | 2012-09-13 | 2016-09-13 | Intel Corporation | Methods and apparatus for providing improved access to applications |
US9507755B1 (en) * | 2012-11-20 | 2016-11-29 | Micro Strategy Incorporated | Selecting content for presentation |
US9639247B2 (en) | 2013-07-19 | 2017-05-02 | Fuji Xerox Co., Ltd. | Information processing device, information processing method, and computer-readable medium |
US9542544B2 (en) * | 2013-11-08 | 2017-01-10 | Microsoft Technology Licensing, Llc | Correlated display of biometric identity, feedback and user interaction state |
US9696886B2 (en) * | 2013-12-12 | 2017-07-04 | Google Technology Holdings LLC | Systems and methods for communicating task reminders on portable electronic devices |
KR20150112337A (en) * | 2014-03-27 | 2015-10-07 | 삼성전자주식회사 | display apparatus and user interaction method thereof |
EP3154052A4 (en) * | 2014-06-03 | 2018-01-10 | Sony Corporation | Information processing device, information processing method, and program |
JP2016006613A (en) | 2014-06-20 | 2016-01-14 | ソニー株式会社 | Information processing device, information processing method, and program |
US9338493B2 (en) * | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9767794B2 (en) * | 2014-08-11 | 2017-09-19 | Nuance Communications, Inc. | Dialog flow management in hierarchical task dialogs |
US9691361B2 (en) * | 2015-08-03 | 2017-06-27 | International Business Machines Corporation | Adjusting presentation of content on a display |
US9965247B2 (en) * | 2016-02-22 | 2018-05-08 | Sonos, Inc. | Voice controlled media playback system based on user profile |
JP6649614B2 (en) | 2016-02-24 | 2020-02-19 | コニカミノルタ株式会社 | Information processing apparatus, conference support method, and conference support program |
KR102537543B1 (en) * | 2016-03-24 | 2023-05-26 | 삼성전자주식회사 | Intelligent electronic device and operating method thereof |
US20170289766A1 (en) * | 2016-03-29 | 2017-10-05 | Microsoft Technology Licensing, Llc | Digital Assistant Experience based on Presence Detection |
US20180157397A1 (en) * | 2016-04-08 | 2018-06-07 | Maxx Media Group, LLC | System and Method for Adding Three-Dimensional Images to an Intelligent Virtual Assistant that Appear to Project Forward of or Vertically Above an Electronic Display |
US20190065975A1 (en) * | 2017-08-31 | 2019-02-28 | Microsoft Technology Licensing, Llc | Contextual skills discovery |
-
2019
- 2019-01-04 WO PCT/US2019/012347 patent/WO2019136248A1/en unknown
- 2019-01-04 CN CN201980011938.4A patent/CN111684438A/en active Pending
- 2019-01-04 JP JP2020537174A patent/JP7164615B2/en active Active
- 2019-01-04 KR KR1020207022605A patent/KR102498263B1/en active IP Right Grant
- 2019-01-04 EP EP19703417.6A patent/EP3555761A1/en not_active Withdrawn
- 2019-01-04 US US16/621,987 patent/US11455176B2/en active Active
-
2022
- 2022-10-20 JP JP2022168504A patent/JP7471371B2/en active Active
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2008167132A (en) * | 2006-12-28 | 2008-07-17 | Kyocera Corp | Personal digital assistant |
JP2011242943A (en) * | 2010-05-17 | 2011-12-01 | Canon Inc | Information processor, control method and program |
US20140142953A1 (en) * | 2012-11-20 | 2014-05-22 | Lg Electronics Inc. | Mobile terminal and controlling method thereof |
JP2015046818A (en) * | 2013-08-29 | 2015-03-12 | 三菱電機インフォメーションシステムズ株式会社 | Application system, portable terminal, server computer, and computer program |
WO2017057010A1 (en) * | 2015-10-02 | 2017-04-06 | シャープ株式会社 | Terminal device and control server |
JP2017123564A (en) * | 2016-01-07 | 2017-07-13 | ソニー株式会社 | Controller, display unit, method, and program |
Also Published As
Publication number | Publication date |
---|---|
EP3555761A1 (en) | 2019-10-23 |
KR20200102513A (en) | 2020-08-31 |
US11455176B2 (en) | 2022-09-27 |
KR102498263B1 (en) | 2023-02-09 |
WO2019136248A1 (en) | 2019-07-11 |
US20200125377A1 (en) | 2020-04-23 |
JP7164615B2 (en) | 2022-11-01 |
JP2023017791A (en) | 2023-02-07 |
JP7471371B2 (en) | 2024-04-19 |
CN111684438A (en) | 2020-09-18 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7247271B2 (en) | Proactively Incorporating Unsolicited Content Within Human-to-Computer Dialogs | |
JP6911155B2 (en) | Memory of metadata associated with acquired images | |
JP2022079458A (en) | Automated assistant having conferencing ability | |
JP7384976B2 (en) | determining whether to automatically resume the first automated assistant session upon termination of the interrupting second session; | |
KR20210008521A (en) | Dynamic and/or context-specific hot words to invoke automated assistants | |
JP2021515938A (en) | Transitions between previous dialogue contexts with automated assistants | |
JP7471371B2 (en) | Selecting content to render on the assistant device's display |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20200825 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20200825 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20210817 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20210823 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20211105 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20220411 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20220601 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20220920 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20221020 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7164615Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |