US11158128B2 - Spatial and semantic augmented reality autocompletion in an augmented reality environment - Google Patents
Spatial and semantic augmented reality autocompletion in an augmented reality environment Download PDFInfo
- Publication number
- US11158128B2 US11158128B2 US16/395,505 US201916395505A US11158128B2 US 11158128 B2 US11158128 B2 US 11158128B2 US 201916395505 A US201916395505 A US 201916395505A US 11158128 B2 US11158128 B2 US 11158128B2
- Authority
- US
- United States
- Prior art keywords
- physical
- computing device
- physical environment
- suggested
- environment
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/22—Matching criteria, e.g. proximity measures
-
- G06K9/00664—
-
- G06K9/6201—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/60—Analysis of geometric attributes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
- G06T7/73—Determining position or orientation of objects or cameras using feature-based methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/10—Terrestrial scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/20—Scenes; Scene-specific elements in augmented reality scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/70—Labelling scene content, e.g. deriving syntactic or semantic representations
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2200/00—Indexing scheme for image data processing or generation, in general
- G06T2200/24—Indexing scheme for image data processing or generation, in general involving graphical user interfaces [GUIs]
Definitions
- This description generally relates to the use of computing devices in an augmented, or mixed, reality environment.
- Augmented, or mixed reality systems, or virtual reality systems may allow users to view scenes, for example, scenes corresponding to their physical environment, and to augment the scenes of their physical environment with virtual objects.
- a user may experience an augmented or mixed reality system and/or a virtual reality system may generate a three-dimensional augmented and/or mixed and/or virtual reality environment through interaction with various electronic devices.
- This type of augmented reality, or mixed reality, or virtual reality, experience would be enhanced by germane, pertinent, contextual suggestion and/or placement of virtual object(s) in a scene viewed by the user.
- a computer-implemented method may include detecting, by a sensor of a computing device, physical objects in a physical environment, identifying the detected physical objects, identifying at least one unoccupied space in the physical environment, analyzing the detected physical objects and the at least one unoccupied space, identifying at least one suggested object for placement in the unoccupied space in the physical environment, and placing at least one virtual representation corresponding to the at least one suggested object in a mixed reality scene of the physical environment generated by the computing device, at a position in the mixed reality scene corresponding to the at least one unoccupied space identified in the physical environment.
- detecting the physical objects in the physical environment may include capturing, by an image sensor of the computing device, image frames of the physical environment, detecting the physical objects in the image frames captured by the image sensor, comparing the detected physical objects to images stored in a database accessible to the computing device, and identifying the physical objects based on the comparison.
- detecting the physical objects may include detecting occupied spaces in the physical environment, including directing a plurality of rays from the computing device toward the physical environment as the image frames of the physical environment are captured, detecting a plurality of keypoints at points where the plurality of rays intersect with the physical objects, marking three-dimensional bounds of the physical objects in response to the detection of the plurality of keypoints, each of the plurality of keypoints being defined by a three-dimensional coordinate, and marking locations on a plane respectively corresponding to the physical objects as occupied spaces based on the marked bounds of each of the physical objects.
- identifying the at least one unoccupied space may include superimposing a two-dimensional grid on the plane, the two-dimensional grid defining a plurality of tiles, projecting the plurality of keypoints onto the grid, detecting points at which the projected keypoints intersect tiles of the grid, marking the tiles, of the plurality of tiles, at which the keypoints intersect the grid, as occupied spaces, and marking remaining tiles, of the plurality of tiles, as unoccupied spaces.
- the computer-implemented method may include rearranging the placement of the at least one virtual representation corresponding to the at least one suggested object in the mixed reality scene of the physical environment in response to a user input.
- the at least one virtual representation corresponding to the at least one suggested object is manipulatable by the user so as to allow the user to select the at least one suggested object, or access additional information related to the at least one suggested object.
- a system may include at least one computing device, including a memory storing executable instructions, and a processor configured to execute the instructions. Execution of the instructions may cause the at least one computing device to detect, by a sensor of the at least one computing device, physical objects in a physical environment, identify the detected physical objects, identify at least one unoccupied space in the physical environment, analyze the detected physical objects and the at least one unoccupied space, identify at least one suggested object for placement in the unoccupied space in the physical environment, and place at least one virtual representation corresponding to the at least one suggested object in a mixed reality scene of the physical environment generated by the computing device, at a position in the mixed reality scene corresponding to the at least one unoccupied space identified in the physical environment.
- the instructions may cause the computing device to capture, by an image sensor of the computing device, image frames of the physical environment, detect the physical objects in the image frames captured by the image sensor, compare the detected physical objects to images stored in a database accessible to the computing device, and identify the physical objects based on the comparison.
- the instructions may cause the computing device to detect occupied spaces in the physical environment, including direct a plurality of rays from the computing device toward the physical environment as the image frames of the physical environment are captured, detect a plurality of keypoints at points where the plurality of rays intersect with the physical objects, mark three-dimensional bounds of the physical objects in response to the detection of the plurality of keypoints, each of the plurality of keypoints being defined by a three-dimensional coordinate, and mark locations on a plane respectively corresponding to the physical objects as occupied spaces based on the marked bounds of each of the physical objects.
- the instructions may cause the computing device to superimpose a two-dimensional grid on the plane, the two-dimensional grid defining a plurality of tiles, project the plurality of keypoints onto the grid, detect points at which the projected keypoints intersect tiles of the grid, mark the tiles, of the plurality of tiles, at which the keypoints intersect the grid, as occupied spaces, and mark remaining tiles, of the plurality of tiles, as unoccupied spaces.
- the instructions may cause the computing device to determine a size of each of the unoccupied spaces based on at least one of a size, a contour, or an orientation of an identified physical object occupying one of the marked occupied spaces.
- the instructions may cause the computing device to determine a context of the physical environment based on the identification of the detected physical objects, and identify the at least one suggested object based on the determined context of the physical environment and the identified at least one unoccupied space. In some implementations, the instructions may cause the computing device to identify the at least one suggested object based on the determined size of the at least one unoccupied space relative to a position and an orientation of the identified physical objects. In some implementations, the instructions may cause the computing device to identify the at least one suggested object based on at least one of user preferences, user profile information, user browsing history, or an inventory of available items.
- the instructions may cause the computing device to rearrange the placement of the at least one virtual representation corresponding to the at least one suggested object in the mixed reality scene of the physical environment in response to a user input.
- the at least one virtual representation corresponding to the at least one suggested object is manipulatable by the user so as to allow the user to select the at least one suggested object, or access additional information related to the at least one suggested object.
- FIGS. 1A-1C illustrate exemplary systems for generating an augmented, or mixed reality environment.
- FIGS. 2A-2B illustrate exemplary physical environments, in which a system for generating an augmented, or mixed reality environment may be operated, in accordance with implementations described herein.
- FIGS. 3A-3E illustrate scanning and virtual object placement in an exemplary physical environment shown in FIG. 2A , in accordance with implementations described herein.
- FIG. 4 is a schematic diagram of an object detection, identification and localization system, in accordance with implementations described herein.
- FIGS. 5A-5J illustrate scanning and virtual object placement in the exemplary physical environment shown in FIG. 2B , based on detected unoccupied spaces in the exemplary physical environment, in accordance with implementations described herein.
- FIG. 6 is a block diagram of an exemplary system for generating an augmented, or mixed reality environment, in accordance with implementations described herein.
- FIG. 7 is a flowchart of a method, in accordance with implementations described herein
- FIG. 8 shows an example of a computer device and a mobile computer device that can be used to implement the techniques described herein.
- this document describes example approaches for modeling spatial relations between objects in an ambient, or physical, or real world environment, and for providing automatic suggestion and/or placement of augmented, or mixed reality objects in the real world environment.
- AR augmented reality
- MR mixed reality
- VR virtual reality
- the system may analyze what is visible in the real world environment, and provide for placement of three-dimensional (3D) virtual objects, or augmented/mixed reality objects, in a view of the real world environment.
- the system may analyze a stream of image information, to gain a semantic understanding of 3D pose information and location information related to real objects in the real world environment, as well as object identification information related to the real objects, and other such information associated with real objects in the real world environment.
- the system may also take into account empty spaces between the identified real objects, to make a determination of empty space(s) in the real world environment that may be available for virtual object placement in the real world environment.
- suggestions for virtual object(s) to be placed in the real world environment may be pertinent, or germane, or contextual, to the real world environment, thus enhancing the user's experience.
- the image frames may be fed through an auto-completion algorithm, or model, to gain a semantic understanding of the physical, real world environment, and in particular, 3D pose and location information of real object(s) in the physical, real world environment.
- real objects in the physical, real world environment may be identified through correlation with images in a database, including not just correlation of classes of items, but also correlation of spacing between objects.
- Empty spacing for example, spacing available for placement of virtual objects, may be taken into account by transmitting beams through the detected real object(s) in a point cloud, to detect whether or not a space is occupied by a real object.
- the detection of unoccupied space may be used to determine spacing available for the selection and suggestion/placement of contextual virtual object(s), based on the semantic understanding of the physical real world environment.
- the system may recommend items, in particular, germane, pertinent, contextual items, for virtual placement.
- the suggested items for placement may be ranked by a recommendation model, that may be trained through pair-wise scoring between pairs of objects. In some implementations, this can include known/observed user brand interest, location, and other such factors. This suggestion and placement of relevant virtual objects into the physical, real world environment may allow a user to view and identify pertinent items which may be of interest in a particular situation, without the need for extensive searching.
- images of the real world environment may be viewed, for example, within a camera view, or scene, on a display portion of an electronic device, with 3D virtual/augmented/mixed reality objects semantically selected and placed in the scene.
- the real world environment, and virtual object(s) semantically selected and placed in the real world environment may be viewed through a wearable device such as, for example, a head mounted electronic device.
- the 3D virtual/augmented/mixed reality object(s) may be suggested, or selected, and placed, contextually, or semantically with respect to the real world environment based on identification of real objects in the real world environment, and empty, or unoccupied, or available spaces identified in the real world environment.
- FIGS. 1A-1C illustrate various exemplary electronic devices that can generate an augmented or mixed reality, or virtual reality environment through, for example, an application executed by the electronic device.
- the user may view and experience an AR/MR environment on a display portion 12 of a handheld device 10 .
- An imaging device 14 of the exemplary handheld device 10 may provide images for display of a camera view, or scene, of the physical, real world environment, together with virtual object(s) selected and placed in the camera view, or scene, of the physical, real world environment based on the semantic understanding of the physical, real world environment.
- FIG. 1A illustrate various exemplary electronic devices that can generate an augmented or mixed reality, or virtual reality environment through, for example, an application executed by the electronic device.
- the user may view and experience an AR/MR environment on a display portion 12 of a handheld device 10 .
- An imaging device 14 of the exemplary handheld device 10 may provide images for display of a camera view, or scene, of the physical, real world environment, together with virtual object(s) selected and placed in the
- the user may view and experience an AR/MR environment via a display portion of a wearable device 16 .
- the wearable device is a head mounted device 16 .
- the physical, real world environment may be visible to the user through the head mounted device 16 , and suggested virtual object(s) may be placed in, or superimposed on the user's view of the physical, real world environment based on the semantic understanding of the physical, real world environment.
- the user may view and experience an AR/MR environment via a display portion of a head mounted device 18 which essentially occludes the user's direct visibility of the physical, real world environment.
- a pass through image of the physical environment may displayed on a display portion of the head mounted device 18 , with suggested virtual object(s) placed in the camera view, or scene, for example, superimposed on the pass through image, based on the semantic understanding of the physical, real world environment.
- Each of the handheld device 10 and the head mounted devices 16 and 18 illustrated in FIGS. 1A-1C are electronic devices including display devices capable of displaying virtual objects in an AR, or MR environment.
- examples of a system providing for placement of suggested virtual objects in an AR/MR environment based on a semantic understanding of a corresponding physical, real world environment will be presented based on a camera view, or scene, displayed on a display device of an electronic device similar to the exemplary electronic device 10 illustrated in FIG. 1A .
- the principles to be described herein may be applied to other electronic device(s) and/or systems capable of generating and presenting an AR/MR environment in which virtual objects, such as, for example, 3D virtual objects, may be placed based on a semantic understanding of the physical, real world environment.
- FIGS. 2A, 2B and 2C are third person views of a user in exemplary physical environments 100 (in particular, physical environments 100 A, 100 B and 100 C, respectively), including a variety of different physical objects.
- the user may experience the AR environment 200 through a camera view, or scene, provided on a display device of an electronic device 110 .
- the AR environment 200 may be generated by an AR application running on the electronic device 110 .
- the AR application may cause the AR environment 200 to be displayed to the user through the camera view, or scene, displayed on the display device of the electronic device 110 .
- the AR environment 200 may be an MR environment including a mixture of virtual objects and physical objects (e.g., virtual objects placed within the scene of the physical environment 100 ).
- the AR environment 200 may be an environment in which virtual objects may be selected and/or placed, by the system and/or by the user, allowing the user to view, interact with, and manipulate the virtual objects displayed within the AR environment 200 .
- a user in the physical environment 100 A may use a sensor, for example, a camera of the electronic device 110 to stream images of the physical environment 100 A.
- a sensor for example, a camera of the electronic device 110 to stream images of the physical environment 100 A.
- 3D pose and location information related to real objects in the physical environment 100 A may be detected. Images of the real objects may be correlated against images stored in an image database, to identify the detected real objects.
- Identification of the real objects in this manner may provide for development of a semantic understanding of the physical, real world environment.
- the 3D pose and location information related to the detected real objects, and relative arrangement of the real objects, may provide for further semantic understanding of the real world environment.
- This semantic understanding may be used to place suggested virtual items in a scene of the physical environment, based on detected unoccupied spaces in the physical, real world environment.
- FIGS. 3A-3D illustrate sequential camera views 300 , or scenes 300 , of the physical environment 100 A shown in FIG. 2A , as viewed by the user on, for example, a display of the electronic device, as the user moves the electronic device 110 through the physical environment 100 A shown in FIG. 2A .
- FIGS. 3A-3D illustrate sequential camera views, or scenes, 300 A, 300 B, 300 C and 300 D of the physical environment 100 A as the electronic device 110 is moved in this manner.
- Each of these exemplary camera views 300 A, 300 B, 300 C and 300 D views is a still image, representing a snapshot at a corresponding intermediate location in the physical environment 100 A.
- a continuous stream of images, or a dynamic series of images forming such a stream may be displayed, for example, on a display device of the electronic device 110 .
- images of the physical environment 100 A may be streamed in response to execution of an application running on the electronic device 110 , and movement of the electronic device 110 and orientation of the camera of the electronic device 110 through the relevant portion of the physical environment 100 A.
- the algorithm may detect a first series of surfaces 310 A, for example, a series of adjacent horizontal and vertical surfaces.
- the algorithm may correlate the detected surface(s) 310 A with shelves 310 , based on the identification of books 315 positioned relative to the first series of surfaces 310 A.
- the system may detect a second surface 320 A, for example, a horizontal surface, in the collected image frames, and correlate the detected second surface 320 A with a desk 320 , based on the identification of a keyboard 330 and a monitor 335 positioned on the second surface 320 A, and a desk chair 340 positioned adjacent the surface 320 A.
- physical items in the physical environment 100 A such as, for example, the books 315 , the keyboard 300 , the monitor 335 and the chair 340 , may be identified or recognized based on comparison and/or matching with images of previously identified items in a database accessible to the application. Based on the identification and/or recognition of these physical items, the system may identify surrounding items, context and/or functionality associated with the physical environment and the like.
- the first series of horizontal and/or vertical surfaces may be identified as shelving based on the identification and/or recognition of the books 315 positioned thereon.
- the second surface may be identified as a work surface, or desk (rather than, for example, a table) based on the identification and/or recognition of the keyboard 330 and the monitor 335 positioned thereon.
- the system may identify empty, or unoccupied spaces in the physical space 100 A.
- the system may also provide suggestions or recommendations of items which may occupy the currently unoccupied physical spaces in the physical environment 100 A, for consideration by the user.
- These suggestions or recommendations may be provided to the user, in the form of, for example, virtual items positioned in the camera view 300 , or scene 300 , of the physical environment 100 A. This may allow the user to view the suggested items in the physical environment 100 A, for consideration with regard to placement, purchase and the like.
- FIG. 3E illustrates an exemplary camera view 300 E, or scene 300 E, of the physical environment 100 A, after the image frames have been analyzed and the physical environment 100 A and physical objects therein, and unoccupied spaces in the physical environment 100 A, have been identified.
- the exemplary camera view 300 E shown in FIG. 3E includes contextually appropriate, suggested items, represented by virtual objects, placed in the camera view 300 E of the physical environment 100 A, based on the semantic understanding of the physical environment 100 A gained from the collection and processing of the image frames as described above with respect to FIGS. 3A-3D .
- FIG. 3E illustrates an exemplary camera view 300 E, or scene 300 E, of the physical environment 100 A, after the image frames have been analyzed and the physical environment 100 A and physical objects therein, and unoccupied spaces in the physical environment 100 A, have been identified.
- the exemplary camera view 300 E shown in FIG. 3E includes contextually appropriate, suggested items, represented by virtual objects, placed in the camera view 300 E of the physical environment 100 A, based on the semantic understanding of the physical
- a virtual object representing a printer 350 V, and virtual objects representing photo frames 355 V have been virtually placed on the shelves 310 , based on the semantic understanding of the physical environment 100 A and the detected unoccupied space on the shelves 310 .
- a virtual object representing a lamp 360 V has been placed in the camera view 300 E of the physical environment 100 A, based on the semantic understanding of the physical environment 100 A and the detected unoccupied floor space.
- Virtual objects representing headphones 370 V, a pen 375 V, and a pad of paper 380 V have been placed on the desk 320 , based on the semantic understanding of the physical environment 100 A and the detected unoccupied space on the desk 320 .
- the user may interact with one or more of the virtual objects representing suggested items (such as, for example, the virtual items representing the printer 350 V, the frames 355 V, the lamp 360 V, the headphones 370 V, the pen 375 V and the paper 380 V shown in FIG. 3E ).
- User interaction with the virtual objects representing the suggested items may, for example, provide access to additional information about related to the item, direct the user to purchasing information related to the item, direct the user to other, similar items which may also be suitable in the physical environment, allow the user to move the item to a different portion of the camera view of the physical environment, allow the user to remove the item from the camera view of the physical environment, allow the user to add more of the same and/or similar items to the camera view of the physical environment, and other such actions.
- the suggestion and placement of virtual objects in the camera view, or scene, of the physical environment may include determination of semantic labels for physical objects detected in the physical environment, and determination of 3D spatial coordinates of the detected physical items, as shown in FIG. 4 .
- This information may be used develop recommendations for, and auto-placement of, relevant, suggested items semantically placed in the camera view, or scene, of the physical environment.
- the object detector may be an image based object detector. Images captured by the object detector may be compared may be compared to, and matched with, images stored in an image database, for recognition/identification of the physical objects.
- hit testing may be performed along a camera direction of the electronic device to localize a detected physical object, and determine 3D coordinates of the detected physical object.
- hittests may be performed along the camera direction, from a center of a two dimensional bounding box bounding the detected object.
- the hittest result may be further projected to the nearest plane that is lower in height that the previous result, to localize the detected physical object.
- one or more recommended items may be considered for addition to the camera view, or scene, based on the detection of the physical objects and the semantic understanding of the physical environment as described above, with placement of the 3D virtual objects (representative of the recommended items) in detected empty, or unoccupied spaces in the physical environment.
- placement of the 3D virtual objects in an appropriate location in the camera view, or scene, in terms of the physical (x, y, z) coordinates may be accomplished as shown in FIGS. 5A through 5D .
- FIG. 5A a user in the physical environment 100 B (shown in FIG. 2B ) may use a sensor, for example, a camera of the electronic device 110 to stream images of the physical environment 100 B.
- FIG. 5A illustrates an image frame 500 A in which a plurality of physical objects may be detected, and identified as a first plate 520 and a first set of utensils 525 , a second plate 530 and a second set of utensils 535 , and a floral arrangement 540 .
- the system may determine that a detected horizontal surface 510 A corresponds to a table 510 (rather than, for example, a desk 310 , as in FIGS. 3A-3D ), based on the semantic understanding of the physical environment gained by the identification of the plates 520 , 530 , the utensils 525 , 535 and the floral arrangement 540 .
- the system may utilize feature points to mark regions that are occupied. For example, as shown in FIG. 5B , the system may apply 3D keypoints (illustrated by the triangles in FIG. 5B ) to camera view 500 A shown in FIG. 5A (illustrated as an image frame 500 B in FIG. 5B ) to mark the bounds of identified physical objects, in three dimensions, to in turn mark regions (in three dimensions) that are occupied by physical objects.
- rays generated by the electronic device 110 may be directed into the physical environment 100 B, and keypoints may be detected at each location where the rays intersect a physical object. Each keypoint may be defined by a 3D coordinate, marking a location on a corresponding plane that is occupied. The marked location may be a projection of the keypoint onto the corresponding plane.
- a two-dimensional (2D) grid 550 of tiles may be overlaid on the plane discussed above with respect to FIG. 5B (illustrated as an image frame 500 C in FIG. 5C ), to simplify the identification of unoccupied and occupied regions or spaces.
- the system may track occupied and unoccupied tiles, marking tiles having contained projections of taller feature points as occupied. For example, if the feature point is higher than the plane (in the example orientation illustrated in FIG. 5C ), the tile on which the point is projected would be marked as occupied.
- a point of reference at the origin O may be defined, and a reference anchor may be set, based on the pose of the central portion of the planar surface at detection.
- the grid 550 may be extended in both the positive and the negative directions of the x axis and the y axis.
- a 2D plane coordinate may be defined relative to the reference anchor. Mapping may be achieved by applying the inverse pose of the reference anchor to the feature point, and discarding the y component, as shown in Equation 1 and Equation 2 below.
- Coord2 d.x anchor.getPose( ).inverse( ).transform(Coord3 d ) Equation 1
- Coord2 d.y anchor.getPose( ).inverse( ).transform(Coord3 d ) Equation 2
- the grid 550 may be a 2D Boolean array corresponding to the state of each of the tiles.
- the tiles may be indexed by a pair of integers (for example, row and column), with the plane origin O at substantially the center of the grid 550 .
- col int(Math.round( y /tileSize)+(numCols ⁇ 1)/2 Equation 4
- occupied tiles 555 may be distinguished from unoccupied tiles 558 , or unoccupied space 558 (illustrated in white), relative to the center, or origin O.
- a location search for a recommended object may be performed on the 2D grid 550 having occupied spaces or tiles 555 identified. The search may be terminated (and a virtual representation of the recommended or suggested object placed) when an available region of a requisite size is found to be available, and unoccupied in the search location. In some implementations, the search region may be sized to account for the anticipated dimensions of the suggested object. Once a virtual object representing the suggested item is placed, area(s) occupied by the virtual object may also be marked as occupied space 558 , or occupied tiles 558 .
- a semantic understanding of the physical environment 100 B may be gained through the identification of physical objects in the physical environment (i.e., the first and second plates 510 , 530 , the first and second sets of utensils 525 , 535 , and the floral arrangement 540 , all positioned on the table 510 ).
- the system may recognize and/or identify these physical objects, and apply labels to these physical objects, based on image recognition and comparison with an extensive collection of labeled, annotated images stored in a database that is accessible to the application running on the electronic device 110 .
- findings may be scored, or ranked, to reflect a degree of correlation between a detected physical object and a possible identified match from the database.
- similar scoring, or ranking may be taken into consideration when determining connections, or relationships between objects, based on the semantic understanding of the physical environment, and selecting suggested objects to add to the camera view of the physical environment.
- images of the physical environment 100 B may be streamed, and the image frames may be fed into the recognition algorithm or model, to obtain 3D pose and location information, and identification/recognition of real objects in the physical environment 100 B to provide for semantic understanding of the physical environment 100 B.
- the system may determine that the physical environment 100 B is a dining room, or a kitchen, or in particular, a dining room table, or a kitchen table as shown in the camera view 500 E, or scene 500 E, of the physical environment illustrated in FIG. 5E .
- the system may determine that the physical environment 100 B is a restaurant, or a restaurant table, as shown in the camera view 500 E, or scene 500 E, of the physical environment illustrated in FIG. 5E .
- the system may place virtual objects, representing suggested items, in the camera view, or scene, of the physical environment 100 B, for consideration by the user.
- a virtual object representing a cup of coffee 550 V has been placed in the camera view 500 F of the physical environment 100 B, based on the semantic understanding of the physical environment 100 B and the detected unoccupied space on the physical table 510 .
- a virtual object representing a beverage glass 555 V has been placed in the camera view 500 G of the physical environment 100 B shown in FIG. 5G .
- a virtual object representing a first entree 560 V has been placed in the camera view 500 H of the physical environment 100 B shown in FIG.
- FIGS. 5I and 5J illustrate camera views 500 I and 500 J, respectively, of the physical environment 100 B, in which different virtual objects, representing different entrees 565 V and 570 V, are presented for consideration by the user.
- the system may take into account other factors such as, for example, geographic location, time of day, user preferences and the like, in determining appropriate virtual objects for placement in the scene of the virtual environment. For example, in placing virtual objects representing the cup of coffee 550 V, the beverage glass 555 V and the pancakes 560 V in the camera view 500 H (and/or the bagel 565 V and the waffle 570 V in the camera views 500 I and 500 J, respectively), the system may take into consideration the time of day, indicating it is time for breakfast.
- the system may take into account what may be available at a particular location. For example, if it is determined that the physical environment 100 B is in the home of the user, the system may provide recommendations based on what is available for breakfast selections in the home of the user (based on, for example, access to shopping lists, stored inventories, user preferences and habits, and the like). In another example, if, based on geographic location information, it is determined that the physical environment is a restaurant, then suggestions/recommendations may be presented to the user for consideration based on items that are available on a menu, as well as user preferences and the like.
- the user may interact with the virtual objects representing the cup of coffee 550 V, the beverage glass 555 V, one of the entrees 560 V, 565 V, 570 V and the like to, for example, select the corresponding item.
- the user may, for example, access additional information related to the item, order the item, move or in some manner change the item, and the like.
- the user may interact with a virtual object to discard the corresponding item.
- the system may place virtual objects corresponding to alternative items for consideration by the user, in response to discarded items.
- FIG. 6 is block diagram of computing device 600 that can generate an augmented reality, or mixed reality environment, and that can provide for user interaction with virtual objects presented in a camera view, or scene, of a physical environment, in accordance with implementations described herein.
- the computing device 600 may include a user interface system 620 including at least one output device and at least one input device.
- the at least one output device may include, for example, a display for visual output, a speaker for audio output, and the like.
- the at least one input device may include, for example, a touch input device that can receive tactile user inputs, a microphone that can receive audible user inputs, and the like.
- the computing device 600 may also include a sensing system 640 .
- the sensing system 640 may include, for example, a light sensor, an audio sensor, an image sensor/imaging device, or camera, a distance/proximity sensor, a positional sensor, and/or other sensors and/or different combination(s) of sensors. Some of the sensors included in the sensing system 640 may provide for positional detection and tracking of the computing device 600 . Some of the sensors of the sensing system 640 may provide for the capture of images of the physical environment for display on a component of the user interface system 620 .
- the computing device 600 may also include a control system 680 .
- the control system 680 may include, for example, a power control device, audio and video control devices, an optical control device, and/or other such devices and/or different combination(s) of devices.
- the user interface system 620 , and/or the sensing system 640 and/or the control system 680 may include more, or fewer, devices, depending on a particular implementation, and may have a different physical arrangement that shown.
- the computing device 600 may also include a processor 690 in communication with the user interface system 620 , the sensing system 640 and the control system 680 , a memory 685 , and a communication module 695 .
- the communication module 695 may provide for communication between the electronic device 600 and other, external devices, external data sources, databases, and the like, through a network.
- a method 700 of generating an augmented reality, or mixed reality environment, and providing for user interaction with virtual objects presented in a camera view, or scene, of a physical environment, in accordance with implementations described herein, is shown in FIG. 7 .
- a user may initiate an augmented reality, or mixed reality, or virtual reality experience, through, for example, an application executing on a computing device to display an augmented reality, or a mixed reality scene including a view of a physical environment (block 710 ).
- the augmented/mixed reality scene including the view of the physical environment may be, for example, a camera view of the physical environment captured by an imaging device of the computing device, and displayed on a display device of the computing device.
- Physical objects detected, for example, through a scan of the physical environment may be analyzed for recognition/identification (block 730 ). Based on the physical objects identified in the physical environment, a semantic understanding of the physical environment may be developed (block 740 ). Based on the semantic understanding developed in the analysis of the physical objects detected in the physical environment, appropriate contextual objects may be selected for the physical environment, and virtual representations of the suggested objects may be placed in the AR/MR scene of the physical environment (block 750 ). The process may continue until the augmented reality/mixed reality experience has been terminated (block 760 ).
- FIG. 8 shows an example of a generic computer device 2000 and a generic mobile computer device 2050 , which may be used with the techniques described here.
- Computing device 2000 is intended to represent various forms of digital computers, such as laptops, desktops, tablets, workstations, personal digital assistants, televisions, servers, blade servers, mainframes, and other appropriate computing devices.
- Computing device 2050 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart phones, and other similar computing devices.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- Computing device 2000 includes a processor 2002 , memory 2004 , a storage device 2006 , a high-speed interface 2008 connecting to memory 2004 and high-speed expansion ports 2010 , and a low speed interface 2012 connecting to low speed bus 2014 and storage device 2006 .
- the processor 2002 can be a semiconductor-based processor.
- the memory 2004 can be a semiconductor-based memory.
- Each of the components 2002 , 2004 , 2006 , 2008 , 2010 , and 2012 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 2002 can process instructions for execution within the computing device 2000 , including instructions stored in the memory 2004 or on the storage device 2006 to display graphical information for a GUI on an external input/output device, such as display 2016 coupled to high speed interface 2008 .
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 2000 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 2004 stores information within the computing device 2000 .
- the memory 2004 is a volatile memory unit or units.
- the memory 2004 is a non-volatile memory unit or units.
- the memory 2004 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 2006 is capable of providing mass storage for the computing device 2000 .
- the storage device 2006 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in an information carrier.
- the computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 2004 , the storage device 2006 , or memory on processor 2002 .
- the high speed controller 2008 manages bandwidth-intensive operations for the computing device 2000 , while the low speed controller 2012 manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only.
- the high-speed controller 2008 is coupled to memory 2004 , display 2016 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 2010 , which may accept various expansion cards (not shown).
- low-speed controller 2012 is coupled to storage device 2006 and low-speed expansion port 2014 .
- the low-speed expansion port which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 2000 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 2020 , or multiple times in a group of such servers. It may also be implemented as part of a rack server system 2024 . In addition, it may be implemented in a personal computer such as a laptop computer 2022 . Alternatively, components from computing device 2000 may be combined with other components in a mobile device (not shown), such as device 2050 . Each of such devices may contain one or more of computing device 2000 , 2050 , and an entire system may be made up of multiple computing devices 2000 , 2050 communicating with each other.
- Computing device 2050 includes a processor 2052 , memory 2064 , an input/output device such as a display 2054 , a communication interface 2066 , and a transceiver 2068 , among other components.
- the device 2050 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage.
- a storage device such as a microdrive or other device, to provide additional storage.
- Each of the components 2050 , 2052 , 2064 , 2054 , 2066 , and 2068 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
- the processor 2052 can execute instructions within the computing device 2050 , including instructions stored in the memory 2064 .
- the processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors.
- the processor may provide, for example, for coordination of the other components of the device 2050 , such as control of user interfaces, applications run by device 2050 , and wireless communication by device 2050 .
- Processor 2052 may communicate with a user through control interface 2058 and display interface 2056 coupled to a display 2054 .
- the display 2054 may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 2056 may comprise appropriate circuitry for driving the display 2054 to present graphical and other information to a user.
- the control interface 2058 may receive commands from a user and convert them for submission to the processor 2052 .
- an external interface 2062 may be provide in communication with processor 2052 , so as to enable near area communication of device 2050 with other devices. External interface 2062 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 2064 stores information within the computing device 2050 .
- the memory 2064 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- Expansion memory 2074 may also be provided and connected to device 2050 through expansion interface 2072 , which may include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- expansion memory 2074 may provide extra storage space for device 2050 , or may also store applications or other information for device 2050 .
- expansion memory 2074 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- expansion memory 2074 may be provide as a security module for device 2050 , and may be programmed with instructions that permit secure use of device 2050 .
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory, as discussed below.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 2064 , expansion memory 2074 , or memory on processor 2052 , that may be received, for example, over transceiver 2068 or external interface 2062 .
- Device 2050 may communicate wirelessly through communication interface 2066 , which may include digital signal processing circuitry where necessary. Communication interface 2066 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 2068 . In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module 2070 may provide additional navigation- and location-related wireless data to device 2050 , which may be used as appropriate by applications running on device 2050 .
- GPS Global Positioning System
- Device 2050 may also communicate audibly using audio codec 2060 , which may receive spoken information from a user and convert it to usable digital information. Audio codec 2060 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 2050 . Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 2050 .
- Audio codec 2060 may receive spoken information from a user and convert it to usable digital information. Audio codec 2060 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 2050 . Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 2050 .
- the computing device 2050 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 2080 . It may also be implemented as part of a smart phone 2082 , personal digital assistant, or other similar mobile device.
- implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
Coord2d.x=anchor.getPose( ).inverse( ).transform(Coord3d) Equation 1
Coord2d.y=anchor.getPose( ).inverse( ).transform(Coord3d) Equation 2
row=int(Math.round(x/tileSize)+(numRows−1)/2 Equation 3
col=int(Math.round(y/tileSize)+(numCols−1)/2 Equation 4
Claims (19)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/395,505 US11158128B2 (en) | 2019-04-26 | 2019-04-26 | Spatial and semantic augmented reality autocompletion in an augmented reality environment |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/395,505 US11158128B2 (en) | 2019-04-26 | 2019-04-26 | Spatial and semantic augmented reality autocompletion in an augmented reality environment |
Publications (2)
Publication Number | Publication Date |
---|---|
US20200342668A1 US20200342668A1 (en) | 2020-10-29 |
US11158128B2 true US11158128B2 (en) | 2021-10-26 |
Family
ID=72917291
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/395,505 Active US11158128B2 (en) | 2019-04-26 | 2019-04-26 | Spatial and semantic augmented reality autocompletion in an augmented reality environment |
Country Status (1)
Country | Link |
---|---|
US (1) | US11158128B2 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200401804A1 (en) * | 2019-06-19 | 2020-12-24 | Apple Inc. | Virtual content positioned based on detected object |
US20210349383A1 (en) * | 2020-05-11 | 2021-11-11 | Anthony Goolab | Lunar Image Projection System |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20200144196A (en) * | 2019-06-17 | 2020-12-29 | 삼성전자주식회사 | Electronic device and method for providing function using corneal image thereof |
US20220335661A1 (en) * | 2020-02-28 | 2022-10-20 | Google Llc | System and method for playback of augmented reality content triggered by image recognition |
US20210375065A1 (en) * | 2020-05-29 | 2021-12-02 | Unity IPR ApS | Method and system for matching conditions for digital objects in augmented reality |
US11475582B1 (en) | 2020-06-18 | 2022-10-18 | Apple Inc. | Method and device for measuring physical objects |
US11803829B2 (en) | 2020-09-30 | 2023-10-31 | Block, Inc. | Device-aware communication requests |
US11341473B2 (en) | 2020-09-30 | 2022-05-24 | Block, Inc. | Context-based communication requests |
US11494996B2 (en) * | 2020-11-30 | 2022-11-08 | International Business Machines Corporation | Dynamic interaction deployment within tangible mixed reality |
US11640700B2 (en) | 2021-02-26 | 2023-05-02 | Huawei Technologies Co., Ltd. | Methods and systems for rendering virtual objects in user-defined spatial boundary in extended reality environment |
USD1021943S1 (en) | 2021-12-15 | 2024-04-09 | Block, Inc. | Display screen or portion thereof with a graphical user interface |
US11798447B1 (en) * | 2022-07-29 | 2023-10-24 | Zoom Video Communications, Inc. | Illumination testing for shared device displays |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110288912A1 (en) | 2010-05-21 | 2011-11-24 | Comcast Cable Communications, Llc | Content Recommendation System |
US9087403B2 (en) * | 2012-07-26 | 2015-07-21 | Qualcomm Incorporated | Maintaining continuity of augmentations |
US20160193732A1 (en) | 2013-03-15 | 2016-07-07 | JIBO, Inc. | Engaging in human-based social interaction with members of a group using a persistent companion device |
US9723226B2 (en) | 2010-11-24 | 2017-08-01 | Aria Glassworks, Inc. | System and method for acquiring virtual and augmented reality scenes by a user |
US9824495B2 (en) | 2008-09-11 | 2017-11-21 | Apple Inc. | Method and system for compositing an augmented reality scene |
US20190121522A1 (en) * | 2017-10-21 | 2019-04-25 | EyeCam Inc. | Adaptive graphic user interfacing system |
US20190197599A1 (en) * | 2017-12-22 | 2019-06-27 | Houzz, Inc. | Techniques for recommending and presenting products in an augmented reality scene |
US10339721B1 (en) * | 2018-01-24 | 2019-07-02 | Apple Inc. | Devices, methods, and graphical user interfaces for system-wide behavior for 3D models |
US10339718B1 (en) * | 2017-12-29 | 2019-07-02 | Verizon Patent And Licensing Inc. | Methods and systems for projecting augmented reality content |
US20200184653A1 (en) * | 2018-12-06 | 2020-06-11 | Microsoft Technology Licensing, Llc | Enhanced techniques for tracking the movement of real-world objects for improved positioning of virtual objects |
-
2019
- 2019-04-26 US US16/395,505 patent/US11158128B2/en active Active
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9824495B2 (en) | 2008-09-11 | 2017-11-21 | Apple Inc. | Method and system for compositing an augmented reality scene |
US20110288912A1 (en) | 2010-05-21 | 2011-11-24 | Comcast Cable Communications, Llc | Content Recommendation System |
US9723226B2 (en) | 2010-11-24 | 2017-08-01 | Aria Glassworks, Inc. | System and method for acquiring virtual and augmented reality scenes by a user |
US9087403B2 (en) * | 2012-07-26 | 2015-07-21 | Qualcomm Incorporated | Maintaining continuity of augmentations |
US20160193732A1 (en) | 2013-03-15 | 2016-07-07 | JIBO, Inc. | Engaging in human-based social interaction with members of a group using a persistent companion device |
US20190121522A1 (en) * | 2017-10-21 | 2019-04-25 | EyeCam Inc. | Adaptive graphic user interfacing system |
US20190197599A1 (en) * | 2017-12-22 | 2019-06-27 | Houzz, Inc. | Techniques for recommending and presenting products in an augmented reality scene |
US10339718B1 (en) * | 2017-12-29 | 2019-07-02 | Verizon Patent And Licensing Inc. | Methods and systems for projecting augmented reality content |
US10339721B1 (en) * | 2018-01-24 | 2019-07-02 | Apple Inc. | Devices, methods, and graphical user interfaces for system-wide behavior for 3D models |
US20200184653A1 (en) * | 2018-12-06 | 2020-06-11 | Microsoft Technology Licensing, Llc | Enhanced techniques for tracking the movement of real-world objects for improved positioning of virtual objects |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200401804A1 (en) * | 2019-06-19 | 2020-12-24 | Apple Inc. | Virtual content positioned based on detected object |
US11710310B2 (en) * | 2019-06-19 | 2023-07-25 | Apple Inc. | Virtual content positioned based on detected object |
US20210349383A1 (en) * | 2020-05-11 | 2021-11-11 | Anthony Goolab | Lunar Image Projection System |
US11789350B2 (en) * | 2020-05-11 | 2023-10-17 | Anthony Goolab | Celestial body image projection system |
Also Published As
Publication number | Publication date |
---|---|
US20200342668A1 (en) | 2020-10-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11158128B2 (en) | Spatial and semantic augmented reality autocompletion in an augmented reality environment | |
US9342930B1 (en) | Information aggregation for recognized locations | |
US10346684B2 (en) | Visual search utilizing color descriptors | |
US11714509B2 (en) | Multi-plane reflective sensor | |
US10789699B2 (en) | Capturing color information from a physical environment | |
US10038839B2 (en) | Assisted text input for computing devices | |
EP2444918B1 (en) | Apparatus and method for providing augmented reality user interface | |
EP3201833B1 (en) | Schemes for retrieving and associating content items with real-world objects using augmented reality and object recognition | |
US10210423B2 (en) | Image match for featureless objects | |
US7599561B2 (en) | Compact interactive tabletop with projection-vision | |
US8611667B2 (en) | Compact interactive tabletop with projection-vision | |
US9094670B1 (en) | Model generation and database | |
EP3341851B1 (en) | Gesture based annotations | |
US20160314512A1 (en) | Visual search in a controlled shopping environment | |
US20130135295A1 (en) | Method and system for a augmented reality | |
US20160180441A1 (en) | Item preview image generation | |
CN104871214A (en) | User interface for augmented reality enabled devices | |
US20140085245A1 (en) | Display integrated camera array | |
US11126845B1 (en) | Comparative information visualization in augmented reality | |
CN102792338A (en) | Information processing device, information processing method, and program | |
JP2010519656A (en) | Remote object recognition | |
CN109196577A (en) | Method and apparatus for providing user interface for computerized system and being interacted with virtual environment | |
US9672436B1 (en) | Interfaces for item search | |
CN113891166A (en) | Data processing method, data processing device, computer equipment and medium | |
US10304120B2 (en) | Merchandise sales service device based on dynamic scene change, merchandise sales system based on dynamic scene change, method for selling merchandise based on dynamic scene change and non-transitory computer readable storage medium having computer program recorded thereon |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CHOJNACKA, ROZA;PATEL, RAJAN;LUO, XIYANG;AND OTHERS;SIGNING DATES FROM 20190529 TO 20190530;REEL/FRAME:049509/0346 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP., ISSUE FEE NOT PAID |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |