CN114008590B - Providing an auxiliary user interface using execution blocks - Google Patents
Providing an auxiliary user interface using execution blocks Download PDFInfo
- Publication number
- CN114008590B CN114008590B CN201980097694.6A CN201980097694A CN114008590B CN 114008590 B CN114008590 B CN 114008590B CN 201980097694 A CN201980097694 A CN 201980097694A CN 114008590 B CN114008590 B CN 114008590B
- Authority
- CN
- China
- Prior art keywords
- user
- computer
- execution
- user interface
- execution block
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 claims abstract description 85
- 230000003993 interaction Effects 0.000 claims abstract description 59
- 230000001404 mediated effect Effects 0.000 claims abstract description 50
- 230000015654 memory Effects 0.000 claims description 27
- 230000004044 response Effects 0.000 claims description 20
- 238000013507 mapping Methods 0.000 claims description 13
- 230000004913 activation Effects 0.000 claims description 3
- 238000010801 machine learning Methods 0.000 description 27
- 230000006870 function Effects 0.000 description 24
- 238000012545 processing Methods 0.000 description 19
- 238000004891 communication Methods 0.000 description 16
- 230000009471 action Effects 0.000 description 13
- 230000001149 cognitive effect Effects 0.000 description 13
- 238000012549 training Methods 0.000 description 10
- 230000008569 process Effects 0.000 description 9
- 101150054987 ChAT gene Proteins 0.000 description 8
- 101100203187 Mus musculus Sh2d3c gene Proteins 0.000 description 8
- 238000013528 artificial neural network Methods 0.000 description 8
- 230000000007 visual effect Effects 0.000 description 7
- 238000010586 diagram Methods 0.000 description 5
- 230000000694 effects Effects 0.000 description 4
- 230000006855 networking Effects 0.000 description 4
- 238000013475 authorization Methods 0.000 description 3
- 238000004590 computer program Methods 0.000 description 3
- 238000012790 confirmation Methods 0.000 description 3
- 239000011521 glass Substances 0.000 description 3
- 239000011159 matrix material Substances 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000006403 short-term memory Effects 0.000 description 3
- 230000029305 taxis Effects 0.000 description 3
- 101100264195 Caenorhabditis elegans app-1 gene Proteins 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 230000003930 cognitive ability Effects 0.000 description 2
- 230000003920 cognitive function Effects 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 238000012015 optical character recognition Methods 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 238000004088 simulation Methods 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 238000012360 testing method Methods 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 230000001755 vocal effect Effects 0.000 description 2
- 208000018737 Parkinson disease Diseases 0.000 description 1
- 206010047513 Vision blurred Diseases 0.000 description 1
- 230000003213 activating effect Effects 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 230000000386 athletic effect Effects 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000010411 cooking Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 201000006517 essential tremor Diseases 0.000 description 1
- 238000010191 image analysis Methods 0.000 description 1
- 230000001771 impaired effect Effects 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 238000012886 linear function Methods 0.000 description 1
- 230000033001 locomotion Effects 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 206010027175 memory impairment Diseases 0.000 description 1
- 239000008267 milk Substances 0.000 description 1
- 210000004080 milk Anatomy 0.000 description 1
- 235000013336 milk Nutrition 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 230000002787 reinforcement Effects 0.000 description 1
- 230000003997 social interaction Effects 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/54—Interprogram communication
Abstract
Embodiments described herein relate to methods, systems, and computer-readable media for providing an auxiliary user interface. In some implementations, a computer-implemented method of providing an auxiliary user interface includes identifying a user itinerary by analyzing data including user interaction data, the user itinerary including a plurality of operations to perform computer-mediated tasks. The method also includes analyzing one or more of an Application Programming Interface (API) definition of the plurality of software applications or a User Interface (UI) element of the plurality of software applications to identify the plurality of executable units. The method further includes generating an execution block defining a sequence of two or more of the plurality of executable units based on the user tour and the plurality of executable units. Execution of the execution block completes the computer-mediated task. The method also includes providing an auxiliary user interface including an execution block.
Description
Background
User computing devices such as telephones, tablet computers, laptop and desktop computers, wearable devices, smart speakers, smart appliances, in-vehicle devices, and the like include functionality for users to entertain, process documents, conduct financial transactions, participate in social interactions, navigate to destinations, and the like. Such functionality is provided by various software applications provided on the user computing device and/or servers accessed from the user computing device.
Many computer-mediated tasks require a user to perform operations using a user computing device, for example, to navigate a user interface by providing selection inputs; providing text, audio and/or image input; etc. Many tasks also require a user to interact with multiple software applications, each with a corresponding user interface, or with multiple user computing devices. Such user interactions may require the user to possess some cognitive and/or motor skills.
The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.
Disclosure of Invention
Embodiments described herein relate to methods, systems, and computer-readable media for providing an auxiliary user interface. In some implementations, a computer-implemented method of providing an auxiliary user interface includes identifying a user itinerary (journ ey) by analyzing data including user interaction data, the user itinerary including a plurality of operations to perform computer-mediated tasks. The method also includes analyzing one or more of an Application Programming Interface (API) definition of the plurality of software applications or a User Interface (UI) element of the plurality of software applications to identify the plurality of executable units. The method further includes generating an execution block defining a sequence of two or more of the plurality of executable units based on the user tour and the plurality of executable units. Execution of the execution block completes the computer-mediated task. The method also includes providing an auxiliary user interface including an execution block.
In some implementations, the method further includes receiving a user input indicating activation of the execution block and executing the execution block in response to the user input. In some embodiments, the method further includes providing a prompt to request a parameter for a particular executable unit of the sequence and receiving a user response including the parameter.
In some implementations, the auxiliary user interface is provided by a virtual assistant application, and wherein execution of the execution block includes invoking, by the virtual assistant application, a respective software application associated with each of two or more of the plurality of executable units. In some implementations, the call includes executing, by the virtual assistant application, the API call using an API definition of the respective software application. In some implementations, the invocation includes the virtual assistant application automatically navigating a user interface provided by the corresponding software application. In some implementations, automatically navigating the user interface includes identifying a user interface provided by the respective software application and, based on the identification, automatically providing one or more of a click input, a touch input, a voice input, or a keyboard input to the respective software application.
In some implementations, generating the execution block includes mapping each of the plurality of operations of the user tour to a particular executable unit of the plurality of executable units.
In some implementations, analyzing the API definitions of the plurality of software applications includes obtaining semantic annotations associated with each API definition. In these embodiments, the method further includes mapping the API associated with the API definition to a particular executable unit of the plurality of executable units based on the semantic annotation.
In some implementations, analyzing the UI elements of the plurality of software applications includes mapping each UI element to a particular executable unit of the plurality of executable units. In some implementations, the UI element includes a display UI element. In these implementations, analyzing the display UI elements includes identifying one or more of a shape, a size, a placement, text, or graphical content of each display UI element. In these embodiments, the method further includes matching the display UI element with a particular executable unit based on the identification of the shape, size, placement, text, or graphical content of the display UI element.
In some implementations, the UI elements include audio UI elements. In these implementations, analyzing the UI elements includes utilizing speech recognition techniques to detect one or more of tones of the audio UI elements or text of the audio UI elements. In these embodiments, the method further includes matching the audio UI element to a particular executable unit based on the tone or text.
In some implementations, the user interaction data includes a plurality of user interaction events, and analyzing the user interaction data includes classifying the plurality of user interaction events into a plurality of event clusters. In these embodiments, the method further includes mapping each of the plurality of user interaction events to a particular one of the plurality of executable units based on the plurality of event clusters. In some implementations, the plurality of user interaction events includes a start event indicating a start of the computer-mediated task and an end event indicating an end of the computer-mediated task. In some implementations, the plurality of user interaction events includes at least one event associated with each of a first software application and a second software application of the plurality of software applications.
In some implementations, the auxiliary user interface further includes one or more other execution blocks, each other execution block associated with a respective computer-mediated task. In these implementations, providing the secondary user interface includes determining the usage context based on one or more of a system state of the client computing device or a usage pattern associated with the client computing device. In these implementations, providing the secondary user interface further includes determining that the computer-mediated task and the corresponding computer-mediated task match a context of use of the computing device.
Some implementations include a computing device including a processor and a memory coupled to the processor. The memory has instructions stored thereon that, when executed by the processor, cause the processor to perform operations comprising identifying a user itinerary comprising a plurality of operations to perform computer-mediated tasks by analyzing data comprising user interaction data. The operations further include analyzing one or more of an Application Programming Interface (API) definition of the plurality of software applications or a User Interface (UI) element of the plurality of software applications to identify the plurality of executable units. The operations also include generating an execution block defining a sequence of two or more of the plurality of executable units based on the user tour and the plurality of executable units. Execution of the execution blocks, e.g., by a processor, completes the computer-mediated task. The operations also include providing an auxiliary user interface that includes an execution block.
In some implementations, analyzing the API definitions of the plurality of software applications includes obtaining semantic annotations associated with each of the API definitions and mapping the API associated with the API definition to a particular executable unit of the plurality of executable units based on the semantic annotations.
In some implementations, the user interaction data includes a plurality of user interaction events. In these embodiments, analyzing the user interaction data includes classifying the plurality of user interaction events into a plurality of event clusters. The operation of analyzing the user interaction data further includes mapping each of the plurality of user interaction events to a particular one of the plurality of executable units based on the plurality of event clusters.
In some implementations, the computing device is a server and the secondary user interface further includes one or more other execution blocks, each other execution block associated with a respective computer-mediated task. In these implementations, providing the secondary user interface includes causing the secondary user interface to be displayed by a client computing device other than the server; determining a usage context of the client computing device based on one or more of a system state of the client computing device or a usage pattern associated with the client computing device; and determining that the computer-mediated task and the corresponding computer-mediated task match a usage context of the client computing device.
Some embodiments include a non-transitory computer-readable medium having instructions stored thereon that, when executed by a processor, cause the processor to perform operations including identifying a user itinerary by analyzing data including user interaction data, the user itinerary including a plurality of operations to perform computer-mediated tasks. The operations further include analyzing one or more of: an Application Programming Interface (API) of the plurality of software applications defines or User Interface (UI) elements of the plurality of software applications to identify the plurality of executable units. The operations also include generating an execution block defining a sequence of two or more of the plurality of executable units based on the user tour and the plurality of executable units. Execution of the execution block completes the computer-mediated task. The operations also include providing an auxiliary user interface that includes an execution block. In some implementations, the operation of generating the execution block includes mapping each of the plurality of operations of the user tour to a particular executable unit of the plurality of executable units.
Drawings
FIG. 1 is a block diagram of an example network environment that may be used for one or more embodiments described herein.
FIG. 2A illustrates an example user tour using a computing device to send pictures to another user.
FIG. 2B illustrates another example user tour using a computing device to send pictures to another user.
FIG. 2C illustrates another example user tour using a computing device to send pictures to another user.
FIG. 3 is a block diagram illustrating an example method for providing an auxiliary user interface, according to some embodiments.
Fig. 4A-4C each illustrate an example of using an execution block.
FIG. 5 is a block diagram of an example device that may be used in one or more embodiments described herein.
Detailed Description
Embodiments described herein relate to the creation and use of execution blocks comprising a sequence of executable elements that enable a user to perform particular computer-mediated tasks without interaction with various software applications that may include functionality to perform various operations associated with the computer-mediated tasks. In various implementations, the execution block may simplify user interaction with the computing device by reducing the need for the user to make selections, reducing the information density of the user interface, and standardizing the user interface. The execution block may be provided through an auxiliary user interface, which may be provided by a virtual assistant.
Fig. 1 illustrates a block diagram of an example network environment 100, which may be used in some embodiments described herein. In some implementations, the network environment 100 includes one or more server systems, such as the server system 102 and the second server system 140 in the example of fig. 1. For example, server systems 102 and 140 may be in communication with network 130. The server system 102 may include a server device 104 and a database 106 or other storage device. In some implementations, the server device 104 can provide the virtual assistant application 154b and/or the execution block application 156b. The second server system 140 may include a second server device 142 configured to provide one or more applications, such as application a 144, application B146, and application C148. In fig. 1 and the remaining figures, letters following a reference numeral, e.g., "156a", denote references to elements having that particular reference numeral. Reference numerals, such as "156", which are not followed by letters herein, represent general references to embodiments of elements with such reference numerals.
Network environment 100 may also include one or more client devices, such as client devices 120, 122, 124, and 126, that may communicate with each other and/or with server system 102 and/or second server system 140 over network 130. The network 130 may be any type of communication network including one or more of the internet, a Local Area Network (LAN), a wireless network, a switch or hub connection, and the like. In some implementations, the network 130 may include peer-to-peer communication between devices, e.g., using a peer-to-peer wireless protocol (e.g., Wi-Fi direct, etc.), etc. One example of peer-to-peer communication between two client devices 120 and 122 is shown by arrow 132.
For ease of illustration, fig. 1 shows one block for server system 102, server device 104, database 106, second server system 140, and second server device 142, and four blocks for client devices 120, 122, 124, and 126. Server blocks 102, 104, 106, 140, and 142 may represent multiple systems, server devices, and network databases, and these blocks may be provided in different configurations than shown. For example, server system 102 and/or second server system 140 may represent multiple server systems that may communicate with other server systems over network 130. In some implementations, for example, the server system 102 and/or the second server system 140 can include cloud-hosted servers. In some examples, database 106 and/or other storage devices may be provided in server system box(s) separate from server device 104 and may communicate with server device 104 and other server systems over network 130.
Further, there may be any number of client devices. Each client device may be any type of electronic device, such as a desktop computer, laptop computer, portable or mobile device, cellular telephone, smart phone, tablet computer, television set-top box or entertainment device, wearable device (e.g., display glasses or goggles, wristwatches, head-mounted viewers, arm-bands, jewelry, etc.), personal Digital Assistant (PDA), media player, gaming device, etc. Some client devices may also have a local database similar to database 106 or other storage. In some implementations, the network environment 100 may not have all of the components shown and/or may have other elements, including other types of elements in place of or in addition to those described herein.
In various implementations, end users Ul, U2, U3, and U4 may communicate with server system 102 and/or each other using respective client devices 120, 122, 124, and 126. In some examples, users Ul, U2, U3, and U4 may interact with each other through applications running on respective client devices and/or server systems 102 or second server system 140 and/or through network services (e.g., social networking services or other types of network services) implemented on server systems 102 or second server system 140. For example, the respective client devices 120, 122, 124, and 126 may communicate data to and from one or more server systems (e.g., system 102, second server system 140).
In some implementations, the server system 102 and/or the second server system 140 can provide appropriate data to the client devices such that each client device can receive the transmitted content or shared content and/or web services uploaded to the server system 102 or the second server system 140. In some examples, users U1-U4 may interact through audio or video conferences, audio, video, or text chat, or other communication modes or applications.
The web services implemented by server system 102 or second server system 140 may include systems that allow users to perform various communications, form links and associations, upload and publish shared content such as images, text, video, audio, and other types of content, and/or perform other functions. For example, the client device may display received data, such as content distribution and/or web services sent or streamed to the client device and originating from a different client device (or directly from a different client device) or from a server system through a server and/or web service. In some implementations, the client devices may communicate directly with each other, e.g., using peer-to-peer communication between the client devices as described above. In some embodiments, a "user" may include one or more programs or virtual entities, as well as people interfacing with a system or network.
In some implementations, any of the client devices 120, 122, 124, and/or 126 can provide one or more applications. For example, as shown in fig. 1, the client device 120 may provide a virtual assistant application 154a, an execution block application 156a, and one or more other applications 158. Client devices 122-126 may also provide similar applications. Virtual assistant application 154a, execution block application 156a, and other applications 158 may be implemented using hardware and/or software of client device 120. In different implementations, the virtual assistant application 154a and the execution block application 156a may each be separate client applications executing on any of the client devices 120-124, for example, or may work in conjunction with the virtual application 154b and the execution block application 156b provided on the server system 102.
The virtual assistant application 154 may provide various functions. For example, such functionality may include one or more of providing an auxiliary user interface, interacting with a user through a conversational user interface, responding to a user query, performing one or more operations in response to a user request, running an execution block, and so forth. The virtual assistant application 154 may invoke execution of the block application 156 and/or any other application, such as any application from other applications 158, automatically or upon user request.
Execution block application 156 may provide various functions. In some implementations, the execution block application can generate the execution block based on the user interaction data. In different embodiments, the execution blocks may be stored, for example, in database 106. In some implementations, the execution block application 156 can analyze the user interaction data to generate an execution block. In some implementations, the execution block application 156 can generate semantic annotations for one or more execution blocks. In some implementations, the execution block application 156 can edit or customize execution blocks based on, for example, versions of one or more other applications 158, based on an operating system of the client device 120 and/or the server device 104, and/or the like. In some implementations, the execution block application 156 can provide access to a library of execution blocks.
In some implementations, the client device 120 may include one or more other applications 158. For example, other applications 158 may be applications that provide various types of functionality, such as calendars, address books, email, web browsers, shopping, traffic (e.g., taxis, trains, airline reservations, etc.), entertainment (e.g., music players, video players, gaming applications, etc.), social networks (e.g., messaging or chat, audio/video conversations, shared images/video, etc.), and so forth. In some implementations, one or more other applications 158 may be stand-alone applications executing on the client device 120. In some implementations, one or more other applications 158 may access a server system, such as server system 102 and/or second server system 140, that provides data and/or functionality of other applications 158. For example, any of the applications 144, 146, and 148 shown as being provided by the second server system 140 may provide data and/or commands to one or more other applications 158. In some implementations, the server applications 144-148 can be stand-alone applications accessed by client devices, such as through a web browser or other client-side program.
The user interface, for example, which is provided by the virtual assistant application 154, the execution block application 156, or other application 158 and displayed on the client devices 120, 122, 124, and/or 126 as an on-screen visual user interface, may include user content and other content, including images, video, data, and other content, as well as communications, privacy settings, notifications, and other data. Such a user interface may be displayed using software on a client device, software on a server device, and/or a combination of client software and server software executing on server device 104 and/or second server device 142 (e.g., application software or client software in communication with server system 102 and/or second server device 142). The user interface may be displayed by a display device (e.g., a touch screen or other display screen, projector, etc.) of the client device or the server device. In some implementations, an application running on a server system may communicate with a client device to receive user input at the client device and output data, such as visual data, audio data, and the like, at the client device. In some implementations, the user interface may include an audio user interface in addition to or in lieu of a visual user interface.
Other implementations of the features described herein may use any type of system and/or service. For example, other networking services (e.g., connected to the internet) may be used in place of or in addition to social networking services. Any type of electronic device may utilize the features described herein. Some implementations may provide one or more features described herein on one or more client or server devices that are disconnected from or intermittently connected to a computer network. In some examples, a client device that includes or is connected to a display device may display content publications stored on a storage device local to the client device, such as previously received over a communication network.
FIG. 2A illustrates an example user tour 200 that uses a computing device to send pictures to another user. User interface states 210A-210F illustrate a user tour. As shown in fig. 2A, the home screen (202) of the user's computing device includes a plurality of application icons, for example, for various applications (apps), such as "contact list," gallery, "" email, "" IM App 1, "" IM App 2, "and" camera.
The contact list app may enable a user of the computing device to access a contact list including a contact name, a phone number, an email id, an instant message identifier, and the like. Gallery apps may enable users to view, edit, or share images, such as photographs and/or videos. The email app may enable a user to send and receive email via a computing device. IM app 1 and IM app 2 may each enable users to engage in instant messaging conversations through different Instant Messaging (IM) service providers. The camera app may enable a user to capture pictures or video through a camera of the computing device.
The user may select an app icon by launching an application, for example, by via touch, gesture, or click; through voice command; etc., to access application functions of these and other applications of the computing device. In addition, applications may provide Application Programming Interfaces (APIs) that may be used to programmatically access application functions. For example, the gallery app may utilize an application API of the IM app to automatically launch the IM app in a particular state.
A user of a computing device may utilize the computing device to perform one or more computer-mediated tasks using an application. Each task may include a plurality of operations that together constitute a user's journey to accomplish the computer-mediated task. For example, FIG. 2A illustrates an example user tour in which a user uses a computing device to complete a task of sending pictures to another user ("mom").
As shown in fig. 2A (210A), when the user selects the contact list app icon on the home screen (202), the user's journey starts. In response to the user selection, a contact list app (204) is launched. The contact list app (shown in 210B) shows a list of contacts of the user, such as "Jane Doe", "John Doe", "mom", and the like. The contact list app enables the user to select a particular contact, e.g., "mom". In response to the user selecting a particular contact, the user interface is updated to display details of the contact (210C), such as a mom's phone number (SMS number) that can receive messages through the SMS service, an email address, and an instant message address.
The user itinerary also includes specific details that the user selects to be associated with the contact "mom" in the contact list app (204), e.g., an instant messaging service that identifies the contact ("alice.m") in the instant messaging service IM 1. In response to the user selection, a corresponding instant message app (e.g., "IM 1") is launched and a user interface of the IM app (206) is displayed on the computing device (210D). As shown, the user interface enables the user to enter a message ("hi-here some pictures … …") and include selectable options ("add pictures").
The user itinerary also includes a user selection option. In response to the selection, a gallery app (208) is launched and a user interface of the gallery app is displayed (210E). The user interface displays a plurality of pictures (shown as blank rectangles) and enables the user to select a picture (shown with check marks) to be sent to the mother through the IM App (206). After the user has completed selecting and selecting the "send" button, the picture is sent to the mother through the IM App and the user interface is updated accordingly (210F).
In many computing devices, alternative user tours may accomplish the same task. FIG. 2B illustrates another example user tour 214 that uses a computing device to send pictures to another user.
As shown in FIG. 2B, an alternate user tour begins by the user selecting gallery app (208) from home screen (220A). In response, a user interface of gallery app (208) is displayed (220B) on the computing device. The user interface enables the user to select a picture and select the "share with. When the user selects a picture and presses a button, a contact list app (204) is displayed. The user scrolls through the contact list and selects the user's mother. In response to the selection, contact details are displayed (220D). The user then selects the email option. In response, an email app (212) is launched, for example in an email composition mode, wherein the selected picture is included as an attachment, and the recipient: the field is populated with an email address associated with the contact "mom". The user may then type in an email message (e.g., add the subject line "birthday picture" and text content) and send an email containing the picture by selecting the "send email" button.
FIG. 2C illustrates another example user tour 222 that uses a computing device to send pictures to another user. As shown in FIG. 2C, an alternate user tour begins by the user selecting an IM app (206) from the home screen (230A). In response, a user interface of the IM app (206) is displayed (230B) on the computing device. The user interface enables a user to select a particular chat conversation from a list of chat conversations (e.g., between the user and other users Jane Doe, john Doe, and mom). When the user selects a conversation with mom, detailed information of the chat conversation is displayed (230C). The user then enters a message ("hi-here some pictures … …") and selects the "send pictures" button. In response to the selection, a gallery app (208) user interface is displayed (230D). The user selects the picture and selects the "done" button. In response, the picture is sent through the IM app (230E).
In the user tour shown in fig. 2A, to complete the task of "share pictures with mom", the user needs to perform several operations-selecting a contact list app, scrolling through the contact list and selecting mom, selecting an IM app, selecting an add picture option in the IM app, and selecting a picture and selecting to send. Further, the operations performed span multiple applications-home screen, contact list app, IM app, and gallery app.
In the user tour shown in FIG. 2B, to complete the task of "share pictures with mom," the user needs to perform a number of operations—select gallery app, select pictures (which may include scrolling), select email addresses of contacts from a contact list (which may also include scrolling), and select the "send email" option after composing the mail. The operations performed span multiple applications-home screen, gallery app, contact list app, and email app.
In the user tour shown in fig. 2C, to complete the task of "share pictures with mom", the user needs to perform a number of operations—select IM app, select a specific chat conversation, select "send pictures" button, select pictures through gallery app (which may include scrolling), and select "complete" button. The operations performed span multiple applications-home screen, IM app, and gallery app.
In each of these user tours, the user utilizes multiple applications and corresponding user interfaces to accomplish tasks. In performing different tasks, a user needs to perform many operations to complete the tasks, even in a single application.
Execution of computing tasks that require manipulation of multiple software applications may be a source of user dissatisfaction. For example, each application used in the user itinerary shown in FIGS. 2A-2C may have a different design language and may provide a different and possibly incompatible way of performing the same action. When users switch between these different applications in order to perform tasks, they may feel disoriented or cognitive burden in doing so. This may be true even if both applications are designed well alone.
Furthermore, the difficulty may be higher when the application runs on different devices; for example, when completing a task requires manipulating an application on a smart watch and at the same time an application on an automobile entertainment system. There are many tasks where such "application-switching" is the only way to accomplish the tasks, and performing these tasks can place a significant cognitive burden on the user if there is no continuity in the user experience.
When a user uses a UI to complete a task, the process involves a certain amount of cognitive functions. For example, in each user trip shown in FIGS. 2A-2C, the user needs to look at the screen and understand the placement of different pieces of information. The user must read and understand the information presented on the screen and make a decision on the next ambulatory effort. The user must also perform a series of operations to navigate through a set of screens in a planned sequence. Such navigation may involve a discovery process by trial and error when a user attempts to find a function that they may not have used before. In each step, the user may have to enter in text form (e.g., search input of a search app, contacts, pictures, etc.; message content of an IM or email message); audio (e.g., using a voice-use application; selecting an image using voice; etc.); gestures or touches (e.g., mouse movements and clicks, or touch screen inputs to select pictures, select buttons, and other UI elements); and the like, to provide input to the computing device.
Through a user's journey, the user also needs to retain certain elements of their intent and actions in short-term memory. The integration of all of these cognitive functions requires the user to pay a certain amount of attention to the UI and to exercise the executive functions to control their behavior during the execution of tasks using the UI.
Thus, using a computing device to accomplish a task requires a user to exercise memory, reasoning, solving problems, prior knowledge, attention, comprehension, perform functions, and the like. Depending on the inherent complexity of the task and the available applications to perform the task, such user interactions may be overly demanding for the user, resulting in one or more failures before the task is successfully completed. This can lead to complaints of a feeling of hating and depression.
Furthermore, the user interface may contain inherent developer assumptions that may make certain groups of users inaccessible to the UI. For example, such assumptions may relate to language capabilities, making a UI that is easy for a person in native English (for example) to use difficult for English learners to use. Other assumptions may not hold when the UI is used by people with limited cognitive abilities.
Furthermore, if a user uses a variety of different devices, each having its own input and output capabilities and modalities, these assumptions may also make it difficult for the user interface to be used in a consistent manner across the different device classes. For example, while a device with a screen may include a UI that presents a large number of options in the form of icons that enable a user to quickly scan and select a particular option, presenting the same set of options through a voice interface may be overwhelming to the user because the user needs to exercise short-term memory to remember the options. The user may not be happy to wait for the voice interface to present all options. Furthermore, when options must be presented to the user in an environment such as an automobile or while in kitchen work, the process of scanning and selecting options may require attention and levels of attention that the user is unable to focus on. User interfaces do not always switch easily between different types of devices and require costly modifications to access functionality across multiple platforms.
At least some of the embodiments described herein provide techniques to address these issues. The technique provides an auxiliary user interface with execution blocks that expose application functions in a consistent, user-friendly manner to assist a user in completing tasks.
FIG. 3 is a flowchart illustrating an example method 300 for providing an auxiliary user interface, according to some embodiments. In some implementations, the method 300 may be implemented, for example, on the server system 102 as shown in fig. 1. In some implementations, some or all of the method 300 may be implemented on one or more client devices 120, 122, 124, or 126, one or more server devices, and/or server device(s) and client device(s) as shown in fig. 1.
In the depicted example, the implementation system includes one or more digital processors or processing circuits ("processors") and one or more storage devices (e.g., database 106 or other storage). In some implementations, different components of one or more servers and/or clients may perform different blocks or other portions of method 300. In some examples, a first device is described as performing blocks of method 300. Some implementations may cause one or more blocks of method 300 to be performed by one or more other devices (e.g., other client devices or server devices) that may send results or data to the first device.
The method 300 may begin at block 302. At block 302, it is checked whether user consent (e.g., user permissions) to use the user data has been obtained in an embodiment of the method 300. For example, the user data may include user interaction data such as click streams, user schedules, user data related to using a messaging application, user preferences, user biometric information, user characteristics (e.g., identity, name, age, gender, occupation, user's awareness and/or athletic ability, etc.), information about the user's social network and contacts, social and other types of actions and activities, content created or submitted by the user, ratings and opinion, the user's current location, historical user data (such as usage patterns associated with software applications on the user's computing device), images generated, received and/or accessed by the user, images viewed or shared by the user, and so forth. One or more blocks of the methods described herein may use such user data in some implementations.
In method 300, if user consent has been obtained from the relevant user that user data may be used, then in block 304, it is determined that the method herein may be implemented with the user data described for these blocks available, and the method continues to block 308. If no user consent is obtained, then in block 306 it is determined that the block is not to be implemented using user data, and the method continues to block 308. In some implementations, if user consent is not obtained, the block is implemented without using user data and using comprehensive data and/or data that is commonly or publicly accessible and publicly available. In some embodiments, if no user consent is obtained, the method 300 is not performed.
At block 308, the user itinerary is identified. The identification of the user itinerary may include identifying a plurality of operations to perform computer-mediated tasks. In the case of user permissions, user interaction data may be obtained to identify the user itinerary. In various embodiments, the user interaction data may include a sequence of actions performed to complete a computer-mediated task. For example, the user interaction data may include a click stream, e.g., a set of user-performed actions, such as a tap at a particular location on the touch screen device (e.g., corresponding to a user interface element such as an icon, button, etc.); clicking on a specific location, for example using a mouse or touch pad; providing keystroke input through a keyboard or other device; executing a gesture; or other input operations.
In some implementations, the user interaction data can include one or more voice commands, for example, voice commands provided to the virtual assistant. In some implementations, the user interaction data may include a chat sequence, such as a chat between a user and a virtual assistant, with text corresponding to various events entered by the user, such as commands or requests provided to the virtual assistant.
In some implementations, the user interaction data can include, for example, event logs stored by applications of the user device and/or the device operating system. For example, the event log may include a sequence of events determined based on user interactions with the application, such as selecting menu options, scrolling, clicking a button, and the like. In another example, the event log may include a sequence of events determined based on an automated test suite, e.g., operations for automatic execution of a test software application.
In some implementations, multiple events in user interaction data obtained, for example, as a click stream, through an event log, or from a test suite, can be classified into one or more event clusters using a clustering technique (e.g., machine learning based or other clustering technique). For example, event clusters may include a "start event" (associated with the start of a computer-mediated task) and an "end event" (associated with the end of a computer-mediated task) for each user trip. For example, in a user journey to reserve a taxi, the start event may be the launching of a taxi reservation application or website. Continuing with the example of taxis, other event clusters may include clusters associated with events in the interaction data corresponding to specified start and end addresses, confirmed journey prices, specified taxi types, specified payment methods, authorized payments, and the like. The ending event may be identified as authorization for payment or to provide feedback for taxi taking.
In some implementations, the user interaction event can be associated with a different application. For example, in a taxi example, the first application is a taxi booking application having an input address, a selection of a taxi type, a designation of a payment method, and an event confirming a price associated with the taxi booking application, and the second application may be an e-wallet application having an event authorizing payment associated with the e-wallet application.
The identified user itinerary may include a sequence of operations to perform computer-mediated tasks. Each operation in the sequence may be annotated to indicate the position of the operation in the sequence. Operations in the sequence may also be annotated to include other information, such as a software application for performing the operation, a displayed UI corresponding to the operation, a user selection of a particular user interface element of the displayed UI, information provided to complete the operation, and so forth. Furthermore, if event logs are utilized to identify user tours, operation-triggered events, such as updates to the file system, commands sent to the browser, etc., may also be stored as annotations of the operation. Block 308 may be followed by block 310.
At block 310, a plurality of software applications (e.g., one application, two applications, or a plurality of applications) are analyzed to identify a plurality of executable units of the software applications. In some implementations, analyzing the software application may include analyzing Application Programming Interface (API) definitions of the plurality of software applications. In some implementations, analyzing the software application can include analyzing User Interface (UI) elements of the plurality of software applications.
In some implementations, analyzing the API definition can include obtaining semantic annotations associated with the API definition. For example, the semantic annotations may include API names, such as "GetAddress", "GetPhotos", and the like. The semantic annotations may also include API specifications, for example, provided by a developer of the software application. The API specification may indicate, for example, how to call the API and associated input parameters, and/or what the API returns, such as output parameters. Analysis of the API definition may also include accessing functionality of the software application that is registered with a device operating system of a device on which the application may execute, e.g., such that an Operating System (OS) or other application may access the functionality by calling the API.
Based on semantic annotations (e.g., API names, API specifications, functions registered with the device OS, etc.), APIs associated with API definitions may be mapped to particular executable units. For example, based on the API name "GetAddress," the API may map to an executable unit that, when run on a processor, obtains an address, such as through a sensor on a GPS or other device, through user input, or the like. Different APIs may be mapped to different executable units. Further, a single API may be mapped to different executable units, for example, when parameters associated with the API are available to specify different functions of the API. Parameters associated with the API may also be utilized to identify one or more parameters of a particular executable unit.
In some implementations, analyzing User Interface (UI) elements of the software application can include mapping each UI element to a particular executable unit. For example, image analysis and recognition techniques and/or Optical Character Recognition (OCR) techniques may be used to recognize that UI buttons in a software application are marked as "send" (text on UI elements) or have corresponding graphical content, e.g., icons such as "paper airplane icons. Based on the identification, the UI element may be matched with a particular executable unit that provides a corresponding function (e.g., a "send" function) within the application context. For example, if the software application is an electronic wallet, the "send" button may match the "send money" executable unit, while if the software application is an instant messaging application, the "send" button may match the "send message" executable unit. Furthermore, a particular executable unit may be configured with one or more parameters based on the UI. For example, if a "send money" UI element is placed next to a text box with a monetary value as input, a particular executable unit may be set with a "monetary" parameter that will be obtained to execute the particular executable unit.
In addition, other UI elements that appear on the user interface as particular UI elements may be used to determine a context, which may then be used to identify the particular executable unit to which the UI element is mapped. For example, a UI element including an up arrow, such as "∈f placed in a UI including a" file transfer "menu, may provide a context in which the UI element corresponds to" upload ". Furthermore, the shape, size, or placement of the UI elements may also be used to map the UI elements to particular executable units.
In some implementations, the UI elements may include audio UI elements, e.g., audio UI elements that are aloud to the user. In these embodiments, analyzing the audio UI element may include detecting a tone of the audio UI element or text of the audio UI element using a speech recognition technique. Based on the detected tone and/or text, the audio UI element may be matched with a particular executable unit. For example, with a parameter "song identifier" to be obtained to execute a music playing executable unit, for example, prompt the user "what song does you want to play? An "audio UI element may be associated with a music play executable unit. In some implementations, the tones and/or text can be analyzed to determine the input of the audio UI element. For example, the text "what song you want to play? "may be determined to require the parameter" song identifier "as an input. Block 310 may be followed by block 312.
At block 312, an execution block is generated based on the user itinerary identified in block 308 and the plurality of executable units identified in block 310. In some implementations, an execution block may define a sequence of two or more of the plurality of executable units that, when executed, accomplish computer-mediated tasks for a particular user trip. In some implementations, generating the execution block may include mapping each of the plurality of operations of the user tour to a particular executable unit of the plurality of executable units.
For example, the user itinerary shown in fig. 2A-2C may be mapped to an associated plurality of executable units, each of which performs a respective operation, such as "identify contact address and modality" (e.g., alice. M on IMl), "select photo" (e.g., from a photo gallery of the user), and "send over IM" (e.g., over IM 1) executable blocks. The executable unit may be determined as described above with reference to block 310. Furthermore, execution blocks may combine the executable units in a sequence such that the corresponding software application from which the executable unit is obtained may be invoked during execution of the execution block.
For example, running the execution unit may include invoking the corresponding software application by executing an API call on the software application using the corresponding API definition or automatically navigating a user interface of the software application. For example, the execution blocks may be provided by a virtual assistant on the computing device, and the virtual assistant application may invoke corresponding software applications defined in a sequence of executable units of the execution blocks. In some implementations, the software application may call through an API such that the user interface is not displayed when the executable unit is run. In some implementations, the software application may be silently invoked such that the user interface of the software application is not displayed to the user, but automatically navigates as specified in the executable unit.
In some implementations, automatically navigating the user interface provided by the respective software application may include the virtual assistant identifying the user interface provided by the respective software application and automatically providing a click input, touch input, voice input, or keyboard input to the respective software application based on the identification. In some implementations, the automatic navigation may include the virtual assistant automatically triggering events associated with button clicks, menu selections, scrolling, or other operations with reference to a user interface provided by the corresponding software application.
The identifying may include matching the user interface with user interface information stored as part of the executable unit (e.g., identifying a "send" button). In some implementations, matching may be performed by utilizing computer vision techniques. Block 312 may be followed by block 314.
At block 314, an execution block may be provided for selection by the user. In some implementations, an auxiliary user interface can be provided that includes the execution block and other execution blocks, each execution block associated with a respective computer-mediated task. For example, the auxiliary user interface may be provided by a virtual assistant. In some implementations, the auxiliary user interface can be a visual display, for example, that includes a plurality of icons, each icon corresponding to a particular execution block. In some implementations, the auxiliary user interface may be provided as an audio UI, for example, by an audio or visual prompt such as "say send photo to X", "say view my beach photo". The user may activate a particular execution block by selecting from a visual display, by providing verbal commands, and the like.
In some implementations, the auxiliary user interface may be displayed on a computing device, such as a client computing device. In some implementations, the auxiliary user interface may be provided based on a context of use of the computing device. For example, if a large number of execution blocks are available, a subset of execution blocks that fit into a context (e.g., may be used by a user) may be identified and provided through an auxiliary user interface. In some implementations, a context of use of a computing device may be determined based on a system state of the computing device and/or a usage pattern associated with the computing device with user permissions. In these implementations, an execution block may be selected for the auxiliary user interface by determining whether a computer-mediated task associated with the execution block matches a context of use of the computing device.
For example, the system state may include system parameters such as current time, current location, whether the computing device was recently used for a particular action, such as taking a photograph, and the like. For example, if the current location is different from home, an execution block associated with the "order taxi to home" task may be displayed, while if the current location is home, an execution block such as "turn on light" may be displayed. In another example, if the computing device was recently used to capture a photo, execution blocks such as "automatically enhancing the most recent photo", "sharing a photo with mom", etc. may be displayed. In another example, if the system state of the computing device indicates a low battery state, the execution block "save power" may be displayed. For example, the "power saving" execution block may include an executable unit that performs actions such as turning on a low power consumption mode of the device, reducing screen brightness, putting a power-consuming app to sleep, and the like.
In the event of user permission, a usage pattern associated with the computing device may be identified and used to select execution blocks to be displayed in the secondary user interface. For example, if allowed by the user, data such as the time certain execution blocks are typically used, calendar information for the user, usage data associated with various software applications, etc., may be used to determine a usage pattern of the computing device. For example, if the user has a mode of playing podcasts using the computing device while driving on duty, an execution block such as "play the latest podcasts" may be displayed when it is determined that the user is likely to be on duty based on the location data or calendar information of the computing device. In another example, if the user's calendar indicates a daily routine, such as an exercise appointment, a corresponding execution block, such as "play exercise music," may be displayed.
In some implementations, the auxiliary user interface may be customized based on user preferences or other user-specific factors. For example, users with limited physical ability to perform accurate touch gestures, such as users with essential tremors, suffering from parkinson's disease, etc., may be provided with UIs that include large or sticky icons or otherwise customized to receive input from such users or through audio. In another example, users with limited cognitive abilities, e.g., users that do not read the UI language, users with difficulty perceiving certain colors or shapes, users with memory impairment, etc., are provided with a suitable auxiliary user interface that takes into account the user's abilities, e.g., they display graphical icons, provide navigational cues, memory assistance, etc. For example, the auxiliary user interface may be a conversational user interface in which the virtual assistant obtains user input regarding execution block parameters in a direction-by-direction manner.
In some implementations, the user or another user, such as a parent or caretaker, can provide information about the execution blocks, which can be used to select the execution blocks to be provided through the auxiliary user interface. For example, a parent may specify that an execution block such as "call mom" or "tell me" is provided through an auxiliary user interface in a particular context, e.g., when a child goes home, goes to bed, etc. The information may include contextual factors associated with the execution block, the device on which the execution block is provided, configuration of an auxiliary user interface, such as color scheme, layout, icon size, speech language, etc.
In some implementations, the secondary user interface may be provided based on contextual factors obtained under user permission. The auxiliary user interface may be presented in different manners on different computing devices. For example, on a device with a touch screen display (such as a smart phone, tablet computer, or smart display), the auxiliary user interface may be presented as a grid of icons, while on a smart speaker without a display, the auxiliary user interface may be presented by an audio prompt, such as "welcome home, do you want to turn on a light? "to provide the user with easy access to the" turn on lights at home "execution block. Such customization of the presentation assistance user interface may make the execution block accessible on the device and in use cases where selection from a long list is difficult, such as voice input devices, cognitively impaired users, etc. Block 314 may be followed by block 316.
At block 316, a user input indicating activation of a particular execution block may be received from the secondary user interface. In some implementations, if the execution block is configured to accept user input for one or more parameters, a further user interface may be provided that enables the user to specify one or more parameters.
For example, providing a further user interface may include providing a prompt to the user to request parameters for a particular executable unit of the sequence (e.g., specifying a contact to which to send a photograph, specifying a departure address for a taxi reservation, etc.). A user response may be received that includes the parameters and may be used during execution of the execution block. In some implementations, a virtual assistant providing an auxiliary user interface may also present a further user interface, providing a consistent user interaction experience for the user, even if a particular execution block includes executable units that utilize different software applications. In response to user input, the execution block may be run to complete the computer-mediated task. Block 316 may be followed by block 318.
At block 318, an execution block is run. For example, a sequence of executable units of an execution block may be run. If the user provides input of one or more parameters, the execution block is run using the user-specified parameters. For example, if the user selects the execution block of "share photos with mom," the sequence of executable units may include a first unit for identifying an address and using a modality, a second unit for selecting a photo, and a third unit for activating the selected modality and sending the photo.
For example, the address and modality may be determined by running the first unit to identify the mom's IM address through IM 1. Furthermore, in case of permission of the user, the selection of photos may be performed automatically by the second unit, e.g. photos that have not been previously shared with the mother, such as photos that have been recently taken. Alternatively or additionally, the selection of photos may be performed based on user input parameters, such as "birthday photo", "yesterday photo", etc. The selected modality/address and photograph may be provided to a third unit, which may send the photograph over IM 1.
In some implementations, execution blocks may be stored at different levels of abstraction or hierarchies. For example, when an execution block is generated based on user interaction data (e.g., selection of a particular icon or other user interface element or a particular location within a user interface via gestures, touch inputs, etc.), an executable unit within the execution block may be represented as an automatically provided input on a particular UI view of a software application associated with the executable unit. In this example, running the execution block may include running a software application and automatically providing icon selection or other user interaction. In different embodiments, the UI of the software application may be displayed (allowing the user to view the execution blocks in the actions and learn the sequence of actions) or may be hidden from view.
At a higher level of abstraction, the corresponding semantics can be used to represent input operations (gestures, touch inputs, etc.) as well as user interfaces of software applications (e.g., send buttons, save buttons, etc.). The representation may enable the executable unit to be transferred across software application versions and device configurations. For example, the executable unit may specify at a high level of abstraction that the action is selecting a send button, and the virtual assistant invoking the execution block may analyze the software UI to identify the send button and automatically perform the selection by appropriate input (e.g., a tap or click action). At this interaction level, the virtual assistant can simply map executable elements defined with reference to one software application to another similar software application based on the user interface of the respective application.
In some implementations, semantics associated with the executable unit may be automatically inferred. For example, internal states of a software application, such as button tags and accessibility notes, may be used to infer abstractions, e.g., through a machine learning model. In some implementations, the machine learning model may be trained by executing a software application in a simulated environment and mapping various user interface elements of the application to operations in a user's journey using reinforcement learning.
In some implementations, the method 300 or portions of the method may be initiated automatically by the system. In some implementations, the implemented system is a first device. For example, the method (or portions thereof) may be performed periodically or based on one or more specific events or conditions, such as user initiation of a user tour, user provision of commands, and the like.
In some implementations, the method 300 may be performed by a server device (e.g., the server device 104), a client device (e.g., any of the client devices 120-126), or a combination of the server device and the client device. For example, in some implementations, blocks 308-312 may be performed by a server device to generate execution blocks and blocks 314-318 may be performed by a client device to provide an auxiliary user interface that enables a user to select and run execution blocks. In some implementations, the method 300 may be performed entirely by a server device or by a client device.
Auxiliary use: as described herein, the execution block provides a higher level of abstraction, grouping the operations associated with the user itinerary together into a sequence of executable units. This makes the execution block suitable for many use cases where it is not feasible or cumbersome at finer granularity levels. For example, an executive block may be provided that enables users of unfamiliar technology (e.g., elderly patients) to pay, for example, a caregiver. In another example, an execution block may be provided that enables an unfamiliar user (e.g., child) to perform an action such as calling a parent or purchasing a product. In some implementations, an interlock may be provided in which one or more executable units (e.g., money transfers) are executed after approval by another trusted user. In these embodiments, the trusted person may provide approval at the computer-mediated task level associated with the user's intent.
Downloadable execution block: execution blocks generated based on the user itinerary (e.g., from a click stream or other user interaction data) may be provided for download, for example, through an online repository. For example, execution blocks in a repository may be tested in a virtualized environment for different devices and different versions of applications from which individual executable units are obtained. In some implementations, execution blocks in a repository may be automatically and dynamically modified to ensure that they function correctly across different versions and devices. In some implementations, execution blocks in the repository may be automatically (or using input from an expert) organized to highlight execution blocks associated with popular user tours or user tours that are appropriate for a particular user, and ensure that the execution blocks are understandable, functional, and non-malicious. The repository may be presented to the user through a searchable or browsable interface.
Auxiliary user interface through home screen, AAC, or physical buttons: in some embodiments, the execution block of the auxiliary user interface for a particular user may be selected by the user, or by a caregiver or support professional of the user. The selected execution block may be provided by a "home screen" of the virtual assistant or equivalent. In some implementations, if the user uses an enhanced and alternative communications (AAC) application on their device, the selected execution block can be provided as an AAC button. In some implementations, the execution block may be made available to the user as a physical button or as a simpler auxiliary device than a computing device (such as a smart phone). For example, an elderly user may be provided with a single button labeled "call help" configured to trigger an execution block that includes a user tour that sends pre-filled messages to previously configured recipients (e.g., relatives, doctors, emergency professionals, etc.) through a message application.
Generating an execution block based on the other execution blocks: in some implementations, execution blocks may be generated based on previously identified executable units and previously generated execution blocks. For example, a machine learning model may be trained based on known execution blocks to automatically identify executable units from various software applications and to add semantic annotations to the identified executable units. For example, if the known execution blocks include execution blocks that send photos, such as "send photos to user X", "send yesterday photos", etc., that include executable elements corresponding to multiple applications, the machine learning model may be trained to identify component functions of other applications, such as "send" functions in the new instant messaging app, even if the application developer does not provide explicit comments about such functions through the API definition. For example, the user interface of the app may be analyzed by a trained machine learning model to map "paper airplane icons" to "send" executable units (e.g., based on icon shape, size, or placement, or associated user interactions), which may then be added to execution blocks that include such units.
Device independent or virtualized execution block: in some implementations, the auxiliary user interface may be provided by a device that is different from the device on which the execution blocks (or separate executable units) may be run. For example, the supplementary UI may be provided by a wearable device (e.g., a smart watch), and upon selection, the execution block may run on a different device (e.g., a smart phone or laptop) or a virtual device (e.g., an analog device provided by a server). For example, a virtual assistant providing a supplementary UI may run execution blocks using a simulation device. These embodiments allow the supplementary UI to be presented on simpler and/or cheaper devices, or on user-preferred devices, none of which may have the ability to execute one or more executable units of an execution block. Further, in these embodiments, user-specified parameters, such as parameters obtained through user interaction or determined automatically, such as location or other sensor data, may be obtained from the device displaying the supplementary UI and may be provided to the simulation device over a network.
Reducing cognitive load (load) by using execution blocks: when performing computer-mediated tasks, execution blocks as described herein may help reduce user cognitive load. For example, by encapsulating the user itinerary in an execution block (which may reduce the burden of remembering the sequence of operations), allowing for prior configuration or automation, contextual selection of parameters (which may reduce or eliminate the decision burden), and/or presenting an auxiliary user interface customized for the user (which allows users with different cognitive and/or motor capabilities to run the execution block), the execution block may enable the user to perform computer-mediated tasks with a lower cognitive load. Thus, the short term, work, and transactional memory requirements of a user using an execution block may be lower than when performing computer-mediated tasks through a user's itinerary that requires direct use of one or more software applications.
Providing execution blocks on different user devices: the execution blocks described herein may be provided by any user device. Execution blocks may be particularly useful in situations where a user's cognitive and/or motor abilities are limited (e.g., disabled users, users performing computer-mediated tasks concurrently with other activities (e.g., jogging, cooking, driving, etc.), or where a user device from which computer-mediated tasks are performed has limited capabilities (e.g., no screen or has a small screen (e.g., smart speaker, wearable device, etc.)). Further, physical buttons may be provided that enable a user to run the execution block. In some implementations, an association may be established between the execution block and the physical object, and the secondary user interface may include a user presenting the physical object to a camera (or other sensor) to trigger execution of the execution block.
Fig. 4A shows an example of the use of execution blocks. User interface states 410A-410B illustrate a user's journey to complete the task of "send photo to mom" using an execution block. As shown in fig. 4A, a home screen (410A) of a user's computing device displays a virtual assistant (402) that includes a plurality of execution blocks, each execution block having a corresponding icon.
Each icon corresponds to an execution block that enables a user to execute a corresponding task. For example, the icons in fig. 4 correspond to execution blocks for the following tasks: "send photograph to mother", "call taxi home", "speak with milk", "send money", "view vacation photograph" and "self-timer". In different embodiments, the plurality of execution blocks may include any number of execution blocks, such as 1 block, 2 blocks, 5 blocks, 10 blocks, 100 blocks, etc.
The user may select a particular icon, e.g., by touch input, by click, by voice command, etc., to run the corresponding execution block. In the example shown in fig. 4A, the user selects "send photo to mom". In response to the selection, the execution block is run and the photo is sent to the user contact identified as mom and a confirmation is provided to the user (410B). In this example, the user performs a single operation and the task is completed automatically. Intermediate steps of the task, such as navigating the gallery and selecting photos, navigating the contact list and selecting contacts to send photos to, and selecting the modality of sending photos (e.g., via links or URL sharing, via email sending, via instant messaging, etc.), are accomplished automatically by the virtual assistant (402) without further user input. Thus, the amount of input from the user and the cognitive burden of completing the task is much lower than the user tours shown in FIGS. 2A-2C.
In some implementations, the execution block may be parameterized, e.g., configured such that a user running the execution block may provide input for performing certain steps of the execution block. Fig. 4B shows an example of an execution block using parameterization. User interface states 420A-420C illustrate a user's journey to complete the task of "send photo to mom" using an execution block. As shown in fig. 4B, a home screen (420A) of a user's computing device displays a virtual assistant (402) that includes a plurality of execution blocks, each execution block having a corresponding icon.
The user may select a particular icon, e.g., by touch input, by click, by voice command, etc., to run the corresponding execution block. In the example shown in fig. 4B, the user selects "send photo to mom". In response to the selection, the execution block is run. The virtual assistant automatically determines the contact associated with the mother and the modality of sending the photograph. The photograph to be sent is a parameter that the execution block is configured to receive through user input. As shown in fig. 4B, a set of photographs of the user is displayed (420B) and a prompt is provided to the user to select a photograph to send to the mother ("tap or speak to select a photograph").
In some implementations, the execution block can provide an initial selection of photos, e.g., based on the user context. For example, if the user sends photos to the mother on a regular basis, the photo captured after the user last sent the photo to the mother may be automatically selected. Further, a subset of such photos may be selected, for example, based on quality criteria, photo content, and the like. The user may confirm one or more of selecting, selecting additional photos, or deselecting automatically selected photos. After user confirmation, the selected photo is sent to the mother and confirmation is provided to the user (420C).
In this example, the user performs a single operation within the virtual assistant application to select the execution block and provide additional input. Intermediate steps of the task, such as navigating the contact list and selecting contacts to send photos to and selecting the modality to send photos, are accomplished automatically by the virtual assistant (402) without further user input. An execution block may be defined as having one or more parameters entered by a user. Furthermore, the user interfaces presented during execution of the block run are consistent, e.g., using the same design paradigm, thereby reducing the cognitive burden of navigating different UIs of multiple apps. Thus, the amount of input from the user and the cognitive burden of completing the task is much lower than the user tours shown in FIGS. 2A-2C.
Fig. 4C illustrates the execution of a different execution block "call out rental car home". In this example, the user selects a corresponding icon from a home screen (430A) that displays a plurality of execution blocks provided by the virtual assistant (402). In response to the user selection, the execution block is run to complete the task (430B).
In this example, the execution block may perform multiple steps to complete a task. For example, in the event of user approval, the following steps- (a) determining the user's location, e.g., by utilizing an on-board Global Positioning Sensor (GPS) of the user's computing device; (b) Accessing a taxi booking application, for example, navigating an app user interface through an app API or programmatically and providing a location and destination address (home) of the user; (c) confirming departure time and price; and (d) making payment by means of payment authorized by the user. Thus, the user does not need to provide input for these steps, nor does the user have a cognitive burden of navigating the taxi booking application.
Furthermore, execution blocks may be parameterized and simplified user interfaces may be presented within the virtual assistant. For example, the user location may be determined by asking "where? "questions and receives verbal or typed replies" i am in garden "are obtained by user input. In another example, the payment means may be confirmed, for example by asking "how do you want to pay? "questions and receives a reply" credit card ". In addition, other options may be provided to the user to customize the journey, for example, booking regular or top grade taxis; selecting or rejecting a bid; etc.
In this way, the virtual assistant may preserve the customizable nature of taxi reservations while reducing the cognitive burden on the user. For example, when a user has limited ability to provide input (e.g., the user cannot use a touch screen in cold weather, but can provide voice input); when the user is unfamiliar with the interface of the taxi booking application; when the user does not understand the language of the user interface of the taxi booking application; etc., a simplified user interface may be suitable.
Fig. 5 is a block diagram of an example device 500 that may be used to implement one or more features described herein. In one example, device 500 may be used to implement a client device, such as any of client devices 120-126 shown in FIG. 1. Alternatively, device 500 may implement a server device, such as server device 104 or server device 142. In some implementations, the device 500 may be used to implement a client device, a server device, or both a client and a server device. Device 500 may be any suitable computer system, server, or other electronic or hardware device as described above.
One or more of the methods described herein may be run in a standalone program that may be executed on any type of computing device, a program running on a web browser, a mobile application ("app") running on a mobile computing device (e.g., a cellular telephone, a smart phone, a tablet computer, a wearable device (wristwatch, arm band, jewelry, headwear, virtual reality goggles or glasses, augmented reality goggles or glasses, head mounted display, etc.), a laptop computer, etc.). In one example, a client/server architecture may be used, for example, a mobile computing device (as a client device) to send user input data to a server device and receive final output data from the server for output (e.g., for display). In another example, all of the computations may be performed within a mobile app (and/or other apps) on the mobile computing device. In another example, the computation may be split between the mobile computing device and one or more server devices.
In some implementations, the device 500 includes a processor 502, a memory 504, and an input/output (I/O) interface 506. The processor 502 may be one or more processors and/or processing circuits to execute program code and to control the basic operations of the device 500. A "processor" includes any suitable hardware system, mechanism, or component that processes data, signals, or other information. A processor may include a system having a general purpose Central Processing Unit (CPU) with one or more cores (e.g., in a single-core, dual-core, or multi-core configuration), multiple processing units (e.g., in a multi-processor configuration), a Graphics Processing Unit (GPU), a Field Programmable Gate Array (FPGA), an Application Specific Integrated Circuit (ASIC), a Complex Programmable Logic Device (CPLD), dedicated circuitry to implement functions, a dedicated processor that implements neural network model-based processing, a neural circuit, a processor optimized for matrix computation (e.g., matrix multiplication), or other system.
In some implementations, the processor 502 may include one or more coprocessors that implement neural network processing. In some implementations, the processor 502 may be a processor that processes data to produce a probabilistic output, e.g., the output produced by the processor 502 may be inaccurate or may be accurate within a range from an expected output. The processing need not be limited to a particular geographic location, or have time constraints. For example, a processor may perform its functions in a "real-time," "offline," "batch mode," or the like manner. Portions of the processing may be performed by different (or the same) processing systems at different times and at different locations. The computer may be any processor in communication with the memory.
Memory 504 is typically provided in device 500 for access by processor 502 and may be any suitable processor-readable storage medium, such as Random Access Memory (RAM), read Only Memory (ROM), electrically erasable read only memory (EEPROM), flash memory, etc., adapted to store instructions for execution by the processor and that is located separately from and/or integrated with processor 502. Memory 504 may store software operated on device 500 by processor 502, including an operating system 508, machine learning applications 530, other applications 512, and application data 514. Other applications 512 may include applications such as virtual assistant applications, execution block applications, data display engines, web hosting engines, image display engines, notification engines, social networking engines, and the like. In some implementations, the machine learning application 530 and the other applications 512 may each include instructions that enable the processor 502 to perform the functions described herein (e.g., some or all of the method of fig. 3).
Other applications 512 may include, for example, image editing applications, media display applications, communication applications, web hosting engines or applications, map applications, media sharing applications, shopping or financial applications, and the like. One or more of the methods disclosed herein may operate in a variety of environments and platforms, e.g., as stand-alone computer programs that may be run on any type of computing device, as web applications with web pages, as mobile applications ("apps") running on mobile computing devices, etc.
In various implementations, the machine learning application may utilize a bayesian classifier, a support vector machine, a neural network, or other learning techniques. In some implementations, the machine learning application 530 can include a trained model 534, an inference engine 536, and data 532. In some implementations, the data 532 can include training data, such as data used to generate a trained model 534. For example, the training data may include any type of data, such as text, images, audio, video, user interaction data, application API specifications, and the like.
The training data may be obtained from any source, such as a data repository specially marked for training, data for which permissions are provided for use as training data for machine learning, and the like. In embodiments where one or more users are allowed to train a machine learning model (e.g., trained model 534) using their respective user data, the training data may include such user data. In embodiments where users allow access to their respective user data, the data 532 may include licensed data, such as click-streams or other user interaction data, usage patterns of the computing device, calendars or other information of the user, and the like.
In some implementations, the training data may include synthetic data generated for training purposes, such as data that is not based on user input or activity in the context being trained, e.g., data generated from simulating a user tour. In some implementations, the machine learning application 530 does not include the data 532. For example, in these embodiments, the trained model 534 may be generated, e.g., on a different device, and provided as part of the machine learning application 530. In various embodiments, trained models 534 may be provided as data files that include model structures or forms and associated weights. Inference engine 536 may read the data files of trained model 534 and implement a neural network with node connectivity, layers, and weights based on the model structure or form specified in trained model 534.
In some implementations, the trained models 534 can include one or more model forms or structures. For example, the model form or structure may include any type of neural network, such as a linear network, a deep neural network that implements multiple layers (e.g., a "hidden layer" between an input layer and an output layer, where each layer is a linear network), a convolutional neural network (e.g., a network that splits or separates input data into multiple portions or tiles (tiles), processes each tile individually using one or more neural network layers, and aggregates results from the processing of each tile), a sequence-to-sequence neural network (e.g., a network that takes sequence data (such as words in sentences, frames in video, etc.) as input, and produces a sequence of results as output), and so forth. The model form or structure may specify connectivity between individual nodes and node organization into layers.
For example, a node of a first layer (e.g., an input layer) may receive data (e.g., data 532 or application data 514) as input. For example, when trained model 534 is a model that identifies execution blocks comprising a plurality of executable units, the input data may include click-streams or other user interaction data, application API specifications for one or more applications, user interfaces and/or user interface elements for the applications, and the like. The subsequent middle tier may receive as input the output of the nodes of the previous tier according to connectivity specified in the model form or structure. These layers may also be referred to as hidden layers or potential layers.
The last layer (e.g., output layer) produces the output of the machine learning application. For example, the output may be an execution block including a sequence of executable units, associated semantic annotations. In some implementations, one or more parameters for executing the block may also be output. In some implementations, the model form or structure also specifies the number and/or type of nodes in each layer.
In various embodiments, trained model 534 may include a plurality of nodes arranged in layers according to a model structure or form. In some embodiments, the node may be a computing node without memory, e.g., configured to process an input unit to produce an output unit. The computation performed by the node may include, for example, multiplying each of the plurality of node inputs by a weight, obtaining a weighted sum, and adjusting the weighted sum with a deviation or intercept value to produce a node output. In some implementations, the computation performed by the node may further include applying a step/activate function to the adjusted weighted sum. In some embodiments, the step/activate function may be a non-linear function.
In various embodiments, such computation may include operations such as matrix multiplication. In some implementations, the computations performed by the multiple nodes may be performed in parallel, e.g., multiple processor cores using a multi-core processor, individual processing units using GPUs, or dedicated neural circuits. In some implementations, a node may include memory, for example, capable of storing one or more earlier inputs and using the one or more earlier inputs in processing subsequent inputs. For example, the nodes having memory may include Long Short Term Memory (LSTM) nodes. LSTM nodes may use memory to maintain a "state" that allows the node to behave like a Finite State Machine (FSM). Models with such nodes may be useful in processing sequence data, such as words in sentences or paragraphs; frames in video, speech, or other audio; sequence of operations in a user's journey, etc.
In some implementations, the trained models 534 can include embedding or weighting of individual nodes. For example, a model may be initiated as a plurality of nodes organized into layers specified by the model form or structure. At initialization, a corresponding weight may be applied to the connection between each pair of nodes (e.g., nodes in successive layers of the neural network) connected according to the model form. For example, the individual weights may be randomly assigned or initialized to default values. The model may then be trained, for example using data 532, to produce results.
For example, training may include applying supervised learning techniques. In supervised learning, training data may include a plurality of inputs and an expected output corresponding to each input. Based on a comparison of the model output to the expected output, the value of the weight is automatically adjusted, e.g., in a manner that increases the probability that the model will produce the expected output when provided with similar inputs.
The machine learning application 530 also includes an inference engine 536. Inference engine 536 is configured to apply trained models 534 to data, such as application data 514, to provide inferences. In some implementations, inference engine 536 may include software code that is executed by processor 502. In some implementations, inference engine 536 can specify a circuit configuration (e.g., for a programmable processor, for a Field Programmable Gate Array (FPGA), etc.) to configure processor 502 to apply the trained model. In some implementations, inference engine 536 may include software instructions, hardware instructions, or a combination. In some implementations, inference engine 536 can provide an Application Programming Interface (API) that operating system 508 and/or other applications 512 can use to call inference engine 536, e.g., apply trained model 534 to application data 514 to generate inferences, e.g., executable units or sequences of executable units that form execution blocks.
In some implementations, the machine learning application 530 may be implemented in an offline manner. In these embodiments, trained model 534 may be generated in a first stage and provided as part of machine learning application 530. In some implementations, the machine learning application 530 may be implemented in an online manner. For example, in such embodiments, an application (e.g., one or more of the operating system 508, other applications 512) invoking the machine learning application 530 may utilize the inferences made by the machine learning application 530, e.g., provide the inferences to the user, and may generate a system log (e.g., actions taken by the user based on the inferences if allowed by the user, or results of further processing if used as input for further processing). The system log may be generated periodically, e.g., hourly, monthly, quarterly, etc., and may be used to update trained model 534, e.g., to update the embedding of trained model 534, if permitted by the user.
In some implementations, the machine learning application 530 may be implemented in a manner that can accommodate the particular configuration of the device 500 on which the machine learning application 530 is executed. For example, the machine learning application 530 may determine a computational graph that utilizes available computational resources (e.g., the processor 502). For example, if the machine learning application 530 is implemented as a distributed application across multiple devices, the machine learning application 530 may determine to perform the calculations on the respective devices in a manner that optimizes the calculations. In another example, the machine learning application 530 may determine that the processor 502 includes a GPU having a particular number of GPU cores (e.g., 1000) and implement the inference engine accordingly (e.g., as 1000 separate processes or threads).
Any software in memory 504 may alternatively be stored on any other suitable storage location or computer readable medium. In addition, the memory 504 (and/or other connected storage device (s)) may store one or more messages, one or more taxonomies, electronic encyclopedias, dictionaries, knowledge bases, message data, grammars, user preferences, and/or other instructions and data used in the features described herein. Memory 504 and any other type of storage (magnetic disk, optical disk, magnetic tape, or other tangible medium) may be considered a "storage" or "storage device.
The I/O interface 506 may provide functionality that enables the device 500 to interface with other systems and devices. The interface device may be included as part of the device 500 or may be separate and in communication with the device 500. For example, network communication devices, storage devices (e.g., memory and/or database 106), and input/output devices may communicate via I/O interface 506. In some implementations, the I/O interface may be connected to interface devices such as input devices (keyboard, pointing device, touch screen, microphone, camera, scanner, sensor, etc.) and/or output devices (display device, speaker device, printer, motor, etc.).
Some examples of interface devices that may be connected to the I/O interface 506 may include one or more display devices 520 that may be used to display content, such as images, video, and/or user interfaces of the output applications described herein. The display device 520 may be connected to the device 500 by a local connection (e.g., a display bus) and/or by a networked connection, and may be any suitable display device. Display device 520 may include any suitable display device, such as an LCD, LED, or plasma display screen, CRT, television, monitor, touch screen, 3-D display screen, or other visual display device. For example, the display device 520 may be a flat display screen provided on a mobile device, a plurality of display screens provided in goggles or a head-mounted viewer device, or a monitor screen for a computer device.
The I/O interface 506 may interface to other input and output devices. Some examples include one or more cameras that may capture images. Some implementations may provide a microphone for capturing sound (e.g., as part of a captured image, voice command, etc.), an audio speaker device for outputting sound, or other input and output devices.
For ease of illustration, FIG. 5 shows one block for each of the processor 502, memory 504, I/O interface 506, and software blocks 508, 512, and 530. These blocks may represent one or more processors or processing circuits, operating systems, memories, I/O interfaces, applications, and/or software modules. In other embodiments, device 500 may not have all of the components shown and/or may have other elements, including other types of elements in place of or in addition to those shown herein. Although some components are described as performing the blocks and operations as described in some embodiments herein, any suitable component or combination of components of environment 100, device 500, similar system, or any suitable one or more processors associated with such systems may perform the described blocks and operations.
The methods described herein may be implemented by computer program instructions or code, which may be executed on a computer. For example, the code may be implemented by one or more digital processors (e.g., microprocessors or other processing circuits) and may be stored on a computer program product comprising a non-transitory computer readable medium (e.g., a storage medium), such as a magnetic, optical, electromagnetic, or semiconductor storage medium, including semiconductor or solid state memory, magnetic tape, removable computer diskette, random Access Memory (RAM), read-only memory (ROM), flash memory, rigid magnetic disk, optical disk, solid state memory drive, and the like. The program instructions may also be embodied in and provided as an electronic signal, such as in software as a service (SaaS) delivered from a server (e.g., a distributed system and/or cloud computing system). Alternatively, one or more of the methods may be implemented in hardware (logic gates, etc.) or in a combination of hardware and software. Example hardware may be a programmable processor (e.g., a Field Programmable Gate Array (FPGA), a complex programmable logic device), a general purpose processor, a graphics processor, an Application Specific Integrated Circuit (ASIC), etc. One or more methods may be performed as part or component of an application running on a system or as an application or software running in conjunction with other applications and operating systems.
Although the description has been described with respect to specific embodiments thereof, these specific embodiments are merely illustrative, and not restrictive. The concepts shown in the examples may be applied to other examples and implementations.
Where certain embodiments discussed herein may collect or use personal information about a user (e.g., user data, information about a user's social network, a user's location and time at the location, user's biometric information, user's activities and demographic information), the user is provided with one or more opportunities to control whether information is collected, whether personal information is stored, whether personal information is used, and how information about the user is collected, stored, and used. That is, the systems and methods discussed herein collect, store, and/or specifically use user personal information after receiving explicit authorization from an associated user. For example, a user is provided with control over whether a program or function gathers user information about that particular user or other users associated with the program or function. Each user whose personal information is to be collected is provided with one or more options to allow control of the collection of information associated with that user, whether to collect the information and which portions of the collected information provide permissions or authorizations. For example, one or more such control options may be provided to the user over a communications network. In addition, certain data may be processed in one or more ways prior to storage or use to remove personally identifiable information. For example, the user's identification may be processed such that no personally identifiable information may be determined. As another example, the geographic location of the user device may be generalized to a larger area such that a particular location of the user cannot be determined.
Note that the functional blocks, operations, features, methods, devices, and systems described in this disclosure may be integrated or partitioned into different combinations of systems, devices, and functional blocks as known to those of skill in the art. The routines of the particular embodiments may be implemented using any suitable programming language and programming technique. Different programming techniques may be employed, such as, for example, procedures or object-oriented. The routines may execute on a single processing device or multiple processors. Although steps, operations, or computations may be presented in a specific order, the order may be changed in different specific implementations. In some embodiments, a plurality of steps or operations shown as sequential in this specification can be performed at the same time.
Claims (20)
1. A computer-implemented method of providing an auxiliary user interface, the method comprising:
identifying a user itinerary by analyzing data comprising user interaction data, the user itinerary comprising a plurality of operations to perform computer-mediated tasks;
analyzing one or more of the following: an application programming interface, API, definition of a plurality of software applications or a user interface, UI, element of the plurality of software applications to identify a plurality of executable units;
Generating, based on the user itinerary and the plurality of executable units, an execution block defining a sequence of two or more of the plurality of executable units, and wherein execution of the execution block completes the computer-mediated task, and wherein execution of the execution block includes execution of each of the sequence of two or more of the plurality of executable units; and
providing the auxiliary user interface including the execution block.
2. The computer-implemented method of claim 1, further comprising:
receiving user input indicating activation of the execution block; and
the execution block is executed in response to the user input.
3. The computer-implemented method of claim 2, further comprising:
providing a hint requesting parameters for a particular executable unit of the sequence; and
a user response including the parameters is received.
4. The computer-implemented method of claim 2, wherein the auxiliary user interface is provided by a virtual assistant application, and wherein execution of the execution block includes invoking, by the virtual assistant application, a respective software application associated with each of the two or more of the plurality of executable units.
5. The computer-implemented method of claim 4, wherein the invoking comprises:
executing, by the virtual assistant application, an API call using the API definition of the respective software application; or (b)
Automatically navigating, by the virtual assistant application, a user interface provided by a corresponding software application, wherein the automatically navigating comprises:
identifying the user interface provided by the respective software application; and
based on the identifying, one or more of the following is automatically provided to the respective software application: click input, touch input, voice input, or keyboard input.
6. The computer-implemented method of claim 1, wherein generating the execution block includes mapping each of the plurality of operations of the user tour to a particular executable unit of the plurality of executable units.
7. The computer-implemented method of claim 1, wherein analyzing the API definitions of the plurality of software applications comprises, for each API definition:
obtaining semantic annotations associated with the API definition; and
based on the semantic annotations, an API associated with the API definition is mapped to a particular executable unit of the plurality of executable units.
8. The computer-implemented method of claim 1, wherein analyzing the UI elements of the plurality of software applications includes mapping each UI element to a particular executable unit of the plurality of executable units.
9. The computer-implemented method of claim 8, wherein the UI element comprises a display UI element, and wherein analyzing the display UI element comprises:
identifying one or more of the following: the shape, size, placement, text, or graphical content of each display UI element; and
the display UI element is matched to the particular executable unit based on the identifying.
10. The computer-implemented method of claim 8, wherein the UI elements comprise audio UI elements, and wherein analyzing the UI elements comprises, for each audio UI element:
using speech recognition techniques to detect one or more of the following: tones of the audio UI elements or text of the audio UI elements; and
the audio UI element is matched to the particular executable unit based on the tone or the text.
11. The computer-implemented method of claim 1, wherein the user interaction data comprises a plurality of user interaction events, and analyzing the user interaction data comprises:
Classifying the plurality of user interaction events into a plurality of event clusters; and
each of the plurality of user interaction events is mapped to a particular executable unit of the plurality of executable units based on the plurality of event clusters.
12. The computer-implemented method of claim 11, wherein the plurality of user interaction events includes a start event indicating a start of the computer-mediated task and an end event indicating an end of the computer-mediated task.
13. The computer-implemented method of claim 11, wherein the plurality of user interaction events includes at least one event associated with each of a first software application and a second software application of the plurality of software applications.
14. The computer-implemented method of claim 1, wherein the auxiliary user interface further comprises one or more other execution blocks, each other execution block associated with a respective computer-mediated task, and wherein providing the auxiliary user interface comprises:
the usage context is determined based on one or more of: a system state of a client computing device or a usage pattern associated with the client computing device; and
Determining that the computer-mediated task and the corresponding computer-mediated task match the usage context of the client computing device.
15. A computing device, comprising:
a processor; and
a memory coupled to the processor, on which are stored instructions that, when executed by the processor, cause the processor to perform operations comprising:
identifying a user itinerary by analyzing data comprising user interaction data, the user itinerary comprising a plurality of operations to perform computer-mediated tasks;
analyzing one or more of the following: an application programming interface, API, definition of a plurality of software applications or a user interface, UI, element of the plurality of software applications to identify a plurality of executable units;
generating, based on the user itinerary and the plurality of executable units, an execution block defining a sequence of two or more of the plurality of executable units, and wherein execution of the execution block completes the computer-mediated task, and wherein execution of the execution block includes execution of each of the sequence of two or more of the plurality of executable units; and
Providing an auxiliary user interface comprising the execution block.
16. The computing device of claim 15, wherein analyzing the API definitions of the plurality of software applications comprises, for each API definition:
obtaining semantic annotations associated with the API definition; and
based on the semantic annotations, an API associated with the API definition is mapped to a particular executable unit of the plurality of executable units.
17. The computing device of claim 15, wherein the user interaction data comprises a plurality of user interaction events, and wherein analyzing the user interaction data comprises:
classifying the plurality of user interaction events into a plurality of event clusters; and
each of the plurality of user interaction events is mapped to a particular executable unit of the plurality of executable units based on the plurality of event clusters.
18. The computing device of claim 15, wherein the computing device is a server, wherein the auxiliary user interface further comprises one or more other execution blocks, each other execution block associated with a respective computer-mediated task, and wherein providing the auxiliary user interface comprises:
Causing the auxiliary user interface to be displayed by a client computing device other than the server;
a usage context of the client computing device is determined based on one or more of: a system state of the client computing device or a usage pattern associated with the client computing device; and
determining that the computer-mediated task and the respective computer-mediated task associated with the one or more other execution blocks match the usage context of the client computing device.
19. A non-transitory computer-readable medium having instructions stored thereon, which when executed by a processor, cause the processor to perform operations comprising:
identifying a user itinerary by analyzing data comprising user interaction data, the user itinerary comprising a plurality of operations to perform computer-mediated tasks;
analyzing one or more of the following: an application programming interface, API, definition of a plurality of software applications or a user interface, UI, element of the plurality of software applications to identify a plurality of executable units;
generating, based on the user itinerary and the plurality of executable units, an execution block defining a sequence of two or more of the plurality of executable units, and wherein execution of the execution block completes the computer-mediated task, and wherein execution of the execution block includes execution of each of the sequence of two or more of the plurality of executable units; and
Providing an auxiliary user interface comprising the execution block.
20. The non-transitory computer-readable medium of claim 19, wherein generating the execution block comprises mapping each of the plurality of operations of the user tour to a particular executable unit of the plurality of executable units.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/054159 WO2021066818A1 (en) | 2019-10-01 | 2019-10-01 | Providing assistive user interfaces using execution blocks |
Publications (2)
Publication Number | Publication Date |
---|---|
CN114008590A CN114008590A (en) | 2022-02-01 |
CN114008590B true CN114008590B (en) | 2024-04-09 |
Family
ID=68296752
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980097694.6A Active CN114008590B (en) | 2019-10-01 | 2019-10-01 | Providing an auxiliary user interface using execution blocks |
Country Status (4)
Country | Link |
---|---|
US (1) | US20220300307A1 (en) |
EP (1) | EP3827340A1 (en) |
CN (1) | CN114008590B (en) |
WO (1) | WO2021066818A1 (en) |
Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102279791A (en) * | 2010-06-11 | 2011-12-14 | 微软公司 | User interface inventory |
KR20130058947A (en) * | 2011-11-28 | 2013-06-05 | 삼성전자주식회사 | Supplementary window displaying method and portable device supporting the same |
CN105830150A (en) * | 2013-12-18 | 2016-08-03 | 微软技术许可有限责任公司 | Intent-based user experience |
CN106657625A (en) * | 2016-12-06 | 2017-05-10 | 深圳市国华识别科技开发有限公司 | Terminal calling method and system |
CN107077287A (en) * | 2014-12-04 | 2017-08-18 | 谷歌公司 | Start the application with interface switching |
CN107666523A (en) * | 2016-07-29 | 2018-02-06 | 富士施乐株式会社 | Message processing device and information processing method |
CN107871501A (en) * | 2016-09-27 | 2018-04-03 | Fmr有限责任公司 | The automated software identified using intelligent sound performs method |
CN108566334A (en) * | 2018-05-02 | 2018-09-21 | 张昭远 | Householder method, terminal based on chat software and medium |
CN108710795A (en) * | 2018-04-18 | 2018-10-26 | Oppo广东移动通信有限公司 | Information cuing method, device, mobile terminal and storage medium |
CN109474735A (en) * | 2017-09-07 | 2019-03-15 | 珠海格力电器股份有限公司 | The display methods and electronic equipment of a kind of electronic equipment |
CN110035171A (en) * | 2018-01-12 | 2019-07-19 | 益富可视精密工业（深圳）有限公司 | Electronic equipment and auxiliary operation method |
Family Cites Families (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP5205965B2 (en) * | 2004-04-28 | 2013-06-05 | 富士通株式会社 | Computer system, server processing apparatus, terminal apparatus and method |
US7490295B2 (en) * | 2004-06-25 | 2009-02-10 | Apple Inc. | Layer for accessing user interface elements |
US9292307B2 (en) * | 2008-07-30 | 2016-03-22 | Kyocera Corporation | User interface generation apparatus |
US9569069B2 (en) * | 2011-09-29 | 2017-02-14 | Avaya Inc. | System and method for adaptive communication user interface |
US20130246920A1 (en) * | 2012-03-19 | 2013-09-19 | Research In Motion Limited | Method of enabling voice input for a visually based interface |
US20150067503A1 (en) * | 2013-08-27 | 2015-03-05 | Persais, Llc | System and method for virtual assistants with agent store |
US9729592B2 (en) * | 2013-08-27 | 2017-08-08 | Persais, Llc | System and method for distributed virtual assistant platforms |
US9891780B2 (en) * | 2013-08-30 | 2018-02-13 | Verizon Patent And Licensing Inc. | User-based customization of a user interface |
US10671428B2 (en) * | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
DK179343B1 (en) * | 2016-06-11 | 2018-05-14 | Apple Inc | Intelligent task discovery |
KR102374910B1 (en) * | 2017-08-22 | 2022-03-16 | 삼성전자주식회사 | Voice data processing method and electronic device supporting the same |
US10599469B2 (en) * | 2018-01-30 | 2020-03-24 | Motorola Mobility Llc | Methods to present the context of virtual assistant conversation |
US10496705B1 (en) * | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US11016722B2 (en) * | 2018-09-24 | 2021-05-25 | Salesforce.Com, Inc. | Database systems and methods for conversation-driven dynamic updates |
US11226833B2 (en) * | 2018-11-12 | 2022-01-18 | International Business Machines Corporation | Determination and initiation of a computing interface for computer-initiated task response |
US10964321B2 (en) * | 2018-12-06 | 2021-03-30 | Sap Se | Voice-enabled human tasks in process modeling |
US11307752B2 (en) * | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US20220284901A1 (en) * | 2019-05-31 | 2022-09-08 | Apple Inc. | Voice assistant discoverability through on-device targeting and personalization |
DK180129B1 (en) * | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
-
2019
- 2019-10-01 US US17/634,519 patent/US20220300307A1/en active Pending
- 2019-10-01 CN CN201980097694.6A patent/CN114008590B/en active Active
- 2019-10-01 EP EP19791024.3A patent/EP3827340A1/en active Pending
- 2019-10-01 WO PCT/US2019/054159 patent/WO2021066818A1/en unknown
Patent Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102279791A (en) * | 2010-06-11 | 2011-12-14 | 微软公司 | User interface inventory |
KR20130058947A (en) * | 2011-11-28 | 2013-06-05 | 삼성전자주식회사 | Supplementary window displaying method and portable device supporting the same |
CN105830150A (en) * | 2013-12-18 | 2016-08-03 | 微软技术许可有限责任公司 | Intent-based user experience |
CN107077287A (en) * | 2014-12-04 | 2017-08-18 | 谷歌公司 | Start the application with interface switching |
CN107666523A (en) * | 2016-07-29 | 2018-02-06 | 富士施乐株式会社 | Message processing device and information processing method |
CN107871501A (en) * | 2016-09-27 | 2018-04-03 | Fmr有限责任公司 | The automated software identified using intelligent sound performs method |
CN106657625A (en) * | 2016-12-06 | 2017-05-10 | 深圳市国华识别科技开发有限公司 | Terminal calling method and system |
CN109474735A (en) * | 2017-09-07 | 2019-03-15 | 珠海格力电器股份有限公司 | The display methods and electronic equipment of a kind of electronic equipment |
CN110035171A (en) * | 2018-01-12 | 2019-07-19 | 益富可视精密工业（深圳）有限公司 | Electronic equipment and auxiliary operation method |
CN108710795A (en) * | 2018-04-18 | 2018-10-26 | Oppo广东移动通信有限公司 | Information cuing method, device, mobile terminal and storage medium |
CN108566334A (en) * | 2018-05-02 | 2018-09-21 | 张昭远 | Householder method, terminal based on chat software and medium |
Non-Patent Citations (1)
Title |
---|
移动购物助手用户决策辅助功能的设计;刘晓倩 等;《第七届和谐人机环境联合学术会议》;20110917;38-44 * |
Also Published As
Publication number | Publication date |
---|---|
US20220300307A1 (en) | 2022-09-22 |
EP3827340A1 (en) | 2021-06-02 |
WO2021066818A1 (en) | 2021-04-08 |
CN114008590A (en) | 2022-02-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11303590B2 (en) | Suggested responses based on message stickers | |
US10862836B2 (en) | Automatic response suggestions based on images received in messaging applications | |
US10809876B2 (en) | Virtual assistant conversations | |
CN108733438B (en) | Application integration with digital assistant | |
US10146768B2 (en) | Automatic suggested responses to images received in messages using language model | |
US20210224474A1 (en) | Automatic grammar detection and correction | |
CN101495965B (en) | Dynamic user experience with semantic rich objects | |
KR20210035319A (en) | Web page analysis to facilitate automatic navigation | |
CN111611088A (en) | Method, electronic device and system for synchronization and task delegation of digital assistants | |
CN115344119A (en) | Digital assistant for health requests | |
CN111399714A (en) | User activity shortcut suggestions | |
CN110612566B (en) | Privacy maintenance of personal information | |
CN114008590B (en) | Providing an auxiliary user interface using execution blocks | |
CN117940879A (en) | Digital assistant for providing visualization of clip information | |
EP4242875A2 (en) | Voice assistant discoverability through on-device targeting and personalization | |
CN110574023A (en) | offline personal assistant |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |