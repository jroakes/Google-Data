CN114902229A - System and method for identifying the most appropriate grammar suggestion among suggestions from machine translation models - Google Patents
System and method for identifying the most appropriate grammar suggestion among suggestions from machine translation models Download PDFInfo
- Publication number
- CN114902229A CN114902229A CN202180007830.5A CN202180007830A CN114902229A CN 114902229 A CN114902229 A CN 114902229A CN 202180007830 A CN202180007830 A CN 202180007830A CN 114902229 A CN114902229 A CN 114902229A
- Authority
- CN
- China
- Prior art keywords
- word
- sentence
- edit
- candidate
- words
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/12—Use of codes for handling textual entities
- G06F40/151—Transformation
- G06F40/16—Automatic learning of transformation rules, e.g. from examples
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/166—Editing, e.g. inserting or deleting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/211—Syntactic parsing, e.g. based on context-free grammar [CFG] or unification grammars
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/216—Parsing using statistical methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/253—Grammatical analysis; Style critique
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/58—Use of machine translation, e.g. for multi-lingual retrieval, for server-side translation for client devices or for real-time translation
Abstract
A system and method for obtaining a candidate edit set of words of a sentence is disclosed, wherein each of the candidate edit set includes an edit word, two or more surrounding words are identified in the sentence, each word having a dependency relationship with the edit word, wherein at least one of the two or more surrounding words is identified regardless of their proximity to the edit word, the dependency relationship between the edit word and each surrounding word and the candidate edit set are provided as inputs to a grammar accuracy prediction model, one or more outputs are obtained from the grammar accuracy prediction model, wherein the one or more outputs indicate the grammar accuracy of each candidate edit of the set in the sentence according to the dependency relationship with the surrounding words, and a most accurate candidate edit is selected from the candidate edit set of sentences.
Description
Technical Field
Aspects and embodiments of the present disclosure are directed to identifying the most appropriate grammar suggestion among suggestions from a machine translation model.
Background
The grammar error correction system is used for correcting grammar errors existing in sentences provided by platform users. The grammar error correction system may identify a set of candidate edits to replace a word or phrase in a sentence provided by a user. In some instances, the candidate editing suggestions are from a machine translation model that identifies a suitable replacement set of words or phrases in the sentence (e.g., replacement words or phrases of the sentence that the user may accept).
Disclosure of Invention
The following summary is a simplified summary of the disclosure in order to provide a basic understanding of some aspects of the disclosure. This summary is not an extensive overview of the disclosure. It is intended to neither identify key or critical elements of the disclosure nor delineate any scope of particular embodiments of the disclosure or any scope of the claims. Its sole purpose is to present some concepts of the disclosure in a simplified form as a prelude to the more detailed description that is presented later.
In some implementations, a system and method are disclosed for selecting a recommended edit to a sentence based on a grammatical accuracy of each edit in a set of candidate edits. In an embodiment, a set of candidate edits is obtained for a sentence. The dependencies (i.e., syntactic and/or semantic relationships) between the candidate edits and their surrounding words in the sentence are provided as input to a machine learning model to predict the candidate edits that are most likely syntactically correct for the sentence. Based on the relationships between the candidate edits and their surrounding words, the machine learning model outputs a score from the set of candidate edits in the sentence that indicates the grammatical accuracy of each candidate edit. The candidate edit with the highest score is selected as the recommended grammar correction for the sentence.
In some implementations, a system and method are disclosed for training a machine learning model to determine the accuracy of suggested edits to document content. In an embodiment, training data is generated for a machine learning model. To generate the training data, the corrected sentence is identified. Words that are replaced by the edited words in the corrected sentence are also identified. The generated training input includes the edited words, the original words, and the dependencies between the edited words, the original words, and other words in the sentence. The ML model output includes an indication of whether the edited word in the corrected sentence is syntactically accurate based on a dependency between the edited word and each of the one or more second words. The training data is provided to a machine learning model to predict how accurately the compilation of a given sentence is grammatically accurate.
Drawings
Aspects and embodiments of the present disclosure will be understood more fully from the detailed description given below and from the accompanying drawings of various aspects and embodiments of the disclosure, which, however, should not be taken to limit the disclosure to the specific aspects or embodiments, but are for explanation and understanding only.
Fig. 1 illustrates an example system architecture according to embodiments of the present disclosure.
FIG. 2 is a block diagram of a syntax error correction engine according to an embodiment of the present disclosure.
Fig. 3 is a block diagram illustrating a grammar detection engine according to an embodiment of the present disclosure.
Fig. 4A illustrates a user-provided sentence according to an embodiment of the present disclosure.
Fig. 4B illustrates a syntactic property data structure of a sentence according to an embodiment of the present disclosure.
FIG. 5 depicts a flow diagram of a method of re-evaluating grammar suggestions from a machine translation model in accordance with an embodiment of the present disclosure.
FIG. 6 depicts a flowchart of a method for training a machine learning model for reevaluating grammar suggestions from a machine translation model in accordance with an embodiment of the present disclosure.
Fig. 7 is a block diagram illustrating an exemplary computer system according to an embodiment of the present disclosure.
Detailed Description
Aspects of the present disclosure relate to identifying the most likely of all candidate edits to improve the grammatical correctness of a sentence. Grammar error correction systems are often used to correct grammar errors present in sentences provided by users of platforms (e.g., document editing platforms, electronic mail (e-mail) platforms, electronic messaging platforms, etc.).
The grammar error correction system may use Machine Translation (MT) models (e.g., statistical MT models, neural MT models, etc.) whose parameters are derived from phrase analysis to accomplish translation from one language to another. The grammar error correction system may use the MT model to identify a set of candidate grammar-related edits to replace a word or phrase in a sentence provided by the user. In some instances, the MT model may identify a set of grammar-related candidate edits for a particular word of a sentence by identifying one or more predefined (e.g., previously recorded) candidate edits associated with the particular word. The MT engine may further identify a set of candidate edits based on a likelihood that the user will accept the edits as recommended edits to the user-provided sentence. In some instances, the MT model may determine the likelihood of a user accepting an edit based on the direct context of the candidate edit in the sentence provided by the user. The syntax error correction system may further include a language model. For each of the set of candidate edits identified by the MT engine, the language model may determine a likelihood or probability that the particular candidate edit is used in a file of the platform (e-document file, e-mail file, etc.).
In general, a grammar error correction system proposes a suggestion to recommend an edit by determining a particular edit from the set of candidate edits that has the highest likelihood of user acceptance and is most frequently used in the files of the platform. As described above, the likelihood of user acceptance is based on the direct context of the candidate edits in the user-provided sentence, rather than the global context of the candidate edits in the user-provided sentence (i.e., the context of a word or phrase from the entire sentence). Thus, a particular candidate edit may be associated with the highest likelihood of user acceptance as compared to other candidate edits in the set, even if the particular candidate edit is syntactically inaccurate given the global context of the candidate edit. By recommending an inaccurate edit of the grammar to the user-provided sentence, conventional grammar error correction systems are unable to improve the grammar of the user-provided sentence, thereby reducing the overall effectiveness of grammar error correction.
In some instances, the set of candidate edits may include a word or phrase that was originally provided in the user sentence. Candidate edits containing the original word or phrase may be syntactically accurate, but may be used infrequently in the platform's files. As a result, even if the user-provided sentence is syntactically accurate without recommended edits, the conventional syntax error correction system can select different candidate edits from the candidate edit set as recommended edits to the user-provided sentence. By providing unnecessary edits to recommendations for sentences that have been provided by users that are grammatically accurate, the user experience with the platform may be negative (e.g., the user may be annoyed because the platform is providing recommendations to sentences that are grammatically accurate). In addition, the syntax error correction system wastes a large amount of computing resources, increasing the overall latency of the syntax error correction system and the platform, thereby negatively impacting overall system efficiency.
Embodiments of the present disclosure address the above and other deficiencies by providing a machine learning model that re-evaluates candidate edits from an MT model of a user-provided sentence according to the grammatical accuracy of each of the candidate edits. In response to a user providing a sentence to a platform (e.g., a document editing platform, an email platform, etc.), a particular word of the user-provided sentence may be identified and a set of candidate edits for the word may be obtained (e.g., from an MT model used to check grammatical accuracy of the user-provided sentence). The processing device may identify one or more grammar attributes associated with each of the user-provided sentences and the set of candidate edits. For example, the processing device may identify a dependency (e.g., a syntactic dependency, a semantic dependency, etc.) between each of the set of candidate edits and two or more additional words (surrounding each candidate edit word) in the sentence provided by the user. Dependencies between each of the candidate edits and at least one of the two or more surrounding words may be identified without regard to the proximity of the corresponding surrounding word to the edited word (i.e., by using words of the entire sentence and only words immediately after and before the original word or words from the same phrase). The processing device may provide each of the set of dependencies and candidate edits as input to a machine learning model that is trained to determine the grammatical accuracy of each candidate edit. The processing device may receive one or more outputs from the trained machine learning model, where each of the one or more outputs indicates a grammatical accuracy of each candidate edit in the user-provided sentence. The processing device may select the candidate edit as the recommended edit based on the grammatical accuracy of each of the set of candidate edits.
The machine learning model may be trained using training data collected for previously corrected sentences. In some embodiments, the previously corrected sentence may be a sentence corrected by a recognized linguistic authority (e.g., a professional linguist). In other or similar embodiments, the previously corrected sentence may be a user-provided sentence that was previously provided to the platform and corrected using the grammar error correction system. The training data may include training inputs and associated target outputs. The training input may be generated using the original words edited in the corrected sentence and additional words each having a dependency relationship with the edited words, wherein the additional words may be identified regardless of their proximity to the corresponding edited words. Based on the dependency between the edited word and each of the additional words, a target output may be generated for the associated training input based on an indication of whether the edited word in each corrected sentence is syntactically accurate.
Accordingly, aspects of the present disclosure re-evaluate grammar suggestions from the machine translation model to ensure that only grammatically correct edits in the context of an entire sentence are provided to users of the platform. By determining grammar accuracy based on dependencies between an original word to be edited in a sentence and two or more additional words of the sentence, where the additional words are identified without regard to their proximity to the original word, candidate edits may be selected for recommendation to a user based on a global context of the sentence rather than a direct context. By selecting candidate edits to be recommended to a user based on the global context of a sentence, the overall accuracy of recommended edits provided by the platform is improved, thereby improving the overall effectiveness of the grammar error correction system.
Furthermore, by using the dependency (i.e., syntactic and/or semantic relationships) of a word with other words of a sentence to determine the accuracy of a candidate edit, the weight of the frequency of use of a particular candidate edit in selecting a recommended edit is reduced. As a result, even if an original word is not frequently used in the platform file, it is not suggested to edit the original word of a sentence that has been grammatically accurate, thereby reducing the unnecessary number of suggestions and the amount of wasted computing resources. As a result, the overall quality of the syntax error correction system is improved. Furthermore, the overall latency of the syntax error correction system is reduced and the overall system efficiency is improved.
Fig. 1 illustrates an example system architecture 100 in accordance with an embodiment of the present disclosure. System architecture 100 (also referred to herein as a "system") includes client devices 102A-N, data store 110, platform 120, and one or more server machines 130 and 150, each connected to network 104.
In an embodiment, the network 104 may include a public network (e.g., the internet), a private network (e.g., a Local Area Network (LAN) or a Wide Area Network (WAN)), a wired network (e.g., ethernet), a wireless network (e.g., an 802.11 network or a Wi-Fi network), a cellular network (e.g., a Long Term Evolution (LTE) network), a router, a hub, a switch, a server computer, and/or combinations thereof.
In some implementations, the data store 110 is a persistent store capable of storing content items as well as data structures for tagging, organizing, and indexing the content items. The data store 110 may be hosted by one or more storage devices, such as main memory, magnetic or optical storage-based disks, tape or hard drives, NAS, SAN, or the like. In some implementations, the data store 110 may be a network-attached file server, while in other embodiments, the data store 110 may be some other type of persistent storage, such as an object-oriented database, a relational database, or the like, which may be hosted by the platform 120 or one or more different machines coupled to the platform 120 via the network 104.
In some implementations, platform 120 and/or server machine 130 and 150 may be one or more computing devices (such as a rack-mounted server, a router computer, a server computer, a personal computer, a mainframe computer, a laptop computer, a tablet computer, a desktop computer, etc.), data stores (e.g., hard disk, memory, database), networks, software components, and/or hardware components that may be used to provide a user with access to files 121 (e.g., electronic documents, email messages, etc.) and/or to provide files 121 to a user. For example, the platform 120 may be an electronic document platform. The electronic document platform may allow a user to create, edit (e.g., collaborate with other users), access, or share electronic documents stored at the data store 110 with other users. In another example, the platform 120 may be an electronic messaging platform (e.g., an email platform). The electronic messaging platform may allow a user to create, edit, or access electronic messages (e.g., emails) sent to other users of the electronic messaging platform or users of client devices outside of the electronic messaging platform. Platform 120 may also include a website (e.g., a web page) or application backend software that may be used to provide users with access to files 121.
In embodiments of the present disclosure, a "user" may be represented as a single individual. However, other embodiments of the present disclosure contemplate that the "user" is an entity controlled by the user and/or the set of automated sources. For example, a collection of individual users that are joined as a community in a social network may be considered a "user". In another example, the automated consumer may be an automated ingestion pipeline of the platform 120, such as a theme channel.
As previously described, the platform 120 may allow a user to create or edit files 121 (e.g., electronic document files, email files, etc.) via a user interface of a content viewer. Each user-created file 121 may be stored at data store 110. In one example, a user may provide one or more sentences to be included in the file 121 via a user interface. In some instances, the user-provided sentence may include one or more errors that result in an inaccurate sentence grammar. A sentence is grammatically inaccurate if a word or phrase of the sentence, given the global context of the word or phrase, causes the sentence to violate one or more grammatical rules of the particular language of the sentence (e.g., english). For example, instead of providing the sentences "I walk to the store to buy milk" or "I walk to the store and buy milk" the user may provide the sentence "I walk to the store to buy milk". In this example, given the context of a user-provided sentence, the words "walk" and "bought" in the sentence together result in a sentence that is syntactically inaccurate.
In some implementations, the recommendation system can be configured to identify words or phrases of the sentence that cause the sentence to be syntactically inaccurate, and recommend edits 122 to the sentence to make the sentence syntactically accurate (i.e., the sentence satisfies one or more grammatical rules for a particular language). The recommended edits 122 may be indicators (e.g., interface components, electronic messages, recommendation feeds, etc.) that provide editing suggestions to the user that may make the sentence provided by the user grammatically correct. The recommendation system may include at least a grammar detection engine 310, a training data generator 131, a training engine 141, a machine learning model 160, and a grammar error correction engine 151. In some implementations, the recommended edits 122 can be based on the output of trained machine learning models, such as machine learning models 160A-N.
Server machine 130 may include a training set generator 131 capable of generating training data (e.g., a set of training inputs and a set of target outputs) to train ML models 160A-N. Training data may be generated based on sentences that have been previously corrected by one or more edits. In some implementations, the one or more previously corrected sentences can be sentences previously corrected by a recognized linguistic authority (e.g., a professional linguist). For example, the data store 110 can store (e.g., in a data structure) sentences that include one or more grammatical errors corrected by a professional linguist. The data store 110 may also store an indication of one or more words of a sentence, the sentence being edited according to one or more grammar rules. For example, the data store 110 may store an indication of the original words included in the sentence and an edited form of the original words (i.e., edited words) provided by a recognized language authority, which results in the sentence becoming syntactically correct.
In other or similar embodiments, training data may be generated based on previously corrected user-provided sentences. For example, according to the previously described embodiment, a user may provide a sentence to the platform 120. The platform 120 may provide recommended edits 122 of the words of the sentence to the client devices 102A-N of the users, where the client devices 102A-N present the recommended edits 122 to the users through the user interfaces. The user may accept or decline the recommended edits 122, for example, by selecting a user interface element of the user interface. The client devices 102A-N may send an indication of acceptance or rejection to the platform 120. In response to receiving the indication, the platform 120 may store the sentence, the recommended edit 122, and the indication of acceptance or rejection at the data store 110.
The training data generator 131 may generate a set of training data by identifying data corresponding to previously corrected sentences stored at the data store 110. The set of training data may include a subset of training inputs and target outputs based on the identified data. In some embodiments, the training data generator 131 may obtain one or more grammatical attributes of each previously corrected sentence using the grammar detection engine 310. Further details regarding grammar detection engine 310 will be provided with reference to FIG. 3. In some implementations, each subset of the training input can include a portion of the original sentence before the original sentence was corrected, edits made to the previously corrected sentence that resulted in the sentence being grammatically correct, and one or more grammatical attributes of the previously corrected sentence and/or the corrected sentence. Each subset of target outputs may include data related to whether the edits to the previously corrected sentence are syntactically accurate. In some implementations, the edits are determined to be syntactically accurate if a recognized language authority provides the edits to a previously corrected sentence. In other or similar implementations, according to the previously described embodiment, if the user providing the previously corrected sentence accepts the recommended edit 122 to the sentence, the edit is determined to be syntactically accurate. Further details regarding generating training data are provided herein.
The server machine 140 may include a training engine 141. The training engine 141 may train the machine learning models 160A-N using training data from the training set generator 131. The machine learning models 160A-N may refer to model artifacts, which are created by the training engine 141 using training data that includes training inputs and corresponding target outputs (correct answers to the respective training inputs). The training engine 141 may find patterns in the training data that map the training inputs to the target outputs (answers to be predicted) and provide machine learning models 160A-N that capture these patterns. The machine learning models 160A-N may be comprised of, for example, a single level of linear or non-linear operation (e.g., a Support Vector Machine (SVM) or may be a deep network, i.e., a machine learning model comprised of multiple levels of non-linear operation). An example of a deep network is a neural network with one or more hidden layers, and such a machine learning model may be trained by adjusting neural network weights, e.g., according to a back-propagation learning algorithm or the like. For convenience, the remainder of this disclosure refers to embodiments as neural networks, although some embodiments may employ SVMs or other types of learning machines instead of or in addition to neural networks. In one aspect, the training set is obtained by a training set generator 131 hosted by the server machine 130. In some embodiments, the machine learning model 160 may be the edit accuracy model described with respect to fig. 2.
It should be noted that in some other embodiments, the functionality of server machines 130, 140, and 150 or platform 120 may be provided by a smaller number of machines. For example, in some embodiments, server machines 130 and 140 may be integrated into a single machine, while in other embodiments, server machines 130, 140, and 150 may be integrated into multiple machines. Further, in some implementations, one or more of server machines 130, 140, and 150 may be integrated into platform 120.
In general, the functions described in embodiments as being performed by the platform 120 or server machines 130, 140, 150 may also be performed on the client devices 102A-N in other embodiments, if appropriate. Further, functionality attributed to a particular component may be performed by different or multiple components operating together. The platform 120 or server machines 130, 140, 150 may also be accessed as a service provided to other systems or devices through an appropriate application programming interface and is therefore not limited to use in a website.
Although embodiments of the present disclosure are discussed in terms of a platform 120 and providing access to files 121 on the platform 120, embodiments may generally be applied to any type of grammar error correction system configured to correct user-provided sentences. Embodiments of the present disclosure are not limited to platforms that provide users with access to electronic documents or electronic messages.
Where the systems discussed herein collect or may utilize personal information about a user, the user may be provided with an opportunity to control whether the platform 120 collects user information (e.g., information about the user's social network, social behavior or activity, profession, the user's preferences, or the user's current location), or to control whether and/or how to receive content from content servers that may be more relevant to the user. In addition, certain data may be processed in one or more ways before it is stored or used, thereby deleting the personal identification information. For example, the user's identity may be processed such that no personal identity information of the user can be determined, or the user's geographic location may be summarized where location information is obtained (e.g., at a city, zip code, or state level) such that no particular location of the user can be determined. Thus, the user may control how information about the user is collected and used by the platform 120.
FIG. 2 is a block diagram of a syntax error correction engine according to an embodiment of the present disclosure. In some embodiments, the syntax error correction engine may be syntax error correction engine 151 of FIG. 1. As previously described, grammar error correction engine 151 may determine recommended edits 122 for one or more portions of a sentence provided by a user of platform 120 via client device 102. For example, the user may provide a sentence 400, as shown in FIG. 4A (i.e., "Below letters Aristotle's dependencies and brieflies extensions the person connected. (the dependencies of Arithodles are listed Below and their requirements are briefly explained.)"). It should be noted that some embodiments described herein provide recommended edits for words of a sentence provided to a user. However, grammar error correction engine 151 may provide recommended edits 122 to any portion of a sentence provided by a user, including a word, a plurality of words, a phrase, and the like.
Grammar error correction engine 151 may include a candidate edit engine 210 and an edit accuracy engine 220. The candidate edit engine 210 may be configured to identify one or more sets of candidate edits that may be recommended as edits to a portion of a sentence provided by a user. As previously described, a user may provide a sentence to the platform 120 via the client device 102. The candidate editing engine 210 may parse the user-provided sentence to identify one or more words or phrases of the sentence. According to the previous example, the candidate editing engine 210 may identify the word "explains" of the sentence 400. Each of the identified words or phrases may be provided to the edit generation model 212. In some embodiments, the editorial generative model 212 can be an MT model (e.g., a statistical MT model, a neural MT model, etc.).
The edit generation model 212 is a model configured to identify a set of candidate edits for a particular word or phrase, each of which may be applied to a particular word or phrase in a sentence provided by a user. In some embodiments, the edit generation model 212 may be a trained machine learning model configured to receive as input a particular word or set of words from a user-provided sentence, and to provide as output a set of candidate edits that may be applied to the particular word or set of words. The edit generation model 212 can also provide as an additional output an indication of the likelihood that a user presented with each of the set of candidate edits will accept the particular candidate edit as a recommended edit 122 of the user-provided sentence.
In some embodiments, the edit generation model 212 may identify a set of candidate edits based on a predefined set of candidate edits associated with a particular word or phrase. In some embodiments, each of the set of candidate edits may correspond to a potentially corrected form of a word of the sentence. According to the previous example, the candidate edit engine 210 may identify the word "explains" from the sentence 400 and provide the word "explains" to the edit generation model 212. Edit generation model 212 may identify (e.g., using a lookup table or other suitable data structure or database) a set of candidate predefined candidate edits associated with the word "explains. For example, edit generation model 212 may provide a set of candidate edits as output, including "explains", "explainers", and the like. The edit generation model 212 can also determine, for each of the set of candidate edits, a likelihood that the user will accept the edit as a recommended edit 122 of the user-provided sentence. In some embodiments, the edit generation model 212 may determine the likelihood of user acceptance based on the direct context of a particular word (i.e., from the context of the particular word of words immediately surrounding the particular word). In other or similar embodiments, the edit generation model 212 may determine a likelihood of user acceptance based on historical data associated with the candidate edits. For example, the edit generation model 212 may determine a likelihood of user acceptance based on historical data indicating the number of users that have previously accepted or rejected a particular candidate edit for a particular word in a sentence.
For illustrative purposes only, edit generation model 212 may determine that a first candidate edit that includes the word "explain" is associated with a highest likelihood, a second candidate edit that includes the word "explains" is associated with a second highest likelihood, a third candidate edit that includes the word "explained" is associated with a third highest likelihood, and a fourth candidate edit that includes the word "explainer" is associated with a lowest likelihood. In some embodiments, the edit generation model 212 may provide as an additional output an indication of the likelihood that the user accepts the candidate edits as recommended edits 122 of the user-provided sentence. For example, the edit generation model 212 may provide each of the set of candidate edits with a user acceptance rating or a user acceptance score (e.g., a highest rating for a first candidate edit, a second highest rating for a second candidate edit, a third highest rating for a third candidate edit, and a lowest rating for a fourth candidate edit) indicating a likelihood of the user accepting the candidate edit.
In some embodiments, the candidate edit engine 210 may provide each of the set of candidate edits identified by the edit generation model 212 to the language frequency model 214. Language frequency model 214 may be configured to determine a frequency of use of a particular word or phrase in a particular set of files (e.g., all electronic documents of an electronic document platform, all email files of an electronic messaging platform, etc.). The frequency of use may correspond to the number of instances that a particular word or phrase is used or appears in a particular set of documents. In some embodiments, language frequency model 214 may be a trained machine learning model configured to receive a particular word or phrase as input (e.g., to form candidate edits) and provide as output a frequency of use indicative of the particular word or phrase. For example, the language frequency model 214 may provide a frequency of use rating or frequency of use score for each of the set of candidate edits that indicates a frequency of use for the corresponding candidate edit.
Edit accuracy engine 220 may be configured to determine a grammatical accuracy of each of the set of candidate edits identified by edit generation model 212. Edit accuracy engine 220 may include a grammar detection engine 310 and an edit accuracy model 224. Grammar detection engine 310 may be configured to detect one or more attributes corresponding to a grammar of a sentence provided by a user. For example, grammar detection engine 310 may detect a dependency relationship (e.g., a semantic relationship, a syntactic relationship, etc.) between an identified word of a user-provided sentence (e.g., "explains") and one or more additional words of the user-provided sentence. Further details regarding grammar detection engine 310 are provided with reference to FIG. 3.
Edit accuracy engine 220 may determine the grammatical accuracy of each of the candidate edit sets based on the output provided by edit accuracy model 224. The edit accuracy model 224 may be configured to receive as input words identified from a user-provided sentence, a set of candidate edits corresponding to the identified words, and one or more attributes corresponding to a grammar of the user-provided sentence identified by the grammar detection engine 310. For example, edit accuracy model 224 may receive as input a recognized word "explains", a set of candidate edits including at least "explains", "explained", "explainer", and dependencies (e.g., syntactic dependencies, semantic dependencies, etc.) between the recognized word "explains" and one or more additional words of a user-provided sentence. The edit accuracy model 224 may determine a grammatical accuracy of each candidate edit of the set of candidate edits in the user-provided sentence based on one or more grammatical attributes (e.g., syntactic and/or semantic dependencies between the identified word and one or more additional words) and provide an indication of the grammatical accuracy of each candidate edit as an output. For example, the edit accuracy model 224 may provide each of the set of candidate edits with an accuracy level or accuracy score that indicates the accuracy of the grammar of the corresponding candidate edit.
In response to the edit accuracy engine 220 determining the grammar accuracy of each of the set of candidate edits, the grammar error correction engine 151 may select a recommended edit 122 for the sentence provided by the user. In some embodiments, grammar error correction engine 151 may select recommended edits 122 based on the accuracy level or score of each of the set of candidate edits. For example, the grammar error correction engine 151 may select the candidate edit associated with the highest accuracy level or score as the recommended edit 122 of the sentence provided by the user. In other or similar embodiments, grammar error correction engine 151 may select recommended edits 122 based on at least one of an accuracy rating or score for each of the set of candidate edits and a user acceptance rating (or score) or a frequency of use rating (or score) for each of the set of candidate edits determined by edit generation model 212 and language frequency model 214, respectively, according to previously described embodiments. For example, grammar error correction engine 151 may determine, for each of the set of candidate edits, an overall rating based on the accuracy rating and at least one of a user acceptance rating and a frequency of use rating or a corresponding candidate edit. The grammar error correction engine 151 may select the candidate edit with the highest overall rating as the recommended edit 122 of the sentence provided by the user.
In some embodiments, grammar error correction engine 151 may determine to remove one or more candidate edits from the set of candidate edits prior to selecting recommended edit 122. For example, the grammar error correction engine 151 may determine whether the grammar accuracy of each of the candidate edit sets satisfies a grammar accuracy criterion. In some embodiments, the grammar accuracy may satisfy the grammar accuracy criteria in response to a determination that the grammar accuracy satisfies (i.e., meets or exceeds) a grammar accuracy threshold. If one or more particular candidate edits do not meet the grammar accuracy criteria, the grammar error correction engine 151 may remove the particular candidate edit from the set of candidate edits. According to the previously described embodiment, grammar error correction engine 151 may select recommended edits 122 from the remaining set of candidate edits. In some examples, no candidate edit in the set of candidate edits is associated with a grammar accuracy that satisfies the grammar accuracy criteria. In such instances, the grammar error correction engine 151 may provide suggestions for edits 122 that do not recommend the user-provided sentence (i.e., the originally-provided word in the user-provided sentence is grammatically accurate).
Fig. 3 is a block diagram illustrating a grammar detection engine 310 according to an embodiment of the present disclosure. In some embodiments, grammar detection engine 310 may correspond to grammar detection engine 310 of fig. 1 and/or fig. 2. As shown in fig. 3, grammar detection engine 310 may be a component of a computing device that includes a processing apparatus 300 coupled to a data store 350. In some embodiments, data store 350 may be included in data store 110 of FIG. 1. Processing device 300 may be configured to execute a grammar detection engine 310. The processing device 300 may correspond to the server machine 130, 140, or 150, the platform 120, or the client device 102 of fig. 1. The data store 350 may be a portion of memory corresponding to the volatile memory 704 and/or the non-volatile memory 706, as described with respect to fig. 7.
The dependency module 312 may be configured to identify dependencies (i.e., syntactic and/or semantic relationships) between words of the sentence 352 and one or more additional words of the sentence 352. A dependency refers to a relationship between a parent dependency and a child dependency of a particular sentence. Parent and child dependent words share at least one or more syntactic properties (i.e., the form or structure of the word based on sentence syntax) or one or more semantic properties (i.e., the form or structure of the word based on sentence meaning). The parent dependent word controls at least the form or structure of the child dependent word based on one or more shared syntactic and/or semantic properties. For a particular word of a sentence, the dependency module 312 may identify parent dependent words and/or child dependent words associated with the particular word. In some embodiments, a child dependent word may be a word that surrounds (i.e., is immediately adjacent to) a parent dependent word. In other or similar embodiments, a child dependent word may be a word that does not surround (i.e., is not immediately adjacent to) a parent dependent word. In embodiments of the present disclosure, child dependent words may be identified for parent dependent words regardless of their proximity to the parent dependent words.
Referring to sentence 400 of FIG. 4, the first word of sentence 400 may be "explains". In one example, "explains" may be a child dependent word of a first dependency relationship with a parent dependent word "lists". As a result of the first dependency, at least the word "lists" given the syntax or meaning of the sentence 400 controls at least the form or structure of the word "explains" in the sentence 400. In another example, "explains" may be a parent dependency of a second dependency relationship with a child dependency word "need". As a result of the second dependency, the word "explains" controls at least the form or structure of the word "need" in the sentence 400.
In some embodiments, the dependency module 312 may generate dependency tags that indicate the dependency status (e.g., parent or child) within the dependency relationship. As shown in FIG. 4A, the dependency module 312 may generate a parent dependency tag 402A for the word "lists" according to a first dependency relationship and a parent dependency tag 402B for the word "explains" according to a second dependency relationship. Similarly, dependency module 312 may generate child dependency label 404A for the word "explains" according to a first dependency relationship and generate child dependency label 404B for the word "need" according to a second dependency relationship. Grammar detection engine 310 may store each dependency tag generated by dependency module 312 as a grammar attribute 356 at data store 350. In some embodiments, the grammar detection engine 310 may store each dependency tag in a particular entry of the data structure 450 for the corresponding word, as shown in fig. 4B.
In some embodiments, the dependency module 312 may also generate pointers between parent dependent words and child dependent words that indicate the dependency relationship between the parent dependent words and the child dependent words. As shown in the figure. As shown in FIG. 4, the dependency module 312 may generate a first pointer between the parent dependency word "lists" and the child dependency word "explains" according to a first dependency relationship. The dependency module 312 may also generate a second pointer between the parent dependency word "explains" and the child dependency word "need" according to the second dependency relationship. Grammar detection engine 310 may store each pointer generated by dependency module 312 as a grammar attribute 356 at data store 350. The grammar detection engine 310 may store each pointer in a particular entry of the data structure 450 for the corresponding word, as shown in fig. 4B.
In some embodiments, the dependency module 312 may also determine the grammatical accuracy of the dependency relationship between the parent dependent word and the child dependent word. The dependency module 312 may determine the grammatical accuracy of the dependency by determining that one or more grammatical and/or semantic rules associated with the dependency are satisfied. The dependency module 312 may store an indication of whether the dependency is accurate as a grammar attribute 356. For example, the dependency module 312 may store a grammatically accurate indication of the dependency in a particular entry of the data structure 450 for the corresponding word, as shown in FIG. 4B.
The fluency module 320 may be configured to detect fluency of sentences 352 that include particular words. The fluency of sentence 352 may refer to a generally accepted expression for sentence 352. The fluency module 320 may compare a particular wording of the sentence 352 to other sentences included in a particular set of documents (e.g., a set of documents of a document editing platform, a set of documents of an email platform, etc.) to determine whether the wording that causes the sentence 352 to contain the particular word is fluent. Depending on the particular words, grammar detection engine 310 may store an indication of the fluency of sentence 352 (i.e., whether sentence 352 is fluent or not fluent) as grammar attribute 356 at data store 350. In some embodiments, the grammar detection engine 310 may store an indication of the fluency of the sentence 352 in a particular entry of the particular word's data structure 450 according to the particular word, as shown in FIG. 4B.
As described above, grammar detection engine 310 may detect previously provided grammar attributes for one or more words of a sentence (e.g., sentence 400 or a previously corrected sentence described with respect to fig. 1). In some embodiments, grammar detection engine 310 may further detect previously-provided grammar attributes for one or more editorial words not included in sentence 352. In some embodiments, the edited words may include each word of the candidate edits described with respect to FIG. 2. In other or similar embodiments, the edited words may include one or more words that replace words in a previously corrected sentence. Grammar detection engine 310 may store the detected attributes for each edit word as grammar attributes 356 stored at data store 350. In some embodiments, the grammar detection engine 310 may store the detected attributes of the authored word 354 in a particular entry of the data structure 450 for a particular authored word 354. As shown in fig. 4B, grammar detection engine 310 may detect one or more grammar attributes for the word "explains" in sentence 400 and store the attributes in an entry for the word "explains" in data structure 450. Grammar detection engine 310 may also detect one or more grammar attributes for each candidate edit of the words "explains" (i.e., "explains," "explained," and "explainers," as described with respect to fig. 2) and store each detected grammar attribute in an entry for each candidate edit in data structure 450.
In some embodiments, grammar detection engine 310 may detect one or more additional grammar attributes for sentence 352 and/or edit word 354. For example, grammar detection engine 310 may select one or more words of sentence 352 and determine one or more alternative combinations of the selected words in sentence 352. In accordance with the previously described embodiments, grammar detection engine 310 may then determine one or more grammar attributes associated with the alternative combinations of the selected words and store the grammar attributes in data store 350. In another example, grammar detection engine 310 may determine similarities (e.g., structural similarities, contextual similarities, etc.) between words of sentence 352 and edit words 354. According to the previously described embodiments, grammar detection engine 310 may store indications of similarities as grammar attributes in data store 350.
Referring back to fig. 1, the training data generator 131 may use the grammar detection engine 310 to generate training data for training the machine learning model, such as the edit accuracy model 224. As previously described, the training data generator 131 may generate training data to train the editing accuracy model 224 using previously corrected sentences stored at the data store 110. In some instances, the previously corrected sentences may be provided by a recognized linguistic authority (e.g., a professional linguist). According to the previously described embodiment, the training data generator 131 may obtain one or more grammatical attributes of a previously corrected sentence. The training set generator 131 may generate a first subset of training inputs, which may include a portion of the original sentence before the original sentence was corrected, edits made to the previously corrected sentence, and one or more grammatical attributes of the previously corrected sentence (e.g., dependencies between words in the corrected portion and two or more surrounding words). The first subset of target outputs of the first subset of training inputs may include an indication that edits made to previously corrected sentences are syntactically accurate (i.e., because the edits were made by a recognized language authority). According to the previously described embodiments, a first subset of training inputs and a first subset of target outputs may be provided to train the editing accuracy model 224.
In some embodiments, the training data generator 131 may provide the original, uncorrected sentence to the candidate editing engine 210. In some instances, the edit generation model 212 may provide a set of candidate edits for the original, uncorrected sentence. A first candidate edit of the set of candidate edits may correspond to an edit of the sentence by the recognized language authority, while at least a second candidate edit is different from the edit of the sentence by the recognized language authority. According to the previously described embodiment, the training data generator 131 may obtain one or more grammar attributes associated with the original sentence and a second candidate edit of the set of candidate edits. The training data generator 131 may generate a second subset of training inputs from the second candidate edits, including the previous portion of the original, uncorrected sentence, the second candidate edits of the set of candidate edits, and one or more grammatical attributes of the previously corrected sentence. The training data generator 131 may generate a second subset of target outputs for a second subset of training inputs that includes a second candidate edit that is syntactically inaccurate to correct the corrected sentence (i.e., because the edit is not by a recognized language authority). According to the previously described embodiments, a first subset of training inputs and a first subset of target outputs may be provided to train the editing accuracy model 224.
As previously described, the training data generator 131 may use the grammar detection engine 310 to generate training data based on previously corrected user-provided sentences. According to the previously described embodiment, the training data generator 131 may generate training data based on previously corrected user-provided sentences.
Referring back to fig. 2, the edit accuracy engine 220 may use the grammar detection engine 310 to detect one or more grammar attributes associated with words of the user-provided sentence as identified by the candidate edit engine 210 and each of the set of candidate edits as identified by the edit generation model 212. The edit accuracy engine 220 can provide the words of the user-provided sentence, the detected one or more grammar attributes associated with the words of the user-provided sentence, each of the set of candidate edits, and the one or more grammar attributes associated with each of the set of candidate edits as inputs to the edit accuracy model 224. As previously described, the edit accuracy model 224 may provide as output the grammatical accuracy of each candidate edit from the set of candidate edits based on one or more grammatical attributes associated with the user-provided word and the candidate edit.
FIG. 5 depicts a flowchart of a method 500 of re-evaluating grammar suggestions from a machine translation model in accordance with an embodiment of the present disclosure. Fig. 6 depicts a flow diagram of a method 600 for training a machine learning model for reevaluating grammar suggestions from a machine translation model in accordance with an embodiment of the present disclosure. Methods 500 and 600 may be performed by processing logic that may comprise hardware (circuitry, dedicated logic, etc.), software (e.g., instructions run on a processing device), or a combination thereof. In one embodiment, some or all of the operations of methods 500 and 600 may be performed by one or more components of system 100 of FIG. 1.
Referring now to FIG. 5, at block 510, the processing device obtains a set of candidate edits to a word of a sentence. The sentence may be a sentence provided by a user of the platform 120, as described with respect to fig. 1. In some embodiments, each of the set of candidate edits may include an edit word. The processing device may obtain a candidate set from the candidate editing engine 210 of fig. 2 according to the previously described embodiments. For example, the processing device may provide words of a sentence as input to an edit generation model (e.g., edit generation model 212). The edit generation model may be configured to identify one or more potential corrected forms of the given word for the given word. The processing device may obtain a set of candidate edits for the word from the edit generation model as one or more second outputs. The edit word for each of the candidate edit collections may correspond to a potential corrected form of the word. At block 520, the processing device identifies two or more surrounding words in the sentence that have semantic and syntactic dependencies with the edited word. In some embodiments, the processing device identifies at least one of the two or more surrounding words without regard to the proximity of the corresponding surrounding word to the first word. The processing device may identify two or more surrounding words having a dependency relationship with the edited word using a grammar detection engine, such as grammar detection engine 310 described with respect to fig. 3. In some embodiments, the processing device may identify the set of surrounding words having semantic dependencies and syntactic dependencies by determining that each of the set of edited words and surrounding words has a shared semantic characteristic based at least on the semantics of the sentence and that each of the set of edited words and surrounding words has a shared syntactic characteristic based at least on the syntax of the sentence.
In some embodiments, the processing device may identify other grammar attributes associated with the edited words and/or sentences. For example, the processing device may identify a part-of-speech identifier for each word in the sentence. In another example, the processing device may identify a contextual relationship between the edit word and one or more additional words of the sentence. In such an example, each of the one or more additional words is identified based on a direct proximity of the corresponding additional word to an edit word in the sentence.
At block 530, the processing device provides semantic and syntactic dependencies between the first word and each of the surrounding words and a set of candidate edits as inputs to train the machine learning model. In some embodiments, the trained machine learning model may be a grammar accuracy prediction model, such as edit accuracy model 224 of fig. 2. In some embodiments, the processing device may provide one or more additional grammatical attributes associated with the edited word and/or sentence as additional inputs to training the machine learning model. For example, the processing device may provide a part-of-speech identifier for each word of the sentence to the training machine learning model. In another example, the processing device may provide the context relationship between the edited word and the one or more additional words to the trained machine learning model.
At block 540, the processing device obtains one or more outputs from the trained machine learning model that indicate the grammar accuracy of each candidate edit from the set of candidate edits. In some embodiments, the one or more outputs from the trained machine learning model indicate a grammatical accuracy of each candidate edit based on semantic and syntactic dependencies between the edited word and one or more of the surrounding words.
At block 550, the processing device selects a candidate edit from the set of candidate edits based on the indicated syntax accuracy of each candidate edit from the set of candidate edits. In some embodiments, the processing device may provide each of the candidate edit sets of edit words of the sentence to a language frequency model, such as language frequency model 214. The processing device may obtain one or more third outputs from the language frequency model that indicate a frequency with which the edit word of each of the candidate edit collections is used in a particular collection of files. In such embodiments, the processing device may select a candidate edit from the set of candidate edits based on a frequency with which each edit word of each of the set of candidate edits is used in the particular set of files.
In some embodiments, the processing device may remove one or more candidate edits from the set of candidate edits. For example, the processing device may determine whether the indicated grammar accuracy for each candidate edit from the set of candidate edits meets the grammar accuracy criteria. In response to determining that the respective grammar accuracies of the one or more particular candidate edits do not satisfy the grammar accuracy criteria, the processing device may remove the one or more particular candidate edits from the set of candidate edits.
As described above, fig. 6 depicts a flow diagram of a method 600 for training a machine learning model to select recommended edits for a sentence provided by a user based on the grammatical accuracy of each of a set of candidate edits according to an embodiment of the present disclosure.
At block 610, the processing device generates training data for a machine learning model. The machine learning model may be a grammar accuracy prediction model, such as edit accuracy model 224 of fig. 2. In some embodiments, the processing device generates training data for the machine learning model according to the operations performed at blocks 512-518 of the method 600.
At block 612, the processing device identifies the corrected sentence and the word that is replaced with the edit word in the corrected sentence. In some embodiments, the processing device may identify a sentence from a previously corrected set of sentences provided by a linguistic authority entity (e.g., a professional linguist). In other or similar embodiments, the processing device may identify sentences from previous user-provided sentences (i.e., to the platform 120).
At block 614, the processing device identifies two or more surrounding words that each have semantic and syntactic dependencies with the edited word. In some embodiments, at least one of the two or more surrounding words is identified without regard to the proximity of the corresponding surrounding word to the edited word. According to the embodiment described with respect to fig. 3, the processing device may identify two or more surrounding words. For example, the processing device may identify the set of surrounding words having semantic and syntactic dependencies by determining that each of the set of edited words and surrounding words will have shared semantic attributes based at least on the semantics of the sentence and that each of the set of edited words and surrounding words will have shared syntactic attributes based at least on the syntax of the sentence.
In some embodiments, the processing device may identify other grammar attributes associated with the edited words and/or sentences. For example, the processing device may identify a part-of-speech identifier for each word in the sentence. In another example, the processing device may identify a contextual relationship between the edit word and one or more additional words of the sentence. In such an example, each of the one or more additional words is identified based on a direct proximity of the corresponding additional word to an edit word in the sentence.
At block 616, the processing device generates a first training input including an edited word, a word, and semantic and syntactic dependencies between the edited word and each of a set of surrounding words. In some embodiments, the generated first training input may further include one or more grammatical attributes of the edited word and/or sentence. For example, the generated first training input may include a part-of-speech identifier for each word of the corrected sentence. In another example, the generated first training input may include a contextual relationship between an editing word and one or more additional words of the sentence.
In some embodiments, the training input comprising semantic and syntactic dependencies between the edited word and each of the set of surrounding words comprises at least one of a dependency status label for each of the edited word and the set of surrounding words. Dependency status tags indicate the status of a particular word in semantic and syntactic dependencies. The dependency status tags may further or alternatively include pointers indicating semantic and syntactic dependencies between the edited word and each of the set of surrounding words.
At block 618, the processing device generates a first target output for the first training input. The target output may include an indication of whether the edited word in the corrected sentence is syntactically accurate based on semantic and syntactic dependencies between the edited word and each of the one or more surrounding words.
At block 620, the processing device provides training data to train the machine learning model on (i) a set of training inputs comprising a first training input and (ii) a set of target outputs comprising a first target output. In some embodiments, each training input in the set of training inputs is mapped to a target output in the set of target outputs.
Fig. 7 is a block diagram illustrating an example computer system 700, according to an embodiment of the present disclosure. The computer system 700 may be the server machine 130 or the platform 120 of fig. 1. The machine may operate in the capacity of a server or an endpoint machine in an endpoint server network environment or as a peer machine in a peer-to-peer (or distributed) network environment. The machine may be a television, a Personal Computer (PC), a tablet PC, a set-top box (STB), a Personal Digital Assistant (PDA), a cellular telephone, a web appliance, a server, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine. Further, while only a single machine is illustrated, the term "machine" shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein.
Processor (processing device) 702 represents one or more general-purpose processing devices such as a microprocessor, central processing unit, or the like. More specifically, the processor 702 may be a Complex Instruction Set Computing (CISC) microprocessor, Reduced Instruction Set Computing (RISC) microprocessor, Very Long Instruction Word (VLIW) microprocessor, or a processor implementing other instruction sets or processors implementing a combination of instruction sets. The processor 802 may also be one or more special-purpose processing devices such as an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Digital Signal Processor (DSP), network processor, or the like. The processor 702 is configured to execute instructions 705 (e.g., for predicting channel program viewership) in order to perform the operations discussed herein.
The computer system 700 may also include a network interface device 708. Computer system 800 may also include a video display unit 710 (e.g., a Liquid Crystal Display (LCD) or a Cathode Ray Tube (CRT)), an input device 712 (e.g., a keyboard and alphanumeric keyboard, a motion-sensing input device, a touch screen), a cursor control device 714 (e.g., a mouse), and a signal generation device 720 (e.g., a speaker).
The data storage 718 may include a non-transitory machine-readable storage medium 724 (also a computer-readable storage medium) having stored thereon one or more sets of instructions 705 (e.g., for correcting one or more grammatical errors in a user-provided sentence) embodying any one or more of the methodologies or functions described herein. The instructions may also reside, completely or at least partially, within the main memory 704 and/or within the processor 702 during execution thereof by the computer system 700, the main memory 704 and the processor 702 also constituting machine-readable storage media. The instructions may also be transmitted or received over a network 730 via the network interface device 708.
In one embodiment, instructions 705 include instructions for predicting a channel program rating. While the computer-readable storage medium 724 (machine-readable storage medium) is shown in an exemplary embodiment to be a single medium, the terms "computer-readable storage medium" and "machine-readable storage medium" should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions. The terms "computer-readable storage medium" and "machine-readable storage medium" shall also be taken to include any medium that is capable of storing, encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present disclosure. Accordingly, the terms "computer-readable storage medium" and "machine-readable storage medium" should be taken to include, but not be limited to, solid-state memories, optical media, and magnetic media.
Reference throughout this specification to "one embodiment" or "an embodiment" means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. Thus, the appearances of the phrases "in one embodiment" or "in an embodiment" in various places throughout this specification may, but do not necessarily, all refer to the same embodiment, depending on the circumstances. Furthermore, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.
To the extent that the terms "includes," "including," "has," "contains," variants thereof, and other similar words are used in either the detailed description or the claims, these terms are intended to be inclusive in a manner similar to the term "comprising" as an open transition word without precluding any additional or other elements.
As used in this application, the terms "component," "module," "system," and the like are generally intended to refer to a computer-related entity, either hardware (e.g., circuitry), software, a combination of hardware and software, or an entity associated with an operating machine that has one or more specific functions. For example, a component may be, but is not limited to being, a process running on a processor (e.g., a digital signal processor), a processor, an object, an executable, a thread of execution, a program, and/or a computer. By way of illustration, both an application running on a controller and the controller can be a component. One or more components may reside within a process and/or thread of execution and a component may be localized on one computer and/or distributed between two or more computers. Further, "devices" may come in the form of specially designed hardware; general purpose hardware specialized by executing software thereon that enables the hardware to perform specific functions (e.g., generating points of interest and/or descriptors); software on a computer readable medium; or a combination thereof.
The foregoing systems, circuits, modules, and the like have been described with respect to interaction between several components and/or blocks. It is to be understood that such systems, circuits, components, blocks, etc. can include those components or specified sub-components, some of the specified components or sub-components, and/or additional components, and that sub-components can also be implemented as components communicatively coupled to other components rather than included within parent components (hierarchical) in accordance with the various permutations and combinations described above. Additionally, it should be noted that one or more components may be combined into a single component providing aggregate functionality or divided into several separate sub-components, and any one or more middle layers, such as a management layer, may be provided to communicatively couple to these sub-components to provide integrated functionality. Any components described herein may also interact with one or more other components not specifically described herein but known to those of skill in the art.
Moreover, the words "example" or "exemplary" are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the words "example" or "example" are intended to present concepts in a concrete fashion. As used in this application, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or". That is, unless specified otherwise, or clear from context, "X employs A or B" is intended to mean any of the natural inclusive permutations. That is, if X employs A; x is B; or X employs both A and B, then "X employs A or B" is true in any of the above examples. In addition, the articles "a" and "an" as used in this application and the appended claims should generally be construed to mean "one or more" unless specified otherwise or clear from context to be directed to a singular form.
Finally, implementations described herein include the collection of data describing a user and/or user activity. In one embodiment, such data is collected only if the user agrees to collect this data. In some embodiments, the user is prompted to explicitly allow data collection. In addition, the user may choose to join or withdraw from participating in such data collection activities. In one embodiment, the collected data is anonymized before performing any analysis to obtain any statistical patterns, such that the identity of the user cannot be determined from the collected data.
Claims (20)
1. A method, comprising:
obtaining a set of candidate edits to a word of a sentence, wherein each of the candidate edits comprises an edited word;
identifying a set of surrounding words in the sentence that each have a semantic dependency and a syntactic dependency with the edited word, wherein at least one surrounding word in the set of surrounding words is identified without regard to proximity to the edited word;
providing the semantic and syntactic dependencies between the edited word and each surrounding word in the set of surrounding words of the candidate edited set as input to a grammar accuracy prediction model;
obtaining one or more first outputs from the grammar accuracy prediction model, wherein the one or more first outputs indicate a grammar accuracy of each candidate edit from the set of candidate edits in the sentence according to the semantic and syntactic dependencies between the edited word and each surrounding word in the set of surrounding words; and
selecting a candidate edit from the set of candidate edits based on the indicated grammatical accuracy of each candidate edit in the sentence from the set of candidate edits.
2. The method of claim 1, wherein obtaining the set of candidate edits to the word of the sentence comprises:
providing the words of the sentence as input to an edit generation model, wherein the edit generation model is configured to identify, for a given word, one or more potential corrected forms of the given word; and
obtaining a set of candidate edits to the word as one or more second outputs from the edit generation model, wherein the edited word of each candidate edit in the set of candidate edits corresponds to a potentially corrected form of the word.
3. The method of claim 1, wherein identifying the set of surrounding words that each have the semantic dependency and the syntactic dependency with the edit word comprises:
determining that the editing word and each surrounding word in the set of surrounding words will have a shared semantic attribute based at least on the semantics of the sentence; and
determining that the edit word and each surrounding word in the set of surrounding words will have a shared syntactic property based at least on the syntax of the sentence.
4. The method of claim 1, further comprising:
identifying at least one of a part-of-speech identifier of each word of the sentence or a contextual relationship between the edit word and one or more additional words of the sentence, wherein each of the one or more additional words is identified based on a direct proximity of the corresponding additional word to the edit word in the sentence; and
providing at least one of a part-of-speech identifier for each word of the sentence or a context relationship between the edited word and the one or more additional words as an additional input to the grammar accuracy prediction model.
5. The method of claim 1, further comprising:
providing each candidate edit in the set of candidate edits to the edit word of the sentence as an input to a language frequency model; and
obtaining one or more third outputs from the language frequency model, wherein the one or more third outputs indicate a frequency of using the edit word of each candidate edit in the set of candidate edits in a particular set of files.
6. The method of claim 5, wherein selecting the candidate edit from the set of candidate edits is further based on a frequency of using each edit word of each candidate edit in the set of candidate edits in the particular set of files.
7. The method of claim 1, further comprising:
determining whether the indicated syntax accuracy of each candidate edit from the set of candidate edits meets a syntax accuracy criterion; and
in response to determining that the grammar accuracy of the one or more particular candidate edits does not satisfy the grammar accuracy criteria, removing the one or more particular candidate edits from the set of candidate edits.
8. A method for training a grammar accuracy prediction model to determine accuracy of suggested edits to document content, the method comprising:
generating training data for the grammar accuracy prediction model, wherein generating the training data comprises:
identifying the corrected sentence and the word replaced with the edit word in the corrected sentence;
identifying a set of surrounding words in the corrected sentence that each have a semantic dependency and a syntactic dependency with the edited word, wherein at least one surrounding word in the set of surrounding words is identified without regard to a proximity of the corresponding surrounding word to the edited word;
generating a first training input comprising an edited word, and the semantic and syntactic dependencies between the edited word and each surrounding word in the set of surrounding words; and
generating a first target output for the first training input, wherein the first target output comprises an indication of: whether an edited word in the corrected sentence is syntactically accurate or not according to the semantic dependency and the syntactic dependency between the edited word and each surrounding word in the set of surrounding words; and
providing the training data to train the grammar accuracy prediction model on: (i) a set of training inputs comprising the first training input and (ii) a set of target outputs comprising the first target output.
9. The method of claim 8, wherein the sentence being corrected is identified from a sentence database comprising one or more previously corrected sentences, wherein each of the one or more previously corrected sentences is corrected by at least one of a user of a recognized language authority entity or platform.
10. The method of claim 8, wherein identifying the set of surrounding words that each have the semantic dependency and the syntactic dependency with the edit word comprises:
determining that the edited word and each surrounding word in the set of surrounding words will have a shared semantic attribute based at least on the semantics of the sentence; and
determining that the edited word and each surrounding word in the set of surrounding words will have a shared syntactic property based at least on a syntax of the sentence.
11. The method of claim 8, wherein generating training data for the grammar accuracy prediction model further comprises:
identifying at least one of a part-of-speech identifier of each word of the corrected sentence or a contextual relationship between the edited word and one or more additional words of the sentence, wherein each of the one or more additional words is identified based on a direct proximity of the corresponding additional word to the edited word in the sentence,
wherein the generated first training input further comprises at least one of a part-of-speech identifier for each word of the corrected sentence or a contextual relationship between the edited word and one or more additional words of the sentence.
12. The method of claim 8, wherein a training input comprising the semantic and syntactic dependencies between the edited word and each surrounding word in the set of surrounding words comprises at least one of a dependency status label for each of the edited word and the set of surrounding words, wherein the dependency status label indicates: a state of a particular word in the semantic and syntactic dependencies, or a pointer indicating the semantic and syntactic dependencies between the edited word and each surrounding word in the set of surrounding words.
13. The method of claim 8, wherein each training input of the set of training inputs is mapped to a target output of the set of target outputs.
14. A system, comprising:
a memory component; and
a processing device communicatively coupled to the memory component, the processing device to:
obtaining a set of candidate edits to a first word of a sentence, wherein each of the candidate edits comprises an edit word;
identifying a set of surrounding words in the sentence that each have a semantic dependency and a syntactic dependency with the edited word, wherein at least one surrounding word in the set of surrounding words is identified without regard to a proximity of the corresponding surrounding word to the edited word;
providing semantic and syntactic dependencies between the edited word and each of the surrounding words and the candidate edit set as inputs to a grammar accuracy prediction model;
obtaining one or more first outputs from the grammar accuracy prediction model, wherein the one or more first outputs indicate a grammar accuracy of each candidate edit from the set in the sentence according to the semantic and syntactic dependencies between the edited word and each surrounding word in the set of surrounding words; and
selecting a candidate edit from the set of candidate edits based on the indicated grammatical accuracy of each candidate edit from the set in the sentence.
15. The system of claim 14, wherein to obtain the set of candidate edits to the word of the sentence, the processing device is to:
providing the words of the sentence as input to an edit generation model, wherein the edit generation model is configured to identify, for a given word, one or more potential corrected forms of the given word; and
obtaining the set of candidate edits to the word as one or more second outputs from the edit generation model, wherein the edited word of each candidate edit in the set of candidate edits corresponds to a potentially corrected form of the word.
16. The system of claim 14, wherein to identify the set of surrounding words each having semantic and syntactic dependencies on the first word, the processing device is to:
determining that the edited word and each surrounding word in the set of surrounding words will have a shared semantic attribute based at least on the semantics of the sentence; and
determining that the edited word and each surrounding word in the set of surrounding words will have a shared syntactic property based at least on a syntax of the sentence.
17. The system of claim 14, wherein the processing device is further to:
identifying at least one of a part-of-speech identifier of each word of the sentence or a contextual relationship between the edit word and one or more additional words of the sentence, wherein each of the one or more additional words is identified based on a direct proximity of the corresponding additional word to the edit word in the sentence; and
providing at least one of a part-of-speech identifier of each word of the sentence or a context relationship between the edited word and the one or more additional words as an additional input to the grammar accuracy prediction model.
18. The system of claim 14, wherein the processing device is further to:
providing each candidate edit in the set of candidate edits to the edit word of the sentence as an input to a language frequency model; and
obtaining one or more third outputs from the language frequency model, wherein the one or more third outputs indicate a frequency of using the edit word of each candidate edit in the set of candidate edits in a particular set of files.
19. The system of claim 18, wherein the processing device to select the candidate edit from the set of candidate edits is further based on a frequency of using each edit word of each candidate edit in the set of candidate edits in the particular set of files.
20. The system of claim 14, wherein the processing device is further to:
determining whether the indicated syntax accuracy of each candidate edit from the set of candidate edits meets a syntax accuracy criterion; and
in response to determining that the grammar accuracy of the one or more particular candidate edits does not satisfy the grammar accuracy criteria, removing the one or more particular candidate edits from the set of candidate edits.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
IN202041020306 | 2020-05-14 | ||
IN202041020306 | 2020-05-14 | ||
US16/922,951 US11636274B2 (en) | 2020-05-14 | 2020-07-07 | Systems and methods to identify most suitable grammar suggestions among suggestions from a machine translation model |
US16/922,951 | 2020-07-07 | ||
PCT/US2021/032532 WO2021231917A1 (en) | 2020-05-14 | 2021-05-14 | Systems and methods to identify most suitable grammar suggestions among suggestions from a machine translation model |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114902229A true CN114902229A (en) | 2022-08-12 |
Family
ID=78512468
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180007830.5A Pending CN114902229A (en) | 2020-05-14 | 2021-05-14 | System and method for identifying the most appropriate grammar suggestion among suggestions from machine translation models |
Country Status (4)
Country | Link |
---|---|
US (2) | US11636274B2 (en) |
EP (1) | EP4150500A1 (en) |
CN (1) | CN114902229A (en) |
WO (1) | WO2021231917A1 (en) |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11941372B2 (en) * | 2021-04-01 | 2024-03-26 | Microsoft Technology Licensing, Llc | Edit automation using an anchor target list |
US11875136B2 (en) | 2021-04-01 | 2024-01-16 | Microsoft Technology Licensing, Llc | Edit automation using a temporal edit pattern |
US20220004701A1 (en) * | 2021-06-22 | 2022-01-06 | Samsung Electronics Co., Ltd. | Electronic device and method for converting sentence based on a newly coined word |
CN117010364A (en) * | 2022-04-29 | 2023-11-07 | 青岛海尔科技有限公司 | Method and device for extracting event slots, storage medium and electronic device |
Family Cites Families (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6085206A (en) * | 1996-06-20 | 2000-07-04 | Microsoft Corporation | Method and system for verifying accuracy of spelling and grammatical composition of a document |
US6434523B1 (en) | 1999-04-23 | 2002-08-13 | Nuance Communications | Creating and editing grammars for speech recognition graphically |
US7003444B2 (en) * | 2001-07-12 | 2006-02-21 | Microsoft Corporation | Method and apparatus for improved grammar checking using a stochastic parser |
US7003445B2 (en) * | 2001-07-20 | 2006-02-21 | Microsoft Corporation | Statistically driven sentence realizing method and apparatus |
US7917355B2 (en) * | 2007-08-23 | 2011-03-29 | Google Inc. | Word detection |
IL186505A0 (en) * | 2007-10-08 | 2008-01-20 | Excelang Ltd | Grammar checker |
US9471561B2 (en) * | 2013-12-26 | 2016-10-18 | International Business Machines Corporation | Adaptive parser-centric text normalization |
US9639522B2 (en) | 2014-09-02 | 2017-05-02 | Google Inc. | Methods and apparatus related to determining edit rules for rewriting phrases |
US20170286376A1 (en) | 2016-03-31 | 2017-10-05 | Jonathan Mugan | Checking Grammar Using an Encoder and Decoder |
US11042796B2 (en) * | 2016-11-03 | 2021-06-22 | Salesforce.Com, Inc. | Training a joint many-task neural network model using successive regularization |
-
2020
- 2020-07-07 US US16/922,951 patent/US11636274B2/en active Active
-
2021
- 2021-05-14 WO PCT/US2021/032532 patent/WO2021231917A1/en unknown
- 2021-05-14 CN CN202180007830.5A patent/CN114902229A/en active Pending
- 2021-05-14 EP EP21730752.9A patent/EP4150500A1/en active Pending
-
2023
- 2023-04-24 US US18/306,174 patent/US20230259720A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2021231917A1 (en) | 2021-11-18 |
EP4150500A1 (en) | 2023-03-22 |
US20210357599A1 (en) | 2021-11-18 |
US20230259720A1 (en) | 2023-08-17 |
US11636274B2 (en) | 2023-04-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11636274B2 (en) | Systems and methods to identify most suitable grammar suggestions among suggestions from a machine translation model | |
US11977847B2 (en) | Dynamically updated text classifier | |
US11042709B1 (en) | Context saliency-based deictic parser for natural language processing | |
US11748232B2 (en) | System for discovering semantic relationships in computer programs | |
Mostafa | More than words: Social networks’ text mining for consumer brand sentiments | |
RU2657173C2 (en) | Sentiment analysis at the level of aspects using methods of machine learning | |
RU2571373C2 (en) | Method of analysing text data tonality | |
US20170308523A1 (en) | A method and system for sentiment classification and emotion classification | |
US20180136794A1 (en) | Determining graphical element(s) for inclusion in an electronic communication | |
US9881010B1 (en) | Suggestions based on document topics | |
WO2015176518A1 (en) | Reply information recommending method and device | |
US9047563B2 (en) | Performing an action related to a measure of credibility of a document | |
CN109992771B (en) | Text generation method and device | |
RU2639655C1 (en) | System for creating documents based on text analysis on natural language | |
JP2009076043A (en) | System to infer context information for activity from message | |
CN106610990B (en) | Method and device for analyzing emotional tendency | |
CN107077640B (en) | System and process for analyzing, qualifying, and ingesting unstructured data sources via empirical attribution | |
US20150324339A1 (en) | Providing factual suggestions within a document | |
Rastas et al. | Explainable publication year prediction of eighteenth century texts with the BERT model | |
US11631021B1 (en) | Identifying and ranking potentially privileged documents using a machine learning topic model | |
GB2608112A (en) | System and method for providing media content | |
Ren | Emotion analysis of cross-media writing text in the context of big data | |
Tardy et al. | Semantic enrichment of places with VGI sources: a knowledge based approach | |
Musso et al. | Opinion mining of online product reviews using a lexicon-based algorithm | |
Lau et al. | Cat's Eye: Media Insights Analyzer |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |