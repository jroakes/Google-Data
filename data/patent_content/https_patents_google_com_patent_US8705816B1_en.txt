US8705816B1 - Face recognition with discriminative face alignment - Google Patents
Face recognition with discriminative face alignment Download PDFInfo
- Publication number
- US8705816B1 US8705816B1 US13/438,582 US201213438582A US8705816B1 US 8705816 B1 US8705816 B1 US 8705816B1 US 201213438582 A US201213438582 A US 201213438582A US 8705816 B1 US8705816 B1 US 8705816B1
- Authority
- US
- United States
- Prior art keywords
- face
- alignment
- individual
- recognition
- persons
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/74—Image or video pattern matching; Proximity measures in feature spaces
- G06V10/75—Organisation of the matching processes, e.g. simultaneous or sequential comparisons of image or video features; Coarse-fine approaches, e.g. multi-scale approaches; using context analysis; Selection of dictionaries
- G06V10/755—Deformable models or variational models, e.g. snakes or active contours
- G06V10/7553—Deformable models or variational models, e.g. snakes or active contours based on shape, e.g. active shape models [ASM]
Definitions
- Embodiments of the present invention relate to face recognition.
- Face recognition uses computers to recognize a person from a digital image or a video frame. Face recognition can be used for a variety of purposes including identification, security, law enforcement, and digital photography and video. A number of methods have been developed for face recognition. For instance, a typical automatic face recognition (AFR) system is composed of three parts or levels: face detection, face alignment and face recognition. Given images containing faces, face detection locates a face, face alignment locates key feature points of the face, and face recognition determines whose face it is. Many algorithms have been proposed for human face recognition. However, these algorithms have focused only on each separate part of a face recognition system. Conventionally, these three parts are processed as follows: face detection is performed first, detection results are then passed to face alignment, and then results of face alignment are passed to face recognition. This is a bottom-up approach, as shown in FIG. 1 . Another approach is the top-down approach, as shown in FIG. 2 , which operates in a reverse order than a bottom-up approach.
- AFR automatic face recognition
- each part or level provides data to the next level. It is a data-driven approach. This approach may use only class-independent information or information that is not specific to a class of persons.
- a class may be one or more specific persons to be recognized or identified.
- Typical bottom-up approaches may not rely on class-specific knowledge. For such AFR systems, face detection and face alignment may not use knowledge about the classes of persons to be recognized.
- GPFA general purpose face alignment
- the higher level guides the lower level.
- the top-down approach could perform better for the objects that provide this knowledge.
- there are difficulties with the top-down approach First, there may be large variations within the classes. If the variations cannot be properly modeled, they will introduce unexpected errors.
- a system for face recognition includes a face alignment module, a signature extractor and a recognizer.
- the face alignment module can locate feature points of a face in an image using a face alignment model.
- the signature extractor can generate a reconstruction error.
- the signature generator can also extract signature features from the face in an image.
- the recognizer can identify a person from the face in the image.
- a method for face recognition includes extracting signature features of a face in an image based upon face alignment localization.
- the extracting may use a face alignment model.
- the method also includes generating reconstruction errors based upon the face alignment localization.
- the generating may use the face alignment model.
- the method further includes identifying a person from the face in the image. This identification may be based upon the extracted signature features and the generated reconstruction errors.
- a method for face recognition includes extracting signature features of a face in an image based upon face alignment localization.
- the extracting may use an individual face alignment model.
- the method also includes generating reconstruction errors based upon the face alignment localization.
- the generating may use the individual face alignment model.
- the method further includes producing an individual alignment result ranking. This ranking may be based upon the extracted signature features and the reconstruction errors.
- the method also includes repeating the extracting, generating, and producing steps to produce additional individual alignment result rankings.
- the method includes identifying a person from the face in an image based upon the individual alignment result rankings.
- FIG. 1 illustrates bottom-up approach to face recognition.
- FIG. 2 illustrates top-down approach to face recognition.
- FIG. 3 illustrates a system for face recognition, according to an embodiment of the present invention.
- FIG. 4 illustrates a face alignment module for use in a system for face recognition, according to an embodiment of the present invention.
- FIG. 5 illustrates a signature extractor for use in a system for face recognition, according to an embodiment of the present invention.
- FIG. 6 illustrates a system for face recognition including an iterative mixture recognizer, according to an embodiment of the present invention.
- FIG. 7 shows a flowchart illustrating a method for face recognition including direct mixture recognition, according to an embodiment of the present invention.
- FIG. 8 shows a flowchart illustrating a method for face recognition including iterative mixture recognition, according to an embodiment of the present invention.
- FIG. 9 shows a flowchart illustrating a method for face recognition including direct mixture recognition, according to an embodiment of the present invention.
- FIG. 10 shows a flowchart illustrating a method for face recognition including iterative mixture recognition, according to an embodiment of the present invention.
- Embodiments described herein refer to systems and methods for face recognition.
- typical automatic face recognition systems may be composed of three parts or levels: face detection, face alignment and face recognition. As shown in FIGS. 1 and 2 , these three parts may be processed in a bottom-up or top-down manner.
- bottom-up and top-down face recognition may be combined with discriminative face alignment for better results.
- Discriminative face alignment may incorporate knowledge about classes of persons or individuals to be recognized into the face alignment step. This top-down approach of discriminative face alignment may be combined with a traditional bottom-up approach for face recognition. This is a mixture of top-down and bottom-up, or mixture face recognition.
- direct mixture recognition may build a discriminative face alignment model only on a set of persons to be recognized rather than from a larger general person group. The subsequent face recognition step will likely be bottom-up.
- iterative mixture recognition may build discriminative face alignment models on each individual person to be recognized. Iterative mixture recognition may work in an iterative manner. For example, face recognition is performed from the results of face alignment in a bottom-up way. Then, face alignment is performed in a top-down way based on the results of face recognition. This process may repeat for any number of iterations.
- FIG. 3 illustrates an exemplary system 300 for face recognition, according to an embodiment.
- System 300 includes face detection module 310 , face alignment module 320 , signature extractor 330 , direct mixture recognizer 340 and alignment and recognition storage 350 .
- Face detection module 310 is configured to detect a face or a location of a face. Face detection module can pass this information to the face alignment module 320 . Alternatively, face detection module 310 may pass this information to or receive such information from alignment and recognition storage 350 .
- Alignment and recognition storage 350 may be a storage device or storage means. It may store annotated or non-annotated facial feature points, alignment models, signature features or signatures, reconstruction errors, recognition information, and/or any other information related to face detection, face alignment, and/or face recognition.
- Face alignment module 320 is configured to perform face alignment.
- face alignment module 320 may be configured to locate feature points of a face in one or more images using a face alignment model.
- a face alignment model may be developed from a general person group. Such a model may be referred to as a general purpose alignment model (GPFA).
- GPFA general purpose alignment model
- a face alignment model may incorporate class-specific knowledge. This class-specific knowledge may include information about a class of persons to be recognized. Such a model may be referred to as a global face alignment model (GLFA).
- class specific knowledge may include information only for a specific individual or person. Such a model may be referred to as an individual face alignment model (IFA).
- individual face alignment models may be developed only for a person, a group of persons, or a group of n persons to be recognized.
- face alignment may be performed based upon face alignment localization.
- global face alignment localization may be performed using global face alignment models.
- face alignment models including general purpose alignment models, global face alignment models and individual face alignment models, may be developed in alignment module 320 .
- face alignment models may be developed by or with the aid of alignment trainer 410 , as shown in FIG. 4 .
- the alignment trainer 410 of FIG. 4 is shown in the face alignment module 320 , but other embodiments may place alignment trainer 410 in another location in the system.
- face alignment models may be developed with information about facial feature points 312 .
- these facial feature points may be labeled or annotated to identify certain points on a face or facial features.
- face alignment may be performed with information received from face detection module 310 .
- face alignments 370 may be stored or received from alignment and recognition storage 350 .
- Face alignment models may be developed in different ways. According to an embodiment, face alignment models may be developed using Active Shape Models (ASM). In another embodiment, face alignment models may be developed using Active Appearance Models (AAM). ASM and AAM are popular face alignment methods. ASM uses a local appearance model, which represents local statistics around each landmark or feature to efficiently find target landmarks. The solution space may be constrained by a properly trained global shape model. AAM combines constraints on both shape and texture. A result shape may be extracted by minimizing a texture reconstruction error. According to optimization criteria, ASM may perform more accurately in shape localization while AAM may give a better match to image texture.
- ASM Active Shape Models
- AAM Active Appearance Models
- ASM is composed of two parts: a shape subspace model and a search procedure.
- a shape subspace model is a statistical model for a tangent shape space.
- a search procedure uses local appearance models to locate target shapes in an image. Some efforts may concentrate on a search procedure, while others may focus on a subspace model. However, it is possible that these methods may only concentrate on general purpose face alignment (GPFA) and may not consider their higher-level tasks.
- GPFA general purpose face alignment
- shapes may first be annotated in the image domain, according to an embodiment.
- shapes are aligned in a tangent shape space with Procrustes Analysis.
- ⁇ t ⁇ , which is a submatrix of ⁇ (the eigenvector matrix of the covariance matrix), contains the principle eigenvectors corresponding to the largest eigenvalues, and s is a vector of shape parameters. For a given shape, its shape parameter can be given by s ⁇ t T ( S ⁇ S ) (2)
- a search procedure may be performed with local appearance models, according to an embodiment.
- the local appearance models may describe local image features around each landmark.
- the local appearance model of each landmark may be modeled as the first derivative of the samples' profiles perpendicular to the landmark contour to reduce the effects of global intensity changes. They may be normalized by dividing by the sum of absolute element values. It may be assumed that the local models are distributed as a multivariate gaussian. For the jth landmark, we can derive the mean profile P j and the covariance matrix C Pj from all the jth profiles of the training examples.
- L the eigenvector matrix
- ⁇ the eigenvalue diagonal matrix.
- ASM may be performed to implement two kinds of class-specific face alignment methods: GLFA and IFA.
- ASM may be performed using GPFA, a traditional use of ASM. It may also be called GP-ASM.
- the training samples may come from a set of general persons which are not the persons to be recognized for face recognition.
- GP-ASM has the ability of generalization. However, it may lack the ability of specialization. Accordingly, GP-ASM may do well for all the persons in the general statistical sense, but it may not do very well for specific classes of persons.
- class-specific knowledge may be incorporated in ASM, according to another embodiment. There are two possible kinds of incorporation: global ASM (GL-ASM) and individual ASM (I-ASM).
- GL-ASM may use all the classes of persons to be recognized as the training samples for ASM. In some cases, GL-ASM may achieve better results because testing faces may also come from the training persons.
- an I-ASM model may be built semi-automatically. Images are first labeled or annotated with the help of efficient tools, such as constrained search, and then an I-ASM model is built. According to another embodiment, an I-ASM model may be built for each individual. A straightforward way to build this may be to collect some samples for each individual and train the I-ASM model with these samples. For an AFR system, if images of each person are annotated during enrollment or registration, the I-ASM model could be built directly from these samples.
- images may be manually or semi-automatically annotated with the help of constrained search and GP-ASM.
- face variation may be acquired for each individual. For example, in a BANCA database, each person may have images recorded with face variation by speaking some words.
- an I-ASM model may be built automatically, without manual help. There may be some occasions where there are not enough images. In other cases, face variation may not be available. For example, in a FERET fafb database, there is only one image for each person. In this case, an I-ASM model may be automatically set up.
- this one-dimensional point or space may be expanded to a multi-dimensional space to allow for variation, according to an embodiment.
- both a shape subspace and an appearance subspace (local appearance) model may be expanded.
- the global variation of persons may be used as the variation of each person.
- S S l + ⁇ t s l (8)
- Equation (8) shows that any shape S can be represented by a PCA shape space centered at point S l .
- Equation (1) is a global shape space, it represents the variation of a set of faces. The most significant variations may be the global changes that apply to most of the faces.
- FIG. 3 also shows signature extractor 330 .
- Signature extractor 330 may be configured to extract signature features or signatures.
- signature extractor 330 may be configured to generate reconstruction errors.
- signatures and reconstruction errors 390 may be stored in alignment and recognition storage 350 .
- Signature extractor 330 may also receive signatures and reconstructions errors 390 from alignment and recognition storage 350 .
- signatures and reconstruction errors 390 may be provided from signature extractor 330 to direct mixture recognizer 340 . It may also be possible for direct mixture recognizer 340 to receive signatures and reconstructions errors 390 from alignment and recognition storage 350 .
- FIG. 5 shows signature extractor 330 .
- signature extractor 330 may include key feature generator 510 .
- Key feature generator 510 may be configured to generate key features. Key features may include points or key features of a face. Key features may also include signature features or signatures. According to another embodiment, key feature generator 510 may generate one or more key features based upon location of facial feature points 380 received from face alignment module 320 .
- Key features may include distinguishable features based upon discriminative face alignment.
- I-ASM may provide distinguishable features for face recognition based upon positions of key feature points. After alignment is performed, key feature points may be used to extract an image patch for recognition.
- I-ASM may provide accurate alignment or localization for faces corresponding to face alignment models developed from the same faces. Likewise, I-ASM may provide bad alignment or localization for other faces. As a result, key feature points may be distinguishable for different individuals.
- Signature extractor 330 may include reconstruction error generator 520 .
- Reconstruction error generator 520 may be configured to generate reconstruction errors.
- signature extractor 330 may generate any measurements that can determine how well face alignment is performed for a face. For example, it may be determined how likely a face alignment result is like a face. In a further embodiment, any other metrics or measurements may be generated or used to determine how well face alignment is performed.
- I-ASM may provide distinguishable features for face recognition based upon reconstruction error.
- texture reconstruction error may derive from a texture PCA subspace. In this case, the texture inside a shape may be modified. The texture inside a shape may also be warped to a mean shape.
- textures from samples may be modeled with principle component analysis (PCA).
- PCA principle component analysis
- a texture inside the shape will likely be a face. In this case, a reconstruction error will be small. Otherwise, if a face is badly aligned, a texture inside the shape will likely not be a good face. Therefore, a reconstruction error will be large. As a result, a texture reconstruction error may also be a distinguishing feature.
- Positions of key feature points are more related to a local structure model, and reconstruction errors are more related to a shape subspace.
- positions of key feature points may be used by a recognition algorithm to produce a recognition confidence.
- a recognition confidence may be a similarity, probability, score, or similar representative metric.
- Reconstruction errors are normally not used by traditional recognition algorithms, as they are not distinguishable with GPFA.
- individual face alignment IFA
- signature extractor 330 may produce a confidence or confidence score.
- direct mixture recognizer 340 may also produce a confidence or confidence score.
- FIG. 3 shows direct mixture recognizer 340 .
- Direct mixture recognizer 340 may be configured to identify a face or a person by a face.
- Direct mixture recognizer 340 may be a recognizer, according to an embodiment.
- direct mixture recognizer 340 may be configured to cooperate with face alignment module 310 and signature extractor 330 to perform face recognition with discriminative face alignment.
- direct mixture recognizer 340 may be configured to provide identification and confidence score 360 .
- FIG. 6 illustrates an exemplary system 600 for face recognition, according to an embodiment.
- System 600 shows face detection module 310 , face alignment module 320 , signature extractor 330 , iterative mixture recognizer 610 and alignment and recognition storage 350 .
- Face detection module 310 is configured to detect a face.
- Face alignment module 320 is configured to perform face alignment.
- Face alignment module 320 may also develop individual face alignments, according to an embodiment.
- alignment trainer 410 may develop individual face alignments.
- Alignment and recognition storage 350 may store or provide individual face alignments 630 .
- FIG. 6 shows signature extractor 330 .
- signature extractor 330 may provide signatures.
- signature extractor 330 may provide reconstruction errors.
- signature extractor 330 may include key feature generator 510 .
- Key feature generator 510 may be configured to generate one or more key features.
- Key features may include signature features or signatures.
- Signature extractor 330 may include reconstruction error generator 520 , according to another embodiment.
- Reconstruction error generator 520 may be configured to generate reconstructions errors.
- Signatures and reconstruction errors 390 may be provided to iterative mixture recognizer 610 .
- FIG. 6 shows iterative mixture recognizer 610 .
- Iterative mixture recognizer 610 may be configured to identify a face or a person by a face. Iterative mixture recognizer 610 may be a recognizer, according to an embodiment. According to another embodiment, iterative mixture recognizer 610 may be configured to cooperate with face alignment module 310 and signature extractor 330 to perform face recognition with discriminative face alignment. According to a further embodiment, iterative mixture recognizer 610 may be configured to provide identification and confidence score 360 .
- Iterative mixture recognizer may also produce an individual alignment result ranking.
- An individual alignment result ranking may be based upon signatures and reconstruction errors 390 .
- face recognition may be performed with the results of face alignment in a bottom-up way.
- Appropriate IFA models may be chosen based on the results of face recognition to further improve face alignment in a top-down way, further improving face recognition.
- face recognition may work in an iterative way.
- ranking results of n persons 620 may be provided to face alignment module 320 . The process may be repeated starting with face alignment. Other iterations of this process may take place as necessary.
- Iterative mixture recognition may work similarly to direct mixture recognition.
- direct mixture recognition is shown in exemplary method 700 in FIG. 7 , according to an embodiment ( 710 - 730 ). Face detection 730 is first performed. Face recognition 710 and face alignment 720 may then work together as a mixture of top-down and bottom-up approaches. The embodiment shown by exemplary method 700 uses global face alignment (GLFA), incorporating knowledge of all persons to be recognized.
- GLFA global face alignment
- iterative mixture recognition is shown in exemplary system 800 in FIG. 8 , according to an embodiment ( 810 - 830 ). Face detection 830 is first performed. Face recognition 810 and face alignment 820 then work together as a mixture of top-down and bottom-up approaches.
- iterative mixture recognition may have a different approach to incorporating knowledge of persons to be recognized than direct mixture recognition.
- a training process of the iterative mixture face recognition may label or annotate faces of an individual by employing constrained or automatic general purpose face alignment (GPFA).
- the process may also employ global face alignment (GLFA).
- Individual face alignment (IFA) models may also be built based upon annotation. Face alignment models may be trained using results of GPFA/GLFA 828 and IFA 822 respectively.
- a testing process may use GPFA to align a testing face.
- a face alignment model trained from results of GPFA/GLFA 828 may use GPFA or GLFA to recognize the testing face.
- the top n recognition results 824 may be selected. For example, the top n possible persons for this testing face may be selected as the n possible persons.
- the testing face may be re-aligned with the IFA models 826 from each of the possible persons. This may provide n face alignment results.
- n face alignment results may be evaluated.
- the better of the n well-aligned results may be selected for refinement recognition.
- recognition may be refined.
- the better of the n well-aligned results may likely be the results modeled with an IFA model of person i. If an alignment result is recognized as person i with refinement recognition, or person i is the top recognition result 824 , person i may be called a recognition consistent person.
- the person is a recognition result, or identified.
- recognition consistent person There may be more than one recognition consistent person, according to another embodiment. For example, a recognition consistent person with the highest recognition confidence may have attained a recognition confidence that is high enough. In addition, a maximum number of iterations may have been reached. If this is the case, this recognition consistent person is the person that is identified. Otherwise, another n persons from the initial recognition result may be added to the possible persons. At this point, the process may return to the re-alignment step mentioned above. According to another embodiment, if there is no recognition consistent person and the maximum number of iterations has been reached, the top initial recognition result 824 will be the identification result. Otherwise, another n persons from the initial recognition result may be added to the possible person list and the process may return to the re-alignment step. This process may repeat for as many iterations as necessary.
- FIG. 9 illustrates an exemplary method 900 for face recognition, according to an embodiment.
- Facial feature points on faces of persons to be recognized may be annotated in one or more images in step 902 .
- Shapes may be trained and models may be textured for all persons based on the annotated facial feature points to get global face alignment models in step 904 .
- This step may be performed by face alignment module 320 .
- This step may also be performed by alignment trainer 410 .
- a face may be detected in one or more images. This may be performed by face detection module 310 .
- Signature features may be extracted in step 908 based upon global face alignment localization using the global face alignment models. This may be performed by signature extractor 330 . This may also be performed by key feature generator 510 . Reconstruction errors may be generated from global face alignment localization in step 910 using the global face alignment models. This may be performed by, signature extractor 330 . This may also be performed by reconstruction error generator 520 . In step 912 , extracted features and generated reconstruction errors may be used to identify persons to be recognized from faces in the images. This may be performed by direct mixture recognizer 340 .
- FIG. 10 illustrates an exemplary method 1000 for face recognition, according to an embodiment.
- Facial feature points on face of n persons to be recognized may be annotated in one or more images in step 1002 .
- Shapes may be trained and models may be textured for each person based on the annotated facial feature points to get individual face alignment models in step 1004 .
- This step may be performed by face alignment module 320 .
- This step may also be performed by alignment trainer 410 .
- a face may be detected in one or more images. This may be performed by face detection module 310 .
- Signature features may be extracted in step 1008 based upon face alignment localization using the individual face alignment models. This may be performed by signature extractor 330 . This may also be performed by key feature generator 510 . Reconstruction errors may be generated from face alignment localization in step 1010 using the individual face alignment models. This may be performed by signature extractor 330 . This may also be performed by reconstruction error generator 520 . In step 1012 , extracted features and generated reconstruction errors may be used to determine matching confidences for the n persons to be recognized from faces in the images. Matching confidences may be used to produce individual alignment result rankings in step 1014 . Persons to be recognized form faces in the images may be identified in step 1016 based upon the rankings. Steps 1012 , 1014 and 1016 may be performed by iterative mixture recognizer 610 . Steps 1006 through 1016 may be repeated as necessary. This may include passing rankings from iterative mixture recognizer 610 to face alignment module 320 .
- system 300 , system 400 , system 500 , system 600 , method 700 , method 800 , method 900 , and/or method 1000 or any part(s) or function(s) thereof may be implemented using hardware, software, computer program instructions recorded on a tangible medium to be executed by a processor, firmware, or a combination thereof and may be implemented in one or more computer systems or other processing systems.
- embodiments included herein may refer to human individuals or persons, the present invention is not limited to only human faces.
Abstract
Description
S=
where
s=φ t T(S−
F j=(P j)=(P j =
To reduce noise, PCA is applied,
C Pj =LΛL T ,P j =
where, L is the eigenvector matrix, and Λ is the eigenvalue diagonal matrix. So,
F j(P j)=(Lp)T(LΛL T)(Lp)=pΛp (5)
Therefore, the local appearance model may be an appearance subspace model.
Subtracting Equation (6) from Equation (1), we have:
S−
where sl=s−
S=
S l =
P jl =
where Ln is the first n eigenvectors with the largest eigenvalues from L.
T=
where t is a vector of texture parameters,
t=A l T(T−
The texture reconstruction error Errr of a given texture T is:
Err r =|[
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/438,582 US8705816B1 (en) | 2008-03-18 | 2012-04-03 | Face recognition with discriminative face alignment |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/050,872 US8165354B1 (en) | 2008-03-18 | 2008-03-18 | Face recognition with discriminative face alignment |
US13/438,582 US8705816B1 (en) | 2008-03-18 | 2012-04-03 | Face recognition with discriminative face alignment |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/050,872 Division US8165354B1 (en) | 2008-03-18 | 2008-03-18 | Face recognition with discriminative face alignment |
Publications (1)
Publication Number | Publication Date |
---|---|
US8705816B1 true US8705816B1 (en) | 2014-04-22 |
Family
ID=45953559
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/050,872 Expired - Fee Related US8165354B1 (en) | 2008-03-18 | 2008-03-18 | Face recognition with discriminative face alignment |
US13/438,582 Active US8705816B1 (en) | 2008-03-18 | 2012-04-03 | Face recognition with discriminative face alignment |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/050,872 Expired - Fee Related US8165354B1 (en) | 2008-03-18 | 2008-03-18 | Face recognition with discriminative face alignment |
Country Status (1)
Country | Link |
---|---|
US (2) | US8165354B1 (en) |
Families Citing this family (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2010186288A (en) * | 2009-02-12 | 2010-08-26 | Seiko Epson Corp | Image processing for changing predetermined texture characteristic amount of face image |
KR101633620B1 (en) * | 2010-01-04 | 2016-06-27 | 삼성전자 주식회사 | Feature registration apparatus for image based localization and method the same |
US8824808B2 (en) * | 2011-08-19 | 2014-09-02 | Adobe Systems Incorporated | Methods and apparatus for automated facial feature localization |
US11074495B2 (en) | 2013-02-28 | 2021-07-27 | Z Advanced Computing, Inc. (Zac) | System and method for extremely efficient image and pattern recognition and artificial intelligence platform |
US11195057B2 (en) | 2014-03-18 | 2021-12-07 | Z Advanced Computing, Inc. | System and method for extremely efficient image and pattern recognition and artificial intelligence platform |
US8873813B2 (en) | 2012-09-17 | 2014-10-28 | Z Advanced Computing, Inc. | Application of Z-webs and Z-factors to analytics, search engine, learning, recognition, natural language, and other utilities |
US9916538B2 (en) | 2012-09-15 | 2018-03-13 | Z Advanced Computing, Inc. | Method and system for feature detection |
US11914674B2 (en) | 2011-09-24 | 2024-02-27 | Z Advanced Computing, Inc. | System and method for extremely efficient image and pattern recognition and artificial intelligence platform |
US8666992B2 (en) * | 2012-06-15 | 2014-03-04 | Xerox Corporation | Privacy preserving method for querying a remote public service |
US8886953B1 (en) * | 2012-09-14 | 2014-11-11 | Google Inc. | Image processing |
CN103714318A (en) * | 2013-12-13 | 2014-04-09 | 谭玉波 | Three-dimension face registration method |
CN104715227B (en) | 2013-12-13 | 2020-04-03 | 北京三星通信技术研究有限公司 | Method and device for positioning key points of human face |
US9286682B1 (en) * | 2014-11-21 | 2016-03-15 | Adobe Systems Incorporated | Aligning multi-view scans |
CN108304846B (en) | 2017-09-11 | 2021-10-22 | 腾讯科技（深圳）有限公司 | Image recognition method, device and storage medium |
CN108764306B (en) | 2018-05-15 | 2022-04-22 | 深圳大学 | Image classification method and device, computer equipment and storage medium |
CN109359575B (en) * | 2018-09-30 | 2022-05-10 | 腾讯科技（深圳）有限公司 | Face detection method, service processing method, device, terminal and medium |
EP3965002B1 (en) * | 2020-09-03 | 2023-08-16 | Toyota Jidosha Kabushiki Kaisha | Anisotropic loss function for training a model to localise feature points |
Citations (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5710833A (en) | 1995-04-20 | 1998-01-20 | Massachusetts Institute Of Technology | Detection, recognition and coding of complex objects using probabilistic eigenspace analysis |
US5799098A (en) | 1994-10-20 | 1998-08-25 | Calspan Corporation | Fingerprint identification system |
US20010028731A1 (en) | 1996-05-21 | 2001-10-11 | Michele Covell | Canonical correlation analysis of image/control-point location coupling for the automatic location of control points |
US20040170323A1 (en) * | 2001-05-25 | 2004-09-02 | Cootes Timothy F | Object identification |
US20050169505A1 (en) | 1999-12-23 | 2005-08-04 | National University Of Singapore | Wavelet-enhanced automated fingerprint identification system |
US7058209B2 (en) | 2001-09-20 | 2006-06-06 | Eastman Kodak Company | Method and computer program product for locating facial features |
US20060153432A1 (en) | 2005-01-07 | 2006-07-13 | Lo Peter Z | Adaptive fingerprint matching method and apparatus |
US20060193515A1 (en) * | 2002-10-31 | 2006-08-31 | Korea Institute Of Science And Technology | Image processing method for removing glasses from color facial images |
US7120279B2 (en) | 2003-01-30 | 2006-10-10 | Eastman Kodak Company | Method for face orientation determination in digital color images |
US20060253491A1 (en) * | 2005-05-09 | 2006-11-09 | Gokturk Salih B | System and method for enabling search and retrieval from image files based on recognized information |
US20070046426A1 (en) | 2005-08-26 | 2007-03-01 | Kabushiki Kaisha Toshiba | Admittance management system and admittance management method |
US20080013798A1 (en) * | 2006-06-12 | 2008-01-17 | Fotonation Vision Limited | Advances in extending the aam techniques from grayscale to color images |
US20080075336A1 (en) | 2006-09-26 | 2008-03-27 | Huitao Luo | Extracting features from face regions and auxiliary identification regions of images for person recognition and other applications |
US20080273767A1 (en) | 2007-05-01 | 2008-11-06 | Motorola, Inc. | Iterative print matching method and system |
US20080298642A1 (en) | 2006-11-03 | 2008-12-04 | Snowflake Technologies Corporation | Method and apparatus for extraction and matching of biometric detail |
US20090169072A1 (en) | 2007-12-31 | 2009-07-02 | Motorola, Inc. | Method and system for comparing prints using a reconstructed direction image |
US20090180672A1 (en) | 2006-04-14 | 2009-07-16 | Nec Corporation | Collation apparatus and collation method |
US7587068B1 (en) | 2004-01-22 | 2009-09-08 | Fotonation Vision Limited | Classification database for consumer digital images |
US20100013832A1 (en) * | 2008-07-16 | 2010-01-21 | Jing Xiao | Model-Based Object Image Processing |
US20100316265A1 (en) | 2006-12-13 | 2010-12-16 | Panasonic Corporation | Face authentication device |
-
2008
- 2008-03-18 US US12/050,872 patent/US8165354B1/en not_active Expired - Fee Related
-
2012
- 2012-04-03 US US13/438,582 patent/US8705816B1/en active Active
Patent Citations (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5799098A (en) | 1994-10-20 | 1998-08-25 | Calspan Corporation | Fingerprint identification system |
US5710833A (en) | 1995-04-20 | 1998-01-20 | Massachusetts Institute Of Technology | Detection, recognition and coding of complex objects using probabilistic eigenspace analysis |
US20010028731A1 (en) | 1996-05-21 | 2001-10-11 | Michele Covell | Canonical correlation analysis of image/control-point location coupling for the automatic location of control points |
US20050169505A1 (en) | 1999-12-23 | 2005-08-04 | National University Of Singapore | Wavelet-enhanced automated fingerprint identification system |
US20040170323A1 (en) * | 2001-05-25 | 2004-09-02 | Cootes Timothy F | Object identification |
US7295709B2 (en) | 2001-05-25 | 2007-11-13 | The Victoria University Of Manchester | Object identification |
US7058209B2 (en) | 2001-09-20 | 2006-06-06 | Eastman Kodak Company | Method and computer program product for locating facial features |
US20060193515A1 (en) * | 2002-10-31 | 2006-08-31 | Korea Institute Of Science And Technology | Image processing method for removing glasses from color facial images |
US7120279B2 (en) | 2003-01-30 | 2006-10-10 | Eastman Kodak Company | Method for face orientation determination in digital color images |
US7587068B1 (en) | 2004-01-22 | 2009-09-08 | Fotonation Vision Limited | Classification database for consumer digital images |
US20060153432A1 (en) | 2005-01-07 | 2006-07-13 | Lo Peter Z | Adaptive fingerprint matching method and apparatus |
US20060253491A1 (en) * | 2005-05-09 | 2006-11-09 | Gokturk Salih B | System and method for enabling search and retrieval from image files based on recognized information |
US20070046426A1 (en) | 2005-08-26 | 2007-03-01 | Kabushiki Kaisha Toshiba | Admittance management system and admittance management method |
US20090180672A1 (en) | 2006-04-14 | 2009-07-16 | Nec Corporation | Collation apparatus and collation method |
US20080013798A1 (en) * | 2006-06-12 | 2008-01-17 | Fotonation Vision Limited | Advances in extending the aam techniques from grayscale to color images |
US20080075336A1 (en) | 2006-09-26 | 2008-03-27 | Huitao Luo | Extracting features from face regions and auxiliary identification regions of images for person recognition and other applications |
US7689011B2 (en) | 2006-09-26 | 2010-03-30 | Hewlett-Packard Development Company, L.P. | Extracting features from face regions and auxiliary identification regions of images for person recognition and other applications |
US20080298642A1 (en) | 2006-11-03 | 2008-12-04 | Snowflake Technologies Corporation | Method and apparatus for extraction and matching of biometric detail |
US20100316265A1 (en) | 2006-12-13 | 2010-12-16 | Panasonic Corporation | Face authentication device |
US20080273767A1 (en) | 2007-05-01 | 2008-11-06 | Motorola, Inc. | Iterative print matching method and system |
US20090169072A1 (en) | 2007-12-31 | 2009-07-02 | Motorola, Inc. | Method and system for comparing prints using a reconstructed direction image |
US20100013832A1 (en) * | 2008-07-16 | 2010-01-21 | Jing Xiao | Model-Based Object Image Processing |
Non-Patent Citations (25)
Title |
---|
Bailly-Baillere, Enrique et al., The BANCA Database and Evaluation Protocol, In 4th International Conference on Audio- and Video-Based Biometric Person Authentication, Surrey, Berlin, 2003, Springer-Verlag, pp. 625-638. |
Ballard, Dana H. et al., Computer Vision, Chapter 10, pp. 340-351, Prentice-Hall, 1982. |
Bolme, David S. et al., The CSU Face Identification Evaluation System: Its Purpose, Features, and Structure, In Third International Conference on Computer Vision Systems, pp. 304-313, 2003. |
Borenstein, Eran et al., Combining Top-Down and Bottom-Up Segmentation, In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, vol. 4, 8 pages. |
Borenstein, Iran et al., Class-Specific, Top-Down Segmentation, In ECCV, pp. 109-122, Copenhagen, Denmark, May 28-31, 2002. |
Chellappa, Rama et al., Human and Machine Recognition of Faces: A Survey, Proceedings of the IEEE, vol. 83, No. 5, May 1995, pp. 705-740. |
Cootes, T.F. et al., Active Shape Models: Their Training and Application, CVGIP: Image Understanding, vol. 61, No. 1, Jan. 1995, pp. 38-59. |
Cootes, T.F. et al., Constrained Active Appearance Models, In Proceedings of IEEE International Conference on Computer Vision, pp. 748-754, Vancouver, Canada, Jul. 2001. |
Cootes, T.F. et al., Statistical Models of Appearance for Computer Vision, Technical Report, www.isbe.man.ac.uk/~bim/refs.html (2001), pp. 1-120. |
Cootes, T.F. et al., Statistical Models of Appearance for Computer Vision, Technical Report, www.isbe.man.ac.uk/˜bim/refs.html (2001), pp. 1-120. |
Cootes, Timothy F. et al., Active Appearance Models, IEEE Transactions on Pattern Analysis and Machine intelligence, vol. 23, No. 6, Jun. 2001, pp. 681-685. |
Cristinacce, D. et al., A Comparison of Shape Constrained Facial Feature Detectors, In: Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition (2004), pp. 375-380. |
Davies, Rhodri H. et al., A Minimum Description Length Approach to Statistical Shape Modeling, IEEE Transactions on Medical Imaging, 21:525-537, 2002. |
Ginneken, Bram van et al., Active Shape Model Segmentation with Optimal Features, IEEE Transactions on Medical Imaging 21 (2002) pp. 924-933. |
Gross, Ralph et al., Generic vs. Person Specific Active Appearance Models, In British Machine Vision Conference, Sep. 2004, pp. 457-466. |
Hill, Andrew et al., A Framework for Automatic Landmark Identification Using a New Method of Nonrigid Correspondence, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, No. 3, Mar. 2000, pp. 241-251. |
Lienhart, Rainer et al., An Extended Set of Haar-Like Features for Rapid Object Detection, In: Proceedings of IEEE International Conference on Image Processing, vol. 1, (2002), pp. 900-903. |
Liu, Ce et al., Hierarchical Shape Modeling for Automatic Face Localization, In Proceedings of the European Conference on Computer Vision, No. II, Copenhagen, Denmark, May 2002, pp. 687-703. |
Phillips, P. Jonathan et al., The FERET Evaluation Methodology for Face-Recognition Algorithms, IEEE Transactions on Pattern Analysis and Machine Intelligence, 22 (2000) pp. 1090-1104. |
Rogers, Mike et al., Robust Active Shape Model Search, In Proceedings of the European Conference on Computer Vision, No. IV, pp. 517-530, Copenhagen, Denmark, May 2002. |
Viola, Paul et al., Robust Real Time Object Detection, In IEEE ICCV Workshop on Statistical and Computational Theories of Vision, Vancouver, Canada, Jul. 13, 2001, p. 747. |
Yan, Shuicheng et al., Ranking Prior Likelihood Distribution for Bayesian Shape Localization Framework, IEEE International Conference on Computer Vision, vol. 1, pp. 51-58, Nice, France, Oct. 2003. |
Yan, Shuicheng et al., Texture-Constrained Active Shape Models, May 2002, pp. 107-113. |
Zhao, Ming et al., Face Aligment with Unified Subspace Optimization of Active Statistical Models, In the 7th IEEE International Conference on Automatic Face and Gesture Recognition, Southampton, UK, Apr. 2006, pp. 67-72. |
Zhao, W. et al., Face Recognition: A Literature Survey, ACM Computing Surveys, vol. 35, No. 4, Dec. 2003, pp. 399-458. |
Also Published As
Publication number | Publication date |
---|---|
US8165354B1 (en) | 2012-04-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8705816B1 (en) | Face recognition with discriminative face alignment | |
Dantone et al. | Real-time facial feature detection using conditional regression forests | |
Gkioxari et al. | Using k-poselets for detecting people and localizing their keypoints | |
Liang et al. | Face alignment via component-based discriminative search | |
US7596247B2 (en) | Method and apparatus for object recognition using probability models | |
Ferrari et al. | From images to shape models for object detection | |
Alyuz et al. | Regional registration for expression resistant 3-D face recognition | |
WO2017024963A1 (en) | Image recognition method, measure learning method and image source recognition method and device | |
WO2020211339A1 (en) | Finger vein recognition method and apparatus, and computer device and storage medium | |
Dibeklioglu et al. | 3D facial landmarking under expression, pose, and occlusion variations | |
Sangineto | Pose and expression independent facial landmark localization using dense-SURF and the Hausdorff distance | |
US20110293189A1 (en) | Facial Analysis Techniques | |
US11495057B2 (en) | Person verification device and method and non-transitory computer readable media | |
Kacete et al. | Real-time eye pupil localization using Hough regression forest | |
Campadelli et al. | Eye localization: a survey | |
Lin et al. | Learning contour-fragment-based shape model with and-or tree representation | |
Hasan et al. | Improving alignment of faces for recognition | |
Lenc et al. | Face Recognition under Real-world Conditions. | |
Jiang et al. | Unifying spatial and attribute selection for distracter-resilient tracking | |
Xiao et al. | Object detection based on contour learning and template matching | |
Hahmann et al. | Combination of facial landmarks for robust eye localization using the Discriminative Generalized Hough Transform | |
De Marsico et al. | Measuring sample distortions in face recognition | |
CN112270275B (en) | Commodity searching method and device based on picture identification and computer equipment | |
Ghaffary et al. | Profile-based face recognition using the outline curve of the profile Silhouette | |
Hsu et al. | Robust cross-pose face recognition using landmark oriented depth warping |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:ZHAO, MING;REEL/FRAME:032327/0146Effective date: 20080318 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044277/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |