US10402469B2 - Systems and methods of distributed optimization - Google Patents
Systems and methods of distributed optimization Download PDFInfo
- Publication number
- US10402469B2 US10402469B2 US15/045,707 US201615045707A US10402469B2 US 10402469 B2 US10402469 B2 US 10402469B2 US 201615045707 A US201615045707 A US 201615045707A US 10402469 B2 US10402469 B2 US 10402469B2
- Authority
- US
- United States
- Prior art keywords
- local
- model
- update
- global
- user device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/17—Function evaluation by approximation methods, e.g. inter- or extrapolation, smoothing, least mean square method
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/11—Complex mathematical operations for solving equations, e.g. nonlinear equations, general mathematical optimization problems
-
- G06F17/50—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F30/00—Computer-aided design [CAD]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2111/00—Details relating to CAD techniques
- G06F2111/02—CAD in a network environment, e.g. collaborative CAD or distributed simulation
-
- G06F2217/04—
Definitions
- the present disclosure relates generally to distributed optimization algorithms, and more particularly to performing optimization algorithms using data distributed unevenly among a large number of computational nodes.
- One example aspect of the present disclosure is directed to a computer-implemented method of updating a global model based on unevenly distributed data.
- the method includes receiving, by one or more computing devices, one or more local updates from a plurality of user devices. Each local update is determined by the respective user device based at least in part on one or more data examples stored on the respective user device.
- the one or more data examples stored on the plurality of user devices are distributed on an uneven basis, such that no user device includes a representative sample of an overall distribution of data examples.
- the method further includes aggregating, by the one or more computing devices, the received local updates to determine a global model.
- FIG. 1 depicts an example computing system for implementing optimization techniques over a plurality of nodes according to example embodiments of the present disclosure
- FIG. 2 depicts a flow diagram of an example method of determining a global model according to example aspects of the present disclosure
- FIG. 3 depicts an example system according to example embodiments of the present disclosure.
- Example aspects of the present disclosure are directed to solving optimization problems defined by an unevenly distributed dataset. For instance, data points used in solving an optimization problem may be stored across a plurality of devices. In this manner, the data on each device may be differently distributed, and may not be a representative sample of the overall distribution of data. For instance, each device may store different amounts of data, and/or different types of data. For instance, some data features may occur on only a small subset of devices.
- a central computing device such as a server or data center can provide data indicative of a current global model to the plurality of computing devices.
- the plurality of computing devices can determine one or more local updates to the model using data stored on the respective computing devices.
- at least one of the computing devices can be a user device, such as a wearable computing device or other user device.
- the data examples may be generated, for instance, through interaction of a user with the user device.
- the local update can correspond to a model using data generated through use of the user device by the user.
- the data examples may include, without limitation, image files, video files, inputted text, or data indicative of other interactions by the user with the user device.
- the model may be implemented in solving one or more problems, such as predictive typing, predictive image sharing, image classification, voice recognition, next-word-prediction, and/or various other suitable problems relating to use of the user device.
- the local update can be a gradient vector.
- the local update may be determined using one or more gradient descent techniques.
- the local update may be determined using batch gradient descent techniques, stochastic gradient descent techniques, or other gradient descent techniques.
- the local update does not include the training data used to determine the local update. In this manner, the size of the local update can be independent of the training data used to determine the local update, thereby reducing bandwidth requirements and maintaining user privacy.
- a global model can be updated based at least in part on the received local updates.
- the global model update can be determined using reduced bandwidth requirements, and without compromising the security of potentially privacy sensitive data stored on the user devices.
- the global model can be trained using a smaller number of communication rounds, and so this also reduces the network bandwidth needed for training by orders of magnitude compared to copying the data to the datacenter.
- the global model update can be a global model parameter vector.
- the global model update can be determined by aggregating each local update received by the server. In some implementations, the aggregation can include determining a weighted average of the received local updates.
- randomization techniques from differential privacy can be used.
- the centralized algorithm could be modified to produce a differentially private model, which allows the model to be released while protecting the privacy of the individuals contributing updates to the training process. If protection from even a malicious (or compromised) centralized model trainer is needed, techniques from local differential privacy can be applied to privatize the individual updates.
- a set of input-output data can be used to describe a global objective via a loss function.
- Such functions can be, for instance, a convex or non-convex function, such as a linear regression function, logistic regression function, a support vector machine, neural network function and/or various other suitable functions.
- a local objective (F k ) can also be defined using data stored on a computing device.
- the global objective can be defined as:
- the local objective can specify an optimization problem to be solved by device k.
- the global objective can be solved by aggregating a plurality of local updates provided by a plurality of remote computing devices.
- Each remote computing device can, for instance, be a user device, such as a laptop computing device, desktop computing device, smartphone, tablet, wearable computing device, or other computing device.
- the local updates can be determined based at least in part on the respective local objectives.
- a gradient of the global objective can be determined.
- each computing device can be configured to determine the gradient of the local objective using the data examples stored on the computing device.
- the global gradient may be equal to a weighted sum of the local gradients.
- each remote computing device can be configured to provide the determined gradient to the central computing device (e.g. server device, data center, etc.).
- the central device can then be configured to determine a gradient of the global objective based at least in part on the local objective gradients, and then to provide the gradient to the remote computing devices.
- the gradient can be defined as:
- Each remote computing device can then determine a local update based at least in part on the global gradient.
- the local update can be determined using one or more gradient descent techniques (e.g. stochastic gradient descent).
- each remote computing device can perform one or more stochastic updates or iterations to determine the local update.
- each remote computing device can initialize one or more parameters associated with the local objective.
- Each remote computing device can then, for instance, uniformly, randomly sample Pk for one or more stochastic iterations.
- the local update can be determined based at least in part on the sampled data.
- the local update can be defined as:
- the local updates can then be provided to the central computing device.
- the local updates can be gradient vectors.
- the central computing device can then aggregate the local updates to determine a global update to the model.
- the aggregation can be an averaging aggregation defined as:
- the threshold can be determined based at least in part on a desired accuracy of the global model.
- the threshold can be determined based at least in part on the amount of time required for communication between the central computing device and the remote computing devices.
- the above process can be modified to achieve good empirical performance in the full federated setting.
- This setting can come with the data available locally being clustered around specific pattern, and thus not being a representative sample of the overall distribution that the system is attempting to learn.
- aspects of the present disclosure provide solutions when encountering a setting with data sparsity. If one looks at how frequently a feature appears in the whole dataset, and find out that locally, the feature appears much more often, the node with such local data is going to overshoot in estimates of gradient in this feature. Simply because the device samples this feature more often than the system would if it were sampling from the whole dataset. For this reason, among others, embodiments of the present disclosure scale local stochastic gradients by ratio of per-feature appearance frequencies globally, and locally.
- n number of data examples/data points/functions.
- n k
- n j
- n k j
- ⁇ j n j /n—frequency of appearance of nonzero elements in j th coordinate.
- ⁇ k j n k j /n k —frequency of appearance of nonzero elements in j th coordinate on node k.
- s k j ⁇ j / ⁇ k j —ratio of global and local appearance frequencies on node k in j th coordinate.
- S k Diag(s k j )—diagonal matrix, composed of s k j as j th diagonal element.
- w j
- a j K/w j —aggregation parameter for coordinate j.
- A Diag(a j )—diagonal matrix composed of a j as j th diagonal element.
- the local objective can further be determined based at least in part on the distribution of data among the remote computing devices.
- the local objective or stochastic gradient can be scaled using a diagonal matrix (S k ). In this manner, the local objective can be defined as:
- the aggregation of local updates by the central computing device can be proportional to the number of data examples (e.g.,
- the aggregation can further include a per-coordinate scaling of aggregated updates (e.g., A(w k ⁇ tilde over (w) ⁇ )).
- the global model can be an additive aggregation determined as follows:
- Such embodiments can be determined under the assumption that, as some point in time, each remote computing device has the same local iterate. In this manner, each remote computing device can begin from the same point.
- a linear term: ⁇ F k ( ⁇ tilde over (w) ⁇ s ) ⁇ ⁇ circumflex over (f) ⁇ ( ⁇ tilde over (w) ⁇ s ) can be added to the local objective to force all nodes to initially move in the same direction.
- the local stepsize h k can be included, for instance, when the number of data examples on each remote computing device is not equal. In such embodiments, it may not be beneficial to perform the same number of stochastic iterations on each remote device. In this manner, setting the stepsize as inversely proportional to the number of data examples on the remote device can allow each remote device to travel roughly the same distance. In this manner, the local and global model updates can be implemented as follows:
- the local objective can be defined as follows:
- w k argmin w ⁇ R d ⁇ ⁇ n k ⁇ F k ⁇ ( w ) + ( n ⁇ ⁇ f ⁇ ( w ⁇ s ) - n k ⁇ ⁇ F k ⁇ ( w ⁇ s ) ) T ⁇ w + ⁇ 2 ⁇ ⁇ U 1 2 ⁇ ( w - w ⁇ s ) ⁇ 2 ⁇ .
- a diagonal matrix U ⁇ d ⁇ d is defined having diagonal entries:
- the u j can correspond to a curvature in dimension i.
- w j can be determined as desired by the remote computing device.
- the regularization is infinitely strong, and the coefficients of features for which no data is present may not be changed.
- stochastic gradient descent techniques can be naively applied to the optimization problem, wherein one or more “minibatch” gradient calculations (e.g. using one or more randomly selected use devices) are performed per round of communication.
- the minibatch can include at least a subset of the training data stored locally on the user devices.
- one or more user devices can be configured to determine the average gradient associated with the local training data respectively stored on the user devices for a current version of a model.
- the user devices can be configured to provide the determined gradients to the server, as part of the local updates.
- the server can then aggregate the gradients to determine a global model update.
- one or more user devices can perform one or more “steps” of gradient descent on the current model using at least a subset of the local training data respectively stored on the user devices, and provide data indicative of the one or more gradient descent steps to the server.
- the server can then aggregate the data, for instance, by determining a weighted average.
- the user devices may determine an updated version of the model (e.g. using one or more stochastic gradient descent techniques) using local data.
- the server can then determine a weighted average of the resulting models to determine a global update to the model.
- FIG. 1 depicts an example system 300 for training one or more global machine learning models 306 using training data 308 stored locally on a plurality of user devices 302 .
- System 300 can further include a server device 304 .
- Server 304 can be configured to access machine learning model 306 , and to provide model 306 to a plurality of user devices 302 .
- Model 306 can be, for instance, a linear regression model, logistic regression model, a support vector machine model, a neural network (e.g. convolutional neural network, recurrent neural network, etc.), or other suitable model.
- sever 304 can be configured to communicate with user devices 302 over one or more networks, such as network 240 of FIG. 3 .
- Training data 308 can be data stored locally on the user devices 306 .
- training data 308 can include audio files, image files, video files, a typing history, location history, and/or various other suitable data.
- the training data can be any data derived through a user interaction with a user device 302 .
- the local updates can be a gradient vector associated with the model.
- user devices 302 can determine a gradient (e.g. an average gradient) associated with the model based at least in part on training data 308 respectively stored on user devices 302 .
- the local update can include an updated version of model 306 determined by taking one or more stochastic gradient descent steps. It will be appreciated that the local updates can include any suitable data indicative of one or more local training techniques respectively performed by user devices 302 based at least in part on training data 308 .
- the local updates can include one or more training passes, or epochs. In this manner the minibatch of training data can be applied one or more times to determine the local update.
- Training data 308 may be privacy sensitive. In this manner, the local updates can be performed and provided to server 304 without compromising the privacy of training data 308 . For instance, in such implementations, training data 308 is not provided to server 304 . The local update does not include training data 308 .
- privacy sensitive data may be able to be derived or inferred from the model parameters. In such implementations, one or more encryption, random noise techniques, and/or other security techniques can be added to the training process to obscure any inferable information.
- server 304 can receive each local update from user device 302 , and can aggregate the local updates to determine a global update to the model. In some implementations, server 304 can determine a weighted average of the local updates and determine the global update based at least in part on the average.
- scaling or other techniques can be applied to the local updates to determine the global update. For instance, a local stepsize can be applied for each user device 302 , the aggregation can be performed proportionally to various data partition sizes of user devices 302 , and/or one or more scaling factors can be applied to the local and/or aggregated updates. It will be appreciated that various other techniques can be applied without deviating from the scope of the present disclosure.
- FIG. 2 depicts a flow diagram of an example method ( 100 ) of determining a global model according to example embodiments of the present disclosure.
- Method ( 100 ) can be implemented by one or more computing devices, such as one or more of the computing devices depicted in FIG. 3 .
- FIG. 2 depicts steps performed in a particular order for purposes of illustration and discussion. Those of ordinary skill in the art, using the disclosures provided herein, will understand that the steps of any of the methods discussed herein can be adapted, rearranged, expanded, omitted, or modified in various ways without deviating from the scope of the present disclosure.
- method ( 100 ) can include determining, by a user device, a local gradient based on one or more local data examples.
- the local gradient can be determined for a loss function using the one or more data examples.
- the user device can be a wearable computing device.
- the data examples may be generated, for instance, through interaction of a user with the user device.
- method ( 100 ) can include providing, by the user device, the local gradient to a server, and at ( 106 ), method ( 100 ) can include receiving, by the server, the local gradient.
- method ( 100 ) can include determining, by the server, a global gradient based at least in part on the received local gradient. For instance, the can be determined based at least in part on a plurality of local gradients provided by a plurality of user devices, each having a plurality of unevenly distributed data examples.
- the data example may be distributed among the user devices such that no user device includes a representative sample of the overall distribution of data.
- the number of user devices may exceed the number of data examples on any one user device.
- method ( 100 ) can include providing the global gradient to each user device, and at ( 112 ), method ( 100 ) can include receiving the global gradient.
- method ( 100 ) can include determining, by the user device, a local update.
- the local update can be determined based at least in part on the global update.
- the global gradient may not be required to determine the local update.
- each local gradient may be identical.
- the global gradient may not be determined by the server, and method ( 100 ) can bypass ( 104 )-( 112 ).
- the local update can be determined based at least in part using one or more stochastic updates or iterations. For instance, the user device may randomly sample a partition of data examples stored on the user device to determine the local update. In particular, the local update may be determined using stochastic gradient descent techniques to determine a direction in which to adjust one or more parameters of the loss function.
- a stepsize associated with the local update determination can be determined based at least in part on a number of data examples stored on the user device.
- the stochastic gradient can be scaled using a diagonal matrix, or other scaling technique.
- the local update can be determined using a linear term that forces each user device to update the parameters of the loss function in the same direction.
- method ( 100 ) can include providing, by the user device, the local update to the server, and at ( 118 ), method ( 100 ) can include receiving, by the server, the local update.
- the server can receive a plurality of local updates from a plurality of user devices.
- method ( 100 ) can include determining a global model.
- the global model can be determined based at least in part on the received local update(s).
- the received local updates can be aggregated to determine the global model.
- the aggregation can be an additive aggregation and/or an averaging aggregation.
- the aggregation of the local updates can be proportional to the partition sizes of the data examples on the user devices.
- the aggregation of the local updates can be scaled on a per-coordinate basis. Any number of iterations of local and global updates can be performed.
- FIG. 3 depicts an example computing system 200 that can be used to implement the methods and systems according to example aspects of the present disclosure.
- the system 200 can be implemented using a client-server architecture that includes a server 210 that communicates with one or more client devices 230 over a network 240 .
- the system 200 includes a server 210 , such as a web server.
- the server 210 can be implemented using any suitable computing device(s), and can correspond to server 304 of FIG. 1 .
- the server 210 can have one or more processors 212 and one or more memory devices 214 .
- the server 210 can also include a network interface used to communicate with one or more client devices 230 over the network 240 .
- the network interface can include any suitable components for interfacing with one more networks, including for example, transmitters, receivers, ports, controllers, antennas, or other suitable components.
- the one or more processors 212 can include any suitable processing device, such as a microprocessor, microcontroller, integrated circuit, logic device, or other suitable processing device.
- the one or more memory devices 214 can include one or more computer-readable media, including, but not limited to, non-transitory computer-readable media, RAM, ROM, hard drives, flash drives, or other memory devices.
- the one or more memory devices 214 can store information accessible by the one or more processors 212 , including computer-readable instructions 216 that can be executed by the one or more processors 212 .
- the instructions 216 can be any set of instructions that when executed by the one or more processors 212 , cause the one or more processors 212 to perform operations. For instance, the instructions 216 can be executed by the one or more processors 212 to implement global updater 220 .
- the global updater 220 can be configured to receive one or more local updates and to determine a global model based at least in part on the local updates.
- the one or more memory devices 214 can also store data 218 that can be retrieved, manipulated, created, or stored by the one or more processors 212 .
- the data 218 can include, for instance, local updates, global parameters, and other data.
- the data 218 can be stored in one or more databases.
- the one or more databases can be connected to the server 210 by a high bandwidth LAN or WAN, or can also be connected to server 210 through network 240 .
- the one or more databases can be split up so that they are located in multiple locales.
- the server 210 can exchange data with one or more client devices 230 over the network 240 .
- client devices 230 can be connected to the server 210 over the network 240 .
- Each of the client devices 230 can be any suitable type of computing device, such as a general purpose computer, special purpose computer, laptop, desktop, mobile device, navigation system, smartphone, tablet, wearable computing device, a display with one or more processors, or other suitable computing device.
- the client devices 230 can correspond to user devices 302 of FIG. 1 .
- a client device 230 can include one or more processor(s) 232 and a memory 234 .
- the one or more processor(s) 232 can include one or more central processing units (CPUs), graphics processing units (GPUs) dedicated to efficiently rendering images or performing other specialized calculations, and/or other processing devices.
- the memory 234 can include one or more computer-readable media and can store information accessible by the one or more processors 232 , including instructions 236 that can be executed by the one or more processors 232 and data 238 .
- the data 238 can include one or more data examples to be used in solving one or more optimization problems.
- each client device 230 can be distributed unevenly among the client devices, such that no client device 230 includes a representative sample of the overall distribution of the data examples.
- the memory 234 can store instructions 236 for implementing a local updater configured to determine one or more local updates according to example aspects of the present disclosure.
- the client device 230 of FIG. 3 can include various input/output devices for providing and receiving information from a user, such as a touch screen, touch pad, data entry keys, speakers, and/or a microphone suitable for voice recognition.
- the client device 230 can also include a network interface used to communicate with one or more remote computing devices (e.g. server 210 ) over the network 240 .
- the network interface can include any suitable components for interfacing with one more networks, including for example, transmitters, receivers, ports, controllers, antennas, or other suitable components.
- the network 240 can be any type of communications network, such as a local area network (e.g. intranet), wide area network (e.g. Internet), cellular network, or some combination thereof.
- the network 240 can also include a direct connection between a client device 230 and the server 210 .
- communication between the server 210 and a client device 230 can be carried via network interface using any type of wired and/or wireless connection, using a variety of communication protocols (e.g. TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g. HTML, XML), and/or protection schemes (e.g. VPN, secure HTTP, SSL).
- server processes discussed herein may be implemented using a single server or multiple servers working in combination.
- Databases and applications may be implemented on a single system or distributed across multiple systems. Distributed components may operate sequentially or in parallel.
Abstract
Description
-
- for t=1 to m do
- Sample i∈
w k =w k −h(∇f i(w k)−∇f i({tilde over (w)} s)+∇f({tilde over (w)} s))
wherein m is a number of stochastic steps per iteration, and h is the stepsize.
- Sample i∈
- for t=1 to m do
This can be repeated for one or more iterations, for instance, until the loss function reaches a threshold (e.g. converges). The threshold can be determined based at least in part on a desired accuracy of the global model. As another example, the threshold can be determined based at least in part on the amount of time required for communication between the central computing device and the remote computing devices.
-
- Let {ik}t=1 n
k be random permutation of - for t=1, . . . , nk do
w k =w k −h k(S h[∇f M(w k)−∇f M({tilde over (w)})]+∇f({tilde over (w)}))
- Let {ik}t=1 n
The aggregation can further include a per-coordinate scaling of aggregated updates (e.g., A(wk−{tilde over (w)})). In this manner, the global model can be an additive aggregation determined as follows:
∇F k({tilde over (w)} s)−∇{circumflex over (f)}({tilde over (w)} s)
can be added to the local objective to force all nodes to initially move in the same direction.
G k =∇f i(w)−∇f i({tilde over (w)})+∇f({tilde over (w)})
wherein i is sampled uniformly at random from Pk. If there is only one remote computing device then:
If there are more than one remote computing device, the values of Gk are biased estimates of the gradient of f(w). In this manner,
can be achieved for some choice of αk. This can be motivated by a desire of stochastic first-order methods to perform a gradient step on expectation. In this manner, we have:
By setting αk to nk/n, we get:
parameters: h = stepsize, data partition {Pk}k=1 K, | |
diagonal matrices A, Sk ∈ | |
for s = 0, 1, 2, . . . do | |
|
|
for k = 1 to K do in parallel over nodes k | |
Initialize: wk = {tilde over (w)} and hk = h/nk | |
Let {it}t=1 n |
|
for t = 1, . . . , nk do | |
wk = wk − hk (Sk [∇fi |
|
end for | |
end for | |
|
|
end for | |
A regularizer term is defined as:
The uj can correspond to a curvature in dimension i. For instance, uj=β can be very small when each ith data example is on the current remote computing device. In this manner, wj can be determined as desired by the remote computing device. In such embodiments, when nk j=0, the regularization is infinitely strong, and the coefficients of features for which no data is present may not be changed.
Claims (20)
Priority Applications (7)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/045,707 US10402469B2 (en) | 2015-10-16 | 2016-02-17 | Systems and methods of distributed optimization |
CN201680060524.7A CN108351881A (en) | 2015-10-16 | 2016-10-14 | The system and method for optimally in distributed mode |
PCT/US2016/056954 WO2017066509A1 (en) | 2015-10-16 | 2016-10-14 | Systems and methods of distributed optimization |
EP16856233.8A EP3362918A4 (en) | 2015-10-16 | 2016-10-14 | Systems and methods of distributed optimization |
US16/558,945 US11023561B2 (en) | 2015-10-16 | 2019-09-03 | Systems and methods of distributed optimization |
US17/004,324 US11120102B2 (en) | 2015-10-16 | 2020-08-27 | Systems and methods of distributed optimization |
US17/406,377 US20210382962A1 (en) | 2015-10-16 | 2021-08-19 | Systems and Methods of Distributed Optimization |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562242771P | 2015-10-16 | 2015-10-16 | |
US15/045,707 US10402469B2 (en) | 2015-10-16 | 2016-02-17 | Systems and methods of distributed optimization |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/558,945 Continuation US11023561B2 (en) | 2015-10-16 | 2019-09-03 | Systems and methods of distributed optimization |
Publications (2)
Publication Number | Publication Date |
---|---|
US20170109322A1 US20170109322A1 (en) | 2017-04-20 |
US10402469B2 true US10402469B2 (en) | 2019-09-03 |
Family
ID=58517950
Family Applications (4)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/045,707 Active 2037-07-02 US10402469B2 (en) | 2015-10-16 | 2016-02-17 | Systems and methods of distributed optimization |
US16/558,945 Active US11023561B2 (en) | 2015-10-16 | 2019-09-03 | Systems and methods of distributed optimization |
US17/004,324 Active US11120102B2 (en) | 2015-10-16 | 2020-08-27 | Systems and methods of distributed optimization |
US17/406,377 Pending US20210382962A1 (en) | 2015-10-16 | 2021-08-19 | Systems and Methods of Distributed Optimization |
Family Applications After (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/558,945 Active US11023561B2 (en) | 2015-10-16 | 2019-09-03 | Systems and methods of distributed optimization |
US17/004,324 Active US11120102B2 (en) | 2015-10-16 | 2020-08-27 | Systems and methods of distributed optimization |
US17/406,377 Pending US20210382962A1 (en) | 2015-10-16 | 2021-08-19 | Systems and Methods of Distributed Optimization |
Country Status (4)
Country | Link |
---|---|
US (4) | US10402469B2 (en) |
EP (1) | EP3362918A4 (en) |
CN (1) | CN108351881A (en) |
WO (1) | WO2017066509A1 (en) |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180032869A1 (en) * | 2016-07-29 | 2018-02-01 | Fujitsu Limited | Machine learning method, non-transitory computer-readable storage medium, and information processing apparatus |
US20200099720A1 (en) * | 2018-09-26 | 2020-03-26 | Bank Of America Corporation | Security tool |
US20210065038A1 (en) * | 2019-08-26 | 2021-03-04 | Visa International Service Association | Method, System, and Computer Program Product for Maintaining Model State |
US20210073677A1 (en) * | 2019-09-06 | 2021-03-11 | Oracle International Corporation | Privacy preserving collaborative learning with domain adaptation |
US20220182802A1 (en) * | 2020-12-03 | 2022-06-09 | Qualcomm Incorporated | Wireless signaling in federated learning for machine learning components |
US11487698B2 (en) * | 2017-06-01 | 2022-11-01 | Electronics And Telecommunications Research Institute | Parameter server and method for sharing distributed deep learning parameter using the same |
US11494671B2 (en) | 2019-07-14 | 2022-11-08 | Olivia Karen Grabmaier | Precision hygiene using reinforcement learning |
US11580380B2 (en) * | 2016-08-19 | 2023-02-14 | Movidius Limited | Systems and methods for distributed training of deep learning models |
Families Citing this family (39)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10460255B2 (en) * | 2016-07-29 | 2019-10-29 | Splunk Inc. | Machine learning in edge analytics |
US11087236B2 (en) | 2016-07-29 | 2021-08-10 | Splunk Inc. | Transmitting machine learning models to edge devices for edge analytics |
US10536351B2 (en) | 2016-07-29 | 2020-01-14 | Splunk Inc. | Analytics for edge devices |
WO2018031958A1 (en) * | 2016-08-11 | 2018-02-15 | Twitter, Inc. | Aggregate features for machine learning |
US10769549B2 (en) * | 2016-11-21 | 2020-09-08 | Google Llc | Management and evaluation of machine-learned models based on locally logged data |
US11062215B2 (en) | 2017-03-17 | 2021-07-13 | Microsoft Technology Licensing, Llc | Using different data sources for a predictive model |
US10963813B2 (en) * | 2017-04-28 | 2021-03-30 | Cisco Technology, Inc. | Data sovereignty compliant machine learning |
JP6816824B2 (en) * | 2017-06-06 | 2021-01-20 | 日本電気株式会社 | Distributed systems, data management devices, data management methods, and programs |
US11108575B2 (en) * | 2017-07-26 | 2021-08-31 | Amazon Technologies, Inc. | Training models for IOT devices |
US10980085B2 (en) | 2017-07-26 | 2021-04-13 | Amazon Technologies, Inc. | Split predictions for IoT devices |
US11902396B2 (en) | 2017-07-26 | 2024-02-13 | Amazon Technologies, Inc. | Model tiering for IoT device clusters |
JP6936474B2 (en) * | 2017-07-28 | 2021-09-15 | プラスマン合同会社 | Information processing equipment, systems and information processing methods |
CN109388661B (en) * | 2017-08-02 | 2020-04-21 | 创新先进技术有限公司 | Model training method and device based on shared data |
US11216745B2 (en) | 2017-11-07 | 2022-01-04 | Google Llc | Incognito mode for personalized machine-learned models |
US11488054B2 (en) * | 2017-12-06 | 2022-11-01 | Google Llc | Systems and methods for distributed on-device learning with data-correlated availability |
EP3503117A1 (en) * | 2017-12-20 | 2019-06-26 | Nokia Technologies Oy | Updating learned models |
JP7058440B2 (en) * | 2018-01-10 | 2022-04-22 | 国立研究開発法人情報通信研究機構 | Learning system and learning method |
US10878482B2 (en) | 2018-01-19 | 2020-12-29 | Hypernet Labs, Inc. | Decentralized recommendations using distributed average consensus |
US11244243B2 (en) * | 2018-01-19 | 2022-02-08 | Hypernet Labs, Inc. | Coordinated learning using distributed average consensus |
WO2019232471A1 (en) | 2018-06-01 | 2019-12-05 | Nami Ml Inc. | Machine learning at edge devices based on distributed feedback |
US10762616B2 (en) * | 2018-07-09 | 2020-09-01 | Hitachi, Ltd. | Method and system of analytics system balancing lead time and accuracy of edge analytics modules |
US11657322B2 (en) * | 2018-08-30 | 2023-05-23 | Nec Corporation | Method and system for scalable multi-task learning with convex clustering |
FR3094109A1 (en) | 2019-03-21 | 2020-09-25 | Roofstreet | Process and system for processing digital data from connected equipment while ensuring data security and protection of privacy |
US11138003B2 (en) * | 2019-04-02 | 2021-10-05 | Taplytics Inc. | Methods and systems for automatic determination of a device-specific configuration for a software application operating on a user device |
US11531912B2 (en) * | 2019-04-12 | 2022-12-20 | Samsung Electronics Co., Ltd. | Electronic apparatus and server for refining artificial intelligence model, and method of refining artificial intelligence model |
KR20200120469A (en) * | 2019-04-12 | 2020-10-21 | 삼성전자주식회사 | Electronic device, Server and operating method for updating a artificial intelligence model |
CN114391159A (en) * | 2019-05-02 | 2022-04-22 | 斯波莱史莱特控股有限责任公司 | Digital anthropology and anthropology system |
EP3742669B1 (en) * | 2019-05-20 | 2023-11-08 | Nokia Technologies Oy | Machine learning in radio access networks |
WO2021090323A1 (en) * | 2019-11-05 | 2021-05-14 | Technion Research & Development Foundation Limited | Gap-aware mitigation of gradient staleness |
CN111076161B (en) * | 2019-12-06 | 2022-04-22 | 华北电力科学研究院有限责任公司 | Method and device for determining drum water level in subcritical boiler of coal-fired unit |
US11551083B2 (en) | 2019-12-17 | 2023-01-10 | Soundhound, Inc. | Neural network training from private data |
CN111401913A (en) * | 2020-03-19 | 2020-07-10 | 支付宝(杭州)信息技术有限公司 | Model learning method, device and system for private data protection |
US11741340B2 (en) * | 2020-03-23 | 2023-08-29 | D5Ai Llc | Data-dependent node-to-node knowledge sharing by regularization in deep learning |
WO2021198742A1 (en) * | 2020-04-03 | 2021-10-07 | Telefonaktiebolaget Lm Ericsson (Publ) | Method for efficient distributed machine learning hyperparameter search |
CN111753827B (en) * | 2020-05-15 | 2024-02-13 | 中国科学院信息工程研究所 | Scene text recognition method and system based on semantic enhancement encoder and decoder framework |
US20210365841A1 (en) * | 2020-05-22 | 2021-11-25 | Kiarash SHALOUDEGI | Methods and apparatuses for federated learning |
CN111709533B (en) * | 2020-08-19 | 2021-03-30 | 腾讯科技（深圳）有限公司 | Distributed training method and device of machine learning model and computer equipment |
CN113627519B (en) * | 2021-08-07 | 2022-09-09 | 中国人民解放军国防科技大学 | Distributed random gradient descent method with compression and delay compensation |
CN114118437B (en) * | 2021-09-30 | 2023-04-18 | 电子科技大学 | Model updating synchronization method for distributed machine learning in micro cloud |
Citations (29)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6687653B1 (en) | 2002-08-13 | 2004-02-03 | Xerox Corporation | Systems and methods for distributed algorithm for optimization-based diagnosis |
US6708163B1 (en) | 1999-02-24 | 2004-03-16 | Hillol Kargupta | Collective data mining from distributed, vertically partitioned feature space |
US6879944B1 (en) | 2000-03-07 | 2005-04-12 | Microsoft Corporation | Variational relevance vector machine |
US20050138571A1 (en) | 2003-12-18 | 2005-06-23 | Keskar Dhananjay V. | Dynamic detection of device characteristics |
US7069256B1 (en) | 2002-05-23 | 2006-06-27 | Oracle International Corporation | Neural network module for data mining |
US20060224579A1 (en) | 2005-03-31 | 2006-10-05 | Microsoft Corporation | Data mining techniques for improving search engine relevance |
US20080209031A1 (en) | 2007-02-22 | 2008-08-28 | Inventec Corporation | Method of collecting and managing computer device information |
US7664249B2 (en) | 2004-06-30 | 2010-02-16 | Microsoft Corporation | Methods and interfaces for probing and understanding behaviors of alerting and filtering systems based on models and simulation from logs |
US20100132044A1 (en) | 2008-11-25 | 2010-05-27 | International Business Machines Corporation | Computer Method and Apparatus Providing Brokered Privacy of User Data During Searches |
US20110085546A1 (en) | 2008-05-30 | 2011-04-14 | Telecom Italia S.P.A. | Method and devices for multicast distribution optimization |
US8018874B1 (en) | 2009-05-06 | 2011-09-13 | Hrl Laboratories, Llc | Network optimization system implementing distributed particle swarm optimization |
US20120016816A1 (en) | 2010-07-15 | 2012-01-19 | Hitachi, Ltd. | Distributed computing system for parallel machine learning |
US8239396B2 (en) | 2009-03-20 | 2012-08-07 | Oracle International Corporation | View mechanism for data security, privacy and utilization |
US20120226639A1 (en) | 2011-03-01 | 2012-09-06 | International Business Machines Corporation | Systems and Methods for Processing Machine Learning Algorithms in a MapReduce Environment |
US8321412B2 (en) | 2009-07-21 | 2012-11-27 | National Taiwan University | Digital data processing method for personalized information retrieval and computer readable storage medium and information retrieval system thereof |
US20120310870A1 (en) | 2011-05-31 | 2012-12-06 | Oracle International Corporation | Application configuration generation |
US8429103B1 (en) | 2012-06-22 | 2013-04-23 | Google Inc. | Native machine learning service for user adaptation on a mobile platform |
US20140214735A1 (en) | 2013-01-28 | 2014-07-31 | Pagebites, Inc. | Method for an optimizing predictive model using gradient descent and conjugate residuals |
US8954357B2 (en) | 2011-05-12 | 2015-02-10 | Xerox Corporation | Multi-task machine learning using features bagging and local relatedness in the instance space |
US20150186798A1 (en) | 2013-12-31 | 2015-07-02 | Cisco Technology, Inc. | Learning data processor for distributing learning machines across large-scale network infrastructures |
US20150195144A1 (en) | 2014-01-06 | 2015-07-09 | Cisco Technology, Inc. | Distributed and learning machine-based approach to gathering localized network dynamics |
US20150193695A1 (en) | 2014-01-06 | 2015-07-09 | Cisco Technology, Inc. | Distributed model training |
US20150242760A1 (en) * | 2014-02-21 | 2015-08-27 | Microsoft Corporation | Personalized Machine Learning System |
US20150324690A1 (en) | 2014-05-08 | 2015-11-12 | Microsoft Corporation | Deep Learning Training System |
US9190055B1 (en) | 2013-03-14 | 2015-11-17 | Amazon Technologies, Inc. | Named entity recognition with personalized models |
US9275398B1 (en) | 2012-12-10 | 2016-03-01 | A9.Com, Inc. | Obtaining metrics for client-side display of content |
US9336483B1 (en) | 2015-04-03 | 2016-05-10 | Pearson Education, Inc. | Dynamically updated neural network structures for content distribution networks |
US9390370B2 (en) | 2012-08-28 | 2016-07-12 | International Business Machines Corporation | Training deep neural network acoustic models using distributed hessian-free optimization |
US9424836B2 (en) | 2012-11-05 | 2016-08-23 | Nuance Communications, Inc. | Privacy-sensitive speech model creation via aggregation of multiple user models |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9703610B2 (en) * | 2011-05-16 | 2017-07-11 | Oracle International Corporation | Extensible centralized dynamic resource distribution in a clustered data grid |
US9633315B2 (en) * | 2012-04-27 | 2017-04-25 | Excalibur Ip, Llc | Method and system for distributed machine learning |
CN103327094A (en) * | 2013-06-19 | 2013-09-25 | 成都市欧冠信息技术有限责任公司 | Data distributed type memory method and data distributed type memory system |
US20150170053A1 (en) * | 2013-12-13 | 2015-06-18 | Microsoft Corporation | Personalized machine learning models |
-
2016
- 2016-02-17 US US15/045,707 patent/US10402469B2/en active Active
- 2016-10-14 EP EP16856233.8A patent/EP3362918A4/en active Pending
- 2016-10-14 WO PCT/US2016/056954 patent/WO2017066509A1/en unknown
- 2016-10-14 CN CN201680060524.7A patent/CN108351881A/en active Pending
-
2019
- 2019-09-03 US US16/558,945 patent/US11023561B2/en active Active
-
2020
- 2020-08-27 US US17/004,324 patent/US11120102B2/en active Active
-
2021
- 2021-08-19 US US17/406,377 patent/US20210382962A1/en active Pending
Patent Citations (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6708163B1 (en) | 1999-02-24 | 2004-03-16 | Hillol Kargupta | Collective data mining from distributed, vertically partitioned feature space |
US6879944B1 (en) | 2000-03-07 | 2005-04-12 | Microsoft Corporation | Variational relevance vector machine |
US7069256B1 (en) | 2002-05-23 | 2006-06-27 | Oracle International Corporation | Neural network module for data mining |
US6687653B1 (en) | 2002-08-13 | 2004-02-03 | Xerox Corporation | Systems and methods for distributed algorithm for optimization-based diagnosis |
US20050138571A1 (en) | 2003-12-18 | 2005-06-23 | Keskar Dhananjay V. | Dynamic detection of device characteristics |
US7664249B2 (en) | 2004-06-30 | 2010-02-16 | Microsoft Corporation | Methods and interfaces for probing and understanding behaviors of alerting and filtering systems based on models and simulation from logs |
US20060224579A1 (en) | 2005-03-31 | 2006-10-05 | Microsoft Corporation | Data mining techniques for improving search engine relevance |
US20080209031A1 (en) | 2007-02-22 | 2008-08-28 | Inventec Corporation | Method of collecting and managing computer device information |
US20110085546A1 (en) | 2008-05-30 | 2011-04-14 | Telecom Italia S.P.A. | Method and devices for multicast distribution optimization |
US20100132044A1 (en) | 2008-11-25 | 2010-05-27 | International Business Machines Corporation | Computer Method and Apparatus Providing Brokered Privacy of User Data During Searches |
US8239396B2 (en) | 2009-03-20 | 2012-08-07 | Oracle International Corporation | View mechanism for data security, privacy and utilization |
US8018874B1 (en) | 2009-05-06 | 2011-09-13 | Hrl Laboratories, Llc | Network optimization system implementing distributed particle swarm optimization |
US8321412B2 (en) | 2009-07-21 | 2012-11-27 | National Taiwan University | Digital data processing method for personalized information retrieval and computer readable storage medium and information retrieval system thereof |
US20120016816A1 (en) | 2010-07-15 | 2012-01-19 | Hitachi, Ltd. | Distributed computing system for parallel machine learning |
US20120226639A1 (en) | 2011-03-01 | 2012-09-06 | International Business Machines Corporation | Systems and Methods for Processing Machine Learning Algorithms in a MapReduce Environment |
US8954357B2 (en) | 2011-05-12 | 2015-02-10 | Xerox Corporation | Multi-task machine learning using features bagging and local relatedness in the instance space |
US20120310870A1 (en) | 2011-05-31 | 2012-12-06 | Oracle International Corporation | Application configuration generation |
US8429103B1 (en) | 2012-06-22 | 2013-04-23 | Google Inc. | Native machine learning service for user adaptation on a mobile platform |
US9390370B2 (en) | 2012-08-28 | 2016-07-12 | International Business Machines Corporation | Training deep neural network acoustic models using distributed hessian-free optimization |
US9424836B2 (en) | 2012-11-05 | 2016-08-23 | Nuance Communications, Inc. | Privacy-sensitive speech model creation via aggregation of multiple user models |
US9275398B1 (en) | 2012-12-10 | 2016-03-01 | A9.Com, Inc. | Obtaining metrics for client-side display of content |
US20140214735A1 (en) | 2013-01-28 | 2014-07-31 | Pagebites, Inc. | Method for an optimizing predictive model using gradient descent and conjugate residuals |
US9190055B1 (en) | 2013-03-14 | 2015-11-17 | Amazon Technologies, Inc. | Named entity recognition with personalized models |
US20150186798A1 (en) | 2013-12-31 | 2015-07-02 | Cisco Technology, Inc. | Learning data processor for distributing learning machines across large-scale network infrastructures |
US20150193695A1 (en) | 2014-01-06 | 2015-07-09 | Cisco Technology, Inc. | Distributed model training |
US20150195144A1 (en) | 2014-01-06 | 2015-07-09 | Cisco Technology, Inc. | Distributed and learning machine-based approach to gathering localized network dynamics |
WO2015126858A1 (en) | 2014-02-21 | 2015-08-27 | Microsoft Technology Licensing, Llc | Personalized machine learning system |
US20150242760A1 (en) * | 2014-02-21 | 2015-08-27 | Microsoft Corporation | Personalized Machine Learning System |
US20150324690A1 (en) | 2014-05-08 | 2015-11-12 | Microsoft Corporation | Deep Learning Training System |
US9336483B1 (en) | 2015-04-03 | 2016-05-10 | Pearson Education, Inc. | Dynamically updated neural network structures for content distribution networks |
Non-Patent Citations (78)
Title |
---|
"Distributed Subgradient Methods for Multi-Agent Optimization" Nedic et al. IEEE Transactions on Automatic Control, vol. 54, No. 1, Jan. 2009. * |
Ailon et al., "Approximate Nearest Neighbors and the Fast Johnson-Lindenstrauss Transform", 38th Annual ACM Symposium on Theory of Computing, Seattle, Washington, May 21-23, 2006, 7 pages. |
Alistarh et al., "QSGB: Randomized Quantization for Communication-Optimal Stochastic Gradient Descent", arXiv:610.02132v1,, Oct. 7, 2016, 22 pages. |
Al-Rfou et al., "Conversational Contextual Cues: The Case of Personalization and History for Response Ranking", arXiv:1606.00372v1, Jun. 1, 2016, 10 pages. |
Arjevani et al., "Communication Complexity of Distributed Convex Learning and Optimization", Neural Information Processing Systems, Montreal, Canada, Dec. 7-12, 2015, 9 pages. |
Balcan et al., "Distributed Learning, Communication Complexity and Privacy", Conference on Learning Theory, Edinburgh, Scotland, Jun. 25-27, 2012. |
Bonawitz et al., "Practical Secure Aggregation for Federated Learning on User-Held Data", arXiv1611.04482v1, Nov. 14, 2016, 5 pages. |
Braverman et al., "Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality", 48th Annual ACM Symposium on Theory of Computing, Cambridge, Massachusetts, Jun. 19-21, 2016, pp. 1011-1020, 11 pages. |
Chaudhuri et al., "Differentially Private Empirical Risk Minimization", Journal of Machine Learning Research, vol. 12, Jul. 12, 2011, pp. 1069-1109. |
Chen et al., "Communication-Optimal Distributed Clustering", Neural Information Processing Systems, Barcelona, Spain, Dec. 5-10, 2016, 9 pages. |
Chen et al., "Revisiting Distributed Synchronous SGD", arXiv:1604.00981v3, Mar. 21, 2017, 10 pages. |
Chilimbi et al., "Project Adam: Building an Efficient and Scalable Deep Learning Training System", 11th USENIX Symposium on Operating Systems Design and Implementation, Broomfield, Colorado, Oct. 6-8, 2014, pp. 571-582. |
Dasgupta et al., "An Elementary Proof of a Theorem of Johnson and Lindenstrauss", Random Structures & Algorithms, vol. 22, Issue 1, 2003, pp. 60-65. |
Dean et al., "Large Scale Distributed Deep Networks", Neural Information Processing Systems, Dec. 3-8, 2012, Lake Tahoe, 9 pages. |
Denil et al., "Predicting Parameters in Deep Learning",26th International Conference on Neural Information Processing Systems, Lake Tahoe, Nevada, Dec. 5-10, 2013, pp. 2148-2156. |
Duchi et al., "Privacy Aware Learning", arXiv:1210.2085v2, Oct. 10, 2013, 60 pages. |
Dwork et al., "The Algorithmic Foundations of Differential Privacy", Foundations and Trends in Theoretical Computer Science, vol. 9, Nos. 3-4, 2014, pp. 211-407. |
Efron et al., "The Jackknife Estimate of Variance", The Annals of Statistics, vol. 9, Issue 3, May 1981, pp. 586-596. |
Elias, "Universal Codeword Sets and Representations of the Integers", IEEE Transactions on Information Theory, vol. 21, Issue 2, Mar. 1975, pp. 194-203. |
European Search Report for Application No. 16856233.8, dated Nov. 20, 2018, 9 pages. |
Falahatgar et al., "Universal Compression of Power-Law Distributions", arXiv:1504.08070v2, May 1, 2015, 20 pages. |
Fercoq et al., "Fast Distributed Coordinate Descent for Non-Strongly Convex Losses", arXiv:1405.5300v1, May 21, 2014, 6 pages. |
Gamal et al., "On Randomized Distributed Coordinate Descent with Quantized Updates", arXiv:1609.05539v1, Sep. 18, 2016, 5 pages. |
Garg et al., "On Communication Cost of Distributed Statistical Estimation and Dimensionality", Neural Information Processing Systems, Montreal, Canada, Dec. 8-13, 2014, 9 pages. |
Golovin et al., "Large-Scale Learning with Less Ram via Randomization", arXiv:1303.4664v1, Mar. 19, 2013, 10 pages. |
Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding", arXiv:1510.00149v5, Nov. 20, 2015, 13 pages. |
Ho et al., "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server", Neural Information Processing Systems, Dec. 5-10, 2013, Lake Tahoe, 9 pages. |
Horadam, "Hadamard Matrices and Their Applications", Princeton University Press, 2007. |
International Search Report from PCT/US2016/056954 dated Jan. 25, 2017, 15 pages. |
Jaggi et al., "Communication-Efficient Distributed Dual Coordinate Ascent", arXiv:1409.1458v2, Sep. 29, 2014, 15 pages. |
Johnson et al., "Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction" Advances in Neural Information Processing Systems, Lake Tahoe, Nevada, Dec. 5-10, 2013, pp. 315-323. |
Konecny et al., "Federated Learning: Strategies for Improving Communication Efficiency", arXiv:610.05492v1, Oct. 18, 2016, 5 pages. |
Konecny et al., "Federated Optimization: Distributed Machine Learning for On-Device Intelligence", arXiv:1610.02527v1, Oct. 8, 2016, 38 pages. |
Konecny et al., "Federated Optimization: Distributed Optimization Beyond the Datacenter", arXiv:1511.03575v1, Nov. 11, 2015, 5 pages. |
Konecny et al., "Randomized Distributed Mean Estimation: Accuracy vs. Communication", arXiv:1611.07555v1, Nov. 22, 2016, 19 pages. |
Konecny et al., "Semi-Stochastic Gradient Descent Methods", arXiv:1312.1666v1, Dec. 5, 2013, 19 pages. |
Krichevsky et al., "The Performance of Universal Encoding", IEEE Transactions on Information Theory, vol. 27, Issue 2, Mar. 1981, pp. 199-207. |
Krizhevsky, "Learning Multiple Layers of Features from Tiny Images", Techncal Report, Apr. 8, 2009, 60 pages. |
Krizhevsky, "One Weird Trick for Parallelizing Convolutional Neural Networks", arXiv:1404.59997v2, Apr. 26, 2014, 7 pages. |
Kumar et al., "Fugue: Slow-Worker-Agnostic Distributed Learning for Big Models on Big Data", Journal of Machine Learning Research: Workshop and Conference Proceedings, Apr. 2014, 9 pages. |
Livni et al., "An Algorithm for Training Polynomial Networks", arXiv:1304.7045v1, Apr. 26, 2013, 22 pages. |
Livni et al., "On the Computational Efficiency of Training Neural Networks" arXiv:1410.1141v2, Oct. 28, 2014, 15 pages. |
Lloyd, "Least Squares Quantization in PCM", IEEE Transactions on Information Theory, vol. 28, Issue 2, Mar. 1982, pp. 129-137. |
Ma et al., "Adding vs. Averaging in Distributed Primal-Dual Optimization", arXiv:1502.03508v2, Jul. 3, 2015, 19 pages. |
Ma et al., "Distributed Optimization with Arbitrary Local Solvers", arXiv:1512.04039v2, Aug. 3, 2016, 38 pages. |
MacKay, "Information Theory, Inference and Learning Algorithms", Cambridge University Press, 2003. |
Mahajan, et al., "A Functional Approximation Based Distributed Learning Algorithm", Oct. 31, 2013, https://arXiv.org/pdf/1310.8418v1, retrieved on Nov. 15, 2018. |
Mahajan, et al., "An Efficient Distributed Learning Algorithm Based on Approximations", Journal of Machine Learning Research, Mar. 16, 2015, pp. 1-32, https://arXiv.org/pdf/1310.8418.pdf, retrieved on Nov. 15, 2018. |
McDonald et al., "Distributed Training Strategies for the Structures Perceptron", Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, California, Jun. 2-4, 2010, 9 pages. |
McMahan et al., "Communication-Efficient Learning of Deep Networks from Decentralized Data", arXiv:1602.05629v3, Feb. 28, 2017, 11 pages. |
McMahan et al., "Federated Learning of Deep Networks using Model Averaging", arXiv:1602.05629, Feb. 17, 2016, 11 pages. |
McMahan et al., "Federated Learning: Collaborative Machine Learning without Centralized Training Data", Apr. 6, 2017, https://research.googleblog.com/2017/04/federated-learning-collaborative.html, retrieved on Oct. 3, 2018, 5 pages. |
Povey et al., "Parallel Training of Deep Neural Networks with Natural Gradient and Parameter Averaging", arVix:1410.7455v1, Oct. 27, 2014, 21 pages. |
Qu et al., "Coordinate Descent with Arbitrary Sampling I: Algorithms and Complexity", arXiv:1412.8060v2, Jun. 15, 2015, 32 pages. |
Qu et al., Quartz: Randomized Dual Coordinate Ascent with Arbitrary Sampling, arXiv:1411.5873v1, Nov. 21, 2014, 34 pages. |
Rabbat et al., "Quantized Incremental Algorithms for Distributed Optimization", Journal on Selected Areas in Communications, vol. 23, No. 4, 2005, pp. 798-808. |
Reddi et al., "AIDE: Fast and Communication Efficient Distributed Optimization", arXiv:1608.06879v1, Aug. 24, 2016, 23 pages. |
Richtarik et al., "Distributed Coordinate Descent Method for Learning with Big Data", arXiv:1310.2059v1, Oct. 8, 2013, 11 pages. |
Seide et al., "1-Bit Stochastic Gradient Descent and Application to Data-Parallel Distributed Training of Speech DNNs", 15th Annual Conference of the International Speech Communication Association, Singapore, Sep. 14-18, 2014, pp. 1058-1062. |
Shamir et al., "Communication-Efficient Distributed Optimization Using an Approximate Newton-Type Method", arXiv1312.7853v4, May 13, 2013, 22 pages. |
Shamir et al., "Distributed Stochastic Optimization and Learning", 52nd Annual Allerton Conference on Communication, Control, and Computing, Monticello, Illinois, Oct. 1-3, 2014, pp. 850-857. |
Shokri et al., "Privacy-Preserving Deep Learning" Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, Denver, Colorado, Oct. 12-16, 2015, 12 pages. |
Springenberg et al., "Striving for Simplicity: The All Convolutional Net", arXiv:1412.6806v3, Apr. 13, 2015, 14 pages. |
Suresh et al., "Distributed Mean Estimation with Limited Communication", arXiv.1611.00429v3, Sep. 25, 2017, 17 pages. |
Tsitsiklis et al., "Communication Complexity of Convex Optimization", Journal of Complexity, vol. 3, Issue 3, Sep. 1, 1987, pp. 231-243. |
Wikipedia, "Rounding", https://en.wikipedia.org/wiki/Rounding, retrieved on Aug. 14, 2017, 13 pages. |
Woodruff, "Sketching as a Tool for Numerical Linear Algebra", arXiv:1411.4357v3, Feb. 10, 2015, 139 pages. |
Xie et al., "Distributed Machine Learning via Sufficient Factor Broadcasting", arXiv:1409.5705v2, Sep. 7, 2015, 15 pages. |
Xing et al., "Petuum: A New Platform for Distributed Machine Learning on Big Data", Conference on Knowledge Discovery and Data Mining, Aug. 10-13, 2015, Hilton, Sydney, 10 pages. |
Yadan et al., "Multi-GPU Training of ConvNets", International Conference on Learning Representations, Apr. 14-16, 2014, Banff, Canada, 4 pages. |
Yang, "Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent", Advances in Neural Information Processing Systems, Lake Tahoe, Nevada, Dec. 5-10, 2013, pp. 629-637. |
Yu et al., "Circulant Binary Embedding", arXiv:1405.3162v1, May 13, 2014, 9 pages. |
Yu et al., "Orthogonal Random Features", Neural Information Processing Systems, Barcelona, Spain, Dec. 5-10, 2016, 9 pages. |
Zhang et al., "Communication-Efficient Algorithms for Statistical Optimization", arXiv.1209.4129v3, Oct. 11, 2013, 44 pages. |
Zhang et al., "Communication-Efficient Distributed Optimization of Self Concordant Empirical Loss", arXiv:1501.00263v1, Jan. 1, 2015, 46 pages. |
Zhang et al., "DISCO: Distributed Optimization for Self-Concordant Empirical Loss", 32nd International Conference on Machine Learning, vol. 37, 2015, pp. 362-370. |
Zhang et al., "Information-Theoretic Lower Bounds for Distributed Statistical Estimation with Communication Constraints", Neural Information Processing Systems, Lake Tahoe, Nevada, Dec. 5-10, 2013, 9 pages. |
Zhang et al., "Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple Machines", arXiv:1512.06216v1, Dec. 19, 2015, 14 pages. |
Cited By (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180032869A1 (en) * | 2016-07-29 | 2018-02-01 | Fujitsu Limited | Machine learning method, non-transitory computer-readable storage medium, and information processing apparatus |
US11580380B2 (en) * | 2016-08-19 | 2023-02-14 | Movidius Limited | Systems and methods for distributed training of deep learning models |
US11769059B2 (en) | 2016-08-19 | 2023-09-26 | Movidius Limited | Systems and methods for distributed training of deep learning models |
US11487698B2 (en) * | 2017-06-01 | 2022-11-01 | Electronics And Telecommunications Research Institute | Parameter server and method for sharing distributed deep learning parameter using the same |
US20200099720A1 (en) * | 2018-09-26 | 2020-03-26 | Bank Of America Corporation | Security tool |
US10805353B2 (en) * | 2018-09-26 | 2020-10-13 | Bank Of America Corporation | Security tool |
US11494671B2 (en) | 2019-07-14 | 2022-11-08 | Olivia Karen Grabmaier | Precision hygiene using reinforcement learning |
US20210065038A1 (en) * | 2019-08-26 | 2021-03-04 | Visa International Service Association | Method, System, and Computer Program Product for Maintaining Model State |
US20210073677A1 (en) * | 2019-09-06 | 2021-03-11 | Oracle International Corporation | Privacy preserving collaborative learning with domain adaptation |
US11443240B2 (en) * | 2019-09-06 | 2022-09-13 | Oracle International Corporation | Privacy preserving collaborative learning with domain adaptation |
US20220182802A1 (en) * | 2020-12-03 | 2022-06-09 | Qualcomm Incorporated | Wireless signaling in federated learning for machine learning components |
Also Published As
Publication number | Publication date |
---|---|
US20170109322A1 (en) | 2017-04-20 |
US20200394253A1 (en) | 2020-12-17 |
US20210382962A1 (en) | 2021-12-09 |
CN108351881A (en) | 2018-07-31 |
EP3362918A1 (en) | 2018-08-22 |
EP3362918A4 (en) | 2018-12-26 |
WO2017066509A1 (en) | 2017-04-20 |
US20200004801A1 (en) | 2020-01-02 |
US11120102B2 (en) | 2021-09-14 |
US11023561B2 (en) | 2021-06-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11023561B2 (en) | Systems and methods of distributed optimization | |
US20230376856A1 (en) | Communication Efficient Federated Learning | |
US11475350B2 (en) | Training user-level differentially private machine-learned models | |
Che et al. | A collaborative neurodynamic approach to global and combinatorial optimization | |
US11836615B2 (en) | Bayesian nonparametric learning of neural networks | |
US9256838B2 (en) | Scalable online hierarchical meta-learning | |
JP6901633B2 (en) | Capsule neural network | |
EP3629250A1 (en) | Parameter-efficient multi-task and transfer learning | |
US11797864B2 (en) | Systems and methods for conditional generative models | |
US20230385652A1 (en) | System and Method of Federated Learning with Diversified Feedback | |
US20180114145A1 (en) | Structured orthogonal random features for kernel-based machine learning | |
US20210326757A1 (en) | Federated Learning with Only Positive Labels | |
US20200349418A1 (en) | Gated linear networks | |
Li et al. | A unifying framework for typical multitask multiple kernel learning problems | |
Wu et al. | A stochastic maximum-likelihood framework for simplex structured matrix factorization | |
Toda et al. | Online model-selection and learning for nonlinear estimation based on multikernel adaptive filtering | |
Lin et al. | Generalized non-convex non-smooth sparse and low rank minimization using proximal average | |
Li et al. | Sampling attacks on meta reinforcement learning: A minimax formulation and complexity analysis | |
US20240144029A1 (en) | System for secure and efficient federated learning | |
US20240037410A1 (en) | Method for model aggregation in federated learning, server, device, and storage medium | |
Gau et al. | Bayesian approach for mixture models with grouped data | |
US20220329408A1 (en) | Secure gradient descent computation method, secure deep learning method, secure gradient descent computation system, secure deep learning system, secure computation apparatus, and program | |
CN116150774A (en) | Data protection model training and data protection method, device and storage medium | |
CN117829228A (en) | Neural network adjustment method, device, electronic equipment and readable storage medium | |
Arruda et al. | Differential Entropy Estimation via One-Class SVM |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MCMAHAN, HUGH BRENDAN;KONECNY, JAKUB;SIGNING DATES FROM 20151022 TO 20151023;REEL/FRAME:037754/0103 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MOORE, EIDER BRANTLY;RAMAGE, DANIEL;AGUERA-ARCAS, BLAISE H.;SIGNING DATES FROM 20160323 TO 20160325;REEL/FRAME:038101/0586 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044129/0001Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |