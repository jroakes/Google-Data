CN112771554A - Predictive variables in programming - Google Patents
Predictive variables in programming Download PDFInfo
- Publication number
- CN112771554A CN112771554A CN201880098131.4A CN201880098131A CN112771554A CN 112771554 A CN112771554 A CN 112771554A CN 201880098131 A CN201880098131 A CN 201880098131A CN 112771554 A CN112771554 A CN 112771554A
- Authority
- CN
- China
- Prior art keywords
- computer
- machine learning
- learning system
- variables
- implemented method
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/01—Dynamic search techniques; Heuristics; Dynamic trees; Branch-and-bound
Abstract
The present disclosure is directed to a new framework that can combine symbolic programming with machine learning, where programmers retain control over the overall architecture of the functional mapping and the ability to inject domain knowledge, while allowing their programs to evolve by learning from examples. In some instances, the framework provided herein may be referred to as "predictive programming.
Description
Technical Field
The present disclosure relates generally to the intersection of machine learning and computer programming. More particularly, the present disclosure relates to predicting variables defined in a computer program using a machine learning system.
Background
Machine learning has experienced rapid development as a field in the past decade, both in technology and systems, and in the ever-increasing number of applications that rely on them. It involves a large number of domains and some of the most critical systems in each domain. The basic premise of its operation is to learn from the real world paradigm (example) or by making decisions in the real world and viewing the results. While these systems can be made to work well, in addition to building an actual machine learning system, they also require a significant amount of complex work in order for their results to be consumable as part of the larger product/system being built.
In particular, modern machine learning operates in a model in which it learns from paradigms, derives its techniques from nonlinear optimization, and implicitly performs numerical reasoning. As the scope of the field continues to increase, it eventually conflicts with traditional methods of building computer systems based on explicit (e.g., deterministic/predefined) operations/transformations encoded in symbolic logic (e.g., programming languages). This conflict has a fundamental meaning: while machine learning systems can learn very complex functions that map input/output behaviors, there is little progress in understanding what these functions are and how to adjust them to achieve the specific behaviors needed for domain knowledge/environmental constraints.
Symbolic logic, on the other hand, provides full control and understandability, but placing the responsibility of building functions entirely on the programmer results in the system being built on top of the heuristics (hearistics). In particular, with the most advanced practice today, mainframe computers/computing systems are built with symbolic logic, which generally corresponds to a pre-specified set of handwritten instructions that direct the flow of data and control through them. This enables the programmer to precisely control the computing mechanism and its results. These systems are also understandable because the operations are represented by explicit symbols. The full control aspect allows system developers to explicitly express domain constraints and environmental limitations. However, the same control becomes a disadvantage because it is not possible to optimize all cases in a limited set of handwritten instructions.
Thus, the field of machine learning and the field of traditional symbolic logic-based computer programming present two halves of a method, each with its own limitations and advantages. One solution proposed for such dichotomy is to build a machine learning system that generates code that can be edited later by programmers. However, this places too much of a burden on machine learning systems, as they need to learn the basic semantics of programming and code structure before they can even start producing anything useful, and the problem of human readability of machine code is also revealed.
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the description which follows, or may be learned by practice of the embodiments.
One example aspect of the present disclosure is directed to a computer-implemented method. The method includes obtaining, by one or more computing devices, a computer program comprising a set of computer-executable instructions. The computer program defines variables that serve as placeholders for storing data. The method includes providing, by one or more computing devices, observation data to a machine learning system. The method includes receiving, by one or more computing devices, predicted values of variables generated by a machine learning system based at least in part on observed data. The method includes setting, by the one or more computing devices, a variable equal to the predicted value. The method includes, after setting, by the one or more computing devices, the variable equal to the predicted value, executing, by the one or more computing devices, the computer program. Executing the computer program includes implementing at least one instruction of a set of computer-executable instructions that controls operation of one or more computing devices based at least in part on the variable.
As such, various examples described herein apply machine learning techniques to computer-implemented (i.e., implemented on a computer) technical goals. Various examples described herein enable secure deployment in critical applications, which enables deployment with fewer errors (e.g., memory leaks and/or other runtime errors). Other aspects of the disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the relevant principles.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended drawings, in which:
fig. 1 depicts a block diagram of an example computing architecture, according to an example embodiment of the present disclosure.
Fig. 2 depicts graphs of the cost of different example variants of binary search, cumulative regret compared to normal binary search, and initial function usage.
Fig. 3A and 3B depict graphs of the results of selecting the number of hubs (pivot) using example predicted variables (predicted variables) in quick sort (Quicksort).
FIG. 4 depicts a graphical diagram of the fraction (fraction) of the axis selected by the example predictor variables in quick sort after 5000 event periods (epode).
Fig. 5 depicts a block diagram of an architecture of an example neural network for TD3 with a key embedded network.
6A-6D depict graphs of example cache performance for power law access patterns (power law access patterns).
Fig. 7A depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Fig. 7B depicts a block diagram of an example computing device, according to an example embodiment of the disclosure.
Fig. 7C depicts a block diagram of an example computing device, according to an example embodiment of the disclosure.
Reference numerals repeated across multiple figures are intended to identify identical features in various embodiments.
Detailed Description
SUMMARY
In general, the present disclosure is directed to a new framework capable of combining symbolic programming with machine learning, where programmers retain control over the overall architecture of the functional mapping and the ability to inject domain knowledge, while allowing their programs to evolve by learning from examples. In some instances, the framework provided herein may be referred to as "predictive programming.
In particular, the present disclosure provides a framework for blending symbolic and numeric computations. In particular, this can be expressed as the ability to define "predicted" variables in a program. The predicted variables are similar to local variables with some important differences, such as, for example, the values of some of the predicted variables may be determined using machine learning when evaluated (evaluated).
These variables may be bound to specific contexts that are either explicitly provided by the programmer or implicitly determined by the underlying machine learning system/engine. This allows the programmer to still govern the overall flow of the program and maintain control, while wrapping out-sourcing certain aspects of decision making to predictive variables and leveraging the ability to learn from large data sets or real-world user traffic. It also allows the underlying machine learning system that provides these predictive capabilities to be part of the overall system to observe the effect of its decisions, which is the way it will be used in the field, minimizing so-called online-offline bias.
The predictor variables include a new interface to machine learning that is intended to make machine learning as simple as an "if" statement. Predictive variables provide an interface that allows machine learning to be applied in areas where machine learning has not traditionally been used, thereby enabling, for example, machine learning to help improve the performance of "traditional" algorithms that rely on heuristics. The minimum predictor interface may be used to replace and enhance existing heuristics (such as LRU heuristics in cache) in traditional algorithms using predictor variables.
In particular, contrary to previous work applying machine learning to algorithmic problems, predictive variables have the advantage of being usable within existing frameworks and not requiring existing domain knowledge to be replaced. Thus, the developer may use the predictor variables as any other variables, combining the predictor variables with heuristics, domain specific knowledge, problem constraints, etc. in a manner that is completely under the developer's control. This represents the inverse of the control compared to how machine learning systems are typically built: predictive variables allow machine learning to be tightly integrated into an algorithm, whereas traditional machine learning systems are built around models.
In some embodiments, the internal predictor variables may be based on standard reinforcement learning methods. For example, a standardized API call may be used to provide observation data (e.g., a description of the current state) to the machine learning system, which may then implement a current policy to perform an action (e.g., predict a value of a predictor variable) based on the current state. Further, the standardized API calls may be used to provide rewards based on the results of the actions that the machine learning system may use to optimize or otherwise update the current policy. Therefore, a reinforcement learning scheme may be used to optimize the strategy of predicting the predicted variables.
In some embodiments, to learn faster, the predictor variables may use the heuristic functions they are replacing as the initial strategy or initial function. Thus, the predictor variable may replace an existing heuristic, and the existing heuristic may be used as an initial function of the predictor variable. This allows predictive variables to be minimized in critical applications and allows for secure deployment. Indeed, the experimental results further included herein indicate that the predictor variables quickly pick the behavior of the initial strategy and then improve performance without otherwise significantly degrading performance-allowing for safe real-time deployment in critical applications. Thus, using an initial function may help make machine learning more stable and/or robust, and/or may enable the system to provide guarantees that performance is hardly worse than heuristics.
The concepts described herein can be implemented in almost any programming language, and in most cases can be accomplished without any work being done on the language itself (although incorporating the concepts described herein into the language itself can make the learning system more powerful). This may take the form of an additional (add-on) library that provides a prediction layer, for example. To illustrate a specific example, a simplified programming language that looks more like pseudo code is used to focus on the main concept.
While the next few sections describe the predictor variables under the assumption that they are deterministic, the random variables will be covered in subsequent sections.
Thus, the present disclosure provides predictive variables, a solution that makes machine learning the first citizen in programming languages, making machine learning as simple as "if" statements in programming. In the following section, the feasibility of this approach is demonstrated on three algorithmic problems by enriching and replacing the commonly used heuristics with predictive variables: binary search, quick sort, and cache. Experimental results show that the predicted variables can be improved over the commonly used heuristics and result in better performance than the traditional algorithms.
Introduction to software development Using predictive variables
Predictive variables are intended to facilitate the use of machine learning in software development by avoiding the overhead of going through a complete machine learning development workflow that includes (1) collecting and preparing training data, (2) defining training losses, (3) training initial models, (4) tuning and optimizing models, (5) integrating models into their products, and (6) continually updating and improving models to accommodate drift in the distribution of processed data. The predictor variables may provide a simple API that allows developers to read from, provide them with sufficient information about their context, and provide feedback about their decision results.
In some embodiments, to create a predictor variable, a developer may select its output data type (e.g., floating point (float), integer (int), class (category)), shape, and desired output scope, define which observation the predictor variable can observe, and optionally pass an initial policy. In the following example, the floating point predictor variable is instantiated to take a value on a scalar value between 0 and 1, which can observe three scalar floating points and use a simple (constant) initial strategy:
one idea is that the predictor variable can be used like the usual variable, but the developer does not need to assign it a value. Instead, the predictor variables are read using reasoning in the underlying machine learning model to determine their values, which is triggered by the developer by calling Predict ():
value＝pvar.Predict()
this behavior makes it possible to use the predictor variables as a natural part of any program. Specifically, the developer may use only the predicted variables, rather than any heuristics or arbitrarily chosen constants. The predictive variables may also take the form of random variables, protecting the developer from the potential complexity of reasoning, sampling, and exploring/utilizing countermeasures (strategies).
The predictor variables may use observations about the developer's passing context to determine their values:
pvar.Observe(’low’,0.12)
pvar.Observe({’high’:0.56,’target’:0.43})
in some instances, developers may also provide additional side-information (side-information) into the predictor variables that the engineered heuristics would not use, but that powerful models can use, in order to improve performance.
Once this information becomes available, the developer can provide feedback on the quality of previous predictions. In this example, numerical feedback is provided:
pvar.Feedback(reward＝10)
some embodiments of this framework may follow common reinforcement learning practices: the predictive variable is intended to maximize the sum of the reward values (and possibly discounts) received over time. In other embodiments, the computer program or associated system may be aware of the correct value in the case of a backsight and provide a "ground truth" answer as feedback, transforming the learning task into a supervised learning problem. Some issues may have multiple metrics (metrics) that need to be optimized (e.g., runtime, memory, network bandwidth), and the developer may want to give feedback for each dimension. Other machine learning techniques may additionally or alternatively be incorporated, including, for example, a robbery (bandit) technique (e.g., dobby), black box optimization, and evolutionary countermeasures.
In addition to the API calls described above, the developer may also use additional configuration parameters to specify the model for predictive variable usage. For example, model hyper-parameters may be specified, and may be adjusted independently. The definition of a predictor variable typically determines only its interface (e.g., type and shape of inputs and outputs).
The API allows predictive variables to be easily and transparently integrated into existing applications with little development overhead.
Example predictive variables
Variables are part of most programming languages and serve as placeholders for storing data (and most commonly data that changes over time). In some languages, variables and functions may be considered interchangeably. Although the frameworks are presented here in terms of predictor variables, they may alternatively be considered as predictor functions or classes.
Variables capture the results of critical computations and are most often the basis of control decisions about which instruction set will be executed next. However, the computations required to achieve the desired results are often not explicit, but rather are specified by heuristic rules as contemplated by the programmer. Examples include determinations regarding: whether the user wants a UI for a red theme or a UI for a blue theme, to which support technology the question is routed; or estimate the amount of time required for the job to run so that it can be scheduled accordingly. By predicting variables, programmers obtain an abstraction (abstraction) in which they can give part of the control flow to the learning system.
Let us take a simple example of this, the Hello World program in predictive programming:
the above examples also include the concept of feedback. Some measure of goodness (goodness) from decisions/predictions made by the predictor variables is fed back. For example, in the If statement above, every place where a variable is evaluated is equal to the evaluation of the variable and affects something in the program context. Feedback allows us to concatenate their real-world results back to the variables that led to them. If the program is run, the output should be:
Still need to improve the predictor<may be printed multiple times if the initial decision is false>
Hello World
in the hello world program, the constant BAD is simply the sign for negative feedback, which can be defined as any negative number.
Another property of the hello world program is that its predictor variable does not depend on anything, and once it learns a value, it remains in this state. In other words, there is a predictive constant. Its value does not change after the feedback is completely absorbed.
However, in many cases, developers will want to write predictors (predictors) that are not constant, that is, they depend on some context. To express semantics for the predictive expression context, observed concepts may be used. That is, the predicted variables can be used to observe other variables (from the start of non-prediction, later covering the case of prediction).
One example is as follows:
// in the map navigation context. GPS is not accurate enough to tell us whether on a highway or on a road next to a highway.
Appropriate routing actions are taken.
Later on after the vehicle has continued to drive forward, we realize if the vehicle is actually on the highway.
Here, the variable on _ free depends on the current speed of the vehicle, and thus its value is observed. Each time on _ free is evaluated, its value may depend on the value of the variable it is allowed to observe. To allow on _ street to be more optimized in its evaluation, it may be allowed to observe more variables in the environment, such as, for example, geographic location, current traffic, etc. This basic syntax allows us to extend the span of observation by simply adding other factors in our context.
The predictor variables may be of the same type as any other type, and the range of available types will span from basic types to complex/derivative types. In some cases, the base type may be modified in a manner that is better suited for this context, e.g., in addition to or alternatively to the floating point type, the predictor variable may also be an n floating point (nfloat) or normalized floating point type with a value in (0, 1).
Some example base types of predictor variables include: boolean, enumeration, short int, floating point. Some example modification types of predictor variables include: n floating point (normalized floating point range (0, 1)), r _ int (range int, taking range (1, n)) sequence types: a string, a vector (vec), a list of basic or modified types, etc. Some example complex types include: structure (struct) (a combination of the above types).
Example predictor variables and persistence
The initial example of predicting variables is such that the variables exist only in the context of the running program. Now, the framework is extended to allow them a global namespace and to allow persistence across runs. To this end, each predictor variable may have a global name.
One example is as follows:
the problem we have to solve is to make a stack of user dependent UI decisions. The UX team has identified 3 types of users, those who spend a significant amount of time exploring the page content, those who simply quickly browse the page content before taking action, and those who are in between.
The ui _ choice variable here is given a global name, making it persistent. This means that virtually one copy of the variable (e.g., its predictor) is being used by any number of programs that use the variable. It is indeed an underlying model, with uniform namespaces and feedback statements meaning that the model can be trained in a distributed manner, and can be trained in one setting and adapted in another (possibly with real-time traffic).
Note that the above concept of global names can also be extended to include group names. For example, variables used together in a particular context can be part of a group, allowing them to share observables and pass learning to each other to obtain the best joint result. This may be accomplished by creating a prediction group and adding a variable as a member thereto.
These groups may have two purposes:
first, they make it easier for developers to pass feedback to multiple variables, while making the process of giving correct feedback to all the variables involved less prone to error.
Second, they allow the model to explicitly learn the relationships between variables in the group. Each variable in a group may also observe all other variables in the group, which may enable them to learn "invalid" combinations. In the above example, certain selections may render some assignments to other variables useless (e.g., the single _ angle mode may have no variables to control the number of options displayed in the explorer (explorer) mode).
One example is as follows:
// declaration prediction group
PredictionGroup ui_vars
UI _ Vars. ("Watch Page UI Vars")// global name of group.
V/declaring the first predictor variable
Predicted enum ui_choice{explorers,moderate,single_glance}
ui_choice.set_global_name(“Watch Page UI Choice”)
Setting ui _ choice as part of ui _ vars group
ui_vars.add_variable(ui_choice)
// context in which it can be observed
ui_choice.observes(user_click_rate)
ui_choice.observes(user_id)
// add other ui variables to the group, e.g., font _ size, thumbnail size, etc. Using variables to draw UI
Computer engage// based on how the user reacts to the UI
// now feeding back the entire team
feedback(ui_vars,engagement)
Initial function in predicted variables
To begin taking predictive variables, it may be helpful to allow for the provision of an initial value (or function) that can be used as a default before a good value is learned.
One area of application for predictive variables is to replace heuristics in existing code. In many cases, these heuristics will work for a long period of time and will present some resistance to "replacing it with a machine learning solution".
Thus, in many cases, one starting point may be to continue working with the old solution while starting to learn a good model of the predicted variables.
In this way, in some embodiments, the developer can pass the initial function to the predictor variable. In many cases, the initial function will be a heuristic that the predictor variable is replacing. Ideally, it is a reasonable guess as to what values the predictor variables return are good. The predictor variables may use this initial function to avoid very poor performance in the initial prediction and observe the behavior of the initial function to guide their own learning process, similar to mock learning. However, contrary to mock learning, where an agent tries to become as excellent as an expert, the predictor variables are explicitly aimed at outperforming the initial functions as quickly as possible.
The presence of the initial function should strictly improve the performance of the predictor variables. In the worst case, the predictor variable may choose to ignore it altogether, but ideally it will allow the predictor variable to explore solutions that are not easily reachable from a random starting point.
Specifically, having an initial strategy will help predict variables in three different ways: i) using it in an initial step will help limit regret before the predictor variables learn to a valid model; ii) provide relevant training experience for our off-policy training algorithm. Given that the initial strategy performs reasonably well, it is expected that better training data will be generated than a purely random strategy; iii) as a safety net (safety net). If the predictor variables do not learn a good strategy, then the initial strategy can be used to alleviate the very high regret.
In some embodiments, the predicted variable may also allow for monitoring changes compared to the original value, and it would ideally allow for measuring changes by experiment of the predicted variable compared to the original heuristic. The predictive variables may derive metrics that allow easy meter display (dashboarding) of the feedback obtained for both modes (default and predicted).
In some embodiments, an initial heuristic is used to predict variables, and a balance is struck between learning good strategies and the security of the initial function, which relies on strategy selection countermeasures. The strategy switches between utilizing learned policies, exploring alternative values, and using an initial function. It may be applied to an action or event period level depending on the requirements. Policy selection may compare the observed jackpot to decide which policy to implement among the random exploration, initial policy, and learned policy, and at which rate to implement.
As one example, in some embodiments, only the initial policy is used at the beginning, allowing only a small amount of exploration. Initial policy rewards may be accumulated to estimate their performance. After many (e.g., fixed number) steps, the learned policy may be used in small percentages. If the accumulated reward for the learned policy is far less than the initial policy, it is disabled again and only retested at a later time. However, if the learned policy performs at least as well as the initial policy, its usage will increase until the initial policy can be completely phased out.
Example solution countermeasures
Given the above framework, this section now describes some potential solution strategies. First, attention is directed to the manner in which the framework is outlined, which ensures that appropriate billing/logging (logging) of the various methods can be performed without relying on a particular solver. In some embodiments, each time a variable is evaluated, the control flow falls in a Predictive Programming stack, which allows us to see the state of all variables being observed and the current state of variables inferred from the observed variables. This information can be used for billing or logging so that when feedback is later received, the credit (credit) of the feedback can be traced back to all predictions made before that point, and a specific learning signal can be generated to refine the model used to evaluate the variables given the observed context.
The above framework for logging also abstracts another important factor in building machine learning systems today. In some embodiments of predictive programming, logging is handled entirely by the system, and this can be handled exclusively by the super user (power user), who for ordinary programmers does not need to consider what to log to train the concept of a machine learning system, but instead they express their intent only through predictive variables, the rest of which have already been handled by them.
If the underlying programming language is considered a black box, there are still plausible solution countermeasures. Based on the type of problem, some example solutions are outlined below. The solution used may be specified by a user or programmer.
Predicting a constant: operations research, black box optimization, reinforcement learning.
Single use variables: a pirate solver, black-box optimization, reinforcement learning, and supervised learning (in the case of explicit true signals, discussed later).
The sequential decision makes: black box optimization such as, for example, evolutionary countermeasures, reinforcement learning.
Even if not explicitly stated, the sample run may provide information about which of the above classes the variable belongs to, and may introduce a solver of appropriate complexity/efficiency for it.
Thus, some embodiments of the present disclosure take advantage of recent advances in deep reinforcement learning to implement a predictor variable, as it will allow the predictor variable to be applied to the most general usage scenario. Aspects of the interface described herein do translate naturally to reinforcement learning, where the input of an observation call (observer-call) is an observation combined into a state, the output of a prediction call (Predict-call) is an action, and feedback is translated into a reward. However, the predictor variables can be explicitly used with other learning methods, such as supervised learning methods or pirate-based methods.
With regard to the special case of supervised learning, problems are available for certain truth in the case of back vision, such as, for example, clicks in user interaction prediction, or actual system performance in system optimization, etc. This information may be available where it is not part of the same program context. These additional information may be conveyed in feedback as follows:
Predicted float predicted_variable
predicted_variable.set_global_name(“global name of this variable”).
predicted_variable.set_instance_id(instance_id_from_logs)
the/prediction variables may record an instance id for each prediction log they make, which may be accessed in the log and used to provide feedback at any particular point in time or to a particular instance.
feedback(predicted_variable,feedback_value,TRUTH)
This will make the system aware that it has a complete supervisory signal here and will enable these resolvers to function.
Background randomness (Stochastic)
Under
the
Hood) example variables
So far, predictor variables are described as if they were always deterministic. However, even the predicted or estimated deterministic quantities always have some uncertainty around them, the degree of uncertainty being affected by the quantity and quality of the data, constraints, a priori knowledge, etc. Furthermore, some quantities may be best modeled as explicit random, such as a particular decision to be made by a single user, or whether a snow chain is required when the car arrives in a mountain area. For both of these reasons, the predictor variables can be considered to be substantially random quantities in the background. In general, this randomness need not be fully exposed to the user, for example, when a simple numerical estimate needs to be used in the algorithm. This keeps the interface clean and does not require the programmer to consider dealing with randomness. In other cases (e.g. for super users) it may be necessary to estimate error bars around (estimate), or even a full a posteriori prediction probability distributions. The proposed framework allows such access and allows expressing constraints or a priori knowledge in probabilistic terms when it is useful to do so. Thus, the predictor variables can be entered into areas like randomization algorithms, confidence/variance based optimization, etc., to maintain a simple interface for the programmer in many usage scenarios while providing full background control when needed.
Example application of predictive variables in an Algorithm
This section describes how the predictor variables can be used for three different algorithmic problems, and how the developer can easily exploit the capabilities of machine learning with only a few lines of code. Experimental results show how using predictive variables helps improve algorithm performance.
The above example interface naturally translates to a reinforcement learning setting: the inputs of the observation calls may be combined into states, the outputs of the prediction calls may be actions, and the feedback may be rewards.
To evaluate the impact of predictive variables, cumulative regret was measured over the course of the training event. Unfortunately, it measures how much worse one method performs (or how better it performs when it is negative) than the other. Cumulatively unfortunately captures whether one approach is better than another over all previous decisions. For some practical usage scenarios, we are interested in two attributes: (1) unfortunately, it should never be so high as to guarantee acceptable performance of the predictor variables in all cases. (2) The accumulation unfortunately should become permanently negative as early as possible. This corresponds to the desire to achieve better performance than the baseline model as soon as possible.
Unlike the usual set of differentiating training and assessment modes, assessment is from the developer's perspective, without such differentiation. The developer simply inserts the predictor variables and begins running the program as usual. Overfitting does not cause problems due to the online learning settings in which the predictor variables are running. Thus, the (accumulated) regret does contain potential performance degradation due to the exploration noise. This effect can be mitigated by performing only a small portion of the exploration run.
For feasibility studies, the computational cost of reasoning in the model is not considered. Predictive variables will be applicable to a wide variety of problems, even if these are costly.
Example Experimental setup
FIG. 1 provides a block diagram of an example architecture for the experiments described herein. FIG. 1 illustrates how client code communicates with predicted variables and how models of the predicted variables are trained and updated via a machine learning system. The program binary includes a mini-library (shown as "PVar" in FIG. 1) that exposes a predictor variable interface to client applications. The predictor variables aggregate observations, actions, and feedback into an event period log that is passed to a replay buffer. The model is asynchronously trained. When a new checkpoint becomes available, the predictor variable loads it for use in successive steps.
To support predictive variables, recent advances in reinforcement learning are used for modeling and training. It allows predictive variables to be applied to the most general usage scenarios.
Example experimental models were established for DDQN (Hasselt et al, 2016) for classification output and TD3(Fujimoto et al, 2018) for continuous output. Since the success of DDQN in AlphaGo, it is a de facto standard in reinforcement learning. TD3 is the most recent modification to DDPG (Lillicrap et al, 2015) that uses a second network of criterions to avoid overestimating the expected prize.
Table 1 immediately below provides parameters for different experiments described below (FC ═ full connectivity layer, LR ═ learning rate).
The example policy selection strategy used in the experiment begins by evaluating only the initial function and then gradually begins to increase the use of the learned policy. It tracks the rewards received for these policies and adjusts the use of learned policies based on their performance. The bottom pane of fig. 2 shows the usage rate when the initial function is used, demonstrating the effectiveness of this strategy.
Similar to much of the work built on reinforcement learning techniques, the experiments described herein face the reproducibility problem described by Henderson et al, 2018. Only a few runs showed the desired behavior over multiple runs of any experiment, which were reported. However, in a "failed" run, baseline performance was observed, as the initial function acts as a safety net. Thus, experiments have shown that the proposed system can outperform the baseline heuristic without significant risk of serious failure.
Example application to binary search
Binary search (Williams 1976) is a standard algorithm for sorting arrays a ═ a in size N0,a1,…,aN-1Find the position l of the target value x inx. The binary search has when no further knowledge about the data distribution is available
The simplest method of using a predictor variable is to directly estimate the location lxAnd incentivizes the search to do so in as few steps as possible by penalizing each step with the same negative reward (see, e.g., list 1 provided below). At each timeOne step, the predictor variable observes the value a at both ends of the search intervalL、aRAnd a target x. The predictor variable output q is used as the relative position of the next read index m, such that m equals qL + (1-q) R.
List 1 (top is standard binary search, bottom is a simple method of using predictor variables in binary search):
to give the model a stronger learning signal, the developer may incorporate question-specific knowledge into the reward function, or into how the predictive variables are used. One way to form the reward is to consider problem simplification. For binary search, reducing the size of the remaining search space will proportionally increase the search speed and should be rewarded accordingly. Reduction of search Range (R) by replacing step-counting rewards in List 1 (line 9)t-Lt)/(Rt+1-Lt+1) We directly reward for reducing the size of the search space. By forming the reward like this, we can attribute the feedback signal to the current prediction and can reduce the problem from reinforcement learning to a context-dependent robber (we implement by using a discount factor of 0).
Alternatively, we can change the way in which prediction is used to solve the problem in a way that the predictor variables learn faster and cannot predict very poor values. For many algorithms, including binary searches, it is possible to predict combinations of (or selections of) several existing heuristics, rather than predicting values directly. We use two heuristics: (a) ordinary binary search using a segmentation location lvWill search range { a } by (L + R)/2L,…,aRDividing into two equally large portions, and (b) an interpolation search that interpolates the division position by li＝((aR-v)L+(v-aL)R)/(aR-aL). Then, we use the values q of the predictor variables to mix between these heuristics to obtain the predicted segmentation locations lq＝qlv+(1-q)li. Since in practice both of these heuristics work well over many distributions, any point in between will also work well. This reduces the risk of the predictor variable choosing a very poor value, which in turn facilitates learning. One disadvantage is that it is not possible to find a solution with lvAnd liAnd a value outside the interval therebetween.
To evaluate the proposed method, a test environment was used in which, in each event period, we sampled arrays of 5000 elements from a randomly selected distribution (uniform, triangular, normal, pareto, power, gamma, and chi-square), sorted, scaled to [ -10 ] to [ -10 ] distribution, sorted by power, and sorted by power4,104]And searches for random elements.
Fig. 2 shows the cost (top left), cumulative regret (right) and initial function usage (below) for different variants of binary search compared to normal binary search. In particular, FIG. 2 shows the results of different variants of binary search using predictor variables and comparing them to the common binary search baseline. The results show that the simplest case (location, simple, no initial function) where we predict relative location directly with a simple reward without using an initial function, initially underperforms, but then becomes almost as good as baseline (cumulatively regrettably becomes almost constant after the initial bad period). The next case (location, simple reward) has the same settings, but we use the initial function and we see that the initial regret is much smaller. By using tangible (shaped) rewards (locations, tangible rewards), predictor variables can quickly learn the behavior of the baseline. Both methods of the hybrid heuristic are significantly better than baseline.
Table 2 cumulative regrettings to become permanently negative (compared to all baselines) required training event periods ("-": not occurred over 5000 event periods) for all combinations of prediction, reward, and use of initial functions.
Table 2 immediately above compares the time to reach break-even using different variants of predictor variables in a binary search. These numbers indicate how many event periods are cumulatively unfortunately becoming permanently negative, which means that for any additional evaluation after that, the user gets a net gain from using the predictor variables as compared to not using ML at all. The table shows that reward formation and smart use of predictions improve performance, but it also shows that even simple methods can bring improvements. Note that on a uniform distribution, no model is preferred over the interpolation search because it is the best approximation of the distribution.
Example quick sort application
Fast sorting (Hoare 1962) sorts the array in-place by recursively dividing the array into two sets (smaller/larger than the hub) until the array is fully sorted. Quick sorting is one of the most common sorting algorithms, where a number of heuristics have been proposed to select the hub element. While the average temporal complexity of fast ordering is θ (nlog (N)), a worst-case temporal complexity O (N) may occur when the axis elements are poorly selected2). The best choice for the axis is the median of the range, which divides the range into two equal sized portions.
To improve the quick sort using the predictor variables, one example approach is to adjust the axis selection heuristic. To allow for sorting of any type, we decide to use the predictor variables to determine the number of elements sampled from the array to be sorted, and then choose the median from these samples as the axis (see, e.g., list 2).
In particular, Table 2 provides a quick sort implementation that uses predictor variables to select the number of samples to calculate the next hub. As feedback, we use the step cost compared to the optimal partitioning.
List 2:
as a feedback signal for the recursive step, an estimate ac of its impact on the computation cost can be used:
where n is the size of the array, a and b are the size of the partitions, n ═ a + b, cpivot＝cmedian+cpartitionIs the cost of computing the median of the samples and partitioning the log group. Δ crecursiveThe proximity of the current partition to the ideal case (median) is considered. The cost is a weighted sum of the read, write and compare times. Similar to the tangible reward in binary search, this reward allows us to reduce the reinforcement learning problem to a context-dependent robbery problem, and we use discount 0.
For evaluation, we used a test environment in which we ordered randomly shuffled arrays. The results of the experiments using predictor variables to select the number of axes in the quick sort are shown in fig. 3A and 3B. In particular, fig. 3A illustrates the overall cost over the training event period for different baseline approaches and variations using predictive variables. FIG. 3B illustrates the cumulative regret of the predictive variable approach over the training event period as compared to each baseline.
Fig. 4 shows the scores of the axes selected by the predictor variables in the quick sort after 5000 event periods. The expected approximation error for the median is given in the legend, alongside the number of samples. Fig. 4 shows that the predictor variables learn a non-trivial (non-trivial) strategy. Predictor variable learning selects more samples on larger array sizes, similar to our behavior of manual coding in adaptive baselines, but in this case, no manual heuristic engineering is required and a better strategy is learned. Further, note that predictive variable-based methods can adapt to changing environments, which is not the case with engineered heuristics. One surprising result is that predictor variables prefer 13 samples over 15 samples in large array sizes. We assume this happens because the large array example is rarely seen during training (one per event period, while the smaller size array can be seen multiple times per event period).
Example cache applications
Caches are a common component for accelerating computing systems. When the cache is full and needs to store new elements, they use Cache Replacement Policy (CRP) to determine which element to evict. Probably the most popular CRP is the Least Recently Used (LRU) heuristic, which evicts the element with the earliest access timestamp. Many approaches have been proposed to improve cache performance using machine learning. The present disclosure provides two different example methods of how predictor variables may be used in CRP to improve cache performance.
Discrete (e.g., table 3 below): the predictor variables either directly predict which element to evict or choose not to evict at all (by predicting invalid indices). That is, the predictive variable learns to become CRP by itself. While this is the simplest method of using the predictor variables, it makes it more difficult to learn CRPs better than LRUs (in fact, in this case, it is not trivial to learn even shoulders to LRUs).
List 3 (cache replacement policy direct prediction eviction decision):
consecutive (e.g., list 4 below): the predictor variable is used to enhance the LRU by predicting the offset to the last access timestamp. Here, the predictor variables learn which entries are to be kept longer in cache and which entries are to be evicted more quickly. In this case, by predicting the zero offset, it is trivial to become as good as LRU. The predictor variable values in (-1, 1) are scaled to obtain a reasonable range of values for the offset. Alternatively, the element may be selected not to be stored by predicting a sufficiently negative score.
List 4 (cache replacement policy using priority queues):
in both methods, feedback to the predictor variables is whether an entry (+1) is found in the cache or whether an entry (-1) is not found. In the discrete approach, we also give a reward of-1 if eviction does occur.
In an example embodiment, the observation is a history of accesses, memory contents, and elements that were evicted. The predictor variables may observe (1) the key entered as a classification or (2) the characteristics of the key.
Observing the keys as sort inputs allows avoiding feature engineering and enables direct learning of the properties of a particular key (e.g., which keys are accessed most), but makes it difficult to handle rare and unseen keys. To process keys as input, one example approach is to train an embedded layer shared between an actor (actor) network and a judge network. In particular, fig. 5 shows the architecture of an example neural network with a TD3 of a key-embedded network.
As a feature of the key, we observe the historical frequency calculated over a fixed size window. This approach requires more effort by the developer to implement such features, but rewards with better performance and the fact that the model does not rely on specific key values.
The experiment was performed with three combinations of these options: (1) discrete cache observation keys, (2) continuous cache observation keys, (3) continuous cache observation frequencies. For evaluation, a cache of size 10 and integer keys from 1 to 100 are used. Two synthetic access patterns of length 1000 are used, with independent iso-distribution (i.i.d.) sampling from the power-law distribution of α -0.1 and α -0.5.
Fig. 6A-6D show results of three variants of the predictive cache, the standard LRU cache, and the oracle cache, giving a theoretical, unachievable upper limit of performance. In particular, FIGS. 6A-6D show the cache performance of the power-law access mode. For fig. 6A and 6B: α is 0.1, and for fig. 6C and 6D: α is 0.5. Fig. 6A and 6C show hit rates (no exploration), and fig. 6B and 6D show cumulative regressions (exploration).
Once learning has converged, we look at the unexplored hit rate to understand the potential performance of the model. However, cumulative regrettings are still reported under exploratory noise.
Both implementations working directly on key embedding learn to behave like an LRU baseline (comparable hit rate) without exploration. However, the continuous variant puts higher penalties for exploration (higher cumulative regret). Note that this means that continuous variants learn to predict constant offsets (which is trivial), whereas discrete implementations actually learn to be non-trivial LRU CRPs. Frequent continuous implementations quickly outperform the LRU baseline, making cost/benefit long-term worthwhile (regret with negative accumulation after hundreds of event periods).
Additional example uses
Example context-free predictor variables, e.g., Hello
World
Context-free variables can still be learned from the past history of their own evaluations. They may also be background random, so while a variable may be boolean, it may hold more state internally to provide the best boolean answer whenever evaluated.
Variables that do not observe any context, do not use past history, and are not random are equivalent to constants. The main difference is that this "constant" value is learned by the predictive system to maximize the score received in the feedback, and may be different each time it is read (if the variable is background random).
Some example issues include:
tuning the learning coefficient of the formula by a heuristic method;
learning coefficients for the approximation;
determining a good threshold; and
the best parameters in the configuration are learned.
Such variables may prove useful in replacing a large number of heuristic formulas and temporary (ad-hoc) choices that are typically made in building a complete stacked software system with choices that are aware of the downstream impact of the choice (because the variables maximize some of the final number we are concerned with)
Example context-dependent variables used only once
The predictive variables that use context can be further divided into variables that are used exactly once before feedback is received and variables that can be used multiple times.
Example questions for a single user variable include:
making a category decision;
inferring attributes of the user;
a probability that the transaction is fraudulent;
how likely the user likes this item;
intelligent UI elements such as, for example, UI elements that can adapt themselves to a given context, user, device, geography, content within them, etc.; and
distributed system configuration for optimal throughput.
This category covers all cases where decisions are made and executed at once. After this decision, there can still be any domain knowledge and real world interaction. The reason for separating this situation is to show the gradual increase in complexity required to implement a system like this behind the scenes.
Example sequential decision making
Next, a more complex example shows how variables are repeatedly called to make a series of decisions. To do this, please consider the load balancing problem. The request enters the API endpoint, which must forward it to one of the n copies. The goal is to achieve as good a delay as possible at the 90 th percentile. (a very practical problem to be dealt with by most products). In context, let us assume that the current load of each copy and its average response time to the last 30 requests are known. Thus, in this simple example, each copy has two features.
The variable will decide which copy to send the query to and will be invoked repeatedly for each query. The feedback will be the 90 th percentile of the delay. One example is as follows:
if these variables already contain relevant information about the copy
Here, the copy selection variable is repeatedly called in the loop. Each decision is made based on this variable and we get feedback on the overall outcome of these decisions. Furthermore, unlike the previous case, the predictor variable does not have a single optimum-in a load balancing system, returning the same value for the chosen _ replica again and again can be problematic.
This opens the system for handling a large class of problems, including for example:
combined search and optimization;
system optimization, such as caching, load balancing, scheduling, etc.;
graph-based questions such as, for example, TSPs, route finding, navigation, and the like;
a market algorithm, which is used for searching the optimal parameter of the market efficiency; and
an outer loop that is any set of heuristics or learned countermeasures, such as, for example, search/fast indexing, divide and conquer methods, and the like.
Example advanced usage
Much of the description above is directed to programmers who do not have a priori knowledge or deep contact with machine learning. However, if the user has in-depth knowledge of ML, such abstraction may still be useful to them while maintaining a clean interface and giving them sufficient (or complete) control over the process.
For each predicted variable, we can expose a FullConfig object with specifications and controls for all the different aspects of the machine learning system that is used by the background for that variable.
In one example, we wish to incorporate local event and weather information into our path finding algorithm for map navigation:
here we intend to train a system to decide which node to visit next in order to get a good path quickly, and also to consider other side information like weather, local events, etc. The above pseudo-code provides a simple abstraction that subtly separates the machine learning component from the overall structure of the program while keeping control in the hands of the programmer.
If a supervisor wants to adjust the details of the solver, they can perform the following operations:
FullLearningSpec spec＝next_node.FullLearningSpec()
spec.set_learning_rate(0.001)
spec.set_network_depth_for_observer(current_path,[100 20])
spec.set_network_depth_for_observer(current_graph,[1000 200])
spec.set_network_depth_for_observer(local_events,[30 20])
spec.set_network_depth_for_observer(local_weather,[100 20])
spec.set_fusion_level(FullLearningSpec.LateFusion())
and so on.
One major benefit left in abstraction for super-users is the ease of evolving their system. As an example, if they get more than one sideband information, they do not need to write a large number of flash jobs and data converters to merge them, they only need to read it in the context of their main program and have the variable depend on it. If they want to predict the goodness value of the entire path, they may only need to add one more variable. If they want the next node predictor and goodness metric to share parameters, they can add both variables to the group, and so on.
In further embodiments, the framework may provide additional advanced features such as multiple feedback metrics, automated a/B experiments, distributed log aggregation and efficiency, and/or other features.
Additional example computer science problem applications
The following are example issues that may be addressed using the provided framework.
High speed buffer (LRU high speed buffer, etc.)
1. What is evicted from the cache instead of using the LRU policy. LRU works well as a heuristic in many cases, but may not be perfect and may fail severely in some cases.
The predictor variables may be intended to learn what the "oracle" evicts from the cache.
2. Cache size: memory is wasted when the cache is too large and does not perform well when the cache is too small. A good cache size is determined as a predictor variable.
Search algorithm (A, etc.)
A uses heuristics to estimate the remaining cost to guide its search. A good heuristic function is learned as a predictor variable to make a perform well and eliminate the need to manually specify a heuristic.
Branch and bound algorithm
A predictor variable is used to determine a good branch countermeasure.
Note that Branch and Bound (Branch and Bound) algorithms are often used to approximate NP difficult problems. A good goal here is to obtain a better approximation using the predictor variables.
Divide and conquer algorithm
The predictor variables are used to determine good partitioning countermeasures (compare, for example, binary search & quicksort, below).
Search algorithm
A binary search is improved, where a "binary" partition point may be determined from the predictor variables.
Sorting algorithm
Fast sequencing is improved by selecting a hub using a predictor variable. This may only have an impact on very large ordering problems.
Merging and sequencing: branch factors are determined as predictor variables.
Approximate TSP
The general idea is as follows: the quality of the empirical approximation is improved by using predictive variables.
Some additional ideas:
greedy algorithm: nearest neighbor selection: computing neighborhood distances using predictor variables-with the aim of making it better than using actual nearest neighbors
Pairwise exchange: selecting edges to swap using predictor variables + selecting edges to insert using predictor variables
Ant colony optimization: replacing ants with few predictor variables; )
Replacing smalls in UXHeuristics
Many applications have built-in small heuristics-replacing them with predictive variables.
Determining learning rate/step size in a gradient descent/optimizer
Many optimizers only work if you work out the right learning rate — there may be large differences between different optimizers. Furthermore, many optimizers have more than one meta-parameter (beta 1, beta2, epsilon in Adam) that are rarely adjusted. These learning rates are set as predictive variables and improved over time.
Example apparatus and System
Fig. 7A depicts a block diagram of an example computing system 100, according to an example embodiment of the present disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled via a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., a smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and may be one processor or operatively connected processors. Memory 114 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, etc., and combinations thereof. The memory 114 may store data 116 and instructions 118 that are executed by the processor 112 to cause the user computing device 102 to perform operations.
In some implementations, the user computing device 102 can store or include one or more machine-learned models 120. For example, the machine-learned model 120 may be or may otherwise include various machine-learned models, such as a neural network (e.g., a deep neural network) or other types of machine-learned models, including non-linear models and/or linear models. The neural network may include a feed-forward neural network, a recurrent neural network (e.g., a long-short term memory recurrent neural network), a convolutional neural network, or other form of neural network.
In some implementations, the one or more machine-learned models 120 can be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 may implement multiple parallel instances of a single OVERALL (overlap) model 120.
Additionally or alternatively, one or more machine-learned models 140 may be included in the server computing system 130, or otherwise stored in the server computing system 130, and implemented by the server computing system 130, where the server computing system 130 communicates with the user computing device 102 according to a client-server relationship. For example, the machine-learned model 140 may be implemented by the server computing system 140 as part of a web service. Thus, one or more models 120 can be stored and implemented at the user computing device 102, and/or one or more models 140 can be stored and implemented at the server computing system 130.
The user computing device 102 may also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other device through which a user may provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and may be one processor or operatively connected processors. Memory 134 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, etc., and combinations thereof. The memory 134 may store data 136 and instructions 138 that are executed by the processor 132 to cause the server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances where the server computing system 130 includes multiple server computing devices, such server computing devices may operate according to a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 may store or otherwise include one or more machine-learned models 140. For example, the model 140 may be or may otherwise include various machine-learned models. Example machine-learned models include neural networks or other multi-layered nonlinear models. Example neural networks include feed-forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks.
The user computing device 102 and/or the server computing system 130 may train the models 120 and/or 140 via interaction with a training computing system 150 communicatively coupled through a network 180. The training computing system 150 may be separate from the server computing system 130 or may be part of the server computing system 130.
In particular, in some instances, model trainer 160 may train machine-learned models 120 and/or 140 based on a set of training data 162. The training data 162 may include, for example, feedback data provided by a computer program.
In some implementations, the training examples may be provided by the user computing device 102 if the user has provided consent. Thus, in such implementations, the model 120 provided to the user computing device 102 may be trained by the training computing system 150 on user-specific data received from the user computing device 102. In some instances, this process may be referred to as a personalization model.
The model trainer 160 includes computer logic for providing the desired functionality. Model trainer 160 may be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some embodiments, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other embodiments, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium, such as a RAM hard disk or an optical or magnetic medium.
FIG. 7A illustrates one example computing system that can be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the user computing device 102 may include a model trainer 160 and a training data set 162. In such implementations, the model 120 may be trained and used locally at the user computing device 102. In some such implementations, the user computing device 102 may implement a model trainer 160 to personalize the model 120 based on user-specific data.
Fig. 7B depicts a block diagram of an example computing device 10, performed in accordance with an example embodiment of the present disclosure. Computing device 10 may be a user computing device or a server computing device.
As shown in fig. 7B, each application may communicate with a plurality of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or an additional component. In some implementations, each application can communicate with each device component using an API (e.g., a public API). In some embodiments, the API used by each application is specific to that application.
Fig. 7C depicts a block diagram of an example computing device 50, performed in accordance with an example embodiment of the present disclosure. Computing device 50 may be a user computing device or a server computing device.
The central smart inlay includes a number of machine-learned models. For example, as shown in fig. 7C, a respective machine-learned model (e.g., model) may be provided for each application and managed by the central intelligence layer. In other embodiments, two or more applications may share a single machine-learned model. For example, in some embodiments, the central smart inlay may provide a single model (e.g., a single model) for all applications. In some implementations, the central smart inlay is included in or otherwise implemented by the operating system of the computing device 50.
The central smart inlay may communicate with a central device data plane. The central device data layer may be a centralized data repository for the computing device 50. As shown in fig. 7C, the central device data layer may communicate with a plurality of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a private API).
Additional disclosure
The technology discussed herein relates to servers, databases, software applications, and other computer-based systems, and the actions taken and information sent to and from these systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components operating in combination. Databases and applications may be implemented on a single system or may be distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Alterations, permutations, and equivalents will readily occur to those skilled in the art upon a reading of the foregoing description. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Thus, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (37)
1. A computer-implemented method, the method comprising:
obtaining, by one or more computing devices, a computer program comprising a set of computer-executable instructions, wherein the computer program defines variables that serve as placeholders for storing data;
providing, by the one or more computing devices, observation data to a machine learning system;
receiving, by the one or more computing devices, predicted values for the variables generated by the machine learning system based at least in part on the observation data;
setting, by the one or more computing devices, the variable equal to the predicted value; and
after setting, by the one or more computing devices, the variable equal to the predicted value, executing, by the one or more computing devices, the computer program, wherein executing the computer program comprises implementing at least one instruction of the set of computer-executable instructions that controls an operation of the one or more computing devices based at least in part on the variable.
2. The computer-implemented method of claim 1, wherein receiving, by the one or more computing devices, the predicted values for the variables produced by the machine learning system comprises receiving, by the one or more computing devices, the predicted values for the variables produced by the machine learning system via a predefined application programming interface.
3. The computer-implemented method of any preceding claim, wherein the machine learning system combines the observation data into a current state, and wherein the variables comprise stateful variables.
4. The computer-implemented method of any preceding claim, wherein providing, by the one or more computing devices, the observation data to the machine learning system comprises providing, by the one or more computing devices, the observation data to the machine learning system via a predefined application programming interface.
5. A computer-implemented method as in any preceding claim, wherein the providing, by the one or more computing devices, the observation data to the machine learning system is caused by and a result of execution of at least one instruction included in the computer program.
6. A computer-implemented method as in any preceding claim, wherein the observation data describes current values of one or more other variables defined by the computer program.
7. The computer-implemented method of any preceding claim, further comprising:
providing, by the one or more computing devices, feedback data to the machine learning system, wherein the feedback data is used to train the machine learning system to predict the variable.
8. The computer-implemented method of claim 7, wherein providing, by the one or more computing devices, the feedback data to the machine learning system comprises providing, by the one or more computing devices, the feedback data to the machine learning system via a predefined application programming interface.
9. A computer-implemented method as in claim 7 or 8, wherein the providing, by the one or more computing devices, the feedback data to the machine learning system is caused by and a result of execution of at least one instruction included in the computer program.
10. The computer-implemented method of any of claims 7-9, wherein the feedback data describes a result of an operation of the one or more computing devices controlled based at least in part on the variable.
11. The computer-implemented method of any of claims 7-10, further comprising:
determining, by the machine learning system, a reward value based at least in part on the feedback data; and
modifying, by the machine learning system, a strategy implemented by the machine learning system to produce predicted values of the variables based at least in part on the reward values.
12. A computer-implemented method according to any of claims 7-11, wherein the feedback data describes a base fact value of the variable observed after receiving a predicted value for the variable.
13. The computer-implemented method of any of claims 7-12, further comprising:
performing, by the machine learning system, supervised learning based at least in part on a loss function that pairwise compares predicted values for the variable with base fact values for the variable.
14. The computer-implemented method of any of claims 7-13, further comprising:
determining, by the machine learning system, a fitness value based at least in part on the feedback data; and
determining, by the machine learning system, based at least in part on the fitness value, whether to select a mutation model implemented by the machine learning system to produce predicted values for the variable or to select a surrogate model.
15. The computer-implemented method of any of claims 7-14, wherein providing, by the one or more computing devices, the feedback data to the machine learning system comprises providing, by the one or more computing devices, the feedback data to a prediction group, wherein the prediction group comprises the variable and one or more additional variables, such that feedback is provided for multiple variables simultaneously.
16. A computer-implemented method as in any preceding claim, wherein the machine learning system comprises a software agent that generates predicted values for the variables.
17. A computer-implemented method as in any preceding claim, wherein the machine learning system comprises a machine-learned model that produces predicted values for the variables.
18. A computer-implemented method as in any preceding claim, wherein the machine learning system comprises a machine-learned neural network that produces predicted values for the variables.
19. The computer-implemented method of any preceding claim, wherein the variable comprises one of the following types:
boolean;
enumerating;
short shaping;
a floating point;
normalizing the floating point;
integer within the defined range;
a sequence of character strings;
the vector of any of the above;
a vector of vectors;
a list of any of the above; or
Combinations thereof.
20. The computer-implemented method of any preceding claim, wherein the variable is present only in a single running instance of the computer program.
21. The computer-implemented method of any of claims 1-19, wherein the variable persists across multiple running instances of the computer program.
22. A computer-implemented method as any one of claims 1-19 or 21 recites, wherein the variable persists across the computer program and one or more additional computer programs.
23. The computer-implemented method of any preceding claim, wherein the computer program is executed by a first computing device and the machine learning system is executed by a second computing device different and distinct from the first computing device.
24. The computer-implemented method of claim 23, wherein the first computing device comprises a user computing device and the second computing device comprises a server computing device.
25. A computer-implemented method as in any preceding claim, wherein the computer program and the machine learning system are executed by the same single device.
26. The computer-implemented method of claim 25, wherein the machine learning system comprises a library that has been added to the computer program.
27. The computer-implemented method of any preceding claim, wherein the set of computer-executable instructions included in the computer program encodes symbolic logic, and wherein the machine learning system performs numerical reasoning to produce predicted values for the variables.
28. The computer-implemented method of any preceding claim, wherein the initial policy of the machine learning system comprises a user-defined heuristic.
29. A computer-implemented method, the method comprising:
receiving, by a machine learning system, observation data from a computer program via a first application programming interface;
determining, by the machine learning system, a predicted value for a variable of a computer program based at least in part on the observation data; and
providing, by the machine learning system, the predicted values for the variables to the computer program via a second application programming interface.
30. The computer-implemented method of claim 29, further comprising:
receiving, by the machine learning system, feedback data from the computer program via a third application programming interface; and
modifying, by the machine learning system, a machine learned model or strategy based at least in part on the feedback data, the machine learned model or strategy generating predicted values for the variables of the computer program based at least in part on the observation data.
31. The computer-implemented method of any preceding claim, wherein the computer program comprises a mobile application.
32. A computer system, comprising:
one or more processors; and
one or more non-transitory computer-readable media collectively storing instructions that, when executed by the one or more processors, cause the computer system to perform the method of any of claims 1-31.
33. One or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more processors, cause the one or more processors to perform the method of any of claims 1-31.
34. One or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more processors, cause the one or more processors to communicate using an application programming interface that enables a set of client code to interface with a machine learning system to receive predicted values for predictor variables defined within the set of client code.
35. The one or more non-transitory computer-readable media of claim 34, wherein the application programming interface further enables the set of client code to communicate observation data to the machine learning system, wherein the machine learning system infers predicted values for the predictor variables based at least in part on the observation data.
36. The one or more non-transitory computer-readable media of claim 34 or 35, wherein the application programming interface further enables the set of client code to communicate feedback data to the machine learning system, wherein the machine learning system uses the feedback data to update a learned inference model that predicts predicted values for the predictor variables.
37. The one or more non-transitory computer-readable media of any of claims 34-36, wherein the application programming interface is embodied in a library, wherein the library is incorporated into a computer program that also includes the set of client code.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862737048P | 2018-09-26 | 2018-09-26 | |
US62/737,048 | 2018-09-26 | ||
PCT/US2018/062050 WO2020068141A1 (en) | 2018-09-26 | 2018-11-20 | Predicted variables in programming |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112771554A true CN112771554A (en) | 2021-05-07 |
Family
ID=64664829
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880098131.4A Pending CN112771554A (en) | 2018-09-26 | 2018-11-20 | Predictive variables in programming |
Country Status (3)
Country | Link |
---|---|
US (1) | US20220036216A1 (en) |
CN (1) | CN112771554A (en) |
WO (1) | WO2020068141A1 (en) |
Families Citing this family (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3871100A4 (en) * | 2019-02-11 | 2021-12-01 | Bitmovin, Inc. | Chunk-based prediction adaptation logic |
US11663522B2 (en) * | 2020-04-27 | 2023-05-30 | Microsoft Technology Licensing, Llc | Training reinforcement machine learning systems |
CN112286203B (en) * | 2020-11-11 | 2021-10-15 | 大连理工大学 | Multi-agent reinforcement learning path planning method based on ant colony algorithm |
CN112512003B (en) * | 2020-11-19 | 2021-11-05 | 大连理工大学 | Dynamic trust model based on long-time and short-time memory network in underwater acoustic sensor network |
CN113064422B (en) * | 2021-03-09 | 2022-06-28 | 河海大学 | Autonomous underwater vehicle path planning method based on double neural network reinforcement learning |
US20230029024A1 (en) * | 2021-07-21 | 2023-01-26 | Big Bear Labs, Inc. | Systems and Methods for Failed Payment Recovery Systems |
CN113342700B (en) * | 2021-08-04 | 2021-11-19 | 腾讯科技（深圳）有限公司 | Model evaluation method, electronic device and computer-readable storage medium |
US11967200B2 (en) | 2022-01-12 | 2024-04-23 | Lnw Gaming, Inc. | Chip tracking system |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8229864B1 (en) * | 2011-05-06 | 2012-07-24 | Google Inc. | Predictive model application programming interface |
EP4328816A1 (en) * | 2014-06-30 | 2024-02-28 | Amazon Technologies, Inc. | Machine learning service |
US10839329B2 (en) * | 2016-10-25 | 2020-11-17 | Sap Se | Process execution using rules framework flexibly incorporating predictive modeling |
-
2018
- 2018-11-20 US US17/280,034 patent/US20220036216A1/en active Pending
- 2018-11-20 WO PCT/US2018/062050 patent/WO2020068141A1/en active Application Filing
- 2018-11-20 CN CN201880098131.4A patent/CN112771554A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20220036216A1 (en) | 2022-02-03 |
WO2020068141A1 (en) | 2020-04-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN112771554A (en) | Predictive variables in programming | |
Qian et al. | Orchestrating the development lifecycle of machine learning-based IoT applications: A taxonomy and survey | |
CN112101562B (en) | Implementation method and system of machine learning modeling process | |
JP2024026276A (en) | Computer-based systems, computer components and computer objects configured to implement dynamic outlier bias reduction in machine learning models | |
CN107908688B (en) | A kind of data classification prediction technique and system based on improvement grey wolf optimization algorithm | |
Srinivasan et al. | Machine learning approaches to estimating software development effort | |
Fang et al. | An estimation of distribution algorithm and new computational results for the stochastic resource-constrained project scheduling problem | |
Wellman et al. | State-space abstraction for anytime evaluation of probabilistic networks | |
US7493300B2 (en) | Model and system for reasoning with N-step lookahead in policy-based system management | |
CN109313720A (en) | The strength neural network of external memory with sparse access | |
Lyu et al. | An empirical study of the impact of data splitting decisions on the performance of aiops solutions | |
Liu et al. | Blockchain-based task offloading for edge computing on low-quality data via distributed learning in the internet of energy | |
Vasudevan et al. | Learning semantic representations to verify hardware designs | |
Audet et al. | A general mathematical framework for constrained mixed-variable blackbox optimization problems with meta and categorical variables | |
Bruns et al. | Bat4CEP: a bat algorithm for mining of complex event processing rules | |
Violos et al. | Predicting resource usage in edge computing infrastructures with CNN and a hybrid Bayesian particle swarm hyper-parameter optimization model | |
Simons et al. | A comparison of meta-heuristic search for interactive software design | |
Lemos et al. | Repairing Boolean logical models from time-series data using Answer Set Programming | |
KR20210149393A (en) | Apparatus and method for training reinforcement learning model in use of combinational optimization | |
Vieira et al. | Dyna: Toward a self-optimizing declarative language for machine learning applications | |
Chugh et al. | Towards better integration of surrogate models and optimizers | |
KR102259945B1 (en) | System and method for A/B testing utilizing prediction based on artificial intelligence | |
CN116055209A (en) | Network attack detection method based on deep reinforcement learning | |
US20220207413A1 (en) | Loss augmentation for predictive modeling | |
Sagaama et al. | Automatic parameter tuning for big data pipelines with deep reinforcement learning |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |