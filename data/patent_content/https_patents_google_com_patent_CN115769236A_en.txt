CN115769236A - Modeling long-range interactions with reduced feature materialization via lambda functions - Google Patents
Modeling long-range interactions with reduced feature materialization via lambda functions Download PDFInfo
- Publication number
- CN115769236A CN115769236A CN202180045698.7A CN202180045698A CN115769236A CN 115769236 A CN115769236 A CN 115769236A CN 202180045698 A CN202180045698 A CN 202180045698A CN 115769236 A CN115769236 A CN 115769236A
- Authority
- CN
- China
- Prior art keywords
- lambda
- data
- machine learning
- computing system
- learning model
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
Abstract
The present disclosure provides systems, methods, and computer program products for modeling that performs long-range interactions with reduced feature materialization, for example, in machine learning models. A computer-implemented method may include: receiving layer input comprising input data and context data; generating one or more lambda functions based at least in part on the content function and the location function for each of the plurality of context elements in the context data; and applying one or more of the generated lambda functions to the input data in association with generating a layer output associated with the respective lambda layer. Experimental results of image classification on ResNet and object detection by RetinaNet show that examples of the present disclosure are significantly superior to convolution and attention counterparts while providing improved accuracy and efficiency.
Description
RELATED APPLICATIONS
This application claims the benefit of U.S. provisional patent application No. 63/051,969, filed on 7, 15, 2020, which is incorporated herein by reference in its entirety.
Technical Field
The present disclosure relates generally to machine learning architectures. More particularly, the present disclosure relates to systems, methods, and computer program products for modeling long-range (long-range) interactions in a machine learning model with reduced feature materialization (materialization) using lambda functions.
Background
Modeling of long-range interactions is important in machine learning. Attention has become a common method for capturing long-range interactions and is preferred over recursive-based methods. However, attentive operation is affected by the complexity of the quadratic memory per example. In fact, significant challenges in applying attention to large inputs come from the large memory footprint and computational requirements associated with materialized attention-graphs (such as per-example attention-graphs). This resource burden prevents the use of attention, for example, in long sequences and multi-dimensional inputs such as images.
Disclosure of Invention
Various aspects and advantages of examples of the disclosure will be set forth in part in the description which follows, or can be learned by practice of the examples.
One example aspect of the present disclosure relates to a computer-implemented method to perform modeling of long-range interactions with reduced feature materialization in machine learning systems and models. For example, a computer-implemented method may be performed by a computing device associated with one or more machine learning models. In an example, a computing device receives one or more layer inputs including input data and context (context) data. Context data generally refers to one or more contextual elements, each having a location associated with content, such as image data, text data, audio data, video data, or any other type of data. The computing device generates a lambda function for each of one or more elements associated with the input data. For example, the lambda function may be calculated based on context data using the content and location information. In addition, the computing device applies each of the generated lambda functions to a respective corresponding input element.
In various example implementations, the lambda function is a transformation that is generated based on the available context and that is directly applied to the respective input elements without materializing the attention maps of each example. Avoiding per-example secondary memory complexity per-example attention maps allows implementation in more limited memory space, while still taking into account context, particularly long-range interactions, as compared to other approaches that use attention.
In an example, the key sum value may be determined based on linear projection context data, where the keys are normalized across contextual locations based on a softmax function. The lambda layer provides functional messaging in which each context element is associated with a content function that encodes how query content is transformed based on the content and a location function that encodes how query content is transformed based on the content and associated location (i.e., query location and context location). In addition, the function messages are averaged over all elements in the context, resulting in the desired lambda function.
In an example, the computing device applies the one or more generated lambda functions to the input data as part of generating a layer output for the lambda layer. For example, the input data may be used to generate one or more queries. One or more lambda functions may then be applied to the queries such that each query interacts with each content location based on the content and query location/context location coordinates to capture long-range content and location-based interactions between the query and a context without an attention map.
The example embodiments described herein are particularly, but not exclusively, applicable to image processing. In some cases, the machine learning model is configured to perform image processing tasks, i.e., receive an input image and process the input image, i.e., process intensity values of pixels of the input image, to generate a model output for the input image. For example, the task may be an image classification, and the model output for a given image may be a score for each object class in a set of object classes, each score representing an estimated likelihood that the image contains an image of an object belonging to that class.
Other aspects of the disclosure relate to various systems, apparatuses, non-transitory computer-readable media, user interfaces, cloud services, and electronic devices.
These and other features, aspects, and advantages of various examples of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the relevant principles.
Drawings
A detailed discussion of embodiments involving one of ordinary skill in the art is set forth in the specification, which makes reference to the appended figures, wherein:
FIG. 1 depicts a flowchart of an example method of performing modeling of long-range interactions with reduced feature materialization according to an example of the present disclosure.
FIG. 2A depicts a block diagram of an example computing system that performs modeling of long-range interactions with reduced feature materialization, according to an example of the present disclosure.
Fig. 2B depicts a block diagram of an example computing device that performs modeling of long-range interactions with reduced feature materialization, according to an example of the present disclosure.
Fig. 2C depicts a block diagram of an example computing device that performs modeling of long-range interactions with reduced feature materialization, according to an example of the present disclosure.
Detailed Description
SUMMARY
More particularly, the present disclosure relates to systems, methods, and computer program products for modeling that perform long-range interactions with reduced feature materialization using machine learning models. Examples described in this disclosure enable dense, long-range content and location-based interactions without materializing per-example attention diagrams and without the associated per-example secondary memory complexity. Thus, examples of the present disclosure provide improved performance and reduced computational requirements compared to methods that operate based on attention.
While attention has been the preferred way to capture long-range interactions, attention manipulation is affected by the complexity of secondary memory per example due to attention-seeking. For example, applying a single multi-headed attention layer to a batch of 256 sequences of length 2048 with 8 heads requires 8GB of memory, which is difficult to withstand in practice. Further, the large memory requirements for self-attention have prevented the use of attention operations in long sequences and multi-dimensional inputs (such as images), which typically include tens of thousands of pixels.
To address these issues, the present disclosure provides an example of lambda function generation as an alternative to attentive operation. In examples of the present disclosure, lambda function generation enables dense, long-range content and location-based interactions without materializing an attention map per example. For example, lambda function generation transforms the available context into individual lambda functions that are applied directly to each query without an attention effort, thereby significantly improving performance and reducing computational requirements.
lambda networks are a new class of neural networks described in examples of the present disclosure that are capable of modeling long range dependencies with efficient computation and reduced memory requirements compared to alternative operations. Thus, the lambda function, lambda layer and lambda network can be conventionally applied to very large inputs such as long sequences and high resolution pictures.
U.S. provisional patent application No. 63/051,969 contains a summary of example embodiments and example experimental results of the present disclosure, with examples of the present disclosure being superior to convolution and attention counterparts in terms of accuracy and efficiency. For example, in experiments, examples of the present disclosure improve various image classification and object detection models (e.g., resNet, retinaNet).
The systems, methods, and computer program products described herein provide several technical effects and benefits. As one example, the lambda functions, lambda layers, and lambda networks described in this disclosure provide modeling of long-range dependencies more quickly and use less computing resources (e.g., less processing power, less memory usage, less power consumption, etc.) than, for example, attentional operation.
As another example of technical effects and benefits, the lambda layer uses lambda functions generated from available contexts, which are applied directly to queries, thus capturing long-range dependencies at significantly reduced memory costs compared to attention-force maps. Furthermore, lambda networks employing lambda layers provide significant performance improvements and computational efficiency compared to convolution and attention techniques. In addition, lambda networks provide increased accuracy via fewer layers of computation.
Referring now to the drawings, examples of the present disclosure will be discussed in further detail.
Example method for modeling to perform long-range interactions with reduced feature materialization
FIG. 1 depicts a flowchart of an example method 100 of performing modeling of long-range interactions with reduced feature materialization in accordance with an example of the present disclosure. Although fig. 1 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the specifically illustrated order or arrangement.
The various steps of the method 100 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure. Further, the operations and features described with respect to fig. 1 may also be performed by one or more computing devices of a computing system and/or by one or more processing devices executing computer-readable instructions provided via a non-transitory computer-readable medium.
In some examples, a machine learning model associated with one or more computing devices is configured to receive model inputs, process the model inputs, and generate model outputs, e.g., based on the operations of example method 100 and features described throughout this disclosure. Thus, in some examples, a machine learning model may be operable, by itself or through another associated machine learning model, to perform modeling of long-range interactions with reduced feature materialization as described herein.
The method 100 begins at block 102 when a computing system receives a layer input that includes input data and context data. For example, a computing system may receive, collect, or obtain input data and context data together, separately, at one time, and/or at different times. The input data and context data can then be provided to one or more models or their specific lambda layers to perform modeling of long-range interactions with reduced feature materialization.
In an example, the input data can be expressed as
In an example, the contextual data can be expressed as
At block 104, the computing system generates one or more lambda functions based on the content function and the location function for each of the plurality of context elements in the context data. In an example, a lambda layer of a machine learning model receives layer input comprising input data and context data. For example, a lambda layer can accept input
In general, a lambda layer refers to a model layer that transforms a context into a lambda function that is applied to input data or data derived therefrom, such as a query generated from the input data. In some examples, the lambda function may be a linear function applied to the input data. A lambda network generally refers to a neural network that includes one or more lambda layers that dynamically generate its own computations based on its inputs. lambda function generation may include generating lambda functions from global or local scopes (e.g., global lambda functions, local lambda functions). A content lambda function generally refers to a lambda function that models content-based interactions. Location lambda generally refers to a lambda function that models location-based interactions.
As one example process for generating the lambda function at block 104, the lambda layer can first use the key tensors W, respectively K Sum tensor W V The context data is transformed into a key K and a value V. As an example of this, it is possible to use,
In some implementations, the keys are normalized across contextual locations based on a softmax function. In some implementations, the lambda layer utilizes functional messaging, where each context element is functional with content
In an example, the function messages are averaged over elements in the context, generating the expected
lambda function lambda n As
In the above example, the hyper-parameters of the example lambda layer may include key/query depth | k |, value depth | v |, and internal depth | u |. The internal depth | u | can be adjusted to provide additional dimensionality and parameterization so that more complex relationships can be learned. In an example, the parameters of the lambda layer can include linear projection inputs
At block 106, the computing system applies the one or more generated lambda functions to the input data as part of generating a layer output for the respective lambda layer. In an example, a computing system transforms input data into one or more queries (e.g., q) n ＝W Q x n ). For example, input data may be transformed into one or more queries. Further, each of the one or more generated lambda functions may be applied to each of the one or more queries. One or more queries can generally be generated from input data at any time.
In an example, the computing system applies the one or more generated lambda functions to each of the respective queries as part of generating a layer output for the respective lambda layer. For example, a lambda function output associated with a layer output of a lambda function can be based on
An example complexity analysis of the above example is as follows. For a batch of | b | elements, each containing | n | inputs, the number of arithmetic operations and memory footprint required to apply our lambda layer are Θ (bnmkv) and Θ (bnkv + knm), respectively. Despite E due to capture of location-based interactions nm Parameters, there is still a secondary memory footprint with respect to the input length, but the secondary term does not scale with batch size as would be the case for the attention operation that produces each example attention map. In some examples, the hyperparameter | k | can be set to a small value (e.g., | k | = 8), and large batches of large inputs may be processed without attention.
In an example, the lambda layer performs multi-query operations as part of generating layer outputs, e.g., to mitigate large computational costs associated with large output dimensions. In an example, a multi-query operation may be used to generate an output by dividing a query into a plurality of queries. For example, the same lambda function can then be applied to each of the plurality of queries, and the resulting outputs of applying the lambda function to each of the queries can be concatenated or otherwise combined to generate the layer output.
In particular, recall that the example lambda layer discussed above with reference to FIG. 1 will be input
Example embodiments of the present disclosure decouple the temporal and spatial complexity of the example lambda layer from the output dimension d. In particular, in some implementations, | h | queries
This operation can be referred to as a multi-query lambda layer because each lambda is applied to the | h | query. While this is similar to a multi-head or multi-query attention formula, the motivation is different: in this case, the use of multiple query lambda reduces complexity and representative power. This can also be interpreted as forcing the lambda matrix to be arranged in blocks, | h | being equal repeated blocks. We now let d = | hv |, and the temporal and spatial complexity becomes Θ (bnmkd/h) and Θ (bnkd/h + knm).
In an example, one or more lambda layers can be configured to operate with a structured context (such as a relative context) that enables translation and other variability that provides strong inductive biases in many learning scenarios. For example, panning an equal shift position may be based on ensuring that the position embeds E that satisfies any panning t nm ＝E t(n)t(m) To be determined. In addition, relative position embedding can be defined as
In an example, one or more lambda layers can be configured to generate a location lambda function for a local context based on a regular convolution that considers a value V dimension in V as an additional spatial dimension. Thus, the computation can be limited to a local range, where the lambda convolution yields linear time and memory based on complexity relative to the input length. These embodiments are easy to use with additional functionality such as expansion and striding, and enjoy highly optimized implementations on dedicated hardware accelerators. This is in stark contrast to local self-attention implementations, which require materializing feature patches that overlap queries and memory blocks, thereby increasing memory consumption and latency.
In an example, one or more lambda layers can be configured to operate in association with a masked context. For example, when generating a lambda function based on applying a mask before summing contributions to a context location, interaction between the query and the masked context location may be limited or prevented. In addition, the keys can be normalized based on considering the elements in view of their context.
Table 1: the hyper-parameters, parameters and quantities of interest of an example lambda layer are described.
Example apparatus and System
FIG. 2A depicts a block diagram of an example computing system 200 that executes modeling of long-range interactions with reduced feature materialization, according to an example of the present disclosure. The system 200 includes a user computing device 202, a server computing system 230, and a training computing system 250 communicatively coupled by a network 280.
The user computing device 202 can be any type of computing device, such as, for example, a personal computing device (e.g., a laptop or desktop computer), a mobile computing device (e.g., a smartphone or tablet computer), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 202 includes one or more processors 212 and memory 214. The one or more processors 212 can be any suitable processing device (e.g., processor cores, microprocessors, ASICs, FPGAs, controllers, microcontrollers, etc.) and can be one processor or a plurality of processors operatively connected. The memory 214 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, as well as combinations thereof. The memory 214 is capable of storing data 216 and instructions 218, the instructions 218 being executable by the processor 212 to cause the user computing device 202 to perform operations.
In some examples, the user computing device 202 can store or include one or more machine learning models 220. For example, the machine learning model 220 can be or can otherwise include various machine learning models, such as a neural network (e.g., a deep neural network) or other types of machine learning models, including non-linear models and/or linear models. The neural network can include a feed-forward neural network, a recurrent neural network (e.g., a long-short term memory recurrent neural network), a convolutional neural network, or other form of neural network.
In some examples, the one or more machine learning models 220 can be received from the server computing system 230 over the network 280, stored in the user computing device memory 214, and then used or otherwise implemented by the one or more processors 212. In some examples, the user computing device 202 is capable of implementing multiple parallel instances of a single machine learning model 220.
Additionally or alternatively, one or more machine learning models 240 can be included or otherwise stored in the server computing system 230 and implemented by the server computing system 230, the server computing system 230 communicating with the user computing device 202 according to a client-server relationship. For example, the machine learning model 240 can be implemented by the server computing system 240 as part of a web service (e.g., a cloud-based machine learning platform service). Thus, one or more models 220 can be stored and implemented at the user computing device 202 and/or one or more models 240 can be stored and implemented at the server computing system 230.
The user computing device 202 can also include one or more user input components 222 that receive user input. For example, the user input component 222 can be a touch sensitive component (e.g., a touch display screen or a touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component can be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other components by which a user can provide user input.
The server computing system 230 includes one or more processors 232 and memory 234. The one or more processors 232 can be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and can be one processor or a plurality of processors operatively connected. The memory 234 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, as well as combinations thereof. The memory 234 is capable of storing data 236 and instructions 238, the instructions 238 being executed by the processor 232 to cause the server computing system 230 to perform operations.
In some examples, server computing system 230 includes or is otherwise implemented by one or more server computing devices. In examples where server computing system 230 includes multiple server computing devices, such server computing devices can operate according to a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 230 can store or otherwise include one or more machine learning models 240. For example, the model 240 can be or can otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layer non-linear models. Example neural networks include feed-forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks.
The user computing device 202 and/or the server computing system 230 can train the machine learning models 220 and/or 240 via interaction with a training computing system 250, the training computing system 250 being communicatively coupled by a network 280. The training computing system 250 can be separate from the server computing system 230 or can be part of the server computing system 230.
The training computing system 250 can include a model trainer 260 that uses various training or learning techniques, such as, for example, back propagation of errors, to train the machine learning models 220 and/or 240 stored at the user computing device 202 and/or the server computing system 230. For example, the loss function can be propagated back through the model(s) to update one or more parameters of the model(s) (e.g., based on the gradient of the loss function). Various loss functions can be used, such as mean square error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. A gradient descent technique can be used to iteratively update the parameters through several training iterations.
In some examples, performing back-propagation of the error can include performing truncated back-propagation over time. The model trainer 260 can perform several generalization techniques (e.g., weight decay, regression, etc.) to improve the generalization capability of the trained model. Additionally, the model trainer 260 is capable of training a model based on a training data set 262. Training data 262 can include, for example, image data, text data, audio data, video data, and/or other forms and types of data.
In some examples, the training examples can be provided by the user computing device 202 if the user has provided consent. Thus, in such an example, the model 220 provided to the user computing device 202 can be trained by the training computing system 250 on user-specific data received from the user computing device 202. In some examples, this process can be referred to as a personalization model.
The model trainer 260 includes computer logic for providing the desired functionality. The model trainer 260 can be implemented in hardware, firmware, and/or software controlling a general purpose processor. For example, in some instances, model trainer 260 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other examples, model trainer 260 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium (such as a RAM hard disk or an optical or magnetic medium).
The network 280 can be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), or some combination thereof, and can include any number of wired or wireless links. In general, communications over network 280 can be performed using various communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML) and/or protection schemes (e.g., VPN, secure HTTP, SSL) via any type of wired and/or wireless connection.
Fig. 2A illustrates one example computing system that can be used to implement various examples provided in this disclosure. Other computing systems can also be used. For example, in some instances, the user computing device 202 can include a model trainer 260 and a training data set 262. In such an example, the machine learning model 220 can be trained and used locally at the user computing device 202. In some of such examples, the user computing device 202 can implement the model trainer 260 to personalize the machine learning model 220 based on user-specific data.
In some examples, the input to the machine learning model(s) of the present disclosure (e.g., machine learning model 220, machine learning model 240) can be image data. The machine learning model(s) can process the image data to generate an output. As an example, the machine learning model(s) can process the image data to generate an image recognition output (e.g., an identification of the image data, a potential embedding of the image data, an encoded representation of the image data, a hash of the image data, etc.). As another example, the machine learning model(s) can process image data to generate an image segmentation output. As another example, the machine learning model(s) can process image data to generate an image classification output. As another example, the machine learning model(s) can process the image data to generate an image data modification output (e.g., a change in the image data, etc.). As another example, the machine learning model(s) can process the image data to generate an encoded image data output (e.g., an encoded and/or compressed representation of the image data, etc.). As another example, the machine learning model(s) can process the image data to generate an enlarged image data output. As another example, the machine learning model(s) can process image data to generate a prediction output.
In some examples, the input to the machine learning model(s) of the present disclosure can be text or natural language data. The machine learning model(s) can process textual or natural language data to generate an output. As an example, the machine learning model(s) can process natural language data to generate a language encoded output. As another example, the machine learning model(s) can process text or natural language data to generate potential text-embedded output. As another example, the machine learning model(s) can process text or natural language data to generate translation output. As another example, the machine learning model(s) can process text or natural language data to generate a classification output. As another example, the machine learning model(s) can process text or natural language data to generate a text segmentation output. As another example, the machine learning model(s) can process text or natural language data to generate a semantic intent output. As another example, the machine learning model(s) can process text or natural language data to generate an amplified text or natural language output (e.g., text or natural language data that is higher quality than the input text or natural language, etc.). As another example, the machine learning model(s) can process textual or natural language data to generate a prediction output.
In some examples, the input to the machine learning model(s) of the present disclosure can be speech data. The machine learning model(s) can process the speech data to generate an output. As an example, the machine learning model(s) can process speech data to generate a speech recognition output. As another example, the machine learning model(s) can process speech data to generate speech translation output. As another example, the machine learning model(s) can process speech data to generate potential embedded output. As another example, the machine learning model(s) can process speech data to generate an encoded speech output (e.g., an encoded and/or compressed representation of the speech data, etc.). As another example, the machine learning model(s) can process speech data to generate an amplified speech output (e.g., speech data that is of higher quality than the input speech data, etc.). As another example, the machine learning model(s) can process speech data to generate a textual representation output (e.g., a textual representation of the input speech data, etc.). As another example, the machine learning model(s) can process speech data to generate a prediction output.
In some examples, the input to the machine learning model(s) of the present disclosure can be potentially encoded data (e.g., a potential spatial representation of the input, etc.). The machine learning model(s) can process the latent encoding data to generate an output. As an example, the machine learning model(s) can process the potentially encoded data to generate the recognition output. As another example, the machine learning model(s) can process the potentially encoded data to generate a reconstructed output. As another example, the machine learning model(s) can process the potentially encoded data to generate a search output. As another example, the machine learning model(s) can process the potential encoding data to generate a re-clustering output. As another example, the machine learning model(s) can process the potentially encoded data to generate a prediction output.
In some examples, the input to the machine learning model(s) of the present disclosure can be statistical data. The machine learning model(s) can process the statistics to generate an output. As an example, the machine learning model(s) can process the statistics to generate recognition output. As another example, the machine learning model(s) can process the statistics to generate a prediction output. As another example, the machine learning model(s) can process the statistics to generate a classification output. As another example, the machine learning model(s) can process the statistics to generate a segmented output. As another example, the machine learning model(s) can process the statistics to generate a visual output. As another example, the machine learning model(s) can process the statistics to generate a diagnostic output.
In some examples, the input to the machine learning model(s) of the present disclosure can be sensor data. The machine learning model(s) can process the sensor data to generate an output. As an example, the machine learning model(s) can process sensor data to generate recognition output. As another example, the machine learning model(s) can process sensor data to generate a predicted output. As another example, the machine learning model(s) can process sensor data to generate classification outputs. As another example, the machine learning model(s) can process sensor data to generate a segmented output. As another example, the machine learning model(s) can process sensor data to generate a visual output. As another example, the machine learning model(s) can process sensor data to generate a diagnostic output. As another example, the machine learning model(s) can process sensor data to generate a detection output.
Fig. 2B depicts a block diagram of an example computing device 270, the example computing device 270 performing modeling of long-range interactions with reduced feature materialization, in accordance with an example of the present disclosure. Computing device 270 can be a user computing device (e.g., user computing device 202) or a server computing device (e.g., server computing system 230).
As illustrated in fig. 2B, each application is capable of communicating with several other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some examples, each application can communicate with each device component using an Application Programming Interface (API) (e.g., public API, private API, secure open API, web API, etc.). In some examples, the APIs used by each application are specific to that application.
FIG. 2C depicts a block diagram of an example computing device 280, the example computing device 280 performing modeling of long-range interactions with reduced feature materialization, in accordance with an example of the present disclosure. Computing device 280 can be a user computing device (e.g., user computing device 202) or a server computing device (e.g., server computing system 230).
The central smart tier includes a number of machine learning models. For example, as illustrated in fig. 2C, a respective machine learning model (e.g., model) can be provided for each application and managed by the central intelligence layer. In other examples, two or more applications can share a single machine learning model. For example, in some examples, the central smart inlay can provide a single model (e.g., a single model) for all applications. In some examples, the central smart inlay is included within or otherwise implemented by the operating system of the computing device 280.
The central smart inlay is capable of communicating with the central device data plane. The central device data layer can be a centralized repository for data of the computing device 280. As illustrated in fig. 2C, the central device data layer can communicate with several other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some examples, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
Additional disclosure
The techniques discussed herein make reference to servers, databases, software applications, and other computer-based systems and actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functionality between components. For example, the processes discussed herein can be implemented using a single device or component or multiple devices or components operating in combination. Databases and applications can be implemented on a single system or distributed among multiple systems. The distributed components can operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Modifications, variations and equivalents of such embodiments may readily occur to those skilled in the art upon a reading of the foregoing description. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, the present disclosure is intended to cover such modifications, alterations, and equivalents.
Claims (14)
1. A computing system for modeling long range interactions with reduced feature materialization, comprising:
one or more processors; and
one or more non-transitory computer-readable media collectively storing:
a machine learning model configured to receive a model input and process the model input to generate a model output, wherein the machine learning model comprises one or more lambda layers, wherein each lambda layer of the one or more lambda layers is configured to perform operations comprising:
receiving a layer input comprising input data and contextual data, the contextual data comprising a plurality of contextual elements;
generating one or more lambda functions based at least in part on the content function and the location function for each of the plurality of context elements in the context data; and
applying one or more generated lambda functions to the input data as part of generating a layer output associated with a respective lambda layer.
2. The computing system of claim 1, wherein generating the one or more lambda functions comprises:
averaging the content function and the location function of the plurality of context elements.
3. The computing system of any preceding claim, wherein the operations further comprise:
determining a key sum value based on linearly projecting the context data.
4. The computing system of any preceding claim, wherein each respective content function encodes a transform of query content based on the context data, regardless of a target query location.
5. The computing system of any preceding claim, wherein each respective location function encodes a transform of query content based on the context data, a query location, and a location in the context data.
6. The computer system of any preceding claim, wherein pan-equal position interaction is determined based on a plurality of query locations and relative locations of one or more pairs of the plurality of locations in the contextual data.
7. The computing system of any preceding claim, wherein the operations further comprise:
transforming the input data into one or more queries, wherein applying the one or more generated lambda functions to the input data comprises: applying at least one of the generated lambda functions to each of the one or more queries.
8. The computing system of any preceding claim, wherein applying the one or more generated lambda functions to the input data comprises: combining a series of outputs resulting from applying at least one of the generated lambda functions to a plurality of queries associated with the input data.
9. The computer system of any preceding claim, wherein one or more of the lambda functions is a global lambda function.
10. The computer system of any preceding claim, wherein one or more of the lambda functions are local lambda functions.
11. The computer system of any preceding claim, wherein generating the one or more lambda functions comprises: masking one or more locations of the context data.
12. The computer system of any preceding claim, wherein the machine learning model is configured to perform image processing tasks, wherein the image processing tasks include image classification, object detection, image recognition, image segmentation, image data modification, image encoding, image compression, or image magnification.
13. A computer-implemented method for performing modeling of long-range interactions with reduced feature materialization in a machine learning model, the computer-implemented method being performed by one or more computing devices performing operations of the computing system of any one of claims 1 to 12.
14. One or more non-transitory computer-readable media storing one or both of:
instructions for performing the operations of any of claims 1 to 12; and/or
A model generated by performing the operations of any of claims 1 to 12.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063051969P | 2020-07-15 | 2020-07-15 | |
US63/051,969 | 2020-07-15 | ||
PCT/US2021/040664 WO2022015546A1 (en) | 2020-07-15 | 2021-07-07 | Modeling of long-range interactions with reduced feature materialization via lambda functions |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115769236A true CN115769236A (en) | 2023-03-07 |
Family
ID=77155894
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180045698.7A Pending CN115769236A (en) | 2020-07-15 | 2021-07-07 | Modeling long-range interactions with reduced feature materialization via lambda functions |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230229886A1 (en) |
EP (1) | EP4150529A1 (en) |
CN (1) | CN115769236A (en) |
WO (1) | WO2022015546A1 (en) |
-
2021
- 2021-07-07 WO PCT/US2021/040664 patent/WO2022015546A1/en unknown
- 2021-07-07 CN CN202180045698.7A patent/CN115769236A/en active Pending
- 2021-07-07 EP EP21749037.4A patent/EP4150529A1/en active Pending
- 2021-07-07 US US18/011,636 patent/US20230229886A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230229886A1 (en) | 2023-07-20 |
EP4150529A1 (en) | 2023-03-22 |
WO2022015546A1 (en) | 2022-01-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7423828B2 (en) | Attention-based sequence transformation neural network | |
US11886998B2 (en) | Attention-based decoder-only sequence transduction neural networks | |
US20230359865A1 (en) | Modeling Dependencies with Global Self-Attention Neural Networks | |
KR20210029785A (en) | Neural network acceleration and embedding compression system and method including activation sparse | |
JP2021521505A (en) | Application development platform and software development kit that provides comprehensive machine learning services | |
CN114175018A (en) | New word classification technique | |
KR102561799B1 (en) | Method and system for predicting latency of deep learning model in device | |
CN115769236A (en) | Modeling long-range interactions with reduced feature materialization via lambda functions | |
US11755883B2 (en) | Systems and methods for machine-learned models having convolution and attention | |
US20220245917A1 (en) | Systems and methods for nearest-neighbor prediction based machine learned models | |
US20240135187A1 (en) | Method for Training Large Language Models to Perform Query Intent Classification | |
US20210256388A1 (en) | Machine-Learned Models Featuring Matrix Exponentiation Layers | |
US20220245428A1 (en) | Machine-Learned Attention Models Featuring Omnidirectional Processing | |
JP2023527511A (en) | A Cross-Transformer Neural Network System for Few-Shot Similarity Determination and Classification | |
WO2023064158A1 (en) | Self-adapting forecasting for multi-horizon forecasting machine learning models | |
CN115803753A (en) | Multi-stage machine learning model synthesis for efficient reasoning |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |