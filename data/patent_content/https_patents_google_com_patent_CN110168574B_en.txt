CN110168574B - Unsupervised detection of intermediate reinforcement learning targets - Google Patents
Unsupervised detection of intermediate reinforcement learning targets Download PDFInfo
- Publication number
- CN110168574B CN110168574B CN201780074215.XA CN201780074215A CN110168574B CN 110168574 B CN110168574 B CN 110168574B CN 201780074215 A CN201780074215 A CN 201780074215A CN 110168574 B CN110168574 B CN 110168574B
- Authority
- CN
- China
- Prior art keywords
- feature
- image
- subtask
- feature values
- reinforcement learning
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/004—Artificial life, i.e. computing arrangements simulating life
- G06N3/006—Artificial life, i.e. computing arrangements simulating life based on simulated virtual individual or collective life forms, e.g. social simulations or particle swarm optimisation [PSO]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/211—Selection of the most significant subset of features
- G06F18/2111—Selection of the most significant subset of features by using evolutionary computational techniques, e.g. genetic algorithms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/213—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods
- G06F18/2132—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods based on discrimination criteria, e.g. discriminant analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/217—Validation; Performance evaluation; Active pattern learning techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/092—Reinforcement learning
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for detecting intermediate reinforcement learning objectives. One of the methods comprises: obtaining a plurality of presentation sequences, each presentation sequence being a sequence of images of the environment while a respective instance of the reinforcement learning task is being performed; for each presentation sequence, processing each image in the presentation sequence by an image processing neural network to determine a feature value of a corresponding feature set of the image; determining to partition the reinforcement learning task into a plurality of subtasks according to the presentation sequences, wherein each image in each presentation sequence is assigned to a respective subtask of the plurality of subtasks; and determining a respective set of distinguishing features for each of the plurality of subtasks based on the feature values of the images in the presentation sequence.
Description
Technical Field
The present description relates to reinforcement learning.
Background
In the reinforcement learning system, the agent interacts with the environment by performing actions selected by the reinforcement learning system in response to receiving observations characterizing a current state of the environment.
Some reinforcement learning systems select actions to be performed by an agent in response to receiving a given observation from the output of the neural network.
Neural networks are machine learning models that employ one or more layers of nonlinear cells to predict output for a received input. Some neural networks are deep neural networks that include one or more hidden layers in addition to an output layer. The output of each hidden layer serves as an input to the next layer in the network, i.e., the next hidden layer or output layer. Each layer of the network generates an output from the received inputs according to the current values of the respective parameter sets.
Disclosure of Invention
This specification generally describes how a system implemented as one or more computers in one or more locations processes a presentation sequence to determine features that distinguish for each of a plurality of subtasks of a reinforcement learning task to be performed by an agent interacting with an environment. The system may then use the distinguishing features to generate rewards to train the agent to perform the reinforcement learning task.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The described system may partition reinforcement learning tasks into subtasks and thereby detect intermediate targets in an unsupervised manner, i.e., directly from a video or other image sequence of a presenter performing the task without any explicit designation of the subtasks. More specifically, the system may determine the partitions from a very small number of task presentations, for example, from less than 20 presentation sequences. The system may then use the generated partitions to generate rewards functions for training the reinforcement learning agent to perform tasks. Thus, using the described techniques, dense and smooth reward functions may be generated in an unsupervised manner and used to effectively train reinforcement learning agents to perform complex tasks from only a small number of unlabeled presentation images. Alternatively, the reward function may be used to augment existing reward signals, such as manually designed signals, to speed training of the reinforcement learning agent, to improve the performance of the trained reinforcement learning agent, or both. Because a given reinforcement learning task may have implicit sub-goals and steps involving more complex behavior, extracting these sub-goals described in this specification may allow an agent to maximize use of the information contained in the presentation during training without requiring expensive and often infeasible labeling of presentation data.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example reinforcement learning system.
FIG. 2 is a flow chart of an example process for partitioning reinforcement learning tasks into subtasks.
FIG. 3 is a flow chart of an example process for generating rewards.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This specification generally describes how a system implemented as one or more computers in one or more locations uses a presentation sequence to partition reinforcement learning tasks performed by an agent into sub-tasks, determine features that distinguish each sub-task, and then use those distinguishing features during training of the agent to generate rewards.
FIG. 1 illustrates an example reinforcement learning system 100. Reinforcement learning system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations in which the systems, components, and techniques described below may be implemented.
The reinforcement learning system 100 is a system that trains reinforcement learning agents 102 to interact with the environment 106 to perform reinforcement learning tasks using presentation data 130 of the tasks performed by the presenter.
In general, environment 106 is a real-world environment and reinforcement learning agent 102 is a mechanical agent that interacts with environment 102. For example, the agent 102 may be a robot 102 that interacts with the environment to perform reinforcement learning tasks. Examples of tasks that the agent 102 may perform include moving a specified object from one location to another location in the environment, locating a specified object in the environment, or navigating from one location to another location in the environment.
The reinforcement learning system 100 trains the reinforcement learning agent 102 by: the policies for controlling the agent 102 are trained, i.e., policies defining actions to be performed by the agent 102 when the agent 102 attempts to perform reinforcement learning tasks.
For example, the policy may be a neural network that receives observations characterizing the state of the environment, e.g., images of the environment captured by the proxy 102, and generates a policy output defining the actions to be performed by the proxy 102 in response to the observations, e.g., generates a policy output for parameters of the distribution of possible joint torques of the joints of the proxy 102.
As another example, the strategy may be a simpler controller, such as a linear gaussian parameterization of the strategy, that takes as input a lower dimensional state representation of, for example, joint angles and angular velocities of the proxy 102, and outputs parameters for the distribution of possible joint torques of the proxy 102.
In particular, the presentation data 130 used by the system 100 to train the agent 102 includes a plurality of presentation sequences. A presentation sequence is a sequence of images of an environment captured when a presenter (e.g., another mechanical agent or a human presenter) performs an instance of a reinforcement learning task.
To train the agent 102 using the presentation sequence, the system 100 processes each image in the presentation sequence through the image processing neural network 140 to determine feature values 142 for the respective feature set of the image.
In general, the image processing neural network 140 is a neural network that has been pre-trained on image processing tasks, such as object detection or image classification tasks, without further modification of the values of the parameters of the image processing neural network 140. For example, the image processing neural network 140 may be an acceptance image classification neural network that has been pre-trained on an image classification dataset. Such a neural network is described in more detail below: christian Szegedy, vincent Vanhoucke, sergey Ioffe, jonathon shelves and Zbigniew Wojna, rethinking the inception architecture for computer vision, CVPR, 2016.
For each image, the system 100 determines a feature value of the image from the activations generated by one or more hidden layers of the network 140 during processing of the image. For example, the feature value may be an output of a predetermined one of the hidden layers of the neural network 140. As another example, the feature values may include the output of multiple hidden layers of the neural network 140 (e.g., each layer after a predetermined layer in the network).
The task partitioning engine 150 determines to partition the reinforcement learning task into subtasks based on the feature values 142 of the images in the presentation sequence such that each image in each presentation sequence is assigned to a respective subtask. Thus, completing the reinforcement learning task may be broken down into achieving multiple intermediate goals, i.e., completing each sub-task in the partition. The task partition engine 150 may make this determination based on the images in the presentation sequence without any external marking of the subtasks, i.e., in an unsupervised manner. Partitioning reinforcement learning tasks into subtasks using presentation images is described in more detail below with reference to fig. 2.
The task partition engine 150 then determines a corresponding set of distinguishing features for each subtask based on the feature values of the images in the presentation sequence. The distinguishing features of a given subtask are highly distinguishing features of the particular object to be achieved in that subtask (i.e., relative to other subtasks in the partition) while maintaining unchanged features for uncorrelated changes (e.g., illumination, color, and view). Determining distinguishing features for each subtask is described in more detail below with reference to fig. 2.
Once the task partition engine 150 determines the distinguishing characteristics of each subtask, the rewards engine 160 may use this information to generate rewards for the agents 102 based on the characteristic values of the distinguishing characteristics for the images of the environment 160 generated during training of the agents 102, i.e., reflecting the progress of the agents in completing one or more subtasks. The reward may then be used to improve the training of the agent, i.e., to speed up training, to improve the performance of the task by the agent after training, or both.
In particular, the training engine 170 trains the agent 102, i.e., training strategies, using reinforcement learning techniques. In reinforcement learning, a strategy is learned that, when used to select actions to be performed by an agent, maximizes a measure of the jackpot received by the agent as a result of interacting with the environment. For example, the metric may be a time discount sum of rewards received during execution of the task. Typically, rewards are numerical values that represent the extent to which an agent performs a given task and are used in reinforcement learning techniques to adjust policies to improve the performance of the task by the agent.
The training engine 170 may train the agent 102 using any suitable reward-based reinforcement learning technique. For example, when the policy is a deep neural network, the engine may use a Deep Deterministic Policy Gradient (DDPG) technique. Such techniques are described in detail in Continuous control with Deep Reinforcement Learning (continuous control with deep reinforcement learning) in ICLR 2016, lillicrap, timothy et al. As another example, when the policy is a simpler controller, the engine may use a path integration based technique. Examples of such techniques are described in yevogen Chebotar, mrinal Kalakrishnan, ali Yahya, adrian Li, stefan Schaal and Sergey Levine, path integral guided policy search (path integral guide strategy search).
In some cases, the rewards generated by the rewards engine 160 are the only rewards that the training engine 170 uses to train the agent 102 using reinforcement learning techniques. In other cases, the training engine 170 may also receive other rewards from external sources and combine rewards generated by the reward engine 160 with external rewards, e.g., as a weighted average or weighted sum, and use the total rewards as rewards for reinforcement learning techniques.
FIG. 2 is a flow diagram of an example process 200 for partitioning reinforcement learning tasks into subtasks. For convenience, process 200 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed reinforcement learning system, such as reinforcement learning system 100 of fig. 1, may perform process 200.
The system obtains a plurality of presentation sequences (step 202). As described above, each presentation sequence is a sequence of images of a presenter that performs a reinforcement learning task that the agent is to be trained to perform. In some cases, certain characteristics of the tasks performed by one or more demonstrators differ between various presentation sequences and from the tasks that the agent will be required to perform. For example, the initial state of a task may differ between some or all of the presentation sequences.
The system processes each image in each presentation sequence through the image processing neural network to determine a feature value for a corresponding feature set of the image for each image (step 204). In particular, the system determines a feature value for a given image from the activation generated by one or more hidden layers of the image processing neural network. For example, the characteristic value may be the activation of one predetermined hidden layer of the neural network or the activation of a plurality of predetermined hidden layers of the neural network.
The system determines to partition the enhanced task into a plurality of subtasks (step 206), wherein each image in each presentation sequence is assigned to a respective subtask of the plurality of subtasks.
Typically, the system partition reinforcement learning task such that each image in each partition is abstractly similar to every other image in the partition. In some cases, the system achieves this by selecting a partition that (approximately) minimizes the mean eigenvalue variance between images within each partition.
In some embodiments, the system selects the partition that minimizes the average eigenvalue variance by recursively adjusting the partition to determine the partition that minimizes the average eigenvalue variance and meets certain criteria starting with a predetermined minimum subtask size, e.g., each subtask includes at least a predetermined minimum number of images and the total number of partitions is fixed to a predetermined number.
In some other embodiments, the system selects the partition that approximately minimizes the mean eigenvalue variance by first dividing the entire sequence into two as described above, and then recursively dividing each new sequence into two until the total number of partitions has been reached.
In some implementations, the system separately partitions each presentation sequence and then aligns the partitions prior to selection of the distinguishing features. In some other embodiments, the system partitions the presentation sequence jointly, wherein the eigenvalues in all presentation sequences are recursively considered.
The system determines a respective set of distinguishing features for each of a plurality of subtasks based on the feature values of the images in the presentation sequence (step 208). As described above, the distinguishing features of a given subtask are highly distinguishing features of the particular object to be achieved in the subtask, while maintaining unchanged features for irrelevant changes (e.g., illumination, color, and view).
In some implementations, to determine distinguishing features of a given subtask, the system processes the features using a classifier, i.e., a deep or shallow classifier, that is configured to generate an output identifying the distinguishing features of the given subtask.
In some other embodiments, the system first normalizes the feature values of the features for the images in the presentation sequence to determine a normalized feature value for the features of each image, i.e., by subtracting the average of the features of all the presentation images and dividing by the standard deviation of the features of all the presentation images.
The system then calculates for each subtask: distribution statistics, such as mean and standard deviation, of normalized feature values for features of the image that are not in subtasks; and the same distribution statistics for the normalized feature values of the features of the image in the subtasks.
For a given subtask, the system may then determine a score for each feature of the subtask based on the distribution statistics of the normalized feature values for the features of the image that are not in the subtask and the distribution statistics of the normalized feature values for the features of the image in the subtask. For example, the score z for feature i of a given subtask i Can satisfy the following conditions:
where alpha is a predetermined normal number,is the average feature value of the features of the image in the subtask,/->Is the average feature value of the features of the image not in the subtask, < >>Is the standard deviation of the feature values of the features of the image in the subtask and +.>Is the standard deviation of the features of the image that are not in the subtask.
The system may then select a fixed number of highest scoring features for each subtask as the distinguishing features of the subtask.
FIG. 3 is a flow diagram of an example process 300 for generating rewards for reinforcement learning techniques. For convenience, process 300 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed reinforcement learning system, such as reinforcement learning system 100 of fig. 1, may perform process 300.
The system may repeat the process 300 during training of the augmented agent to provide rewards for training of the agent.
The system receives a current image characterizing a current state of the environment (step 302). Typically, an image is an image of an agent that performs a task during training of the agent.
The system processes the current image through an image processing neural network to determine a feature value of the current image (step 304). The system uses the same intermediate output of the image processing neural network to generate the feature values for the presentation image, i.e. uses the output of the same hidden layer or layers of the neural network as the feature values.
The system, for each of one or more subtasks, slavesThe feature values of the features of the task generate corresponding perception-based rewards (step 306). Typically, the rewards of a given subtask are based on how similar the feature values of the distinguishing features of the current image are to the feature values of the distinguishing features of the presentation image in the given subtask. In some implementations, the rewards for a given subtask are based on a comparison between the eigenvalues of the current image and an average of the eigenvalues of the presentation image in the subtask. For example, in some embodiments, the current image t sRewards R of subtask g of (2) g (s t ) The method meets the following conditions:
where n is the total number of subtasks, j ranges over M distinguishing features of subtask g, s ijt Is the feature value of the feature j of the current image,is the average value of the feature values of the feature j of the presentation image in the subtask, and +.>Is the standard deviation of the feature value of the feature j of the presentation image in the subtask.
In some other embodiments, the system processes feature values (for distinguishing features or for all features) of the current image using a classifier that has been trained to receive feature values and generate rewards for subtasks. That is, the system maintains a corresponding classifier for each subtask and processes the feature values through each classifier to determine rewards for each subtask. The classifier may be a linear layer that has been trained based on the presentation image to receive feature values and map the received feature values to a single bonus point.
The system provides one or more perceptually based rewards for training the agent to perform reinforcement learning tasks (step 308). In particular, because reinforcement learning techniques typically train agents using a single reward received at each training time step, the system combines perceived-based rewards and uses the combined rewards in training of agents, i.e., as a unique reward or in combination with other rewards derived from different sources. In some embodiments, the combined rewards partially rewards the intermediate steps but emphasizes subsequent rewards, i.e., because successful completion of subsequent subtasks is more indicative of successful completion of the task. For example, in some embodiments, the combined prize R (a) for a given set of characteristic values a satisfies:
where n is the total number of subtasks, R i (a) Is the reward for subtask i. In this example, the first subtask is ignored because it is assumed to be the initial start-up state in the presentation sequence.
The term "configured" is used in this specification in connection with systems and computer program components. A system for one or more computers configured to perform a particular operation or action means that the system has installed thereon software, firmware, hardware, or a combination thereof that in operation causes the system to perform the operation or action. For one or more computer programs configured to perform particular operations or actions, it is meant that the one or more programs include instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise a dedicated logic circuit, for example an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an operating environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, also known or described as a program, software application, app, module, software module, script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and the computer program may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, such as files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of data: the data need not be structured in any particular way or not at all, and it may be stored on one or more storage devices in one or more locations. Thus, for example, an index database may include multiple data sets, each of which may be organized and accessed differently.
Similarly, in this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer or on multiple computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers running one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, for example, dedicated logic circuits of an FPGA or ASIC, or a combination of dedicated logic circuits and one or more programmed computers.
A computer suitable for the operation of a computer program may be based on a general-purpose or special-purpose microprocessor or both, or any other kind of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, such devices are not required for a computer. In addition, the computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; and CD ROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device and a keyboard, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, a web page is sent to a web browser on a user device in response to a request received from the web browser. Further, the computer may interact with the user by sending a text message or other form of message to a personal device, such as a smart phone running a messaging application, and receiving a response message as a return from the user.
The data processing means for implementing the machine learning model may also comprise, for example, dedicated hardware accelerator units for handling public and computationally intensive parts of machine learning training or production, i.e. inference, workload.
The machine learning model can be implemented and deployed using a machine learning framework such as a TensorFlow framework, microsoft cognitive toolkit framework, apache Singa framework, or Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface, a Web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), such as the Internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data, e.g., HTML pages, to the user device, e.g., for the purpose of displaying data to and receiving user input from a user interacting with the device as a client. Data generated at the user device, e.g., results of a user interaction, may be received at the server from the device.
While this specification contains many specifics, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings and described in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. By way of example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (17)
1. A method for unsupervised detection of an intermediate reinforcement learning objective, comprising:
obtaining, by a computing system, a plurality of presentation sequences, each of the presentation sequences being a sequence of images of an environment while a respective instance of a reinforcement learning task is being performed;
for each presentation sequence, processing, by the computing system, each image in the presentation sequence through an image processing neural network comprising a plurality of hidden layers to determine feature values for a respective feature set of the image from activations generated by one or more of the hidden layers;
determining, by the computing system, to partition the reinforcement learning task into a plurality of subtasks according to the presentation sequences, wherein each image in each presentation sequence is assigned to a respective subtask of the plurality of subtasks;
determining, by the computing system, a respective set of distinguishing features for each of the plurality of subtasks based on the feature values of the images in the presentation sequence; and
during execution of the reinforcement learning task by a training agent:
receiving a current image characterizing a current state of the environment;
processing the current image through the image processing neural network to determine feature values of corresponding feature sets of the current image;
for each of one or more of the subtasks, generating a respective perception-based reward in accordance with a feature value of the current image for a distinguishing feature of that subtask; and
one or more of the perception-based rewards are provided for training the agent to perform the reinforcement learning task.
2. The method of claim 1, wherein determining a respective set of distinguishing features from the feature values of the images in the presentation sequence comprises, for each subtask:
the feature values of the images in the presentation sequence in the subtask are processed using a trained classifier to determine the distinguishing features of the task.
3. The method of claim 1, wherein determining a respective set of distinguishing features from the feature values of the images in the presentation sequence comprises, for each feature:
normalizing the feature values of the feature to the images in the presentation sequence to determine normalized feature values of the feature for each image;
for each subtask:
calculating distribution statistics of normalized feature values of the feature of the image not in the subtask;
calculating distribution statistics of normalized feature values of the feature of the image in the subtask; and
a score of the feature of the subtask is determined from the distribution statistics of the normalized feature values for the feature of the image not in the subtask and the distribution statistics of the normalized feature values for the feature of the image in the subtask.
4. A method according to claim 3, further comprising: for each subtask, a plurality of highest scoring features are selected as the distinguishing features for that subtask.
5. The method of claim 1, wherein determining to partition the reinforcement learning task into a plurality of subtasks according to the presentation sequence comprises:
partitioning the reinforcement learning task such that each image in each partition is abstractly similar to each other image in the partition.
6. The method of claim 1, wherein determining to partition the reinforcement learning task into a plurality of subtasks according to the presentation sequence comprises:
the partition that minimizes the mean eigenvalue variance between the images within each partition is selected.
7. The method of claim 6, wherein selecting the partition that minimizes the average eigenvalue variance between images within each partition comprises:
the partitions are recursively adjusted to determine the partitions that minimize the mean eigenvalue variance.
8. A system for unsupervised detection of an intermediate reinforcement learning objective, comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform operations comprising:
obtaining, by a computing system, a plurality of presentation sequences, each of the presentation sequences being a sequence of images of an environment while a respective instance of a reinforcement learning task is being performed;
for each presentation sequence, processing, by the computing system, each image in the presentation sequence through an image processing neural network comprising a plurality of hidden layers to determine feature values for a respective feature set of the image from activations generated by one or more of the hidden layers;
determining, by the computing system, to partition the reinforcement learning task into a plurality of subtasks according to the presentation sequences, wherein each image in each presentation sequence is assigned to a respective subtask of the plurality of subtasks;
determining, by the computing system, a respective set of distinguishing features for each of the plurality of subtasks based on the feature values of the images in the presentation sequence; and
during execution of the reinforcement learning task by a training agent:
receiving a current image characterizing a current state of the environment;
processing the current image through the image processing neural network to determine feature values of corresponding feature sets of the current image;
for each of one or more of the subtasks, generating a respective perception-based reward in accordance with a feature value of the current image for a distinguishing feature of that subtask; and
one or more of the perception-based rewards are provided for training the agent to perform the reinforcement learning task.
9. The system of claim 8, wherein determining a respective set of distinguishing features from the feature values of the images in the presentation sequence comprises, for each subtask:
the feature values of the images in the presentation sequence in the subtask are processed using a trained classifier to determine the distinguishing features of the task.
10. The system of claim 8, wherein determining a respective set of distinguishing features from the feature values of the images in the presentation sequence comprises, for each feature:
normalizing the feature values of the feature to the images in the presentation sequence to determine normalized feature values of the feature for each image;
for each subtask:
calculating distribution statistics of normalized feature values of the feature of the image not in the subtask;
calculating distribution statistics of normalized feature values of the feature of the image in the subtask; and
a score of the feature of the subtask is determined from the distribution statistics of the normalized feature values for the feature of the image not in the subtask and the distribution statistics of the normalized feature values for the feature of the image in the subtask.
11. The system of claim 10, the operations further comprising: for each subtask, a plurality of highest scoring features are selected as the distinguishing features for that subtask.
12. The system of claim 8, wherein determining to partition the reinforcement learning task into a plurality of subtasks according to the presentation sequence comprises:
partitioning the reinforcement learning task such that each image in each partition is abstractly similar to each other image in the partition.
13. The system of claim 8, wherein determining to partition the reinforcement learning task into a plurality of subtasks according to the presentation sequence comprises:
the partition that minimizes the mean eigenvalue variance between the images within each partition is selected.
14. The system of claim 13, wherein selecting the partition that minimizes the average eigenvalue variance between images within each partition comprises:
the partitions are recursively adjusted to determine the partitions that minimize the mean eigenvalue variance.
15. One or more non-transitory computer-readable storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:
obtaining, by a computing system, a plurality of presentation sequences, each of the presentation sequences being a sequence of images of an environment while a respective instance of a reinforcement learning task is being performed;
for each presentation sequence, processing, by the computing system, each image in the presentation sequence through an image processing neural network comprising a plurality of hidden layers to determine feature values for a respective feature set of the image from activations generated by one or more of the hidden layers;
determining, by the computing system, to partition the reinforcement learning task into a plurality of subtasks according to the presentation sequences, wherein each image in each presentation sequence is assigned to a respective subtask of the plurality of subtasks;
determining, by the computing system, a respective set of distinguishing features for each of the plurality of subtasks based on the feature values of the images in the presentation sequence; and
during execution of the reinforcement learning task by a training agent:
receiving a current image characterizing a current state of the environment;
processing the current image through the image processing neural network to determine feature values of corresponding feature sets of the current image;
for each of one or more of the subtasks, generating a respective perception-based reward in accordance with a feature value of the current image for a distinguishing feature of that subtask; and
one or more of the perception-based rewards are provided for training the agent to perform the reinforcement learning task.
16. The computer-readable storage medium of claim 15, wherein determining a respective set of distinguishing features from the feature values of the images in the presentation sequence comprises, for each subtask:
the feature values of the images in the presentation sequence in the subtask are processed using a trained classifier to determine the distinguishing features of the task.
17. The computer-readable storage medium of claim 15, wherein determining a respective set of distinguishing features from the feature values of the images in the presentation sequence comprises, for each feature:
normalizing the feature values of the feature to the images in the presentation sequence to determine normalized feature values of the feature for each image;
for each subtask:
calculating distribution statistics of normalized feature values of the feature of the image not in the subtask;
calculating distribution statistics of normalized feature values of the feature of the image in the subtask; and
a score of the feature of the subtask is determined from the distribution statistics of the normalized feature values for the feature of the image not in the subtask and the distribution statistics of the normalized feature values for the feature of the image in the subtask.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662418122P | 2016-11-04 | 2016-11-04 | |
US62/418,122 | 2016-11-04 | ||
PCT/US2017/060216 WO2018085778A1 (en) | 2016-11-04 | 2017-11-06 | Unsupervised detection of intermediate reinforcement learning goals |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110168574A CN110168574A (en) | 2019-08-23 |
CN110168574B true CN110168574B (en) | 2023-10-13 |
Family
ID=60409418
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780074215.XA Active CN110168574B (en) | 2016-11-04 | 2017-11-06 | Unsupervised detection of intermediate reinforcement learning targets |
Country Status (4)
Country | Link |
---|---|
US (2) | US11580360B2 (en) |
EP (1) | EP3535702B1 (en) |
CN (1) | CN110168574B (en) |
WO (1) | WO2018085778A1 (en) |
Families Citing this family (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2018083667A1 (en) * | 2016-11-04 | 2018-05-11 | Deepmind Technologies Limited | Reinforcement learning systems |
US10748061B2 (en) * | 2016-12-19 | 2020-08-18 | Futurewei Technologies, Inc. | Simultaneous localization and mapping with reinforcement learning |
CN110780982A (en) * | 2018-07-27 | 2020-02-11 | 深圳百迈技术有限公司 | Image processing method, device and equipment |
US11507826B2 (en) * | 2018-08-01 | 2022-11-22 | Osaro | Computerized imitation learning from visual data with multiple intentions |
US10715869B1 (en) * | 2018-12-20 | 2020-07-14 | Rovi Guides, Inc. | Deep reinforcement learning for personalized screen content optimization |
CN110490319B (en) * | 2019-07-30 | 2020-06-26 | 成都蓉奥科技有限公司 | Distributed deep reinforcement learning method based on fusion neural network parameters |
EP4003665A1 (en) * | 2019-09-15 | 2022-06-01 | Google LLC | Determining environment-conditioned action sequences for robotic tasks |
CN112437690A (en) * | 2020-04-02 | 2021-03-02 | 支付宝(杭州)信息技术有限公司 | Determining action selection guidelines for an execution device |
CN112533681A (en) * | 2020-04-02 | 2021-03-19 | 支付宝(杭州)信息技术有限公司 | Determining action selection guidelines for an execution device |
US11663522B2 (en) | 2020-04-27 | 2023-05-30 | Microsoft Technology Licensing, Llc | Training reinforcement machine learning systems |
DE102020209685B4 (en) | 2020-07-31 | 2023-07-06 | Robert Bosch Gesellschaft mit beschränkter Haftung | METHODS OF CONTROLLING A ROBOT DEVICE AND ROBOT DEVICE CONTROL |
CN112957740B (en) * | 2021-03-26 | 2023-09-29 | 南京大学 | Method for automatically decomposing game environment by adapting to hierarchical reinforcement learning |
CN113524186B (en) * | 2021-07-19 | 2023-11-03 | 山东大学 | Deep reinforcement learning double-arm robot control method and system based on demonstration examples |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2016025189A1 (en) * | 2014-08-12 | 2016-02-18 | Siemens Aktiengesellschaft | Multi-layer aggregation for object detection |
CN106056213A (en) * | 2015-04-06 | 2016-10-26 | 谷歌公司 | Selecting reinforcement learning actions using goals and observations |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9679258B2 (en) * | 2013-10-08 | 2017-06-13 | Google Inc. | Methods and apparatus for reinforcement learning |
US10810491B1 (en) * | 2016-03-18 | 2020-10-20 | Amazon Technologies, Inc. | Real-time visualization of machine learning models |
US10388274B1 (en) * | 2016-03-31 | 2019-08-20 | Amazon Technologies, Inc. | Confidence checking for speech processing and query answering |
US20170372225A1 (en) * | 2016-06-28 | 2017-12-28 | Microsoft Technology Licensing, Llc | Targeting content to underperforming users in clusters |
US10650307B2 (en) * | 2016-09-13 | 2020-05-12 | International Business Machines Corporation | Neuromorphic architecture for unsupervised pattern detection and feature learning |
US10026021B2 (en) * | 2016-09-27 | 2018-07-17 | Facebook, Inc. | Training image-recognition systems using a joint embedding model on online social networks |
-
2017
- 2017-11-06 EP EP17801215.9A patent/EP3535702B1/en active Active
- 2017-11-06 US US16/347,651 patent/US11580360B2/en active Active
- 2017-11-06 CN CN201780074215.XA patent/CN110168574B/en active Active
- 2017-11-06 WO PCT/US2017/060216 patent/WO2018085778A1/en active Search and Examination
-
2023
- 2023-02-13 US US18/168,000 patent/US20230196058A1/en active Pending
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2016025189A1 (en) * | 2014-08-12 | 2016-02-18 | Siemens Aktiengesellschaft | Multi-layer aggregation for object detection |
CN106056213A (en) * | 2015-04-06 | 2016-10-26 | 谷歌公司 | Selecting reinforcement learning actions using goals and observations |
Non-Patent Citations (1)
Title |
---|
"TSC-DL: Unsupervised Trajectory Segmentation of Multi-Modal Surgical Demonstrations with Deep Learning";Adithyavairavan Murali, et al;《2016 IEEE International Conference on Robotics and Automation (ICRA)》;20160521;第4150-4157页 * |
Also Published As
Publication number | Publication date |
---|---|
US11580360B2 (en) | 2023-02-14 |
EP3535702B1 (en) | 2024-05-01 |
WO2018085778A1 (en) | 2018-05-11 |
CN110168574A (en) | 2019-08-23 |
US20190332920A1 (en) | 2019-10-31 |
US20230196058A1 (en) | 2023-06-22 |
EP3535702A1 (en) | 2019-09-11 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110168574B (en) | Unsupervised detection of intermediate reinforcement learning targets | |
CN110326004B (en) | Training a strategic neural network using path consistency learning | |
US11651208B2 (en) | Training action selection neural networks using a differentiable credit function | |
US10296825B2 (en) | Dueling deep neural networks | |
CN109478248B (en) | Method, system, and storage medium for classifying input samples using a comparison set | |
US20210089968A1 (en) | Memory augmented generative temporal models | |
US10860928B2 (en) | Generating output data items using template data items | |
US20170132512A1 (en) | Regularizing machine learning models | |
EP3568811A1 (en) | Training machine learning models | |
EP3360086A1 (en) | Training neural networks using a prioritized experience memory | |
US11922281B2 (en) | Training machine learning models using teacher annealing | |
WO2018093926A1 (en) | Semi-supervised training of neural networks | |
US11941088B1 (en) | Image processing of an environment to select an action to be performed by an agent interacting with the environment | |
US20190130267A1 (en) | Training neural networks using priority queues | |
CN111902811A (en) | Proximity-based intervention with digital assistant | |
CN110334244B (en) | Data processing method and device and electronic equipment | |
US20220375457A1 (en) | Apparatus and method for compositional spoken language understanding | |
EP4327245A1 (en) | Enhancing population-based training of neural networks |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |