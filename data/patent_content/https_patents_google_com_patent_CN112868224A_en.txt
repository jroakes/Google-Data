CN112868224A - Techniques to capture and edit dynamic depth images - Google Patents
Techniques to capture and edit dynamic depth images Download PDFInfo
- Publication number
- CN112868224A CN112868224A CN201980059947.0A CN201980059947A CN112868224A CN 112868224 A CN112868224 A CN 112868224A CN 201980059947 A CN201980059947 A CN 201980059947A CN 112868224 A CN112868224 A CN 112868224A
- Authority
- CN
- China
- Prior art keywords
- image
- depth
- camera
- computer
- implementations
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/172—Processing image signals image signals comprising non-image signal components, e.g. headers or format information
- H04N13/178—Metadata, e.g. disparity information
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/271—Image signal generators wherein the generated image signals comprise depth maps or disparity maps
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/10—Segmentation; Edge detection
- G06T7/11—Region-based segmentation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/55—Depth or shape recovery from multiple images
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/161—Encoding, multiplexing or demultiplexing different image signal components
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/67—Focus control based on electronic image sensor signals
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10028—Range image; Depth image; 3D point clouds
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20112—Image segmentation details
- G06T2207/20132—Image cropping
Abstract
Embodiments described herein relate to a computer-implemented method that includes capturing image data using one or more cameras, where the image data includes a main image and associated depth values. The method further includes encoding the image data in an image format. The encoded image data comprises a main image encoded in an image format and image metadata comprising a device element comprising a profile element indicating an image type and a first camera element, wherein the first camera element comprises an image element and a depth map based on depth values. The method further includes storing the image data in a file container based on the image format after the encoding. The method further comprises causing the main image to be displayed.
Description
RELATED APPLICATIONS
This application claims priority to U.S. provisional application No.62/827,739 filed on 2019, 4/1, which is incorporated herein by reference in its entirety.
Background
Users capture images using various devices, e.g., mobile phones, wearable devices, smart appliances, smart speakers, tablets, computers, standalone cameras, etc. Many devices are capable of capturing images with enhanced information. For example, a user may capture a still image and depth information associated with a scene depicted in the still image. In another example, a user may capture a still image and one or more alternate representations of the still image, such as a burst mode image comprising a plurality of image frames. In yet another example, the user may capture an image with a small amount of motion, for example, motion captured within a short period of 0-3 seconds during which the camera is activated. In yet another example, a user may augment a scene with a virtual object, for example, by inserting the object into the scene prior to capturing the image, thereby obtaining an Augmented Reality (AR) image.
A user may view or edit images using different types of devices, e.g., mobile phones, wearable devices, smart appliances, smart speakers, tablets, computers, standalone cameras, etc. Different devices may have different image display capabilities, e.g., two-dimensional or three-dimensional display. Different software applications for image editing may include different features, such as portrait effects (where one or more objects of an image identified as the subject of the image are in focus while other objects are blurred), landscape effects (where a selected object is in focus while other portions of the image are blurred), and so forth. Some software applications may also include functionality to perform image segmentation, e.g., identifying one or more objects. Such an application may allow a user to modify an identified object, for example, by applying an effect to the identified object (e.g., changing it to black and white or tan, softening edges, etc.) while other portions of the image are unchanged, or extracting the identified object, for example, by a cut or copy operation, and inserting the object into another image.
The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.
Disclosure of Invention
Some embodiments relate to a computer-implemented method that includes capturing image data using one or more cameras. The image data includes a primary image (primary image) and associated depth values. The method further includes encoding the image data in an image format. The encoded image data includes a main image encoded in an image format, and image metadata including a device element including a profile element indicating a type of the image and a first camera element. The first camera element comprises an image element and a depth map based on depth values. The method further includes storing the image data in a file container based on the image format. The method further comprises causing the main image to be displayed.
In some implementations, the depth map includes a plurality of pixel values, each pixel value indicating a respective distance from a particular camera of the one or more cameras. In some implementations, the depth map further includes a lens focus model that defines respective radius values of the circle of confusion corresponding to a plurality of distances from the particular camera. In some embodiments, the plurality of distances includes a near distance and a far distance, wherein the near distance is less than the far distance. In some embodiments, the plurality of distances further includes a focal length that is greater than the near distance and less than the far distance.
In some implementations, the method further includes obtaining a plurality of pixel values by converting the depth values into an integer format and compressing the converted depth values based on the image format.
In some implementations, the image data can further include one or more secondary images. In some implementations, each secondary image can be captured by a respective camera of one or more cameras that are different from the particular camera. In some implementations, the image metadata can further include one or more additional camera elements, where each additional camera element corresponds to at least one image of the one or more secondary images and includes a corresponding image element that includes a pointer to the at least one image. In some implementations, the one or more additional camera elements are organized sequentially in the image metadata after the first camera element. In some implementations, storing the image data in the file container includes concatenating (concatenating) the primary image encoded in the image format and the one or more secondary images. In some implementations, the order of the primary image and the one or more secondary images in the file container is the same as the order of the one or more additional camera elements in the image metadata.
In some implementations, the image data can further include one or more secondary images, each captured by a respective camera of the one or more cameras. In some implementations, the one or more secondary images can include depth images or video captured by a depth camera.
Some embodiments relate to a computer-implemented method that includes obtaining image data from a file container. The image data includes a plurality of pixel values corresponding to the main image and image metadata including a lens focus model. The method further comprises causing the main image to be displayed. The method further includes receiving a user input indicating a target focal length. The method further comprises the following steps: in response to a user input, one or more pixel values of the main image are modified based on the lens focus model to obtain a modified image such that an object in the modified image at the target focus distance is in focus. The method further includes causing the modified image to be displayed.
In some implementations, the lens focus model can define respective radius values of the circle of confusion corresponding to a plurality of distances from a camera that captures the primary image. In these implementations, modifying the one or more pixel values blurs one or more objects in the main image, where the one or more objects are associated with depth data indicating that the one or more objects are at a different distance from a camera capturing the main image than the target focal length.
In some implementations, the lens focus model can be stored in a depth map element in the image metadata in a file container. In some implementations, the depth primitive elements may correspond to a camera that captures the main image.
Some embodiments relate to a computer-implemented method that includes obtaining image data from a file container. The image data includes a plurality of pixel values corresponding to a main image and image metadata including a depth map. The method further comprises causing the main image to be displayed. The method further comprises the following steps: user input is received indicating at least one of a cropping operation for the main image or a zooming operation for the main image. The method further includes modifying the primary image based on the user input to obtain a modified image. The method further includes computing an updated depth map, wherein the updated depth map includes depth data corresponding to the modified image. The method further includes updating the file container to replace the main image with the modified image and to replace the depth map with the updated depth map.
Some embodiments relate to a computer-implemented method that includes obtaining image data from a file container. The image data includes a plurality of pixel values, each pixel value corresponding to a particular pixel of a plurality of pixels of the main image; and image metadata comprising a plurality of depth maps. The method further includes generating a three-dimensional image based on the main image and the plurality of depth maps. The method further includes causing a three-dimensional image to be displayed.
In some implementations, the method further includes receiving a user input indicating a tilt operation or a pan operation for the three-dimensional image. The method further comprises the following steps: in response to receiving the user input, an updated three-dimensional image is generated based on the user input, the main image, and the plurality of depth maps. The method further includes causing the updated three-dimensional image to be displayed.
In some embodiments, multiple depth maps may be obtained by using depth according to a motion technique using a single camera. In some implementations, multiple depth maps may be obtained via one or more depth cameras.
Some embodiments relate to a computer-implemented method that includes obtaining image data from a file container. The image data comprises a plurality of pixel values, each pixel value corresponding to a respective pixel of a plurality of pixels of the main image; and image metadata including a depth map. The method further comprises causing the main image to be displayed. The method further includes receiving a user input indicating a selection of an object depicted in the primary image. The method further includes generating a segmentation mask based on the depth map. The segmentation mask may identify a subset of pixels of the main image that correspond to the object. The method further includes obtaining an object image of the object based on the segmentation mask. The method further includes providing a user interface that enables selection of an image of the object.
In some implementations, the depth map includes a plurality of depth values, each depth value corresponding to a particular pixel of the main image. In some implementations, generating the segmentation mask includes selecting pixels of the main image that have respective depth values within a threshold range of depth values.
Some embodiments relate to a computing apparatus, computing system, or computing device that performs any of the methods described above. Some embodiments relate to a computer-readable storage medium having instructions stored thereon or a computer program comprising instructions, which when executed by one or more processors, cause the one or more processors to perform the above-described method.
Drawings
FIG. 1 is a block diagram of an example network environment that may be used for one or more embodiments described herein.
Fig. 2A illustrates an example of image capture by a camera, according to some embodiments.
Fig. 2B illustrates an example image and corresponding depth image captured by a camera.
FIG. 3 illustrates an example image file including a metadata structure for storing dynamic depth information.
Fig. 4 is a flow diagram illustrating an example method according to some embodiments.
Fig. 5 is a flow diagram illustrating another example method according to some embodiments.
Fig. 6 is a block diagram of an example device that may be used in one or more embodiments described herein.
Detailed Description
Depth images and augmented reality images are increasingly popular camera use cases across mobile devices and operating systems. However, there is a lack of standardization in capturing, saving, sharing, and editing such images. Depth images created using a camera application may not be editable by other applications.
The techniques described herein provide a uniform and consistent framework within a camera framework used by various client devices to store image metadata and/or additional images in a single conjoined file container. Images stored using the techniques described herein may be accessed for viewing and/or editing by any application. Thus, the techniques described herein may provide improved interoperability between applications and/or devices when capturing, saving, sharing, or editing depth images and/or augmented reality images.
The techniques are easy to implement, quickly parse and extend existing image formats across a variety of operating systems used in cameras including mobile device cameras and in software including mobile and personal computing device software. The techniques enable storage of device-related metadata, such as depth information, imaging models of device cameras, etc., as well as any number of secondary images in image metadata of captured images. The stored metadata and secondary images may be utilized by any image viewing or editing application.
Like reference numerals are used in fig. 1 to identify like elements. A letter following a reference numeral such as "156 a" indicates that the text refers specifically to the element having that particular reference numeral. Reference numerals that are not included in the following letters in text such as "156" refer to any or all of the elements in the figures that carry the reference numeral (e.g., "156" herein refers to reference numerals "156 a" and/or "156 b" in the figures).
Fig. 1 illustrates a block diagram of an example network environment 100 that may be used in some implementations described herein. In some implementations, the network environment 100 includes one or more server systems, such as the server system 102 in the example of fig. 1. For example, server system 102 may communicate with network 130. Server system 102 may include server device 104 and a storage device (not shown). Server system 102 may store one or more image files 106 b. In some implementations, the server device 104 can provide an image management application 156 b. The image management application 156b may access the image file 106b under the permission of the user providing the corresponding image file 106 b.
For ease of illustration, fig. 1 shows four blocks for server system 102 and server device 104, and for client devices 120, 122, 124, and 126. Server blocks 102 and 104 may represent multiple systems, server devices, and network databases, and these blocks may be provided in different configurations than shown. For example, server system 102 may represent multiple server systems that may communicate with other server systems via network 130. In some implementations, for example, the server system 102 can include a cloud hosting server. In some examples, one or more storage devices may be provided in a server system block separate from server device 104 and may communicate with server device 104 and other server systems via network 130.
There may be any number of client devices. Each client device may be any type of electronic device, such as a desktop computer, laptop computer, portable or mobile device, cellular telephone, smart phone, tablet computer, television, TV set-top box or entertainment device, wearable device (e.g., display glasses or goggles, wrist watch, headset, armband, jewelry, etc.), Personal Digital Assistant (PDA), media player, gaming device, etc. Some client devices may also include a local image file 106a, e.g., a local image file captured using a camera of the client device, or obtained in other ways, e.g., by downloading the image file from a server. In some implementations, network environment 100 may not have all of the components shown and/or may have other elements including other types of elements instead of or in addition to those described herein.
In various implementations, end users U1, U2, U3, and U4 may use respective client devices 120, 122, 124, and 126 to communicate with server system 102 and/or each other. In some examples, users U1, U2, U3, and U4 may interact with each other via web services implemented on server system 102, e.g., social networking services, image hosting services, other types of web services, via applications running on respective client devices and/or server system 102. For example, respective client devices 120, 122, 124, and 126 can communicate data to and from one or more server systems, e.g., system 102.
In some implementations, the server system 102 can provide appropriate data to the client devices so that each client device can receive communication content or shared content uploaded to the server system 102 and/or a network service. In some examples, users U1-U4 may interact via audio or video conferencing, audio, video, or text chat, or other communication modes or applications. The network services implemented by the server system 102 may include systems that allow users to perform various communications, form links and associations, upload and publish shared content such as images, text, video, audio, and other types of content, and/or perform other functions. For example, the client device may display received data, such as content publications sent or streamed to the client device and originating from different client devices (or directly from different client devices) or originating from a server system and/or network service via a server and/or network service. In some implementations, the client devices may communicate directly with each other, e.g., using peer-to-peer communication between the client devices as described above. In some embodiments, a "user" may include one or more programs or virtual entities, as well as a person interfacing with a system or network.
In some implementations, any of client devices 120, 122, 124, and/or 126 may provide one or more applications. For example, as shown in fig. 1, the client device 120 may provide a camera application 152 and an image management application 156 a. Client device 122-126 may also provide similar applications. For example, the camera application 152 may provide users of respective client devices (e.g., users U1-U4) with the ability to capture images using one or more cameras of their respective client devices. For example, the camera application 152 may be a software application executing on the client device 120.
In some implementations, the camera application 152 can provide a user interface. For example, the user interface may enable a user of the client device 120 to select an image capture mode, such as a still image (or photo) mode, a burst mode (e.g., capturing a consecutive number of images within a short period of time), a moving image mode, a video mode, a High Dynamic Range (HDR) mode, and so forth. In some implementations, the camera application 152 can implement (e.g., partially or fully) the methods described with reference to fig. 4 and 5. In some implementations, the image management application 156a and/or the image management application 156b can implement (e.g., partially or wholly) the methods described herein with reference to fig. 4-5.
The camera application 152 and the image management application 156a may be implemented using hardware and/or software of the client device 120. In various embodiments, the image management application 156a may be a stand-alone application, for example, executing on any of the client devices 120 and 124, or may work in conjunction with the image management application 156b provided on the server system 102. Image management application 156 may enable a user to view and/or edit images, such as image file 106.
With the user's permission, the image management application 156 may perform one or more automated functions, such as storing (e.g., backing up) images on a server, editing images, automatically enhancing images, stabilizing images, identifying one or more features in images, e.g., face, body, object type, movement type, and so forth.
The image management application 156 may also provide image management functions such as displaying images in a user interface (e.g., in a one-up view containing a single image, in a grid view including multiple images, etc.), editing images (e.g., adjusting image settings, applying filters, changing image focus, applying portrait effects, applying pop-up effects, applying other effects, viewing three-dimensional images through pan/tilt operations, extracting objects from images using image segmentation techniques, etc.), sharing images with other users (e.g., users of the client device 120 with the 126), archiving images (e.g., storing images so that they do not appear in a main user interface), generating image-based authoring (e.g., collage, photo album, motion-based artifacts such as animation, stories, video loops, etc.). In some implementations, to generate image-based authoring, image management application 156 can utilize one or more tags associated with the image or image metadata stored in the image.
User interfaces on client devices 120, 122, 124, and/or 126 may enable display of user content and other content, including images, videos, data, and other content as well as communications, privacy settings, notifications, and other data. Such a user interface may be displayed using software on a client device, software on a server device, and/or a combination of client software and server software executing on server device 104 (e.g., application software or client software in communication with server system 102). The user interface may be displayed by a display device (e.g., a touch screen or other display screen, a projector, etc.) of the client device or the server device. In some implementations, an application running on a server system may communicate with a client device to receive user input at the client device and output data, such as visual data, audio data, and the like, at the client device.
In some implementations, the server system 102 and/or any of the one or more client devices 120 and 126 can provide a communication application. The communication program may allow a system (e.g., a client device or server system) to provide options for communicating with other devices. The communication program may provide one or more associated user interfaces that are displayed on a display device associated with the server system or the client device. The user interface may provide various options to the user to select a communication mode, etc., with which the user or device communicates. In some examples, the communication program may provide an option to send or broadcast the content publication, e.g., to a broadcast area, and/or may output a notification indicating that the content publication has been received by the device and, for example, the device in the defined broadcast area of the publication. The communication program may display or otherwise output the transmitted content publication and the received content publication, for example, in any of a variety of formats. Content distribution may include, for example, images shared with other users.
Other implementations of the features described herein may use any type of system and/or service. For example, other networking services (e.g., connecting to the internet) may be used instead of or in addition to social networking services. Any type of electronic device may utilize the features described herein. Some embodiments may provide one or more features described herein on one or more client or server devices that are disconnected from or intermittently connected to a computer network. In some examples, a client device that includes or is connected to a display device may display data (e.g., content) stored on a storage device local to the client device, e.g., previously received over a communication network.
Fig. 2A illustrates an example of capturing an image with a camera. As illustrated in fig. 2A, a camera having a lens 202 may be used to capture an image. Although fig. 2A illustrates lens 202, the camera may include other elements, such as an imaging sensor, a focus adjustment device, and the like, which are not shown in fig. 2A. At the time of image capture, the camera is focused on the focal plane 208. The captured image may include multiple objects at different distances from the lens 202 (different distances from the camera). For example, the captured image may include one or more objects in the near plane 206, in the focal plane 208, in the far plane 210, and at any intermediate plane.
A single device for image capture (e.g., any of client devices 120 and 126) may include one or more cameras or imaging sensors. For example, the device may include one or more cameras (e.g., RGB cameras), infrared cameras, and the like that may be used to capture color images. For example, the one or more cameras may include cameras having different configurations, e.g., telephoto cameras, wide-angle cameras, and so on. Other configurations of cameras may also be used. Each camera may generate a corresponding image. In some implementations, image data obtained from different cameras of the device may be combined to obtain the master image. The main image may be displayed, for example, on a screen of the device. Further, according to the techniques described herein, the main image may be stored in an image format such as JPEG, PNG, TIFF, HEIF, or the like. Further, according to the techniques described herein, one or more images obtained from individual cameras may be stored in a container as described below with reference to fig. 3.
Fig. 2B illustrates an example image 220 and a corresponding depth image 230 captured with a camera. For example, a depth image may be captured using an infrared camera or other depth sensor. In another example, the depth image may be generated based on image data captured with a camera that captured the image 220.
As seen in fig. 2B, image 220 depicts a scene that includes flowers that are closer to the camera than leaves seen in the background. Accordingly, in the depth image 230, petals of the flower that are closer to the camera are seen in darker shades than other petals that are farther from the camera and leaves that are farther from the camera. In the depth image 230, darker shadows correspond to objects in the scene that are closer to the camera, while lighter shadows correspond to objects that are further away.
The depth image 230 may be an image of a scalar value (e.g., an integer value, a real number, a floating point value, etc.) representing a distance from a camera viewpoint to an object viewed in a corresponding image (e.g., image 220). The definition of depth values may vary based on the type of depth sensor. For example, two common definitions are depth from the camera capturing the image along the optical axis (typically the z-axis), and depth along the ray passing through each pixel during image capture.
Fig. 3 illustrates an example image file 300 that includes a metadata structure for storing dynamic depth information. Structured metadata may be used for images that include depth information. In some implementations, the image file 300 may be stored as a concatenated file container comprising a composite or concatenated file in which the primary image has zero, one, or more secondary media files attached thereto. In the illustrated example of fig. 3, a primary image 340 and associated metadata 310 are shown, as well as XMP (extensible metadata platform) metadata 320. In some implementations, the secondary media file may include an alternative representation of the primary image or related media such as depth data.
In some implementations, the image file 300 may be stored as an ISO Box media File Format container based on ISO/IEC 14496-12. In these embodiments, the XMP metadata in a container comprises a container XMP metadata catalog element, where each entry in the catalog refers to a box in the ISO/IEC 14496-12 container using a URI (Uniform resource identifier).
In some implementations, a container element encoded into the XMP metadata of the image file can define a catalog of media items in the container. In some implementations, the concatenated media items are located in the container file in the same order as the media item elements in the directory. In some implementations, the concatenated media items are tightly packed, e.g., data corresponding to different media items is placed consecutively in a file container without gaps. For example, the image file may include bytes corresponding to the concatenated media items concatenated after the primary image (e.g., primary image 340).
The main image 340 may be a display-ready image, such as a JPEG image or an image in another format. The image file 300 may include secondary images, such as depth data, intermediate images, alternate representations of the primary image, or may include other media items (e.g., one or more videos, texts, etc.). As described above, the secondary images and/or media items may be stored in a nexus file container. In some embodiments where the image file 300 is an ISO/IEC 14496-12 ISO Box media file format container, the secondary images may be stored in the container as other boxes.
The image file 300 may further include XMP (extensible metadata platform) metadata 320. XMP metadata 320 can include data that specifies one or more devices, such as device element 324. The metadata may be, for example, by following ISO 16684-1: 2011(E) section 1 of the XMP specification is serialized and embedded into the image file 300, for example, as described in section 3 of the Adobe XMP specification stored in the file. In some embodiments, the image file 300 may include items formatted as RDF/XML (resource description framework/extensible markup language).
In some implementations, the XMP metadata 320 (also referred to as a container XMP metadata directory) can define the order and nature of subsequent media files in the nexus file container. Each element may be a structural container: items, and may define the layout and content of the container. The container type may be, for example, a "device" element, a "camera" element, or the like. Each file (e.g., other images, videos, text, etc.) in the nexus container may have a corresponding media item in the directory. The media items may describe locations in the file container and the basic properties of each nexus file in the nexus file container. In some implementations, the media items in the container directory can be referenced by ItemURI (Uniform resource identifier) attributes from image or depth map elements in the metadata. The URI may be based on a namespace, e.g., the prefix is Container using a default namespace. The directory contains only a single master image, which is the first item in the directory. The item element includes a description of each item that the application can use.
In some embodiments, the root metadata object in XMP data 320 may include a device element (324), also referred to as device 324. In some embodiments, the device elements 324 are mandatory, e.g., at least one device 324 is always included in the XMP data 320. In some embodiments, more than one device element may be included.
The device elements 324 may be specified according to one or more profiles or use cases. Each profile may have a corresponding set of required elements and data. The profile may be used by an application accessing the image file 300, such as a camera application generating the image file 300, or other applications, such as an image backup, viewing, or editing application. For example, a profile may be used to identify use cases that a particular image file may support. The image file 300 may include one or more profiles. In some implementations, each profile may correspond to a particular use case and may have a corresponding set of required elements and data. For example, an application accessing the image file 300 to enable a user to view or edit an image may utilize a profile to identify use cases that a given image file may support.
In the example illustrated in fig. 3, the XMP metadata 320 of the image file 300 includes a single device element 324, the device element 324 including a profile element 326 (also referred to as profile 326) of the profile type "DepthPhoto". Further, the profile 326 indicates, for example, the number of cameras from which image data of the image file 300 is obtained at the time of image capturing (or creation) (i "where i is an index value). In some implementations, for example, where the camera index is zero, the primary image may be the same as the image referenced in the camera element. In some implementations, a profile of profile type "DepthPhoto" may be specified to include at least one profile element 326 and at least one camera element (e.g., "camera 328"). In the example illustrated in fig. 3, camera 328 (also referred to as camera element 328) is depicted as including camera element 330 (camera i). In some implementations, the camera 328 is a sequence of one or more camera elements.
In some implementations, each camera element (e.g., camera element 330) includes an image element 332 and a depth map element 334 (also referred to as a depth map 334) for an image provided by the camera. The image element 332 may include an attribute "primary" that indicates whether the image element corresponds to the primary image 340 in the concatenated file container. In some implementations, image element 332 may include an attribute "original" that indicates whether the media item referenced by image element 332 is an original version, e.g., an unfiltered color image that may be used to render depth effects. In some implementations, image element 332 may include an attribute "raw" indicating that the media item, e.g., image, referenced by image element 332 is a raw version that is not ready for display and may be processed to obtain a ready-for-display image. In some implementations, image element 332 may include an attribute "depth" that indicates the media item, e.g., image, referenced by image element 332, including a depth image.
The depth map 334 may include information related to depth and/or a depth map for an image provided by a corresponding camera. In some implementations, the image elements 332 and/or depth map elements 334 can include URIs (uniform resource identifiers) that point to locations of corresponding image data and/or depth data stored in the image file 300, e.g., in a nexus file container. The depth data of the first camera must be free of holes. The application that generates the image file encodes the estimate in an image area where depth values are not available or cannot be calculated.
In some implementations, the depth image includes a plurality of values, each value corresponding to a particular pixel of the depth image. Depth information, such as floating point pointers or 32-bit integer formats, is converted to integer format and compressed using an image codec supported by the file container type (e.g., JPEG). The conversion to integer format may be performed in different ways, including linear range conversion and inverse range conversion. For example, if there is a loss in depth map precision when encoded, e.g., converting floating-point depth values to 16-bit integers, the inverse conversion may be utilized.
Linear transformation (RangeLinear): d is the depth distance value of the pixel, and the minimum and maximum depth values of the near and far can be obtained. The depth values are first normalized to a [0, 1] range using the following formula:
the depth value is then quantized to 16 bits using the following formula:
d16bit＝[dn·65535]
from quantized depth values d using a formulanDepth recovery:
d＝dn·(far-near)+near
reverse conversion (RangeInverse): RangeInverse assigns more bits to near depth values and fewer bits to far depth values. The depth values are first normalized to a [0, 1] range using the following formula:
the depth value is then quantized to 16 bits using the following formula:
d16bit＝[dn·65535]
from the quantized depth values d, the following formula is usednDepth recovery:
in some implementations, the depth map 334 can include a lens focus model for the depth data. In some implementations, the lens model can include metadata stored, for example, in a focalcable attribute. The application may use the metadata, for example, to render depth effects. The lens model defines the radius of the circle of confusion at different distances from the viewer, e.g., the camera that captured the image. The focalcable may include distance and radius pairs that may be used to construct a lookup table that defines a circle of confusion as a function of depth distance values.
In some embodiments, the focalcable attribute may be a string value composed of a based-64 encoded small-endian floating-point pair as the actual distance value. These < distance >, < radius > pairs define a lookup table that may be used, for example, by an image viewing or editing application to calculate the radius of the circle of confusion at the distance value between the near and far values of the depth map.
In some embodiments, the distance-radius pairs are stored in ascending order sorted by distance value. In some implementations, the distance coordinates can be defined in units of depth map distance values. In some embodiments, the radius value may be defined in pixel coordinates. Each radius value is greater than or equal to zero. A radius value of zero may represent a depth-of-focus distance at the focal plane of the image. The look-up table comprises at least two pairs, for example a first pair corresponding to a short distance and a second pair corresponding to a long distance of the depth map. In some implementations, radius values can be interpolated (e.g., linearly interpolated) between points defined in the focalcable to obtain the radius of the circle of confusion at any focal distance from the camera that captured the image.
In some embodiments, the focalcable may include three radius values — a near value for the near plane, a focal plane value for the focal plane, and a far value for the far plane. The image viewing application may access the focalcable and focus the object rendered at the focal plane depth value. In some embodiments, the focalcable may include two or more distance values as in focus, e.g., corresponding to a focal plane.
In some implementations, the primary image 340 is associated with a first camera in the camera element 328. In some implementations, the first camera element listed in the camera 328 may be a primary camera, e.g., a primary camera of a device that is a smartphone or tablet. In some implementations, the additional camera element may correspond to other cameras of the device, e.g., an infrared camera of a smartphone or tablet device. The order of the other cameras may be arbitrary. In some implementations, each additional camera element can include a corresponding pose element that indicates a pose of the camera relative to the device. In some implementations, all images and depth maps within a single camera element may be corrected by the same camera, e.g., the elements have the same pose, scale, and field of view.
If the image file 300 is modified by an image editing application, e.g., cropped, zoomed, etc., one or more camera elements may be updated to reflect the modification to the image, e.g., updating the depth map and image elements of the camera elements.
In some implementations, the image file 300 may include information about image capture, such as a pose, including the position and orientation of an imaging sensor or camera that captured the image relative to the scene. Including this information may enable an application to use images from multiple cameras together (e.g., an image from a color image camera and a depth image from a depth sensor). For example, the pose information may enable an application to map depth data (possibly with different resolutions) onto an image.
In some implementations, the device 324 may include one or more other elements. For example, such elements may include a container element that is an ordered directory of nexus files in a file container, VendorInfo that includes vendor-related information for the device, AppInfo element that includes application-specific or rendering information for the device, and so forth.
In some implementations, the camera element 330 can include one or more additional elements, such as a VendorInfo element that includes information about the vendor of the camera, an AppInfo element that includes application specific or rendering information for the camera, an image element that includes an image (e.g., a depth image) provided by the camera, an ImagingModel element that describes an imaging model of the camera lens. For example, the ImagingModel element may be used to describe a fisheye distortion model or a standard pinhole camera model with 5-DoF radial distortion. In some embodiments, the ImagingModel element may indicate focal lengths along the X-axis and Y-axis, normalized by the size of the imaging sensor, expressed as a real number; principal points (X-position and Y-position) where the camera optical axis intersects the center of the camera image plane along the X and Y axes, respectively, normalized by the sensor height; image width and height in pixels; yaw of the camera, e.g., the y-axis of the image deviates from vertical by a clockwise angle, expressed in degrees; a pixel aspect ratio of a pixel width to a pixel height; and one or more distortion parameters. When an image is edited (e.g., zoomed or cropped), the ImagingModel element may be updated accordingly.
The Item structure may define the layout and content of the container in which the Item is included, e.g., in container: the form of the item. For example, the "profile" container element may include "type" and "cameras" items, as illustrated in FIG. 3. In another example, a "camera" element may include an "image" item that includes a URI to a corresponding image in a file container; the term "depth map" includes a depth map of an image, and the like. Each item element includes information that enables an image backup, viewing, or editing application, for example, to enable a user to view or edit an image.
In some implementations, the first media item in the catalog is the primary image 340. The primary image element has an associated field to indicate the MIME type. This field may be a simple string indicating the MIME type of the media item in the container. For example, the MIME type value of the master image may be:
further, if included, the length field in the main image element may be set to a value of zero. In embodiments where the primary image is the first file in the file container, for example, starting at the beginning of the file container, the length of the primary image is determined by parsing the primary image based on the indicated MIME type.
The first media item in the XMP metadata may optionally include a padding attribute specifying additional padding between the end of the encoded primary image and the beginning of the first secondary image.
In some implementations, where the media items are tightly packed, subsequent media items for the secondary images do not include a fill attribute. Each media item (e.g., an image other than the main image 340) has a corresponding MIME-type attribute and a length attribute. The length attribute may be an integer value and may be necessary for the secondary media item. In some implementations, the length attribute can be a simple string that includes a positive integer length indicating the number of bytes joining the entry in the corresponding file in the file container. Typically, the item element includes a MIME attribute of one of the image MIME types listed in table 1. In some implementations, sequential media items can share resource data within a file container. The first media item may indicate a location of the asset in the file container, and the subsequent shared media item may have a length attribute set to 0. Where the resource data itself is a container, the DataURI may be used to determine the location of the media item data within the resource. The application may determine the location of the media item resource in the nexus file container by adding the length of the preceding secondary item resource to the length of the primary image encoding plus the fill value (if specified) of the primary image.
The following table describes examples of various attributes that may be included in an item element:
in some implementations, XMP metadata can specify parameters for Augmented Reality (AR) images. The profile type of the AR image may be "arpoto". Such parameters may include, for example, pose of the client device capturing the AR image, lighting estimation information, horizontal and/or vertical surface planes in the scene (e.g., floor, walls, etc.), camera parameters, and so forth. Furthermore, the AR parameters may include application specific metadata, e.g., metadata provided by image capture or augmented reality applications. Such application-specific metadata may include identifiers of three-dimensional (3D) assets, e.g., one or more virtual objects used to augment a real-world scene depicted in an AR image, and corresponding poses.
Further, in some implementations, one or more images may be embedded inside an image file for an augmented reality image. An example use case for such embedding is when the embedded image is an image without a 3D asset and the container, e.g. the main image, comprises a 3D asset. This enables a user to preview thumbnails with 3D assets, for example, in an image gallery (gallery), while enabling a viewing or editing application to provide a user interface that enables the user to interact and move virtual objects in augmented reality images. In this example, the main image may be updated when the user saves the augmented reality image after editing. In this example, the XMP metadata stores the embedded image in a camera having an index greater than index 0. If there is no embedded image, camera 0 includes AR metadata.
Other types of profiles, e.g., other than "depthphoto" and "arpoto", can also be specified in XMP metadata to support other functions and use cases.
In some implementations, an application, such as a camera application on a mobile device, can generate the image file 300, for example, using camera hardware of a client device (e.g., any of the client devices 120 and 126). In these embodiments, the camera application may generate the image file 300 in the format described above. To generate the image file 300, image data may be obtained from an imaging sensor, such as a hardware sensor (e.g., CCD, CMOS, infrared, etc.).
The image data obtained from the imaging sensor may be modified, for example, by a camera application, such that the main image and one or more other images (e.g., each corresponding to camera i) in the image file 300 have the same pose and the same aspect ratio. For example, the main image and one or more other images may be cropped to the same aspect ratio. In some implementations, for example, when depth information is available from one or more cameras, the image file 300 may include a depth map. In some implementations, the resolution of the depth map may be the same as the resolution of the main image and the one or more other images. In some implementations, the resolution of the depth map may be different from the resolution of the main image and the one or more other images. In some implementations, one or more other images may be stored in the image file 300 and referenced in additional camera elements.
An image file that is a nexus file container with XMP metadata as described herein enables an image backup, viewing, or editing application to present media, e.g., a depth photograph, an augmented reality photograph, etc., in a manner consistent with the generation of an image, e.g., captured using one or more imaging sensors of a client device. Further, including the primary image in the file container enables applications or services that cannot interpret XMP metadata to obtain a display-ready version of the image.
Fig. 4 is a flow diagram illustrating an example method 400 according to some embodiments. In some implementations, the method 400 may be implemented, for example, on the server system 102 as shown in fig. 1. In some implementations, some or all of method 400 may be implemented on one or more client devices 120, 122, 124, or 126 as shown in fig. 1, on one or more server devices, and/or on both server devices and/or client devices. In the described examples, an implementation system includes one or more digital processors or processing circuits ("processors") and one or more memory devices. In some implementations, different components of one or more servers and/or clients may perform different blocks or other portions of method 400. In some examples, a first device is described as performing the blocks of method 400. Some embodiments may cause one or more blocks of method 400 to be performed by one or more other devices (e.g., other client devices or server devices) that may send results or data to the first device.
In various implementations, the client device 120 may be a standalone camera, another device including a camera, e.g., a smartphone, a tablet, a computer, a wearable device such as a smartwatch, a headset, etc., or other client device that may receive images or video captured by another device. In some implementations, the client device 120 may be a capture-only device, e.g., a camera that does not include a screen. In some implementations, the client device 120 may be a view-only device, e.g., a device that includes a screen on which images or video may be displayed but does not have a camera or other capability to capture images or video. In some implementations, the client device 120 may have both capture and viewing capabilities.
In some implementations, the client device 120 may include a single camera to capture images or video. In some implementations, the client device 120 may include multiple cameras (or lenses). For example, a smartphone or other device may include one or more front-facing cameras (on the same side of the device as the screen) and/or one or more rear-facing cameras. In some implementations, one or more front or rear cameras may operate together during capture, e.g., a first camera may capture depth information and a second camera may capture image pixels of an image or video. In some implementations, different cameras may be used for different types of image or video capture (e.g., telephoto lens, wide-angle lens, etc.), for example, with different zoom levels. In some implementations, the client device 120 may be configured to capture 360 degree images or video. In some implementations, the camera or lens may capture images using a single image sensor (e.g., a CCD or CMOS sensor) or multiple sensors. In some embodiments, other sensors, such as depth sensors, etc., may be used with one or more cameras at the time of image capture.
In some implementations, the client device 120 may combine raw image data captured at an image sensor from one or more cameras (or lenses) with other data obtained from other sensors (e.g., accelerometers, gyroscopes, position sensors, depth sensors, etc.) to form an image and store the image in a file container.
Client device 120 may enable a user to capture images in different modes, e.g., a still image (or photo) mode to capture a single frame, a burst or moving image mode to capture multiple frames, a video mode to capture video including multiple frames, etc. In some implementations, when the camera captures an image, the method 400 may be performed at the time of image capture.
Client device 120 may enable a user to view, in different user interfaces, images or videos captured by client device 120 or associated with the user, for example. For example, a lead mode or slide show mode may be provided that may be provided to enable a user to view and/or edit a single image or video at a time. In another example, a gallery mode may be provided that may enable a user to view and/or edit multiple images simultaneously, e.g., as an image grid. In some implementations, the client device 120 can perform the method 400. In another example, a client device or a server device may perform method 400. In some embodiments, method 400 may be implemented by a server device.
In block 402, it is checked whether user consent (e.g., user permission) to use the user data in embodiments of method 400 has been obtained. For example, the user data may include images or videos captured by a user using a client device, images or videos stored or accessed by a user, e.g., using a client device, image/video metadata, user data related to use of a messaging application, user preferences, user biometric information, user characteristics (e.g., identity, name, age, gender, occupation, etc.), information about a user's social networks and contacts, social and other types of actions and activities, content created or submitted by a user, ratings and opinions, a user's current location, historical user data, images generated, received and/or accessed by a user, images viewed or shared by a user, and so forth. One or more blocks of the methods described herein may use such user data in some embodiments.
If user consent has been obtained from the relevant user that the user data may be used in method 400, then in block 404, blocks are determined in which the methods herein may be implemented where user data may be used as described for those blocks, and the method continues to block 410. If user consent has not been obtained, then in block 406 it is determined that the block will be implemented without using user data, and the method continues to block 410. In some implementations, if user consent has not been obtained, the block is implemented without using user data and with synthetic data and/or data that is generic or publicly accessible and publicly available. In some implementations, the method 400 is not performed if user consent has not been obtained.
In block 410, image data is captured using, for example, one or more cameras of the client device. In some implementations, the captured image data may include a main image and associated depth values. In some embodiments, the captured image data includes one or more secondary images. In some embodiments, the primary image may be a color image. In some implementations, the one or more secondary images can include a color image (e.g., obtained from a different viewpoint or at a different time than the primary image), a grayscale image, a monochrome image, a depth image (e.g., an image that includes depth information but no human-viewable scene description). In some implementations, the one or more secondary images may be captured by a different camera than the camera that captured the primary image. In some implementations, each of the one or more secondary images can be captured by a corresponding camera of the one or more cameras. Block 410 may be followed by block 412.
In block 412, the image data may be encoded in an image format, for example, as a JPEG image, TIFF image, HEIF image, PNG image, or the like. In some implementations, the encoded image data may include a primary image encoded in an image format. The encoded image data may further include image metadata. In some implementations, the image metadata can be encoded as XMP metadata. In some implementations, the image metadata can include a device element that includes a profile element indicating the type of image, e.g., a depth photograph, an Augmented Reality (AR) photograph, etc. In some implementations, the image metadata may further include a camera element including the image element and a depth map obtained based on the depth values.
In some implementations, the depth map can include a plurality of pixel values, each of the plurality of pixel values indicating a respective distance from a particular camera of the one or more cameras used to capture the image data. In some implementations, the depth can further include a lens focus model. In some implementations, the lens focus model can define respective radius values for the circle of confusion corresponding to a plurality of distances from a particular camera. In some implementations, the plurality of distances can include a near distance (corresponding to a near plane) and a far distance (corresponding to a far plane), where the near distance is less than the far distance (the near plane is closer to the camera than the far plane). In some embodiments, the plurality of distances may further include a focal length that is greater than the near distance and less than the far distance. In some implementations, the plurality of pixel values may be obtained by converting the depth values into an integer format (e.g., a 16-bit integer) and compressing the converted depth values based on an image format (e.g., JPEG).
In embodiments including one or more secondary images, the image metadata may further include one or more additional camera elements. Each additional camera element may correspond to at least one of the one or more secondary images and may include a respective image element that includes a pointer (e.g., a uniform resource identifier) to at least one image in the file container. In some implementations, one or more additional camera elements may be organized sequentially in the image metadata following the first camera element. Block 412 may be followed by block 414.
In block 414, the encoded image data may be stored in a file container based on the image format. In some implementations, storing the image data in the file container can include concatenating the primary image and the one or more secondary images encoded in the image format. The order of the primary image and the one or more secondary images in the file container may be the same as the order of the one or more additional camera elements in the image metadata. Block 414 may be followed by block 416.
In block 416, a primary image may be displayed.
Fig. 5 is a flow diagram illustrating another example method 500 according to some embodiments. In some implementations, the method 500 may be implemented, for example, on the server system 102 as shown in fig. 1. In some implementations, some or all of method 500 may be implemented on one or more client devices 120, 122, 124, or 126 as shown in fig. 1, on one or more server devices, and/or on both a server device and a client device. In the described examples, an implementation system includes one or more digital processors or processing circuits ("processors") and one or more memory devices. In some implementations, different components of one or more servers and/or clients may perform different blocks or other portions of method 500. In some examples, a first device is described as performing the blocks of method 500. Some embodiments may cause one or more blocks of method 500 to be performed by one or more other devices (e.g., other client devices or server devices) that may send results or data to the first device.
In various implementations, the client device 120 may be a standalone camera, another device including a camera, e.g., a smartphone, a tablet, a computer, a wearable device such as a smartwatch, a headset, etc., or other client device that may receive images or video captured by the other device. In some implementations, the client device 120 may be a capture-only device, e.g., a camera that does not include a screen. In some implementations, the client device 120 may be a view-only device, e.g., a device that includes a screen on which images or video may be displayed but does not have a camera or other capability to capture images or video. In some implementations, the client device 120 may have both capture and viewing capabilities.
In some implementations, the client device 120 may include a single camera to capture images or video. In some implementations, the client device 120 may include multiple cameras (or lenses). For example, a smartphone or other device may include one or more front-facing cameras (on the same side of the device as the screen) and/or one or more rear-facing cameras. In some implementations, one or more front or rear cameras may operate together during capture, e.g., a first camera may capture depth information and a second camera may capture image pixels of an image or video. In some implementations, different cameras may be used for different types of image or video capture (e.g., telephoto lens, wide-angle lens, etc.), for example, with different zoom levels. In some implementations, the client device 120 may be configured to capture 360 degree images or video. In some implementations, the camera or lens may capture images using a single image sensor (e.g., a CCD or CMOS sensor) or multiple sensors. In some embodiments, other sensors, such as depth sensors, etc., may be used with one or more cameras at the time of image capture.
In some implementations, the client device 120 may combine raw image data captured at an image sensor from one or more cameras (or lenses) with other data obtained from other sensors (e.g., accelerometers, gyroscopes, position sensors, depth sensors, etc.) to form an image and store the image in a file container.
Client device 120 may enable a user to capture images in different modes, e.g., a still image (or photo) mode to capture a single frame, a burst or moving image mode to capture multiple frames, a video mode to capture video including multiple frames, etc. In some implementations, the method 500 may be performed when the camera captures an image, after capture is complete, or at a later time, such as when a user views and/or edits an image using the client device 120.
Client device 120 may enable a user to view, in different user interfaces, images or videos captured by client device 120 or associated with the user, for example. For example, a lead mode or slide show mode may be provided that enables a user to view and/or edit a single image or video at a time. In another example, a gallery mode may be provided that enables a user to view and/or edit multiple images simultaneously, e.g., as an image grid.
In some implementations, the client device 120 may perform the method 500. In another example, a client device or a server device may perform method 500. In some embodiments, method 500 may be implemented by a server device.
In block 502, it is checked whether user consent (e.g., user permission) has been obtained to use the user data in embodiments of method 500. For example, the user data may include images or videos captured by a user using a client device, images or videos stored or accessed by a user, e.g., using a client device, image/video metadata, user data related to use of a messaging application, user biometric information, user characteristics (e.g., identity, name, age, gender, occupation, etc.), information about a user's social networks and contacts, social and other types of actions and activities, content created or submitted by a user, ratings and opinions, a user's current location, historical user data, images generated, received and/or accessed by a user, images viewed or shared by a user, and so forth. One or more blocks of the methods described herein may use such user data in some embodiments.
If user consent has been obtained from the relevant user that the user data may be used in method 500, then in block 504, blocks are determined in which the methods herein may be implemented where user data may be used as described for those blocks, and the method continues to block 510. If user consent has not been obtained, then in block 506 it is determined that the block is to be implemented without using user data, and the method continues to block 510. In some implementations, if user consent has not been obtained, the block is implemented without using user data and with synthetic data and/or data that is generic or publicly accessible and publicly available. In some embodiments, method 500 is not performed if user consent has not been obtained. For example, if the user denies permission to access one or more images, method 500 is not performed or is stopped after performing block 506.
In block 510, image data is obtained from a file container, e.g., a file container that stores images in a particular image format as described with reference to FIG. 3. In different embodiments, block 510 may be followed by block 512, block 522, block 542, or block 562.
In some implementations, block 510 may be followed by block 512. In block 512, a primary image from the image data is caused to be displayed, for example, on a display screen of the client device. In some implementations, the primary image may be displayed in a user interface that enables a user to provide user input. Block 512 may be followed by block 514.
In block 514, a user input may be received indicating a target focal length. For example, the user interface may include a user interface element (e.g., a slider) that allows the user to select a target focal distance by moving the slider between distance values corresponding to the near distance value and the far distance value in the captured image. In some implementations, user input, e.g., touch input, gesture input, voice input, etc., may be received directly as identifying a particular portion of the primary image, and the focal distance may be determined based on the particular portion. Block 514 may be followed by block 516.
In block 516, the main image may be modified based on a lens focus model, e.g., stored in a file container as part of the image metadata. For example, a main image may be modified to apply a portrait effect in which, for example, objects in the image at a distance greater than a target focal length are blurred; a shot effect in which, for example, an object in an image at a different distance from a camera capturing a main image than a target focal length is blurred; other selective blurring effects, and the like. Other types of effects based on target distance may also be applied, e.g. color effects, filters, etc.
In some implementations, the effect can be based on FocalTable in the depth map stored in the image metadata. If the target focal length corresponds to a distance included in the FocalTable, the radius of the circle of confusion may be obtained by a simple lookup in the FocalTable. If the target Focal length is not included in the Focal table, a radius value corresponding to the available distance, e.g., the closest distance to the target Focal length, may be obtained. For example, if the focalcable includes a distance-radius pair of a near plane and a far plane, such a value is obtained. A radius value for the target focal length is then determined based on interpolation between the obtained values. For example, linear interpolation may be performed to obtain a radius value for the target focal length. The radius value thus determined is used to apply an effect, for example, to blur the main image. Block 516 may be followed by block 518.
In block 518, the modified image is caused to be displayed. In some implementations, block 518 may be followed by block 514, e.g., to allow the user to provide further input, e.g., to indicate a different target focal length.
In some implementations, block 510 may be followed by block 522. In block 522, a primary image from the image data is caused to be displayed, for example, on a display screen of the client device. In some implementations, the primary image may be displayed in a user interface that enables a user to provide user input. Block 522 may be followed by block 524.
In block 524, user input may be received indicating a crop operation or a zoom operation. For example, the user interface may include one or more user interface elements that enable a user to indicate a crop or zoom operation. For example, a user may indicate a cropping operation by drawing a shape (e.g., a cropping rectangle) to select a subset of pixels of the main image — removing one or more portions of the main image. In another example, the user may indicate a zoom operation by dragging one or more corners or edges of the primary image — increasing the size of the primary image. Other types of user interfaces may be provided that enable a user to provide input for a crop or zoom operation. Block 524 may be followed by block 526.
In block 526, the primary image is modified-cropped and/or zoomed-out-based on the user input received in block 524. Block 526 may be followed by block 528.
In block 528, the depth map is updated based on the modified main image. For example, a portion of the depth map corresponding to the cropped (removed) portion of the main image may be deleted. For a zoom operation, the depth map may be updated based on the zoom applied to the modified main image. Block 528 may be followed by block 530.
In block 530, the file container may be updated to store the modified main image and the updated depth map.
In some implementations, block 510 may be followed by block 542. In block 542, a three-dimensional image is generated, for example, using computer vision techniques. Image data obtained from a file container that may include a primary image, one or more secondary images, and depth information may be used to generate a three-dimensional image. For example, multiple depth maps may be included in the depth information, each depth map being generated from motion with a single camera at the time of capturing the image, and may be embedded in the image metadata. In another example, one or more depth cameras may be used to generate multiple depth maps. A 3D scene may be reconstructed using multiple depth maps. Block 542 may be followed by block 544.
In block 544, the three-dimensional image is displayed. Block 544 may be followed by block 546.
In block 546, a user input indicating a tilt operation or a pan operation may be received. For example, user input may be received via one or more sensors of the client device, such as a gyroscope, accelerometer, gesture sensor, touch screen, or other sensor. Block 546 may be followed by block 548.
In block 548, an updated three-dimensional image may be generated based on the user input. The three-dimensional nature of the image enables objects to have a fixed depth and enables the image to be displayed such that objects may occlude other objects in the image as they move around. In some embodiments, the effect may be similar to browsing a 3D stereoscopic model. Block 548 may be followed by block 550.
In block 550, the updated three-dimensional image may be displayed. In some implementations, block 550 may be followed by block 546 to receive further user input.
In some implementations, block 510 may be followed by block 562. In block 562, a primary image from the image data is caused to be displayed, for example, on a display screen of the client device. In some implementations, the primary image may be displayed in a user interface that enables a user to provide user input. Block 522 may be followed by block 524.
In block 564, a user input indicating a selection of one or more objects in the image may be received. For example, the user may select an object using a pointing device, a touch screen, a gesture interface, voice, and the like. Block 564 may be followed by block 566.
In block 566, a segmentation mask corresponding to the user selected object is generated. The segmentation mask may identify a subset of pixels of the main image that correspond to the object. A segmentation mask may be generated using a depth map comprising a plurality of depth values. In some implementations, each depth value may correspond to a particular pixel of the main image. In these embodiments, generating the segmentation mask may include selecting pixels of the main image having respective depth values within a threshold range of depth values. For example, the threshold range of depth values may be a range between a minimum depth value and a maximum depth value corresponding to an object identified, for example, using an image segmentation technique that utilizes depth information and information about a depth effect applied by a camera application used during image capture. The object image is obtained based on the segmentation mask, e.g. the object may comprise only a subset of pixels of the main image identified by the segmentation mask. For example, referring to fig. 2B, dark flowers in the depth image 230 may be used to segment out flowers in the image 220. Block 566 may be followed by block 568.
In block 568, a user interface may be provided that enables a user to select an object image and perform one or more operations, such as a cut operation to remove the object from the displayed image, a copy operation to copy the object, or a paste operation to insert the object into a different image or at a different location in the displayed image.
Fig. 6 is a block diagram of an example device 600 that may be used to implement one or more features described herein. In one example, device 600 may be used to implement a client device, such as any of the client devices (120, 122, 124, 126) shown in fig. 1. Alternatively, device 600 may implement a server device, such as server 104. In some implementations, the device 600 can be a device for implementing a client device, a server device, or both a client and server device. Device 600 may be any suitable computer system, server, or other electronic or hardware device as described above.
One or more of the methods described herein may be run in a stand-alone program executable on any type of computing device, a program running on a web browser, a mobile application ("app") running on a mobile computing device (e.g., a cell phone, a smartphone, a tablet, wearable device (wrist watch, armband, jewelry, headwear, virtual reality goggles or glasses, augmented reality goggles or glasses, head mounted display, etc.), laptop computer, etc.). In one example, a client/server architecture may be used, for example, a mobile computing device (as a client device) to send user input data to a server device and receive final output data from the server for output (e.g., for display). In another example, all of the computations may be performed within the mobile app (and/or other apps) on the mobile computing device. In another example, the computing may be split between the mobile computing device and one or more server devices.
In some implementations, the device 600 includes a processor 602, a memory 604 and an input/output (I/O) interface 606, as well as a camera 616. In some implementations, the camera 616 may include multiple cameras or imaging sensors. For example, the cameras 616 may include a front-facing camera and a back-facing camera, such as when the device 600 is a mobile phone, tablet, or computer device. In another example, the cameras 616 may include, for example, cameras that capture RGB images, depth cameras that capture depth information using, for example, infrared or other techniques, grayscale cameras, monochrome cameras, and so forth.
The processor 602 may be one or more processors and/or processing circuits that execute program code and control basic operation of the device 600. A "processor" includes any suitable hardware system, mechanism, or component that processes data, signals, or other information. A processor may include a system with a general purpose Central Processing Unit (CPU) with one or more cores (e.g., in a single-core, dual-core, or multi-core configuration), multiple processing units (e.g., in a multi-processor configuration), a Graphics Processing Unit (GPU), a Field Programmable Gate Array (FPGA), an Application Specific Integrated Circuit (ASIC), a Complex Programmable Logic Device (CPLD), a special purpose circuit for implementing functionality, a special purpose processor for implementing neural network model-based processing, a neural circuit, a processor optimized for matrix computation (e.g., matrix multiplication), or other system. In some implementations, the processor 602 may include one or more coprocessors that implement neural network processing. In some embodiments, processor 602 may be a processor that processes data to generate probabilistic outputs, e.g., the outputs generated by processor 602 may be inaccurate or may be accurate within a range from expected outputs. Processing need not be limited to a particular geographic location or have temporal limitations. For example, a processor may perform its functions "in real-time," "offline," in a "batch mode," and so forth. Portions of the processing may be performed at different times and at different locations by different (or the same) processing systems. The computer may be any processor in communication with a memory.
Memory 604 is typically provided in device 600 for access by processor 602, and may be any suitable processor-readable storage medium suitable for storing instructions for execution by the processor and located separate from and/or integrated with processor 602, such as Random Access Memory (RAM), Read Only Memory (ROM), electrically erasable read-only memory (EEPROM), flash memory, and the like. The memory 604 may store software operated by the processor 602 on the server device 600, including an operating system 608, a machine learning application 630, other applications 612, and application data 614.
In various implementations, the machine learning application 630 may utilize a bayesian classifier, a support vector machine, a neural network, or other learning techniques. In some implementations, the machine learning application 630 can include a training model 634, an inference engine 636, and data 632.
In some implementations, the data 632 may include training data, e.g., data used to generate the training model 634. For example, the training data may include any type of data, such as text, images, audio, video, and so forth. The training data may be obtained from any source, such as data repositories specifically labeled for training, data for which permissions are provided for use as training data for machine learning, and so forth. In embodiments where one or more users are permitted to train a machine learning model, such as training model 634, using their respective user data, the training data may include such user data. In embodiments where users license their respective user data, data 632 may include licensed data, such as image/video or image/video metadata (e.g., video, data related to sharing video with other users, tags associated with video, whether video-based authoring (such as video collage, story, etc.) was generated from video, etc.), communications (e.g., email; chat data such as text messages, voice, video, etc.), documents (e.g., spreadsheets, text documents, presentations, etc.).
In some implementations, the training data can include composite data generated for training, such as data that is not based on user input or activity in the context being trained, e.g., data generated from analog or computer-generated video or the like. In some implementations, the machine learning application 630 excludes the data 632. For example, in these embodiments, the training models 634 may be generated, e.g., on different devices, and the training models 634 provided as part of the machine learning application 630. In various embodiments, the training model 634 may be provided as a data file that includes a model structure or form (e.g., which defines the number and type of neural network nodes, connectivity between nodes, and organization of nodes into multiple layers) and associated weights. Inference engine 636 may read data files for training model 634 and implement a neural network with node connectivity, layers, and weights based on the model structure or form specified in training model 634.
The machine learning application 630 also includes a training model 634. In some embodiments, the training model may include one or more model forms or structures. For example, the model form or structure may include any type of neural network, such as a linear network, a deep neural network implementing multiple layers (e.g., a "hidden layer" between an input layer and an output layer, where each layer is a linear network), a convolutional neural network (e.g., a network that splits or divides input data into multiple portions or blocks, processes each block separately using one or more neural network layers, and aggregates the results from the processing of each block), a sequence-to-sequence neural network (e.g., a network that receives sequential data, such as words in sentences, frames in video, etc., as input and produces a sequence of results as output), and so forth.
The model form or structure may specify the connectivity between various nodes and the organization of the node into layers. For example, a node of a first tier (e.g., input tier) may receive data as input data 632 or application data 614. For example, when the training model is used for analysis, e.g., including a video comprising a plurality of frames, such data may comprise, e.g., one or more pixels per node. Subsequent middle layers may receive as input the output of nodes of a previous layer in terms of connectivity specified in the individual model forms or structures. These layers may also be referred to as hidden layers. The final layer (e.g., output layer) produces the output of the machine learning application. In some implementations, the model form or structure also specifies the number and/or type of nodes in each layer.
In various embodiments, the training models 634 may include one or more models. One or more of the models may include a plurality of nodes arranged in layers in a model structure or form. In some embodiments, a node may be a computing node without memory, e.g., configured to process an input of a cell to produce an output of the cell. The calculations performed by the nodes may include, for example, multiplying each of a plurality of node inputs by a weight, obtaining a weighted sum, and adjusting the weighted sum with an offset or intercept value to produce a node output. In some embodiments, the calculation performed by the node may further include applying a step/activation function to the adjusted weighted sum. In some embodiments, the step/activation function may be a non-linear function. In various embodiments, such calculations may include operations such as matrix multiplication. In some embodiments, computations performed by multiple nodes may be performed in parallel, e.g., using multiple processor cores of a multi-core processor, using individual processing units of a GPU, or dedicated neural circuitry. In some implementations, a node may include memory, e.g., may be able to store and use one or more earlier inputs when processing subsequent inputs. For example, the nodes with memory may include Long Short Term Memory (LSTM) nodes. LSTM nodes may use memory to maintain "states" that permit the nodes to act like Finite State Machines (FSMs). Models with such nodes may be useful in processing sequential data, such as words in sentences or paragraphs, frames in video, speech or other audio, and so forth.
In some embodiments, training model 634 may include an embedding or weighting for each node. For example, the model may be launched as a plurality of nodes organized into layers as specified by the model form or structure. At initialization, a respective weight may be applied to the connections between each pair of nodes connected in the form of a model, e.g., nodes in successive layers of a neural network. For example, the respective weights may be randomly assigned or initialized to default values. The model may then be trained, for example, using data 632 to produce results.
For example, training may include applying supervised learning techniques. In supervised learning, training data may include a plurality of inputs (e.g., a set of videos) and a corresponding expected output for each input (e.g., one or more labels for each video). The values of the weights are automatically adjusted based on a comparison of the output of the model with the expected output, e.g., in a manner that increases the probability that the model will produce the expected output when provided with a similar input.
In some embodiments, training may include applying unsupervised learning techniques. In unsupervised learning, only input data may be provided and a model may be trained to distinguish the data, e.g., to cluster the input data into a plurality of groups, where each group includes input data that is similar in some way.
In various embodiments, the training model includes a set of weights or embeddings corresponding to the model structure. In embodiments where the data 632 is omitted, the machine learning application 630 may include a training model 634 based on prior training, e.g., by a developer of the machine learning application 630, by a third party, etc. In some embodiments, training model 634 may include a fixed set of weights, for example, downloaded from a server that provides the weights.
The machine learning application 630 also includes an inference engine 636. The inference engine 636 is configured to apply the training model 634 to data, such as application data 614 (e.g., video), to provide inferences. In some implementations, the inference engine 636 can include software code to be executed by the processor 602. In some implementations, the inference engine 636 can specify a circuit configuration (e.g., for a programmable processor, for a Field Programmable Gate Array (FPGA), etc.) that enables the processor 602 to apply the training model. In some implementations, the inference engine 636 can include software instructions, hardware instructions, or a combination. In some implementations, inference engine 636 can provide an Application Programming Interface (API) that can be used by operating system 608 and/or other applications 612 to invoke inference engine 636, e.g., to apply training model 634 to application data 614 to generate inferences.
In some implementations, the machine learning application 630 can be implemented in an offline manner. In these embodiments, the training model 634 may be generated in a first stage and provided as part of the machine learning application 630. In some implementations, the machine learning application 630 can be implemented in an online manner. For example, in such embodiments, an application (e.g., operating system 608, one or more of other applications 612, etc.) invoking machine learning application 630 can leverage inferences produced by machine learning application 630, e.g., provide inferences to a user, and can generate a system log (e.g., generate actions to be taken by the user based on the inferences if permitted by the user, or generate results of further processing if used as input for further processing). The system logs may be generated periodically, e.g., hourly, monthly, quarterly, etc., and may be used to update the training models 634 with user approval, e.g., to update the embedding for the training models 634.
In some implementations, the machine learning application 630 can be implemented in a manner that can be adapted to the particular configuration of the device 600 on which the machine learning application 630 is executed. For example, the machine learning application 630 may determine a computational graph that utilizes available computational resources, such as the processor 602. For example, if the machine learning application 630 is implemented as a distributed application on multiple devices, the machine learning application 630 may determine the computations to perform on the various devices in a manner that optimizes the computations. In another example, the machine learning application 630 may determine that the processor 602 includes a GPU having a particular number of GPU cores (e.g., 1000) and implementing the inference engine accordingly (e.g., as 1000 individual processes or threads).
In some implementations, the machine learning application 630 can implement an ensemble of training models. For example, the training model 634 may include multiple training models that each apply to the same input data. In these embodiments, the machine learning application 630 may choose a particular training model based on, for example, available computing resources, success rates of prior inferences, etc. In some implementations, the machine learning application 630 can execute the inference engine 636 such that a plurality of training models are applied. In these embodiments, the machine learning application 630 may combine the outputs from applying the various models, for example, using a voting technique that scores the various outputs from applying each training model or by choosing one or more particular outputs. Additionally, in these embodiments, the machine learning application may apply a time threshold (e.g., 0.5ms) for applying the respective training models and utilize only those respective outputs that are available within the time threshold. Outputs that are not received within the time threshold may not be utilized, e.g., discarded. Such an approach may be appropriate, for example, when a time limit is specified while a machine learning application is invoked, e.g., by operating system 608 or one or more applications 612. In some implementations, the machine learning application 630 can generate output based on a format specified by a calling application, such as the operating system 608 or one or more applications 612. In some implementations, the calling application can be another machine learning application. For example, such a configuration may be used in generating a countermeasure network, where output from the machine learning application 630 is used to train the invoking machine learning application and vice versa.
Any of the software in memory 604 may alternatively be stored on any other suitable storage location or computer readable medium. Further, memory 604 (and/or other connected storage devices) may store one or more messages, one or more taxonomies, electronic encyclopedias, dictionaries, thesauruses, knowledge bases, message data, grammars, user preferences, and/or other instructions and data used in the features described herein. Memory 604 and any other type of storage (magnetic disks, optical disks, tape, or other tangible media) may be considered "storage" or "storage devices.
The I/O interface 606 may provide functionality to enable the server device 600 to interface with other systems and devices. The docking device may be included as part of the device 600 or may be separate and in communication with the device 600. For example, network communication devices, storage devices (e.g., memory), and input/output devices may communicate via the I/O interface 606. In some implementations, the I/O interface can connect to interface devices such as input devices (keyboards, pointing devices, touch screens, microphones, cameras, scanners, sensors, etc.) and/or output devices (display devices, speaker devices, printers, motors, etc.).
Some examples of docking devices that may be connected to the I/O interface 606 may include one or more display devices 620, which display devices 620 may be used to display content, such as images, videos, and/or user interfaces of output applications as described herein. Display device 620 may be connected to device 600 via a local connection (e.g., a display bus) and/or via a networked connection and may be any suitable display device. Display device 620 may include any suitable display device, such as an LCD, LED, or plasma display screen, CRT, television, monitor, touch screen, 3-D display screen, or other visual display device. For example, the display device 620 may be a flat panel display screen provided on a mobile device, multiple display screens provided in a goggle or headphone device, or a monitor screen for a computer device.
The I/O interface 606 may interface with other input and output devices. Some examples include one or more cameras that can capture images. Some implementations may provide a microphone for capturing sound (e.g., as part of a captured image, voice command, etc.), an audio speaker device for outputting sound, or other input and output devices.
Camera 616 may be any type of camera that may capture video including a plurality of frames. A camera as used herein may include any image capture device. In some implementations, the camera 616 may include multiple lenses or imaging sensors with different capabilities, e.g., front to back, different zoom levels, image resolution of captured images, etc. In some implementations, the device 600 may include one or more sensors, such as a depth sensor, an accelerometer, a location sensor (e.g., GPS), a gyroscope, and so forth. In some implementations, one or more sensors may operate with a camera to obtain sensor readings corresponding to different frames of video captured using the camera.
For ease of illustration, fig. 6 shows one block for each of processor 602, memory 604, I/O interface 606, camera 616, and software blocks 608, 612, and 630. These blocks may represent one or more processors or processing circuits, operating systems, memories, I/O interfaces, applications, and/or software modules. In other embodiments, device 600 may not have all of the components shown and/or may also have other elements including other types of elements instead of or in addition to those shown herein. Although some components are described as performing blocks and operations as described in some embodiments herein, any suitable components or combinations of components of network environment 100, device 600, similar systems, or any suitable one or more processors associated with such systems may perform the described blocks and operations.
The methods described herein may be implemented by computer program instructions or code executable on a computer. For example, the code may be embodied by one or more digital processors (e.g., microprocessors or other processing circuits) and may be stored on a computer program product including a non-transitory computer-readable medium (e.g., a storage medium), such as a magnetic, optical, electromagnetic, or semiconductor storage medium, including a semiconductor or solid state memory, magnetic tape, a removable computer diskette, a Random Access Memory (RAM), a read-only memory (ROM), a flash memory, a rigid magnetic disk, an optical disk, a solid state memory drive, or the like. The program instructions may also be embodied in and provided as electronic signals, for example in the form of software as a service (SaaS) delivered from a server (e.g., a distributed system and/or a cloud computing system). Alternatively, one or more of the methods may be implemented in hardware (logic gates, etc.) or in a combination of hardware and software. Example hardware may be a programmable processor (e.g., a Field Programmable Gate Array (FPGA), a complex programmable logic device), a general purpose processor, a graphics processor, an Application Specific Integrated Circuit (ASIC), etc. One or more methods may be performed as part or component of an application running on the system, or as an application or software running in conjunction with other applications and operating systems.
While the specification has been described with respect to specific embodiments thereof, these specific embodiments are merely illustrative, and not restrictive. The concepts illustrated in the examples may be applied to other examples and embodiments.
Certain embodiments discussed herein may provide a user with one or more opportunities to control whether to collect information, whether to store personal information, whether to use personal information, and how to collect, store, and use information about the user in the event that personal information about the user (e.g., user data, information about the user's social network, the user's location and time at the location, biometric characteristic information of the user, activity and demographic information of the user) is collected or used. That is, the systems and methods discussed herein collect, store, and/or use user personal information, particularly upon receiving explicit authorization to do so from an associated user.
For example, the user is provided with control over whether the program or feature collects user information about that particular user or other users related to the program or feature. Each user for whom personal information is to be collected is presented with one or more options to allow control of information collection in relation to that user to provide permission or authorization as to whether information is collected and what portions of the information are to be collected. For example, one or more such control options may be provided to the user over a communications network. In addition, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is removed. As one example, the identity of the user may be processed such that personally identifiable information cannot be determined. As another example, the geographic location of the user may be generalized to a larger area such that a particular location of the user cannot be determined.
It is noted that the functional blocks, operations, features, methods, devices, and systems described in this disclosure can be integrated or divided into different combinations of systems, devices, and functional blocks, as will be known to those skilled in the art. The routines of particular embodiments may be implemented using any suitable programming language and programming techniques. Different programming techniques, such as procedural or object oriented, may be employed. The routines can execute on a single processing device or multiple processors. Although the steps, operations, or computations may be presented in a specific order, this order may be changed in different specific embodiments. In some embodiments, multiple steps or operations shown as sequential in this specification may be performed at the same time.
Claims (24)
1. A computer-implemented method, comprising:
capturing image data using one or more cameras, wherein the image data includes a main image and associated depth values;
encoding the image data in an image format, wherein the encoded image data comprises:
the primary image encoded in the image format; and
image metadata, said image metadata comprising:
a device element including a profile element indicating a type of image; and
a first camera element, wherein the first camera element comprises an image element and a depth map based on the depth values;
after the encoding, storing the image data in a file container based on the image format; and
causing the primary image to be displayed.
2. The computer-implemented method of claim 1, wherein the depth map comprises a plurality of pixel values, each pixel value indicating a respective distance from a particular camera of the one or more cameras.
3. The computer-implemented method of claim 2, wherein the depth map further comprises a lens focus model that defines respective radius values of circles of confusion corresponding to a plurality of distances from the particular camera.
4. The computer-implemented method of claim 3, wherein the plurality of distances includes a near distance and a far distance, wherein the near distance is less than the far distance.
5. The computer-implemented method of claim 4, wherein the plurality of distances further comprises a focal length that is greater than the near distance and less than the far distance.
6. The computer-implemented method of any of claims 2 to 5, further comprising obtaining the plurality of pixel values by converting the depth values to an integer format and compressing the converted depth values based on the image format.
7. The computer-implemented method of any of claims 2 to 6, wherein the image data further comprises one or more secondary images, wherein each secondary image is captured by a respective camera of the one or more cameras that is different from the particular camera.
8. The computer-implemented method of claim 7, wherein:
the image metadata further includes one or more additional camera elements, wherein each additional camera element corresponds to at least one of the one or more secondary images and includes a respective image element that includes a pointer to the at least one image, wherein the one or more additional camera elements are organized sequentially in the image metadata after the first camera element, and
wherein storing the image data in the file container comprises concatenating the primary image and the one or more secondary images encoded in the image format, wherein an order of the primary image and the one or more secondary images in the file container is the same as an order of the one or more additional camera elements in the image metadata.
9. The computer-implemented method of any of claims 1 to 6, wherein the image data further comprises one or more secondary images, each of the one or more secondary images captured by a respective camera of the one or more cameras.
10. The computer-implemented method of claim 9, wherein the one or more secondary images comprise a depth image or video captured by a depth camera.
11. A computer-implemented method, comprising:
obtaining image data from a file container, the image data comprising a plurality of pixel values corresponding to a primary image and image metadata comprising a lens focus model;
causing the primary image to be displayed;
receiving a user input indicating a target focal length;
in response to the user input, modifying one or more pixel values of the main image based on the lens focal length model to obtain a modified image in which an object at the target focal length in the modified image is in focus; and
causing the modified image to be displayed.
12. The computer-implemented method of claim 11, wherein the lens focus model defines respective radius values of circle of confusion corresponding to a plurality of distances from a camera capturing the main image, and wherein modifying the one or more pixel values blurs one or more objects in the main image, wherein the one or more objects are associated with depth data indicating that the one or more objects are at a different distance from the camera capturing the main image than the target focal distance.
13. The computer-implemented method of claim 11, wherein the lens focus model is stored in the file container in a depth map element in the image metadata, wherein the depth map element corresponds to a camera that captured the primary image.
14. A computer-implemented method, comprising:
obtaining image data from a file container, the image data comprising a plurality of pixel values corresponding to a main image and image metadata comprising a depth map;
causing the primary image to be displayed;
receiving user input indicative of at least one of: a cropping operation for the main image or a scaling operation for the main image;
modifying the primary image based on the user input to obtain a modified image;
calculating an updated depth map, wherein the updated depth map comprises depth data corresponding to the modified image; and
updating the file container to replace the main image with the modified image and to replace the depth map with the updated depth map.
15. A computer-implemented method, comprising:
obtaining image data from a file container, the image data comprising a plurality of pixel values and image metadata comprising a plurality of depth maps, each pixel value corresponding to a particular pixel of a plurality of pixels of a main image;
generating a three-dimensional image based on the main image and the plurality of depth maps; and
causing the three-dimensional image to be displayed.
16. The computer-implemented method of claim 15, further comprising:
receiving a user input indicating a tilt operation or a pan operation for the three-dimensional image;
in response to receiving the user input, generating an updated three-dimensional image based on the user input, the primary image, and the plurality of depth maps; and
causing the updated three-dimensional image to be displayed.
17. The computer-implemented method of claim 15 or 16, wherein the plurality of depth maps are obtained via at least one of: depth from motion technology using a single camera or one or more depth cameras.
18. A computer-implemented method, comprising:
obtaining image data from a file container, the image data comprising a plurality of pixel values and image metadata comprising a depth map, each pixel value corresponding to a respective pixel of a plurality of pixels of a main image;
causing the primary image to be displayed;
receiving a user input indicating a selection of an object depicted in the primary image;
generating a segmentation mask based on the depth map, wherein the segmentation mask identifies a subset of pixels of the main image that correspond to the object;
obtaining an object image of the object based on the segmentation mask; and
providing a user interface that enables selection of the object image.
19. The computer-implemented method according to claim 18, wherein the depth map comprises a plurality of depth values, each of which corresponds to a particular pixel of the main image, and wherein generating the segmentation mask comprises selecting pixels of the main image having respective depth values within a threshold range of depth values.
20. A computing device comprising means for performing the method of any of claims 1-19.
21. A computing system comprising means for performing the method of any of claims 1-19.
22. A computer-readable storage medium comprising instructions that, when executed by one or more processors, cause the one or more processors to perform the method of any one of claims 1-19.
23. A computer program comprising instructions which, when executed by one or more processors, cause the one or more processors to carry out the method according to any one of claims 1 to 19.
24. A computing device, comprising:
one or more processors; and
a memory coupled to the one or more processors having instructions stored thereon that, when executed by the one or more processors, cause the one or more processors to perform the method of any of claims 1-19.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202311121209.9A CN117221512A (en) | 2019-04-01 | 2019-11-19 | Techniques for capturing and editing dynamic depth images |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962827739P | 2019-04-01 | 2019-04-01 | |
US62/827,739 | 2019-04-01 | ||
PCT/US2019/062156 WO2020205003A1 (en) | 2019-04-01 | 2019-11-19 | Techniques to capture and edit dynamic depth images |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202311121209.9A Division CN117221512A (en) | 2019-04-01 | 2019-11-19 | Techniques for capturing and editing dynamic depth images |
Publications (2)
Publication Number | Publication Date |
---|---|
CN112868224A true CN112868224A (en) | 2021-05-28 |
CN112868224B CN112868224B (en) | 2023-08-29 |
Family
ID=68848465
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980059947.0A Active CN112868224B (en) | 2019-04-01 | 2019-11-19 | Method, apparatus and storage medium for capturing and editing dynamic depth image |
CN202311121209.9A Pending CN117221512A (en) | 2019-04-01 | 2019-11-19 | Techniques for capturing and editing dynamic depth images |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202311121209.9A Pending CN117221512A (en) | 2019-04-01 | 2019-11-19 | Techniques for capturing and editing dynamic depth images |
Country Status (6)
Country | Link |
---|---|
US (1) | US11949848B2 (en) |
EP (1) | EP3744088A1 (en) |
JP (2) | JP7247327B2 (en) |
KR (2) | KR102647336B1 (en) |
CN (2) | CN112868224B (en) |
WO (1) | WO2020205003A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11106637B2 (en) * | 2019-05-20 | 2021-08-31 | 5Th Kind, Inc. | Metadata-driven tiered storage |
CN113286073A (en) * | 2020-02-19 | 2021-08-20 | 北京小米移动软件有限公司 | Imaging method, imaging device, and storage medium |
US20220188547A1 (en) * | 2020-12-16 | 2022-06-16 | Here Global B.V. | Method, apparatus, and computer program product for identifying objects of interest within an image captured by a relocatable image capture device |
US11893668B2 (en) | 2021-03-31 | 2024-02-06 | Leica Camera Ag | Imaging system and method for generating a final digital image via applying a profile to image information |
US20230328216A1 (en) * | 2022-04-06 | 2023-10-12 | Samsung Electronics Co., Ltd. | Encoding Depth Information for Images |
Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102257827A (en) * | 2008-12-19 | 2011-11-23 | 皇家飞利浦电子股份有限公司 | Creation of depth maps from images |
CN103329548A (en) * | 2010-12-27 | 2013-09-25 | 3D媒体公司 | Primary and auxiliary image capture devices for image processing and related methods |
CN104081414A (en) * | 2011-09-28 | 2014-10-01 | 派力肯影像公司 | Systems and methods for encoding and decoding light field image files |
US20150039621A1 (en) * | 2013-08-05 | 2015-02-05 | Nvidia Corporation | Method for capturing the moment of the photo capture |
CN105052124A (en) * | 2013-02-21 | 2015-11-11 | 日本电气株式会社 | Image processing device, image processing method and permanent computer-readable medium |
CN105165008A (en) * | 2013-05-10 | 2015-12-16 | 皇家飞利浦有限公司 | Method of encoding video data signal for use with multi-view rendering device |
CN105519105A (en) * | 2013-09-11 | 2016-04-20 | 索尼公司 | Image processing device and method |
US20160205341A1 (en) * | 2013-08-20 | 2016-07-14 | Smarter Tv Ltd. | System and method for real-time processing of ultra-high resolution digital video |
CN106233329A (en) * | 2014-04-24 | 2016-12-14 | 高通股份有限公司 | 3D draws generation and the use of east image |
CN106464860A (en) * | 2014-07-10 | 2017-02-22 | 英特尔公司 | Storage of depth information in a digital image file |
CN106576160A (en) * | 2014-09-03 | 2017-04-19 | 英特尔公司 | Imaging architecture for depth camera mode with mode switching |
CN108107571A (en) * | 2013-10-30 | 2018-06-01 | 株式会社摩如富 | Image processing apparatus and method and non-transitory computer readable recording medium |
WO2018129692A1 (en) * | 2017-01-12 | 2018-07-19 | Intel Corporation | Image refocusing |
Family Cites Families (27)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7480382B2 (en) | 2003-09-30 | 2009-01-20 | Microsoft Corporation | Image file container |
JP5096048B2 (en) | 2007-06-15 | 2012-12-12 | 富士フイルム株式会社 | Imaging apparatus, stereoscopic image reproduction apparatus, and stereoscopic image reproduction program |
US20090066693A1 (en) | 2007-09-06 | 2009-03-12 | Roc Carson | Encoding A Depth Map Into An Image Using Analysis Of Two Consecutive Captured Frames |
US8154647B2 (en) | 2008-03-05 | 2012-04-10 | Applied Minds, Llc | Automated extended depth of field imaging apparatus and method |
US8289440B2 (en) | 2008-12-08 | 2012-10-16 | Lytro, Inc. | Light field data acquisition devices, and methods of using and manufacturing same |
JP2011077764A (en) | 2009-09-30 | 2011-04-14 | Fujifilm Corp | Multidimensional image processing device, multidimensional image photographing system, multidimensional image printed matter and multidimensional image processing method |
JP5299214B2 (en) | 2009-10-20 | 2013-09-25 | ソニー株式会社 | Image processing apparatus, image processing method, and program |
US8949913B1 (en) | 2010-09-16 | 2015-02-03 | Pixia Corp. | Method of making a video stream from a plurality of viewports within large format imagery |
JP6032879B2 (en) | 2011-10-03 | 2016-11-30 | キヤノン株式会社 | Imaging information output device and lens device having the same |
EP2817955B1 (en) | 2012-02-21 | 2018-04-11 | FotoNation Cayman Limited | Systems and methods for the manipulation of captured light field image data |
RU2012138174A (en) * | 2012-09-06 | 2014-03-27 | Сисвел Текнолоджи С.Р.Л. | 3DZ TILE FORMAT DIGITAL STEREOSCOPIC VIDEO FLOW FORMAT METHOD |
CN104685860A (en) | 2012-09-28 | 2015-06-03 | 派力肯影像公司 | Generating images from light fields utilizing virtual viewpoints |
US9275078B2 (en) * | 2013-09-05 | 2016-03-01 | Ebay Inc. | Estimating depth from a single image |
JP6476658B2 (en) | 2013-09-11 | 2019-03-06 | ソニー株式会社 | Image processing apparatus and method |
JP2015075675A (en) | 2013-10-10 | 2015-04-20 | キヤノン株式会社 | Imaging apparatus |
KR102082132B1 (en) * | 2014-01-28 | 2020-02-28 | 한국전자통신연구원 | Device and Method for new 3D Video Representation from 2D Video |
US9934252B2 (en) | 2014-03-10 | 2018-04-03 | Microsoft Technology Licensing, Llc | Metadata-based photo and/or video animation |
US9922680B2 (en) | 2015-02-10 | 2018-03-20 | Nokia Technologies Oy | Method, an apparatus and a computer program product for processing image sequence tracks |
CN105163042B (en) | 2015-08-03 | 2017-11-03 | 努比亚技术有限公司 | A kind of apparatus and method for blurring processing depth image |
US10554956B2 (en) * | 2015-10-29 | 2020-02-04 | Dell Products, Lp | Depth masks for image segmentation for depth-based computational photography |
US20170213383A1 (en) | 2016-01-27 | 2017-07-27 | Microsoft Technology Licensing, Llc | Displaying Geographic Data on an Image Taken at an Oblique Angle |
US11189065B2 (en) | 2017-04-17 | 2021-11-30 | Intel Corporation | Editor for images with depth data |
US10373362B2 (en) * | 2017-07-06 | 2019-08-06 | Humaneyes Technologies Ltd. | Systems and methods for adaptive stitching of digital images |
JP7105062B2 (en) * | 2017-12-21 | 2022-07-22 | 株式会社ソニー・インタラクティブエンタテインメント | Image processing device, content processing device, content processing system, and image processing method |
US10643375B2 (en) * | 2018-02-26 | 2020-05-05 | Qualcomm Incorporated | Dynamic lighting for objects in images |
WO2019173672A1 (en) * | 2018-03-08 | 2019-09-12 | Simile Inc. | Methods and systems for producing content in multiple reality environments |
US11741675B2 (en) * | 2020-03-10 | 2023-08-29 | Niantic, Inc. | Determining traversable space from single images |
-
2019
- 2019-11-19 KR KR1020227037003A patent/KR102647336B1/en active IP Right Grant
- 2019-11-19 US US17/422,734 patent/US11949848B2/en active Active
- 2019-11-19 CN CN201980059947.0A patent/CN112868224B/en active Active
- 2019-11-19 KR KR1020217007087A patent/KR102461919B1/en active IP Right Grant
- 2019-11-19 EP EP19818412.9A patent/EP3744088A1/en active Pending
- 2019-11-19 JP JP2021514516A patent/JP7247327B2/en active Active
- 2019-11-19 CN CN202311121209.9A patent/CN117221512A/en active Pending
- 2019-11-19 WO PCT/US2019/062156 patent/WO2020205003A1/en unknown
-
2023
- 2023-03-15 JP JP2023040756A patent/JP2023085325A/en active Pending
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102257827A (en) * | 2008-12-19 | 2011-11-23 | 皇家飞利浦电子股份有限公司 | Creation of depth maps from images |
CN103329548A (en) * | 2010-12-27 | 2013-09-25 | 3D媒体公司 | Primary and auxiliary image capture devices for image processing and related methods |
CN104081414A (en) * | 2011-09-28 | 2014-10-01 | 派力肯影像公司 | Systems and methods for encoding and decoding light field image files |
CN105052124A (en) * | 2013-02-21 | 2015-11-11 | 日本电气株式会社 | Image processing device, image processing method and permanent computer-readable medium |
CN105165008A (en) * | 2013-05-10 | 2015-12-16 | 皇家飞利浦有限公司 | Method of encoding video data signal for use with multi-view rendering device |
US20150039621A1 (en) * | 2013-08-05 | 2015-02-05 | Nvidia Corporation | Method for capturing the moment of the photo capture |
US20160205341A1 (en) * | 2013-08-20 | 2016-07-14 | Smarter Tv Ltd. | System and method for real-time processing of ultra-high resolution digital video |
CN105519105A (en) * | 2013-09-11 | 2016-04-20 | 索尼公司 | Image processing device and method |
CN108107571A (en) * | 2013-10-30 | 2018-06-01 | 株式会社摩如富 | Image processing apparatus and method and non-transitory computer readable recording medium |
CN106233329A (en) * | 2014-04-24 | 2016-12-14 | 高通股份有限公司 | 3D draws generation and the use of east image |
CN106464860A (en) * | 2014-07-10 | 2017-02-22 | 英特尔公司 | Storage of depth information in a digital image file |
CN106576160A (en) * | 2014-09-03 | 2017-04-19 | 英特尔公司 | Imaging architecture for depth camera mode with mode switching |
WO2018129692A1 (en) * | 2017-01-12 | 2018-07-19 | Intel Corporation | Image refocusing |
Also Published As
Publication number | Publication date |
---|---|
KR20210041057A (en) | 2021-04-14 |
US20220132095A1 (en) | 2022-04-28 |
EP3744088A1 (en) | 2020-12-02 |
KR20220150410A (en) | 2022-11-10 |
CN117221512A (en) | 2023-12-12 |
KR102647336B1 (en) | 2024-03-14 |
JP2023085325A (en) | 2023-06-20 |
CN112868224B (en) | 2023-08-29 |
JP2022526053A (en) | 2022-05-23 |
US11949848B2 (en) | 2024-04-02 |
WO2020205003A1 (en) | 2020-10-08 |
KR102461919B1 (en) | 2022-11-01 |
JP7247327B2 (en) | 2023-03-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3791357B1 (en) | Generating and displaying blur in images | |
CN112868224B (en) | Method, apparatus and storage medium for capturing and editing dynamic depth image | |
EP3815042B1 (en) | Image display with selective depiction of motion | |
CN112740709A (en) | Gated model for video analysis | |
JP7257591B2 (en) | Personalized automatic video cropping | |
JP2023529380A (en) | Machine learning-based image compression settings that reflect user preferences | |
US11776201B2 (en) | Video lighting using depth and virtual lights | |
JP2024514728A (en) | Selective Image Blur Using Machine Learning |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |