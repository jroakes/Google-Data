EP3867901B1 - Speech processing - Google Patents
Speech processing Download PDFInfo
- Publication number
- EP3867901B1 EP3867901B1 EP19821483.5A EP19821483A EP3867901B1 EP 3867901 B1 EP3867901 B1 EP 3867901B1 EP 19821483 A EP19821483 A EP 19821483A EP 3867901 B1 EP3867901 B1 EP 3867901B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- language model
- computing device
- language
- transcriptions
- domain
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000012545 processing Methods 0.000 title description 12
- 238000013518 transcription Methods 0.000 claims description 209
- 230000035897 transcription Effects 0.000 claims description 209
- 238000000034 method Methods 0.000 claims description 45
- 230000015654 memory Effects 0.000 description 35
- 230000009471 action Effects 0.000 description 20
- 230000008569 process Effects 0.000 description 18
- 238000004891 communication Methods 0.000 description 17
- 238000013519 translation Methods 0.000 description 17
- 230000014616 translation Effects 0.000 description 17
- 238000012549 training Methods 0.000 description 10
- 230000004044 response Effects 0.000 description 9
- 238000005516 engineering process Methods 0.000 description 7
- 230000008901 benefit Effects 0.000 description 5
- 238000004590 computer program Methods 0.000 description 5
- 230000003993 interaction Effects 0.000 description 4
- 230000001413 cellular effect Effects 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 239000000047 product Substances 0.000 description 3
- 230000000875 corresponding effect Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 241001585714 Nola Species 0.000 description 1
- 241000220010 Rhode Species 0.000 description 1
- 238000004833 X-ray photoelectron spectroscopy Methods 0.000 description 1
- 210000001072 colon Anatomy 0.000 description 1
- 230000001276 controlling effect Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000018109 developmental process Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/45—Example-based machine translation; Alignment
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/005—Language recognition
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/187—Phonemic context, e.g. pronunciation rules, phonotactical constraints or phoneme n-grams
Definitions
- This specification generally relates to systems and methods for processing speech input.
- US 2012/278060 A1 discloses a system and method for building a language model for a translation. The method includes providing a first relative ranking of first and second translations in a target language of a same source string in a source language, determining a second relative ranking of the first and second translations using weights of a language model, the language model including a weight for each of a set of n-gram features, and comparing the first and second relative rankings to determine whether they are in agreement.
- US 2012/029904 A1 discloses a method and apparatus for adding new vocabulary to interactive translation and dialogue systems.
- NAKAJIMA H ET AL "Language Model Adaptation with Additional Text Generated by Machine Translation”
- COLING 2002 Proceedings of the 19th International Conference on Computational Linguistics August 30, 2002, Howard International House, Taipei, Taiwan, vol. 2 26 August 2002 (2002-08-26), pages 716-722,XP002683643 discloses a novel scheme that generates a small target corpus in the language of a model by machine translation of the target corpus in another language.
- WOOSUNG KIM AND SANJEEV KHUDANPUR ET AL "Language Model Adaptation Using Cross-Lingual Information", 20030901, (20030901 ) discloses investigations into language models for an automatic speech recognition system for Mandarin Broadcast News that exploit a large side-corpus of contemporaneous English news articles to adapt a static Chinese language model to the news story being transcribed.
- RAO P S ET AL "MDI Adaptation of Language Models Across Corpora", 5th European Conference on Speech Communication And Technology. EUROSPEECH '97. Rhodes, Greece, SEPT.
- US 2018/336895 A1 discloses systems, methods, devices, and other techniques are described herein for determining dialog states that correspond to voice inputs and for biasing a language model based on the determined dialog states.
- US 2016/140218 A1 discloses a voice search system that includes one or more computers programmed to receive a user's speech input from a user, convert the speech input into a text search query using a dynamic language model, receive search results responsive to the text search query, and provide the search results to the user in response to the speech input.
- ⁇ speech recognition quality may improve for a speech recognition system in a vehicle if the speech recognition system is adapted for use in the vehicle by being able to more quickly and accurately transcribe speech that includes words or phrases that the user is likely to speak in the vehicle.
- One technique used to adapt a speech recognition system to a particular domain may be to analyze the transcription logs received by the speech recognition system operating in the particular domain.
- the adaption process may include identifying the words or phrases that are more common in the transcription logs for the particular domain than in transcriptions logs of a general speech recognition system.
- a language model may be biased such that the language model is more likely to generate candidate transcriptions that include the words or phrases that are more common in the transcription logs for the particular domain.
- a language model adaption system may translate the domain-specific transcription logs into the target language.
- the language model adaption system may analyze the translated transcription logs to identify words and phrases that appear to be more common for users speaking to devices operating in the particular domain. With the common words identified, the language model adaption system may bias a general purpose language model for the target language such that the biased language model may be more likely to generate transcriptions that include the common words when processing audio data that may sound similar to other words in the target language.
- the biased language model it may be possible to launch a speech recognition system for the particular domain in the target language and benefit from improved speech recognition with improved accuracy and latency. Users may benefit from a speech recognition that is configured, at launch of the system, to more quickly and accurately recognize words and phrases that the users are more likely to say when speaking to a system operating in the particular domain.
- a method for processing speech input includes the actions of receiving, by a computing device, transcriptions of utterances that were received by computing devices operating in a domain and that are in a source language; generating, by the computing device, translated transcriptions of the transcriptions of the utterances in a target language; receiving, by the computing device, a language model for the target language; receiving, by the computing device, additional transcriptions of additional utterances that were received by the computing devices operating in domains other than the domain and that are in the source language; generating, by the computing device, additional translated transcriptions of the additional transcriptions; identifying terms that have a higher appearance frequency in the translated transcriptions than in the additional translated transcriptions; biasing, by the computing device, the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions; and generating, by the computing device while operating in the domain, a transcription of an utterance in the target language using the biased language model, wherein biasing the language model for
- the action of generating a transcription of an utterance in the target language using the biased language model includes, while the computing device is operating in the domain, receiving, by the computing device, audio data of the utterance; providing, by the computing device, the audio data as an input to an acoustic model that is configured to identify phonemes of the utterance; based on the computing device operating in the domain, providing, by the computing device, the phonemes of the utterance as an input to the biased language model; and, based on providing the phonemes of the utterance as the input to the biased language model, generating, by the computing device, a transcription of the utterance.
- the language model for the target language is a general language model.
- the biased language for the target language is a language model specific for computing devices operating in the domain.
- the actions include identifying, by the computing device, n-grams that appear in the translated transcription and an appearance frequency of each n-gram; and identifying, by the computing device, a subset of the n-grams that appear in the translated transcriptions more than a threshold appearance frequency.
- the computing device biases the language model by increasing the likelihood of the language model selecting the subset of n-grams.
- the action of increasing the likelihood of the language model selecting the subset of n-grams includes increasing, for each n-gram in the subset of n-grams, the likelihood by a larger amount based on a larger difference between the appearance frequency for the n-gram and the threshold appearance frequency.
- the actions include receiving, by the computing device, audio data of the utterances that were received by the computing device operating in the domain and that are in the source language.
- the action of receiving the transcriptions of the utterances that were received by the computing devices operating in the domain and that are in the source language includes generating, by the computing device, the transcriptions of the utterances that were received by computing devices operating in a domain and that are in a source language.
- the actions include receiving, by the computing device, grammars in the target language.
- the action of biasing the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions includes biasing the language model for the target language by increasing the likelihood of the language model selecting grammars that include the terms included in the translated transcriptions.
- FIG. 1 illustrates an example system 100 that biases a language model 105 for use in a specific domain using translated training data 110.
- the system 100 examines translated training data 110 to identify common words and phrases that are likely to be used by speakers in a specific domain.
- the system 100 biases a language model to favor those common words and phrases.
- a smart speaker 120 may use the biased language model 115 during the automated speech recognition process.
- the user 125 speaks a query 130 to the smart speaker 120, and the smart speaker 120 uses the biased language model 115 to generate a transcription of the query 130.
- the smart speaker 120 may generate a response 135 to the query 130.
- a speech recognizer may use an English acoustic model that is trained to identify English phonemes and an English language model that is configured to identify English words and phrases, which may be referred to as n-grams (e.g., unigram being a single word, a bigram being a two word phrase, etc.).
- n-grams e.g., unigram being a single word, a bigram being a two word phrase, etc.
- Another speech recognizer may use a Turkish acoustic model that is trained to identify Turkish phonemes and a Turkish language model that is configured to identify Turkish words and phrases.
- the transcription generated by the speech recognizer may not be accurate. Additionally, the computational resources required by a speech recognizer accessing multiple acoustic models and multiple language models may be too high, which would result in an unacceptable latency for the speaker.
- Speech recognizers may be adapted for different types of situations.
- a speech recognizer may be adapted for use in a vehicle, by a smart speaker without a display, by a smart speaker with a display, by a digital assistant application running on a mobile phone, by a digital assistant application running on a laptop computer, or any other similar application or situation.
- the application or situation may be referred to as the domain.
- a speech recognizer that is configured to recognize a particular language may be adapted for use in different domains.
- an English speech recognizer may be adapted for use by a digital assistant application running on a mobile phone or for use by a smart speaker with a display.
- a user interacting with the digital assistant application running on a mobile phone may be more likely to say a first set of n-grams, and a user interacting with a smart speaker with a display may be more likely to say a second set of n-grams that may or may not include some of the n-grams of the first set. Therefore, a developer may bias the language model of the speech recognizer for the digital assistant application running on a mobile phone domain to be more likely to recognize the first set of n-grams and bias the language model of the speech recognizer for the smart speaker with a display domain to be more likely to recognize the second set of n-grams.
- Identifying the n-grams to bias the language model may be accomplished by examining transcriptions of utterances received by a computing devices operating in a particular domain. For example, to identify the popular n-grams for the smart speaker without a display domain, the system 100 may examine the transcriptions of utterances received by smart speakers without a display. The system 100 may identify the most common n-grams and bias a language model to favor those n-grams. The smart speakers without a display may then perform automated speech recognition processes using the biased language model.
- This biasing process for a specific language may not be practical for a situation where transcriptions of utterances received by a computing devices operating in the particular domain are not available in the specific language.
- the transcriptions may not be available because the smart speaker without a display may not be available for purchase in the country where the population speaks the specific language. For example, the smart speaker without a display may not be available in Turkey. Therefore, there are likely very little if any transcriptions of people speaking Vietnamese to a smart speaker without a display.
- the system 100 may be able to leverage data from other languages to bias a language model of the target language.
- the system 100 may be able to leverage the English transcription data 140 to bias the Vietnamese language model 115. Doing so, may allow the application or product to launch with a Turkish language model that is biased, or adapted, toward the particular domain, such as a smart speaker without a display.
- the system 100 accesses English transcription data 140 that was received by computing devices operating in the home assistant domain.
- a device operating in the home assistant domain may include devices such as a smart speaker without a display. Other devices may operate in the home assistant domain, and transcriptions collected from those devices may be added to the English transcription data 140.
- the English transcription data 140 may include transcriptions 145 such as "what is the weather,” “play jazz music,” “volume up,” “lights on,” and transcriptions of other utterances received by devices operating in the home assistant domain.
- the system 100 may generate the English transcription data 140 by performing automated speech recognition on audio data of utterances received by devices operating in the home assistant domain. In some implementations, the system 100 may receive the English transcription data 140 from manual transcribers who transcribed the audio data of utterances received by devices operating in the home assistant domain. In some implementations, the English transcription data 140 may include a mix of both automated transcriptions and manual transcriptions.
- the system 100 provides the English transcription data 140 as an input to an English to Vietnamese translator 150.
- the English to Vietnamese translator 150 may be a machine translator that automatically translates the English transcription data 140 to Vietnamese transcription data 110.
- the English to Vietnamese translator 150 may be a manual translator.
- the Turkish transcription data 110 may include transcriptions 155 such as, “hava nas l?”, “caz müzikangolmak,” “sesi aç,” and “yaniyor,” which may be Turkish translations of "what is the weather,” “play jazz music,” “volume up,” and “lights on,” respectively.
- the transcriptions 155 may include additional translated transcriptions of the English transcription data 140.
- the English to Vietnamese translator 150 may make errors in translating the English transcription data 140.
- the system 100 may translate additional English transcription data transcribed from other utterances received in a variety of domains and/or in a general domain.
- the system 100 may provide the general English transcription data as an input to the English to Vietnamese translator 150.
- the system 100 may compare the Turkish transcription data 110 to the general Vietnamese transcription data to identify n-grams that may be more common in the home assistant domain. Using both domain specific translated transcriptions and general translated transcriptions may allow the system 100 to remove some of the errors that occur during translation. Because some errors may be likely to occur when translating domain specific transcriptions as when translating general transcriptions, those errors may cancel out and the system may not identify n-grams related to the errors as more likely to occur in a specific domain.
- the system 100 provides the Vietnamese transcription data 110 and a Turkish language model 105 as an input to the language model adaptor 160.
- the language model adaptor 160 may be configured to bias the Turkish language model 105 to generate the Turkish language model 170 that is adapted for the home assistant domain.
- the Turkish language model 105 may be a general Vietnamese language model that is not biased to any particular n-grams 165.
- the language model adaptor 160 may adapt the Vietnamese language model 105 for the home assistant domain by increasing the weights of the n-grams of the Vietnamese transcription data 110. For example, the language model adaptor 160 may increase the weight for the n-gram "hava nas l" such that if a Turkish speaker says something that sounds similar to "hava nas l," the Vietnamese language model 170 may be more likely to select "hava nas l" as the transcription even if the general Turkish language model 105 would not.
- the language model adaptor 160 may adapt the Vietnamese language model 170 to be more likely to recognize n-grams 175 such as "hava nas l?", “caz müzikangolmak,” “sesi aç,” “yamyor,” and other n-grams included in the Vietnamese transcriptions 110. In some implementations, the language model adaptor 160 may adapt the Turkish language model 170 according to the appearance frequency of n-grams in the Turkish transcriptions 110.
- the language model adaptor 160 may bias the Turkish language model 170 more heavily towards “hava nas l” than “sesi aç.” In some implementations, the language model adaptor 160 may bias the Turkish language model 170 towards the different terms of each n-gram 175. For example, the language model adaptor 160 may bias towards both "hava” and “nas l.” In some instances the language model adaptor 160 may bias towards "hava nas l" more than "hava” and "nas l" individually.
- the smart speaker 120 accesses the Vietnamese language model 170 during processing of the utterance 130.
- the smart speaker 120 may be operating in the home assistant domain, and, therefore, accesses the Turkish language model 170 adapted for the home assistant domain.
- the smart speaker 120 may prompt the user 125 by outputting, "Merhaba, sana nasil yardim edebilirim?"
- the smart speaker 120 may prompt the user 125 in response to the user 125 touching the smart speaker 120, speaking a predetermined hotword, e.g., "ok computer” or a similar Turkish phrase, or any other similar action.
- the user 125 may responds with the utterance 130, "hava nas l?” In some implementations, the user 125 may speak a hotword followed by the utterance 130 without the smart speaker outputting prompt 180.
- the smart speaker 120 receives the utterance 130 and performs automated speech recognition using a Turkish acoustic model and the Turkish language model 170 that is adapted for the home assistant domain.
- the smart speaker 120 initiates an action based on the utterance 130.
- the action may be generating a query for the weather in the location of the smart speaker 120.
- the smart speaker 120 may receive a response to the query and generate the response 135, "Güne li, 20 derece.”
- FIG. 2 illustrates an example system 200 that biases a language model 202 for use in a specific domain using both translated training data 204 and pre-selected grammars 206.
- the system 200 compares pre-selected grammars 206 and translated training data 204 to identify common words and phrases that are likely to be used by speakers in a specific domain.
- the system 200 biases a language model to favor the terms in the grammars 206 and the translated training data.
- a vehicle equipped with an automated digital assistant 208 may use the biased language model 210 during the automated speech recognition process.
- the user 212 speaks a query 214 to the automated digital assistant 208, and the automated digital assistant 208 uses the biased language model 210 to generate a transcription of the query 214.
- the automated digital assistant 208 may generate a response 216 to the query 214.
- system 200 may be configured to generate a biased language model 210 for a language in which the system 200 does not have any access to training data in the language.
- the system 200 generates a biased Basque language model 210 for the vehicle assistant domain despite only having access to English transcriptions 218 received by devices operating in the vehicle assistant domain.
- the system 200 accesses pre-programmed grammars 206.
- a developer may include a set of grammars 206 that the speech recognizer can access to assist in determining the action requested by the user.
- the developer may include a set of grammars 206 to ensure that the user can interact with the device.
- the device may indicate the syntax of the grammars 206 and the action for each grammar by displaying the syntax on a display, outputting audio of examples, or any other similar way of outputting data.
- a grammar may be different than the vocabulary that the language model selects words from.
- a grammar is related to the structure of a transcription of an utterance. If the transcription matches the structure of a grammar, then the system 200 performs the action that corresponds to that grammar.
- a grammar may be any structure of words that can be described using a common notation technique, for example, Bakus-Naur form.
- Each grammar may correspond to a specific user intent. For example, the user intent may be to issue a home automation command or a media playing command.
- One example of a grammar may include a grammar for an alarm.
- a speech recognition system uses the grammar to parse the transcription of the utterance or the typed command and identify an action for the device to perform.
- the grammars 206 may not indicate which of the grammars 206 or words included in the grammars 206 may be more something that a speaker is more likely to say to a device operating in a specific domain.
- the grammars 206 may be a set of grammars for a vehicle assistant domain.
- the grammar examples 220 may also include the $ALARM grammar.
- a speaker may be more likely to say commands for controlling the vehicle, requesting traffic information, requesting directions, and other vehicle related commands. While it may be important for the vehicle to be able to recognize and perform a command to set an alarm, an alarm command may not be as common in the vehicle assistant domain.
- a developer may wish to make the vehicle assistant technology available to Basque speakers. If the developer had access to transcriptions of Basque speakers using the vehicle assistant technology, then the developer would be able to use those transcriptions to adapt a Basque language model for the vehicle assistant domain. Without the transcriptions of Basque speakers using the vehicle assistant technology, the developer may user a general Basque language model 202.
- Basque language model 202 may provide an inferior experience for the speaker because the Basque language model 202 may not recognize the speaker saying, "egungo trafikoa” if the user does not speak clearly or there are other Basque phrases that sound similar to "egungo trafikoa.”
- a Basque language model that is biased toward phrases that a speaker is more likely to say in the vehicle assistant domain may be more likely to recognize the speaker saying, "egungo trafikoa,” in instances where the general Basque language model 202 may not.
- the grammars 206 may be what a developer includes in the vehicle assistant technology when launching the technology for Basque speakers.
- the system 200 illustrates the components for using the grammars 206 and English transcriptions 218 to adapt a Basque language model 202 and generate a Basque language model 210 adapted for the vehicle assistant domain.
- the system 200 accesses English transcription data 218 that was received by computing devices operating in the vehicle assistant domain.
- a device operating in the vehicle assistant domain may include devices such as a car equipped with a voice interaction system. Other devices may operate in the vehicle assistant domain, and transcriptions collected from those devices may be added to the English transcription data 218.
- the English transcription data 218 may include transcriptions 145 such as "current traffic,” “directions to home,” “open garage door,” “lower fan speed,” and transcriptions of other utterances received by devices operating in the vehicle assistant domain.
- the system 200 may generate the English transcription data 218 by performing automated speech recognition on audio data of utterances received by devices operating in the vehicle assistant domain. In some implementations, the system 200 may receive the English transcription data 218 from manual transcribers who transcribed the audio data of utterances received by devices operating in the vehicle assistant domain. In some implementations, the English transcription data 218 may include a mix of both automated transcriptions and manual transcriptions.
- the system 200 provides the English transcription data 218 as an input to an English to Basque translator 224.
- the English to Basque translator 224 may be a machine translator that automatically translates the English transcription data 218 to Basque transcription data 204.
- the English to Basque translator 224 may be a manual translator.
- the Basque transcription data 204 may include transcriptions 226 such as, "egungo trafikoa”, “etxerako jarraibideak,” “garaje ate irekia,” and “behe-fanaren abiadura,” which may be Basque translations of current traffic,” “directions to home,” “open garage door,” and “lower fan speed,” respectively.
- the transcriptions 226 may include additional translated transcriptions of the English transcription data 218.
- the English to Basque translator 224 may make errors in translating the English transcription data 140.
- the system 200 may translate additional English transcription data transcribed from other utterances received in a variety of domains and/or in a general domain.
- the system 200 may provide the general English transcription data as an input to the English to Basque translator 224.
- the system 200 may compare the Basque transcription data 204 to the general Basque transcription data to identify n-grams that may be more common in the home assistant domain. Using both domain specific translated transcriptions and general translated transcriptions may allow the system 200 to remove some of the errors that occur during translation. Because some errors may be likely to occur when translating both domain specific transcriptions and general transcriptions, those errors may cancel out and the system may not identify n-grams related to the errors as more likely to occur in a specific domain.
- the system 200 provides the Basque transcription data 204 and the Basque grammars 206 as inputs to the grammar weighter 228.
- the grammar weighter 228 may be configured generate the weighted grammars 230 by comparing the Basque transcription data 204 to the Basque grammars 206.
- the grammar weighter 228 may identify the grammars included in the Basque grammars 206 that parse the words or phrases stored in the Basque transcription data 204. For example, the Basque grammar "$LOCATION jarraibideak” may parse the Basque transcription "Etxerako jarraibideak” because "etxerako" may be included as a term that can occupy the $LOCATION variable.
- the Basque grammar ezarri irratia $STATION may not parse any of the transcriptions in the Basque transcription data 204.
- the grammar weighter 228 may identify both "egungo trafikoa” and "$LOCATION jarraibideak” as grammars that are more to be spoken by Basque users interacting with a vehicle assistant system.
- the grammar weighter 228 can assign numerical weights to the Basque grammars 206. For example, the grammar weighter 228 may assign a weight of 0.1 to grammars that do not parse any of the transcriptions in the Basque transcription data 204. A weight of 0.1 may not indicate that a vehicle assistant system may not identify that grammar as parsing a transcription, but it may decrease the likelihood of the vehicle assistant system selecting that grammar and performing the corresponding action when there are other grammars with higher weights that also parse the transcription. The grammar weighter 228 may assign a higher weight based on the frequency that a parseable transcriptions appears in the Basque transcription data 204.
- the grammar weighter 228 may assign a weight of 0.3. Some of the seven transcriptions may be identical and others may include a different term for the $STATION variable. If the if the Basque transcription data 204 includes eleven transcriptions that are parsable by the grammar "$NUMBER tenperatura ezarri,” then the grammar weighter 228 may assign a weight of 0.4. In some implementations, the grammar weighter 228 may assign the same weight to grammars that parse at least one transcription and a different weight to grammars that parse no transcriptions.
- the system 200 provides the weighted Basque grammars 230 and the Basque language model 202 as inputs to the language model adaptor 234.
- the language model adaptor 234 may be configured to bias the Basque language model 202 to generate the Basque language model 210 that is adapted for the vehicle assistant domain.
- the Basque language model 202 may be a general Basque language model that is not biased to any particular terms, phrases, words, or n-grams.
- the language model adaptor 234 may adapt the Basque language model 202 for the vehicle assistant domain by increasing the weights of the n-grams of the weighted Basque grammars 230. For example, the language model adaptor 234 may increase the weight for the n-gram "egungo trafikoa" such that if a Basque speaker says something that sounds similar to "egungo trafikoa," the Basque language model 210 may be more likely to select "egungo trafikoa" as the transcription even if the general Basque language model 202 would not.
- the language model adaptor 234 may adapt the Basque language model 210 to be more likely to recognize n-grams 232 such as "egungo trafikoa”, “$LOCATION jarraibideak,” and other n-grams included in the weighted Basque grammars 230. In some implementations, the language model adaptor 234 may adapt the Basque language model 210 according to weight assigned to each of the grammars.
- the language model adaptor 234 may bias the Basque language model 210 such that the Basque language model 210 may be more likely to select "etxerako jarraibideak” over a similar sounding transcription upon receipt of an utterance.
- the Basque language model 210 may still be more likely to select "egungo trafikoa” over a similar sounding transcription upon receipt of an utterance, but the Basque language model 210 biasing may be less than "etxerako jarraibideak.”
- the language model adaptor 234 may bias the Basque language model 210 towards the different terms of each weighted grammar 230. For example, the language model adaptor 234 may bias towards “egungo trafikoa” "trafikoa,” and “egungo.” In some instances the language model adaptor 234 may bias towards “egungo trafikoa” more than "trafikoa” and "egungo” individually. In some implementations, the language model adaptor 234 may identify "trafikoa” as an n-gram to bias towards more heavily than "egungo” because "trafikoa” may appear in more heavily weighted grammars included in the weighted grammars 230.
- the language model adaptor 234 may bias the Basque language model 210 towards n-grams that are parsable by the weighted grammars 230 in addition to each term that may be available for a variable in the grammar. For example, the language model adaptor 234 may bias the Basque language model 210 towards the terms that may occupy the $LOCATION variable in addition to "$LOCATION jarraibideak,” and "jarraibideak.”
- the vehicle 208 may be equipped with a vehicle assistant system.
- the vehicle assistant system may access the Basque language model 210 during processing of the utterance 214.
- the vehicle assistant system by virtue of being a digital assistant system located in a vehicle, is operating in the vehicle assistant domain.
- the vehicle assistant system may listen for voice commands from the user 212.
- the vehicle assistant system may begin processing received audio in response to the user 212 speaking a predetermined hotword, e.g., "ok computer” or a similar Basque phrase, the user 212 pressing a button on the steering wheel, or any other similar action.
- the user may speak utterance 214, "Nola trafikoa da?”
- the vehicle assistant system receives the utterance 214 and performs automated speech recognition using a Basque acoustic model and the Basque language model 210 that is adapted for the vehicle assistant domain.
- the vehicle assistant system initiates an action based on the utterance 214.
- the action may be generating a query for the traffic for the location of the vehicle 208.
- the vehicle assistant system may receive a response to the query and generate the response 216, "Trafikoa argi dago.”
- the Basque language model 210 may not be biased towards the exact phrase of the utterance 214, but the Basque language model 210 may be biased towards the keyword "trafikoa.” Biasing towards an n-gram that the user speaks may assist the vehicle assistant system in identifying an accurate transcription.
- FIG. 3 is a flowchart of an example process 300 for adapting a language model for use in a specific domain.
- the process 300 uses translated training data to adapt a language model for use in a particular domain.
- the process 300 may be useful if no training data for the target language exists in that particular domain.
- the process 300 will be described as being performed by a computer system comprising one or more computers, for example, the system 100 of FIG. 1 or system 200 of FIG. 2 .
- Each of the components of either system 100 or system 100 may be included on a single computing device or distributed across multiple computing devices.
- the system receives transcriptions of utterances that were received by other systems operating in a domain and that are in a source language (310).
- the system may receive audio data of source language utterances that were received by other systems while the other systems operated in the domain.
- the system may generate the transcriptions of the audio data using an automated speech recognizer configured to recognize the source language.
- the domain may be related to the type of system receiving the utterances such as a smart speaker with a display. Other domains may include a smart speaker without a display, a vehicle assistant domain, a maps application domain, a domain dependent on location (e.g., a park or a restaurant), a home assistant domain, and any other similar type of domain.
- the system generates translated transcriptions of the transcriptions of the utterances in a target language (320).
- the system may use machine translations to translate the transcriptions.
- the system may use a machine translator that is configured to translate English to Vietnamese.
- the system may receive and translate transcriptions received by system operating in different domains.
- the system may translate transcriptions received in the maps application domain.
- the system may also translate transcriptions received in other domains that may include multiple domains such as a general domain or a combination of other domains that may or may not include the maps application domain.
- the system receives a language model for the target language (330).
- the target language model is a general purpose language model that may not be biased towards any particular words, phrases, and/or n-grams.
- the target language model may be a general purpose Vietnamese language model.
- the system biases the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions (340).
- the system identifies the words, phrases, and or n-grams that appear in the translated transcriptions.
- the system biases the target language model such that the biased language model may be more likely to generate a transcription that includes the words, phrases, and/or n-grams that appear in the translated transcriptions in instances where the user's speech sounds similar to other words, phrases, and/or n-grams.
- the system may compare the translated transcriptions from a specific domain to translated transcriptions from other domains.
- the system may identify those words, phrases, and/or n-grams that are more common in the translated transcriptions from the specific domain.
- the system may bias the language model to be more likely to generate transcriptions that include these words, phrases, and/or n-grams.
- By using the additional translations from the other domains may allow the system to remove noise from the translation process.
- the translation process may be a machine translation process, the translation process may have some inaccuracies. These inaccuracies may be just as likely to appear in the translated transcriptions from the specific domain as in the translated transcriptions from other domains.
- the system may be able to cancel out words that may not reflect accurate translations and bias the language model to be more likely to generate transcriptions that include words, phrases, and/or n-grams from the translated transcriptions from a specific domain without any words, phrases, and/or n-grams from translation noise.
- Translating the transcriptions from other domains may also help the system identify words, phrases, and/or n-grams that may be closely related to the specific domain instead of words, phrases, and/or n-grams that may be common words in general.
- the system may bias the language model based on the frequency that words, phrases, and/or n-grams appear in the translated transcriptions. For example, if the Turkish word “hava,” which is related to the English word “weather,” appears more frequently in the translated transcriptions than the Vietnamese word “caz,” which is related to the English word “jazz,” then the system may bias the language model more heavily towards “hava” than "caz.” In some implementations, the system may bias the language model equally for words, phrases, and/or n-grams that appear in the translated transcriptions.
- the system may access a set of grammars in the target language that a device operating in the specific domain may use to parse candidate transcriptions.
- the system may compare the n-grams included in the grammars to the n-grams in the translated transcriptions.
- the system may weight the grammars based on the frequency that an n-gram, that appears in the grammars or is parsable by the grammars, appears in the translated transcriptions.
- the system may use the weighted grammars to adapt a language model by biasing the language model to be more likely to select n-grams in both the grammars and the translated transcriptions when generating transcriptions.
- the system, or another system may use the adapted language model during an automated speech recognition process when the system or the other system is operating in the domain for which the system adapted the language model.
- the system or the other system may use an acoustic model trained to identify phonemes for the target language and the adapted language model. If the system or the other system has access to a set of grammars, then the system may use the grammars to parse the candidate transcriptions.
- FIG. 4 shows an example of a computing device 400 and a mobile computing device 450 that can be used to implement the techniques described here.
- the computing device 400 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the mobile computing device 450 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart-phones, and other similar computing devices.
- the components shown here, their connections and relationships, and their functions, are meant to be examples only, and are not meant to be limiting.
- the computing device 400 includes a processor 402, a memory 404, a storage device 406, a high-speed interface 408 connecting to the memory 404 and multiple high-speed expansion ports 410, and a low-speed interface 412 connecting to a low-speed expansion port 414 and the storage device 406.
- Each of the processor 402, the memory 404, the storage device 406, the high-speed interface 408, the high-speed expansion ports 410, and the low-speed interface 412 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 402 can process instructions for execution within the computing device 400, including instructions stored in the memory 404 or on the storage device 406 to display graphical information for a GUI on an external input/output device, such as a display 416 coupled to the high-speed interface 408.
- an external input/output device such as a display 416 coupled to the high-speed interface 408.
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 404 stores information within the computing device 400.
- the memory 404 is a volatile memory unit or units.
- the memory 404 is a non-volatile memory unit or units.
- the memory 404 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 406 is capable of providing mass storage for the computing device 400.
- the storage device 406 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- Instructions can be stored in an information carrier.
- the instructions when executed by one or more processing devices (for example, processor 402), perform one or more methods, such as those described above.
- the instructions can also be stored by one or more storage devices such as computer- or machine-readable mediums (for example, the memory 404, the storage device 406, or memory on the processor 402).
- the high-speed interface 408 manages bandwidth-intensive operations for the computing device 400, while the low-speed interface 412 manages lower bandwidth-intensive operations. Such allocation of functions is an example only.
- the high-speed interface 408 is coupled to the memory 404, the display 416 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 410, which may accept various expansion cards (not shown).
- the low-speed interface 412 is coupled to the storage device 406 and the low-speed expansion port 414.
- the low-speed expansion port 414 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 400 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 420, or multiple times in a group of such servers. In addition, it may be implemented in a personal computer such as a laptop computer 422. It may also be implemented as part of a rack server system 424. Alternatively, components from the computing device 400 may be combined with other components in a mobile device (not shown), such as a mobile computing device 450. Each of such devices may contain one or more of the computing device 400 and the mobile computing device 450, and an entire system may be made up of multiple computing devices communicating with each other.
- the mobile computing device 450 includes a processor 452, a memory 464, an input/output device such as a display 454, a communication interface 466, and a transceiver 468, among other components.
- the mobile computing device 450 may also be provided with a storage device, such as a micro-drive or other device, to provide additional storage.
- a storage device such as a micro-drive or other device, to provide additional storage.
- Each of the processor 452, the memory 464, the display 454, the communication interface 466, and the transceiver 468, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
- the processor 452 can execute instructions within the mobile computing device 450, including instructions stored in the memory 464.
- the processor 452 may be implemented as a chipset of chips that include separate and multiple analog and digital processors.
- the processor 452 may provide, for example, for coordination of the other components of the mobile computing device 450, such as control of user interfaces, applications run by the mobile computing device 450, and wireless communication by the mobile computing device 450.
- the processor 452 may communicate with a user through a control interface 458 and a display interface 456 coupled to the display 454.
- the display 454 may be, for example, a TFT (Thin-Film-Transistor Liquid Crystal Display) display or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 456 may comprise appropriate circuitry for driving the display 454 to present graphical and other information to a user.
- the control interface 458 may receive commands from a user and convert them for submission to the processor 452.
- an external interface 462 may provide communication with the processor 452, so as to enable near area communication of the mobile computing device 450 with other devices.
- the external interface 462 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 464 stores information within the mobile computing device 450.
- the memory 464 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- An expansion memory 474 may also be provided and connected to the mobile computing device 450 through an expansion interface 472, which may include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- the expansion memory 474 may provide extra storage space for the mobile computing device 450, or may also store applications or other information for the mobile computing device 450.
- the expansion memory 474 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- the expansion memory 474 may be provide as a security module for the mobile computing device 450, and may be programmed with instructions that permit secure use of the mobile computing device 450.
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a nonhackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory (non-volatile random access memory), as discussed below.
- instructions are stored in an information carrier. that the instructions, when executed by one or more processing devices (for example, processor 452), perform one or more methods, such as those described above.
- the instructions can also be stored by one or more storage devices, such as one or more computer- or machine-readable mediums (for example, the memory 464, the expansion memory 474, or memory on the processor 452).
- the instructions can be received in a propagated signal, for example, over the transceiver 468 or the external interface 462.
- the mobile computing device 450 may communicate wirelessly through the communication interface 466, which may include digital signal processing circuitry where necessary.
- the communication interface 466 may provide for communications under various modes or protocols, such as GSM voice calls (Global System for Mobile communications), SMS (Short Message Service), EMS (Enhanced Messaging Service), or MMS messaging (Multimedia Messaging Service), CDMA (code division multiple access), TDMA (time division multiple access), PDC (Personal Digital Cellular), WCDMA (Wideband Code Division Multiple Access), CDMA2000, or GPRS (General Packet Radio Service), among others.
- GSM voice calls Global System for Mobile communications
- SMS Short Message Service
- EMS Enhanced Messaging Service
- MMS messaging Multimedia Messaging Service
- CDMA code division multiple access
- TDMA time division multiple access
- PDC Personal Digital Cellular
- WCDMA Wideband Code Division Multiple Access
- CDMA2000 Code Division Multiple Access
- GPRS General Packet Radio Service
- a GPS (Global Positioning System) receiver module 470 may provide additional navigation- and location-related wireless data to the mobile computing device 450, which may be used as appropriate by applications running on the mobile computing device 450.
- the mobile computing device 450 may also communicate audibly using an audio codec 460, which may receive spoken information from a user and convert it to usable digital information.
- the audio codec 460 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of the mobile computing device 450.
- Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on the mobile computing device 450.
- the mobile computing device 450 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 480. It may also be implemented as part of a smart-phone 482, personal digital assistant, or other similar mobile device.
- implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- machine-readable medium and computer-readable medium refer to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal.
- machine-readable signal refers to any signal used to provide machine instructions and/or data to a programmable processor.
- the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (LAN), a wide area network (WAN), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- the delegate(s) may be employed by other applications implemented by one or more processors, such as an application executing on one or more servers.
- the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results.
- other actions may be provided, or actions may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other implementations are also according to the invention, provided that their subject-matter is still within the scope of the following claims.
Description
- This application claims the benefit of
US Application 62/773,361, filed November 30, 2018 - This specification generally relates to systems and methods for processing speech input.
- It is increasingly desired to enable interactions with computers to be performed using speech inputs. This requires developments in input processing, in particular how to program computers to process and analyze natural language data. Such processing may involve speech recognition, which is a field of computational linguistics that enables the recognition and translation of spoken language into text by computers.
US 2012/278060 A1 discloses a system and method for building a language model for a translation. The method includes providing a first relative ranking of first and second translations in a target language of a same source string in a source language, determining a second relative ranking of the first and second translations using weights of a language model, the language model including a weight for each of a set of n-gram features, and comparing the first and second relative rankings to determine whether they are in agreement.US 2012/029904 A1 discloses a method and apparatus for adding new vocabulary to interactive translation and dialogue systems. NAKAJIMA H ET AL: "Language Model Adaptation with Additional Text Generated by Machine Translation", COLING 2002: Proceedings of the 19th International Conference on Computational Linguistics August 30, 2002, Howard International House, Taipei, Taiwan, vol. 2 26 August 2002 (2002-08-26), pages 716-722,XP002683643 discloses a novel scheme that generates a small target corpus in the language of a model by machine translation of the target corpus in another language. WOOSUNG KIM AND SANJEEV KHUDANPUR ET AL, "Language Model Adaptation Using Cross-Lingual Information", 20030901, (20030901) discloses investigations into language models for an automatic speech recognition system for Mandarin Broadcast News that exploit a large side-corpus of contemporaneous English news articles to adapt a static Chinese language model to the news story being transcribed. RAO P S ET AL, "MDI Adaptation of Language Models Across Corpora", 5th European Conference on Speech Communication And Technology. EUROSPEECH '97. Rhodes, Greece, SEPT. 22 - 25, 1997, Grenoble : ESCA, FR, (19970922), pages 1979 - 1982, XP001049250 discloses a method of adapting a prior model from a large corpus to a smaller one of interest.US 2018/336895 A1 discloses systems, methods, devices, and other techniques are described herein for determining dialog states that correspond to voice inputs and for biasing a language model based on the determined dialog states.US 2016/140218 A1 discloses a voice search system that includes one or more computers programmed to receive a user's speech input from a user, convert the speech input into a text search query using a dynamic language model, receive search results responsive to the text search query, and provide the search results to the user in response to the speech input. - For optimal speech recognition accuracy, general purpose speech recognitions systems may need to be adapted to excel at domain specific speech recognition. The domain may be related to the type of device receiving the speech data, an application running on the device receiving the speech data, and/or the context of the device receiving the speech data. For example, speech recognition quality may improve for a speech recognition system in a vehicle if the speech recognition system is adapted for use in the vehicle by being able to more quickly and accurately transcribe speech that includes words or phrases that the user is likely to speak in the vehicle.
- One technique used to adapt a speech recognition system to a particular domain may be to analyze the transcription logs received by the speech recognition system operating in the particular domain. The adaption process may include identifying the words or phrases that are more common in the transcription logs for the particular domain than in transcriptions logs of a general speech recognition system. A language model may be biased such that the language model is more likely to generate candidate transcriptions that include the words or phrases that are more common in the transcription logs for the particular domain.
- This technique may not be practical for launching a speech recognition system for a particular domain in a new target language if there are no transcription logs for that particular domain in the target language. In this instance, it may be helpful to leverage the domain-specific transcription logs from another language. A language model adaption system may translate the domain-specific transcription logs into the target language. The language model adaption system may analyze the translated transcription logs to identify words and phrases that appear to be more common for users speaking to devices operating in the particular domain. With the common words identified, the language model adaption system may bias a general purpose language model for the target language such that the biased language model may be more likely to generate transcriptions that include the common words when processing audio data that may sound similar to other words in the target language.
- With the biased language model, it may be possible to launch a speech recognition system for the particular domain in the target language and benefit from improved speech recognition with improved accuracy and latency. Users may benefit from a speech recognition that is configured, at launch of the system, to more quickly and accurately recognize words and phrases that the users are more likely to say when speaking to a system operating in the particular domain.
- According to an innovative aspect of the subject matter described in this application, a method for processing speech input includes the actions of receiving, by a computing device, transcriptions of utterances that were received by computing devices operating in a domain and that are in a source language; generating, by the computing device, translated transcriptions of the transcriptions of the utterances in a target language; receiving, by the computing device, a language model for the target language; receiving, by the computing device, additional transcriptions of additional utterances that were received by the computing devices operating in domains other than the domain and that are in the source language; generating, by the computing device, additional translated transcriptions of the additional transcriptions; identifying terms that have a higher appearance frequency in the translated transcriptions than in the additional translated transcriptions; biasing, by the computing device, the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions; and generating, by the computing device while operating in the domain, a transcription of an utterance in the target language using the biased language model, wherein biasing the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions comprises: biasing the language model for the target language by increasing the likelihood of the language model selecting terms that have a higher appearance frequency in the translated transcriptions than in the additional translated transcriptions.
- These and other implementations can each optionally include one or more of the following features. The action of generating a transcription of an utterance in the target language using the biased language model includes, while the computing device is operating in the domain, receiving, by the computing device, audio data of the utterance; providing, by the computing device, the audio data as an input to an acoustic model that is configured to identify phonemes of the utterance; based on the computing device operating in the domain, providing, by the computing device, the phonemes of the utterance as an input to the biased language model; and, based on providing the phonemes of the utterance as the input to the biased language model, generating, by the computing device, a transcription of the utterance. The language model for the target language is a general language model. The biased language for the target language is a language model specific for computing devices operating in the domain. The actions include identifying, by the computing device, n-grams that appear in the translated transcription and an appearance frequency of each n-gram; and identifying, by the computing device, a subset of the n-grams that appear in the translated transcriptions more than a threshold appearance frequency.
- The computing device biases the language model by increasing the likelihood of the language model selecting the subset of n-grams. The action of increasing the likelihood of the language model selecting the subset of n-grams includes increasing, for each n-gram in the subset of n-grams, the likelihood by a larger amount based on a larger difference between the appearance frequency for the n-gram and the threshold appearance frequency. The actions include receiving, by the computing device, audio data of the utterances that were received by the computing device operating in the domain and that are in the source language. The action of receiving the transcriptions of the utterances that were received by the computing devices operating in the domain and that are in the source language includes generating, by the computing device, the transcriptions of the utterances that were received by computing devices operating in a domain and that are in a source language.
- The actions include receiving, by the computing device, grammars in the target language. The action of biasing the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions includes biasing the language model for the target language by increasing the likelihood of the language model selecting grammars that include the terms included in the translated transcriptions.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer programs recorded on computer storage devices, each configured to perform the operations of the methods.
- Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages. It may be possible to launch a speech recognition system for a new language and domain that is configured to more accurately and quickly recognize common phases spoken by user in that domain. It may not be necessary to collect any speech data for that domain in the new language.
- The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
-
FIG. 1 illustrates an example system that biases a language model for use in a specific domain using translated training data. -
FIG. 2 illustrates an example system that biases a language model for use in a specific domain using both translated training data and pre-selected grammars. -
FIG. 3 is a flowchart of an example process for adapting a language model for use in a specific domain. -
FIG. 4 is an example of a computing device and a mobile computing device. - Like reference numbers and designations in the various drawings indicate like elements.
-
FIG. 1 illustrates anexample system 100 that biases alanguage model 105 for use in a specific domain using translatedtraining data 110. Briefly, and as described in more detail below, thesystem 100 examines translatedtraining data 110 to identify common words and phrases that are likely to be used by speakers in a specific domain. Thesystem 100 biases a language model to favor those common words and phrases. - A
smart speaker 120 may use the biased language model 115 during the automated speech recognition process. Theuser 125 speaks aquery 130 to thesmart speaker 120, and thesmart speaker 120 uses the biased language model 115 to generate a transcription of thequery 130. Based on thequery 130, thesmart speaker 120 may generate aresponse 135 to thequery 130. - With many different languages around the world, it may not be practical to design a speech recognizer that is capable of accurately recognizing all languages. For example, the same speech recognizer may not be able to recognize both English and Turkish. Instead, a speech recognizer may use an English acoustic model that is trained to identify English phonemes and an English language model that is configured to identify English words and phrases, which may be referred to as n-grams (e.g., unigram being a single word, a bigram being a two word phrase, etc.). Another speech recognizer may use a Turkish acoustic model that is trained to identify Turkish phonemes and a Turkish language model that is configured to identify Turkish words and phrases. When a user speaks a language that the speech recognizer is not configured to recognize, the transcription generated by the speech recognizer may not be accurate. Additionally, the computational resources required by a speech recognizer accessing multiple acoustic models and multiple language models may be too high, which would result in an unacceptable latency for the speaker.
- Speech recognizers may be adapted for different types of situations. For example, a speech recognizer may be adapted for use in a vehicle, by a smart speaker without a display, by a smart speaker with a display, by a digital assistant application running on a mobile phone, by a digital assistant application running on a laptop computer, or any other similar application or situation. The application or situation may be referred to as the domain. A speech recognizer that is configured to recognize a particular language may be adapted for use in different domains. For example, an English speech recognizer may be adapted for use by a digital assistant application running on a mobile phone or for use by a smart speaker with a display. A user interacting with the digital assistant application running on a mobile phone may be more likely to say a first set of n-grams, and a user interacting with a smart speaker with a display may be more likely to say a second set of n-grams that may or may not include some of the n-grams of the first set. Therefore, a developer may bias the language model of the speech recognizer for the digital assistant application running on a mobile phone domain to be more likely to recognize the first set of n-grams and bias the language model of the speech recognizer for the smart speaker with a display domain to be more likely to recognize the second set of n-grams.
- Identifying the n-grams to bias the language model may be accomplished by examining transcriptions of utterances received by a computing devices operating in a particular domain. For example, to identify the popular n-grams for the smart speaker without a display domain, the
system 100 may examine the transcriptions of utterances received by smart speakers without a display. Thesystem 100 may identify the most common n-grams and bias a language model to favor those n-grams. The smart speakers without a display may then perform automated speech recognition processes using the biased language model. - This biasing process for a specific language may not be practical for a situation where transcriptions of utterances received by a computing devices operating in the particular domain are not available in the specific language. The transcriptions may not be available because the smart speaker without a display may not be available for purchase in the country where the population speaks the specific language. For example, the smart speaker without a display may not be available in Turkey. Therefore, there are likely very little if any transcriptions of people speaking Turkish to a smart speaker without a display.
- Despite this lack of transcription data, the
system 100 may be able to leverage data from other languages to bias a language model of the target language. For example, thesystem 100 may be able to leverage theEnglish transcription data 140 to bias the Turkish language model 115. Doing so, may allow the application or product to launch with a Turkish language model that is biased, or adapted, toward the particular domain, such as a smart speaker without a display. - As illustrated in
FIG. 1 , thesystem 100 accessesEnglish transcription data 140 that was received by computing devices operating in the home assistant domain. A device operating in the home assistant domain may include devices such as a smart speaker without a display. Other devices may operate in the home assistant domain, and transcriptions collected from those devices may be added to theEnglish transcription data 140. As an example, theEnglish transcription data 140 may includetranscriptions 145 such as "what is the weather," "play jazz music," "volume up," "lights on," and transcriptions of other utterances received by devices operating in the home assistant domain. - In some implementations, the
system 100 may generate theEnglish transcription data 140 by performing automated speech recognition on audio data of utterances received by devices operating in the home assistant domain. In some implementations, thesystem 100 may receive theEnglish transcription data 140 from manual transcribers who transcribed the audio data of utterances received by devices operating in the home assistant domain. In some implementations, theEnglish transcription data 140 may include a mix of both automated transcriptions and manual transcriptions. - The
system 100 provides theEnglish transcription data 140 as an input to an English toTurkish translator 150. The English toTurkish translator 150 may be a machine translator that automatically translates theEnglish transcription data 140 toTurkish transcription data 110. In some implementations, the English toTurkish translator 150 may be a manual translator. - The
Turkish transcription data 110 may includetranscriptions 155 such as, "hava nastranscriptions 155 may include additional translated transcriptions of theEnglish transcription data 140. - In some implementations, the English to
Turkish translator 150 may make errors in translating theEnglish transcription data 140. In this case, thesystem 100 may translate additional English transcription data transcribed from other utterances received in a variety of domains and/or in a general domain. Thesystem 100 may provide the general English transcription data as an input to the English toTurkish translator 150. Thesystem 100 may compare theTurkish transcription data 110 to the general Turkish transcription data to identify n-grams that may be more common in the home assistant domain. Using both domain specific translated transcriptions and general translated transcriptions may allow thesystem 100 to remove some of the errors that occur during translation. Because some errors may be likely to occur when translating domain specific transcriptions as when translating general transcriptions, those errors may cancel out and the system may not identify n-grams related to the errors as more likely to occur in a specific domain. - The
system 100 provides theTurkish transcription data 110 and aTurkish language model 105 as an input to thelanguage model adaptor 160. Thelanguage model adaptor 160 may be configured to bias theTurkish language model 105 to generate theTurkish language model 170 that is adapted for the home assistant domain. TheTurkish language model 105 may be a general Turkish language model that is not biased to any particular n-grams 165. - The
language model adaptor 160 may adapt theTurkish language model 105 for the home assistant domain by increasing the weights of the n-grams of theTurkish transcription data 110. For example, thelanguage model adaptor 160 may increase the weight for the n-gram "hava nasTurkish language model 170 may be more likely to select "hava nasTurkish language model 105 would not. - The
language model adaptor 160 may adapt theTurkish language model 170 to be more likely to recognize n-grams 175 such as "hava nasTurkish transcriptions 110. In some implementations, thelanguage model adaptor 160 may adapt theTurkish language model 170 according to the appearance frequency of n-grams in theTurkish transcriptions 110. For example, if "hava nasTurkish transcriptions 110 than "sesi aç," then thelanguage model adaptor 160 may bias theTurkish language model 170 more heavily towards "hava naslanguage model adaptor 160 may bias theTurkish language model 170 towards the different terms of each n-gram 175. For example, thelanguage model adaptor 160 may bias towards both "hava" and "naslanguage model adaptor 160 may bias towards "hava nas - The
smart speaker 120 accesses theTurkish language model 170 during processing of theutterance 130. Thesmart speaker 120 may be operating in the home assistant domain, and, therefore, accesses theTurkish language model 170 adapted for the home assistant domain. As illustrated inFIG. 1 , thesmart speaker 120 may prompt theuser 125 by outputting, "Merhaba, sana nasil yardim edebilirim?" Thesmart speaker 120 may prompt theuser 125 in response to theuser 125 touching thesmart speaker 120, speaking a predetermined hotword, e.g., "ok computer" or a similar Turkish phrase, or any other similar action. Theuser 125 may responds with theutterance 130, "hava nasuser 125 may speak a hotword followed by theutterance 130 without the smart speaker outputting prompt 180. - The
smart speaker 120 receives theutterance 130 and performs automated speech recognition using a Turkish acoustic model and theTurkish language model 170 that is adapted for the home assistant domain. Thesmart speaker 120 initiates an action based on theutterance 130. In this case, the action may be generating a query for the weather in the location of thesmart speaker 120. Thesmart speaker 120 may receive a response to the query and generate theresponse 135, "Güne -
FIG. 2 illustrates anexample system 200 that biases alanguage model 202 for use in a specific domain using both translatedtraining data 204 andpre-selected grammars 206. Briefly, and as described in more detail below, thesystem 200 compares pre-selectedgrammars 206 and translatedtraining data 204 to identify common words and phrases that are likely to be used by speakers in a specific domain. Thesystem 200 biases a language model to favor the terms in thegrammars 206 and the translated training data. - A vehicle equipped with an automated
digital assistant 208 may use thebiased language model 210 during the automated speech recognition process. Theuser 212 speaks aquery 214 to the automateddigital assistant 208, and the automateddigital assistant 208 uses thebiased language model 210 to generate a transcription of thequery 214. Based on thequery 214, the automateddigital assistant 208 may generate aresponse 216 to thequery 214. - Similar to
system 100 inFIG. 1 ,system 200 may be configured to generate abiased language model 210 for a language in which thesystem 200 does not have any access to training data in the language. In the example inFIG. 2 , thesystem 200 generates a biasedBasque language model 210 for the vehicle assistant domain despite only having access toEnglish transcriptions 218 received by devices operating in the vehicle assistant domain. - In addition to utilizing the
English transcriptions 218 received by devices operating in the vehicle assistant domain, thesystem 200 accessespre-programmed grammars 206. In order to ensure that a speech recognition product for a new language may function properly, a developer may include a set ofgrammars 206 that the speech recognizer can access to assist in determining the action requested by the user. Instead of launching a speech recognition product in a new language with only a generic language model, the developer may include a set ofgrammars 206 to ensure that the user can interact with the device. The device may indicate the syntax of thegrammars 206 and the action for each grammar by displaying the syntax on a display, outputting audio of examples, or any other similar way of outputting data. - A grammar may be different than the vocabulary that the language model selects words from. A grammar is related to the structure of a transcription of an utterance. If the transcription matches the structure of a grammar, then the
system 200 performs the action that corresponds to that grammar. A grammar may be any structure of words that can be described using a common notation technique, for example, Bakus-Naur form. Each grammar may correspond to a specific user intent. For example, the user intent may be to issue a home automation command or a media playing command. One example of a grammar may include a grammar for an alarm. The alarm grammar may define a digit as 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, or 0 using the notation $DIGIT = (0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9). The alarm grammar may define a time using the notation $TIME = $DIGIT $DIGIT : $DIGIT $DIGIT (am | pm) that indicates the time includes two digits, followed by a colon, followed by a two digits, and followed by "am" or "pm." The alarm grammar may define the mode of the alarm using the notation $MODE = (alarm | timer) that indicates whether the alarm should be in alarm mode or timer mode. Finally, the alarm grammar may define the alarm syntax as $ALARM = set $MODE for $TIME that indicates the user can say "set alarm for 6:00 am" or "set timer for 20:00." A speech recognition system uses the grammar to parse the transcription of the utterance or the typed command and identify an action for the device to perform. - In some instances, the
grammars 206 may not indicate which of thegrammars 206 or words included in thegrammars 206 may be more something that a speaker is more likely to say to a device operating in a specific domain. For example, thegrammars 206 may be a set of grammars for a vehicle assistant domain. In addition to the grammar examples 220 that include "egungo trafikoa," "$LOCATION jarraibideak," "$NUMBER tenperatura ezarri," and "ezarri irratia $STATION," the grammar examples 220 may also include the $ALARM grammar. However, in the vehicle assistant domain a speaker may be more likely to say commands for controlling the vehicle, requesting traffic information, requesting directions, and other vehicle related commands. While it may be important for the vehicle to be able to recognize and perform a command to set an alarm, an alarm command may not be as common in the vehicle assistant domain. - For the example illustrated in
FIG. 2 , a developer may wish to make the vehicle assistant technology available to Basque speakers. If the developer had access to transcriptions of Basque speakers using the vehicle assistant technology, then the developer would be able to use those transcriptions to adapt a Basque language model for the vehicle assistant domain. Without the transcriptions of Basque speakers using the vehicle assistant technology, the developer may user a generalBasque language model 202. Using the generalBasque language model 202 may provide an inferior experience for the speaker because theBasque language model 202 may not recognize the speaker saying, "egungo trafikoa" if the user does not speak clearly or there are other Basque phrases that sound similar to "egungo trafikoa." A Basque language model that is biased toward phrases that a speaker is more likely to say in the vehicle assistant domain may be more likely to recognize the speaker saying, "egungo trafikoa," in instances where the generalBasque language model 202 may not. In the absence of transcriptions of Basque speakers using the vehicle assistant technology and a Basque language model adapted for the vehicle assistant domain, including thegrammars 206 may be what a developer includes in the vehicle assistant technology when launching the technology for Basque speakers. - The
system 200 illustrates the components for using thegrammars 206 andEnglish transcriptions 218 to adapt aBasque language model 202 and generate aBasque language model 210 adapted for the vehicle assistant domain. Thesystem 200 accessesEnglish transcription data 218 that was received by computing devices operating in the vehicle assistant domain. A device operating in the vehicle assistant domain may include devices such as a car equipped with a voice interaction system. Other devices may operate in the vehicle assistant domain, and transcriptions collected from those devices may be added to theEnglish transcription data 218. As an example, theEnglish transcription data 218 may includetranscriptions 145 such as "current traffic," "directions to home," "open garage door," "lower fan speed," and transcriptions of other utterances received by devices operating in the vehicle assistant domain. - In some implementations, the
system 200 may generate theEnglish transcription data 218 by performing automated speech recognition on audio data of utterances received by devices operating in the vehicle assistant domain. In some implementations, thesystem 200 may receive theEnglish transcription data 218 from manual transcribers who transcribed the audio data of utterances received by devices operating in the vehicle assistant domain. In some implementations, theEnglish transcription data 218 may include a mix of both automated transcriptions and manual transcriptions. - The
system 200 provides theEnglish transcription data 218 as an input to an English toBasque translator 224. The English toBasque translator 224 may be a machine translator that automatically translates theEnglish transcription data 218 toBasque transcription data 204. In some implementations, the English toBasque translator 224 may be a manual translator. - The
Basque transcription data 204 may includetranscriptions 226 such as, "egungo trafikoa", "etxerako jarraibideak," "garaje ate irekia," and "behe-fanaren abiadura," which may be Basque translations of current traffic," "directions to home," "open garage door," and "lower fan speed," respectively. Thetranscriptions 226 may include additional translated transcriptions of theEnglish transcription data 218. - In some implementations, the English to
Basque translator 224 may make errors in translating theEnglish transcription data 140. In this case, thesystem 200 may translate additional English transcription data transcribed from other utterances received in a variety of domains and/or in a general domain. Thesystem 200 may provide the general English transcription data as an input to the English toBasque translator 224. Thesystem 200 may compare theBasque transcription data 204 to the general Basque transcription data to identify n-grams that may be more common in the home assistant domain. Using both domain specific translated transcriptions and general translated transcriptions may allow thesystem 200 to remove some of the errors that occur during translation. Because some errors may be likely to occur when translating both domain specific transcriptions and general transcriptions, those errors may cancel out and the system may not identify n-grams related to the errors as more likely to occur in a specific domain. - The
system 200 provides theBasque transcription data 204 and theBasque grammars 206 as inputs to thegrammar weighter 228. Thegrammar weighter 228 may be configured generate theweighted grammars 230 by comparing theBasque transcription data 204 to theBasque grammars 206. Thegrammar weighter 228 may identify the grammars included in theBasque grammars 206 that parse the words or phrases stored in theBasque transcription data 204. For example, the Basque grammar "$LOCATION jarraibideak" may parse the Basque transcription "Etxerako jarraibideak" because "etxerako" may be included as a term that can occupy the $LOCATION variable. The Basque grammar ezarri irratia $STATION" may not parse any of the transcriptions in theBasque transcription data 204. As illustrated withweighted grammars 232, thegrammar weighter 228 may identify both "egungo trafikoa" and "$LOCATION jarraibideak" as grammars that are more to be spoken by Basque users interacting with a vehicle assistant system. - In some implementations, the
grammar weighter 228 can assign numerical weights to theBasque grammars 206. For example, thegrammar weighter 228 may assign a weight of 0.1 to grammars that do not parse any of the transcriptions in theBasque transcription data 204. A weight of 0.1 may not indicate that a vehicle assistant system may not identify that grammar as parsing a transcription, but it may decrease the likelihood of the vehicle assistant system selecting that grammar and performing the corresponding action when there are other grammars with higher weights that also parse the transcription. Thegrammar weighter 228 may assign a higher weight based on the frequency that a parseable transcriptions appears in theBasque transcription data 204. For example, if theBasque transcription data 204 includes seven transcriptions that are parsable by the grammar "ezarri irratia $STATION," then thegrammar weighter 228 may assign a weight of 0.3. Some of the seven transcriptions may be identical and others may include a different term for the $STATION variable. If the if theBasque transcription data 204 includes eleven transcriptions that are parsable by the grammar "$NUMBER tenperatura ezarri," then thegrammar weighter 228 may assign a weight of 0.4. In some implementations, thegrammar weighter 228 may assign the same weight to grammars that parse at least one transcription and a different weight to grammars that parse no transcriptions. - The
system 200 provides the weightedBasque grammars 230 and theBasque language model 202 as inputs to thelanguage model adaptor 234. Thelanguage model adaptor 234 may be configured to bias theBasque language model 202 to generate theBasque language model 210 that is adapted for the vehicle assistant domain. TheBasque language model 202 may be a general Basque language model that is not biased to any particular terms, phrases, words, or n-grams. - The
language model adaptor 234 may adapt theBasque language model 202 for the vehicle assistant domain by increasing the weights of the n-grams of the weightedBasque grammars 230. For example, thelanguage model adaptor 234 may increase the weight for the n-gram "egungo trafikoa" such that if a Basque speaker says something that sounds similar to "egungo trafikoa," theBasque language model 210 may be more likely to select "egungo trafikoa" as the transcription even if the generalBasque language model 202 would not. - The
language model adaptor 234 may adapt theBasque language model 210 to be more likely to recognize n-grams 232 such as "egungo trafikoa", "$LOCATION jarraibideak," and other n-grams included in the weightedBasque grammars 230. In some implementations, thelanguage model adaptor 234 may adapt theBasque language model 210 according to weight assigned to each of the grammars. For example, if "egungo trafikoa" has a weight of 0.3 and "$LOCATION jarraibideak" has a weight of 0.5, then thelanguage model adaptor 234 may bias theBasque language model 210 such that theBasque language model 210 may be more likely to select "etxerako jarraibideak" over a similar sounding transcription upon receipt of an utterance. TheBasque language model 210 may still be more likely to select "egungo trafikoa" over a similar sounding transcription upon receipt of an utterance, but theBasque language model 210 biasing may be less than "etxerako jarraibideak." In some implementations, thelanguage model adaptor 234 may bias theBasque language model 210 towards the different terms of eachweighted grammar 230. For example, thelanguage model adaptor 234 may bias towards "egungo trafikoa" "trafikoa," and "egungo." In some instances thelanguage model adaptor 234 may bias towards "egungo trafikoa" more than "trafikoa" and "egungo" individually. In some implementations, thelanguage model adaptor 234 may identify "trafikoa" as an n-gram to bias towards more heavily than "egungo" because "trafikoa" may appear in more heavily weighted grammars included in theweighted grammars 230. - In some implementations, the
language model adaptor 234 may bias theBasque language model 210 towards n-grams that are parsable by theweighted grammars 230 in addition to each term that may be available for a variable in the grammar. For example, thelanguage model adaptor 234 may bias theBasque language model 210 towards the terms that may occupy the $LOCATION variable in addition to "$LOCATION jarraibideak," and "jarraibideak." - The
vehicle 208 may be equipped with a vehicle assistant system. The vehicle assistant system may access theBasque language model 210 during processing of theutterance 214. The vehicle assistant system, by virtue of being a digital assistant system located in a vehicle, is operating in the vehicle assistant domain. The - As illustrated in
FIG. 2 , the vehicle assistant system may listen for voice commands from theuser 212. The vehicle assistant system may begin processing received audio in response to theuser 212 speaking a predetermined hotword, e.g., "ok computer" or a similar Basque phrase, theuser 212 pressing a button on the steering wheel, or any other similar action. The user may speakutterance 214, "Nola trafikoa da?" - The vehicle assistant system receives the
utterance 214 and performs automated speech recognition using a Basque acoustic model and theBasque language model 210 that is adapted for the vehicle assistant domain. The vehicle assistant system initiates an action based on theutterance 214. In this case, the action may be generating a query for the traffic for the location of thevehicle 208. The vehicle assistant system may receive a response to the query and generate theresponse 216, "Trafikoa argi dago." TheBasque language model 210 may not be biased towards the exact phrase of theutterance 214, but theBasque language model 210 may be biased towards the keyword "trafikoa." Biasing towards an n-gram that the user speaks may assist the vehicle assistant system in identifying an accurate transcription. -
FIG. 3 is a flowchart of anexample process 300 for adapting a language model for use in a specific domain. In general, theprocess 300 uses translated training data to adapt a language model for use in a particular domain. Theprocess 300 may be useful if no training data for the target language exists in that particular domain. Theprocess 300 will be described as being performed by a computer system comprising one or more computers, for example, thesystem 100 ofFIG. 1 orsystem 200 ofFIG. 2 . Each of the components of eithersystem 100 orsystem 100 may be included on a single computing device or distributed across multiple computing devices. - The system receives transcriptions of utterances that were received by other systems operating in a domain and that are in a source language (310). In some implementations, the system may receive audio data of source language utterances that were received by other systems while the other systems operated in the domain. In this instance, the system may generate the transcriptions of the audio data using an automated speech recognizer configured to recognize the source language. The domain may be related to the type of system receiving the utterances such as a smart speaker with a display. Other domains may include a smart speaker without a display, a vehicle assistant domain, a maps application domain, a domain dependent on location (e.g., a park or a restaurant), a home assistant domain, and any other similar type of domain.
- The system generates translated transcriptions of the transcriptions of the utterances in a target language (320). The system may use machine translations to translate the transcriptions. For example, the system may use a machine translator that is configured to translate English to Turkish. In some implementations, the system may receive and translate transcriptions received by system operating in different domains. For example, the system may translate transcriptions received in the maps application domain. The system may also translate transcriptions received in other domains that may include multiple domains such as a general domain or a combination of other domains that may or may not include the maps application domain.
- The system receives a language model for the target language (330). In some implementations the target language model is a general purpose language model that may not be biased towards any particular words, phrases, and/or n-grams. For example, the target language model may be a general purpose Turkish language model.
- The system biases the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions (340). The system identifies the words, phrases, and or n-grams that appear in the translated transcriptions. The system biases the target language model such that the biased language model may be more likely to generate a transcription that includes the words, phrases, and/or n-grams that appear in the translated transcriptions in instances where the user's speech sounds similar to other words, phrases, and/or n-grams.
- In some implementations, the system may compare the translated transcriptions from a specific domain to translated transcriptions from other domains. The system may identify those words, phrases, and/or n-grams that are more common in the translated transcriptions from the specific domain. The system may bias the language model to be more likely to generate transcriptions that include these words, phrases, and/or n-grams. By using the additional translations from the other domains may allow the system to remove noise from the translation process. Because the translation process may be a machine translation process, the translation process may have some inaccuracies. These inaccuracies may be just as likely to appear in the translated transcriptions from the specific domain as in the translated transcriptions from other domains. By comparing the two groups of translated transcriptions, the system may be able to cancel out words that may not reflect accurate translations and bias the language model to be more likely to generate transcriptions that include words, phrases, and/or n-grams from the translated transcriptions from a specific domain without any words, phrases, and/or n-grams from translation noise. Translating the transcriptions from other domains may also help the system identify words, phrases, and/or n-grams that may be closely related to the specific domain instead of words, phrases, and/or n-grams that may be common words in general.
- In some implementations, the system may bias the language model based on the frequency that words, phrases, and/or n-grams appear in the translated transcriptions. For example, if the Turkish word "hava," which is related to the English word "weather," appears more frequently in the translated transcriptions than the Turkish word "caz," which is related to the English word "jazz," then the system may bias the language model more heavily towards "hava" than "caz." In some implementations, the system may bias the language model equally for words, phrases, and/or n-grams that appear in the translated transcriptions.
- In some implementations, the system may access a set of grammars in the target language that a device operating in the specific domain may use to parse candidate transcriptions. The system may compare the n-grams included in the grammars to the n-grams in the translated transcriptions. The system may weight the grammars based on the frequency that an n-gram, that appears in the grammars or is parsable by the grammars, appears in the translated transcriptions. The system may use the weighted grammars to adapt a language model by biasing the language model to be more likely to select n-grams in both the grammars and the translated transcriptions when generating transcriptions.
- In some implementations, the system, or another system, may use the adapted language model during an automated speech recognition process when the system or the other system is operating in the domain for which the system adapted the language model. The system or the other system may use an acoustic model trained to identify phonemes for the target language and the adapted language model. If the system or the other system has access to a set of grammars, then the system may use the grammars to parse the candidate transcriptions.
-
FIG. 4 shows an example of acomputing device 400 and amobile computing device 450 that can be used to implement the techniques described here. Thecomputing device 400 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Themobile computing device 450 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart-phones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be examples only, and are not meant to be limiting. - The
computing device 400 includes a processor 402, amemory 404, astorage device 406, a high-speed interface 408 connecting to thememory 404 and multiple high-speed expansion ports 410, and a low-speed interface 412 connecting to a low-speed expansion port 414 and thestorage device 406. Each of the processor 402, thememory 404, thestorage device 406, the high-speed interface 408, the high-speed expansion ports 410, and the low-speed interface 412, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 402 can process instructions for execution within thecomputing device 400, including instructions stored in thememory 404 or on thestorage device 406 to display graphical information for a GUI on an external input/output device, such as adisplay 416 coupled to the high-speed interface 408. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 404 stores information within thecomputing device 400. In some implementations, thememory 404 is a volatile memory unit or units. In some implementations, thememory 404 is a non-volatile memory unit or units. Thememory 404 may also be another form of computer-readable medium, such as a magnetic or optical disk. - The
storage device 406 is capable of providing mass storage for thecomputing device 400. In some implementations, thestorage device 406 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. Instructions can be stored in an information carrier. The instructions, when executed by one or more processing devices (for example, processor 402), perform one or more methods, such as those described above. The instructions can also be stored by one or more storage devices such as computer- or machine-readable mediums (for example, thememory 404, thestorage device 406, or memory on the processor 402). - The high-
speed interface 408 manages bandwidth-intensive operations for thecomputing device 400, while the low-speed interface 412 manages lower bandwidth-intensive operations. Such allocation of functions is an example only. In some implementations, the high-speed interface 408 is coupled to thememory 404, the display 416 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 410, which may accept various expansion cards (not shown). In the implementation, the low-speed interface 412 is coupled to thestorage device 406 and the low-speed expansion port 414. The low-speed expansion port 414, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 400 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 420, or multiple times in a group of such servers. In addition, it may be implemented in a personal computer such as alaptop computer 422. It may also be implemented as part of arack server system 424. Alternatively, components from thecomputing device 400 may be combined with other components in a mobile device (not shown), such as amobile computing device 450. Each of such devices may contain one or more of thecomputing device 400 and themobile computing device 450, and an entire system may be made up of multiple computing devices communicating with each other. - The
mobile computing device 450 includes aprocessor 452, amemory 464, an input/output device such as adisplay 454, acommunication interface 466, and atransceiver 468, among other components. Themobile computing device 450 may also be provided with a storage device, such as a micro-drive or other device, to provide additional storage. Each of theprocessor 452, thememory 464, thedisplay 454, thecommunication interface 466, and thetransceiver 468, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate. - The
processor 452 can execute instructions within themobile computing device 450, including instructions stored in thememory 464. Theprocessor 452 may be implemented as a chipset of chips that include separate and multiple analog and digital processors. Theprocessor 452 may provide, for example, for coordination of the other components of themobile computing device 450, such as control of user interfaces, applications run by themobile computing device 450, and wireless communication by themobile computing device 450. - The
processor 452 may communicate with a user through acontrol interface 458 and adisplay interface 456 coupled to thedisplay 454. Thedisplay 454 may be, for example, a TFT (Thin-Film-Transistor Liquid Crystal Display) display or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology. Thedisplay interface 456 may comprise appropriate circuitry for driving thedisplay 454 to present graphical and other information to a user. Thecontrol interface 458 may receive commands from a user and convert them for submission to theprocessor 452. In addition, anexternal interface 462 may provide communication with theprocessor 452, so as to enable near area communication of themobile computing device 450 with other devices. Theexternal interface 462 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used. - The
memory 464 stores information within themobile computing device 450. Thememory 464 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units. Anexpansion memory 474 may also be provided and connected to themobile computing device 450 through anexpansion interface 472, which may include, for example, a SIMM (Single In Line Memory Module) card interface. Theexpansion memory 474 may provide extra storage space for themobile computing device 450, or may also store applications or other information for themobile computing device 450. Specifically, theexpansion memory 474 may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example, theexpansion memory 474 may be provide as a security module for themobile computing device 450, and may be programmed with instructions that permit secure use of themobile computing device 450. In addition, secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a nonhackable manner. - The memory may include, for example, flash memory and/or NVRAM memory (non-volatile random access memory), as discussed below. In some implementations, instructions are stored in an information carrier. that the instructions, when executed by one or more processing devices (for example, processor 452), perform one or more methods, such as those described above. The instructions can also be stored by one or more storage devices, such as one or more computer- or machine-readable mediums (for example, the
memory 464, theexpansion memory 474, or memory on the processor 452). In some implementations, the instructions can be received in a propagated signal, for example, over thetransceiver 468 or theexternal interface 462. - The
mobile computing device 450 may communicate wirelessly through thecommunication interface 466, which may include digital signal processing circuitry where necessary. Thecommunication interface 466 may provide for communications under various modes or protocols, such as GSM voice calls (Global System for Mobile communications), SMS (Short Message Service), EMS (Enhanced Messaging Service), or MMS messaging (Multimedia Messaging Service), CDMA (code division multiple access), TDMA (time division multiple access), PDC (Personal Digital Cellular), WCDMA (Wideband Code Division Multiple Access), CDMA2000, or GPRS (General Packet Radio Service), among others. Such communication may occur, for example, through thetransceiver 468 using a radio-frequency. In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, a GPS (Global Positioning System)receiver module 470 may provide additional navigation- and location-related wireless data to themobile computing device 450, which may be used as appropriate by applications running on themobile computing device 450. - The
mobile computing device 450 may also communicate audibly using anaudio codec 460, which may receive spoken information from a user and convert it to usable digital information. Theaudio codec 460 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of themobile computing device 450. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on themobile computing device 450. - The
mobile computing device 450 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as acellular telephone 480. It may also be implemented as part of a smart-phone 482, personal digital assistant, or other similar mobile device. - Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms machine-readable medium and computer-readable medium refer to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term machine-readable signal refers to any signal used to provide machine instructions and/or data to a programmable processor.
- To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- The systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (LAN), a wide area network (WAN), and the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- Although a few implementations have been described in detail above, other modifications are possible. For example, while a client application is described as accessing the delegate(s), in other implementations the delegate(s) may be employed by other applications implemented by one or more processors, such as an application executing on one or more servers. In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other actions may be provided, or actions may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other implementations are also according to the invention, provided that their subject-matter is still within the scope of the following claims.
Claims (15)
- A computer-implemented method comprising:receiving, by a computing device, transcriptions of utterances that were received by computing devices operating in a domain and that are in a source language;generating, by the computing device, translated transcriptions of the transcriptions of the utterances in a target language;receiving, by the computing device, a language model for the target language;receiving, by the computing device, additional transcriptions of additional utterances that were received by the computing devices operating in domains other than the domain and that are in the source language;generating, by the computing device, additional translated transcriptions of the additional transcriptions;identifying terms that have a higher appearance frequency in the translated transcriptions than in the additional translated transcriptions;biasing, by the computing device, the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions; andgenerating, by the computing device while operating in the domain, a transcription of an utterance in the target language using the biased language model,wherein biasing the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions comprises:
biasing the language model for the target language by increasing the likelihood of the language model selecting terms that have a higher appearance frequency in the translated transcriptions than in the additional translated transcriptions. - The method of claim 1, wherein generating a transcription of an utterance in the target language using the biased language model comprises:while the computing device is operating in the domain, receiving, by the computing device, audio data of the utterance;providing, by the computing device, the audio data as an input to an acoustic model that is configured to identify phonemes of the utterance;based on the computing device operating in the domain, providing, by the computing device, the phonemes of the utterance as an input to the biased language model; andbased on providing the phonemes of the utterance as the input to the biased language model, generating, by the computing device, a transcription of the utterance.
- The method of claim 1, wherein:the language model for the target language is a general language model, andthe biased language for the target language is a language model specific for computing devices operating in the domain.
- The method of claim 1, comprising:identifying, by the computing device, n-grams that appear in the translated transcription and an appearance frequency of each n-gram; andidentifying, by the computing device, a subset of the n-grams that appear in the translated transcriptions more than a threshold appearance frequency,wherein the computing device biases the language model by increasing the likelihood of the language model selecting the subset of n-grams.
- The method of claim 4, wherein increasing the likelihood of the language model selecting the subset of n-grams comprises:
increasing, for each n-gram in the subset of n-grams, the likelihood by a larger amount based on a larger difference between the appearance frequency for the n-gram and the threshold appearance frequency. - The method of claim 1, comprising:receiving, by the computing device, audio data of the utterances that were received by the computing device operating in the domain and that are in the source language,wherein receiving the transcriptions of the utterances that were received by the computing devices operating in the domain and that are in the source language comprises:
generating, by the computing device, the transcriptions of the utterances that were received by computing devices operating in a domain and that are in a source language. - The method of claim 1, comprising:receiving, by the computing device, grammars in the target language,wherein biasing the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions comprises:
biasing the language model for the target language by increasing the likelihood of the language model selecting grammars that include the terms included in the translated transcriptions. - A system comprising:one or more computers; andone or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:receiving, by a computing device, transcriptions of utterances that were received by computing devices operating in a domain and that are in a source language;generating, by the computing device, translated transcriptions of the transcriptions of the utterances in a target language;receiving, by the computing device, a language model for the target language;receiving, by the computing device, additional transcriptions of additional utterances that were received by the computing devices operating in domains other than the domain and that are in the source language;generating, by the computing device, additional translated transcriptions of the additional transcriptions;identifying terms that have a higher appearance frequency in the translated transcriptions than in the additional translated transcriptions;biasing, by the computing device, the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions; andgenerating, by the computing device while operating in the domain, a transcription of an utterance in the target language using the biased language model,wherein biasing the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions comprises:
biasing the language model for the target language by increasing the likelihood of the language model selecting terms that have a higher appearance frequency in the translated transcriptions than in the additional translated transcriptions. - The system of claim 8, wherein generating a transcription of an utterance in the target language using the biased language model comprises:while the computing device is operating in the domain, receiving, by the computing device, audio data of the utterance;providing, by the computing device, the audio data as an input to an acoustic model that is configured to identify phonemes of the utterance;based on the computing device operating in the domain, providing, by the computing device, the phonemes of the utterance as an input to the biased language model; andbased on providing the phonemes of the utterance as the input to the biased language model, generating, by the computing device, a transcription of the utterance.
- The system of claim 8, wherein:the language model for the target language is a general language model, andthe biased language for the target language is a language model specific for computing devices operating in the domain.
- The system of claim 8, wherein the operations comprise:identifying, by the computing device, n-grams that appear in the translated transcription and an appearance frequency of each n-gram; andidentifying, by the computing device, a subset of the n-grams that appear in the translated transcriptions more than a threshold appearance frequency,wherein the computing device biases the language model by increasing the likelihood of the language model selecting the subset of n-grams.
- The system of claim 11, wherein increasing the likelihood of the language model selecting the subset of n-grams comprises:
increasing, for each n-gram in the subset of n-grams, the likelihood by a larger amount based on a larger difference between the appearance frequency for the n-gram and the threshold appearance frequency. - The system of claim 8, wherein the operations comprise:receiving, by the computing device, audio data of the utterances that were received by the computing device operating in the domain and that are in the source language,wherein receiving the transcriptions of the utterances that were received by the computing devices operating in the domain and that are in the source language comprises:
generating, by the computing device, the transcriptions of the utterances that were received by computing devices operating in a domain and that are in a source language. - The system of claim 8, wherein the operations comprise:receiving, by the computing device, grammars in the target language,wherein biasing the language model for the target language by increasing the likelihood of the language model selecting terms included in the translated transcriptions comprises:
biasing the language model for the target language by increasing the likelihood of the language model selecting grammars that include the terms included in the translated transcriptions. - A non-transitory computer-readable medium storing software comprising instructions executable by one or more computers which, upon such execution, cause the one or more computers to perform the method of any one of claims 1 to 7.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
EP23200212.1A EP4276816A3 (en) | 2018-11-30 | 2019-11-26 | Speech processing |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862773361P | 2018-11-30 | 2018-11-30 | |
PCT/US2019/063272 WO2020112789A1 (en) | 2018-11-30 | 2019-11-26 | Speech processing |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP23200212.1A Division EP4276816A3 (en) | 2018-11-30 | 2019-11-26 | Speech processing |
Publications (2)
Publication Number | Publication Date |
---|---|
EP3867901A1 EP3867901A1 (en) | 2021-08-25 |
EP3867901B1 true EP3867901B1 (en) | 2023-10-04 |
Family
ID=68919792
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP19821483.5A Active EP3867901B1 (en) | 2018-11-30 | 2019-11-26 | Speech processing |
EP23200212.1A Pending EP4276816A3 (en) | 2018-11-30 | 2019-11-26 | Speech processing |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP23200212.1A Pending EP4276816A3 (en) | 2018-11-30 | 2019-11-26 | Speech processing |
Country Status (6)
Country | Link |
---|---|
US (2) | US11138968B2 (en) |
EP (2) | EP3867901B1 (en) |
JP (2) | JP7077487B2 (en) |
KR (1) | KR20210083331A (en) |
CN (1) | CN113168830A (en) |
WO (1) | WO2020112789A1 (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113168830A (en) * | 2018-11-30 | 2021-07-23 | 谷歌有限责任公司 | Speech processing |
US11074908B2 (en) * | 2019-03-29 | 2021-07-27 | Nuance Communications, Inc. | System and method for aligning ASR model weights with NLU concepts |
US11341340B2 (en) * | 2019-10-01 | 2022-05-24 | Google Llc | Neural machine translation adaptation |
Family Cites Families (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1102719A (en) * | 1993-11-06 | 1995-05-17 | 亨利·C·尤恩 | Keyed language translator |
JP3850742B2 (en) | 2002-02-22 | 2006-11-29 | 株式会社国際電気通信基礎技術研究所 | Language model adaptation method |
JP3920812B2 (en) * | 2003-05-27 | 2007-05-30 | 株式会社東芝 | Communication support device, support method, and support program |
JP3955880B2 (en) * | 2004-11-30 | 2007-08-08 | 松下電器産業株式会社 | Voice recognition device |
US8898052B2 (en) * | 2006-05-22 | 2014-11-25 | Facebook, Inc. | Systems and methods for training statistical speech translation systems from speech utilizing a universal speech recognizer |
US8332207B2 (en) * | 2007-03-26 | 2012-12-11 | Google Inc. | Large language models in machine translation |
WO2009129315A1 (en) * | 2008-04-15 | 2009-10-22 | Mobile Technologies, Llc | System and methods for maintaining speech-to-speech translation in the field |
US9043209B2 (en) * | 2008-11-28 | 2015-05-26 | Nec Corporation | Language model creation device |
US9576570B2 (en) * | 2010-07-30 | 2017-02-21 | Sri International | Method and apparatus for adding new vocabulary to interactive translation and dialogue systems |
US8527270B2 (en) * | 2010-07-30 | 2013-09-03 | Sri International | Method and apparatus for conducting an interactive dialogue |
US8798984B2 (en) * | 2011-04-27 | 2014-08-05 | Xerox Corporation | Method and system for confidence-weighted learning of factored discriminative language models |
US9043205B2 (en) * | 2012-06-21 | 2015-05-26 | Google Inc. | Dynamic language model |
US9058805B2 (en) * | 2013-05-13 | 2015-06-16 | Google Inc. | Multiple recognizer speech recognition |
US10290299B2 (en) | 2014-07-17 | 2019-05-14 | Microsoft Technology Licensing, Llc | Speech recognition using a foreign word grammar |
CN104714943A (en) * | 2015-03-26 | 2015-06-17 | 百度在线网络技术（北京）有限公司 | Translation method and system |
US9704483B2 (en) * | 2015-07-28 | 2017-07-11 | Google Inc. | Collaborative language model biasing |
JP6568429B2 (en) * | 2015-08-27 | 2019-08-28 | 日本放送協会 | Pronunciation sequence expansion device and program thereof |
US10049666B2 (en) * | 2016-01-06 | 2018-08-14 | Google Llc | Voice recognition system |
US9978367B2 (en) * | 2016-03-16 | 2018-05-22 | Google Llc | Determining dialog states for language models |
US10347245B2 (en) | 2016-12-23 | 2019-07-09 | Soundhound, Inc. | Natural language grammar enablement by speech characterization |
US11087098B2 (en) * | 2018-09-18 | 2021-08-10 | Sap Se | Computer systems for classifying multilingual text |
CN113168830A (en) * | 2018-11-30 | 2021-07-23 | 谷歌有限责任公司 | Speech processing |
-
2019
- 2019-11-26 CN CN201980078166.6A patent/CN113168830A/en active Pending
- 2019-11-26 JP JP2021530940A patent/JP7077487B2/en active Active
- 2019-11-26 KR KR1020217016466A patent/KR20210083331A/en unknown
- 2019-11-26 EP EP19821483.5A patent/EP3867901B1/en active Active
- 2019-11-26 WO PCT/US2019/063272 patent/WO2020112789A1/en unknown
- 2019-11-26 EP EP23200212.1A patent/EP4276816A3/en active Pending
- 2019-11-26 US US16/696,111 patent/US11138968B2/en active Active
-
2021
- 2021-09-09 US US17/447,282 patent/US11676577B2/en active Active
-
2022
- 2022-05-18 JP JP2022081300A patent/JP7305844B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
WO2020112789A1 (en) | 2020-06-04 |
CN113168830A (en) | 2021-07-23 |
JP7077487B2 (en) | 2022-05-30 |
JP2022110098A (en) | 2022-07-28 |
KR20210083331A (en) | 2021-07-06 |
US20200175963A1 (en) | 2020-06-04 |
EP4276816A2 (en) | 2023-11-15 |
JP7305844B2 (en) | 2023-07-10 |
US20210398519A1 (en) | 2021-12-23 |
EP3867901A1 (en) | 2021-08-25 |
JP2022510280A (en) | 2022-01-26 |
US11676577B2 (en) | 2023-06-13 |
EP4276816A3 (en) | 2024-03-06 |
US11138968B2 (en) | 2021-10-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11804218B2 (en) | Scalable dynamic class language modeling | |
US10896681B2 (en) | Speech recognition with selective use of dynamic language models | |
US9275635B1 (en) | Recognizing different versions of a language | |
US10026398B2 (en) | Follow-up voice query prediction | |
US11676577B2 (en) | Speech processing | |
KR102375115B1 (en) | Phoneme-Based Contextualization for Cross-Language Speech Recognition in End-to-End Models | |
US10229114B2 (en) | Contextual language translation | |
GB2557714A (en) | Determining phonetic relationships | |
US9135912B1 (en) | Updating phonetic dictionaries | |
KR20210013607A (en) | Continuous conversation detection using computing devices | |
Hsiao et al. | Optimizing components for handheld two-way speech translation for an English-Iraqi Arabic system | |
Alhonen et al. | Mandarin short message dictation on symbian series 60 mobile phones | |
Touch et al. | Voice aided input for phrase selection using a low level ASR approach-application to French and Khmer phrasebooks. | |
Ju et al. | Spontaneous Mandarin speech understanding using Utterance Classification: A case study | |
Yadav et al. | Voice Translation on Windows & Linux machine |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: UNKNOWN |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE INTERNATIONAL PUBLICATION HAS BEEN MADE |
|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: REQUEST FOR EXAMINATION WAS MADE |
|
17P | Request for examination filed |
Effective date: 20210521 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
DAV | Request for validation of the european patent (deleted) | ||
DAX | Request for extension of the european patent (deleted) | ||
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTG | Intention to grant announced |
Effective date: 20230419 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230913 |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602019038781Country of ref document: DE |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20231031Year of fee payment: 5 |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG9D |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20231109Year of fee payment: 5Ref country code: DEPayment date: 20231108Year of fee payment: 5 |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20231004 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1618600Country of ref document: ATKind code of ref document: TEffective date: 20231004 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231004 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240105 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240204 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231004 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231004 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231004 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231004Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240204Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240105Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231004Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240104Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231004Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240205 |