CN116508096A - Indexing application actions for speech-based execution - Google Patents
Indexing application actions for speech-based execution Download PDFInfo
- Publication number
- CN116508096A CN116508096A CN202180076737.XA CN202180076737A CN116508096A CN 116508096 A CN116508096 A CN 116508096A CN 202180076737 A CN202180076737 A CN 202180076737A CN 116508096 A CN116508096 A CN 116508096A
- Authority
- CN
- China
- Prior art keywords
- application
- action
- digital assistant
- index
- deep link
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/62—Protecting access to data via a platform, e.g. using keys or access control rules
- G06F21/6209—Protecting access to data via a platform, e.g. using keys or access control rules to a single file or object, e.g. in a secure envelope, encrypted and accessed using a key, or with access control rules appended to the object itself
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/901—Indexing; Data structures therefor; Storage structures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9038—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/602—Providing cryptographic facilities or services
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L9/00—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols
- H04L9/32—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols including means for verifying the identity or authority of a user of the system or for message authentication, e.g. authorization, entity authentication, data integrity or data verification, non-repudiation, key authentication or verification of credentials
- H04L9/321—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols including means for verifying the identity or authority of a user of the system or for message authentication, e.g. authorization, entity authentication, data integrity or data verification, non-repudiation, key authentication or verification of credentials involving a third party or a trusted authority
- H04L9/3213—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols including means for verifying the identity or authority of a user of the system or for message authentication, e.g. authorization, entity authentication, data integrity or data verification, non-repudiation, key authentication or verification of credentials involving a third party or a trusted authority using tickets or tokens, e.g. Kerberos
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Abstract
Indexing application actions for voice-based execution is provided. An indexer of the device receives, from an application executed by the device, an indication of an action declared by the application via an application programming interface and a deep link corresponding to the action. The indexer accesses an index stored in the memory of the device, the index being accessible by a plurality of applications. The indexer enters actions and deep links into locations in the index with cryptographic tokens that prevent unauthorized applications from accessing the deep links. The digital assistant of the device receives the voice query detected by the microphone and parses the voice query to determine an action. The digital assistant identifies a deep link in the index that corresponds to the action. The digital assistant invokes the deep link using the cryptographic token to cause the application to perform an action.
Description
Background
The computing device may execute an application to perform tasks. As new applications are installed on computing devices, or applications perform more and more tasks, efficiently identifying and executing tasks in applications can be challenging.
Disclosure of Invention
The present disclosure relates generally to indexing application actions for speech-based execution. Applications installed on the device may have various capabilities. For example, an application may order food or beverage items, perform electronic commerce, access information provided by a data source, order a ride, make a call, or provide an exercise routine. However, for various reasons, the digital assistant may not be aware of these capabilities, or may not be able to reliably select actions to perform in response to a voice query. For example, due to latency or unreliability associated with selecting an action associated with an application's capabilities, the application may declare a limited number of capabilities to the digital assistant, such as only 5 to 15 capabilities, to facilitate the digital assistant's ability to select in response to a voice-based query, thereby limiting the types of voice queries that the digital assistant can respond to, or requiring additional interactions to perform actions associated with different capabilities of the application.
Thus, the systems and methods of the present technology can provide an application programming interface that allows application declaration actions and corresponding deep links to be stored in an index on a device. The index may be stored on the local device to reduce latency introduced by network communications or performing remote procedure calls. Furthermore, the index may store actions with the deep links in a secure manner that prevents unauthorized applications from invoking the deep links. The application may declare actions and deep links when the application is installed on the device, or when a user performs actions using the application. A digital assistant executing on a device may receive a voice query, identify an action in the voice query, and perform a lookup in an index to identify one or more applications having deep links corresponding to the action. The digital assistant may rank the applications based on usage (e.g., how many times the application has been executed on the device, or how many times to perform an action) or recency (e.g., the last time the application was executed on the device or was executed to perform an action). The digital assistant may then invoke the deep link associated with the highest ranked application.
At least one aspect relates to a system for dynamically indexing application actions for speech-based execution. The system may include a device having one or more processors and memory. The device may include or execute an indexer and a digital assistant. The device may include, execute, or have installed therein one or more applications. The indexer can receive, from an application executed by the device, an indication of an action and a deep link corresponding to the action. The application may declare an indication of the action and the corresponding deep link via the application programming interface. The indexer may access an index stored in a memory of the device that is accessible by a plurality of applications executable by the device. The indexer may enter the action and the deep links into locations in the index with a cryptographic token that forces only authorized applications (e.g., digital assistants) to perform the action or prevents unauthorized ones of the plurality of applications from accessing the deep links in the index. The digital assistant may receive a voice query detected by a microphone of the device. The digital assistant may parse the voice query to determine an action. The digital assistant may perform a lookup in the index to identify a deep link corresponding to the action. The digital assistant may invoke the deep link with the cryptographic token to cause an application executed by the device to perform the action.
At least one aspect relates to a method of dynamically indexing application actions for speech-based execution. The method may be performed by a device having one or more processors and memory. The method may include an indexer executed by the device receiving, from an application executed by the device, an indication of an action declared by the application via an application programming interface and a deep link corresponding to the action. The method may include the indexer accessing an index stored in a memory of the device, the index being accessible by a plurality of applications executable by the device. The method may include the indexer entering actions and deep links into locations in the index with a cryptographic token that prevents unauthorized ones of the plurality of applications from accessing the deep links in the index. The method may include a digital assistant executed by a device receiving a voice query detected by a microphone of the device. The method may include the digital assistant parsing the voice query to determine an action. The method may include the digital assistant performing a lookup in the index to identify a deep link corresponding to the action. The method may include the digital assistant invoking the deep link with the cryptographic token to cause an application executed by the device to perform the action.
These and other aspects and embodiments are discussed in detail below. The foregoing information and the following detailed description include illustrative examples of various aspects and embodiments, and provide an overview or framework for understanding the nature and character of the claimed aspects and embodiments. The accompanying drawings provide a description and a further understanding of various aspects and embodiments, and are incorporated in and constitute a part of this specification.
Drawings
The drawings are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing. In the drawings:
FIG. 1 is an illustration of an example system for indexing application actions for voice-based execution, according to an embodiment;
FIG. 2 is an illustration of an example method of generating an index with application actions for voice-based execution, according to an embodiment;
FIG. 3 is an illustration of an example method for voice-based execution of an action using an index, according to an embodiment;
FIG. 4 is an illustration of an example method of predicting actions using an index of application actions, according to an embodiment; and
fig. 5 is a block diagram illustrating a general architecture of a computer system that may be used to implement elements of the systems and methods described and illustrated herein, including, for example, the system depicted in fig. 1 and the methods depicted in fig. 3, 4, and 5.
Detailed Description
The following is a more detailed description of various concepts and embodiments thereof related to methods, apparatus, and systems for indexing application actions for voice-based execution. The various concepts introduced above and discussed in more detail below may be implemented in any of a variety of ways.
The technical solution generally relates to indexing application actions for speech-based execution or execution by other digital surfaces. The technical scheme may generate an index that securely stores actions and corresponding deep links of an application declaration installed or executing on a device. The digital assistant may receive the voice query, parse the voice query to identify an action, and perform a lookup in the index to identify one or more deep links provided by one or more applications that may perform the action. The digital assistant can rank the applications to select the highest ranked application and invoke the corresponding deep link to perform the action.
For example, the device may include an application that may order coffee from a coffee shop. When the application is installed on the device, the application may declare the ability to order coffee, or the application may dynamically declare the ability to order coffee in response to the application being used to order coffee. For example, a user may launch an application to order a particular coffee from a coffee shop at a particular location. The application, in response to the subscription, may declare or push the subscription and parameters associated with the particular subscription to an indexer of the device via an application programming interface. The declaration may include an indication of an action (e.g., order coffee) and a deep link that may perform the action. The indexer can store actions (or capabilities or intents) in the index and associate the deep links with the actions in the index. The index may store a large number of actions and deep links for each application, such as hundreds, thousands, tens of thousands, or more of actions and deep links for each application installed on the device. For example, each row in the index may include an intended deep link.
The indexer may encrypt the deep links so that unauthorized applications cannot access or invoke the deep links, thereby preventing undesired or malicious execution of actions by unauthorized applications and reducing wasted computing resource utilization, computing resources, or network bandwidth utilization. The indexer may encapsulate the intent with a cryptographic token. The cryptographic token may represent a programmable access right that can only be accessed by an application having a private key of the address and that can only be signed using the private key.
Once the index is generated, the digital assistant executing on the device may invoke the deep links stored in the index in response to a voice query from a user of the device. The digital assistant may parse the voice query and determine intent or actions in the voice query using natural language processing or understanding. In some cases, the voice query may include an indication (e.g., an identifier or name) of the application to be used to perform the action, while in other cases, the voice query may not include an indication of the application to be used. After identifying an action or intent, the digital assistant may perform a lookup in the index to identify one or more actions in the index that correspond to or match the action. If there are multiple deep links for the action for multiple applications, the digital assistant may rank the applications to select the highest ranked application for the action. The digital assistant may use the token or key of the encrypted deep link to invoke the deep link and perform actions.
In an illustrative example, the voice query may be a user-defined voice query, such as "breakfast" or "lunch time," which may result in the performance of one or more actions. In another illustrative example, the voice query may be to order coffee from a coffee shop. The digital assistant may parse the voice query to determine intent, such as performing tasks associated with the morning routine (e.g., viewing news, weather, upcoming meetings), and one or more contacts with whom to communicate or order coffee. The digital assistant may perform a lookup in the index with the intent to identify one or more deep links of one or more applications associated with the intent. The digital assistant may rank the one or more applications to select the highest ranked application and invoke the corresponding one or more deep links to perform an action or intent.
Thus, the digital assistant of the present technology can use natural language understanding to determine intent and access an index storing intent to identify corresponding deep links, and supply calls to perform intent or actions. In some cases, the intent may be associated with parameters that may be entered into a deep link or application to perform an action. For example, for ordering coffee, the parameters may include the size of the coffee. In another example, for communicating with a contact to share lunch, the parameter may include the name of the contact. In some cases, the digital assistant may use the parameters to perform a lookup. For example, if the parameter includes a contact name, the digital assistant may identify an action or deep link for the contact name in the index. The deep link may be for an application configured with information to communicate with the contact name.
The digital assistant may actively invoke an action or deep link, or actively suggest an action to be performed or a deep link to be invoked. Proactive invocation or suggestion may refer to invoking a deep link or suggesting a deep link without receiving a voice query from a user. The digital assistant may identify a trigger condition separate from the voice query and then perform a lookup in the index for an action corresponding to the trigger condition. The trigger condition may be based on geographic location, time of day, day of week, or other interactions with the device. For example, the digital assistant may detect that it is now lunch time and suggest, based on historical data, that the user invoke an action to communicate with one or more contacts to share in lunch. In another example, the digital assistant may suggest that the user invoke an action to order a cup of coffee via a deep link of the action at a time of day when the user typically orders coffee.
Thus, the technical solution may eliminate the limitation on the number of actions, deep links, or shortcuts an application may declare and store in an index on a computing device for subsequent execution. The index may be stored or represented as structured or hierarchical semantics. The index may store shortcuts to coffee ORDERs by association with the ORDER_MENU_ITEM capability of the application in the index XML file. The shortcut of "cappuccino" may be used as a manifest (inventory) to implement the ORDER_MENU_ITEM capability. Shortcuts (e.g., actions, intents, or deep links) may be dynamically declared or donated by an application to be stored in an index XML file.
In addition to voice-based digital assistant scenarios, the present solution can also be used with other digital surfaces, such as typing in advanced searches (e.g., search engine search suggestions or on-device search features), or active suggestions on the device's home screen, or by notifying active suggestions in digital assistance.
FIG. 1 illustrates an example system 100 for indexing application actions for voice-based execution. The system 100 may include a content selection infrastructure. System 100 may include a data processing system 102. Data processing system 102 may communicate with one or more of computing device 110, service provider device 140, or supplemental digital content provider device 142 via network 105. The network 105 may include a computer network such as the internet, a local area network, a wide area network, a metropolitan area network or other area network, an intranet, a satellite network, and other communication networks such as a voice or data mobile phone network. The network 105 may be used to access information resources, such as web pages, websites, domain names, or uniform resource locators, which may be provided, output, presented, or displayed on at least one local computing device 110, such as a laptop computer, desktop computer, tablet computer, digital assistant device, smart phone, mobile telecommunications device, portable computer, or speaker. For example, via network 105, a user of local computing device 110 may access information or data provided by supplemental digital content provider device 142. In some cases, computing device 110 may or may not include a display; for example, a computing device may include a limited type of user interface, such as a microphone and speaker. In some cases, the primary user interface of the computing device 110 may be a microphone and speaker, or a voice interface. In some cases, computing device 110 includes a display device 122 coupled to computing device 110, and a main user interface of computing device 110 may utilize display device 122.
The local computing device 110 may refer to a computing device 110 that is in use by or owned by a user. Local computing device 110 may refer to a computing device or client device located in a private location such as a residence, home, rental apartment, purchase apartment. The computing device 110 may be located in a public place, such as a hotel, office, restaurant, retail store, mall, or park. The location of the computing device 110 may follow the user as the user travels or moves from one location to another. The term local may refer to a computing device being located where a user may interact with the computing device using voice input or other input. The local computing device may be located remotely from a remote server, such as data processing system 102. Thus, for example, local computing device 110 may be located in a private or public location where a user may interact with local computing device 110 using voice input, while data processing system 102 may be remotely located in a data center. The local computing device 110 may be referred to as a digital assistant device.
The network 105 may include or constitute a display network, such as a subset of information resources available on the Internet associated with a content placement or search engine results system, or a subset of information resources available on the Internet that includes a third party digital component as part of a digital component placement activity. Data processing system 102 can use network 105 to access information resources, such as web pages, websites, domain names, or uniform resource locators that can be provided, output, presented, or displayed by local client computing device 110. For example, via network 105, a user of local client computing device 110 may access information or data provided by supplemental digital content provider device 142 or service provider computing device 140.
The network 105 may be any type or form of network and may include any of the following: point-to-point networks, broadcast networks, wide area networks, local area networks, telecommunication networks, data communication networks, computer networks, ATM (asynchronous transfer mode) networks, SONET (synchronous optical network) networks, SDH (synchronous digital hierarchy) networks, wireless networks, and wired networks. The network 105 may include wireless links such as infrared channels or satellite bands. The topology of the network 105 may include a bus, star, or ring network topology. The network may include a mobile telephone network using any one or more protocols for communication between mobile devices, including advanced mobile phone protocol "(AMPS), time division multiple access" (TDMA), code division multiple access ("CDMA"), global system for mobile communications ("GSM"), general packet radio service ("GPRS"), or universal mobile telecommunications system ("UMTS"). Different types of data may be transmitted via different protocols, or the same type of data may be transmitted via different protocols.
System 100 may include at least one data processing system 102. The data processing system 102 may include at least one logic device, such as a computing device having a processor, to communicate with, for example, the computing device 110, the supplemental digital content provider device 142 (or a third party content provider device, content provider device), or the service provider device 140 (or a third party service provider device) via the network 105. Data processing system 102 may include at least one computing resource, server, processor, or memory. For example, data processing system 102 may include multiple computing resources or servers located at least one data center. Data processing system 102 may include multiple logically grouped servers and facilitate distributed computing techniques. The logical group of servers may be referred to as a data center, a server farm, or a machine farm. Servers may also be geographically dispersed. The data center or machine farm may be managed as a single entity, or the machine farm may include multiple machine farms. The servers within each machine farm may be heterogeneous—one or more of the servers or machines may operate in accordance with one or more types of operating system platforms.
Servers in a machine farm may be housed in a high-density rack system along with associated storage systems and located at an enterprise data center. Integrating servers in this manner may improve system manageability, data security, physical security of the system, and system performance by locating servers and high-performance storage systems on a localized high-performance network, for example. Centralizing all or some of the data processing system 102 components (including servers and storage systems) and coupling them with advanced system management tools allows for more efficient use of server resources, which saves power and processing requirements and reduces bandwidth usage.
The system 100 may include at least one third party device, such as a service provider device 140 or a supplemental digital content provider device 142, to access or otherwise interact with. Service provider device 140 may include at least one logic device, such as a computing device having a processor, to communicate with, for example, computing device 110, data processing system 102, or supplemental digital content provider device 142 via network 105. The service provider device 140 may include at least one computing resource, server, processor, or memory. For example, the service provider device 140 may include a plurality of computing resources or servers located at least one data center.
The supplemental digital content provider device 142 may provide audio-based digital components for display by the local computing device 110 as audio output digital components. This digital component may be referred to as a sponsored digital component because it is provided by a third party sponsor. The digital component may include provision of goods or services, such as voice-based messages stating: "do you want me to book you for your taxi? ". For example, the supplemental digital content provider device 142 may include memory to store a series of audio digital components that may be provided in response to a voice-based query. The supplemental digital content provider device 142 may also provide audio-based digital components (or other digital components) to the data processing system 102, where they may be stored in a data store of the data processing system 102. The data processing system 102 may select an audio digital component and provide (or instruct the supplemental digital content provider device 142 to provide) the audio digital component to the client computing device 110. The audio-based digital component may be audio only or may be combined with text, image or video data.
Service provider device 140 may include, interface with, or otherwise communicate with data processing system 102. The service provider device 140 may include, interface with, or otherwise communicate with the local computing device 110. The service provider device 140 may include, interface with, or otherwise communicate with the computing device 110, which computing device 110 may be a mobile computing device. The service provider device 140 may include a supplemental digital content provider device 142 that interfaces with or otherwise communicates with. For example, the service provider device 140 may provide digital components to the local computing device 110 for execution by the local computing device 110. Service provider device 140 may provide digital components to data processing system 102 for storage by data processing system 102.
Data processing system 102 may include a content placement system having at least one computing resource or server. Data processing system 102 may include, interface with, or otherwise communicate with at least one interface 106. The data processing system 102 can include, interface with, or otherwise communicate with at least one natural language processor component 104. The interface 106 or the natural language processor 104 may form or be referred to as a server digital assistant component. Data processing system 102 can include at least one server digital assistant component that interfaces with or otherwise communicates with. The server digital assistant component can communicate or interface with one or more voice-based interfaces or various digital assistant devices or surfaces to provide data or receive data or perform other functions.
Data processing system 102, interface 106, NLP 104, or content selector 108 may each include at least one processing unit or other logic device, such as a programmable logic array engine, or module configured to communicate with a data store or database of data processing system 102. The interface 106, NLP 104, or content selector 108 may be a separate component, a single component, or a portion of the data processing system 102. System 100 and its components, such as data processing system 102, may include hardware elements, such as one or more processors, logic devices, or circuits.
Data processing system 102 can obtain anonymous computer network activity information associated with a plurality of local computing devices 110 (or computing devices or digital assistant devices). The user of the local computing device 110 or the mobile computing device may positively authorize the data processing system 102 to obtain network activity information corresponding to the local computing device 110 or the mobile computing device. For example, data processing system 102 may prompt a user of computing device 110 to agree to obtain one or more types of network activity information. The local computing device 110 may include a mobile computing device, such as a smart phone, tablet computer, smart watch, or a wearable device. The identity of the user of the local computing device 110 may remain anonymous and the computing device 110 may be associated with a unique identifier (e.g., a unique identifier of the user or computing device provided by the user of the data processing system or computing device). The data processing system may associate each observation with a corresponding unique identifier.
Data processing system 102 may include an interface 106 (or interface component) designed, configured, constructed, or operative to receive and transmit information using, for example, data packets. The interface 106 may use one or more protocols (such as a network protocol) to receive and transmit information. The interface 106 may include a hardware interface, a software interface, a wired interface, or a wireless interface. The interface 106 may facilitate converting or formatting data from one format to another. For example, the interface 106 may include an application programming interface that includes definitions for communication between various components, such as software components. The interface 106 may communicate with one or more of the local computing device 110, the supplemental digital content provider device 142, or the service provider device 140 via the network 105.
The data processing system 102 may interface with an application, script, or program (such as an app) installed at the local client computing device 110 to communicate input audio signals to the interface 106 of the data processing system 102 and drive components of the local client computing device to render output audio signals. The data processing system 102 may receive data packets or other signals that include or identify audio input signals.
The data processing system 102 may include a natural language processor ("NLP") 104. For example, data processing system 102 may execute or run NLP 104 to parse an audio signal. For example, the NLP 104 may provide interaction between a person and a computer. NLP 104 can be configured with techniques for understanding natural language and allowing data processing system 102 to derive meaning from human or natural language input. The NLP 104 may include or be configured with machine learning based techniques, such as statistical machine learning. The NLP 104 may parse the input audio signal using decision trees, statistical models, or probabilistic models. For example, NLP 104 may perform functions such as named entity recognition (e.g., given a text stream, determining which items in the text map to proper names, such as people or places, and what the type of each such name is, such as people, places, or organizations), natural language generation (e.g., converting information or semantic intent from a computer database into understandable human language), natural language understanding (e.g., converting text into more formal representations, such as first order logical structures that a computer module can manipulate), machine translation (e.g., automatically translating text from one human language into another), lexical segmentation (e.g., separating words into individual morphemes and identifying classes of morphemes, which may be challenging based on the lexical or structural complexity of the words of the language being considered), question answering (e.g., determining answers to human language questions, which may be specific or open), semantic processing (e.g., processing that may occur after identifying words and encoding their meanings to correlate the identified words with other words having similar meanings).
The NLP 104 may convert the audio input signal into recognized text by comparing the input signal to a stored set of representative audio waveforms and selecting the closest match. The set of audio waveforms may be stored in a data repository or other database accessible to data processing system 102. The representative waveforms are generated in a large group of users and may then be augmented with speech samples from the users. After the audio signal is converted to recognized text, the NLP 104 matches the text with words that are associated with actions that the data processing system 102 can provide, for example, via training across users or through manual specification. Aspects or functions of NLP 104 may be performed by data processing system 102 or local computing device 110. For example, a local NLP component may execute on local computing device 110 to perform aspects of converting an input audio signal into text and transmitting the text via data packets to data processing system 102 for further natural language processing.
The audio input signal may be detected by a sensor 114 or transducer 112 (e.g., a microphone) of the local client computing device 110. Via transducer 112, audio driver 116, or other component, local client computing device 110 may provide audio input signals to data processing system 102 (e.g., via network 105), where audio input signals may be received (e.g., through interface 106) and provided to NLP 104 or stored in a data store.
The data processing system 102 may receive data packets from the digital assistant 124 via the interface, the data packets including filtered (or unfiltered) input audio signals detected by the sensors. Data processing system 102 can process data packets to perform actions or otherwise respond to voice inputs. In some cases, the data processing system 102 may identify an acoustic signature from the input audio signal. The data processing system 102 may identify an electronic account corresponding to the acoustic signature based on a lookup in a data store (e.g., querying a database). In response to identifying the electronic account, data processing system 102 can establish a session and an account for use in the session. The account may include a profile with one or more policies. The data processing system 102 may parse the input audio signal to identify a request and a trigger key corresponding to the request.
The NLP 104 may obtain an input audio signal. In response to digital assistant 124 detecting the trigger key, NLP 104 of data processing system 102 may receive data packets with voice input or input audio signals. The trigger key may be a wake signal or hotword that instructs the local computing device 110 to convert the subsequent audio input into text and transmit the text to the data processing system 102 for further processing.
Upon receiving the input audio signal, the NLP 104 may identify at least one request or at least one keyword corresponding to the request. The request may indicate an intention or topic of the input audio signal. The keywords may indicate the type of action that may be taken. For example, the NLP 104 may parse the input audio signal to identify at least one request to attend dinner and a movie at night away from home. The trigger key may include at least one word, phrase, root or part or derivative indicating an action to be taken. For example, a trigger keyword "go" or "to go to" from an input audio signal may indicate a need for shipment. In this example, the input audio signal (or identified request) does not directly express the intent of the shipment, but triggers an ancillary action that the keyword indicates that the shipment is at least one other action for which an indication is requested. In another example, the voice input may include a search query, such as "find work in the vicinity of me.
The NLP 104 may parse the input audio signal to identify, determine, retrieve, or otherwise obtain the request and one or more keywords associated with the request. For example, the NLP 104 may apply semantic processing techniques to the input audio signal to identify keywords or requests. The NLP 104 may apply semantic processing techniques to the input audio signal to identify keywords or phrases that include one or more keywords (such as a first keyword and a second keyword). For example, the input audio signal may include the sentence "I want to purchase an audiobook (i want to purchase audio books)". The NLP 104 may apply semantic processing techniques or other natural language processing techniques to the data packets that comprise the sentence to identify keywords or phrases "want to purchase" and "audioook". The NLP 104 can also identify a number of keywords, such as purchase and audiook. For example, NLP 104 may determine that the phrase includes a first keyword and a second keyword.
The NLP 104 may filter the input audio signal to identify trigger keywords. For example, the data packets carrying the input audio signal may include "It would be great if I could get someone that could help me go to the airport", in which case the NLP 104 may filter out one or more of the following terms: "it", "would", "be", "great", "if", "I", "could", "get", "somerone", "that", "could" or "hellp". By filtering out these terms, the NLP 104 can more accurately and reliably identify trigger keywords, such as "go to the airport," and determine that this is a request for taxi or carpool services.
In some cases, NLP 104 may determine that a data packet carrying an input audio signal includes one or more requests. For example, the input audio signal may include the sentence "I want to purchase an audiobook and monthly subscription to movies (i want to purchase audio books and subscribe to movies monthly)". The NLP 104 can determine that this is a request for audio books and streaming multimedia services. NLP 104 can determine whether this is a single request or multiple requests. The NLP 104 can determine that this is two requests: a first request to a service provider providing an audio reading and a second request to a service provider providing a movie stream. In some cases, NLP 104 may combine multiple determined requests into a single request and transmit the single request to service provider device 140. In some cases, NLP 104 may transmit a separate request to another service provider device, or both requests separately to the same service provider device 140.
The digital assistant 124 may transmit a request for content to the content selector 108. The digital assistant 124 may transmit a request for supplemental or sponsored content from a third party content provider. The digital assistant 124 may transmit the request in response to detecting an action or invoking a deep link. The content selector 108 may perform a content selection process to select either the supplemental content item or the sponsored content item based on an action in the voice query. The content item may be a sponsored or supplemental digital component object. The content items may be provided by a third party content provider, such as a supplemental digital content provider device 142. The supplemental content items may include advertisements for goods or services. The content selector 108 may use content selection criteria to select content items in response to receiving a request for content from the digital assistant 124.
The digital assistant 124 may receive supplemental or sponsored content items from the content selector 108. The digital assistant 124 may receive the content item in response to the request. The digital assistant 124 may receive content items from the content selector 108 and present the content items via an audio output or a visual output.
The data processing system 102 may include a content selector 108, the content selector 108 being designed, constructed, or operative to select a supplemental content item (or sponsored content item or digital component object). To select sponsored content items or digital components, the content selector 108 may use the generated content selection criteria to select matched sponsored content items based on broad matches, exact matches, or phrase matches. For example, the content selector 108 may analyze, parse, or otherwise process the topics of the candidate sponsored content items to determine whether the topics of the candidate sponsored content items correspond to the topics (e.g., actions or intents) of keywords or phrases of the content selection criteria. The content selector 108 may use image processing techniques, character recognition techniques, natural language processing techniques, or database lookup to identify, analyze, or discern speech, audio, terms, characters, text, symbols, or images of the candidate digital components. The candidate sponsored content item may include metadata indicating a topic of the candidate digital component, in which case the content selector 108 may process the metadata to determine whether the topic of the candidate digital component corresponds to the input audio signal. The content campaign provided by the supplemental digital content provider device 142 may include content selection criteria that the data processing system 102 may match with criteria indicated in the second profile layer or the first profile layer.
The supplemental digital content provider may provide additional indicators when setting up a content campaign that includes digital components. The supplemental digital content provider device 142 may provide content activity or content group level information that the content selector 108 may identify by performing a lookup using information about candidate digital components. For example, the candidate digital component may include a unique identifier that may be mapped to a content group, content campaign, or content provider.
In response to the request, the content selector 108 may select a digital component object associated with the supplemental digital content provider device 142. The supplemental digital content may be provided by a supplemental digital content provider device other than the service provider device 140. The supplemental digital content may correspond to a service type that is different from the service type of the action data structure (e.g., taxi service versus meal service). Computing device 110 may interact with the supplemental digital content. Computing device 110 may receive an audio response to the digital component. The computing device 110 may receive an indication of a selection of a hyperlink or other button associated with the digital component object that causes or allows the computing device 110 to identify the supplemental digital content provider device 142 or service provider device 140, request a service from the supplemental digital content provider device 142 or service provider device 140, instruct the supplemental digital content provider device 142 or service provider device 140 to perform the service, transmit information to the supplemental digital content provider device 142 or service provider device 140, or otherwise query the supplemental digital content provider device 142 or service provider device 140.
The supplemental digital content provider device 142 may establish an electronic content campaign. An electronic content campaign may refer to one or more groups of content corresponding to a common topic. The content campaign may include a hierarchical data structure including content groups, digital component data objects, and content selection criteria provided by the content provider. The content selection criteria provided by the content provider device 142 may include a content type, such as a digital assistant content type, a search content type, a streaming video content type, a streaming audio content type, or a contextual content type. To create a content campaign, the supplemental digital content provider device 142 may specify a value for a campaign level parameter of the content campaign. The activity level parameters may include, for example, the activity name, the preferred content network for placing the digital component object, the values of the resources to be used for the content activity, the start and end dates of the content activity, the duration of the content activity, the schedule in which the digital component object is placed, the language, the geographic location, the type of computing device on which the digital component object is provided. In some cases, an impression (compression) may refer to when a component object is acquired from its source (e.g., data processing system 102 or supplemental digital content provider device 142) and is countable. In some cases, robot activity may be filtered and excluded as an impression due to the likelihood of click fraud. Thus, in some cases, an impression may refer to a measurement of a response from a Web server to a page request from a browser, filtered from robot activity and error codes, and recorded at a point as close as possible to the opportunity to render a digital component object for display on computing device 110. In some cases, an impression may refer to an observable or audible impression; for example, the digital component object is at least partially (e.g., 20%, 30%, 40%, 50%, 60%, 70%, or more) viewable on the display device 122 of the client computing device 110 or audible via a speaker (e.g., transducer 112) of the computing device 110. Clicking or selecting may refer to user interactions with the digital component object, such as a voice response to an audible impression, a mouse click, a touch interaction, a gesture, a shake, an audio interaction, or a keyboard click. Conversion may refer to a user taking a desired action with respect to a digital component object; such as purchasing a product or service, completing a survey, visiting a physical store corresponding to the digital component, or completing an electronic transaction.
The supplemental digital content provider device 142 may also establish one or more content groups for the content campaign. The content set includes one or more digital component objects and corresponding content selection criteria, such as keywords, words, terms, phrases, geographic locations, type of computing device, time of day, interests, topics, or verticals. Content groups under the same content activity may share the same activity level parameters, but may have customized specifications for particular content group level parameters, such as keywords, negative keywords (e.g., preventing placement of digital components on primary content if negative keywords are present), bids for keywords, or parameters associated with bids or content activities.
To create a new content group, the content provider may provide values of content group level parameters of the content group. Content group level parameters include, for example, content group names or content group titles, and bids for different content placement opportunities (e.g., automatic placement or managed placement) or results (e.g., clicks, impressions, or conversions). The content group name or content group title may be one or more terms that the supplemental digital content provider device 142 may use to capture topics or subjects for which the digital component objects of the content group are to be selected for display. For example, an automobile dealer may create a different set of content for each brand of vehicle it holds, and may also create a different set of content for each model of vehicle it holds. Examples of content group titles that an automobile dealer may use include, for example, "brand a sports car," brand B sports car, "" brand C truck, "" brand C hybrid car, "or" brand D hybrid car. For example, an example content campaign topic may be "hybrid vehicle" and include a content set of both "brand C hybrid vehicle" and "brand D hybrid vehicle".
The supplemental digital content provider device 142 may provide one or more keywords and digital component objects to each content group. Keywords may include terms related to a product or service associated with or identified by a digital component object. Keywords may include one or more terms or phrases. For example, an automobile dealer may include "sports cars", "V-6 engines", "four wheel drive", "fuel efficiency" as keywords for a content group or content campaign. In some cases, the content provider may specify negative keywords to avoid, prevent, block, or disable content placement with respect to certain terms or keywords. The content provider may specify a type of match, such as exact match, phrase match, or broad match, for selecting the digital component object.
The supplemental digital content provider device 142 may provide one or more keywords for use by the data processing system 102 to select digital component objects provided by the supplemental digital content provider device 142. The supplemental digital content provider device 142 can identify one or more keywords for which to bid and further provide the bid amounts for the various keywords. Supplemental digital content provider device 142 may provide additional content selection criteria for use by data processing system 102 to select digital component objects. Multiple supplemental digital content provider devices 142 can bid on the same or different keywords and the data processing system 102 can run a content selection process or advertisement auction in response to receiving an indication of the keywords of the electronic message.
The supplemental digital content provider device 142 may provide one or more digital component objects for selection by the data processing system 102. The data processing system 102 (e.g., via the content selector 108) can select a digital component object when a content placement opportunity that matches resource allocation, content schedule, maximum bid, keywords, and other selection criteria specified for a content group becomes available. Different types of digital component objects may be included in the content group, such as voice digital components, audio digital components, text digital components, image digital components, video digital components, multimedia digital components, digital component links, or assistant application components. The digital component object (or digital component, supplemental content item, or sponsored content item) may include, for example, a content item, an online document, audio, an image, video, multimedia content, sponsored content, or an assistant application. Upon selection of a digital component, data processing system 102 can transmit the digital component object for presentation on computing device 110 or display device 122 of computing device 110. Rendering may include displaying the digital components on a display device, executing an application such as a chat robot or a conference robot, or playing the digital components via speakers of the computing device 110. Data processing system 102 can provide instructions to computing device 110 to present digital component objects. The data processing system 102 can instruct the computing device 110 or an audio driver 116 of the computing device 110 to generate an audio signal or sound wave.
The content selector 108 may perform a real-time content selection process in response to the request. Real-time content selection may refer to or include performing content selection in response to a request. Real-time may refer to or include selecting content within 0.2 seconds, 0.3 seconds, 0.4 seconds, 0.5 seconds, 0.6 seconds, or 1 second of receiving a request. Real-time may refer to selecting content in response to receiving an input audio signal from computing device 110.
The content selector 108 may identify a plurality of candidate supplemental content items. The content selector 108 may determine a score or ranking for each of the plurality of candidate supplemental content items to select the highest ranked supplemental content item for provision to the computing device 110.
Computing device 110 may include, interface with, or otherwise communicate with at least one transducer 112. Computing device 110 may include, interface with, or otherwise communicate with at least one sensor 114. Computing device 110 may include, interface with, or otherwise communicate with at least one audio driver 116. Computing device 110 may include, interface with, or otherwise communicate with at least one indexer 118. Computing device 110 may include, interface with, or otherwise communicate with at least one application programming interface "(API") 118. Computing device 110 may include, interface with, or otherwise communicate with at least one application 120. Computing device 110 may include, interface with, or otherwise communicate with at least one display device 122. Computing device 110 may include, interface with, or otherwise communicate with at least one digital assistant 124. The digital assistant 124 may include at least one natural language processor ("NLP") 126. The digital assistant 124 may include at least one deep link selector 128. The digital assistant may include at least one motion predictor 130. One or more components of device computing device 110 may each include at least one processing unit or other logic device, such as a programmable logic array engine, component, or module. One or more components of device 110, such as indexer 118, digital assistant 124, NLP 126, deep link selector 128, or action predictor 130, may be separate components or a single component. The system 100 and its components may include hardware elements, such as one or more processors, logic devices, or circuits.
The local computing device 110 may include a display device 122, such as a light indicator, a light emitting diode ("LED"), an organic light emitting diode ("OLED"), or other visual indicator configured to provide visual or optical output. The sensor 114 may include, for example, an ambient light sensor, a proximity sensor, a temperature sensor, an accelerometer, a gyroscope, a motion detector, a GPS sensor, a position sensor, a microphone, or a touch sensor. Transducer 112 may include a speaker or microphone. The audio driver 116 may provide a software interface to the hardware transducer 112. The audio driver may execute an audio file or other instructions provided by the data processing system 102 to control the transducer 112 to generate corresponding sound waves or sound waves.
The sensor 114 may receive or detect an input audio signal (e.g., a voice input). The digital assistant 124 may be coupled to audio drivers, transducers, and sensors. The digital assistant 124 may filter the input audio signal to create a filtered input audio signal (e.g., by removing certain frequencies or suppressing noise). The digital assistant 124 may convert the filtered input audio signal into data packets (e.g., using a software or hardware digital-to-analog converter). In some cases, digital assistant 124 may convert the unfiltered input audio signal into data packets and transmit the data packets to data processing system 102. The digital assistant 124 may transmit the data packets to the data processing system 102 that includes one or more processors and memory that execute the natural language processor components.
The data store 132 may include one or more local or distributed databases and may include a database management system. The data store 132 may include computer data storage or memory and may store one or more of an index 134, usage data 136, or models 138. Index 134 may refer to or include one or more lines of data that include deep links for application 120 to perform tasks or actions. Deep links may refer to or include uniform resource locations, references, pointers, locations, or other indications of services or resources in the application 120 or provided by the application 120. Usage data 136 may include information regarding previous executions of one or more applications 120. The usage data may indicate when the application 120 was launched (e.g., date and time stamp), duration of usage, or what action the application 120 was launched to perform. Model 138 may refer to or include a machine learning model trained using machine learning techniques. Model 138 may include predictive information. Model 138 may predict actions to be performed based on input trigger conditions, such as a timestamp or geographic location.
Computing device 110 may include one or more applications 120. One or more applications 120 may be installed on computing device 110. The application 120 may be downloaded from an online application marketplace, such as the online marketplace provided by the data processing system 102. The applications 120 may include native applications installed by a manufacturer of the computing device 110 on an operating system of the computing device 110. The applications 120 may include any type of application that may provide resources or services. For example, the application 120 may be an exercise routine application, a meal ordering application, a car calling application, a weather application, a document processing application, a navigation application, a messaging application, a telephony application, a streaming media application, a social networking application, a calendar application, a camera application, a ticketing application, an e-commerce application, a banking application, a financial services application, and the like.
Computing device 110 may include an indexer 118 designed, constructed, and operative to generate, manage, maintain, update, or otherwise provide an index 134. The indexer 118 can receive information from one or more applications 120 installed on the computing device 110. Indexer 118 can receive information declared by one or more applications 120. The application 120 may declare information in response to being installed on the computing device 110. The application 120 may declare information in response to being invoked, launched, or executed on the computing device 110. The application 120 may declare the information in response to performing a particular task or action. In some cases, the application 120 may declare the information in response to a request from the indexer 118.
Indexer 118 can receive, via API 144, information declared from application 120. The API 144 may be configured to receive a particular type of information from the application 120 and provide that information to the indexer 118 for indexing. The API 144 may be built into the application 120. The API 144 may be built into the indexer 118. The application 120 may be configured to communicate with the indexer 118 via the API 144. The API 144 may refer to a software development kit ("SDK") provided by the data processing system 102 to a developer of the application 120 for inclusion in the application 120.
For example, the API 144 may be installed in the application 120 or be part of the application 120. The API 144 (or the application 120) may identify the capabilities of the application 120. A capability may refer to an action or intent that the application 120 may perform. The capability may be to order coffee, order rides via a carpool service, view weather, make phone calls, send text messages, etc. The API 144 or the application 120 may also identify deep links of the application 120 that may cause the application 120 to perform actions. For example, the deep links may include uniform resource locators, references, identifiers, pointers, scripts, or other code that may cause the application 120 to perform actions in response to a call. For example, invocation of a deep link may cause application 120 to launch and make a telephone call to a particular contact. In another example, a call to a deep link may cause the application 120 to launch and order coffee from a particular coffee shop.
Using the SDK or API 144, the application 120 can provide actions and deep links that are personalized for the device. For example, "latte of 2% milk at coffee shop a on elcamino" deep links. The application 120 may rank the deep links of actions of the same application.
The indexer 118 can receive from the application 120 an indication of an action declared by the application via the API 144 and a deep link corresponding to the action. Indexer 118 can access index 134 stored in data store 132 (e.g., memory of device 110). Indexer 118 can input actions and deep links into positions in index 134. The location may refer to a row in the index, as shown in table 1 below.
Action | Deep linking | Parameters (parameters) |
Text message | application_1://send_SMS | Contact_ID_1 |
Ordering beverages | application_2://order_coffee | Size, milk quantity, sugar quantity |
Table 1: example index
Table 1 shows an example index 134 that may be generated, maintained, or updated by the indexer 118 and used by the deep link selector 128 to invoke the deep link. Deep links may refer to hyperlinks or uniform resource identifiers ("URIs") that link to particular digital resources or content or services in the application 120. The deep link may link to a particular digital resource or service, rather than a home screen or page of the application 120, or simply launch the application 120. Deep linking of the application 120 may include using a URI linked to a particular location within the application 120, rather than merely launching the application.
Indexer 118 can store deep links and actions in index 134. Index 134 may be configured to be accessible only to indexer 118, digital assistant 124, and application 120 authorized by the developer of deeply-linked application 120. The indexer 118 can store the deep links with the cryptographic tokens in the locations in the index 134. The cryptographic token may be designed to allow the target application 120 to verify that the deep link is intended from a shortcut in the index and is not arbitrarily built by another application 120. For example, the cryptographic token may allow the target application 120 to verify that the deep link is authentic.
In some cases, the actions stored in index 134 may be performed by all applications 120 or may be proprietary to a particular application 120. The cryptographic token may allow the application 120 to issue shortcuts supported by a "public" action, but refuse to perform the action unless the action or deep link has a token. This may limit the ability to invoke a shortcut to the indexer 118 or digital assistant 124 because only the digital assistant 120 or indexer 118 may access actions and deep links (or other metadata, such as tokens).
For example, the first application 120 may publish the following deep links: "app:// orderFoodFood = burger & token = abcd). The application 120 may expose a URL scheme that may allow any other application 120 to call "app:// orderFoodfood = pizza". However, since the first application 120 verifies the token, only the system or component of the computing device 110 that owns the token can use the URL scheme. Thus, the cryptographic token may ensure that only authorized applications or components can access or invoke deep links.
Index 134 may be accessed by a plurality of applications 120 executable by device 110. For example, index 134 may include multiple rows of deep links and corresponding actions. Deep links may be to various applications 120. Accordingly, various applications 120 may declare information that indexer 118 may store in index 134. The index 134 may be used to invoke deep links of different applications 120. When the deep link is invoked, the application 120 that provided the deep link may be launched to perform the action. When launched via a deep link, the application 120 may receive information about the deep link or action or other parameters associated with the deep link. In some cases, the application 120 may be authorized to access the deep links stored in the index. In some cases, the application 120 may be able to parse the deep link to perform the deep link. The application 120 may access the deep links in the index 134 provided by the application 120 to invoke the deep links to perform the corresponding actions.
To ensure that the application invoking the deep link or action is authorized to do so and access the index (or otherwise prevent unauthorized applications 120 from accessing the deep link stored in the index 134), the indexer 118 may encapsulate the deep link with a cryptographic token. For example, applications 120 that obtain deep links through some other means but do not have access to the token cannot invoke deep links if they do not have access to the index. The cryptographic token may represent a programmable access right that can only be accessed by an application 120 having a private key for that address or location in the index and can only be signed using that private key. Indexer 118 can access the private key of application 120 to encapsulate the deep link in index 134. The indexer 118 can provide the private key of the application to the digital assistant 124 to allow the digital assistant 124 to access the index 134 to invoke the deep link. However, the indexer 118 may not provide the private key to the different application 120 that does not provide the deep link. For example, the first application 120 may declare an action and a corresponding deep link. The indexer 118 can encapsulate the deep links with a cryptographic token accessible through the first private key. The first application 120 may access the private key. However, the second application 120 downloaded and installed on the device 110 may not have access to the private key and therefore is prevented or precluded from accessing the deep link provided by the first application 120 because the indexer 118 encapsulates the deep link with the cryptographic token. Thus, the indexer 118 may generate a security index 134 configured to prevent unauthorized applications 120 from invoking deep links, thereby preventing or reducing undesirable activity, malicious action execution, wasted computing resource utilization, or wasted network bandwidth utilization.
Indexer 118 can intercept requests to invoke deep links in index 134 and confirm that the caller of the deep link (e.g., digital assistant 124) is authorized to access the particular deep link before granting access.
The indexer 118 can encrypt the deep links using security or encryption techniques. For example, indexer 118 can lock the position of index 134 or the deep link using a cryptographic key and provide the cryptographic key to application 120 that declares the deep link. The cryptographic key may be a long-term key, such as a static key or an archive key. The cryptographic key may last for a period of time, such as 1 day, 2 days, 3 days, 7 days, 30 days, 60 days, 90 days, 1 year, or more, allowing the digital assistant 124 to invoke the deep links stored in the index 134 without the indexer 118 having to continually update or regenerate the index 134.
The indexer 118 can receive a plurality of actions corresponding to a plurality of deep links declared by one or more applications 120. The one or more applications 120 may declare the deep link in response to installation of the application 120 or dynamically declare the deep link in response to the application 120 performing an action. For example, the indexer 118 can receive an action declared by the application 120 and a corresponding deep link in response to a previous execution of the action by the application 120 (such as when a user of the device 110 launches the application 120 to execute the action). The indexer 118 can store the deep links from the different applications 120 with different cryptographic tokens or cryptographic keys. Indexer 118 can store different deep links in different locations or in different rows in index 134. By storing the deep links with different cryptographic tokens or keys, the indexer 118 may prevent applications without cryptographic keys from accessing the deep links at that location in the index.
The computing device 110 may include a digital assistant 124, which digital assistant 124 is designed, constructed, and operative to receive voice queries or other audio inputs detected by the sensors 114 of the device 110, determine intent or actions based on the voice inputs, and invoke deep links to perform the actions. Digital assistant 124 may include NLP 126.NLP 126 may include one or more components or functions of NLP 104 of data processing system 102. For example, the NLP 126 of the computing device 110 may be referred to as a local NLP 126, while the NLP 104 of the data processing system 102 may be referred to as a server NLP 104. Local NLP 126 may interface or communicate with server NLP 104 to parse or process voice input or audio input. In some cases, the local NLP 126 may be configured to process or parse speech input without interfacing or communicating with the server NLP 104. For example, computing device 110 may perform digital assistant functions without communicating with data processing system 102 via network 105. Computing device 110 may receive the voice query, parse the voice query, recognize the action, and invoke application 120 to perform the corresponding deep link without communicating with data processing system 102 via network 105.
For example, local NLP 126 may receive a voice query or voice input from sensor 114 or transducer 112 or other microphone of device 110. Local NLP 126 may detect keywords in a voice query. In some cases, NLP 126 may detect trigger keywords, hotwords, wake words, or other keywords that indicate that digital assistant 124 is to perform an action in response to a voice query. In some cases, local NLP 126 may transmit a voice query to server NLP 104 in response to detecting a trigger keyword to allow server NLP 104 to process the voice query and determine actions, intent, or otherwise perform other semantic understanding of the voice query. In some cases, the local NLP 126 may be configured to perform semantic understanding on the voice query to identify intent or actions in the voice query.
Local NLP 126 (or local NLP 126 via server NLP 104) may recognize actions or intent in a voice query using semantic processing or other semantic understanding. The local NLP 126 may identify actions or intent in the voice query, such as ordering coffee from a coffee shop. Digital assistant 124 may include, interface with, or otherwise access a deep link selector 128, the deep link selector 128 being designed, constructed, and operative to select a deep link based on actions or intent in a voice query identified by local NLP 126.
The deep link selector 128 may identify a plurality of deep links associated with the action. For example, multiple applications 120 may have declared multiple deep links of the same action. Computing device 110 may include multiple applications 120 configured to perform the same action, and each of these multiple applications 120 may declare a deep link for the action. For example, computing device 110 may have installed two different call applications 120 that may reserve a ride to bring a user from a first destination to a second destination. The deep link selector 128 may use one or more techniques to select a particular deep link for a particular application 120 that is invoked in response to an action in a voice query.
For example, the voice query may indicate an application 120 to be used to perform an action. If the voice query includes an indication of an application 120 (e.g., a name of the application or other identifier of the application 120), the deep link selector 128 may select a deep link in the index 134 that corresponds to the identified application 120. The voice query may include an indication or identifier of the application 120 to be used to perform the action. The digital assistant 124 may parse the voice query using natural language processing to identify an indication of an application in the voice query. The deep link selector 128 may perform a lookup in the index 134 to identify one or more deep links declared by the application 120 identified in the voice query. The deep link selector 128 may perform a lookup to identify a deep link corresponding to an action declared by the application 120. The deep link selector 128 may then invoke a deep link of the application 120 to perform an action in the voice query.
However, if the voice query does not include an indication of the application 120, the deep link selector 128 may determine whether the voice query includes parameters of the action. The parameter may refer to a particular value, input, or other instruction that the application 120 may use to execute the deep link. However, not all applications may perform actions with this parameter. For example, the action may include calling a particular contact identifier. The first application 120 may not have access to the contact information for the particular contact identifier, while the second application 120 may have access to the contact information for the particular contact identifier. Index 134 may store the deep links along with the available parameters. The deep link selector 128 may perform a lookup in the index 134 for the action as well as the parameters to identify candidate deep links configured to perform the action using the parameters. Accordingly, the deep link selector 128 may select a deep link declared by the application 120 to perform an action based on the application 120 being configured to perform the action in the voice query with the parameter.
The deep link selector 128 may identify a plurality of candidate deep links in response to a lookup of actions identified in the voice query. The deep link selector 128 may rank candidate deep links or applications 120 corresponding to the deep links using the usage data 136. The usage data 136 may indicate a date or time stamp for each application 120 associated with the candidate deep link. The deep link selector 128 may rank the applications 120 based on how recently the applications 120 were executed on the computing device 110 and select the applications 120 associated with the candidate deep links that were recently executed on the device 110.
For example, the deep link selector 128 may perform a lookup with an action in the index 134 to identify a deep link declared by the first application 120 and a second deep link declared by the second application 120 in response to the action. The deep link selector 128 may select a deep link declared by the application to perform an action based on historical execution of the application and the second application on the device. Historical execution may refer to the deep link selector selecting an application that was recently executed on computing device 110, or the most recently executed application on computing device 110 during a window of time (or all times).
The deep link selector 128 may determine the execution frequency of each application 120 over a time interval from the usage data. For example, the deep link selector 128 may determine the number of times the application 120 was executed or launched on the computing device 110 over the last 24 hours, 48 hours, 72 hours, 7 days, 30 days, 60 days, 90 days, or other time interval. In another example, the deep link selector 128 may determine which application is executed at the highest frequency on a particular day of the week (e.g., weekday versus weekend) or time of day (e.g., between 9 and 12 am, 12-6 pm, or 6 pm and 9 am). The deep link selector 128 may rank the applications 120 associated with the candidate deep links based on the execution frequency during the time interval such that the highest ranked application 120 corresponds to the highest execution frequency. The deep link selector 128 may select candidate deep links for the application 120 having the highest execution frequency.
Upon selecting a deep link for application 120, deep link selector 128 may invoke the deep link to perform an action. Invoking the deep link may include launching the application 120 and executing the application. The digital assistant 124 may access the cryptographic token used by the indexer 118 to securely store the deep link in the index 134. The digital assistant 124 may use the deep link and a cryptographic token or cryptographic key of the application 120 to access the deep link and invoke the deep link to cause the application 120 executed by the device to perform an action.
The digital assistant 124 may actively recognize actions to be performed. The digital assistant 124 may perform actions or suggest actions independent of receiving a voice query from a user of the computing device 110. Computing device 110 may suggest a deep link of application 120 to a user of computing device 110 without the user entering a voice query. For example, the digital assistant 124 may include an action predictor 130 that is designed, constructed, and operative to identify a trigger condition or trigger event. The trigger condition is based on one or more trigger parameters, such as a time stamp, a time interval, a geographic location, a mode of transportation (e.g., driving, walking, running, riding, trains, buses), and the like. The action predictor 130 may identify one or more trigger conditions based on the trigger parameters. The action predictor 130 may determine a trigger condition based on the trigger parameter. For example, the action predictor 130 may determine that a user historically ordering coffee from a coffee shop with the computing device 110 is located at a work location at 9 am. The action predictor 130 may determine that the current time is 9 am (e.g., a first trigger parameter) and that the geographic location corresponds to the operational location (e.g., a second trigger parameter). The action predictor 130 may input the first trigger parameter and the second trigger parameter into a model 138 to predict an action (e.g., order coffee).
Model 138 may be trained using machine learning techniques, such as statistical techniques, regression techniques, neural networks, or other machine learning techniques. The training data may include historical parameters associated with actions performed on the computing device 110. For example, the digital assistant 124 may train the model 138 using data or parameters associated with actions performed by the application 120 to predict the actions based on the trigger parameters. The data may include a timestamp or location or other information associated with historical execution of the application performing the action. After training model 138, action predictor 130 may input one or more trigger parameters into model 138 to predict an action to be performed. Model 138 may be trained to predict actions based on historical behavior or habits of the user.
After predicting the action, the action predictor 130 may provide the action to the deep link selector 128. The deep link selector 128 may receive the predicted action and perform a lookup in the index 134 to identify one or more deep links declared by the application 120 in response to the action. The deep link selector 128 may select the highest ranked deep link. The deep link selector may present the deep link or application 120 as a suggestion via the display device 122 or audio output. The digital assistant 124 may provide a visual prompt or an audio prompt (e.g., an audio query) requesting to invoke a deep link. The digital assistant 124 may receive a selection or indication to invoke a corresponding deep link and then launch the application 120 to perform an action via the deep link.
FIG. 2 is an illustration of an example method of generating an index with application actions for voice-based execution, according to an embodiment. Method 200 may be performed by one or more systems or components depicted in fig. 1, including, for example, a computing device, an indexer, a digital assistant, or a data processing system. At act 202, an indexer executing on a computing device having one or more processors can receive a claim from an application. The indexer can receive the declaration via an application programming interface. The declaration may include an indication of the capabilities of the application along with a deep link for invoking, running, executing, or otherwise providing the capabilities. The declaration may be provided to the indexer by the application in response to the application being installed or in response to execution of an action or capability by the application. In some cases, the claim may be provided in response to an instruction or request to provide the claim. The indexer can receive one or more claims and one or more actions from one or more applications.
In act 204, the indexer may access indexes stored in memory or other storage or memory of the computing device. The index may be stored locally on the device. The index may not be uploaded or otherwise provided to a data processing system or server. The index may include one or more rows. Multiple applications on the computing device may access the index. However, not all applications on the device may be authorized to access all of the content of the index. For example, a first application may be able to access a first row or rows in an index containing a first one or more deep links declared by the first application. The first application may be prevented or blocked or otherwise unable to access a second row or rows containing deep links declared by a second application different from the first application. For example, the indexer may encrypt or otherwise protect the rows so that only applications (e.g., digital assistants) that have declared or otherwise been authorized to access the deep links may access the deep links.
In act 206, the indexer may input the deep links into the index. The indexer can store the actions along with the deep links in the rows of the index. The indexer can store one or more parameters associated with the deep link. The indexer can store an indication of the application that declared the deep link, such as the name of the application or other unique identifier of the application that declared the deep link.
In act 208, the indexer may encrypt the deep link. The indexer may encrypt the deep links using a cryptographic token or other encryption technique. The indexer may encapsulate the action or deep links with cryptographic keys or tokens that can prevent unauthorized applications from invoking or accessing the deep links.
FIG. 3 is an illustration of an example method of voice-based execution of an action using an index, according to an embodiment. Method 300 may be performed by one or more systems or components depicted in fig. 1, including, for example, a computing device, an indexer, a digital assistant, or a data processing system. At act 302, method 300 may include receiving a voice input. For example, the voice input may include a voice input query provided by a user or other speaker and detected by a microphone of a computing device, such as a smart phone or tablet computing device. The voice query may include a request, instruction, or command to perform an action. The voice query may include an intent. The voice query may include a command to order coffee, ride, purchase merchandise, or other instructions or commands.
In act 304, the digital assistant may parse the voice query. A digital assistant executing on a computing device may parse a voice query using natural language processing to determine intent or actions associated with the voice query. The digital assistant may perform semantic understanding to identify intent or actions in the voice query. In some cases, the digital assistant may interface with a server digital assistant or a data processing system to determine intent or actions. The server digital assistant or data processing system may include a more powerful natural language processor that can parse voice queries more accurately, reliably, or quickly to determine intent or actions. The local digital assistant may transmit data packets corresponding to the audio input to the server digital assistant for natural language processing. The server digital assistant may transmit the determined intent or action back to the local digital assistant for further processing.
At act 306, the local digital assistant on the computing device that detected the audio input may receive the action in the voice query and perform a lookup in an index stored locally on the computing device. The digital assistant may identify one or more deep links in the index in response to the lookup.
At decision block 308, the digital assistant may determine whether the lookup resulted in multiple deep links. For example, there may be multiple applications declaring deep links for the same action provided in a voice query. If the digital assistant identifies multiple results, the digital assistant may proceed to act 310 to rank the results. At act 310, the digital assistant may rank the results using a ranking technique to select the highest ranked deep link or application. Ranking techniques may be based on recency or frequency of use. The digital assistant may select a deep link of an application that was recently executed on the computing device. The digital assistant may select the deep link of the application that is executing the most on the computing device so far or during a previous time interval (e.g., 24 hours, 48 hours, 7 days, 30 days, or during a corresponding time window).
Upon selecting the highest ranked application at act 310, or if only one result is identified in response to the lookup at act 306, the digital assistant may proceed to act 312 to invoke the deep link. The digital assistant may access the deep links using a cryptographic token or key for encrypting the deep links in the index. The digital assistant may decrypt the deep link or otherwise access the deep link to invoke the deep link. The digital assistant may invoke the deep link to cause the application to perform an action.
FIG. 4 is an illustration of an example method of predicting an action using an index of application actions, according to an embodiment. Method 400 may be performed by one or more systems or components depicted in fig. 1, including, for example, a computing device, an indexer, a digital assistant, or a data processing system. In act 402, the digital assistant may detect a trigger condition. The trigger condition may correspond to one or more trigger parameters, such as time, location, or mode of transportation. The trigger parameters may be used to detect a trigger condition. For example, the trigger parameter at 9 am of the operating position may be a trigger condition that causes the digital assistant to predict an action. The trigger parameter may be associated with a user context.
At act 404, the digital assistant may input trigger parameters into the model to predict the action. The model may be trained using machine learning techniques and historical training data. Training data may be collected based on historical execution of the application on the computing device. Training data may be generated for a particular computing device or account thereof. The digital assistant may input trigger parameters into the model to output actions. An action may refer to an action that a user of the computing device may want to perform based on the current trigger parameters.
In act 406, the digital assistant may perform a lookup in the index with the act to identify one or more deep links. If the digital assistant does not identify any deep links (e.g., a seek returns an empty set), the digital assistant may terminate the method 400 and not suggest any actions. However, if one or more deep links are identified in response to the lookup digital assistant, the digital assistant may proceed to decision block 408.
At decision block 408, the digital assistant may determine whether multiple deep links exist in response to the lookup with an action. If there are multiple results (e.g., two or more results), the digital assistant may proceed to act 410 to rank the results (e.g., similar to act 310).
The digital assistant may proceed to act 412 after selecting the highest ranked deep link according to act 410, or proceed directly from decision block 408 to act 412 if there is a single result in response to the lookup performed at act 406. In act 412, the digital assistant may provide or otherwise present the deep link with a prompt. The prompt may include a request to select or invoke a deep link. The prompt may be a visual or audible prompt. The user may respond to the prompt via selection (e.g., touch interface, gesture, keyboard, mouse) or audio input (e.g., voice input or voice command). In act 414, the digital assistant may invoke the deep link in response to selecting to cause the application to perform the corresponding action.
Fig. 5 is a block diagram of an example computer system 500. Computer system or computing device 500 may include or be used to implement system 100 or components thereof, such as data processing system 102, or computing device 110. The data processing system 102 or the computing device 110 may include an intelligent personal assistant or a voice-based digital assistant. Computing system 500 includes a bus 505 or other communication component for communicating information, and a processor 510 or processing circuit coupled to bus 505 for processing information. Computing system 500 may also include one or more processors 510 or processing circuits coupled to the bus for processing information. Computing system 500 also includes a main memory 515, such as a Random Access Memory (RAM) or other dynamic storage device, coupled to bus 505 for storing information, and instructions to be executed by processor 510. Main memory 515 may be or include data store 132. Main memory 515 may also be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 510. Computing system 500 may also include a Read Only Memory (ROM) 520 or other static storage device coupled to bus 505 for storing static information and instructions for processor 510. A storage device 525, such as a solid state device, magnetic disk, or optical disk, may be coupled to bus 505 to persistently store information and instructions. Storage device 525 may include data store 132 or be part of data store 132.
The computing system 500 may be coupled via bus 505 to a display 535, such as a liquid crystal display or an active matrix display, for displaying information to a user. An input device 530, such as a keyboard including alphanumeric and other keys, may be coupled to bus 505 for communicating information and command selections to processor 510. The input device 530 may include a touch screen display 535. The input device 530 may also include a cursor control, such as a mouse, a trackball, or cursor direction keys for communicating direction information and command selections to the processor 510 and for controlling cursor movement on the display 535. For example, the display 535 may be part of the data processing system 102, the client computing device 110, or other components of fig. 1.
The processes, systems, and methods described herein may be implemented by computing system 500 in response to processor 510 executing an arrangement of instructions contained in main memory 515. Such instructions may be read into main memory 515 from another computer-readable medium, such as storage device 525. Execution of the arrangement of instructions contained in main memory 515 causes computing system 500 to perform the illustrative processes described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 515. Hardwired circuitry may be used in place of or in combination with software instructions in combination with the systems and methods described herein. The systems and methods described herein are not limited to any specific combination of hardware circuitry and software.
While an example computing system is depicted in fig. 5, the subject matter including the operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
For the case where the system discussed herein gathers personal information about a user or may use personal information, the user may be provided with an opportunity to control whether programs or functions may gather personal information (e.g., information about the user's social network, social actions or activities, the user's preferences, or the user's location), or whether or how to receive content from a content server or other data processing system that may be more relevant to the user. Furthermore, certain data may be anonymized in one or more ways before it is stored or used, such that personally identifiable information is removed when the parameters are generated. For example, the identity of the user may be anonymized such that personally identifiable information of the user cannot be determined, or the geographic location of the user may be generalized where location information is obtained (such as to a city, zip code, or state level), such that a particular location of the user cannot be determined. Thus, the user can control how his or her information is collected and used by the content server.
The subject matter and operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs, e.g., one or more circuits of computer program instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium may be or be included in a computer readable storage device, a computer readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Although the computer storage medium is not a propagated signal, the computer storage medium may be a source or destination of computer program instructions encoded in an artificially generated propagated signal. Computer storage media may also be or be included in one or more separate components or media (e.g., multiple CDs, discs, or other storage devices). The operations described in this specification may be implemented as operations performed by a data processing apparatus on data stored on one or more computer readable storage devices or received from other sources.
The terms "data processing system," "computing device," "component" or "data processing apparatus" include various means, devices and machines for processing data, including, for example, a programmable processor, a computer, a system on a chip, or a plurality of programmed processors, computers, a system on a chip, or a combination of the foregoing. The apparatus may comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may include code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures. For example, the NLP 126, the deep link selector 128, and the action predictor 130 or other components may include or share one or more data processing apparatuses, systems, computing devices, or processors.
A computer program (also known as a program, software application, app, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. The computer program may correspond to a file in a file system. A computer program can be stored in a portion of a file (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code) that store other programs or data. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs (e.g., components of data processing system 102) to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; and CD ROM and DVD-ROM discs. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
The subject matter described herein may be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification, or a combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), internetworks (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
A computing system, such as system 100 or system 500, may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network (e.g., network 105). The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, the server transmits data (e.g., data packets representing digital components) to the client device (e.g., for the purpose of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., results of user interactions) may be received at the server from the client device (e.g., received by the data processing system 102 from the local computing device 110 or the supplemental digital content provider device 142 or the service provider device 140).
Although operations are depicted in the drawings in a particular order, such operations do not require execution in the particular order shown or in sequential order, and not all of the operations shown need be performed. The acts described herein may be performed in a different order.
The separation of various system components does not require separation in all embodiments, and the described program components can be included in a single hardware or software product. For example, interface 106 or NLP 104 may be a single component, app or program, or logic device with one or more processing circuits, or a portion of one or more servers of data processing system 102.
Having now described a few illustrative embodiments, it should be apparent that the foregoing is illustrative and not limiting and has been provided by way of example. In particular, although many of the examples presented herein involve specific combinations of method acts or system elements, these acts and these elements may be combined in other ways to achieve the same objectives. Acts, elements and features discussed in connection with one embodiment are not intended to be excluded from a similar role in one or more other embodiments.
The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of "including," "comprising," "having," "containing," "involving," "characterized by … …," and variations thereof herein, is meant to encompass the items listed thereafter and equivalents thereof as well as additional items and alternative embodiments consisting solely of the items listed thereafter. In one embodiment, the systems and methods described herein consist of one, each combination of more than one, or all of the elements, acts, and components described.
Any reference to an embodiment or element or act of a system and method referred to herein in the singular may also encompass embodiments comprising a plurality of such elements, and any plural reference to any embodiment or element or act herein may also encompass embodiments comprising only a single element. Singular or plural references are not intended to limit the presently disclosed systems or methods, their components, acts, or elements to a single or multiple configurations. References to any action or element based on any information, action or element may include implementations in which the action or element is based at least in part on any information, action or element.
Any embodiment disclosed herein may be combined with any other embodiment or example, and references to "one embodiment," "some embodiments," "one embodiment," etc., are not necessarily mutually exclusive, but are intended to indicate that a particular feature, structure, or characteristic described in connection with the embodiment may be included in at least one embodiment or example. Such terms as used herein do not necessarily all refer to the same embodiment. Any embodiment may be combined with any other embodiment, either implicitly or exclusively, in any manner consistent with aspects and embodiments disclosed herein.
Reference to "or" may be construed as inclusive and, thus, any term described using "or" may indicate any one of a single, more than one, and all of the described terms. Reference to at least one of the joined list of terms may be construed as an inclusive "or" to indicate any one of the singular, more than one, and all of the terms described. For example, a reference to "at least one of a 'and B' may include only" a ", only" B ", and both" a "and" B ". Such references used in connection with "comprising" or other open terms may include additional items.
Where technical features in the drawings, detailed description, or any claim are followed by reference signs, the reference signs have been included to increase the intelligibility of the drawings, detailed description, and claims. Accordingly, neither the presence of a reference numeral nor the absence of a reference numeral shall have any limiting effect on the scope of any claim element.
The systems and methods described herein may be embodied in other specific forms without departing from the characteristics thereof. The foregoing embodiments are illustrative and not limiting of the systems and methods described. The scope of the systems and methods described herein are, therefore, indicated by the appended claims rather than by the foregoing description, and all changes which come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein.
Claims (20)
1. A system configured to index application actions for voice-based execution, comprising:
a device comprising one or more processors and memory;
an indexer, the indexer being executable by the apparatus to:
receiving, from an application executed by the device, an indication of an action declared by the application via an application programming interface and a deep link corresponding to the action;
Accessing an index stored in a memory of the device, the index being accessible by a plurality of applications executable by the device; and
inputting the action and the deep link into a location in the index with a cryptographic token that prevents unauthorized ones of the plurality of applications from accessing the location in the index; and
a digital assistant, the digital assistant being executed by the apparatus to:
receiving a voice query detected by a microphone of the device;
parsing the voice query to determine the action;
performing a lookup in the index to identify the deep link corresponding to the action; and
the deep link is invoked using the cryptographic token to cause an application executed by the device to perform the action.
2. The system of claim 1, wherein:
the indexer is configured to receive, in response to installation of the application on the device, a plurality of actions declared by the application and a corresponding plurality of deep links.
3. The system of claim 1 or claim 2, wherein:
the indexer is configured to receive an action declared by the application and the corresponding deep link in response to a previous execution of the action by the application.
4. The system of any preceding claim, wherein the indexer is configured to:
receiving a plurality of actions and a corresponding plurality of deep links from a second application installed on the device that is different from the application;
the plurality of actions and the corresponding plurality of deep links are stored with a second cryptographic token at a second one or more locations in the index that are different from the locations, the second cryptographic token configured to prevent the application from accessing the second one or more locations in the index.
5. The system of any of the preceding claims, wherein the digital assistant is configured to:
parsing the voice query using natural language processing to identify an indication of the application in the voice query;
performing a first lookup in the index to identify one or more deep links declared by the application; and
the lookup is performed to identify the deep link declared by the application that corresponds to the action.
6. The system of any of the preceding claims, wherein the digital assistant is configured to:
performing a lookup in the index with the action to identify the deep link declared by the application and a second deep link declared by a second application of the plurality of applications in response to the action; and
The deep link declared by the application is selected to perform the action based on historical execution of the application and the second application on the device.
7. The system of claim 6, wherein:
the digital assistant is configured to select the deep link declared by the application to perform the action based on the application being used more times to perform the action on the device than the second application.
8. The system of claim 6, wherein:
the digital assistant is configured to select the deep link declared by the application to perform the action based on the application being more recently used to perform the action on the device than the second application.
9. The system of any of claims 6 to 8, wherein the digital assistant is configured to:
identifying parameters associated with the action in the voice query; and
the deep link declared by the application is selected to perform the action based on the application being configured to perform the action using the parameters in the voice query.
10. The system of any of the preceding claims, wherein the digital assistant is configured to:
Identifying parameters in the voice query; and
invoking the deep link to cause the application to perform the action using the parameters in the voice query.
11. The system of any of the preceding claims, wherein the digital assistant is configured to:
identifying a trigger condition;
determining a second action based on the model in response to the trigger condition;
performing a lookup in the index with the second action to identify a second deep link declared by the application; and
an indication of the second action is provided with a prompt invoking the second deep link for presentation via the device to cause the application to perform the second action.
12. A method of indexing application actions for voice-based execution, comprising:
an indexer executed by a device comprising one or more processors and memory receives, from an application executed by the device, an indication of an action declared by the application via an application programming interface and a deep link corresponding to the action;
accessing, by the indexer, an index stored in the memory of the device, the index being accessible by a plurality of applications executable by the device;
Inputting, by the indexer, the action and the deep link into a location in the index with a cryptographic token that prevents unauthorized ones of the plurality of applications from accessing the location in the index;
a digital assistant executed by the device receives a voice query detected by a microphone of the device;
parsing, by the digital assistant, the voice query to determine the action;
performing, by the digital assistant, a lookup in the index to identify the deep link corresponding to the action; and
the deep link is invoked by the digital assistant using the cryptographic token to cause the application executed by the device to perform the action.
13. The method of claim 12, comprising:
a plurality of actions and a corresponding plurality of deep links declared by the application in response to installation of the application on the device are received by the indexer.
14. The method according to any one of claims 12 to 13, comprising:
an action declared by the application and a corresponding deep link in response to a previous execution of the action by the application are received by the indexer.
15. The method according to any one of claims 12 to 14, comprising:
receiving, by the indexer, a plurality of actions and a corresponding plurality of deep links from a second application installed on the device different from the application;
the plurality of actions and the corresponding plurality of deep links are stored by the index at a second one or more locations in the index that are different from the locations with a second cryptographic token configured to prevent the application from accessing the second one or more locations in the index.
16. The method according to any one of claims 12 to 15, comprising:
parsing, by the digital assistant, the voice query using natural language processing to identify an indication of the application in the voice query;
performing, by the digital assistant, a first lookup in the index to identify one or more deep links declared by the application; and
the lookup is performed by the digital assistant to identify the deep link declared by the application that corresponds to the action.
17. The method according to any one of claims 12 to 16, comprising:
performing, by a digital assistant, a lookup in the index with the action to identify the deep link declared by the application and a second deep link declared by a second application of the plurality of applications in response to the action; and
The deep link declared by the application is selected by the digital assistant to perform the action based on historical execution of the application and the second application on the device.
18. The method of claim 17, comprising:
the deep link declared by the application is selected by the digital assistant to perform the action based on the application being used more times to perform the action on the device than the second application.
19. The method of claim 17, comprising:
the deep link declared by the application is selected by the digital assistant to perform the action based on the application being more recently used to perform the action on the device than the second application.
20. The method of claim 17, comprising:
identifying, by the digital assistant, parameters in the voice query that are associated with the action; and
the deep link declared by the application is selected to perform the action based on the parameters in the voice query that the application is configured to perform the action.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/032939 WO2022245339A1 (en) | 2021-05-18 | 2021-05-18 | Indexing application actions for voice-based execution |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116508096A true CN116508096A (en) | 2023-07-28 |
Family
ID=76375657
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180076737.XA Pending CN116508096A (en) | 2021-05-18 | 2021-05-18 | Indexing application actions for speech-based execution |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230267928A1 (en) |
EP (1) | EP4241178A1 (en) |
CN (1) | CN116508096A (en) |
WO (1) | WO2022245339A1 (en) |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10957326B2 (en) * | 2016-12-30 | 2021-03-23 | Google Llc | Device identifier dependent operation processing of packet based data communication |
US11397558B2 (en) * | 2017-05-18 | 2022-07-26 | Peloton Interactive, Inc. | Optimizing display engagement in action automation |
WO2020106314A1 (en) * | 2018-11-21 | 2020-05-28 | Google Llc | Consolidation of responses from queries to disparate data sources |
-
2021
- 2021-05-18 WO PCT/US2021/032939 patent/WO2022245339A1/en active Application Filing
- 2021-05-18 EP EP21731652.0A patent/EP4241178A1/en active Pending
- 2021-05-18 CN CN202180076737.XA patent/CN116508096A/en active Pending
- 2021-05-18 US US18/004,971 patent/US20230267928A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2022245339A1 (en) | 2022-11-24 |
EP4241178A1 (en) | 2023-09-13 |
US20230267928A1 (en) | 2023-08-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8745057B1 (en) | Creating and organizing events in an activity stream | |
US11829435B2 (en) | Secure digital assistant integration in web pages | |
CN108140194B (en) | Transition latency reduction in an online chat-based communication infrastructure | |
US20100042619A1 (en) | Method and system of triggering a search request | |
US11893993B2 (en) | Interfacing with applications via dynamically updating natural language processing | |
EP3762889B1 (en) | Secure digital assistant integration in web pages | |
US11514896B2 (en) | Interfacing with applications via dynamically updating natural language processing | |
US20240062758A1 (en) | Network source identification via audio signals | |
JP2023515158A (en) | Interface and mode selection for digital action execution | |
US20220308987A1 (en) | Debugging applications for delivery via an application delivery server | |
US20200258524A1 (en) | Network source identification via audio signals | |
EP3729259B1 (en) | Assessing applications for delivery via an application delivery server | |
US20230267928A1 (en) | Indexing Application Actions for Voice-Based Execution | |
KR102664371B1 (en) | System and method for validating trigger keywords in sound-based digital assistant applications | |
WO2023282890A1 (en) | Real-time micro-profile generation using a dynamic tree structure | |
EP4150893A1 (en) | Virtual remote control on first device to control second device, eg tv | |
KR20240067283A (en) | Systems and methods to verify trigger keywords in acoustic-based digital assistant applications |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |