US20230205813A1 - Training Image and Text Embedding Models - Google Patents
Training Image and Text Embedding Models Download PDFInfo
- Publication number
- US20230205813A1 US20230205813A1 US18/171,511 US202318171511A US2023205813A1 US 20230205813 A1 US20230205813 A1 US 20230205813A1 US 202318171511 A US202318171511 A US 202318171511A US 2023205813 A1 US2023205813 A1 US 2023205813A1
- Authority
- US
- United States
- Prior art keywords
- image
- embedding
- search
- training
- query
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012549 training Methods 0.000 title claims abstract description 247
- 238000000034 method Methods 0.000 claims abstract description 68
- 230000008569 process Effects 0.000 claims description 45
- 238000012545 processing Methods 0.000 claims description 34
- 230000004044 response Effects 0.000 claims description 27
- 238000003062 neural network model Methods 0.000 claims description 11
- 238000003860 storage Methods 0.000 abstract description 19
- 238000004590 computer program Methods 0.000 abstract description 17
- 230000006870 function Effects 0.000 description 36
- 238000010801 machine learning Methods 0.000 description 13
- 238000011524 similarity measure Methods 0.000 description 13
- 238000013528 artificial neural network Methods 0.000 description 10
- 230000026676 system process Effects 0.000 description 8
- 238000004891 communication Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 5
- 238000004422 calculation algorithm Methods 0.000 description 4
- 230000003993 interaction Effects 0.000 description 4
- 238000013515 script Methods 0.000 description 4
- 239000013598 vector Substances 0.000 description 4
- 238000013527 convolutional neural network Methods 0.000 description 3
- 230000006872 improvement Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- PEDCQBHIVMGVHV-UHFFFAOYSA-N Glycerine Chemical compound OCC(O)CO PEDCQBHIVMGVHV-UHFFFAOYSA-N 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 235000013305 food Nutrition 0.000 description 2
- 238000011478 gradient descent method Methods 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 238000010295 mobile communication Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 241000282326 Felis catus Species 0.000 description 1
- 241000009334 Singa Species 0.000 description 1
- 230000004931 aggregating effect Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 230000009193 crawling Effects 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000003064 k means clustering Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/55—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/538—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9538—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
- G06F18/2148—Generating training patterns; Bootstrap methods, e.g. bagging or boosting characterised by the process organisation or structure, e.g. boosting cascade
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/22—Matching criteria, e.g. proximity measures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/40—Software arrangements specially adapted for pattern recognition, e.g. user interfaces or toolboxes therefor
- G06F18/41—Interactive pattern learning with a human teacher
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/042—Knowledge-based neural networks; Logical representations of neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Definitions
- This specification relates to processing data using machine learning models.
- Machine learning models receive an input and generate an output, e.g., a predicted output, based on the received input.
- Some machine learning models are parametric models and generate the output based on the received input and on values of the parameters of the model.
- Some machine learning models are deep models that employ multiple layers of models to generate an output for a received input.
- a deep neural network is a deep machine learning model that includes an output layer and one or more hidden layers that each apply a non-linear transformation to a received input to generate an output.
- This specification describes a training system implemented as computer programs on one or more computers in one or more locations that trains an image embedding model and a text embedding model using training data derived from a historical query log of a search system.
- a method performed by one or more data processing apparatus including: obtaining training data including multiple training examples, where each training example includes: an image pair including a first image and a second image; and selection data indicating one or more of: (i) a co-click rate of the image pair, and (ii) a similar-image click rate of the image pair, where: the co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query; and the similar-image click rate of the image pair characterizes how often users select either the first image or the second image in response to the first image or the second image being identified by a search result for a search query respectively including either the second image or the first image; and using the training data to train an image embedding model having multiple image embedding model parameters, where the training includes, for each of the multiple training examples: processing the first image of the training example using the image embedding model to
- the training data is generated using a historical query log of a web search system.
- the co-click rate for each training example indicates a fraction of times users selected both the first image and the second image of the training example in response to both the first image and the second image of the training example being concurrently identified by search results for a search query.
- the similar-image click rate for each training example indicates a fraction of times users selected either the first image or the second image in response to the first image or the second image being identified by a search result for a search query respectively including either the second image or the first image.
- the image embedding model includes a convolutional neural network.
- adjusting the image embedding model parameters includes: determining a gradient of a loss function that depends on: (i) the measure of similarity between the embedding of the first image and the embedding of the second image, and (ii) the selection data of the training example; and using the gradient to adjust the image embedding model parameters.
- the multiplicative scaling factor is determined as a linear combination of the co-click rate and the similar-image click rate of the training example.
- determining a measure of similarity between the embedding of the first image and the embedding of the second image includes: determining a Euclidean distance between the embedding of the first image and the embedding of the second image.
- a system including: one or more computers; and one or more storage devices communicatively coupled to the one or more computers, where the one or more storage devices store instructions that, when executed by the one or more computers, cause the one or more computers to perform operations including the operations of the previously described method.
- one or more non-transitory computer storage media storing instructions that when executed by one or more computers cause the one or more computers to perform operations including the operations of the previously described method.
- the training system described in this specification can generate a large amount of training data (e.g., tens or hundreds of millions of training examples) for use in training an image embedding model and a text embedding model by processing data from a historical query log of a search system.
- the large amount of training data that can be efficiently derived from historical query logs e.g., of web search systems
- Such scalability is a technical improvement in the field of model training.
- this scalability enables training of an image embedding model that generates image embeddings that implicitly characterize a wide range of concepts (e.g., foods, scenes, landmarks, man-made products, and the like).
- image embedding models generate image embeddings which can implicitly characterize only a narrow range of concepts (e.g., only food, or only landmarks).
- the training system can process a historical query log to generate “query-image” training examples which associate a textual search query with a related image (e.g., an image that users frequently select when it is identified by a search result for the textual search query).
- the query-image training examples can associate highly specific textual search queries (e.g., “red 2014 ford mustang”) with related images (e.g., which depict objects specified by the textual search queries).
- the training system can cause the image embedding model to generate image embeddings which implicitly represent highly specific concepts.
- the trained image embedding model may process an image to generate an embedding of the image that implicitly represents the color, make, and model of a car depicted in the image.
- This is a technical improvement in the field of model training.
- training the image embedding model and the text embedding model using training examples which associate images with generic labels (e.g., “car”), as in some conventional training data sets, may cause the image embedding model to generate relatively uninformative embeddings.
- the training system can generate query-image training examples which include search queries expressed in a large number of different natural languages (e.g., English, French, German, and the like).
- the training system can train the text embedding model to generate informative text embeddings independent of the language of the text. For example, the training system can train the text embedding model to generate similar embeddings of the text “young Queen Elizabeth” (in English) and “jeune Reine Elizabeth” (in French) based on the similarity of images associated with search queries including this text. This is another technical improvement in the field of model training,
- the training system can train an image embedding model and a text embedding model based on selection data which characterizes, for example, how frequently two images are “co-clicked” or how frequently a given image is selected when it is identified by a search result for a search query (i.e., through “image-image” training examples).
- the selection data can be determined by aggregating user-derived signals (e.g., clicks) over millions of users and enables the training system to train the image embedding model and the text embedding model more effectively.
- Generating training data using conventional methods lacks many of the advantages of generating training data by processing a historical query log of a search system. For example, manually generating training data (e.g., by a person manually specifying textual labels for images) is time-consuming and difficult, and generally only relatively small amounts of training data can be generated in this manner. As another example, generating training data by associating images and captions drawn from a social network (or other source) may produce less and lower-quality training data than generating training data from a historical query log, for example, because the captions may not accurately characterize the contents of the images.
- FIG. 2 shows an example text embedding model
- FIG. 3 shows an example training system for training the image embedding model and the text embedding model using training data derived from a historical query log of a search system.
- FIG. 4 shows an example search results page provided by the search system that includes image search results for a search query that includes a sequence of one or more words.
- FIG. 5 shows an example search results page provided by the search system that includes image search results for a search query that includes an image.
- FIG. 6 illustrates an example process for jointly training the image embedding model and the text embedding model using a query-image training example.
- FIG. 7 A illustrates an example process for training the image embedding model using an image-image training example.
- FIG. 7 B illustrates an example process for jointly training the image embedding model and the text embedding model using query-image training examples and image-image training examples.
- FIG. 8 is a flow diagram of an example process for jointly training an image embedding model and a text embedding model using query-image training examples derived from a historical query log of a search system.
- FIG. 9 is a flow diagram of an example process for training an image embedding model using image-image training examples derived from a historical query log of a search system.
- FIG. 10 shows an example of a portion of a graph representation of query-image training examples and image-image training examples.
- FIG. 11 shows an example search system.
- FIG. 12 shows an example computer system.
- This specification describes a training system for training an image embedding model and a text embedding model using training data derived from a historical query log of a search system.
- the training data derived from the historical query log can include: query-image training examples, image-image training examples, or both.
- a query-image training example includes: (i) a textual search query, (ii) an image, and (iii) selection data characterizing how often users selected the image in response to the image being identified by a search result for the textual search query.
- the training system can jointly train the image embedding model and the text embedding model to generate similar embeddings of the textual search query and the image if the selection data indicates that users frequently select the image when it is identified by a search result for the search query.
- An image-image training example includes an image pair (including a first image and a second image) and selection data that indicates: (i) a co-click rate of the image pair, (ii) a similar-image click rate of the image pair, or (iii) both.
- the co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query.
- the similar-image click rate of the image pair characterizes how often users select the first image in response to the first image being identified by a search result for a search query that includes the second image, or vice versa.
- the training system can train the image embedding model to generate similar embeddings of the first image and the second image if their co-click rate, similar-image click rate, or both, indicates they are related.
- the training system can use the query-image training examples and the image-image training examples derived from the historical query log to jointly train the image embedding model and the query embedding model using a graph-regularized loss function.
- the query-image training examples and the image-image training examples can be represented as a graph structure, and the training system can jointly train the image embedding model and the query embedding model using a loss function based on this graph structure.
- FIG. 1 shows an example image embedding model 100 .
- the image embedding model 100 is configured to process an image 102 in accordance with current values of a set of image embedding model parameters to generate an embedding 104 of the image 102 .
- the embedding 104 is a representation of the image 102 as an ordered collection of numerical values, for example, as a vector or matrix.
- the image embedding model 100 can be trained using machine learning techniques to generate an embedding 104 of an image 102 which implicitly represents the semantic content of the image 102 (e.g., objects depicted by the image 102 ).
- the image embedding model 100 may be configured to process images 102 which are represented in any appropriate format.
- the image embedding model 100 may be configured to process images 102 which are represented in a red-green-blue (RGB) color format (i.e., a format which represents an image by associating respective red, green, and blue color values with each pixel of the image).
- the image embedding model 100 may be configured to process feature representations of the images 102 , for example, histogram of oriented gradient (HOG), scale-invariant feature transform (SIFT), or speeded up robust feature (SURF) representations of the images 102 .
- HOG histogram of oriented gradient
- SIFT scale-invariant feature transform
- SURF speeded up robust feature
- the image embedding model 100 may be a neural network model implemented by computer programs on one or more computers in one or more locations.
- the image embedding model 100 may be a convolutional neural network with an architecture derived from the Inception neural network or the ResNet neural network.
- FIG. 2 shows an example text embedding model 200 .
- the text embedding model 200 is configured to process a representation of a sequence of one or more words in a natural language (i.e., the text 202 ) in accordance with current values of a set of text embedding model parameters to generate an embedding 204 of the text 202 .
- the embedding 204 is a representation of the text 202 as an ordered collection of numerical values, for example, as a vector or matrix.
- the text embedding model 200 can be trained using machine learning techniques to generate an embedding 204 of the text 202 which implicitly represents the semantic content of the text 202 (e.g., objects described by the text 202 ).
- the text embedding model 200 may be configured to process text 202 which is represented in any appropriate format.
- the text embedding model 200 may be configured to process text 202 which is represented as a sequence of “one-hot” vectors, where each one-hot vector represents a respective character (or word) of the text 202 .
- the text embedding model 200 may be configured to process text 202 which is represented by the output of a Word2vec model.
- the text embedding model 200 may be a neural network model implemented by computer programs on one or more computers in one or more locations.
- the text embedding model 200 may be a convolutional neural network with an architecture that includes multiple one-dimensional (ID) convolutional layers.
- the text embedding model 200 may be a lookup based mapping from text 202 to embeddings 204 .
- the text embedding model 200 may be a sequence of fully-connected layers configured to process n-gram text tokens.
- the text embedding model may be a recurrent neural network model (e.g., an LSTM) that is configured to sequentially process representations of characters of the text 202 .
- LSTM recurrent neural network model
- FIG. 3 shows an example training system 300 for training the image embedding model 100 and the text embedding model 200 using training data 302 derived from a historical query log 304 of a search system 306 .
- the training system 300 trains the image embedding model 100 and the text embedding model 200 by determining the values of the image embedding model parameters 308 and the text embedding model parameters 310 .
- the training system 300 can iteratively adjust the parameters of the neural networks using gradients of a loss function, as will be described in more detail below.
- the training system 300 may be implemented by computer programs on one or more computers in one or more locations.
- the training system 300 uses one or more tensor processing units (TPUs—an application-specific integrated circuit (ASIC) designed for machine learning) during training of the image embedding model 100 and the text embedding model 200 .
- TPUs an application-specific integrated circuit (ASIC) designed for machine learning
- the search system 306 can be any system configured to perform image searches by processing search queries which includes text, images, or both, to generate search results which identify images responsive to the search queries.
- An example search system is described in more detail with reference to FIG. 11 .
- the historical query log 304 of the search system 306 indexes a large number (e.g., millions) of search queries previously processed by the search system 306 .
- the historical query log 304 can index a search query by maintaining data including: (i) the search query, and (ii) data which specifies one or more search results that were selected by the user of the device which transmitted the search query.
- a user can “select” a search result by expressing an interest in the search result through any kind of interaction with the search result.
- a user can select a search result by clicking on a hypertext link included in the search result, or by hovering a cursor over the search result for a predefined period of time, to generate a request for an electronic document (e.g., image) identified by the search result.
- an electronic document e.g., image
- the training system 300 can process data from the historical query log 304 to generate query-image training examples and image-image training examples used to train the image embedding model 100 and the text embedding model 200 .
- a query-image training example includes: (i) a textual search query, (ii) an image, and (iii) selection data characterizing how often users selected the image in response to the image being identified by a search result for the textual search query.
- the training system 300 can jointly train the image embedding model 100 and the text embedding model 200 to generate similar embeddings of the search query and the image if the selection data indicates that users frequently select the image when it is identified by a search result for the search query.
- a similarity between embeddings can be determined using any appropriate numerical similarity measure (e.g., a Euclidean distance, if the image embedding model 100 and the text embedding model 200 are configured to generate embeddings of the same dimensionality).
- An image-image training example includes an image pair (including a first image and a second image) and selection data that indicates: (i) a co-click rate of the image pair, (ii) a similar-image click rate of the image pair, or (iii) both.
- the co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query.
- the similar-image click rate of the image pair characterizes how often users select the first image in response to the first image being identified by a search result for a search query that includes the second image, or vice versa.
- the training system 300 can train the image embedding model 100 to generate similar embeddings of the first image and the second image if their co-click rate, similar-image click rate, or both, indicates they are related.
- FIG. 4 shows an example search results page 400 provided by the search system 306 that includes image search results for a search query that includes a sequence of one or more words.
- the search results page 400 displays search results 402 , 404 , and 406 for the search query 408 : “red ford mustang”.
- FIG. 5 shows an example search results page 500 provided by the search system 306 that includes image search results for a search query that includes an image.
- the search results page 500 displays search results 502 , 504 , and 506 for the search query 508 that includes an image depicting a truck.
- the search system 306 may be configured to provide search results which identify images similar to the query image.
- each of the search results 502 , 504 , and 506 identify images which are similar to the query image.
- a user is said to “co-click” a first image and a second image if the user selects search results which respectively identify the first image and the second image from the same set of search results. For example, a user may co-click the image identified by the search result 402 and the image identified by the search result 404 by selecting both of the search results (e.g., one after another) on the search results page 400 . As another example, a user may co-click the image identified by the search result 504 and the image identified by the search result 506 by selecting both of the search results (e.g., one after another) on the search results page 500 .
- the images identified by each pair of selected search results can be considered to be co-clicked. For example, if a user selects search results A, B, and C from the same set of search results, then the pairs of images identified by the search results ⁇ A, B ⁇ , ⁇ A, C ⁇ , and ⁇ B, C ⁇ can be each considered to be co-clicked.
- FIG. 6 illustrates an example process for jointly training the image embedding model 100 and the text embedding model 200 using a query-image training example 600 .
- the query-image training example 600 includes a search query 602 which includes a sequence of one or more words and an image 604 .
- the training system 300 processes the image 604 using the image embedding model 100 and in accordance with current values of the image embedding model parameters 308 to generate an embedding 606 of the image 604 .
- the training system 300 processes the search query 602 using the text embedding model 200 and in accordance with current values of the text embedding model parameters 310 to generate an embedding 608 of the search query 602 .
- the training system 300 determines a similarity measure 610 between the embedding 606 of the image 604 and the embedding 608 of the search query 602 , and determines model parameter adjustments 612 based on the similarity measure 610 . Thereafter, the training system 300 uses the model parameter adjustments 612 to adjust the values of the image embedding model parameters 308 and the text embedding model parameters 310 . In some implementations, the training system 300 uses the selection data characterizing how often users selected the image 604 in response to the image 604 being identified by a search result for the search query 602 in determining the model parameter adjustments 612 . An example process for jointly training an image embedding model and a text embedding model using query-image training examples is described in more detail with reference to FIG. 8 .
- FIG. 7 A illustrates an example process for training the image embedding model 100 using an image-image training example 700 including a first image 702 and a second image 704 .
- the training system 300 processes the first image 702 using the image embedding model 100 and in accordance with current values of the image embedding model parameters 308 to generate an embedding 706 of the first image 702 .
- the training system 300 processes the second image 704 using the image embedding model 100 and in accordance with current values of the image embedding model parameters 308 to generate an embedding 708 of the second image 704 .
- the training system 300 determines a similarity measure 710 between the embedding 706 of the first image 702 and the embedding 708 of the second image 704 , and determines model parameter adjustments 712 based on the similarity measure 710 .
- the training system 300 uses the selection data characterizing the co-click rate, the similar-image click rate, or both of the first image 702 and the second image 704 in determining the model parameter adjustments 712 .
- An example process for training an image embedding model using image-image training examples is described in more detail with reference to FIG. 9 .
- FIG. 7 B illustrates an example process for jointly training the image embedding model and the text embedding model using query-image training examples and image-image training examples.
- one or more query-image training examples 600 and one or more image-image training examples 700 can be processed by the image embedding model to generate respective embeddings (as described with reference to FIG. 6 and FIG. 7 A ).
- the training system 300 can determine respective model parameter adjustments 714 based on the query-image training examples 600 (as described with reference to FIG. 6 ) and the image-image training examples (as described with reference to FIG. 7 A ).
- the training system 300 can thereafter use the model parameter adjustments 714 to adjust the current values of the image embedding model parameters 308 and the text embedding model parameters 310 .
- the model parameter adjustments that are determined based on the query-image training examples may be weighted more or less heavily (e.g., using a gradient scaling factor) than the model parameter adjustments that are determined based on the image-image training examples.
- the weighting applied to the model parameter adjustments may be a tunable system hyper-parameter.
- FIG. 8 is a flow diagram of an example process 800 for jointly training an image embedding model and a text embedding model using query-image training examples derived from a historical query log of a search system.
- the process 800 will be described as being performed by a system of one or more computers located in one or more locations.
- a training system e.g., the training system 300 of FIG. 3 , appropriately programmed in accordance with this specification, can perform the process 800 .
- the system processes data from a historical query log of a search system to generate a candidate set of query-image training examples ( 802 ).
- Each of the query-image training examples includes: (i) a search query including a sequence of one or more words, (ii) an image, and (iii) selection data characterizing how often users selected the image in response to the image being identified by a search result for the search query.
- the selection data may indicate the fraction of times users selected the image in response to the image being identified by a search result for the search query.
- the system selects query-image training examples for use in jointly training the image embedding model and the text embedding model from the candidate set of training examples based at least in part on the selection data of the training examples ( 804 ). For example, the system may select a particular query-image training example if the image of the particular training example is most frequently selected by users in response to the image being identified by a search result for the search query of the particular query-image training example. As another example, the system may select a particular query-image training example if the image of the particular query-image training example is in the top N images that are most frequently selected by users after being identified by search results for the search query of the particular query-image training example.
- the system can use any of a variety of other appropriate criteria in selecting query-image training examples for use in jointly training the image embedding model and the text embedding model.
- the system may limit the number of selected query-image training examples which include search queries that specify the names of particular people, and corresponding images which depict the particular people.
- the appearance of the same person can vary substantially between images (e.g., due to the person wearing different clothing, shoes, glasses, and the like), including a large number of query-image training examples corresponding to particular people may reduce the effectiveness of the training process.
- Steps 806 - 812 describe an example process that can be performed for each selected query-image training example to jointly train the image embedding model and the text embedding model.
- steps 806 - 812 describe steps that can be performed for a given query-image training example.
- any appropriate method can be used to jointly train the image embedding model and the text embedding model.
- a stochastic gradient descent method can be used to jointly train the image embedding model and the text embedding model, where the steps 806 - 812 are iteratively repeated for “batches” (i.e., sets) of query-image training examples.
- the system may determine that the training is complete when a training termination criterion is satisfied.
- the training termination criterion may be that a predetermined number of iterations of the steps 806 - 812 have been performed.
- the training termination criterion may be that a change in the values of the parameters of the image embedding model and the text embedding model between iterations of the steps 806 - 812 is below a predetermined threshold.
- the system processes the image of the given query-image training example using the image embedding model and in accordance with current values of the image embedding model parameters to generate an embedding of the image ( 806 ).
- the image embedding model is a neural network model
- the system processes the image using a sequence of neural network layers defined by the architecture of the neural network model.
- the system processes a representation of the search query of the given query-image training example using the text embedding model and in accordance with current values of the text embedding model parameters to generate an embedding of the search query ( 808 ).
- the text embedding model is a neural network model
- the system processes the representation of the search query using a sequence of neural network layers defined by the architecture of the neural network model.
- the system determines a measure of similarity between the embedding of the image and the embedding of the search query of the given query-image training example ( 810 ).
- the embedding of the image and the embedding of the search query may have the same dimensionality and the system may determine the measure of similarity by determining a Euclidean distance or cosine similarity measure between the respective embeddings.
- the system adjusts the image embedding model parameters and the text embedding model parameters based at least in part on the measure of similarity between the embedding of the image and the embedding of the search query of the given query-image training example ( 812 ).
- the system may determine the gradient of a loss function and use the gradient of the loss function to adjust the image embedding model parameters and the text embedding model parameters.
- the system can determine the gradient of the loss function using any appropriate method, for example, backpropagation.
- the loss function can be any appropriate loss function that depends on the measure of similarity between the embedding of the image and the embedding of the search query of the given query-image training example. A few examples follow.
- the loss function may be a classification loss function.
- the search query of the given query-image training example is considered to identify a “positive” label for the image of the given query-image training example.
- the search queries of the other query-image training examples are considered to identify respective “negative” labels for the image of the given query-image training example.
- the system may determine the similarity measure between the embedding of the image of the given query-image training example and the embedding of the search query of the given query-image training example as a “positive” score.
- the system may determine respective “negative” scores for each other training example as a similarity measure between the embedding of the image of the given query-image training example and an embedding of the search query of the other training example.
- the system can process the positive and negative scores using a soft-max (or sampled soft-max) function, and provide the output of the soft-max (or sampled soft-max) function to a cross-entropy loss function (or any other appropriate classification loss function).
- the loss function may be a triplet loss function.
- the system may determine the embedding of the image of the given query-image training example to be the “anchor”, the embedding of the search query of the given query-image training example to be the “positive”, and the embedding of the search query of another query-image training example to be the “negative”.
- the loss function may depend on the selection data for the given query-image training example which characterizes how often users selected the image in response to the image being identified by a search result for the search query of the given query-image training example.
- the loss function may include a multiplicative scaling factor based on the fraction of times users selected the image in response to the image being identified by a search result for the search query of the given query-image training example.
- FIG. 9 is a flow diagram of an example process 900 for training an image embedding model using image-image training examples derived from a historical query log of a search system.
- the process 900 will be described as being performed by a system of one or more computers located in one or more locations.
- a training system e.g., the training system 300 of FIG. 3 , appropriately programmed in accordance with this specification, can perform the process 900 .
- the system processes data from a historical query log of a search system to generate image-image training examples ( 902 ).
- Each of the image-image training examples includes: (i) an image pair including a first image and a second image, (ii) selection data indicating a co-click rate of the image pair, a similar-image click rate of the image pair, or both.
- the co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query.
- the co-click rate of the image pair may indicate the fraction of times users selected both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query.
- the similar-image click rate of the image pair characterizes how often users select the first image in response to the first image being identified by a search result for a search query that includes the second image, or vice versa.
- the similar-image click rate of the image pair may indicate the fraction of times users selected the first image in response of the first image being identified by a search result for a search query that includes the second image, or vice versa.
- Steps 904 - 908 describe an example process that can be performed for each image-image training example to train the image embedding model.
- steps 904 - 908 describe steps that can be performed for each image-image training example.
- any appropriate method can be used to train the image embedding model.
- a stochastic gradient descent method can be used to train the image embedding model, where the steps 904 - 908 are iteratively repeated for “batches” (i.e., sets) of selected training examples.
- the system may determine that the training is complete when a training termination criterion is satisfied.
- the training termination criterion may be that a predetermined number of iterations of the steps 904 - 908 have been performed.
- the training termination criterion may be that a change in the values of the parameters of the image embedding model between iterations of the steps 904 - 908 is below a predetermined threshold.
- the image-image training examples can also be used in conjunction with query-image training examples to jointly train the image embedding model and the text embedding model using a graph-regularized loss function.
- the system processes the first image and the second image of the training example using the image embedding model and in accordance with current values of the image embedding model parameters to generate respective embeddings of the first image and the second image of the training example ( 904 ).
- the image embedding model is a neural network model
- the system processes the first image and the second image (e.g., one after another) using a sequence of neural network layers defined by the architecture of the neural network model.
- the system determines a measure of similarity between the embedding of the first image and the embedding of the second image ( 906 ). For example, the system may determine the measure of similarity by determining a Euclidean distance or cosine similarity measure between the respective embeddings of the first image and the second image.
- the system adjusts the image embedding model parameters based at least in part on: (i) the measure of similarity between the respective embeddings of the first image and the second image, and (ii) the selection data (i.e., the co-click rate, similar-image click rate, or both) ( 910 ).
- the system may determine the gradient of a loss function and use the gradient of the loss function to adjust the image embedding model parameters.
- the system can determine the gradient of the loss function using any appropriate method, for example, backpropagation.
- the loss function can be any appropriate loss function that depends on the measure of similarity between the respective embeddings and the selection data. For example, the loss function may be given by:
- h 0(/ 1 ) represents the embedding of the first image of the image-image training example
- h 0 (l 2 ) represents the embedding of the second image of the image-image training example
- D( ⁇ , ⁇ ) is a similarity measure (e.g., a Euclidean similarity measure)
- the scaling factor w can be determined in any appropriate manner using the co-click rate, similar-image click rate, or both, of the image-image training example.
- the scaling strength w can be determined as a linear combination (e.g., using predetermined weighting factors) of the co-click rate and similar-image click rate of the image-image training example.
- the training system 300 can use the query-image training examples and the image-image training examples to jointly train the image embedding model 100 and the text embedding model 200 using a graph-regularized loss function.
- the query-image training examples and image-image training examples can be understood to represent a graph structure, where each node of the graph corresponds to a query-image training example and each edge corresponds to an image-image training example. More specifically, an edge in the graph which connects a first and a second node which respectively correspond to a first and a second query-image training example may be defined by an image-image training example which includes the image pair specified by the first and second query-image training examples.
- the “strength” of the edge connecting the first node and the second node may be defined based on the co-click rate, the similar-image click rate, or both, specified by the image-image training example corresponding to the edge.
- FIG. 10 shows an example of a portion of a graph representation, where nodes 1002 and 1004 represent query-image training examples which include respective images 1006 and 1008 of cars.
- the edge 1010 connecting the nodes 1002 and 1004 is associated with a co-click rate of X (where X can be a real number) and a similar-image click rate of Y (where Y can be a real number) for the image pair 1006 and 1008 .
- some or all of the nodes of the graph may correspond to images included in image-image training examples, where the image is not included in any query-image training examples (i.e., is not associated with a corresponding textual search query).
- the graph-regularized loss function may have the form:
- N is the total number of nodes
- Ii represents the image associated with the i-th node (e.g., of the image-query training example corresponding to the i-th node, if there is one)
- Qi represents the search query of the image-query training example corresponding to the i-th node (if there is one)
- £ 1 (Ii,Qa represents the loss function associated with the image-query training example corresponding to the i-th node (e.g., the classification loss or triplet loss described with reference to 812 )
- N (i) represent the set of “neighbors” of node i in the graph representation
- wij represents the strength of the edge connecting node i and node j in the graph representation
- heUa represents the embedding of the image of Ii associated with the i-th node
- he(Ij) represents the embedding of the image Ij of the image associated with the j-th node
- the strength wij of the edge connecting nodes i and j can be determined in any appropriate manner using the co-click rate, similar-image click rate, or both, of the image-image training example which defines the edge.
- the strength wij of the edge connecting nodes i and j can be determined as a linear combination (e.g., using predetermined weighting factors) of the co-click rate and similar-image click rate.
- the £ 1 (Ii,Qa component of the loss defied by equation 2 may be removed.
- the training system 300 can jointly train the image embedding model 100 and the text embedding model 200 using a graph-regularized loss function (e.g., as described by equation 2) using any appropriate machine learning training technique.
- the training system 300 can jointly train the image embedding model 100 and the text embedding model 200 by stochastic gradient descent using an alternative representation of the loss function in equation 2:
- the training system 300 can perform stochastic gradient descent by sampling edges from the graph representation at each training iteration and using backpropagation (or any other appropriate technique) to determine the gradient of the loss function given by equations 3 and 4.
- the training system 300 can determine the training is complete when any appropriate training termination criterion is satisfied, for example, when a predetermined number of iterations of stochastic gradient descent have been performed.
- the trained image embedding model 100 and text embedding model 200 can be used for any of a variety of purposes. A few examples follow.
- the trained image embedding model 100 can be used by the search system 306 in ranking image search results responsive to a search query that includes a query image. More specifically, the search system 306 can use the image embedding model 100 to generate a respective embedding of each image in a search index maintained by the search system (as described with reference to FIG. 11 ). After receiving a search query that includes a query image, the search system 306 can use the image embedding model to generate an embedding of the query image, and thereafter use the generated embedding to determine a respective relevance score for each of multiple images in the search index.
- the search system 306 can determine the relevance score for a given image in the search index based on a measure of similarity (e.g., a Euclidean distance) between the embedding of the given image and the embedding of the query image.
- the search system 306 can determine the ranking of the image search results for the search query based at least in part on the relevance scores determined using the embeddings generated by the image embedding model 100 .
- the trained text embedding model and the trained image embedding model can both be used by the search system 306 in ranking image search results responsive to a search query that includes a sequence of one or more words. More specifically, the search system 306 can use the image embedding model 100 to generate a respective embedding of each image in a search index maintained by the search system. After receiving a search query that includes a sequence of one or more words, the search system can use the text embedding model to generate an embedding of the sequence of words, and thereafter use the generated embedding to determine a respective relevance score for each of multiple images in the search index.
- the search system can determine the relevance score for a given image in the search index based on a measure of similarity (e.g., a Euclidean distance) between the embedding of the given image and the embedding of the sequence of words of the search query.
- the search system 306 can determine the ranking of the image search results for the search query based at least in part on the relevance scores determined using the embeddings generated by the image embedding model and the text embedding model.
- the trained text embedding model can be used to determine “clusters” of similar keywords (or keyword sequences), that is, sets of keywords which express similar semantic content.
- a cluster of similar keywords may be: “shoes”, “shoe”, “footwear”, “boots”, “cleats”, “heels”, “slippers”, “sneakers”, and the like.
- Keyword clusters can be generated using the text embedding model by determining a respective embedding of each keyword in a corpus of keywords, and thereafter using a clustering algorithm to cluster the keywords based on their respective embeddings.
- the clustering algorithm may be, for example, a k-means clustering algorithm or an expectation maximization clustering algorithm. Keyword clusters generated using the trained text embedding model can be used as distribution parameters that condition the transmission of digital components (e.g., advertisements) for presentation with electronic documents (e.g., webpages).
- the trained text embedding model and the trained image embedding model can both be used in an image classification system configured to process an image to generate an output which associates the image with a label from a predetermined set of labels.
- the labels may specify object classes (e.g., person, cat, vehicle, and the like), and the image classification system may be trained to associate an image with the label of an object depicted in the image.
- the image classification system may use the image embedding model 100 to generate an embedding of an input image, and the text embedding model to generate a respective embedding of each search query in a corpus of search queries.
- the image classification system may determine a respective measure of similarity between the embedding of the input image and the respective embedding of each search query, and may thereafter associate the input image with a particular search query with the highest measure of similarity.
- the image classification system may determine the label to associate with the input image based on both: (i) visual features derived from the input image, and (ii) semantic features derived from the particular search query.
- An example of an image classification system that can use the text embedding model 200 and the image embedding model 100 is described with reference to U.S. Patent Application No. 62/768,701.
- FIG. 11 shows an example search system 100 .
- the search system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations in which the systems, components, and techniques described below are implemented.
- the search system 1100 is configured to receive a search query 1102 from a user device 1104 , to process the search query 1102 to determine one or more search results 1106 responsive to the search query 1102 , and to provide the search results 1106 to the user device 1104 .
- the search query 1102 can include search terms expressed in a natural language (e.g., English), images, audio data, or any other appropriate form of data.
- a search result 1106 identifies an electronic document 1108 from a website 1110 that is responsive to the search query 1102 , and includes a link to the electronic document 1108 .
- Electronic documents 1108 can include, for example, images, HTML webpages, word processing documents, portable document format (PDF) documents, and videos.
- the electronic documents 1108 can include content, such as words, phrases, images, and audio data, and may include embedded information (e.g., meta information and hyperlinks) and embedded instructions (e.g., scripts).
- a website 1110 is a collection of one or more electronic documents 1108 that is associated with a domain name and hosted by one or more servers.
- a website 1110 may be a collection ofwebpages formatted in hypertext markup language (HTML) that can contain text, images, multimedia content, and programming elements (e.g., scripts).
- HTML hypertext markup language
- a search query 1102 can include the search terms “Apollo moon landing”, and the search system 1100 may be configured to perform an image search, that is, to provide search results 1106 which identify respective images that are responsive to the search query 1102 .
- the search system 1100 may provide search results 1106 that each include: (i) a title of a webpage, (ii) a representation of an image extracted from the webpage, and (iii) a hypertext link (e.g., specifying a uniform resource locator (URL)) to the webpage or to the image itself
- search system 1100 may provide a search result 1106 that includes: (i) the title “Apollo moon landing” of a webpage, (ii) a reduced-size representation (i.e., thumbnail) of an image of the Apollo spacecraft included in the webpage, and (iii) a hypertext link to the image.
- a computer network 1112 such as a local area network (LAN), wide area network (WAN), the Internet, a mobile phone network, or a combination thereof, connects the websites 1110 , the user devices 1104 , and the search system 1100 (i.e., enabling them to transmit and receive data over the network 1112 ).
- the network 1112 can connect the search system 1100 to many thousands of websites 1110 and user devices 1104 .
- a user device 1104 is an electronic device that is under control of a user and is capable of transmitting and receiving data (including electronic documents 1108 ) over the network 1112 .
- Example user devices 1104 include personal computers, mobile communication devices, and other devices that can transmit and receive data over the network 1112 .
- a user device 1104 typically includes user applications (e.g., a web browser) which facilitate transmitting and receiving data over the network 1112 .
- user applications included in a user device 1104 enable the user device 1104 to transmit search queries 1102 to the search system 1100 , and to receive the search results 1106 provided by the search system 1100 in response to the search queries 1102 , over the network 1112 .
- the user applications included in the user device 1104 can present the search results 1106 received from the search system 1100 to a user of the user device (e.g., by rendering a search results page which shows an ordered list of the search results 1106 ).
- the user may select one of the search results 1106 presented by the user device 1104 (e.g., by clicking on a hypertext link included in the search result 1106 ), which can cause the user device 1104 to generate a request for an electronic document 1108 identified by the search result 1106 .
- the request for the electronic document 1108 identified by the search result 1106 is transmitted over the network 1112 to a website 1110 hosting the electronic document 1108 .
- the website 1110 hosting the electronic document 1108 can transmit the electronic document 1108 to the user device 1104 .
- the search system 1100 processes a search query 1102 using a ranking engine 1114 to determine search results 1106 responsive to the search query 1102 .
- the search system 1100 uses an indexing engine 1120 to generate and maintain the search index 1116 by “crawling” (i.e., systematically browsing) the electronic documents 1108 of the websites 1110 .
- the search index 1116 indexes the electronic document by maintaining data which: (i) identifies the electronic document 1108 (e.g., by a link to the electronic document 1108 ), and (ii) characterizes the electronic document 1108 .
- the data maintained by the search index 1116 which characterizes an electronic document may include, for example, data specifying a type of the electronic document (e.g., image, video, PDF document, and the like), a quality of the electronic document (e.g., the resolution of the electronic document when the electronic document is an image or video), keywords associated with the electronic document, a cached copy of the electronic document, or a combination thereof.
- a type of the electronic document e.g., image, video, PDF document, and the like
- a quality of the electronic document e.g., the resolution of the electronic document when the electronic document is an image or video
- keywords associated with the electronic document e.g., a cached copy of the electronic document, or a combination thereof.
- the search system 1100 can store the search index 1116 in a data store which may include thousands of data storage devices.
- the indexing engine 1120 can maintain the search index 1116 by continuously updating the search index 1116 , for example, by indexing new electronic documents 1108 and removing electronic documents 1108 that are no longer available from the search index 1116 .
- the search system 1100 uses a query logging engine 1122 to generate and maintain a historical query log 1118 (as described earlier).
- the search system 1100 can store the historical query log 1118 in a data store which may include thousands of data storage devices.
- the query logging engine 1122 can maintain the historical query log 1118 by continuously updating the historical query log 1118 (e.g., by indexing new search queries as they are processed by the search system 1100 ).
- the ranking engine 1114 determines search results 1106 responsive to the search query 1102 by scoring electronic documents 1108 indexed by the search index 1116 .
- the ranking engine 1114 can score electronic documents 1108 based in part on data accessed from the historical query log 1118 .
- the score determined by the ranking engine 1114 for an electronic document 1108 characterizes how responsive (e.g., relevant) the electronic document is to the search query 1102 .
- the ranking engine 1114 determines a ranking of the electronic documents 1108 indexed by the search index 1116 based on their respective scores, and determines the search results based on the ranking. For example, the ranking engine 1114 can generate search results 1106 which identify the highest-ranked electronic documents 1108 indexed by the search index 1116 .
- FIG. 12 is block diagram of an example computer system 1200 that can be used to perform operations described above.
- the system 1200 includes a processor 1210 , a memory 1220 , a storage device 1230 , and an input/output device 1240 .
- Each of the components 1210 , 1220 , 1230 , and 1240 can be interconnected, for example, using a system bus 1250 .
- the processor 1210 is capable of processing instructions for execution within the system 1200 .
- the processor 1210 is a single-threaded processor.
- the processor 1210 is a multi-threaded processor.
- the processor 1210 is capable of processing instructions stored in the memory 1220 or on the storage device 1230 .
- the memory 1220 stores information within the system 1200 .
- the memory 1220 is a computer-readable medium.
- the memory 1220 is a volatile memory unit.
- the memory 1220 is a non-volatile memory unit.
- the storage device 1230 is capable of providing mass storage for the system 1200 .
- the storage device 1230 is a computer-readable medium.
- the storage device 1230 can include, for example, a hard disk device, an optical disk device, a storage device that is shared over a network by multiple computing devices (e.g., a cloud storage device), or some other large capacity storage device.
- the input/output device 1240 provides input/output operations for the system 1200 .
- the input/output device 1240 can include one or more network interface devices, e.g., an Ethernet card, a serial communication device, e.g., and RS-232 port, and/or a wireless interface device, e.g., and 802.11 card.
- the input/output device can include driver devices configured to receive input data and send output data to other input/output devices, e.g., keyboard, printer and display devices 1260 .
- Other implementations, however, can also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, etc.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non-transitory storage medium for execution by, or to control the operation of, data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- an artificially-generated propagated signal e.g., a machine-generated electrical, optical, or electromagnetic signal
- data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- engine is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions.
- an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- the central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser.
- a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- a machine learning framework e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client.
- Data generated at the user device e.g., a result of the user interaction, can be received at the server from the device.
Abstract
Description
- This specification relates to processing data using machine learning models.
- Machine learning models receive an input and generate an output, e.g., a predicted output, based on the received input. Some machine learning models are parametric models and generate the output based on the received input and on values of the parameters of the model.
- Some machine learning models are deep models that employ multiple layers of models to generate an output for a received input. For example, a deep neural network is a deep machine learning model that includes an output layer and one or more hidden layers that each apply a non-linear transformation to a received input to generate an output.
- This specification describes a training system implemented as computer programs on one or more computers in one or more locations that trains an image embedding model and a text embedding model using training data derived from a historical query log of a search system.
- According to a first aspect there is provided a method performed by one or more data processing apparatus, the method including: obtaining training data including multiple training examples, where each training example includes: an image pair including a first image and a second image; and selection data indicating one or more of: (i) a co-click rate of the image pair, and (ii) a similar-image click rate of the image pair, where: the co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query; and the similar-image click rate of the image pair characterizes how often users select either the first image or the second image in response to the first image or the second image being identified by a search result for a search query respectively including either the second image or the first image; and using the training data to train an image embedding model having multiple image embedding model parameters, where the training includes, for each of the multiple training examples: processing the first image of the training example using the image embedding model to generate an embedding of the first image; processing the second image of the training example using the image embedding model to generate an embedding of the second image; determining a measure of similarity between the embedding of the first image and the embedding of the second image; and adjusting the image embedding model parameters based at least in part on: (i) the measure of similarity between the embedding of the first image and the embedding of the second image, and (ii) the selection data of the training example.
- In some implementations, the training data is generated using a historical query log of a web search system.
- In some implementations, the co-click rate for each training example indicates a fraction of times users selected both the first image and the second image of the training example in response to both the first image and the second image of the training example being concurrently identified by search results for a search query.
- In some implementations, the similar-image click rate for each training example indicates a fraction of times users selected either the first image or the second image in response to the first image or the second image being identified by a search result for a search query respectively including either the second image or the first image.
- In some implementations, the image embedding model includes a convolutional neural network.
- In some implementations, adjusting the image embedding model parameters includes: determining a gradient of a loss function that depends on: (i) the measure of similarity between the embedding of the first image and the embedding of the second image, and (ii) the selection data of the training example; and using the gradient to adjust the image embedding model parameters.
- In some implementations, the loss function includes a multiplicative scaling factor that is determined based on the selection data of the training example.
- In some implementations, the multiplicative scaling factor is determined as a linear combination of the co-click rate and the similar-image click rate of the training example.
- In some implementations, determining a measure of similarity between the embedding of the first image and the embedding of the second image includes: determining a Euclidean distance between the embedding of the first image and the embedding of the second image.
- According to a second aspect there is provided a system including: one or more computers; and one or more storage devices communicatively coupled to the one or more computers, where the one or more storage devices store instructions that, when executed by the one or more computers, cause the one or more computers to perform operations including the operations of the previously described method.
- According to a third aspect there is provided one or more non-transitory computer storage media storing instructions that when executed by one or more computers cause the one or more computers to perform operations including the operations of the previously described method.
- Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages.
- The training system described in this specification can generate a large amount of training data (e.g., tens or hundreds of millions of training examples) for use in training an image embedding model and a text embedding model by processing data from a historical query log of a search system. The large amount of training data that can be efficiently derived from historical query logs (e.g., of web search systems) enables the training system to train highly effective image and text embedding models. Such scalability (for example, to training examples which can potentially include hundreds of millions of search queries) is a technical improvement in the field of model training. For example, this scalability enables training of an image embedding model that generates image embeddings that implicitly characterize a wide range of concepts (e.g., foods, scenes, landmarks, man-made products, and the like). In contrast, some conventional image embedding models generate image embeddings which can implicitly characterize only a narrow range of concepts (e.g., only food, or only landmarks).
- The training system can process a historical query log to generate “query-image” training examples which associate a textual search query with a related image (e.g., an image that users frequently select when it is identified by a search result for the textual search query). In particular, the query-image training examples can associate highly specific textual search queries (e.g., “red 2014 ford mustang”) with related images (e.g., which depict objects specified by the textual search queries). By jointly training an image embedding model and a text embedding model using query-image training examples derived from a historical query log, the training system can cause the image embedding model to generate image embeddings which implicitly represent highly specific concepts. For example, the trained image embedding model may process an image to generate an embedding of the image that implicitly represents the color, make, and model of a car depicted in the image. This is a technical improvement in the field of model training, In contrast, for example, training the image embedding model and the text embedding model using training examples which associate images with generic labels (e.g., “car”), as in some conventional training data sets, may cause the image embedding model to generate relatively uninformative embeddings.
- In some implementations, the training system can generate query-image training examples which include search queries expressed in a large number of different natural languages (e.g., English, French, German, and the like). By jointly training an image embedding model and a text embedding model using multi-lingual query-image training examples, the training system can train the text embedding model to generate informative text embeddings independent of the language of the text. For example, the training system can train the text embedding model to generate similar embeddings of the text “young Queen Elizabeth” (in English) and “jeune Reine Elizabeth” (in French) based on the similarity of images associated with search queries including this text. This is another technical improvement in the field of model training,
- The training system can train an image embedding model and a text embedding model based on selection data which characterizes, for example, how frequently two images are “co-clicked” or how frequently a given image is selected when it is identified by a search result for a search query (i.e., through “image-image” training examples). The selection data can be determined by aggregating user-derived signals (e.g., clicks) over millions of users and enables the training system to train the image embedding model and the text embedding model more effectively.
- Generating training data using conventional methods lacks many of the advantages of generating training data by processing a historical query log of a search system. For example, manually generating training data (e.g., by a person manually specifying textual labels for images) is time-consuming and difficult, and generally only relatively small amounts of training data can be generated in this manner. As another example, generating training data by associating images and captions drawn from a social network (or other source) may produce less and lower-quality training data than generating training data from a historical query log, for example, because the captions may not accurately characterize the contents of the images.
- The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
FIG. 1 shows an example image embedding model. -
FIG. 2 shows an example text embedding model. -
FIG. 3 shows an example training system for training the image embedding model and the text embedding model using training data derived from a historical query log of a search system. -
FIG. 4 shows an example search results page provided by the search system that includes image search results for a search query that includes a sequence of one or more words. -
FIG. 5 shows an example search results page provided by the search system that includes image search results for a search query that includes an image. -
FIG. 6 illustrates an example process for jointly training the image embedding model and the text embedding model using a query-image training example. -
FIG. 7A illustrates an example process for training the image embedding model using an image-image training example. -
FIG. 7B illustrates an example process for jointly training the image embedding model and the text embedding model using query-image training examples and image-image training examples. -
FIG. 8 is a flow diagram of an example process for jointly training an image embedding model and a text embedding model using query-image training examples derived from a historical query log of a search system. -
FIG. 9 is a flow diagram of an example process for training an image embedding model using image-image training examples derived from a historical query log of a search system. -
FIG. 10 shows an example of a portion of a graph representation of query-image training examples and image-image training examples. -
FIG. 11 shows an example search system. -
FIG. 12 shows an example computer system. - Like reference numbers and designations in the various drawings indicate like elements.
- This specification describes a training system for training an image embedding model and a text embedding model using training data derived from a historical query log of a search system. The training data derived from the historical query log can include: query-image training examples, image-image training examples, or both.
- A query-image training example includes: (i) a textual search query, (ii) an image, and (iii) selection data characterizing how often users selected the image in response to the image being identified by a search result for the textual search query. The training system can jointly train the image embedding model and the text embedding model to generate similar embeddings of the textual search query and the image if the selection data indicates that users frequently select the image when it is identified by a search result for the search query.
- An image-image training example includes an image pair (including a first image and a second image) and selection data that indicates: (i) a co-click rate of the image pair, (ii) a similar-image click rate of the image pair, or (iii) both. The co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query.
- The similar-image click rate of the image pair characterizes how often users select the first image in response to the first image being identified by a search result for a search query that includes the second image, or vice versa. The training system can train the image embedding model to generate similar embeddings of the first image and the second image if their co-click rate, similar-image click rate, or both, indicates they are related.
- In some implementations, the training system can use the query-image training examples and the image-image training examples derived from the historical query log to jointly train the image embedding model and the query embedding model using a graph-regularized loss function. In particular, the query-image training examples and the image-image training examples can be represented as a graph structure, and the training system can jointly train the image embedding model and the query embedding model using a loss function based on this graph structure.
- These features and other features are described in more detail below.
-
FIG. 1 shows an exampleimage embedding model 100. Theimage embedding model 100 is configured to process animage 102 in accordance with current values of a set of image embedding model parameters to generate an embedding 104 of theimage 102. The embedding 104 is a representation of theimage 102 as an ordered collection of numerical values, for example, as a vector or matrix. As will be described in more detail below, theimage embedding model 100 can be trained using machine learning techniques to generate an embedding 104 of animage 102 which implicitly represents the semantic content of the image 102 (e.g., objects depicted by the image 102). - The
image embedding model 100 may be configured to processimages 102 which are represented in any appropriate format. For example, theimage embedding model 100 may be configured to processimages 102 which are represented in a red-green-blue (RGB) color format (i.e., a format which represents an image by associating respective red, green, and blue color values with each pixel of the image). As another example, theimage embedding model 100 may be configured to process feature representations of theimages 102, for example, histogram of oriented gradient (HOG), scale-invariant feature transform (SIFT), or speeded up robust feature (SURF) representations of theimages 102. Other feature representations can also be used for training. - The
image embedding model 100 may be a neural network model implemented by computer programs on one or more computers in one or more locations. For example, theimage embedding model 100 may be a convolutional neural network with an architecture derived from the Inception neural network or the ResNet neural network. -
FIG. 2 shows an exampletext embedding model 200. Thetext embedding model 200 is configured to process a representation of a sequence of one or more words in a natural language (i.e., the text 202) in accordance with current values of a set of text embedding model parameters to generate an embedding 204 of the text 202. The embedding 204 is a representation of the text 202 as an ordered collection of numerical values, for example, as a vector or matrix. As will be described in more detail below, thetext embedding model 200 can be trained using machine learning techniques to generate an embedding 204 of the text 202 which implicitly represents the semantic content of the text 202 (e.g., objects described by the text 202). - The
text embedding model 200 may be configured to process text 202 which is represented in any appropriate format. For example, thetext embedding model 200 may be configured to process text 202 which is represented as a sequence of “one-hot” vectors, where each one-hot vector represents a respective character (or word) of the text 202. As another example, thetext embedding model 200 may be configured to process text 202 which is represented by the output of a Word2vec model. - The
text embedding model 200 may be a neural network model implemented by computer programs on one or more computers in one or more locations. For example, thetext embedding model 200 may be a convolutional neural network with an architecture that includes multiple one-dimensional (ID) convolutional layers. As another example, thetext embedding model 200 may be a lookup based mapping from text 202 to embeddings 204. As another example, thetext embedding model 200 may be a sequence of fully-connected layers configured to process n-gram text tokens. As another example, the text embedding model may be a recurrent neural network model (e.g., an LSTM) that is configured to sequentially process representations of characters of the text 202. -
FIG. 3 shows anexample training system 300 for training theimage embedding model 100 and thetext embedding model 200 usingtraining data 302 derived from a historical query log 304 of asearch system 306. Thetraining system 300 trains theimage embedding model 100 and thetext embedding model 200 by determining the values of the image embeddingmodel parameters 308 and the text embeddingmodel parameters 310. For example, when theimage embedding model 100 and thetext embedding model 200 are implemented as respective neural networks, thetraining system 300 can iteratively adjust the parameters of the neural networks using gradients of a loss function, as will be described in more detail below. Thetraining system 300 may be implemented by computer programs on one or more computers in one or more locations. In some implementations, thetraining system 300 uses one or more tensor processing units (TPUs—an application-specific integrated circuit (ASIC) designed for machine learning) during training of theimage embedding model 100 and thetext embedding model 200. - The
search system 306 can be any system configured to perform image searches by processing search queries which includes text, images, or both, to generate search results which identify images responsive to the search queries. An example search system is described in more detail with reference toFIG. 11 . - The historical query log 304 of the
search system 306 indexes a large number (e.g., millions) of search queries previously processed by thesearch system 306. In particular, the historical query log 304 can index a search query by maintaining data including: (i) the search query, and (ii) data which specifies one or more search results that were selected by the user of the device which transmitted the search query. A user can “select” a search result by expressing an interest in the search result through any kind of interaction with the search result. For example, a user can select a search result by clicking on a hypertext link included in the search result, or by hovering a cursor over the search result for a predefined period of time, to generate a request for an electronic document (e.g., image) identified by the search result. - The
training system 300 can process data from thehistorical query log 304 to generate query-image training examples and image-image training examples used to train theimage embedding model 100 and thetext embedding model 200. - A query-image training example includes: (i) a textual search query, (ii) an image, and (iii) selection data characterizing how often users selected the image in response to the image being identified by a search result for the textual search query. The
training system 300 can jointly train theimage embedding model 100 and thetext embedding model 200 to generate similar embeddings of the search query and the image if the selection data indicates that users frequently select the image when it is identified by a search result for the search query. A similarity between embeddings can be determined using any appropriate numerical similarity measure (e.g., a Euclidean distance, if theimage embedding model 100 and thetext embedding model 200 are configured to generate embeddings of the same dimensionality). - An image-image training example includes an image pair (including a first image and a second image) and selection data that indicates: (i) a co-click rate of the image pair, (ii) a similar-image click rate of the image pair, or (iii) both. The co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query.
- The similar-image click rate of the image pair characterizes how often users select the first image in response to the first image being identified by a search result for a search query that includes the second image, or vice versa. The
training system 300 can train theimage embedding model 100 to generate similar embeddings of the first image and the second image if their co-click rate, similar-image click rate, or both, indicates they are related. -
FIG. 4 shows an examplesearch results page 400 provided by thesearch system 306 that includes image search results for a search query that includes a sequence of one or more words. In particular, the search resultspage 400displays search results -
FIG. 5 shows an examplesearch results page 500 provided by thesearch system 306 that includes image search results for a search query that includes an image. In particular, the search resultspage 500displays search results search query 508 that includes an image depicting a truck. In response to receiving a search query that includes a query image, thesearch system 306 may be configured to provide search results which identify images similar to the query image. In this example, each of the search results 502, 504, and 506 identify images which are similar to the query image. - A user is said to “co-click” a first image and a second image if the user selects search results which respectively identify the first image and the second image from the same set of search results. For example, a user may co-click the image identified by the
search result 402 and the image identified by thesearch result 404 by selecting both of the search results (e.g., one after another) on the search resultspage 400. As another example, a user may co-click the image identified by thesearch result 504 and the image identified by thesearch result 506 by selecting both of the search results (e.g., one after another) on the search resultspage 500. If a user selects three or more search results from the same set of search results, the images identified by each pair of selected search results can be considered to be co-clicked. For example, if a user selects search results A, B, and C from the same set of search results, then the pairs of images identified by the search results {A, B}, {A, C}, and {B, C} can be each considered to be co-clicked. -
FIG. 6 illustrates an example process for jointly training theimage embedding model 100 and thetext embedding model 200 using a query-image training example 600. The query-image training example 600 includes a search query 602 which includes a sequence of one or more words and animage 604. Thetraining system 300 processes theimage 604 using theimage embedding model 100 and in accordance with current values of the image embeddingmodel parameters 308 to generate an embedding 606 of theimage 604. Thetraining system 300 processes the search query 602 using thetext embedding model 200 and in accordance with current values of the text embeddingmodel parameters 310 to generate an embedding 608 of the search query 602. - The
training system 300 determines a similarity measure 610 between the embedding 606 of theimage 604 and the embedding 608 of the search query 602, and determines model parameter adjustments 612 based on the similarity measure 610. Thereafter, thetraining system 300 uses the model parameter adjustments 612 to adjust the values of the image embeddingmodel parameters 308 and the text embeddingmodel parameters 310. In some implementations, thetraining system 300 uses the selection data characterizing how often users selected theimage 604 in response to theimage 604 being identified by a search result for the search query 602 in determining the model parameter adjustments 612. An example process for jointly training an image embedding model and a text embedding model using query-image training examples is described in more detail with reference toFIG. 8 . -
FIG. 7A illustrates an example process for training theimage embedding model 100 using an image-image training example 700 including afirst image 702 and asecond image 704. Thetraining system 300 processes thefirst image 702 using theimage embedding model 100 and in accordance with current values of the image embeddingmodel parameters 308 to generate an embedding 706 of thefirst image 702. Similarly, thetraining system 300 processes thesecond image 704 using theimage embedding model 100 and in accordance with current values of the image embeddingmodel parameters 308 to generate an embedding 708 of thesecond image 704. - The
training system 300 determines asimilarity measure 710 between the embedding 706 of thefirst image 702 and the embedding 708 of thesecond image 704, and determines model parameter adjustments 712 based on thesimilarity measure 710. In some implementations, thetraining system 300 uses the selection data characterizing the co-click rate, the similar-image click rate, or both of thefirst image 702 and thesecond image 704 in determining the model parameter adjustments 712. An example process for training an image embedding model using image-image training examples is described in more detail with reference toFIG. 9 . -
FIG. 7B illustrates an example process for jointly training the image embedding model and the text embedding model using query-image training examples and image-image training examples. In particular, at each of multiple training iterations, one or more query-image training examples 600 and one or more image-image training examples 700 can be processed by the image embedding model to generate respective embeddings (as described with reference toFIG. 6 andFIG. 7A ). Thetraining system 300 can determine respective model parameter adjustments 714 based on the query-image training examples 600 (as described with reference toFIG. 6 ) and the image-image training examples (as described with reference toFIG. 7A ). Thetraining system 300 can thereafter use the model parameter adjustments 714 to adjust the current values of the image embeddingmodel parameters 308 and the text embeddingmodel parameters 310. The model parameter adjustments that are determined based on the query-image training examples may be weighted more or less heavily (e.g., using a gradient scaling factor) than the model parameter adjustments that are determined based on the image-image training examples. The weighting applied to the model parameter adjustments may be a tunable system hyper-parameter. -
FIG. 8 is a flow diagram of anexample process 800 for jointly training an image embedding model and a text embedding model using query-image training examples derived from a historical query log of a search system. For convenience, theprocess 800 will be described as being performed by a system of one or more computers located in one or more locations. For example, a training system, e.g., thetraining system 300 ofFIG. 3 , appropriately programmed in accordance with this specification, can perform theprocess 800. - The system processes data from a historical query log of a search system to generate a candidate set of query-image training examples (802). Each of the query-image training examples includes: (i) a search query including a sequence of one or more words, (ii) an image, and (iii) selection data characterizing how often users selected the image in response to the image being identified by a search result for the search query. The selection data may indicate the fraction of times users selected the image in response to the image being identified by a search result for the search query.
- The system selects query-image training examples for use in jointly training the image embedding model and the text embedding model from the candidate set of training examples based at least in part on the selection data of the training examples (804). For example, the system may select a particular query-image training example if the image of the particular training example is most frequently selected by users in response to the image being identified by a search result for the search query of the particular query-image training example. As another example, the system may select a particular query-image training example if the image of the particular query-image training example is in the top N images that are most frequently selected by users after being identified by search results for the search query of the particular query-image training example. The system can use any of a variety of other appropriate criteria in selecting query-image training examples for use in jointly training the image embedding model and the text embedding model. For example, the system may limit the number of selected query-image training examples which include search queries that specify the names of particular people, and corresponding images which depict the particular people. In this example, since the appearance of the same person can vary substantially between images (e.g., due to the person wearing different clothing, shoes, glasses, and the like), including a large number of query-image training examples corresponding to particular people may reduce the effectiveness of the training process.
- Steps 806-812 describe an example process that can be performed for each selected query-image training example to jointly train the image embedding model and the text embedding model. For convenience, steps 806-812 describe steps that can be performed for a given query-image training example. More generally, any appropriate method can be used to jointly train the image embedding model and the text embedding model. For example, a stochastic gradient descent method can be used to jointly train the image embedding model and the text embedding model, where the steps 806-812 are iteratively repeated for “batches” (i.e., sets) of query-image training examples. In this example, the system may determine that the training is complete when a training termination criterion is satisfied. For example, the training termination criterion may be that a predetermined number of iterations of the steps 806-812 have been performed. As another example, the training termination criterion may be that a change in the values of the parameters of the image embedding model and the text embedding model between iterations of the steps 806-812 is below a predetermined threshold.
- The system processes the image of the given query-image training example using the image embedding model and in accordance with current values of the image embedding model parameters to generate an embedding of the image (806). For example, if the image embedding model is a neural network model, the system processes the image using a sequence of neural network layers defined by the architecture of the neural network model.
- The system processes a representation of the search query of the given query-image training example using the text embedding model and in accordance with current values of the text embedding model parameters to generate an embedding of the search query (808). For example, if the text embedding model is a neural network model, the system processes the representation of the search query using a sequence of neural network layers defined by the architecture of the neural network model.
- The system determines a measure of similarity between the embedding of the image and the embedding of the search query of the given query-image training example (810). For example, the embedding of the image and the embedding of the search query may have the same dimensionality and the system may determine the measure of similarity by determining a Euclidean distance or cosine similarity measure between the respective embeddings.
- The system adjusts the image embedding model parameters and the text embedding model parameters based at least in part on the measure of similarity between the embedding of the image and the embedding of the search query of the given query-image training example (812). For example, when the image embedding model and the text embedding model are respective neural network models, the system may determine the gradient of a loss function and use the gradient of the loss function to adjust the image embedding model parameters and the text embedding model parameters. The system can determine the gradient of the loss function using any appropriate method, for example, backpropagation. The loss function can be any appropriate loss function that depends on the measure of similarity between the embedding of the image and the embedding of the search query of the given query-image training example. A few examples follow.
- In some implementations, the loss function may be a classification loss function. In these implementations, the search query of the given query-image training example is considered to identify a “positive” label for the image of the given query-image training example. The search queries of the other query-image training examples are considered to identify respective “negative” labels for the image of the given query-image training example. More specifically, the system may determine the similarity measure between the embedding of the image of the given query-image training example and the embedding of the search query of the given query-image training example as a “positive” score. The system may determine respective “negative” scores for each other training example as a similarity measure between the embedding of the image of the given query-image training example and an embedding of the search query of the other training example. The system can process the positive and negative scores using a soft-max (or sampled soft-max) function, and provide the output of the soft-max (or sampled soft-max) function to a cross-entropy loss function (or any other appropriate classification loss function).
- In some implementations, the loss function may be a triplet loss function. In these implementations, the system may determine the embedding of the image of the given query-image training example to be the “anchor”, the embedding of the search query of the given query-image training example to be the “positive”, and the embedding of the search query of another query-image training example to be the “negative”.
- Optionally, the loss function may depend on the selection data for the given query-image training example which characterizes how often users selected the image in response to the image being identified by a search result for the search query of the given query-image training example. For example, the loss function may include a multiplicative scaling factor based on the fraction of times users selected the image in response to the image being identified by a search result for the search query of the given query-image training example.
-
FIG. 9 is a flow diagram of anexample process 900 for training an image embedding model using image-image training examples derived from a historical query log of a search system. For convenience, theprocess 900 will be described as being performed by a system of one or more computers located in one or more locations. For example, a training system, e.g., thetraining system 300 ofFIG. 3 , appropriately programmed in accordance with this specification, can perform theprocess 900. - The system processes data from a historical query log of a search system to generate image-image training examples (902). Each of the image-image training examples includes: (i) an image pair including a first image and a second image, (ii) selection data indicating a co-click rate of the image pair, a similar-image click rate of the image pair, or both.
- The co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query. For example, the co-click rate of the image pair may indicate the fraction of times users selected both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query.
- The similar-image click rate of the image pair characterizes how often users select the first image in response to the first image being identified by a search result for a search query that includes the second image, or vice versa. For example, the similar-image click rate of the image pair may indicate the fraction of times users selected the first image in response of the first image being identified by a search result for a search query that includes the second image, or vice versa.
- Steps 904-908 describe an example process that can be performed for each image-image training example to train the image embedding model. For convenience, steps 904-908 describe steps that can be performed for each image-image training example. More generally, any appropriate method can be used to train the image embedding model. For example, a stochastic gradient descent method can be used to train the image embedding model, where the steps 904-908 are iteratively repeated for “batches” (i.e., sets) of selected training examples. In this example, the system may determine that the training is complete when a training termination criterion is satisfied. For example, the training termination criterion may be that a predetermined number of iterations of the steps 904-908 have been performed. As another example, the training termination criterion may be that a change in the values of the parameters of the image embedding model between iterations of the steps 904-908 is below a predetermined threshold. As will be described in more detail with reference to
FIG. 10 , the image-image training examples can also be used in conjunction with query-image training examples to jointly train the image embedding model and the text embedding model using a graph-regularized loss function. - The system processes the first image and the second image of the training example using the image embedding model and in accordance with current values of the image embedding model parameters to generate respective embeddings of the first image and the second image of the training example (904). For example, if the image embedding model is a neural network model, the system processes the first image and the second image (e.g., one after another) using a sequence of neural network layers defined by the architecture of the neural network model.
- The system determines a measure of similarity between the embedding of the first image and the embedding of the second image (906). For example, the system may determine the measure of similarity by determining a Euclidean distance or cosine similarity measure between the respective embeddings of the first image and the second image.
- The system adjusts the image embedding model parameters based at least in part on: (i) the measure of similarity between the respective embeddings of the first image and the second image, and (ii) the selection data (i.e., the co-click rate, similar-image click rate, or both) (910). For example, when the image embedding model is a neural network, the system may determine the gradient of a loss function and use the gradient of the loss function to adjust the image embedding model parameters. The system can determine the gradient of the loss function using any appropriate method, for example, backpropagation. The loss function can be any appropriate loss function that depends on the measure of similarity between the respective embeddings and the selection data. For example, the loss function may be given by:
-
w·D(h0U1),h0U2)) (1) - where h0(/
1 ) represents the embedding of the first image of the image-image training example, h0(l2) represents the embedding of the second image of the image-image training example, ‘D(·,·) is a similarity measure (e.g., a Euclidean similarity measure), and the scaling factor w can be determined in any appropriate manner using the co-click rate, similar-image click rate, or both, of the image-image training example. For example, the scaling strength w can be determined as a linear combination (e.g., using predetermined weighting factors) of the co-click rate and similar-image click rate of the image-image training example. - In some implementations, the
training system 300 can use the query-image training examples and the image-image training examples to jointly train theimage embedding model 100 and thetext embedding model 200 using a graph-regularized loss function. The query-image training examples and image-image training examples can be understood to represent a graph structure, where each node of the graph corresponds to a query-image training example and each edge corresponds to an image-image training example. More specifically, an edge in the graph which connects a first and a second node which respectively correspond to a first and a second query-image training example may be defined by an image-image training example which includes the image pair specified by the first and second query-image training examples. In particular, the “strength” of the edge connecting the first node and the second node may be defined based on the co-click rate, the similar-image click rate, or both, specified by the image-image training example corresponding to the edge.FIG. 10 shows an example of a portion of a graph representation, wherenodes respective images edge 1010 connecting thenodes image pair - In one example, the graph-regularized loss function may have the form:
-
- where i indexes the nodes in the graph representation, N is the total number of nodes, Ii represents the image associated with the i-th node (e.g., of the image-query training example corresponding to the i-th node, if there is one), Qi represents the search query of the image-query training example corresponding to the i-th node (if there is one), £1(Ii,Qa represents the loss function associated with the image-query training example corresponding to the i-th node (e.g., the classification loss or triplet loss described with reference to 812), N (i) represent the set of “neighbors” of node i in the graph representation, wij represents the strength of the edge connecting node i and node j in the graph representation, heUa represents the embedding of the image of Ii associated with the i-th node, he(Ij) represents the embedding of the image Ij of the image associated with the j-th node, and ‘D(·,·) is a similarity measure (e.g., a Euclidean similarity measure). Two nodes in the graph representation are said to be neighbors if they are connected by an edge. The strength wij of the edge connecting nodes i and j can be determined in any appropriate manner using the co-click rate, similar-image click rate, or both, of the image-image training example which defines the edge. For example, the strength wij of the edge connecting nodes i and j can be determined as a linear combination (e.g., using predetermined weighting factors) of the co-click rate and similar-image click rate. For nodes which are associated with an image but not a textual search query, the £1(Ii,Qa component of the loss defied by
equation 2 may be removed. - The
training system 300 can jointly train theimage embedding model 100 and thetext embedding model 200 using a graph-regularized loss function (e.g., as described by equation 2) using any appropriate machine learning training technique. For example, thetraining system 300 can jointly train theimage embedding model 100 and thetext embedding model 200 by stochastic gradient descent using an alternative representation of the loss function in equation 2: -
- where i and j index the nodes of the graph representation, (i,j) EE if node i and node j are connected by an edge in the graph representation, |i| represents the number of edges incident to node i, ij| represents the number of edges incident to node j, and the remaining variables are defined in the same manner as for
equation 2. In this example, thetraining system 300 can perform stochastic gradient descent by sampling edges from the graph representation at each training iteration and using backpropagation (or any other appropriate technique) to determine the gradient of the loss function given byequations 3 and 4. Thetraining system 300 can determine the training is complete when any appropriate training termination criterion is satisfied, for example, when a predetermined number of iterations of stochastic gradient descent have been performed. An example method for training theimage embedding model 100 andtext embedding model 200 using a graph-regularized loss function is described with reference to: - T. D. Bui, S. Ravi, V. Ramavajjala, “Neural Graph Machines: Learning Neural Networks Using Graphs”, 2017, arXiv: 1 703.04818v1.
- After the
training system 300 determines the values of the image embeddingmodel parameters 308 and the text embeddingmodel parameters 310, the trainedimage embedding model 100 andtext embedding model 200 can be used for any of a variety of purposes. A few examples follow. - In one example, the trained
image embedding model 100 can be used by thesearch system 306 in ranking image search results responsive to a search query that includes a query image. More specifically, thesearch system 306 can use theimage embedding model 100 to generate a respective embedding of each image in a search index maintained by the search system (as described with reference toFIG. 11 ). After receiving a search query that includes a query image, thesearch system 306 can use the image embedding model to generate an embedding of the query image, and thereafter use the generated embedding to determine a respective relevance score for each of multiple images in the search index. Thesearch system 306 can determine the relevance score for a given image in the search index based on a measure of similarity (e.g., a Euclidean distance) between the embedding of the given image and the embedding of the query image. Thesearch system 306 can determine the ranking of the image search results for the search query based at least in part on the relevance scores determined using the embeddings generated by theimage embedding model 100. - In another example, the trained text embedding model and the trained image embedding model can both be used by the
search system 306 in ranking image search results responsive to a search query that includes a sequence of one or more words. More specifically, thesearch system 306 can use theimage embedding model 100 to generate a respective embedding of each image in a search index maintained by the search system. After receiving a search query that includes a sequence of one or more words, the search system can use the text embedding model to generate an embedding of the sequence of words, and thereafter use the generated embedding to determine a respective relevance score for each of multiple images in the search index. The search system can determine the relevance score for a given image in the search index based on a measure of similarity (e.g., a Euclidean distance) between the embedding of the given image and the embedding of the sequence of words of the search query. Thesearch system 306 can determine the ranking of the image search results for the search query based at least in part on the relevance scores determined using the embeddings generated by the image embedding model and the text embedding model. - In another example, the trained text embedding model can be used to determine “clusters” of similar keywords (or keyword sequences), that is, sets of keywords which express similar semantic content. In a particular example, a cluster of similar keywords may be: “shoes”, “shoe”, “footwear”, “boots”, “cleats”, “heels”, “slippers”, “sneakers”, and the like. Keyword clusters can be generated using the text embedding model by determining a respective embedding of each keyword in a corpus of keywords, and thereafter using a clustering algorithm to cluster the keywords based on their respective embeddings. The clustering algorithm may be, for example, a k-means clustering algorithm or an expectation maximization clustering algorithm. Keyword clusters generated using the trained text embedding model can be used as distribution parameters that condition the transmission of digital components (e.g., advertisements) for presentation with electronic documents (e.g., webpages).
- In another example, the trained text embedding model and the trained image embedding model can both be used in an image classification system configured to process an image to generate an output which associates the image with a label from a predetermined set of labels. For examples, the labels may specify object classes (e.g., person, cat, vehicle, and the like), and the image classification system may be trained to associate an image with the label of an object depicted in the image. In this example, the image classification system may use the
image embedding model 100 to generate an embedding of an input image, and the text embedding model to generate a respective embedding of each search query in a corpus of search queries. The image classification system may determine a respective measure of similarity between the embedding of the input image and the respective embedding of each search query, and may thereafter associate the input image with a particular search query with the highest measure of similarity. The image classification system may determine the label to associate with the input image based on both: (i) visual features derived from the input image, and (ii) semantic features derived from the particular search query. An example of an image classification system that can use thetext embedding model 200 and theimage embedding model 100 is described with reference to U.S. Patent Application No. 62/768,701. -
FIG. 11 shows anexample search system 100. Thesearch system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations in which the systems, components, and techniques described below are implemented. - The
search system 1100 is configured to receive asearch query 1102 from auser device 1104, to process thesearch query 1102 to determine one ormore search results 1106 responsive to thesearch query 1102, and to provide thesearch results 1106 to theuser device 1104. Thesearch query 1102 can include search terms expressed in a natural language (e.g., English), images, audio data, or any other appropriate form of data. Asearch result 1106 identifies anelectronic document 1108 from awebsite 1110 that is responsive to thesearch query 1102, and includes a link to theelectronic document 1108.Electronic documents 1108 can include, for example, images, HTML webpages, word processing documents, portable document format (PDF) documents, and videos. Theelectronic documents 1108 can include content, such as words, phrases, images, and audio data, and may include embedded information (e.g., meta information and hyperlinks) and embedded instructions (e.g., scripts). Awebsite 1110 is a collection of one or moreelectronic documents 1108 that is associated with a domain name and hosted by one or more servers. For example, awebsite 1110 may be a collection ofwebpages formatted in hypertext markup language (HTML) that can contain text, images, multimedia content, and programming elements (e.g., scripts). - In a particular example, a
search query 1102 can include the search terms “Apollo moon landing”, and thesearch system 1100 may be configured to perform an image search, that is, to providesearch results 1106 which identify respective images that are responsive to thesearch query 1102. In particular, thesearch system 1100 may providesearch results 1106 that each include: (i) a title of a webpage, (ii) a representation of an image extracted from the webpage, and (iii) a hypertext link (e.g., specifying a uniform resource locator (URL)) to the webpage or to the image itself In this example, thesearch system 1100 may provide asearch result 1106 that includes: (i) the title “Apollo moon landing” of a webpage, (ii) a reduced-size representation (i.e., thumbnail) of an image of the Apollo spacecraft included in the webpage, and (iii) a hypertext link to the image. - A
computer network 1112, such as a local area network (LAN), wide area network (WAN), the Internet, a mobile phone network, or a combination thereof, connects thewebsites 1110, theuser devices 1104, and the search system 1100 (i.e., enabling them to transmit and receive data over the network 1112). In general, thenetwork 1112 can connect thesearch system 1100 to many thousands ofwebsites 1110 anduser devices 1104. - A
user device 1104 is an electronic device that is under control of a user and is capable of transmitting and receiving data (including electronic documents 1108) over thenetwork 1112.Example user devices 1104 include personal computers, mobile communication devices, and other devices that can transmit and receive data over thenetwork 1112. Auser device 1104 typically includes user applications (e.g., a web browser) which facilitate transmitting and receiving data over thenetwork 1112. In particular, user applications included in auser device 1104 enable theuser device 1104 to transmitsearch queries 1102 to thesearch system 1100, and to receive thesearch results 1106 provided by thesearch system 1100 in response to the search queries 1102, over thenetwork 1112. - The user applications included in the
user device 1104 can present thesearch results 1106 received from thesearch system 1100 to a user of the user device (e.g., by rendering a search results page which shows an ordered list of the search results 1106). The user may select one of thesearch results 1106 presented by the user device 1104 (e.g., by clicking on a hypertext link included in the search result 1106), which can cause theuser device 1104 to generate a request for anelectronic document 1108 identified by thesearch result 1106. The request for theelectronic document 1108 identified by thesearch result 1106 is transmitted over thenetwork 1112 to awebsite 1110 hosting theelectronic document 1108. In response to receiving the request for theelectronic document 1108, thewebsite 1110 hosting theelectronic document 1108 can transmit theelectronic document 1108 to theuser device 1104. - The
search system 1100 processes asearch query 1102 using aranking engine 1114 to determinesearch results 1106 responsive to thesearch query 1102. - The
search system 1100 uses anindexing engine 1120 to generate and maintain thesearch index 1116 by “crawling” (i.e., systematically browsing) theelectronic documents 1108 of thewebsites 1110. For each of a large number (e.g., millions) ofelectronic documents 1108, thesearch index 1116 indexes the electronic document by maintaining data which: (i) identifies the electronic document 1108 (e.g., by a link to the electronic document 1108), and (ii) characterizes theelectronic document 1108. The data maintained by thesearch index 1116 which characterizes an electronic document may include, for example, data specifying a type of the electronic document (e.g., image, video, PDF document, and the like), a quality of the electronic document (e.g., the resolution of the electronic document when the electronic document is an image or video), keywords associated with the electronic document, a cached copy of the electronic document, or a combination thereof. - The
search system 1100 can store thesearch index 1116 in a data store which may include thousands of data storage devices. Theindexing engine 1120 can maintain thesearch index 1116 by continuously updating thesearch index 1116, for example, by indexing newelectronic documents 1108 and removingelectronic documents 1108 that are no longer available from thesearch index 1116. - The
search system 1100 uses aquery logging engine 1122 to generate and maintain a historical query log 1118 (as described earlier). Thesearch system 1100 can store thehistorical query log 1118 in a data store which may include thousands of data storage devices. Thequery logging engine 1122 can maintain thehistorical query log 1118 by continuously updating the historical query log 1118 (e.g., by indexing new search queries as they are processed by the search system 1100). - The
ranking engine 1114 determinessearch results 1106 responsive to thesearch query 1102 by scoringelectronic documents 1108 indexed by thesearch index 1116. Theranking engine 1114 can scoreelectronic documents 1108 based in part on data accessed from thehistorical query log 1118. The score determined by theranking engine 1114 for anelectronic document 1108 characterizes how responsive (e.g., relevant) the electronic document is to thesearch query 1102. Theranking engine 1114 determines a ranking of theelectronic documents 1108 indexed by thesearch index 1116 based on their respective scores, and determines the search results based on the ranking. For example, theranking engine 1114 can generatesearch results 1106 which identify the highest-rankedelectronic documents 1108 indexed by thesearch index 1116. -
FIG. 12 is block diagram of anexample computer system 1200 that can be used to perform operations described above. Thesystem 1200 includes aprocessor 1210, amemory 1220, astorage device 1230, and an input/output device 1240. Each of thecomponents system bus 1250. Theprocessor 1210 is capable of processing instructions for execution within thesystem 1200. In one implementation, theprocessor 1210 is a single-threaded processor. In another implementation, theprocessor 1210 is a multi-threaded processor. Theprocessor 1210 is capable of processing instructions stored in thememory 1220 or on thestorage device 1230. - The
memory 1220 stores information within thesystem 1200. In one implementation, thememory 1220 is a computer-readable medium. In one implementation, thememory 1220 is a volatile memory unit. In another implementation, thememory 1220 is a non-volatile memory unit. - The
storage device 1230 is capable of providing mass storage for thesystem 1200. In one implementation, thestorage device 1230 is a computer-readable medium. In various different implementations, thestorage device 1230 can include, for example, a hard disk device, an optical disk device, a storage device that is shared over a network by multiple computing devices (e.g., a cloud storage device), or some other large capacity storage device. - The input/
output device 1240 provides input/output operations for thesystem 1200. In one implementation, the input/output device 1240 can include one or more network interface devices, e.g., an Ethernet card, a serial communication device, e.g., and RS-232 port, and/or a wireless interface device, e.g., and 802.11 card. In another implementation, the input/output device can include driver devices configured to receive input data and send output data to other input/output devices, e.g., keyboard, printer anddisplay devices 1260. Other implementations, however, can also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, etc. - Although an example processing system has been described in
FIG. 12 , implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. - This specification uses the term “configured” in connection with systems and computer program components. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternative!y or in addition, the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- The term “data processing apparatus” refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- A computer program, which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- In this specification the term “engine” is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. Generally, an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser. Also, a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client. Data generated at the user device, e.g., a result of the user interaction, can be received at the server from the device.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially be claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings and recited in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (21)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18/171,511 US20230205813A1 (en) | 2019-02-01 | 2023-02-20 | Training Image and Text Embedding Models |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/265,793 US11586927B2 (en) | 2019-02-01 | 2019-02-01 | Training image and text embedding models |
US18/171,511 US20230205813A1 (en) | 2019-02-01 | 2023-02-20 | Training Image and Text Embedding Models |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/265,793 Continuation US11586927B2 (en) | 2019-02-01 | 2019-02-01 | Training image and text embedding models |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230205813A1 true US20230205813A1 (en) | 2023-06-29 |
Family
ID=68610304
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/265,793 Active 2041-09-24 US11586927B2 (en) | 2019-02-01 | 2019-02-01 | Training image and text embedding models |
US18/171,511 Pending US20230205813A1 (en) | 2019-02-01 | 2023-02-20 | Training Image and Text Embedding Models |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/265,793 Active 2041-09-24 US11586927B2 (en) | 2019-02-01 | 2019-02-01 | Training image and text embedding models |
Country Status (4)
Country | Link |
---|---|
US (2) | US11586927B2 (en) |
EP (1) | EP3759616A1 (en) |
CN (1) | CN112074828A (en) |
WO (1) | WO2020159592A1 (en) |
Families Citing this family (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11720621B2 (en) * | 2019-03-18 | 2023-08-08 | Apple Inc. | Systems and methods for naming objects based on object content |
US20220301004A1 (en) * | 2019-08-30 | 2022-09-22 | Ntt Docomo, Inc. | Click rate prediction model construction device |
US11216697B1 (en) * | 2020-03-11 | 2022-01-04 | Amazon Technologies, Inc. | Backward compatible and backfill-free image search system |
US11734339B2 (en) * | 2020-10-20 | 2023-08-22 | Adobe Inc. | Generating embeddings in a multimodal embedding space for cross-lingual digital image retrieval |
US11615567B2 (en) | 2020-11-18 | 2023-03-28 | Adobe Inc. | Image segmentation using text embedding |
US11941792B2 (en) * | 2021-04-09 | 2024-03-26 | Dell Products L.P. | Machine learning-based analysis of computing device images included in requests to service computing devices |
US20230048742A1 (en) * | 2021-08-10 | 2023-02-16 | Verizon Media Inc. | Search query generation based upon received text |
US20230315766A1 (en) * | 2022-03-31 | 2023-10-05 | Capital One Services, Llc | Methods, mediums, and systems for reusable intelligent search workflows |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8280881B1 (en) * | 2009-10-29 | 2012-10-02 | Google Inc. | Similar search queries and images |
US20140324836A1 (en) * | 2010-03-29 | 2014-10-30 | Ebay Inc. | Finding similar items using windows of computation |
US20170243082A1 (en) * | 2014-06-20 | 2017-08-24 | Google Inc. | Fine-grained image similarity |
US20180070895A1 (en) * | 2016-09-14 | 2018-03-15 | Dental Imaging Technologies Corporation | Multiple-dimension imaging sensor and state-based operation of an imaging system including a multiple-dimension imaging sensor |
US20200250538A1 (en) * | 2019-02-01 | 2020-08-06 | Google Llc | Training image and text embedding models |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8429168B1 (en) * | 2009-12-15 | 2013-04-23 | Google Inc. | Learning semantic image similarity |
US8788434B2 (en) * | 2010-10-28 | 2014-07-22 | Google Inc. | Search with joint image-audio queries |
US8832096B1 (en) * | 2011-09-01 | 2014-09-09 | Google Inc. | Query-dependent image similarity |
US10311096B2 (en) * | 2012-03-08 | 2019-06-04 | Google Llc | Online image analysis |
US9836641B2 (en) * | 2014-12-17 | 2017-12-05 | Google Inc. | Generating numeric embeddings of images |
CN108319633B (en) * | 2017-11-17 | 2022-02-11 | 腾讯科技（深圳）有限公司 | Image processing method and device, server, system and storage medium |
-
2019
- 2019-02-01 US US16/265,793 patent/US11586927B2/en active Active
- 2019-10-24 CN CN201980029223.1A patent/CN112074828A/en active Pending
- 2019-10-24 WO PCT/US2019/057828 patent/WO2020159592A1/en unknown
- 2019-10-24 EP EP19805801.8A patent/EP3759616A1/en not_active Withdrawn
-
2023
- 2023-02-20 US US18/171,511 patent/US20230205813A1/en active Pending
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8280881B1 (en) * | 2009-10-29 | 2012-10-02 | Google Inc. | Similar search queries and images |
US20140324836A1 (en) * | 2010-03-29 | 2014-10-30 | Ebay Inc. | Finding similar items using windows of computation |
US20170243082A1 (en) * | 2014-06-20 | 2017-08-24 | Google Inc. | Fine-grained image similarity |
US20180070895A1 (en) * | 2016-09-14 | 2018-03-15 | Dental Imaging Technologies Corporation | Multiple-dimension imaging sensor and state-based operation of an imaging system including a multiple-dimension imaging sensor |
US20200250538A1 (en) * | 2019-02-01 | 2020-08-06 | Google Llc | Training image and text embedding models |
Also Published As
Publication number | Publication date |
---|---|
US20200250537A1 (en) | 2020-08-06 |
WO2020159592A1 (en) | 2020-08-06 |
CN112074828A (en) | 2020-12-11 |
EP3759616A1 (en) | 2021-01-06 |
US11586927B2 (en) | 2023-02-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20240078258A1 (en) | Training Image and Text Embedding Models | |
US20230205813A1 (en) | Training Image and Text Embedding Models | |
US20220222920A1 (en) | Content processing method and apparatus, computer device, and storage medium | |
US8515212B1 (en) | Image relevance model | |
US10726297B2 (en) | Systems and methods for identifying semantically and visually related content | |
US20210103783A1 (en) | Deep learning tag-based font recognition utilizing font classification | |
US10891673B1 (en) | Semantic modeling for search | |
US8429173B1 (en) | Method, system, and computer readable medium for identifying result images based on an image query | |
US8370282B1 (en) | Image quality measures | |
US8131786B1 (en) | Training scoring models optimized for highly-ranked results | |
US20230409653A1 (en) | Embedding Based Retrieval for Image Search | |
US7962500B2 (en) | Digital image retrieval by aggregating search results based on visual annotations | |
RU2711125C2 (en) | System and method of forming training set for machine learning algorithm | |
US20210089570A1 (en) | Methods and apparatus for assessing candidates for visual roles | |
US20110191336A1 (en) | Contextual image search | |
US11172040B2 (en) | Method and apparatus for pushing information | |
US10642670B2 (en) | Methods and systems for selecting potentially erroneously ranked documents by a machine learning algorithm | |
US10606910B2 (en) | Ranking search results using machine learning based models | |
US20180101534A1 (en) | Accounting for Positional Bias in A Document Retrieval System Using Machine Learning | |
US10353951B1 (en) | Search query refinement based on user image selections | |
US20200159765A1 (en) | Performing image search using content labels | |
US20230096118A1 (en) | Smart dataset collection system | |
US20220107980A1 (en) | Providing an object-based response to a natural language query | |
WO2021007159A1 (en) | Identifying entity attribute relations | |
US8694529B1 (en) | Refinement surfacing for search hierarchies |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:RAVI, SUJITH;TOMKINS, ANDREW;DUERIG, THOMAS J.;AND OTHERS;SIGNING DATES FROM 20190301 TO 20190604;REEL/FRAME:062744/0415 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |