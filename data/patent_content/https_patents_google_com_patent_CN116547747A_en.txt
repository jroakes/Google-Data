CN116547747A - Weakening the results of automatic speech recognition processing - Google Patents
Weakening the results of automatic speech recognition processing Download PDFInfo
- Publication number
- CN116547747A CN116547747A CN202180081357.5A CN202180081357A CN116547747A CN 116547747 A CN116547747 A CN 116547747A CN 202180081357 A CN202180081357 A CN 202180081357A CN 116547747 A CN116547747 A CN 116547747A
- Authority
- CN
- China
- Prior art keywords
- asr
- processing
- microphone
- level
- audio stream
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
Abstract
A method (300) for attenuating speech processing includes: an indication of a microphone trigger event (202) is received at a sound enabled device (110) indicating a possible interaction with the device through speech, wherein the device has a microphone (116), the microphone (116) being configured to capture speech when turned on. In response to receiving an indication of a microphone trigger event, the method further includes instructing the microphone to open or remain open for a duration window (212) to capture an audio stream (16), and providing the audio stream captured by the open microphone to the speech recognition system (150). During the duration window, the method further includes weakening a level of speech recognition processing based on a function of the duration window (222), and instructing the speech recognition system to use the weakened level of speech recognition processing on the audio stream (204, 222).
Description
Technical Field
The present disclosure relates to attenuating automatic speech recognition results.
Background
Users often interact with sound enabled devices, such as smartphones, smartwatches, and smartspeakers, through a digital assistant interface. These digital assistant interfaces enable users to complete tasks and obtain answers to questions they hold, entirely through natural conversational interactions.
Ideally, when talking with a digital assistant interface, a user should be able to communicate via a verbal request directed to the voice-enabled device they are running the digital assistant interface, as if the user were talking with another person. The digital assistant interface provides these verbal requests to an automatic speech recognizer to process and recognize the verbal requests so that actions can be performed. In practice, however, it is challenging for devices to always respond to these verbal requests, as the cost of running speech recognition on resource-constrained sound-enabled devices (such as smartphones or smartwatches) on a continuous basis is extremely high.
Disclosure of Invention
One aspect of the present disclosure provides a method of mitigating automated speech recognition processing. The method comprises the following steps: an indication of a microphone trigger event is received at data processing hardware of a sound enabled device for indicating a possible user interaction with the sound enabled device through speech, wherein the sound enabled device has a microphone configured to capture the speech when turned on for recognition by an Automatic Speech Recognition (ASR) system. In response to receiving the indication of the microphone trigger event, the method further comprises: instructing, by the data processing hardware, the microphone to turn on or remain on for an on microphone duration window to capture an audio stream in the environment of the sound enabled device; and providing, by the data processing hardware, the audio stream captured by the turned-on microphone to the ASR system to perform ASR processing on the audio stream. When the ASR system performs ASR processing on an audio stream captured by the turned-on microphone, the method further comprises: attenuating, by the data processing hardware, a level of ASR processing performed by the ASR system on the audio stream based on the function of opening the microphone duration window; and instructing, by the data processing hardware, the ASR system to use the attenuated level of ASR processing on the audio stream captured by the turned-on microphone.
In some examples, when the ASR system performs ASR processing on an audio stream captured by the turned-on microphone, the method further comprises: it is determined by the data processing hardware whether sound activity is detected in the audio stream captured by the microphone that is turned on. In these examples, the level of ASR processing performed by the abatement ASR system on the audio stream is further based on a determination of whether any voice activity is detected in the audio stream. In some embodiments, the method further comprises: the current context at the time of receiving the indication of the microphone trigger event is obtained by the data processing hardware. In these embodiments, instructing the ASR system to use the level of attenuation of the ASR process includes: the ASR system is instructed to bias the speech recognition results based on the current context. In some configurations, after instructing the ASR system to use the attenuated level of ASR processing on the audio stream, the method additionally includes: receiving, at the data processing hardware, an indication that a confidence level of a speech recognition result for the voice query output by the ASR system fails to meet a confidence level threshold; and instructing the ASR system by the data processing hardware to increase the level of ASR processing from the attenuated level; and reprocessing the voice query using the increased ASR processing level. In some implementations, when the ASR system performs ASR processing on an audio stream captured by an open microphone: determining, by the data processing hardware, when the level of attenuation of ASR processing performed on the audio stream by the ASR based on the function of the on microphone duration is equal to zero; and commanding, by the data processing hardware, the microphone to shut down when the level of attenuation of the ASR processing is equal to zero. Optionally, the method may further comprise: a graphical indicator indicating a level of attenuation of ASR processing performed on the audio stream by the ASR system is displayed in a graphical user interface of the sound enabled device by the data processing hardware.
Another aspect of the present disclosure provides a system for mitigating automated speech recognition processing. The system includes data processing hardware and memory hardware in communication therewith. The memory hardware stores instructions that, when executed on the data processing hardware, cause the data processing hardware to perform operations. The operation includes: an indication of a microphone trigger event is received at a sound enabled device indicating a possible user interaction with the sound enabled device through speech, wherein the sound enabled device has a microphone configured to capture the speech when turned on for recognition by an Automatic Speech Recognition (ASR) system. In response to receiving an indication of a microphone trigger event, the operations further comprise: instructing the microphone to turn on or remain on for an on microphone duration window to capture an audio stream in the environment of the sound enabled device; and providing the audio stream captured by the open microphone to an ASR system to perform ASR processing on the audio stream. When the ASR system performs ASR processing on an audio stream captured by the turned-on microphone, the operations further include: attenuating the level of ASR processing performed by the ASR system on the audio stream based on the function of opening the microphone duration window; and instructing the ASR system to use the attenuated level of ASR processing on the audio stream captured by the turned-on microphone.
This aspect may include one or more of the following optional features. In some examples, when the ASR system performs ASR processing on an audio stream captured by the open microphone, the operations further comprise: it is determined whether sound activity is detected in an audio stream captured by the microphone that is turned on. In these examples, the level of ASR processing performed by the abatement ASR system on the audio stream is further based on a determination of whether any voice activity is detected in the audio stream. In some implementations, the operations further comprise: a current context at the time of receiving an indication of a microphone trigger event is obtained. In these embodiments, instructing the ASR system to use the level of attenuation of the ASR process includes: the ASR system is instructed to bias the speech recognition results based on the current context. In some configurations, after instructing the ASR system to use the attenuated level of ASR processing on the audio stream, the operations additionally include: the method includes receiving an indication that a confidence level for a speech recognition result of a voice query output by an ASR system fails to meet a confidence threshold, and instructing the ASR system to increase a level of ASR processing from a reduced level, and reprocessing the voice query using the increased level of ASR processing. In some implementations, when the ASR system performs ASR processing on an audio stream captured by the turned-on microphone, the operations further include: determining when the level of attenuation of ASR processing performed on the audio stream by the ASR based on the function of turning on the microphone duration is equal to zero; and commanding the microphone to shut down when the level of attenuation of the ASR process is equal to zero. Optionally, the operations may further comprise: a graphical indicator indicating a level of attenuation of the ASR processing performed on the audio stream by the ASR system is displayed in a graphical user interface of the sound enabled device.
Embodiments of the system or method may include one or more of the following optional features. In some embodiments, the ASR system initially performs ASR processing on the audio stream at the beginning of opening the microphone duration window using a first processing level, wherein the first processing level is associated with full processing capability of the ASR system. In these embodiments, attenuating the level of ASR processing performed by the ASR system on the audio stream based on the function of opening the microphone duration window includes: determining whether a first time interval has elapsed since opening a microphone duration window; and when the first time interval has elapsed, weakening the level of ASR processing performed by the ASR system on the audio stream by reducing the level of ASR processing from a first processing level to a second processing level, wherein the second processing level is lower than the first processing level. In some examples, instructing the ASR system to use the level of attenuation of the ASR process includes: the ASR system is instructed to switch from performing ASR processing on a remote server in communication with the voice-enabled device to performing ASR processing on data processing hardware of the voice-enabled device. In some configurations, instructing the ASR system to use the level of attenuation of the ASR process includes: the ASR system is instructed to switch from using the first ASR model to a second ASR model for performing ASR processing on the audio stream, wherein the second ASR model includes fewer parameters than the first ASR model. Instructing the ASR system to use the level of attenuation of the ASR process may include: the ASR system is instructed to reduce the number of ASR processing steps performed on the audio stream. Instructing the ASR system to use the level of attenuation of the ASR process may further include: the ASR system is instructed to adjust the beam search parameters to narrow the decoding search space of the ASR system. Additionally or alternatively, instructing the ASR system to use the level of attenuation of the ASR process may include: the ASR system is instructed to perform quantization and/or sparsification of one or more parameters of the ASR system. In some configurations, instructing the ASR system to use the level of attenuation of the ASR process may include: the ASR system is instructed to switch from a system-on-chip (SOC-based) process that performs ASR processing on the audio stream to a digital signal processor (DSP-based) process that performs ASR processing on the audio stream. When the ASR system uses the attenuated level of ASR processing on the audio stream captured by the turned-on microphone, the ASR system is configured to: generating a speech recognition result for audio data corresponding to a query spoken by a user; and providing the speech recognition results to the application to perform the query-specified action.
The details of one or more embodiments of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
Drawings
FIG. 1A is a schematic diagram of an exemplary speech environment for mitigating speech processing.
FIG. 1B is a schematic diagram of an exemplary time sequence of the speech environment of FIG. 1A.
Fig. 2A and 2B are schematic diagrams of exemplary interaction analysis to attenuate speech processing.
Fig. 3 is a flow chart of an exemplary arrangement of the operation of a method for attenuating speech processing.
FIG. 4 is a schematic diagram of an exemplary computing device that may be used to implement the systems and methods described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
Ideally, when talking with a digital assistant interface, a user should be able to communicate via a verbal request directed to the voice-enabled device they are running the digital assistant interface, as if the user were talking with another person. The digital assistant interface provides these verbal requests to an automatic speech recognizer to process and recognize the verbal requests so that actions can be performed. In practice, however, it is challenging for devices to always respond to these verbal requests, as the cost of running speech recognition on resource-constrained sound-enabled devices (such as smartphones or smartwatches) on a continuous basis is extremely high.
A session with a digital assistant interface is typically initiated by a user through a convenient interaction, such as the user speaking a fixed phrase (e.g., hotword/keyword/wake-up word) or using some predetermined gesture (e.g., lifting or squeezing a sound enabled device). However, once a session begins, it would be cumbersome and inconvenient for the user to ask the user to speak the same fixed phrase for each subsequent verbal request/query or use the same predetermined gesture. To alleviate this requirement, the microphone of the sound enabled device may be kept on for some predetermined amount of time immediately after the interaction to allow the microphone to capture the immediate subsequent query spoken by the user in a much more natural manner. However, there is a trade-off on how long the microphone should be kept on immediately after the interaction. For example, turning the microphone on too long will result in unnecessary power consumption to perform speech recognition and increase the likelihood of capturing unintended speech in a sound enabled device environment. However, on the other hand, prematurely turning off the microphone results in a poor user experience because the user is required to restart the session via a fixed phrase, gesture, or other potentially inconvenient manner.
Embodiments herein aim to activate a speech recognizer to perform speech recognition in response to an event by turning on the microphone of a sound enabled device and gradually weakening the responsiveness and processing power of the speech recognizer. More specifically, embodiments herein include attenuating a speech recognition processing level based on a likelihood of user interaction with a sound enabled device, such as a subsequent query after an initial query. The attenuation process by the speech recognizer over time may combine to improve the user experience by keeping the microphone on longer to capture subsequent speech directed to the sound enabled device, and to improve power consumption savings by allowing the speech recognizer to operate in different power modes, depending on the confidence level of the upcoming user interaction, as compared to making a binary decision to turn off the microphone and prevent further processing by the speech recognizer at some arbitrary point in time.
Referring to fig. 1A and 1B, in some implementations, a system 100 includes a user 10, the user 10 providing user interactions 12 to interact with a sound enabled device 110 (also referred to as a device 110 or user device 110). Here, the user interaction 12 is a spoken utterance 12, 12U corresponding to a command or query soliciting a response from the device 110 or causing the device 110 to perform a task specified by the query. In this sense, the user 10 may interact conversationally with the sound enabled device 110 to perform computing activities or to find answers to questions.
The apparatus 110 is configured to capture user interactions 12, such as speech, from one or more users 10 in a speech environment. The spoken utterance 12 of the user 10 may be captured by the device 110 and may correspond to a query or command to a digital assistant interface 120 executing on the device 110 to perform an operation/task. Device 110 may correspond to any computing device associated with user 10 and capable of receiving audio signals. Some examples of user devices 110 include, but are not limited to, mobile devices (e.g., mobile phones, tablets, notebooks, e-book readers, etc.), computers, wearable devices (e.g., smartwatches), music players, projection devices, smart appliances (e.g., smarttelevisions) and internet of things (IoT) devices, remote controls, smart speakers, etc. The apparatus 110 includes data processing hardware 112 and memory hardware 114, the memory hardware 114 in communication with the data processing hardware 112 and storing instructions that, when executed by the data processing hardware 112, cause the data processing hardware 112 to perform one or more operations related to speech processing.
The device 110 also includes an audio subsystem having an audio capturing device (e.g., an array of one or more microphones) 116 for capturing and converting audio data in a speech environment into electrical signals. When the device 110 implements an audio capture device 116 (also commonly referred to as a microphone 116) in the illustrated example, the audio capture device 116 need not be physically located on the device 110, but rather communicates with an audio subsystem (e.g., a peripheral of the device 110). For example, the device 110 may correspond to a vehicle infotainment system that utilizes an array of microphones throughout a vehicle.
A voice-enabled interface (e.g., a digital assistant interface) 120 may process queries or commands conveyed in the spoken utterance 12U captured by the device 110. The voice-enabled interface 120 (also referred to as interface 120 or assistant interface 120) generally facilitates receiving audio data 124 corresponding to the utterance 12U and coordinating voice processing of the audio data 124 or other activities originating from the utterance 12U to generate a response 122. The interface 120 may be executed on the data processing hardware 112 of the device 110. The interface 120 may direct the audio data 124 including the utterance 12U to various systems related to speech processing. For example, FIG. 1 shows interface 120 in communication with a speech recognition system 150. Here, the interface 120 receives the audio data 124 corresponding to the speech 12U and provides the audio data 124 to the speech recognition system 150. In some configurations, the interface 120 serves as an open communication channel between the microphone 116 of the device 110 and the speech recognition system 150. In other words, microphone 116 captures the utterance 12U in the audio stream 16 and interface 120 communicates audio data 124 corresponding to the utterance 12U converted from the audio stream 16 to the speech recognition system 150 for processing. More specifically, the speech recognition system 150 processes the audio data 124 to generate a transcription 152 for the utterance 12U, and may perform semantic interpretation of the transcription 152 to identify appropriate actions to perform. The interface 120 for interacting with the user 10 at the device 110 may be any type of program or application configured to perform the functions of the interface 120. For example, interface 120 is an Application Programming Interface (API) that interfaces with other programs carried on device 110 or in communication with device 110.
With specific reference to the example of fig. 1A, the first sound 12u,12ua of the user 10 states "hey, computer, who is the french president? "here, the first utterance 12U includes a hotword 14, which, when detected in the audio data 124, triggers the interface 120 to turn on the microphone 116 and then to capture a query" who is the french president? "corresponding audio data is forwarded to the speech recognition system 150 for processing. That is, the device 110 may be in a sleep or dormant state and operate a hotword detector to detect whether hotwords 14 are present in the audio stream 14. Because it acts as a call phrase, hotword 14 triggers device 110 to wake up and initiate speech recognition on hotword 14 and/or one or more terms following hotword 14 when detected by the hotword detector. The hotword detector may be a neural network-based model configured to detect acoustic features indicative of hotwords without performing speech recognition or semantic analysis.
In response to the hotword detector detecting the hotword 14 in the audio stream 16, the interface 120 forwards the audio data 124 corresponding to this utterance 12Ua to the speech recognition system 150, and the speech recognition system 150 performs speech recognition on the audio data 124 to generate a speech recognition result (e.g., transcription) 152 for the utterance 12 Ua. The speech recognition system 150 and/or the interface 120 performs semantic interpretation on the speech recognition results 152 to determine that the utterance 12Ua corresponds to a search query for a french president identity. Here, the interface 120 may submit the transcription 152 to the search engine 160 for the query "who is the french president? "search engine 160 searches and returns search results 162" angstrom Ma Niuai · Ma Kelong ". The interface 120 receives this search result 162 "angstrom Ma Niuai · Ma Kelong" from the search engine 160 and accordingly communicates "angstrom Ma Niuai · Ma Kelong" to the user 10 as a response 122 to the query of the first utterance 12 Ua. In some examples, response 122 includes synthesized speech audibly output from device 110.
To perform the functions of the assistant interface 120, the interface 120 may be configured to control one or more peripherals (e.g., one or more components of an audio subsystem) of the device 110. In some examples, the interface 120 controls the microphone 116 such that provision is made to actively receive audio data 124 when the microphone 116 is on, or for some speech processing purpose, or to not receive audio data 124 when the microphone 116 is off, or to receive a limited amount of audio data 124 for speech processing purposes. Here, whether the microphone 116 is "on" or "off may refer to the interface 120 passing the audio data 124 received at the microphone 116 to the speech recognition system 150 such that the interface 120 has an open communication channel with the speech recognition system 150 to enable speech recognition for any of the utterances 12U included in the audio stream 16 received at the microphone 116, or such that the interface 120 has a closed communication channel with the speech recognition system 150 to disable the speech recognition system 150 from performing speech recognition on the audio stream 16. In some implementations, the interface 120 specifies or instructs whether such a channel is open or closed based on whether the interface 120 receives or has received the interaction 12 and/or the interaction 12 with the trigger 14. For example, when the interface 120 receives an interaction 12 (e.g., a spoken utterance 12U with a hotword) with the trigger 14, the interface 120 instructs the microphone 116 to turn on and forward audio data 124 converted from audio captured by the microphone 116 to the speech recognition system 150. After the utterance is complete, the interface 120 may instruct the microphone 116 to turn off to prevent the speech recognition system 150 from processing the additional audio data 124.
In some implementations, the device 110 communicates with a remote system 140 via a network 130. Remote system 140 may include remote resources 142, such as remote data processing hardware 144 (e.g., a remote server or CPU) and/or remote memory hardware 146 (e.g., a remote database or other storage hardware). The device 110 may utilize the remote resources 142 to perform various functions related to speech processing. For example, search engine 160 may reside on remote system 140 and/or a portion of the functionality of speech recognition system 150 may reside on remote system 140. In one example, the speech recognition system 150 may reside on the device 110 for performing on-device Automatic Speech Recognition (ASR). In another example, the speech recognition system 150 resides on a remote system to provide server-side ASR. In another example, the functionality of the speech recognition system 150 is separate in the device 110 and the server 140. For example, FIG. 1A illustrates a speech recognition system 150 and a search engine 160 with dashed boxes to indicate that these components may reside on the device 110 or server side (i.e., on the remote system 140).
In some configurations, different types of speech recognition models reside at different locations (e.g., on the device or remotely) depending on the model. Similarly, end-to-end or stream-based speech recognition models may reside on the device 110 due to their space-efficient dimensions, while the larger, more traditional speech recognition models built from multiple models, such as Acoustic Models (AM), pronunciation Models (PM), and Language Models (LM), are server-based models that reside on the remote system 140 rather than the device. In other words, speech recognition may reside on the device (i.e., user side) or remotely (i.e., server side) depending on the desired level of speech recognition and/or the desired speed at which speech recognition is performed.
When the user 10 is engaged in the interface 120 by conversational means, it may be quite inconvenient for the user 10 to repeatedly speak the same call phrase (e.g., hotword) 14 for each interaction 12 in which the user 10 desires to obtain some feedback (e.g., response 122) from the interface 120. In other words, it would be very cumbersome and inconvenient to require the user to speak the same hot word into each of the multiple sequential queries of the user 10. Unfortunately, however, it is also a waste of computational resources and it would be computationally expensive for the device 110 to continue to run speech recognition on any audio data 124 received at the microphone 116.
To address the inconvenience of requiring the user 10 to repeatedly speak the hotword 14 each time the user 10 wants to communicate a new query 14 while the user 10 is actively engaged in a session (or interactive session 12) with the interface 120, the interface 120 may allow the user 10 to provide a subsequent query after the interface 120 responds to the previous query without requiring the user 10 to speak the hotword 14. That is, the response 122 to the query may act as an indication of the microphone trigger event 202 (fig. 1B) indicating a possible user interaction 12 with the sound enabled device 110 through speech, causing the device 110 to instruct the microphone 116 to turn on or remain on for capturing the audio stream 16 and provide the captured audio stream 16 to the speech recognition system 150 to perform a speech recognition process on the audio stream 16. Here, the microphone 116 may be turned on to accept a query from the user 10 without requiring the user 10 to first speak the hotword 14 again to trigger the microphone 116 to be turned on to accept the query.
Microphone trigger event 202 (also referred to as trigger event 202) generally refers to the occurrence of an event that indicates that user 10 may interact with device 110 via voice, and thus requires activation of microphone 116 to capture any voice for voice processing. Here, because the trigger event 202 indicates a possible user interaction 12, the trigger event 202 may range from a gesture to a recognizable user feature (e.g., a behavioral pattern), to any action of the user 10 that the device 110 may distinguish as a potential interaction 12. For example, the user 10 may have a routine during the work week in which the user 10 queries the device 110 for weather and whether there are any events on the user's calendar when the user 10 enters the kitchen. Because of this pattern of behavior, the device 110 may recognize that the user 10 is entering the kitchen at a particular time (e.g., hearing movement in the direction of the kitchen entrance channel) and treat the user's 10 action entering the kitchen in the morning as a trigger event 202 of a possible user interaction 12. For gestures, the triggering event 202 may be an interaction 12 in which the user 10 lifts the device 110, squeezes the device 110, presses a button on the device 110, taps a screen of the device 110, moves a hand in a predetermined manner, or any other type of pre-coding Cheng Shoushi to indicate that the user 10 may be intent to engage in a session with the assistant interface 120. Another example of a triggering event 202 is when the device 110 (e.g., interface 120) communicates a response 122 to a query from the user 10. In other words, when interface 120 forwards response 122 to user 10, response 122 acts as a communicative interaction with user 10 on behalf of device 110; this means that a subsequent query spoken by the user 10 may occur after receiving the response 122 or simply because the user 10 is currently in a session with the device 110. Based on this likelihood, the response 122 may be considered a trigger event 202 such that the microphone 116 is turned on or remains turned on to capture and allow voice processing of subsequent queries spoken to the user 10 after the device 110 outputs the response 122, without requiring the user 10 to add a hotword 14 to the subsequent query.
To implement the microphone trigger event 202 and provide a session between the device 110 and the user 10 without compromising processing resources, the device 110 deploys an interaction analyzer 200 (also referred to as analyzer 200), the analyzer 200 identifying that the user 10 is addressing the assistant interface 120 while also maintaining speech recognition throughout the session with the user 10. In other words, the analyzer 200 may provide a termination function by identifying when a session between the user 10 and the interface 120 begins and when it is assumed that the session has ended best to terminate the interaction 12 and deactivate speech recognition. Furthermore, in addition to termination, analyzer 200 may also modify the speech recognition process by reducing the processing level 222 (fig. 2A and 2B) of speech recognition (i.e., using speech recognition system 150) depending on the nature of one or more interactions of user 10. Specifically, the analyzer 200 may be capable of instructing the speech recognition system 150 to use the attenuated level 222 of speech recognition on the audio stream 16 captured by the microphone 116. For example, when the device 110 instructs the microphone 116 to turn on or remain on for an open microphone duration window to capture and provide the audio stream 16 to the speech recognition system 150 in response to receiving an indication of a microphone trigger event 202, the analyzer 200 may attenuate the level 222 of speech processing performed by the speech recognition system 150 on the audio stream 16 as a function of the open microphone duration window 212. Here, analyzer 200 operates in conjunction with interface 120 to control speech recognition and/or speech-related processing.
Referring to fig. 1B, when device 110 (e.g., interface 120) receives an indication of a trigger event 202, interface 120 instructs microphone 116 to turn on or hold and passes audio data 124 associated with audio stream 16 captured by microphone 116 to speech recognition system 150 for processing. Here, when the interface 120 instructs the microphone 116 to turn on, the analyzer 200 may be configured to designate that the microphone 116 remains on for some open microphone duration window 212 after starting through the trigger event 202. For example, opening the microphone duration window 212 specifies a period of time during which the interface 120 communicates the audio stream 16 of audio data 124 captured by the microphone 116 to the speech recognition system 150. Here, the microphone duration window 212 refers to a set duration that begins when the trigger event 202 is received and stops after the set duration (e.g., experiences a microphone off event). In some examples, the analyzer 200 may instruct the interface 120 to delay the microphone duration window 212 or refresh (i.e., update) the microphone duration window 212 during the reception of another trigger event 202 (e.g., a subsequent trigger event 202) during the microphone duration window 212. For illustration, fig. 1A and 1B show that user 10 generates the statement "hey, computer, who is the french president? "is the first utterance 12Ua and the second utterance 12u,12ub" he older? "French president in response 122" is a subsequent question of Angstrom Ma Niuai. Ma Kelong ". Referring specifically to fig. 1B, the hotword 14 "hey, computer" corresponds to the subsequent portion of the starting utterance, "who is the french president? "fixed phrase of speech recognition. When the interface 120 responds with "angstrom Ma Niuai · Ma Kelong," the analyzer 200 establishes the response 122 as the microphone trigger event 202 starting the first microphone duration window 212, 212 a. Here, window 212 may have a duration defined by a start point 214 and an end point 216, where at the time of end point 216, microphone duration window 212 expires and interface 120 and/or analyzer 200 performs a microphone off event. In this example, however, before the initially specified end points 216a,216 of the first microphone duration window 212a, the user 10 asks for a subsequent question "what is he older? ". Due to the subsequent problem of the second utterance 12Ub (i.e., another user interaction 12), the interface 120 generates a second response 122, 122b stating that the age of angstrom Ma Niuai · Ma Kelong is "47". Similar to the first response 122, 122a "angstrom Ma Niuai · Ma Kelong", the second response 122, 122B is a second trigger event 202, 202B that starts a new microphone duration window 212, 212B as the second microphone duration window 212, even though the first microphone duration window 212 may expire, and may not expire (e.g., fig. 1B shows that the first window 212a expires before the second response 122B). This may allow for extending the hang window 212 or allowing the hang window 212 to remain open when the interface 120 initiates a new microphone duration window 212 while the microphone duration window 212 is still on hold. In some implementations, the trigger event 202 during the duration window 212 does not necessarily completely update the duration window 212 (i.e., resumes the duration window) while the duration window 212 is still suspended, but rather extends the suspended duration window 212 by some specified amount of time. For example, if the duration window 212 is 10 seconds and the duration window 212 is currently pending, then the triggering event 202 during this pending duration window 212 would extend the pending window 212 an additional 5 seconds instead of a new additional full 10 seconds. In fig. 1B, the first window 212a expires before the second response 122B, such that the second trigger event 202B starts a completely new duration window 212, 212B at the second start point 214, 214B, which duration window 212, 212B will end at the second end point 216, 216B (unless, for example, one or more additional trigger events 202 occur).
With continued reference to fig. 1B, the analyzer 200 is configured to additionally generate the fade state 204 during some portion of the window 212. For example, by transitioning to the fade state 204, the processing level 222 for speech recognition may be modified (e.g., reduced by some amount) during the window 212. In other words, the level 222 of speech processing performed on the audio data 124 by the speech recognition system 150 may be based on the functionality of the window 212. By basing the level 222 of speech processing (e.g., for speech recognition) on the window 212, the amount of speech processing performed on the audio stream 16 of the audio data 124 can be a function of time. This means that over time, the session between the interface 120 and the user 10 appears unlikely to be ongoing (e.g., the audio data 124 does not include the user's voice or voice not directed to the device 110), the analyzer 200 recognizes this reduced likelihood and thus reduces voice recognition. For example, fig. 1B shows that during the second half of the first microphone duration window 212a, speech processing transitions to the fade state 204, wherein the level 222 of speech processing is reduced. The method may allow analyzer 200 and/or interface 120 to reduce the amount of computing resources used to perform speech processing on device 110. Similarly, in the example of fig. 1B, when the analyzer 200 has transitioned speech recognition to the attenuated state 204, the user 10 generates a subsequent question "how old he is? The analyzer 200 also determines that the fade state 204 of the second window 212b should occur faster because speech recognition is already in the fade state 204 in the first window 212 and the user 10 is already unlikely to be performing the second interaction 12Ub; thus, the analyzer 200 determines that the fade state 204 should occur in the second duration window 212b for a longer period of time, for example, approximately 90% of the second duration window 212 b. By utilizing analyzer 200, device 110 and/or interface 120 strives to balance between maintaining the ability to perform speech recognition without hotword 14, and at the same time also strives to avoid actively performing speech recognition when user 10 is unlikely to intend to interact further with assistant interface 120.
Referring to fig. 2A and 2B, the analyzer 200 generally includes a window generator 210 and a reducer 220. Window generator 210 (also referred to as generator 210) is configured to generate an open microphone duration window 212 that specifies the duration of microphone 116 to be opened for capturing and providing audio stream 16 to speech recognition system 150 for processing. Each microphone duration window 212 includes a start point 214 and an end point 216, the start point 214 specifying a time at which the audio stream 16 received at the microphone 116 begins to be sent to the speech recognition system 150 for processing, the end point 216 specifying a time at which the microphone duration window 212 is open to end and after which the audio stream 16 is no longer passed to the speech recognition system 150 for processing. When it is determined that the interface 120 receives the trigger event 202 from the user 10, the window generator 210 initially generates an open microphone duration window 212.
In some implementations, window generator 210 is configured to generate windows 212 of different sizes (i.e., windows 212 having different time lengths) based on the configuration of analyzer 200 or intelligently based on conditions occurring at microphone 116. For example, an administrator of analyzer 200 sets a default duration for windows 212 generated by window generator 210 (e.g., each window 212 is 10 seconds in length). In contrast, the generator 210 may identify the behavior patterns of the user 10 or aspects/features of the utterance 12U of the user 10 and intelligently generate a window 212 having a size that matches or corresponds to those identified features. For example, it may be common for a particular user 10 to participate in multiple questions after each time the particular user 10 generates a trigger 14 to initiate a session with the interface 120. Here, the speech processing system associated with the apparatus 110 can identify the identity of the user 10 and based on the identification, the generator 210 generates a window 212 having a size corresponding to or catering to the frequency of interaction of the user 10 during the interaction session (i.e., the session with the interface 120). For example, instead of the generator 210 generating a default window 212 having a duration of 5 seconds, the generator 210 generates a window 212 having a duration of 10 seconds, as the user 10 submitting the hotword 14 tends to produce a higher frequency of interactions 12 with the interface 120. On the other hand, if the user 10 submitting the hotword 14 tends to query only a single query whenever he/she interacts with the interface 120, the generator 210 may receive this behavioral information about the user 10 and shorten the default window 212 of 5 seconds to a custom window 212 of 3 seconds.
The generator 210 may also generate a custom open microphone duration window 212 based on aspects or features of the utterance 12U or the response 122 to the utterance 12U. As a basic example, the user 10 speaks an utterance 12U stating: "he, computer, play new Smashing Pumpkins album Cyr". ". However, when the interface 120 attempts to respond to this command, the interface 120 determines that a new Smashing Pumpkins album Cyr will be released at a later time of the month and provides a response 122 indicating that the album is not currently available. The response 122 may further ask the user 10 if he wants to hear other content. Here, after the response 122 of the interface 120, the generator 210 may generate a larger window 212 (i.e., a longer duration window 212), or may automatically lengthen the initially generated window 212 based on the fact that: when the interface 120 generates a response 122 that the requested album is not available, the interface 120 and/or the device 110 determines that there is a high likelihood that a subsequent interaction 12 of the user 10 exists. In other words, interface 120 and/or device 110 intelligently recognizes that it is likely that user 10 will submit another music request after response 122 due to the command results of user 10.
As shown in fig. 1B, the generator 210 not only generates the window 212 when the trigger event 202 is initially received (e.g., when the interface 120 communicates the response 122), but may also generate a new window 212 or extend the currently open window 212 when the trigger event 202 is received through the interface 120 during the opening of the window 212. By generating a new window 212 when receiving a trigger event 202 during opening of the window 212 or extending the opening window 212, the generator 210 is configured to continue to keep the window 212 open while a session between the user 10 and the interface 120 is ongoing.
In some examples, such as fig. 2B, when generator 210 extends window 212 in an ongoing session or generates a new window 212, generator 210 is configured to shorten the duration of window 212. For example, the generator 210 is configured to generate a subsequent window 212 of shorter duration after each subsequent interaction 12 of the user 10. This approach may explain the fact that: the second or subsequent interaction 12 has a first occurrence probability after the initial interaction 12, but the probability of additional interactions 12 occurring thereafter is reduced. For example, subsequent queries in the second utterance 12Ub after the initial query in the first utterance 12Ua may occur approximately 20% of the time, but during the interaction session between the user 10 and the interface 120, the third utterance 12U after the subsequent query in the second utterance 12Ub occurs only approximately 5% of the time. Based on this pattern, the generator 210 may reduce the size of the window 212 or as a function of this possibility of another trigger event 202 (i.e., a possible interaction 12) occurring, how long the open window 212 is prolonged. For example, fig. 2B illustrates three trigger events 202, 202a-c, where each trigger event 202 corresponds to a subsequent response 123 (e.g., three responses 122 a-c) from the interface 120, such that the generator 210 generates a first window 212a for a first trigger event 202a, followed by a second window 212B for a second trigger window 202B that occurs after the first trigger window 202a, followed by a third window 212c for a third trigger event 202c that occurs after the second trigger event 202B. In this example, the generator 210 shortens the generated window 212 such that the third window 212c has a shorter duration than the second window 212b, and the second window 212b has a shorter duration than the first window 212 a. Additionally or alternatively, the generator 210 may analyze the audio data 124 of the audio stream 16 to determine whether the sound activity level in the audio data 124 provides any indication that the trigger event 202 is likely to occur. With this information, the generator 210 may modify the size of the currently open window 212 or modify the size of any subsequently generated/extended window 212.
The reducer 220 is configured to specify a processing level 222 of speech recognition that should be performed on the audio stream 16 of the audio data 124. The processing level 222 may be based on a number of variables including, but not limited to, the type of speech recognition model that performs speech recognition for the speech recognition system 150, the location where the speech recognition occurs, the speech recognition parameters used to perform the speech recognition, whether the speech model is designated to operate with full capability or some lesser degree of capability, and the like. The processing level 222 may generally represent the amount of computing resources (e.g., local resources or remote resources such as data processing hardware and memory hardware) dedicated or consumed by speech processing (such as speech recognition) at any given time. This means that the first processing level 222 in the first state is lower than the second processing level 222 in the second state, e.g. the amount of computational resources or computational power dedicated to speech processing in the first state is smaller than in the second state.
In some examples, when speech recognition occurs at a reduced processing level 222 (e.g., when compared to maximum processing capacity), the reduced processing level 222 may act as a first channel to recognize speech, but then result in a second channel having a higher processing level 222 than the first channel. For example, when the speech recognition system 150 is operating at the reduced processing level 222, the speech recognition system 150 identifies that the audio data 124 contains the utterance 12U to command or query low-confidence speech recognition results (e.g., low-confidence hypotheses) of the interface 120. Because of this low confidence speech recognition result, the reducer 220 may increase the processing level 222 such that it can be determined at a higher processing level 222 whether the low confidence speech recognition result actually corresponds to a higher confidence speech recognition result of the audio data 124 containing the utterance 12U to command or query the interface 120. In some examples, the low confidence speech recognition result is a speech recognition result that fails to meet a confidence threshold during speech recognition. In other words, the reducer 220 may not only be based on the variables described above, but may also change the processing level 222 based on the results obtained during speech recognition at a particular processing level 222.
In some implementations, the reducer 220 designates the processing level 222 as a function of the window 212. That is, when there is an open microphone duration window 212, the reducer 220 can reduce the processing level 222 for speech recognition. For example, when window 212 corresponds to a duration of 10 seconds (e.g., as shown in fig. 2A), during the first 5 seconds of window 212, reducer 220 instructs speech recognition system 150 to operate at full capability/processing (e.g., first processing level 222, 222A) for speech recognition. After this 5 seconds has occurred with full processing, the speech processing transitions to the fade state 204, wherein the processing level 222 is reduced to some degree less than full processing. By way of example, from 5 seconds to 7 seconds in the duration of window 212, reducer 220 instructs speech recognition system 150 to execute at a second processing level 222, 222b corresponding to 50% of the full processing of speech recognition system 150. Then, after the 7 th second of duration and until the duration of window 212 ends, reducer 220 instructs speech recognition system 150 to execute at a third processing level 222, 222c corresponding to 25% of the full processing of speech recognition system 150. Thus, the reducer 220 controls the processing level 222 of the speech recognition system 150 during the opening of the microphone duration window 212.
In some implementations, generator 210 and reducer 220 work together such that the duration of window 212 depends on fade state 204. The generator 210 does not necessarily generate the endpoint 216 at a specified time, but rather allows the reducer 220 to reduce the processing level 222 to a level that has the same effect as turning off the microphone 116. For example, after a third processing level 222c corresponding to 25% of the full processing of speech recognition system 150, reducer 220 then turns off microphone 116 because another 25% reduction in processing reduces processing level 222 of speech recognition system 150 to 0% of full processing (i.e., no processing or "off"). This particular example is a layered approach that steps down the processing level 222 in discrete increments, but other types of attenuation may result in the microphone 116 being turned off. For example, the processing level 222 may be reduced in any dotted lines during the opening of the microphone duration window 212. By allowing the reducer 220 to decrease the processing level 222 until the microphone 116 is turned off, the technique may facilitate continuous attenuation of speech processing (e.g., when no interaction 12 occurs and the microphone 116 is turned on).
In some configurations, such as FIG. 2B, the processing level 222 within the window 212 is based on the time at which the interface 120 received the last trigger event 202 of the user 10. In these configurations, the reducer 220 can determine whether the time period 224 from the last trigger event 202 received by the interface 120 to the current time meets the time threshold 226. When the time period 224 meets the time threshold 226 (i.e., no interactions 12 have occurred for a threshold amount of time), the reducer 220 may generate the processing level 222 at the current time. For example, if interface 120 sends response 122 to utterance 12U of user 10, window 212 begins when interface 120 communicates response 122. When the time threshold 226 is set to 5 seconds, the reducer 220 determines whether 5 seconds have elapsed since the interface 120 passed the response 122. When the reducer 220 determines that 5 seconds have elapsed, the reducer 220 may then specify a certain level of processing 222 for voice recognition initiation, for example, at a certain specific time after or at the present time when the reducer 220 determines that 5 seconds have elapsed.
In some configurations, the processing level 222 is changed by adjusting one or more parameters of the speech recognition system 150. In one method of changing the processing level 222 of speech recognition at the speech recognition system 150, the speech recognition is changed with respect to its location of occurrence. The location of speech recognition may be changed from the server side of the occurrence (i.e., remote) to on-device (i.e., local). In other words, the first processing level 222a corresponds to remote speech recognition using a server-based speech recognition model, while the second processing level 222b corresponds to local speech recognition occurring on the device. When the speech recognition system 150 is carried "on-device," the device 110 receives the audio data 124 and uses its processor(s) (e.g., the data processing hardware 112 and the memory hardware 114) to perform the functions of the speech recognition system 150. Speech recognition using a server-based model may be considered to have a greater processing level 222 than an on-device speech recognition model because the server-based model may utilize a greater amount of remote processing resources (e.g., along with other costs such as bandwidth and transmission overhead). For greater throughput, the server-based model may potentially be larger in size than the on-device model and/or perform decoding using a larger search graph than the on-device model. For example, server-based speech recognition models may utilize multiple larger models (e.g., acoustic models (PMs), pronunciation Models (PMs), and language models (PMs)), which may be specifically trained for dedicated speech recognition purposes, while on-device models typically must incorporate these different models into smaller packages to operate efficiently and arrange efficiently on the limited processing resources of device 110. Thus, when the reducer 220 reduces the processing level 222 to the reduced state 204, the reducer 220 may change speech recognition from occurring remotely using the server-based model to occurring locally using the on-device model.
In some cases, there may be more than one device 110 in the vicinity of the user 110 generating the interaction 12, such as the spoken utterance 12U. When multiple devices 110 are located in proximity to the user 10, each device 110 is capable of performing some aspect of speech recognition. Because multiple devices 110 performing speech recognition on the same spoken utterance 12U may be duplicated, the reducer 220 may reduce the processing level 222 of a particular device 110 by turning off the microphone 116 because that device 110 knows that another device 110 is processing or is configured to process speech recognition. To illustrate, when the user 10 has a mobile device and a smart watch, both devices 110 are capable of performing speech recognition. Here, the reducer 220 may turn off the microphone 116 for speech recognition on the mobile device to conserve processing resources of the mobile device for a wide range of other computing tasks that the mobile device may need to perform. For example, a user may prefer to conserve battery power for his or her mobile device rather than his or her smart watch. In some examples, when there are multiple devices, the reducer 220 may attempt to determine characteristics of each device 110 (e.g., current processor consumption, current battery life, etc.) to identify which device 110 is best suited to have its microphone 116 remaining on and which device(s) are best suited to have its microphone 116 turned off.
In addition to the processing level differences between the remote speech recognition system 150 and the on-device speech recognition system 150, different versions of the on-device speech recognition model or server-side speech recognition model may exist. For different versions, the reducer 220 may change the processing level 222 by changing the model or version of the model used for speech recognition. Broadly, the model may have a large version of the high processing level 222, a medium version of the medium processing level 222, and a small version of the low processing level 222. In this sense, if the reducer 220 wants to reduce the processing level 222, the reducer 220 can transition speech recognition from being performed on a first version of the model to a second version of the model having a lower processing level 222 than the first version of the model. In addition to changes between versions of the on-device model or server-side model, the reducer 220 may also change from one version of the server-side model to a particular version of the on-device model. By having models and versions of these models, the reducer 220 has a greater number of processing levels 222 for use thereby weakening the speech recognition processing levels 222 during opening of the microphone window 212.
In some implementations, versions of the on-device speech recognition model have different processing requirements so that the reducer 220 can assign such versions to different processing levels 222. Some examples of on-device speech recognition models include sequence-to-sequence models, such as a cyclic neural network transducer (RNN-T) model, a listen-play-spelling (LAS) model, a neural transducer model, a monotonically aligned model, a cyclic neural alignment (RNA) model, and the like. There may also be on-device models that are a mix of these models, such as a two-channel model that combines the RNN-T model and the LAS model. For different versions of the on-device models, the reducer 220 may rank or identify the processing requirements of each of the versions, thereby generating different processing levels 222. For example, the two-channel model includes a first channel of the RNN-T network followed by a second channel of the LAS network. Because this two-channel model includes multiple networks, the reducer 220 can designate the two-channel model as a large on-device model with a higher processing level 222 for speech recognition. To reduce the processing level 222 of speech recognition from the processing level 222 of the two-channel model, the reducer 220 may change from the two-channel model to the LAS model. Here, the LAS model is an attention-based model that performs attention during its decoding process to generate a sequence of characters that form the transcript 152. In general, attention-based approaches tend to be more computationally intensive to focus attention on specific features of a given speech input. In contrast, the RNN-T model does not employ an attention mechanism and also performs its beam search through a single neural network rather than a large decoder graph; thus, the RNN-T model may be more compact and less computationally expensive than the LAS model. For these reasons, the reducer 220 may change between a two-channel model, a LAS model, and an RNN-T model of speech recognition, thereby reducing the processing level 222. That is, by using the RNN-T network as the first channel and the LAS network as the second channel to re-evaluate the first channel, the two-channel model has a higher processing level 222 than either the LAS model or the RNN-T model alone, while the LAS model is an attention-based model with a higher processing level 222 than the RNN-T model. By identifying the voice recognition processing requirements of different versions of the on-device voice recognition model, the reducer 220 can attenuate the voice processing (or increase the processing) by switching among any of the different versions of the on-device model. In addition, attenuating speech processing by the reducer 220 has a number of potential processing level hierarchies when the reducer 220 combines processing level options of the on-device model with processing level options of the server-side model.
To further expand the potential number of processing level hierarchies, reducer 220 may be configured to modify the speech processing steps or speech processing parameters of a given model to change the processing level of that particular model. For example, a particular model includes one or more neural network layers (e.g., a recurrent neural network with Long Short Term Memory (LSTM)). In some examples, the output layer may receive information from past states (backward) and future states (forward) to generate its output. When a layer receives backward and forward states, the layer is considered bi-directional. In some configurations, the reducer 220 is configured to modify the processing steps of the model to change the speech recognition model from bi-directional operation (i.e., forward and backward) to simple uni-directional operation (e.g., forward). Additionally or alternatively, the reducer 220 may reduce the number of neural network layers that the model uses to perform speech recognition, thereby changing the processing level 222 of a particular model.
The reducer 220 may also reduce the processing level 222 for the speech recognition model by changing the beam search parameters (or other pruning/search mode parameters) for the model. In general, the beam search includes a beam size or beam width parameter that specifies how many best potential solutions (e.g., hypotheses or candidates) to evaluate to generate speech recognition results. Thus, the beam search process performs a pruning of potential solutions to reduce the number of solutions evaluated to form speech recognition results. That is, the beam search process may limit the computations involved by using a limited number of active beams to search the most likely word sequence spoken in the utterance 12U to produce a speech recognition result (e.g., transcription 152 of the utterance 12U). Here, the reducer 220 may adjust the beam size to reduce the number of best potential solutions to be evaluated, which correspondingly reduces the amount of computation involved in the beam search process. For example, the reducer 220 changes the beam size from 5 to 2 so that the speech recognition model evaluates 2 best candidates instead of 5 best candidates.
In some examples, the reducer 220 performs quantization or sparsification on one or more parameters of the speech recognition model, thereby generating a lower processing level 222 for the model. In generating speech recognition results, speech recognition models typically generate a large number of weights. For example, the speech recognition model weights different speech parameters and/or speech-related features to output speech recognition results (e.g., transcription 152). Because of these large numbers of weights, the reducer 220 may discrete the values of these weights by performing quantization. For example, the quantization process converts floating point weights into weights expressed as fixed point integers. While this quantization process loses some information or quality, it allows resources to process these quantization parameters to use less memory and may allow more efficient operations (e.g., multiplication) to be performed on some hardware.
In a similar respect, the purpose of the sparsification is also to reduce the throughput of the running model. Here, the thinning refers to a process of removing redundant parameters or features in the speech recognition model, thereby focusing on more relevant features. For example, in determining the speech recognition result, the speech model may determine the likelihood of all speech-related features (e.g., characters, symbols, or words), even if not all speech-related features are related to a certain speech input. By using sparsification, the model may expend less computational resources by determining the likelihood of speech-related features related to the input, rather than the likelihood of all speech-related features; allowing the sparsification process to ignore speech-related features that are not related to the input.
Optionally, the reducer 220 may generate a lower processing level 222 for speech recognition by determining the context of the interaction 12 (e.g., spoken utterance 12U) that originally generated or caused the opening of the microphone duration window 212. Once the reducer 220 identifies the context, the reducer 220 may use the context to reduce the processing level 222 for the speech recognition system 150 by biasing the speech recognition results based on the context. In some implementations, the reducer 220 biases the speech recognition results based on context by limiting the speech recognition system 150 to words that are relevant to the context. As an example, the user 10 may query the interface 120, "how to tie the prussian knot? ". According to this problem, the reducer 220 determines that the Prussian junction is mainly used for mountain climbing or rock climbing. In other words, reducer 220 identifies that the context of interaction 12 is mountain climbing. In this example, the reducer 220 limits the speech recognition output to the vocabulary associated with mountain climbing as the reducer 220 continues to decrease the processing level 222 for speech recognition. Thus, if the user 10 subsequently asks a subsequent question about the abacavia mountain, the speech recognition system 150 may generate possible speech recognition results that include the terms "Application" and "Appalachian", but because "Appalachian" is associated with the context of "mountain climbing", the reducer 220 ensures that the speech recognition system 150 is biased towards the term "Appalachian". For example, the reducer 220 instructs the speech recognition system 150 to increase the likelihood score of potential results related to the identified context (e.g., mountain climbing). In other words, the speech recognition system increases the likelihood score of having potential results for mountain climbing related vocabulary.
When the speech recognition system 150 is operating on the device 110, the speech recognition system 150 may typically perform speech recognition using a system-on-chip (SOC-based) processor. A system on a chip (SOC) processor refers to a general purpose processor, a signal processor, and additional peripherals. By instructing the speech recognition system 150 to change from SOC-based processing to a Digital Signal Processor (DSP), the reducer 220 may generate a reduced processing level 222 when the SOC-based processing is used for speech recognition. Here, this change results in a lower processing level 222 because the DSP tends to consume less power and memory than SOC-based processing.
When the reducer 220 fades the processing level 222 for speech recognition, it may be advantageous to provide the user 10 with an indication to some extent that the fade is occurring. To provide such an indication, a Graphical User Interface (GUI) associated with the apparatus 110 may include a graphical indicator to indicate the current processing level 222 of the speech recognition system 150. In some examples, the graphical indicator has a brightness level configured to fade in proportion to the fade of the processing level 222. For example, the screen of device 110 includes a GUI that displays a red microphone dot to instruct microphone 116 to turn on (i.e., listen to interaction 12), with the red microphone dot gradually fading as reducer 220 fades processing level 222 for speech recognition. Here, when microphone 116 is off, the red microphone will go out. Additionally or alternatively, the indicator indicating the degree of attenuation of the microphone 116 being on and/or the processing level 222 for speech recognition may be a hardware indicator, such as a light (e.g., a Light Emitting Diode (LED)) on the device 110. For example, as the processing level 222 decreases, the LEDs dim or flash (e.g., slower) at a decreasing rate until they go off until the microphone 116 is turned off.
Fig. 3 is a flow chart of an exemplary arrangement of the operation of a method 300 for attenuating speech processing. In operation 302, the method 300 receives an indication of a microphone trigger event 202 at the sound enabled device 110 indicating a possible user interaction with the sound enabled device 110 through speech, wherein the sound enabled device 110 has a microphone 116, the microphone 116 being configured to capture speech when turned on for recognition by an Automatic Speech Recognition (ASR) system 150. Operation 304 includes two sub-operations 304, 304a-b that occur in response to receiving an indication of microphone trigger event 202. In operation 304a, the method 300 instructs the microphone 116 to turn on or remain on for the on microphone duration window 212 to capture the audio stream 16 in the environment of the sound enabled device 110. In operation 304b, the method 300 provides the audio stream 16 captured by the open microphone 116 to the ASR system 150 to perform ASR processing on the audio stream 16. Operation 306 includes two sub-operations 306, 306a-b that occur when the ASR system 150 performs ASR processing on the audio stream 16 captured by the open microphone 116. At operation 306a, the method 300 weakens the level of ASR processing performed on the audio stream 16 by the ASR system 150 based on the function of opening the microphone duration window 212. At operation 306b, the method 300 instructs the ASR system 150 to use the attenuated level 204, 222 of ASR processing on the audio stream 16 captured by the turned-on microphone 116.
FIG. 4 is a schematic diagram of an exemplary computing device 400 that may be used to implement the systems (e.g., device 110, interface 120, remote system 140, speech recognition system 150, search engine 160, and/or analyzer 200) and methods (e.g., method 300) described herein. Computing device 400 is intended to represent various forms of digital computers, such as notebook computers, desktop computers, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown herein, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed herein.
Computing device 400 includes a processor 410, a memory 420, a storage device 430, a high-speed interface/controller 440 connected to memory 420 and high-speed expansion ports 450, and a low-speed interface/controller 460 connected to low-speed bus 470 and storage device 430. Each of the components 410, 420, 430, 440, 450, and 460 are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 410 may process instructions for execution within the computing device 400, including instructions stored in the memory 420 or on the storage device 430, to display graphical information for a Graphical User Interface (GUI) on an external input/output device, such as a display 480 coupled to the high-speed interface 440. In other embodiments, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Furthermore, multiple computing devices 400 may be connected, with each device providing a portion of the necessary operations (e.g., as a server bank, a blade server bank, or a multiprocessor system).
Memory 420 stores information non-temporarily within computing device 400. Memory 420 may be a computer-readable medium, volatile memory unit(s), or nonvolatile memory unit(s). Non-transitory memory 420 may be a physical device for storing programs (e.g., sequences of instructions) or data (e.g., program state information) for use by computing device 400 on a temporary or permanent basis. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware such as a boot program). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and magnetic disk or tape.
Storage device 430 is capable of providing mass storage for computing device 400. In some implementations, the storage device 430 is a computer-readable medium. In various embodiments, storage device 430 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory, or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional embodiments, the computer program product is embodied in an information carrier in a tangible manner. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 420, the storage device 430, or memory on processor 410.
The high speed controller 440 manages bandwidth-intensive operations for the computing device 400, while the low speed controller 460 manages lower bandwidth-intensive operations. Such assignment of tasks is merely an example. In some implementations, the high-speed controller 440 is coupled to the memory 420, the display 480 (e.g., via a graphics processor or accelerator), and the high-speed expansion port 450, which high-speed expansion port 450 may accept various expansion cards (not shown). In some embodiments, low speed controller 460 is coupled to storage 430 and low speed expansion port 490. The low-speed expansion port 490, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a network device such as a switch or router, for example, through a network adapter.
Computing device 400 may be implemented in a number of different forms, as shown. For example, it may be implemented as a standard server 400a or multiple times in a group of such servers 400a, may be implemented as a notebook computer 400b, or as part of a rack server system 400 c.
Various implementations of the systems and techniques described here can be realized in digital electronic and/or optical circuits, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various embodiments may include embodiments in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. The terms "machine-readable medium" and "computer-readable medium" as used herein refer to any computer program product, non-transitory computer-readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
The processes and logic flows described herein can be performed by one or more programmable processors (also referred to as data processing hardware) executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, one or more aspects of the invention can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor or a touch screen for displaying information to the user) and, optionally, a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with the user, for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user, for example, by sending web pages to a web browser on the user's client device in response to requests received from the web browser.
Many embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other embodiments are within the scope of the following claims.
Claims (28)
1. A method (300) comprising:
receiving an indication of a microphone trigger event (202) at data processing hardware (112) of a sound enabled device (110), the indication being indicative of a possible user interaction (12) with the sound enabled device (110) by speech, the sound enabled device (110) having a microphone (116), the microphone (116) being configured to capture speech when turned on for recognition by an Automatic Speech Recognition (ASR) system (150);
-in response to (122) receiving the indication of the microphone trigger event (202):
instructing, by the data processing hardware (112), the microphone (116) to open or remain open for an open microphone duration window (212) to capture an audio stream (16) in the environment of the sound enabled device (110); and
-providing the audio stream (16) captured by the microphone (116) turned on to the ASR system (100) by the data processing hardware (112) to perform ASR processing on the audio stream (16); and
when the ASR system (150) performs the ASR processing on the audio stream (16) captured by the microphone (116) that is turned on:
-weakening, by the data processing hardware (112), a level of the ASR processing performed by the ASR system (150) on the audio stream (16) based on the function of opening a microphone duration window (212); and
-instructing, by the data processing hardware (112), the ASR system (150) to use the attenuated level (204, 222) of the ASR processing on the audio stream (16) captured by the microphone (116) that is turned on.
2. The method (300) of claim 1, further comprising, when the ASR system (150) performs the ASR processing on the audio stream (16) captured by the microphone (116) that is turned on:
determining, by the data processing hardware (112), whether sound activity is detected in the audio stream (16) captured by the microphone (116) that is turned on;
wherein attenuating the level of the ASR processing performed by the ASR system (150) on the audio stream (16) is further based on the determination of whether any sound activity is detected in the audio stream (16).
3. The method (300) of claim 1 or 2, wherein:
the ASR system (150) initially uses a first processing level (222) to perform the ASR processing on the audio stream (16) at the beginning of the open microphone duration window (212), the first processing level (222) being associated with full processing capability of the ASR system (150); and
attenuating the level of the ASR processing performed by the ASR system (150) on the audio stream (16) based on the function of the open microphone duration window (212) includes:
Determining whether a first time interval has elapsed since the start of the open microphone duration window (212); and
-weakening the level of the ASR processing performed by the ASR system (150) on the audio stream (16) by reducing the level of the ASR processing from the first processing level (222) to a second processing level (222) when the first time interval has elapsed, the second processing level (222) being lower than the first processing level (222).
4. The method (300) of any of claims 1-3, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing comprises: the ASR system (150) is instructed to switch from executing the ASR process on a remote server (140) in communication with the sound-enabled device (110) to executing the ASR process on the data processing hardware (112) of the sound-enabled device (110).
5. The method (300) of any of claims 1-4, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing comprises: the ASR system (150) is instructed to switch from using a first ASR model to a second ASR model for performing the ASR processing on the audio stream (16), the second ASR model comprising fewer parameters than the first ASR model.
6. The method (300) of any of claims 1-5, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing comprises: the ASR system (150) is instructed to reduce the number of ASR processing steps performed on the audio stream (16).
7. The method (300) of any of claims 1-6, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing comprises: the ASR system (150) is instructed to adjust beam search parameters to narrow a decoding search space of the ASR system (150).
8. The method (300) of any of claims 1-7, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing comprises: the ASR system (150) is instructed to perform quantization and/or sparsification of one or more parameters of the ASR system (150).
9. The method (300) of any one of claims 1 to 8, further comprising:
obtaining, by the data processing hardware (112), a current context at the time of receiving the indication of the microphone trigger event (202);
wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing comprises: the ASR system (150) is instructed to bias a speech recognition result (152) based on the current context.
10. The method (300) of any of claims 1 to 9, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing comprises: the ASR system (150) is instructed to switch from a system-on-chip (100) (SOC-based) processing that performs the ASR processing on the audio stream (16) to a digital signal processor (DSP-based) processing that performs the ASR processing on the audio stream (16).
11. The method (300) of any of claims 1 to 10, wherein, when the ASR system (150) uses the attenuated level (204, 222) of ASR processing on the audio stream (16) captured by the microphone (116) that is on, the ASR system (150) is configured to:
generating a speech recognition result (152) for audio data (124) corresponding to a query spoken by the user; and
the speech recognition results (152) are provided to an application to perform the query-specified action.
12. The method (300) of any of claims 1 to 11, further comprising, after instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing on the audio stream (16):
Receiving, at the data processing hardware (112), an indication that a confidence level for a speech recognition result (152) of a voice query output by the ASR system (150) fails to meet a confidence threshold; and
instructing the ASR system (150) by the data processing hardware (112):
increasing the level of ASR processing from the attenuated level (204, 222); and
the voice query is reprocessed using the increased level of ASR processing.
13. The method (300) of any of claims 1 to 12, further comprising, when the ASR system (150) performs the ASR processing on the audio stream (16) captured by the microphone (116) that is turned on:
determining, by the data processing hardware (112), when the attenuated level (204, 222) of the ASR processing performed by the ASR on the audio stream (16) based on the function of the on microphone (116) duration is equal to zero; and
the microphone (116) is instructed to turn off by the data processing hardware (112) when the attenuated level (204, 222) of the ASR processing is equal to zero.
14. The method (300) of any one of claims 1 to 13, further comprising: -displaying, by the data processing hardware (112), a graphical indicator in a graphical user interface (120) of the sound enabled device (110) indicating a attenuated level (204, 222) of ASR processing performed on the audio stream (16) by the ASR system (150).
15. A system (100) comprising:
data processing hardware (112); and
memory hardware (114), the memory hardware (114) in communication with the data processing hardware (112), the memory hardware (114) storing instructions that, when executed on the data processing hardware (112), cause the data processing hardware (112) to perform operations comprising:
receiving an indication of a microphone trigger event (202) at a sound enabled device (110), the indication being indicative of a possible user interaction (12) with the sound enabled device (110) by speech, the sound enabled device (110) having a microphone (116), the microphone (116) being configured to capture speech when turned on for recognition by an Automatic Speech Recognition (ASR) system (150);
-in response to (122) receiving the indication of the microphone trigger event (202):
instructing the microphone (116) to turn on or remain on for an open microphone duration window (212) to capture an audio stream (16) in the environment of the sound enabled device (110); and
-providing the audio stream (16) captured by the microphone (116) turned on to the ASR system (100) to perform ASR processing on the audio stream (16); and
When the ASR system (150) performs the ASR processing on the audio stream (16) captured by the microphone (116) that is turned on:
attenuating the level of ASR processing performed by the ASR system (150) on the audio stream (16) based on the function of opening a microphone duration window (212); and
the ASR system (150) is instructed to use the attenuated level (204, 222) of the ASR processing on the audio stream (16) captured by the microphone (116) that is turned on.
16. The system (100) of claim 15, wherein the operations further comprise, when the ASR system (150) performs the ASR processing on the audio stream (16) captured by the microphone (116) that is turned on:
determining whether sound activity is detected in the audio stream (16) captured by the microphone (116) that is turned on;
wherein attenuating the level of the ASR processing performed by the ASR system (150) on the audio stream (16) is further based on the determination of whether any sound activity is detected in the audio stream (16).
17. The system (100) according to claim 15 or 16, wherein:
the ASR system (150) initially performs the ASR process on the audio stream (16) at the beginning of the open microphone duration window (212) using a first processing level (222), the first processing level (222) being associated with full processing capacity of the ASR system (150); and
Attenuating the level of the ASR processing performed by the ASR system (150) on the audio stream (16) based on the function of the open microphone duration window (212) includes:
determining whether a first time interval has elapsed since the start of the open microphone duration window (212); and
-weakening the level of the ASR processing performed by the ASR system (150) on the audio stream (16) by reducing the level of the ASR processing from the first processing level (222) to a second processing level (222) when the first time interval has elapsed, the second processing level (222) being lower than the first processing level (222).
18. The system (100) of any of claims 15 to 17, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing comprises: the ASR system (150) is instructed to switch from executing the ASR process on a remote server (140) in communication with the sound-enabled device (110) to executing the ASR process on the data processing hardware (112) of the sound-enabled device (110).
19. The system (100) of any of claims 15 to 18, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing includes: the ASR system (150) is instructed to switch from using a first ASR model to a second ASR model for performing the ASR processing on the audio stream (16), the second ASR model comprising fewer parameters than the first ASR model.
20. The system (100) of any of claims 15 to 19, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing includes: the ASR system (150) is instructed to reduce the number of ASR processing steps performed on the audio stream (16).
21. The system (100) of any of claims 15 to 20, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing comprises: the ASR system (150) is instructed to adjust beam search parameters to narrow a decoding search space of the ASR system (150).
22. The system (100) of any of claims 15 to 21, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing includes: the ASR system (150) is instructed to perform quantization and/or sparsification of one or more parameters of the ASR system (150).
23. The system (100) of any one of claims 15 to 22, wherein the operations further comprise:
obtaining a current context at the time of receiving the indication of the microphone trigger event (202);
wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing comprises: the ASR system (150) is instructed to bias a speech recognition result (152) based on the current context.
24. The system (100) of any of claims 15 to 23, wherein instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing includes: the ASR system (150) is instructed to switch from a system-on-chip (100) (SOC-based) processing that performs the ASR processing on the audio stream (16) to a digital signal processor (DSP-based) processing that performs the ASR processing on the audio stream (16).
25. The system (100) according to any one of claims 15 to 24, wherein, when the ASR system (150) uses the attenuated level (204, 222) of ASR processing on the audio stream (16) captured by the microphone (116) that is on, the ASR system (150) is configured to:
generating a speech recognition result (152) for audio data (124) corresponding to a query spoken by the user; and
the speech recognition results (152) are provided to an application to perform the query-specified action.
26. The system (100) of any of claims 15 to 25, wherein the operations further comprise, after instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing on the audio stream (16):
Receiving an indication that a confidence level of a speech recognition result (152) for a voice query output by the ASR system (150) fails to meet a confidence threshold; and
instructing the ASR system (150):
increasing the level of ASR processing from the attenuated level (204, 222); and
the voice query is reprocessed using the increased level of ASR processing.
27. The system (100) of any of claims 15 to 26, wherein the operations further comprise, when the ASR system (150) performs the ASR processing on the audio stream (16) captured by the microphone (116) that is turned on:
determining when the attenuated level (204, 222) of the ASR processing performed on the audio stream (16) by the function of the on microphone (116) duration is equal to zero; and
the microphone (116) is instructed to turn off when the attenuated level (204, 222) of the ASR processing is equal to zero.
28. The system (100) of any one of claims 15 to 27, wherein the operations further comprise: a graphical indicator indicating a reduced level (204, 222) of ASR processing performed on the audio stream (16) by the ASR system (150) is displayed in a graphical user interface (120) of the sound enabled device (110).
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/111,467 US11676594B2 (en) | 2020-12-03 | 2020-12-03 | Decaying automated speech recognition processing results |
US17/111,467 | 2020-12-03 | ||
PCT/US2021/059588 WO2022119705A1 (en) | 2020-12-03 | 2021-11-16 | Decaying automated speech recognition processing results |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116547747A true CN116547747A (en) | 2023-08-04 |
Family
ID=78827641
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180081357.5A Pending CN116547747A (en) | 2020-12-03 | 2021-11-16 | Weakening the results of automatic speech recognition processing |
Country Status (6)
Country | Link |
---|---|
US (2) | US11676594B2 (en) |
EP (1) | EP4244848A1 (en) |
JP (1) | JP7436757B2 (en) |
KR (1) | KR20230109711A (en) |
CN (1) | CN116547747A (en) |
WO (1) | WO2022119705A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11955137B2 (en) * | 2021-03-11 | 2024-04-09 | Apple Inc. | Continuous dialog with a digital assistant |
US20240013782A1 (en) * | 2022-07-11 | 2024-01-11 | Google Llc | History-Based ASR Mistake Corrections |
Family Cites Families (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8463610B1 (en) * | 2008-01-18 | 2013-06-11 | Patrick J. Bourke | Hardware-implemented scalable modular engine for low-power speech recognition |
US9070367B1 (en) * | 2012-11-26 | 2015-06-30 | Amazon Technologies, Inc. | Local speech recognition of frequent utterances |
US11393461B2 (en) * | 2013-03-12 | 2022-07-19 | Cerence Operating Company | Methods and apparatus for detecting a voice command |
US9305554B2 (en) * | 2013-07-17 | 2016-04-05 | Samsung Electronics Co., Ltd. | Multi-level speech recognition |
US9613624B1 (en) * | 2014-06-25 | 2017-04-04 | Amazon Technologies, Inc. | Dynamic pruning in speech recognition |
US9646628B1 (en) | 2015-06-26 | 2017-05-09 | Amazon Technologies, Inc. | Noise cancellation for open microphone mode |
US10115399B2 (en) * | 2016-07-20 | 2018-10-30 | Nxp B.V. | Audio classifier that includes analog signal voice activity detection and digital signal voice activity detection |
US20180025731A1 (en) * | 2016-07-21 | 2018-01-25 | Andrew Lovitt | Cascading Specialized Recognition Engines Based on a Recognition Policy |
US10924605B2 (en) * | 2017-06-09 | 2021-02-16 | Onvocal, Inc. | System and method for asynchronous multi-mode messaging |
JP6862582B2 (en) | 2017-10-03 | 2021-04-21 | グーグル エルエルシーＧｏｏｇｌｅ ＬＬＣ | Display mode-dependent response generation considering latency |
US20190325862A1 (en) * | 2018-04-23 | 2019-10-24 | Eta Compute, Inc. | Neural network for continuous speech segmentation and recognition |
US11062703B2 (en) * | 2018-08-21 | 2021-07-13 | Intel Corporation | Automatic speech recognition with filler model processing |
US11170761B2 (en) * | 2018-12-04 | 2021-11-09 | Sorenson Ip Holdings, Llc | Training of speech recognition systems |
US11017778B1 (en) * | 2018-12-04 | 2021-05-25 | Sorenson Ip Holdings, Llc | Switching between speech recognition systems |
US11475880B2 (en) | 2019-04-16 | 2022-10-18 | Google Llc | Joint endpointing and automatic speech recognition |
US11094324B2 (en) * | 2019-05-14 | 2021-08-17 | Motorola Mobility Llc | Accumulative multi-cue activation of domain-specific automatic speech recognition engine |
US20200410991A1 (en) * | 2019-06-28 | 2020-12-31 | Nuance Communications, Inc. | System and method for predictive speech to text |
CN110362290A (en) * | 2019-06-29 | 2019-10-22 | 华为技术有限公司 | A kind of sound control method and relevant apparatus |
US11749284B2 (en) * | 2020-11-13 | 2023-09-05 | Google Llc | Dynamically adapting on-device models, of grouped assistant devices, for cooperative processing of assistant requests |
-
2020
- 2020-12-03 US US17/111,467 patent/US11676594B2/en active Active
-
2021
- 2021-11-16 WO PCT/US2021/059588 patent/WO2022119705A1/en active Application Filing
- 2021-11-16 KR KR1020237020588A patent/KR20230109711A/en active Search and Examination
- 2021-11-16 CN CN202180081357.5A patent/CN116547747A/en active Pending
- 2021-11-16 EP EP21823435.9A patent/EP4244848A1/en active Pending
- 2021-11-16 JP JP2023534026A patent/JP7436757B2/en active Active
-
2023
- 2023-04-26 US US18/307,736 patent/US20240096320A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20220180866A1 (en) | 2022-06-09 |
EP4244848A1 (en) | 2023-09-20 |
JP7436757B2 (en) | 2024-02-22 |
WO2022119705A1 (en) | 2022-06-09 |
KR20230109711A (en) | 2023-07-20 |
US11676594B2 (en) | 2023-06-13 |
US20240096320A1 (en) | 2024-03-21 |
JP2023549976A (en) | 2023-11-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11132509B1 (en) | Utilization of natural language understanding (NLU) models | |
US20210082420A1 (en) | Dynamically adapting assistant responses | |
US11676625B2 (en) | Unified endpointer using multitask and multidomain learning | |
US20240096320A1 (en) | Decaying Automated Speech Recognition Processing Results | |
JP2021533398A (en) | Dynamic and / or context-specific hotwords for launching automatic assistants | |
KR20200002924A (en) | Hot Word-Recognized Speech Synthesis | |
JP2022534888A (en) | Two-pass end-to-end speech recognition | |
KR20220088926A (en) | Use of Automated Assistant Function Modifications for On-Device Machine Learning Model Training | |
US11763819B1 (en) | Audio encryption | |
JP7173049B2 (en) | Information processing device, information processing system, information processing method, and program | |
US20230298575A1 (en) | Freeze Words | |
KR20200124298A (en) | Mitigate client device latency when rendering remotely generated automated assistant content | |
CN116830075A (en) | Passive disambiguation of assistant commands | |
WO2021145895A1 (en) | Selectively invoking an automated assistant based on detected environmental conditions without necessitating voice-based invocation of the automated assistant | |
JP2023553994A (en) | Adaptation of automatic speech recognition parameters based on hotword characteristics | |
JP2024062993A (en) | Attenuation of automatic speech recognition processing results | |
US11853649B2 (en) | Voice-controlled entry of content into graphical user interfaces | |
US20240161741A1 (en) | Short-Lived Repeat Voice Commands | |
US20230186909A1 (en) | Selecting between multiple automated assistants based on invocation properties | |
KR20180109214A (en) | Touch input processing method and electronic device supporting the same | |
KR20230025907A (en) | Adjust automated assistant functions based on generated proficiency measures | |
KR20240033006A (en) | Automatic speech recognition with soft hotwords | |
WO2023113877A1 (en) | Selecting between multiple automated assistants based on invocation properties | |
KR20230153450A (en) | Device arbitration for local implementation of automatic speech recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |